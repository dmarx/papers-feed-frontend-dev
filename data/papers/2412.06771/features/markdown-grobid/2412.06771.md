# Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty

## Abstract

## 

User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn

## Introduction

A fundamental challenge in the development of AI agents is how to foster effective and efficient multiturn communication and collaboration with human users to achieve user-defined goals, especially when faced with the common issue of vague or incomplete instructions from humans. We focus specifically on text-to-image (T2I) generation, where recent advancements [(Baldridge et al., 2024;](#b3)[Betker et al., 2023;](#b4)[Podell et al., 2023;](#b66)[Yu et al., 2023)](#) have enabled the creation of stunning images from complex text descriptions. However, users often struggle to describe the image they would like to generate in a way that T2I systems can fully understand. This leads to unsatisfactory results and repeated iterations of prompts.

The prompt underspecification problem arises from the inherent ambiguity of natural language, the different assumptions that humans make and the vast space of potential images that can be generated from a single prompt [(Hutchinson et al., 2022)](#). Imagine a prompt generate an image of a rabbit next to a cat. This seemingly simple prompt leaves many important aspects underspecified: What kind of rabbit? What color is the cat? What is their relative positions? What is the background? While a T2I model can generate an image with a rabbit and a cat in it, it is unlikely that the image captures the specific details a specific user has in mind. For example, people in Holland might assume it is common for rabbits to have lop ears, but people in New England might expect to see a cottontail rabbit with straight ears. The combination of all these factors can lead to a frustrating cycle of trial-and-error, with the user repeatedly refining their prompt in an attempt to steer the model towards the desired output [(Huang et al., 2024;](#b37)[Sun and Guo, 2023;](#b72)[Vodrahalli and Zou, 2024)](#).

Instead of relying on passive T2I models that simply generate images based on potentially vague user instructions, we pursue a quest for agency in T2I generation. The T2I agents should actively engage with human users to provide a collaborative and interactive experience for image creation. We envision that these T2I agents will be able to [(1)](#) express and visualize their beliefs and uncertainty about user intents, (2) allow human users to directly control their beliefs beyond just text descriptions, and (3) proactively seek clarification from the human user to iteratively align their understanding with what the human user intends to generate.

In this work, we develop simple prototypes of such agents. At the core of those agent prototypes, we build in a graph-based symbolic belief state, named belief graph, for agents to understand its own uncertainty about possible entities (e.g., rabbit) that might appear in the image, attributes of entities (e.g., rabbit's color), relations between entities and so on. Given a user prompt, we use an LLM and constrain its generation to the graph structure of beliefs, which include probability estimates on the appearance of entities and the possible values for attributes and relations. Figure [1](#fig_0) illustrates the interface and features of the prototypes. In particular, the agent can ask questions based on its uncertainty. For example, a very simple strategy is to find the most uncertain attribute of an entity (e.g., rabbit's color) and use an LLM to phrase a question about the attribute (e.g., What is the color of the rabbit?). The agent can also guide users to directly edit uncertain items in the graph.

Based on user answers to agent questions or direct edits in the belief graph, the agent uses an LLM to update the prompt. It also transitions to a new belief graph by modifying the uncertainty of items clarified by the user. Based on the updated prompt, the agent calls an off-the-shelf T2I model to generate images. The structure of our agent prototypes is highly modular, making it easy to improve each component individually, e.g., changing the strategy for asking questions, updating the belief graph construction method, and switching to better LLMs or T2I models when they become available.

To evaluate the utility of our agent prototypes, we conduct both automatic evaluations and human studies. We develop automatic evaluation pipelines to assess the effectiveness and efficiency of the T2I agents when interacting with simulated users with underspecified prompts answering questions based on their pre-fixed intents. The human studies aim to understand how helpful simple T2I agents can be and evaluate how good the agents' questions and generated images are.

We also create a hand-curated benchmark called DesignBench which contains aesthetic scenes with multiple entities and interactions between entities; it also contains both a short and long caption. DesignBench features diversity between photo-realism, animation and multiple styles allowing a robust testing with the use case of artists and designers in mind.

We run automatic evaluations on DesignBench, the COCO dataset [(Lin et al., 2014)](#b51) and Im-ageInWords [(Garg et al., 2024)](#b22). Results show that our agents can achieve at least 2 times higher VQAScore [(Lin et al., 2024)](#b52) than the traditional single-turn T2I generation within 5 turns of interaction. In our human study, over 90% human subjects expect proactive clarifications to be helpful, about 85% find belief graphs helpful, and 58% think the question asking feature of agents could deliver value to their work very soon, or immediately. Participants prefer images generated by our simple agents over those of single-turn T2I systems in more than 80% cases of the 550 prompt-image pairs used in the human study.

Our contributions: [(1)](#) the first explainable and controllable belief graph used for T2I, (2) novel design and prototypes for T2I agents that adaptively ask clarification questions and present belief graphs; (3) a new automatic evaluation pipeline with simulated users to assess question-asking skills of T2I agents; and (4) DesignBench: a new T2I agent benchmark. [1](#foot_0) Appendix B details the novelty.

## Related work

From the very outset of artificial intelligence, a core challenge has been to develop intelligent agents capable of representing knowledge and taking actions to acquire knowledge necessary for achieving their goals [(McCarthy and Hayes, 1969;](#b55)[Minsky, 1974;](#b56)[Moore, 1985;](#b59)[Nilsson, 2009;](#b62)[Russell and Norvig, 2016)](#b68). Our work is an attempt to address this challenge for intelligent T2I agents.

In machine learning and statistics, efficient data acquisition has been extensively studied for many problems, including active learning [(Cohn et al., 1996;](#b13)[Gal et al., 2017;](#b21)[Houlsby et al., 2011;](#b34)[Ren et al., 2021;](#b67)[Settles, 2009;](#b69)[Wang et al., 2018)](#b78), Bayesian optimization [(Auer, 2002;](#b2)[Garnett, 2023;](#b23)[Hennig and Schuler, 2012;](#b31)[Kushner, 1964;](#b47)[Mockus, 1974;](#b58)[Srinivas et al., 2010;](#b71)[Wang and Jegelka, 2017;](#b77)[Wang et al., 2024b)](#), reinforcement learning [(Ghavamzadeh et al., 2015;](#b27)[Kaelbling et al., 1996;](#b40)[Sutton, 2018)](#b73) and experimental design [(Chaloner and Verdinelli, 1995;](#b6)[Kirk, 2009)](#b44). We reckon that T2I agents should also be capable of actively seeking important information from human users to quickly reduce uncertainty [(Wang et al., 2024c)](#b81) and generate satisfying images. In Â§E, we detail the implementation of action selection strategies for our T2I agents.

In human-computer interaction, researchers have been extensively studying how to best enable Human-AI interaction especially from user experience perspectives [(Amershi et al., 2019;](#b1)[Cai et al., 2019;](#b5)[Chen et al., 2024;](#b9)[HÃ¶Ã¶k, 2000;](#b33)[Kim et al., 2023;](#)[Norman, 1994;](#b63)[ViÃ©gas and Wattenberg, 2023;](#b74)[Yang et al., 2020)](#b89). Interface design for AI is becoming increasingly challenging due to the lack of transparency [(Chen et al., 2024;](#b9)[ViÃ©gas and Wattenberg, 2023)](#b74), uncertainty about AI capability and complex outputs [(Yang et al., 2020)](#b89). We aim to build user-friendly agents, and an indispensable component is their interface to enable them to effectively act and observe, as detailed in Â§G.

## Interpretebaility.

Surfacing an agent's belief overlaps with interpretability as both aim to understand model or agent's internal. Some methods leverage LLM's natural language interface to surface their reasoning (e.g., chain of thought [(Wei et al., 2023a)](#)), sometime interactively [(Wang et al., 2024a)](#). While these approaches make accessible explanations, whether the explanations represent truth has been questioned [(Chen et al., 2023;](#b8)[Lanham et al., 2023;](#b49)[Wei et al., 2023b)](#). Some studies indicate explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations [(Ye and Durrett, 2022)](#b90). In this work, the belief graph does not correspond to the distribution over outputs of the T2I model itself conditioned on the underspecified prompt. Instead, the belief graph is designed to align with the distribution over images generated by the agent, since the agent can construct detailed prompts according to its belief, and feed them into a high-quality T2I model.

Text-to-Image (T2I) generation. Text-to-image prompts can be ambiguous, subjective [(Hutchinson et al., 2022)](#), or challenging to represent visually [(Wiles et al., 2024)](#b85). Different users often have distinct requirements for image generation, including personal preferences [(Wei et al., 2024)](#b82), style constraints [(Wang et al., 2023)](#b79), and individual interpretations [(Yin et al., 2019)](#b91). To create images that better align with users' specific needs and interpretations, it is essential to actively communicate and interact with the user to understand the user's intent.

Multi-turn T2I. Current multi-turn T2I systems typically focus on multi-turn user instructions. [Huang et al. (2024)](#b37); [Sun and Guo (2023)](#b72) propose multi-modal interactive dialogue systems which passively respond to user's natural language instructions. Mini DALLâ€¢E 3 [(Lai et al., 2023)](#b48) builds an interactive T2I framework that accepts user instructions and responds to user questions. [Vodrahalli and Zou (2024)](#) collected and analyzed a dataset of human-AI interactions where users iteratively refine prompts for T2I models to generate images similar to goal images (goal images are only visible to users). This may require users to actively try prompts to understand model behaviors. On the contrary, our work aims to reduce the burden on the user by actively asking questions to understand user intents.

A core challenge in multi-turn T2I is consistency [(Cheng et al., 2024a,b;](#)[Zeqiang et al., 2023)](#b93). [Hu et al. (2024)](#) introduce Instruct-Imagen, which is a model that follows complex multi-modal instructions. AudioStudio [(Cheng et al., 2024a](#)) is a multi-turn T2I framework aimed at subject consistencies while generating diverse and coherent images. These consistency improvement methods can potentially be integrated into our T2I agents since they are highly modular. However, as an ablation, we only focus on the sequential decision making capability of agents to elicit user intents.

Evaluating image-prompt alignment is important for T2I models. Relevant metrics can be embedding-based, such as CLIPScore [(Hessel et al., 2022)](#b32), ALIGNScore [(Zha et al., 2023)](#), VQAbased such as TIFA [(Hu et al., 2023)](#b11), DSG [(Cho et al., 2023)](#b11) and VQAScore [(Lin et al., 2024)](#b52), and captioning-based like LLMScore [(Lu et al., 2023)](#b54). Approaches such as PickScore [(Kirstain et al., 2023)](#b45), ImageReward [(Xu et al., 2023)](#b88) and HPS-v2 [(Wu et al., 2023)](#b86) finetune models on human ratings to devise a metric that aligns with human preferences. Recently, diversity of generated images [(Naeem et al., 2020)](#b60) is also becoming an important metric of measurement to track progress, especially in the geo-cultural context [(Hall et al., 2024;](#b30)[Kannen et al., 2024)](#b42).

In this work, we develop an automatic approach to evaluate agent-user conversations. We adopt VQAScore [(Lin et al., 2024)](#b52) to evaluate the alignment between a ground truth prompt and an image generated by an agent after interactions with a simulated user. Other T2I metrics can also be used.

Prompt expansion is a widely known technique to improve image generation [(Betker et al., 2023)](#b4). ImageinWords [(Garg et al., 2024)](#b22) proposes to obtain high-quality hyper-detailed captions for images, which significantly improve quality of image generation. [Datta et al. (2024)](#b14) present a generic prompt expansion framework used along Text-to-Image generation and show an increase in user satisfaction through human study. While our work can be viewed as a method to adaptively expand a T2I prompt based on user feedback[foot_1](#foot_1) , evaluating our method as a prompt expansion tool is outside of our scope.

## Background

The belief graph in our work is closely related to symbolic world representations.

World states. In classical AI, researchers use symbolic representations to describe the world state [(Kaelbling and Lozano-PÃ©rez, 2011;](#b39)[McCarthy and Hayes, 1969;](#b55)[Minsky, 1974](#b56)[Minsky, , 1988;;](#b57)[Pasula et al., 2007)](#b65). For example, in the blocks world [(Alkhazraji et al., 2020;](#b0)[Ginsberg and Smith, 1988;](#b28)[Gupta and Nau, 1992)](#b29), a state can be

$ğ‘–ğ‘ _ğ‘ğ‘™ğ‘œğ‘ğ‘˜(ğ‘) âˆ§ ğ‘–ğ‘ _ğ‘Ÿğ‘’ğ‘‘ (ğ‘) âˆ§ ğ‘œğ‘›_ğ‘¡ğ‘ğ‘ğ‘™ğ‘’(ğ‘) âˆ§ ğ‘–ğ‘ _ğ‘ğ‘™ğ‘œğ‘ğ‘˜(ğ‘) âˆ§ ğ‘–ğ‘ _ğ‘ğ‘™ğ‘¢ğ‘’(ğ‘) âˆ§ ğ‘œğ‘›(ğ‘, ğ‘),$describing that there are a red block and a blue block, referred to as ğ‘ and ğ‘, block ğ‘ is on a table, and block ğ‘ is on ğ‘. Such world states must include entities (e.g., ğ‘ and ğ‘), their attributes (e.g., position ğ‘œğ‘›_ğ‘¡ğ‘ğ‘ğ‘™ğ‘’, characteristics ğ‘–ğ‘ _ğ‘ğ‘™ğ‘œğ‘ğ‘˜) and relations (e.g., ğ‘œğ‘›(ğ‘, ğ‘)) which are critical for enabling a robot to know and act in the world.

In linguistics, [Davidson (1965](#b15)[Davidson ( , 1967a,b) ,b)](#) introduce logic-based formalisms of meanings of sentences. The semantics of a sentence is decomposed to a set of atomic propositions, such that no propositions can be added or removed from the set to represent the meaning of the sentence. [(Cho et al., 2023)](#b11) propose Davidsonian Scene Graph (DSG) which represent an image description as a set of atomic propositions (and corresponding questions about each proposition) to evaluate T2I alignment.

We borrow the same concept as symbolic world representations and scene graphs, except that the agent needs to represent an imaginary world. The image generation problem can be viewed as taking a picture of the imaginary world. The world state should include all entities that are in the picture, together with their attributes and relations.

Belief states. Term "belief state" [(Kaelbling et al., 1998;](#b41)[Nilsson, 1986)](#b61) has been used to describe a distribution over states. E.g., for block ğ‘, we might have ğ‘(ğ‘œğ‘›_ğ‘¡ğ‘ğ‘ğ‘™ğ‘’(ğ‘)) = 0.5 and ğ‘(Â¬ğ‘œğ‘›_ğ‘¡ğ‘ğ‘ğ‘™ğ‘’(ğ‘)) = 0.5, which means the agent is unsure whether the block is on a table. To represent the T2I agent's belief on which image to generate, we need to consider the distribution over all possible "worlds" in which the picture can be taken. This distribution can be described by the probabilities that an entity appears in the picture, an attribute gets assigned a certain value, etc.

A core difference between our belief graphs and classic belief states is that belief graphs do not need pre-defined predicates (e.g., ğ‘œğ‘›(ğ‘, ğ‘)), but instead automatically produce useful predicates using LLMs. This makes belief graphs much more generalizable across domains. More details in Â§B.

## Proactive T2I agent design

We provide high-level principles and design that guide our agent how to behave and interact with users to generate desired images from text through multi-turn interactions. The goal of the agent is to generate images that match the user's intended image as closely as possible with minimal back-andforth, particularly in cases with underspecified prompts and the agent needs to gather information proactively. This requires a decision strategy on information gathering to trade off between the cost of interactions and the quality of generated images. The formal problem definition can be found in Â§C.

We equip the agent with the ability to gather information in two ways: ask clarification questions ( Â§4.1) and express its uncertainty and understanding in a way that users can edit ( Â§4.2). Once a piece of information is collected from a user, the agent also need to update its questions and uncertainty ( Â§4.3). To enable all these agent behaviors, we need to situate the agent in an interface to effectively communicate with users ( Â§G). In the following, we introduce the design of the above components under the interface, to ensure information efficiency for T2I generation.

## What kind of questions should be asked?

We explain considerations in question asking and examples of strategies in this section.

## Principles

We identify the following principles for an agent to ask the user questions about the underspecified prompt and their intended image: (i) Relevance: The question should be based on the user prompt. (ii) Uncertainty Reduction: The question should aim to reduce the agent's uncertainty about the attributes and contents of the image, the objects, the spatial layout, and the style. (iii) Easy-to-Answer: The question should be as concise and direct as possible to ensure it is not too difficult for the user to answer. (iv) No Redundancy: The question should not collect information present in the history of interactions with the user. The Relevance and No Redundancy principles are self-explanatory, we detail the other two principles below.

The Uncertainty Reduction principle aims to let agent elicit information about various characteristics of the desired image, which the agent is unsure of.

First, the agent needs to know what characteristics of images are important. Some examples include: (i) Attributes of the subjects, such as breed, size, or color, with questions like What kind of rabbit? What color is the cat?; (ii) Spatial relationships between the subjects, such as proximity and relative position (Are the rabbit and cat close to each other? Are they facing each other?); (iii) Background information, such as location, style and time of day (Are they in a park or at home?); and (iv) Implicit entities that might not be explicitly mentioned in the initial prompt but are relevant to the user's vision (Are there any other animals or people present?).

Second, the agent needs to know its own uncertainty about those characteristics. In the agent's belief, the uncertainty is explicit. One strategy is to form questions about the image characteristics that the agent is most uncertain about. We discuss more in Â§E.2.

Third, the agent needs to update its own uncertainty once the user gives a response to its question (a.k.a. transition in Â§4.3). Then, it can construct questions again based on its updated uncertainty estimates. This iterative clarification process allows the agent to progressively refine its understanding of the user's intent and generate an image that more accurately reflects their desired output.

The Easy-to-Answer principle aims to reduce users' effort to respond to questions. One way is to have the agent provide some answer options, where options are what the agent believes likely to appear. E.g., What color is the cat? (a) Black (b) Brown (c) Orange (d) Other (please specify).

## Examples of question-asking strategies

Given the agent belief constructed from the user prompt (more details in Â§4.2), several basic approaches can be employed following the above principles. We construct simple agents with the following strategies, which are implemented and used in our experiments.

â€¢ Ag1 ( Â§E.5): Rule-based question generation, which leverages predefined rules or heuristics to identify salient attributes, entities, or relationships that require clarification. For example, an LLM could be used to estimate the importance and likelihood of different components within the belief, and a heuristic could be applied to prioritize the most crucial elements for questioning.

â€¢ Ag2 ( Â§E.6): Belief-guided question generation, which involves using natural language to represent the current understanding encapsulated in the belief. This representation, along with the conversation history, is provided as input to an LLM, guiding it to generate clarification questions.

â€¢ Ag3 ( Â§E.7): Direct question generation, which write the above question-asking principles in a prompt for an LLM to generate a question.

## Interacting with the user based on agent beliefs

The Uncertainty Reduction principle inspires the usage of belief graphs for the agent to directly express uncertainty, in addition to reflecting uncertainty through questions. Instead of using hardcoded symbols in classic belief representations [(Fikes and Nilsson, 1971)](#b20) described in Â§3, we employ LLMs to generate names and values for entities, attributes and relations. As a result, this belief construction method can generalize across any prompts. Algorithm 1 summarizes how an agent parses a prompt to a belief graph and allows user interaction[foot_2](#foot_2) . All agents in Â§4.1.2 use the same kind of belief graphs. Parse entity attributes and relations from entities and MP (E.9, E.10) 6:

Display belief graph, and collect interaction feedback (F) 7:

Update MP: MP â† MP + F (E.12) 8: end for Entities. In addition to (a) entities mentioned in the user prompt, a belief graph also includes (b) implicit entities not mentioned in the prompt but likely to appear, e.g., pet owner in the context of a pet-related scene; and (c) background entities, such as image style, time of day, location, which play important roles in constructing the image.

## Attributes and relations.

While the prompt might mention some attributes of a certain entity, they are not enough to describe the exact details of that entity. Hence the agent have to imagine the relevant attributes for each entity, and construct a list of possible values along with their associated probabilities (e.g., the color attribute for the cat entity might have values like black, white, gray with corresponding probabilities). Similarly the agent may have to imagine the possible relations between entities, e.g., spatial relation between rabbit and cat might include values like close, far, touching.

Importance scores. While the agent can be uncertain about many aspects of the user's intended image, some are more important than others. E.g., for prompt "a rabbit and a cat", the agent might be very uncertain about the exact color of a carpet that might appear in the image, but rabbit and cat are more important than the carpet. We enable agents to estimate an importance score for each entity, attribute and relation.

## Extracting beliefs and enabling interactions.

A simple idea is to use a large language model (LLM) via in-context learning. Â§E.1 details how an LLM may analyze the user prompt to identify entities, their attributes, and the relations between them, effectively translating the natural language input into a structured representation within the belief. Once the belief is extracted, a user can edit the belief to adjust uncertainty levels, confirm existence of entities etc, as shown in Figure [1](#fig_0).

## Transition

The agent belief undergoes a transition whenever the agent receives new information through user feedback, either user answers from the agent question or user interactions with the graph-based belief interface (Figure [1](#fig_0)). This transition process integrates information from the initial user prompt, the conversation history, interaction and the previous belief to generate an updated belief of the user's desired image. We use a simple approach: Generate a comprehensive prompt that summarizes all interactions and information gathered thus far. This merged prompt is then used to re-generate the belief, effectively incorporating the new information into a refreshed representation. The implementation details can be found in Â§E.3.

## Experiments

We conduct 2 types of experiments to study the effectiveness of the proposed agent design: automatic evaluation which uses a simulated user to converse with a T2I agent and human study which studies the efficacy of our framework with human subjects.

## Automatic evaluation

We simulate the user-agent conversation using self-play [(Shah et al., 2018)](#b70) between two LLMs. The conversation starts with an arbitrarily chosen image to represent the goal image from a T2I model that the user has in mind[foot_3](#foot_3) . Along with this ground truth image, a user has a detailed prompt (i.e., the ground truth) in mind that describes the image in high-detail. We use the algorithm similar to Ag2 (detailed in Â§E.4) to simulate the user, where the questions are answered based on the ground truth prompt and the belief graph generated from the ground truth prompt. We run the agent-user conversation for a total of 15 turns[foot_4](#foot_4) and compute different metrics at the end of each turn. More details of the simulated user can be found in the appendix, including the prompts provided to the LLM when simulating the user are provided. Figure [2](#) part b shows the multi-turn set up that we use in our results.

## Setups for agents and baseline

Baselines. We use a standard T2I model as a baseline, which directly generates an image based on a prompt without asking any questions. We refer to this baseline as 'T2I'.

Agents. We use Ag1, Ag2 and Ag3 with question-asking strategies introduced in Â§4.1.2. The creation and updates to the belief graph ( Â§4.2), as well as transitions to prompt ( Â§4.3) are consistent among all multi-turn agents. Further implementation details of each agent can be found in Â§E.

## Model Selection.

In this work we use an off-the shelve Text-to-Image (T2I) model and a Multi-Modal Large Language (MLLM) model and build the different components of our agent on top of these models. We keep these models consistent across all agents for fair comparison. We implement the agent on top of the Gemini 1.5 (Gemini Team Google, 2024) using the default temperature and a 32K context length. The in-context examples and the exact prompt used at each step of the agent pipeline is detailed in Â§E.8 - Â§E.15. More agent implementation details are provided in Â§E. For T2I generation, we use Imagen 3 [(Baldridge et al., 2024)](#b3) across all baselines given it's recency and prompt-following capabilities. We used both the models served publically using the Vertex API [6](#foot_5) . 

## Datasets.

Our multi-turn agents aim to facilitate the generation of complex images, a process that often requires users to iteratively refine text-to-image (T2I) prompts until the generated image aligns with their mental picture. To evaluate these agents, we curate datasets comprising complex scenes involving multiple subjects, interactions, backgrounds, and styles. Each dataset consists of tuples: (I, ğ‘ 0 , ğ‘, ğ‘ ğ‘”ğ‘¡ ), where I represents the target image, ğ‘ 0 is an initial (basic) prompt describing only the primary elements of the scene, ğ‘ is a ground truth caption providing a detailed description of I, including spatial layout, background elements, and style, and ğ‘ ğ‘”ğ‘¡ is the ground truth belief graph constructed via parsing ğ‘. The initial prompt ğ‘ 0 is intentionally less detailed than ğ‘ to necessitate multi-turn refinement. This framework allows us to assess the agent's ability to guide the user towards the target image I starting from a simplified prompt.

Existing image-caption datasets primarily focus on simple scenes [(Deng et al., 2009;](#)[Deng, 2012;](#b19)[Krizhevsky et al., 2009)](#b46) or focus on very specific categories [(Liao et al., 2022;](#b50)[Liu et al., 2016)](#b53). With the aim for complex realistic images for testing the robustness of the agents, we evaluate over the validation split of the Coco-Captions dataset [(Chen et al., 2015)](#b7). Five independent human generated captions are provided for each image in the dataset. These captions are often short and describe the basic elements contained in the image and the interactions between objects or persons in the image. We therefore select the shortest of the five human-generated captions and use this as a starting prompt ğ‘ 0 . We then use Gemini 1.5 Pro to expand the starting prompt by adding more details of the attributes of the entities in the image as well as the style and image composition which results in the ground truth caption. We also use the ImageInWords [(Garg et al., 2024)](#b22) dataset which takes a diverse set of realstic and cartoon images and has human annotators create dense detailed captions that describe attribute and relationships between objects in the image. In ImageInWords evaluations we use the long human annotation as the ground truth caption.

While COCO-Captions and ImageInWords provide complex, real-world images across diverse backgrounds, it lacks the artistic or non-photorealistic imagery often desired by designers and artists seeking to generate content outside the distribution of typical training data. To better evaluate our target for flexible use cases such as by artists, we introduce DesignBench, a novel dataset comprising 30 scenes specifically designed for this purpose. Each scene follows the (I, ğ‘ 0 , ğ‘, ğ‘ ğ‘”ğ‘¡ ) format described earlier. DesignBench includes a mix of cartoon graphics, photorealistic yet improbable scenes, and artistic photographic images. Examples from DesignBench and a comparison with COCO-Captions are provided in the Appendix.

## Metrics

The outputs produced by the agent include a final generated image, a final caption and a final belief graph. We evaluate the agents across these modalities and evaluate their alignment to the ground truth image I, caption ğ‘ and belief ğ‘ ğ‘”ğ‘¡ , using the following metrics.

## Text-Text Similarity:

We use 2 metrics for comparing the ground truth caption and the generated caption: 1) T2T -embedding-similarity computed using Gemini 1.5 Pro 7 and 2) DSG [(Cho et al., 2024)](#b12) adapted to parse text prompts into Davidsonian scene graph using the released code.

## Image-Image Similarity (I2I):

We compute cosine similarity between the groundtruth image and the generated image from the agent prompt. We use image features from DINOv2 [(Oquab et al., 2024)](#b64) model following prior works.

## Text-Image Similarity:

We compare the ground truth prompt with the generated image (T2I) using VQAScore [(Lin et al., 2024)](#b52). We use the author released implementation of the metric and use Gemini 1.5 Pro as the underlying MLLM.

## Negative log likelihood (NLL):

We construct the ground truth state of the image in the form of a belief graph but with no uncertainty. We then approximately compute the NLL of the ground truth state given the belief of the agent at each turn, by assuming the independence of all entities, attributes and relations, and summing their log probabilities 8 .

## Results from automated evaluation

The results from the automatic evaluations in Table [1](#tab_0) show the I, ğ‘ and ğ‘ ğ‘”ğ‘¡ against each agents final generated image, text and state. All show the mean and standard deviation of the similarity metric at the final agent state. The blue row shows the baseline method which performs no updates to the prompt and instead applies the T2I model to the first prompt. Therefore this baseline represents the lower bound performance.

To add quantitative validity to the ground truth caption generation we perform Text to Image (VQA) Similarity between the ground truth caption and the ground truth over all images in the DesignBench dataset. The mean T2I VQA similarity between the ground truth caption and ground truth image is 0.99999985 with a median 1.0, and standard deviation of 4.5e-07. The mean is extremely close to 1 as expected of an accurate and well formed caption. These numbers can be compared to the T2I column of Table [1](#tab_0) to observe the delta between the ground truth caption and generated captions.

The results in Table [1](#tab_0) show that significant gains in performance come from using proactive 7 Text embeddings are obtained from Embeddings API: [https://ai.google.dev/gemini-api/docs/embeddings](https://ai.google.dev/gemini-api/docs/embeddings). 8 This approximation does not account for potential similarities in the names of entities or attributes. This could lead to approximation errors if, for example, the model confuses "Persian cat" with "Siamese cat" due to their similar names. Addressing this limitation would require incorporating semantic similarity measures into the NLL computation.

multi-turn agents. The blue row shows the simplest baseline which directly uses a T2I model and performs no updates to the initial prompt ğ‘ 0 . We see that all of the multi-turn agents far exceed the baseline T2I model on both datasets and all metrics. Ag3 (the LLM agent that does not explicitly utilize the belief graph to generate questions) show superior performance across all metrics.

The plots in Figure [3](#fig_2) show the T2T, I2I, T2I and NLL metrics, averaged across all images in the ImageInWords dataset, per turn for 15 turns. We see that the multi-turn agents all improve in every metric as they increase the number of interactions. Interestingly we see the T2T and the T2I VQA similarity metric seems to plateau or decrease after about 10 interactions, while the I2I scores continue to increase. The NLL metric shows large performance gains of the Ag3 agent in comparison to all other methods. The plots in Figure [12](#fig_0) shows the T2T DSG metrics. 

## Analysis of quantitative results

The evaluations on the COCO-captions, ImageInWords, DesignBench datasets show similar results and highlight the same patterns across the different agents.

## Multi-Turn agents show clear advantage:

The immediate take away is the baseline which does not use multi-turn interaction and instead passes in the original prompt into the T2I model performs worse than the multi-turn agents on all metrics on both datasets. This confirms our hypothesis that the current T2I agents often produce less desirable images given ambiguity in prompts. In Figure [2](#) we see real outputs of the multi-turn set up with the Ag3 agent.

## LLMs being a part of agents play a significant role:

The best performers (Ag2 and Ag3) both query and LLM to provide a question to ask the user based on contextual information such as the belief graph and conversation history. They query the LLM to construct a concise and clear question but don't impose further constraints on the question construction. Ag1 provides a programatic template for how the LLM should construct the question based on its belief graph and does not provide any conversation history information. Examples of dialogs and the generated questions produced by the three agents can be found in the Appendix in Figure [6](#fig_5). This figure demonstrates that the templated question creation leads to extremely specific questions that often gather minimal information in return. This is an intrinsic limitation of hard coded question selection strategy but also can be an issue of the heuristic scores we defined for question selection in Ag1. In contrast, Ag2 and Ag3 generate questions that are more open-ended thus allowing the user to provide more nuanced details which in  Table [2](#) | Perceived helpfulness of proposed features (% of users) rated by 143 raters.

consequence enhance the agent's image knowledge.

## Question prompts with question-asking principles show advantage over those with beliefs:

The Ag3 agent (which uses an LLM with question generation instructions about entity, attributes etc related to the belief) dominates across all datasets on almost every metric. Ag2 uses the belief explicitly to construct questions by passing the belief into the LLM as information from which to generate the next question. When inspecting the reasoning steps of Ag2, we found that Ag2 excessively relies on importance scores in beliefs to ask questions, and if the importance scores are not estimated properly, the quality of the questions decreases.

## Human studies on generated images and dialogues

To verify the automatic evaluations of the agents, we performed human studies in which participants were asked to rate the generated images and dialogues along different axes. The detailed design of the studies can be found in Figure [20](#), Figure [21](#fig_14) and Figure [22](#).

Participants are asked to rate the images produced by the three proposed multi-turn agents and a single-turn T2I model against a Ground Truth image for which the original prompt was derived and the answers to the agents questions were derived. Approximately 550 image-dialog pairs per

0% 25% 50% 75% 100% 1 -Best 2 -Moderate 3 -Low 4 -Worst No Agreement Ag3 Ag2 Ag1 T2I Rank of Generated Image in Terms of Content Correctness 0% 25% 50% 75% 100% 1 -Best 2 -Moderate 3 -Low 4 -Worst No Agreement Ag3 Ag2 Ag1 T2I Rank of Generated Image in Terms of Style and Aesthetics Figure 4 | Human Rating of the Generated Images. Ratings are based on Content Correctness and Style and Aesthetics. Each human rater is given the Ground Truth Image and Prompt to compare the Generated Image against. Very Close Fairly close some fair differences Equally close and different Fairly different some similarities Very different no similarities Ag1 Ag2 Ag3 Similarity of Generated Image Vs. Prompt and Dialogue The question is too long and complex The question doesn't c important and uncertain attributes The question is irrelevant to the goal image and prompt The question is unclear and ambiguous The question doesn't gain any new information None of the above 0.00% 25.00% 50.00% 75.00% Ag1 Ag2 Ag3 Distribution of Issues in the Questions produced by the Agents agent are rated using 3 human raters. The generated images were presented in a random order and were unlabeled and the human rater was tasked with ranking the images from best to worst. The results from the study are shown in Figure [4](#). We used 3 raters per set of images and therefore in cases where raters did not agree this has been noted via the No Agreement Column. The graph shows that the agentic systems are selected as the best generated image over the single-turn T2I in 80%+ of cases for both content and style.

To validate the generated dialog, human raters are asked to mark any issues a question contains that could pose a disturbance to the user. Approximately 8k questions per agent are rated. The results are shown in Figure [5](#fig_4), where we see that the agentic systems have issues with their questions in 14% or less cases. For the Ag2 and Ag3 the common complaint is that the question is too long while the most common issue for Ag1 is that the question does not gain any new information.

Human raters are also asked to rank the correspondence of each image to the agent-user dialog and original prompt. Approximately 1.5k image-dialog pairs are rated using 3 human raters. Results in Figure [5](#fig_4) show that for all of our agents, more than 96% of the 1.5k image-dialog pairs are rated as very close or fairly close with some differences. This high rating shows the viability of the T2I model employed by the agents, as well as the agents' ability to combine the dialogue into a coherent prompt to feed the T2I model.

## Human studies on the agent interface

To get real user feedback on the agent interface, we performed a human survey with the objective of understanding user frustrations and validating our solutions. We gathered data from 143 participants who all identified to be regular T2I users (at least once a month). Participants were presented with four hypothesized frustrations (prompt misinterpretation, many iterations, inconsistent generations, incorrect assumptions) and three potential mitigating features (clarifications, entity graph, relationship graph; more details in Â§H).

Table [4](#) in Appendix confirms the prevalence of hypothesized frustrations amongst users, with 83% experiencing occasional, frequent, or very frequent frustration due to prompt iterations, followed by 70% for misinterpretations, 71% for inconsistent generations, and 60% experiencing frustration due to incorrect assumptions. Most acutely 55% of participants reported frequent or very frequent frustration due to the prompt iteration frequency necessary. In Table [2](#), we report the mitigation features that are likely to help. Clarifications reported the highest likelihood to help current workflows (91% could / likely / very likely to be helpful), followed by entity graphs (88% could / likely / very likely to be helpful) and relationship graphs (86% could / likely / very likely to be helpful). Clarifications were expected to deliver value immediately / very soon by 58%.

Overall these suggest strong user desire for & likelihood for success of features that reduce iterations and mitigate misinterpretations in T2I generation. Full explanations of the hypothesized frustrations, mitigation and responses splits are in Â§H. All respondents were compensated for their time as per market rates, and were recruited by our vendor to ensure diversity across age, gender, and T2I usage in terms of models, frequency and purpose (work and non work).

## Discussion and conclusion

This work introduces a design for agents that assist users in generating images through an interactive process of proactive question asking and belief graph refinement. By dynamically updating its understanding of the user's intent, the agent facilitates a more collaborative and precise approach to image generation. Moreover, presenting the agent's belief graph can be a generalizable method for AI transparancy, which is an important factor given the increasing complexity of modern AI models.

Modular design. Our agent prototypes are highly modular: the agents use frozen T2I models to generate images based on the prompts that the agent updated. Therefore when a better off-the-shelf T2I model becomes available, it can be directly plugged into the agents and the system will achieve better performance without any additional adaptation 9 .

Personalized content. By asking clarification questions, our agents enable a more customizable and personalized content creation experience. Because different groups of people may perceive helpfulness and harmfulness of contents differently, learning more about the user through clarification questions before generation can potentially mitigate risks of generating contents that are offensive to each specific user, and increase likelihoods of producing helpful outputs.

## Future work.

Alternative to the modular design, one can explore generating images directly from belief graphs and fine-tuning LLM/VLMs on text/image trajectories that include asking questions. These may require a) collecting data such as gold-standard trajectories or annotations on the quality of trajectories of human-agent conversations and b) new approaches to fine-tune the model on multi-turn trajectories of images and text, which can potentially improve the performance of the agent. 9 T2T scores in Table [1](#tab_0) ablates the T2I model and only performs similarity on the captions. Our agents have achieved a 92%+ T2T score, showing that their performance can be boosted by adopting better T2I models. atoms are instantiated predicates [(Alkhazraji et al., 2020;](#b0)[Garrett et al., 2020a,b)](#), so that whenever an action is applied, the agent can apply transition by adding and deleting items in the set according to the precondition and effect of the action. For T2I tasks, it is more convenient to use a graph to represent the world state associated with an image, since entities and relations naturally form a set of nodes (entities) and edges (relations between entities). Each component of the graph can also have probabilities, making it easy to turn a world state into a belief state using the same data structure. Hence we represent T2I agent beliefs using graphs. The agent can directly update the graph for each transition instead of going through a set or list.

â€¢ Interpretability and controllability: The graph structure makes our agent belief more interpretable than traditional belief states, since we can visualize and progressively disclose the graph to the human user. Moreover, each node or relation in the belief graph has associated descriptions, making it easy for the user to understand and potentially edit every component of the belief graph. In our human studies, about 85% of raters found the belief graph useful. To the best of our knowledge, our work is the first to use the graph-based belief state for human-AI interaction. 3. Automated evaluation of T2I agents: We propose a novel automated evaluation approach for T2I agents using self-play. The agent interacts with a simulated user that has access to the original image and its long caption. See Â§5.1 and Â§E.4 for the full details of how the simulated user is constructed. This evaluation pipeline is easy to use and can help the future development of T2I agents. 4. DesignBench: We envision that a significant fraction of T2I users are artists and designers, and it is important to ensure that T2I agents are evaluated for these use cases. Hence we create DesignBench, featuring photo-realism, animation and multiple styles with short and long captions. DesignBench can be directly plugged into our automated evaluation to streamline the evaluation process.

## C. Formalism of the agent and its objective

We define an interactive T2I agent as a âŸ¨ğµ, ğ´, ğ‘‚, ğœ, ğœ‹âŸ© tuple, where we have

â€¢ ğ‘†: a representation space of images,

â€¢ ğµ: a space of agent beliefs,

â€¢ ğ´: a space of actions that the agent can take,

â€¢ ğ‘‚: a space of agent observations of the user,

â€¢ transition function ğœ : ğµ Ã— ğ´ Ã— ğ‘‚ â†¦ â†’ ğµ for updating beliefs given new interactions,

â€¢ action selection strategy ğœ‹ : ğµ â†¦ â†’ ğ´, which specifies which action to take given a belief.

For each user-initiated interaction, we assume that there exists a specific intent ğ‘  âˆˆ ğ‘†, where ğ‘† is the space of all possible user intents. For a T2I task, we assume that the intent is the image the user would like to generate, and the intent stays the same throughout the interaction with an agent. We discuss more about the validity of this assumption in Â§6.

Each type of T2I agents can have a unique user intent representation, belief representation, construction of the action space, and user interface design to obtain observations of users.

In Â§4, we show the examples for these components.

We use a score function, ğ‘“ : ğµ Ã— ğ‘† â†¦ â†’ â„, to evaluate the alignment between an agent belief and a user intent at any turn of the interaction. Function ğ‘“ can only be evaluated in hindsight once the user intent is revealed. The agent does not have direct access to function ğ‘“ since the user intent is hidden

## Target Image Initial Prompt

A photo of a cake that is adorned with berries sitting on a table. from the agent. However, the agent may construct a probabilistic distribution over function ğ‘“ based on its belief about the user intent. The goal of the agent is to maximize function ğ‘“ with as few turns of interaction with the user as possible.

## D. Visualization of Multi-Turn Agent-User Dialogs and Generated Images

In Figure [6](#fig_5), we show examples of multi-turn dialogs between simulated users and the three agents in Section 5. We also visualize the generated images in Figure [7](#), Figure [8](#), Figure [9](#) and Figure [10](#fig_0).

## E. Implementation details of agent prototypes

We propose three T2I agents, each characterized by a unique configuration of âŸ¨ğµ, ğ´, ğ‘‚, ğœ, ğœ‹âŸ© tuples:

â€¢ Ag1: Heuristic Score Agent. This agent incorporates a human-defined heuristic score based on the belief to guide question generation. This heuristic score reflects the perceived importance of different aspects of the belief in driving the conversation forward; â€¢ Ag2: Belief-prompted Agent. This agent leverages an LLM to generate questions by processing both the conversation history and a structured representation of the belief. â€¢ Ag3: Principle-prompted Agent. This agent generates questions directly from the conversation history based on the principles introduced in Â§4.1.1. The question asking strategy of Ag3 relies solely on the implicit knowledge and reasoning capabilities of the underlying LLM.

Goal Image Ag3 Ag2 Ag1 T2I-Only  history, and the current belief, utilizing an ICL prompt ( Â§E.14) to guide question generation. The LLM then formulates a clarification question aimed at eliciting information about key features of the image, naturally prioritizing those with higher Importance to ask score within the belief. â€¢ Ask Important Clarification Question directly (ğ´ğ¼ğ¶ğ‘„ ğ‘ğ‘ğ‘ ğ‘’ ): This strategy relies on the LLM's inherent ability to identify important aspects of the user prompt and conversation history. The LLM ( Â§E.13) generates an important clarification question based on its implicit understanding of the user's needs, without explicitly relying on the structured information in the belief.

Ag1 employs ğ‘€ ğ» ğ¼ğ‘† strategy for question generation. This strategy leverages the importance scores assigned to entities, attributes, and relations within the belief graph. It identifies the element with the highest heuristic importance score and formulates a question aimed at eliciting further information about that specific element. The question is then verbalized using the LLM described in Section Â§E.15.

Ag2 utilizes the parsed belief graph as the basis for question generation. It employs the ğ´ğ¼ğ¶ğ‘„ ğµ strategy, which leverages the structured information within the belief graph to generate targeted clarification questions.

Ag3 relies solely on the conversation history for question generation. It employs the ğ´ğ¼ğ¶ğ‘„ ğ‘ğ‘ğ‘ ğ‘’ strategy, which leverages the LLM's ability to understand the ongoing dialogue and identify key areas requiring further clarification.

## E.3. Implementation of belief transition

Both Ag1 and Ag2 require belief updating to incorporate new information gained during the interaction in order to compose clarification questions. At each turn, we perform prompt merging to create a comprehensive prompt that summarizes the conversation history. This merged prompt is then used for belief parsing to obtain an updated belief graph. For Ag2 (and Ag3), this updated belief graph directly informs the subsequent interaction. For Ag1, it incorporates additional post-processing mechanisms to enhance memory and prevent redundant questioning: (i) Redundancy elimination: If an attribute or relation has already been addressed in the conversation history, the corresponding user response is assigned as the sole candidate with a probability of 1.0, and its importance score is set to 0. This prevents the agent from repeatedly asking about the same information. (ii) Information retention: If an attribute or relation from the conversation history is absent in the updated belief graph, it is explicitly added. This ensures that the agent retains crucial information even if it's not explicitly present in the latest parsed belief graph.

## E.4. User simulation

To simulate end-to-end agent-user interactions, we implement a user simulator that mimics human question-answering behavior. This simulator operates as follows:

â€¢ It generates a belief graph based on a ground truth prompt, representing the user's intended image. This serves as the simulator's internal representation of the desired image. â€¢ Mirroring the ğ´ğ¼ğ¶ğ‘„ ğµ strategy, the simulator takes the ground truth prompt, conversation history, and its current belief graph as input. It then leverages an ICL prompt (see Â§E.14) to generate a response to the agent's question. This ensures that the simulator's answers are consistent with its internal belief graph and the ongoing conversation.

## E.5. Ag1: Heuristic Score Agent

The Heuristic score agent leverages the importance scores and probabilities within the belief graph to guide its question-asking strategy. The underlying principle is to identify and inquire about the entity, attribute, or relation that exhibits both high importance and high uncertainty. This aligns with the uncertainty reduction principle discussed in Â§4.1.1, which emphasizes minimizing uncertainty through targeted questioning. To achieve this, we define a heuristic importance score as formulated in Equation [1](#), and the agent then selects the attribute or relation with the highest heuristic importance score as the focus of its inquiry. To facilitate easy answering, we utilize an LLM to generate userfriendly questions with multiple-choice options. For example, the agent might ask: What color of the rabbit do you have in [mind? a. black , b. white, c. brown. d. unkown](#). If none of these options , what color of the rabbit do you have in mind?. This format allows users to simply select the most appropriate option or provide their own answer if needed.

Here's a summary of Ag1's implementation: (i) Belief Representation: The agent's belief comprises the merged prompt and the current belief graph. (ii) Select Action: ğ‘€ ğ» ğ¼ğ‘† strategy is employed to identify the attribute or relation of interest based on the heuristic importance score. (iii) Verbalize Action: An LLM ( Â§E.15) is used to generate a clear and concise question about the selected attribute or relation. (iv) Answer Question: The user simulator provides an answer to the agent's question, mimicking human response behavior. (v) Transition: The agent updates the merged prompt with the new information, re-generates the belief graph based on the updated prompt, and applies the post-processing logic outlined in Â§E.3 to ensure consistency and prevent redundancy.

## E.6. Ag2: Belief-prompted Agent

The Ag2 agent incorporates the belief graph into its decision-making process but adopts a different approach compared to Ag1. Instead of relying on a heuristic score, Ag2 leverages the full capacity of an LLM to generate questions. It provides the LLM with comprehensive information, including the merged prompt, belief graph, and conversation history, allowing the LLM to formulate the most informative questions possible. To guide the LLM towards generating effective questions, we incorporate specific instructions in the prompt, emphasizing the following principles: The question should be as concise and direct as possible. The question should aim to obtain the most information about the style, entities, attributes, spatial layout and other contents of the image. Remember to ask for information that are critical to knowing the critical details of the image that is important to the user. The question should reduce your uncertainty about the user intent as much as possible.

Here's a summary of Ag2's implementation: (i) Belief Representation: The same as Ag1, the agent's belief consists of the merged prompt and the current belief graph. (ii) Select Action: ğ´ğ¼ğ¶ğ‘„ ğµ strategy is employed, which leverages an LLM to generate a question based on the comprehensive input information. (iii) Verbalize Action: Since the LLM directly generates the question, no separate verbalization step is required. (iv) Answer Question: The user simulator provides an answer to the agent's question. (v) Transition: The agent updates the merged prompt with the new information and re-generates the belief graph based on the updated prompt.

## E.7. Ag3: Principle-prompted Agent

A simple and effective implementation of LLM-based multi-modal dialogue systems is to use the context to store the history of conversations between the system and the user, and directly generate the next response based on the context.

To align with the principles outlined in Â§4.1.1, we guide the LLM's question generation with a prompt ( Â§E.13) that emphasizes all principles: Based on the original prompt and chat history please provide a question to ask about the image. The question should be as concise and direct as possible. The question should aim to learn more about the attributes and contents of the image, the objects, the spatial layout, and the style. The prompt also includes the history of conversation. This strategy aims to generate questions that are easy for users to understand and answer, while effectively reducing the agent's uncertainty about the desired image.

Here's a summary of Ag3's implementation: (i) Belief Representation: The same as Ag1 and Ag2, the agent's belief consists of the merged prompt and the current belief graph. (ii) Select Action: ğ´ğ¼ğ¶ğ‘„ ğ‘ğ‘ğ‘ ğ‘’ strategy is employed, which leverages an LLM to generate a question based on the conversation history. (iii) Verbalize Action: The LLM directly generates the question, so no separate verbalization step is needed. (iv) Answer Question: The user simulator provides an answer to the agent's question. (v) Transition: The same as Ag2, the agent updates the merged prompt with the new information and re-generates the belief graph based on the updated prompt.

## 57

}}, 58 {{ 59 "name": "tree ", 60 "importance_to_ask_score": 0, 61 " description ": " trees in the background", 62 "entity_type ": " explicit ", 63 "probability_of_appearing": 0 64 }} 65 {{ 66 "name": "camera angle", 67 "importance_to_ask_score": 0.8, 68 " description ": "the camera angle of the image", 69 "entity_type ": "background", 70 "probability_of_appearing":

1.0 71 }}, 72 {{ 73 "name": "weather", 74 "importance_to_ask_score": 0.8, 75 " description ": "weather", 76 "entity_type ": "background", 77 "probability_of_appearing": 1.0 78 }}, 79 {{ 80 "name": "image style ", 81 "importance_to_ask_score": 1.0, 82 " description "the style of the image", 83 "entity_type ": "background", 84 "probability_of_appearing": 1.0 85 }}, 86 {{ 87 "name": "background color", 88 "importance_to_ask_score": 0.8, 89 " description ": "the background color of the image", 90 "entity_type ": "background", 91 "probability_of_appearing": 0.5 92 }} 93 ] 94 95 ... [[a few additional examples]] ... 96 97 98 Identify the entities given the input given below. Strictly stick to the format. 99 Input: {{ 100 "user_prompt": "{user_prompt}" 101 }} 102 Output:

## E.9. Attribute Parser Prompt Instruction

1 Given a text-to-image prompt and a particular entity described in the prompt, and your goal is to identify a list possible attributes that could describe the particular entity . Output Requirements: 2 3 1. if this attribute has already existed as an entity in other existing entity list , then do not include it . 4 2. the attribute candidate could be a mixed of values like ' color A and color B '. 5 3. The output should be a json parse-able format: 6 7 name (str): The name of the attribute . 8 importance_to_ask_score (float): The importance score of asking a question about this attribute to reduce the uncertainty of what the image is given the user prompt. This is a number between 0 and 1, higher means more important. Consider these factors when assigning scores: 1. Increate the score for attributes that are the primary attributes of an important entity; 2. significantly increase the score for attributes that could strongly influence the generation or portrayal of OTHER attributes in the scene; 3. descrease the score for attributes that are already well specified in the prompt. For example, a breed of a dog would impact other attributes like color , size , etc . So the breed attribute should have a higher importance score than color, size , etc . Assign a much lower score if the attribute ' s value is already mentioned in the user prompt. 9 candidates ( List of names and probabilities): List of possible values that the attribute can take. Make sure to generate atleast 5 or more possible values. These should be realistic for the given entity . For each attribute , returns the probability that the user wants this candidate based on the user prompt. If it ' s already mentioned by the user, only generate one candidate (the mentioned one) and assign 1.0 as the probability . The sum of probabilities over all candidates shall be 1. Also infer the probability based on the prompt. For example, for a dog with breed Samoyed, the color attribute has a very high probability of white. 10 11 Below are two examples of input and output pairs: 12 13 Example 1: 14 Input: {{ 15 "user_prompt": "generate an image of a white rabbit running on grass", 16

" entity ": " rabbit ", 17 " other_existing_entities ": "grass" [18 }} 19](#) Output: [ 20 {{ 21 "name": "color ", 22 "importance_to_ask_score": 0.9, 23 "candidates": {{"white":1.0}} 24 }}, 25 {{ 26 "name": "breed", 27 "importance_to_ask_score": 1.0, 28 "candidates": {{"Dutch": 0.20, 29 "Mini Lop": 0.15, 30 "Netherland Dwarf": 0.15, 31 "Lionhead": 0.10, 32 "Flemish Giant": 0.10, 33 "Mini Rex": 0.10,

## E.11. Verbalization Prompt Instruction

1 The chat history is as follows : 2 question: {action. verbalized_action} and answer: {observation}.

3 Turn the question and action into a single declarative sentence that describes the answer -do not phrase it as a question. Example output: the firetruck in the image is red.

## E.12. Merge Prompt Prompt Instruction

1 You are writing a prompt for a text-to-image model based on user feedback. The original prompt is {prompt}. The user has provided some additional information: { additional_info}. Please write a new prompt for the text-to-image model. The new prompt should be a meaningful sentence or a paragraph that combines the original prompt and the additional information. Do not add any new information that is not mentioned in the prompt or the additional information. Make sure the information in the original prompt is not changed. Make sure the additional information is included in the new prompt. Make sure the new prompt is a description of an image. If the additional information or the original prompt specifically says that a thing does not exist in the image, you should make sure the new prompt mentions that this thing does not exist in the image. DO NOT generate rationale or anything that is not part of a description of the image. 4 Each entity has "name", "descriptions ", "importance to ask score" and " probability of appearing". "Name" is the identifier of the entity . "Descriptions" is the description of the entity . "Importance to ask score" is how important it is for the agent to ask whether the entity exists . Probability of appearing" is the probability the agent estimated that this entity exits in the image. 5 6 Each entity has a list of attributes . Each attribute has "name", "importance to ask score" and "candidates". "Name" is the identifier of the attribute . "Importance to ask score" is how important it is to ask about the exact value for the attribute of the entity . "Candidates" is a list of possible values for the attribute . 7 8 Each candidate value has a probability that describes how likely this candidate value should be assigned to the attribute . 9 For example, "Attribute Name: color, Importance to ask Score: 0.9, Candidates: [white: 0.5, black: 0.5]" means the color is either white or black, each with 0.5 probability .

## E.13. ğ´ğ¼ğ¶ğ‘„ ğ‘ğ‘ğ‘ ğ‘’ Prompt Instruction

If you ask about attributes , you should ask about the attribute with the highest uncertainty. Your uncertainty can be judged by the probabilities . If the probabilities are 0.5 and 0.5, you are uncertain. If the probabilities are 0.1 and 0.9, you are fairly certain . 10 11 The agent belief is : 12 { belief_state . __str__()} 13 14 Based on the user prompt "{user_prompt}" and the belief of the agent, please provide a question to ask about the image. The question should be as concise and direct as possible . The question should aim to obtain the most information about the style , entities , attributes , spatial layout and other contents of the image. Remember to ask for information that are critical to knowing the critical details of the image that is important to the user. The question should reduce your uncertainty about the user intent as much as possible. DO NOT ask question that can be answered by common sense. DO NOT ask question that are obvious to answer based on the user prompt "{user_prompt}". DO NOT ask any question about information present in the following user-agent dialogue within <dialogue> and </dialogue> markers. 15 16 <dialogue> 17 {conversation} 18 </dialogue> 19 20 DO NOT ask any question that has been asked in the dialogue above. 21 22 Your question does not have to be entirely decided by the belief . You can construct any question that make yourself more confident about what the image is. 23 Think step by step and reason about your uncertainty of the image to generate. Make sure to ask only one question. Make sure it is not very difficult for the user to answer.

For example, do not ask a very very long question, which can take the user a long time to read and answer. 24 Make sure that you question the answer within <question> and </question> markers.

## E.15. HSA Question Prompt Instruction

1 You are constructing a text-to-image (T2I) prompt and want more details from the user. 2 You have to ask a question about the the most important entity or the attribute of the most important entity. 3 We have entity types: ( i ) explicit : directly ask question with options; ( ii ) implicit : ask whether this entity required for the image with yes or no as options; ( iii ) background: ignore the attribute value and directly ask the value of the entity . (iv) relation : add keyword like ' relation ' to emphasize this entity is a relation . 4 Construct a simple question that directly asks this information from the user and also provides option that the user can pick from. Ask only one question and follow it with options. 5 6

## F. Automated evaluation

In Algorithm 2, we show the user-agent self-play procedures that we used to perform all automated evaluation.

## ImageInWords T2T DSG

Coco Captions T2T DSG

## DesignBench T2T DSG

Figure [12](#fig_0) | DSG score comparison between ground truth prompt and agent generated prompt reported at each turn. The performance of all agents increase with increase in number of turns.

Algorithm 2 User-Agent Self-Play Algorithm

1: Input: Initial prompt ğ‘ 0 , User ğ‘¢, Agent ğ‘ (with ğ‘ 0 ), ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘¢ğ‘Ÿğ‘›ğ‘  2: Output: Refined prompt ğ‘ ğ‘“ 3: ğ‘ ğ‘“ â† ğ‘ 0 4: for ğ‘¡ğ‘¢ğ‘Ÿğ‘›_ğ‘–ğ‘‘ = 0 to ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘¢ğ‘Ÿğ‘›ğ‘  -1 do 5: ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› â† ğ‘.SelectAction() 6: ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› â† ğ‘.VerbalizeAction(ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›) 7: ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ â† ğ‘¢.AnswerQuestion(ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›) 8: ğ‘.Transition(ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ) 9: ğ‘ ğ‘“ â† ğ‘. ğ‘ğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ 10: end for 11: return ğ‘ ğ‘“

## G. Details on the agent interface

Below is a showcase of how users could interact with the belief graph and clarifications in a hypothesised interface, to better iterate their inputs, to reach higher a quality and satisfaction of outputs. This is a crudely hypothesised, intentionally simple interface for the sake of research, but could be iterated and improved upon in many ways depending on application and users.

1. Default state On load of the app, there would be a text prompt input and space for output images, as is common across typical T2I interfaces. There would also be space for the user to view either clarifications from the model, or a graph interface, as part of the overall "input" section as these would act as a further input for future model output iterations. See Figure [13](#fig_9)

## Output images, with Clarifications

Once the user has submitted the prompt and the model has responded, there would be a set of images, as initial outputs from the users prompt. Below the input prompt would be a set of "Clarifications" in its populated state. These clarifications would ask the user specific questions that would be necessary to increase the specificity of the prompt, for the model to get a more accurate results aligned to the users intention, or to help the user realise their intention. Options would be given of the highest probability options for each Clarification, but the user could also fill in a totally new option via a free text field. Once answered by selection or text input, the clarifications would be added to the above, primary prompt for regeneration when the user selects. See Figure [14](#fig_10) below as reference. stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create CLARIFICATIONS GRAPH <prev. 1 of 23 next> Reload refresh Q: Where exactly on the Eiffel Tower the Olympic located? Top Middle Bottom Side Type here Q: What style is the Eiffel Tower depicted in? Photorealistic Digital painting Sketch 3D Render Type here Q: What is the angle from which the Eiffel Tower is depicted ? Front Side High Low angle Type here Q: What is the overall lighting condition in the image? Bright sunlight Overcast soft light Type here Image goes here Image goes here Image goes here goes here 

## Graph Entities & Attributes

Instead of the clarifications, the user could select to instead view a Graph by clicking Tab above the clarifications themselves. This graph would be populated will all Entities from the prompt explicit and implicit visually defined differently (in this diagram by the dotted line surrounds implicit entities, but is a filled line when surrounding explicit entities). The graph layout will be structured, depicting relationships concentrically i.e. "on", "in" or "under" for example, will become child entities, and be displayed within the parent entities' boundary. For example a 'Mug' that has the relationship of 'on' a 'Table' entity, will sit within the boundary of 'Table ', as also would a 'Plate' if that had the same child-parent relationship.

Below the Graph would also be a list of 'cards' (i.e. boxed groups of information), one for each "explicit" or "implicit" entity. Within each card a user could see the status of implicit / explicit, and change this status to confirm or deny its presence. The user could also see a list of "attributes" associated to that entity, which the model has assumed. Each of these attributes could be changed by interacting with a list of alternatives via drop down. These lists are determined in terms of which items and order of items, based on the probability by which the model sees them, ordered with higest first. This probability would be made clear to the user to define the order by seeing the peercentage next to the label. See Figure [15](#fig_4) below as reference.

## Graph Relationships

The user would also be able to change the state of the Graph and Cards,

stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create CLARIFICATIONS GRAPH Background entities 'Camera angle' 'Lighting' 'Daytime' 9. 10. 11. Physical Entities 'Eiffel Tower' 1. 4. 'Sky' (implicit) 'Olympic Rings' 'People' 6. 'Trees' (implicit) 7. 'Seine River' (implicit) 8. 'Clouds' (implicit) 5. 'Paris Cityscape' (implicit) 3. 10. 11. 12. 13. 14. 15. View: center_f Entities & attributes swap_hor Relationships <prev. 1 of 3 next> 1. Eiffel Tower Explicit -100% conf. Description: Eiffel Tower, a prominent landmark in Paris, France" Existence: Yes check No Attributes: Material: Iron arrow_dr Height: Original arrow_dr Detail level: High arrow_dr State of repair: Well-maintained arrow_dr 2. Olympic Rings Explicit -100% Description: "The Olympic rings, a symbol of the Olympic Games" Existence: Yes check No Attributes: Size: Large arrow_dr Material: Metal arrow_dr Position: Near top arrow_dr Orientation: Horizontal arrow_dr 3. Paris Cityscape -82% conf. Description: "The surrounding urban landscape of Paris, France" Existence: Yes No Attributes: Weather: Sunny arrow_dr Time day: Midday arrow_dr Season: arrow_dr Architectural style emphasis: Haussmann arrow_dr OUTPUTS Image goes here Image goes here Image goes Image here Figure 15 | Interface with Graph displaying Entities, with cards below enabling a user to change attributes associated to each entity. to instead focus on the relationships between entities, by toggling to "Relations". In this state the user would be able to focus on two specific entities (e.g. 'mug' and 'table'), see the description of the relationship (e.g. 'the mug is sitting on the table') and if desired change the relationship to an alternative (e.g. 'on', changed to 'under') via a drop down of options which the model determined as alternative options ordered by probability, as per attributes. See Figure Figure 16 below as reference.

Once any of these changes are made the user could initiate a regeneration via the updated prompt to create a new set of output images, which can then be further refined via the same method.

## H. Details on user studies for the agent interface

Below we describe the exact guideline definitions we shared with the user for a user study.

## H.0.1. Hypothesized Frustrations

We presented participants with the following hypothesized frustrations related to T2I model usage:

1. Prompt Misinterpretation: The model misunderstands complex relationships between entities in the input prompt. 2. Many Prompt Iterations: The model does not immediately generate what the user intends, requiring numerous iterative changes to the input prompt. 3. Inconsistent Generations: The model reinterprets the input prompt differently between iterations, causing unwanted changes in the generated images. 4. Incorrect Assumptions: The model makes incorrect assumptions or no assumptions when encountering gaps in the details provided in the input prompt, leading to undesired outputs.

## Graph of Prompt Relationships:

A visual representation of relationships between entities in the prompt, allowing users to see and edit these relationships. E.g., seeing that "donut" is "next to" "coffee" and allowing the user to change the relationship to "on top of."

The questions asked for each feature were:

1. "How likely this feature is to help your current workflow if you had it now?". With response options of: "Very unlikely to help", "Unlikely to help", "Could help", "Likely to help", "Very likely to help". 2. "How soon would this feature deliver value to your work?" with response options of: "Very soon / immediately", "Sometime, "Not very soon".

Image references were given for each Feature as listed out below:

1. Clarifications: stream_c App INPUT A dominating image of the Eiffel Tower with the Olympic rings prominently displayed on the structure. Create Input clarifications (5) Q: Where exactly on the Eiffel Tower are the Olympic rings located? Top Middlecheck Bottom Side Q: What style is the Eiffel Tower depicted in? Photorealistic Digital painting Sketchcheck 3D Render Q: What is the angle from which the Eiffel Tower is depicted ? Front Side High angle Low anglecheck Q: What is the overall lighting condition in the image? bright sunlightcheck Overcast soft light Reload refresh OUTPUTS Image goes here Image goes here Image goes here Image goes here Figure 17 | Stimulus image in the survey to test the Model clarifications feature. 2. Graph of Prompt Entities: stream_c App INPUT A chocolate glazed donut topped with chopped nuts sits on a plate with a bite taken out of it, next to a full mug of coffee Create Input graph Context 'Plate' 'Donut' 'Nuts' 'Mug' 'Coffee' 1. 'In Kitchen' (assumed) 'Morning Light' (assumed) 'Kitchen' (assumed) 'Table' (assumed) 5. 6. 7. 8. 9. 4. 2. 3. Entities center_f Entities & Attributes 6. Assumed entity: Table (94% conf.) Description: "a table on which the plate, donut, mug and coffee sit on" Existence: Yes check No Attributes: Shape: Round arrow_dr Size: Small arrow_dr Table cloth None arrow_dr Material: Wood arrow_dr Other items Empty arrow_dr 7.  5. Relation name: mug-table Description: "mug sitting on a table Spatial relationship: "on" arrow_dr 6. Relation name: plate-table Description: "plate sitting on a table" Spatial relationship: "on" arrow_dr Reload refresh OUTPUTS Image goes here goes here Image here Image goes here Figure 19 | Stimulus image in the survey to test the Model Graph of Entity Relations feature. Template of Human Rater Task 1: Evaluation of Issues in Individual a. 1, 2, equal

## Examples

Task #1 (10 turns * 25 images) * 2 methods

## Example Dialogue Analysis:

Instructions: Pretend you are in the following scenario -you are asking an AI model to create an image you have in mind, this image is displayed on the left. You first prompt the model with a short non-detailed description of the image, this description is called "original prompt" and is below the goal image on the left.

The model proceeds to ask you various questions to understand the specificities of the image you are trying to create. The dialogue between you and the AI model is shown in the middle column. You will go through the dialog turn by turn and answer the same rater questions about each turn so just focus on the highlighted turn rather than the entire dialog.

Your job in this task is to rate the clarity, soundness and efficiency of the highlighted question in the dialogue. This question was asked by the AI model in order to generate an image similar to the one you had in mind. Provide your rating by answering the questions in the rightmost column. 

## Goal

## Example Image Analysis:

Pretend you are in the following scenario -you asking an AI model to create an image you have in mind, this image is displayed on the left. You first prompt the model with a short non-detailed description of the image, this description is called "original prompt" and is below the goal image on the left.

The model proceeds to ask various questions to understand the specificities of the image you are trying to create. The dialogue between you and the AI model is shown in the middle column.

Based on the original prompt and the human and AI dialogue -the AI model produces a final image shown in the 3rd column.

Your job in this task is to rate the produced image based on how well it fits the goal image and separately how well it fits the prompt and the dialogue. Do this by answering the questions in the rightmost column.

## Goal Image Original Prompt

A fluffy gray and white cat with a butterfly on its face. Your job in this task is to compare the two model's produced images against each other given the goal image and original prompt. Do this by answering the questions in the rightmost column.

## Human

## Goal Image

Original Prompt A fluffy gray and white cat with a butterfly on its face. Rank which image matches best to the goal image in terms of style and aesthetics?

## Pilot Study

The pilot study will consist of 20 triplets of prompt, dialog and generated image. We aim to get 1 rater to rate each of these triplets. We will use the same 20 triplets for each of the 3 rater tasks outlined above, leading to a total of 60 tasks completed by single raters.

Figure [22](#) | A template of the task presented to human raters. Human raters are asked to rate the images produced by the three proposed multi-turn agents and a single-turn T2I model against a Ground Truth image for which the original prompt was derived and the answers to the agents questions were derived. Approximately 550 image-dialog pairs per agent are rated using 3 human raters. The generated images were presented in a random order and were unlabeled and the human rater was tasked with ranking the images from best to worst. The results from the study are shown in Figure [4](#).

![Belief Parsing and interaction 1: Input: Initial Prompt (IP) 2: Initialization: Merged Prompt (MP) â† IP 3: for ğ‘¡ğ‘¢ğ‘Ÿğ‘› â† 1 to ğ‘šğ‘ğ‘¥_ğ‘¡ğ‘¢ğ‘Ÿğ‘› do]()

![Figure 2 | a) Each column displays the output of an agent after 15 turns -the right most column shows target image, which belongs to DesignBench. b) A visualization of the multi-turn set up in the experiments. These are real generated outputs and simulated user outputs at turns 3, 10 and 15.]()

![Figure 3 | ImageInWords results, including (a) T2T, (b) I2I, (c) T2I, (d) NLL scores. Agents trend to increase performance up to 10 turns.]()

![]()

![Figure 5 | Human ratings for the dialogues. The left graph shows the rating of how well the final generated image corresponds to the original prompt and dialogue. The right graph shows the distribution of issues of questions per agent.]()

![Figure 6 | Real multi-turn dialogs generated by the Ag1, Ag2, and Ag3 agents on an image from DesignBench. The figure additionally shows the image generated after the 5 turn dialog per agent.]()

![Figure 7 | Agent Generated Image Outputs on DesignBench: a chart of the generated image outputs of the four main Agent types in comparison to the goal image. Each column displays the output of a different agent and the right most column shows the goal image that the agents aimed to recreate.Each agent was provided with the same starting prompt and iterated for 15 turns, with the exception of the "T2I" agent column which produces an image from the starting prompt. Ag1, Ag2 and Ag3 refer to the Agents described in Â§E. Each agent uses the same T2I model to produce the final image. The goal images displayed here are from our DesignBench dataset described in the experiments section.]()

![Figure 11 | An example of the belief graph data structure for a given prompt in Figure 1.]()

![1 ... [[ Instruction for the first question]] ... 2 3 The original prompt was: {self.original_prompt} -Based on the original prompt please provide a question to ask about the image. The question should be as concise and direct as possible . The question should aim to learn more about the attributes and contents of the image, the objects , the spatial layout, and the style . Make sure that you question the answer within <question> and </question> markers 4 5 ... [[ Instruction for the following question]] ... 6 7 Based on the chat history please provide a new question to ask about the image. the chat history is as follows and is enclosed in <chat_history> and </chat_history> markers:{self.chat_history} </chat_history> The question should be as concise and direct as possible. The question should aim to learn more about the attributes and contents of the image, the objects , the spatial layout, and the style . Make sure that you question the answer within <question> and </question> markers.' E.14. ğ´ğ¼ğ¶ğ‘„ ğµ Prompt Instruction 1 You are an intelligent agent that helps users generate images. Before generating the image requested by the user, you should ask the most important clarification questions to make sure you understand the key features of the image. 2 The user describes the image as: {user_prompt}. 3 The following is your belief of what the image contains, including the entities , attributes of each entity and relations between entities .]()

![Figure 13 | Default state of a possible interface.]()

![Figure 14 | Interface once prompt has been input with clarifications.]()

![Figure 18 | Stimulus image in the survey to test the Model Graph of Entities and Attributes feature.]()

![name: donut-coffee Description: "donut next to a mug of coffee" Spatial relationship: "next to" arrow_dr]()

![Figure 20 | An example of the template presented to human raters. Human raters are asked to mark any issues a question contains that could pose a disturbance to the user. Approximately 8k questions per Agent are rated. The results are shown in Figure 5.]()

![Figure 21 | An example of the template presented to human raters. Human raters are asked to rank the correspondence of each image to the agent-user dialog and original prompt. Approximately 1.5k image-dialog pairs are rated using 3 human raters. Results in Figure 5.]()

![Automatic]()

Code and DesignBench can be found at https://github.com/google-deepmind/proactive_t2i_agents.

Samples from the agent belief can be used to construct expanded prompts.

The clarification question part of the interaction is omitted for simplicity

This assumption only applies to the experiments. In practice, users don't necessarily have an image in mind, but they can get inspirations from the belief graphs and questions.

While 15 turns is a suggested approximation of interaction time, accounting for varying difficulty between images, any number of turns can be used with this evaluation approach.

https://cloud.google.com/vertex-ai

Name is a unique identifier for the entity; Importance to ask score: A numerical value indicating the entity's perceived importance in satisfying the user's request. Entities with higher scores are prioritized during question generation, as they are likely to reduce uncertainty and contribute significantly to the final image; Description provides a textual description of the entity; probability of appearing estimates likelihood of the entity being present in the generated image; Attributes is for understanding the detailed attributes of the entities.

Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty

