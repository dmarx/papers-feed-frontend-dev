- Decision to implement proactive clarification questions in T2I agents
- Choice of belief graph as a representation of agent's understanding
- Selection of LLM for generating questions and updating prompts
- Design of the user interface for interaction with belief graphs
- Strategy for determining which attributes to clarify first
- Method for evaluating the effectiveness of T2I agents
- Decision to create the DesignBench benchmark for evaluation
- Choice of datasets (COCO, ImageInWords) for testing
- Approach to handle user intent ambiguity in prompts
- Decision to modularize the agent's architecture for component updates
- Strategy for integrating user feedback into the belief graph
- Choice of metrics (e.g., VQAScore) for assessing image-prompt alignment
- Decision to conduct both human studies and automated evaluations
- Approach to handle multi-turn interactions in T2I generation
- Decision to focus on interpretability and explainability of agent actions
- Choice of active learning techniques to reduce uncertainty in user intent
- Decision to prioritize user experience in agent design
- Strategy for managing the trade-off between question asking and image generation speed
- Decision to allow users to edit the belief graph directly
- Approach to ensure consistency in generated images across interactions