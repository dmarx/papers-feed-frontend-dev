{
  "1310.6753": {
    "id": "1310.6753",
    "title": "Romantic Partnerships and the Dispersion of Social Ties: A Network   Analysis of Relationship Status on Facebook",
    "authors": "Lars Backstrom, Jon Kleinberg",
    "abstract": "A crucial task in the analysis of on-line social-networking systems is to identify important people --- those linked by strong social ties --- within an individual's network neighborhood. Here we investigate this question for a particular category of strong ties, those involving spouses or romantic partners. We organize our analysis around a basic question: given all the connections among a person's friends, can you recognize his or her romantic partner from the network structure alone? Using data from a large sample of Facebook users, we find that this task can be accomplished with high accuracy, but doing so requires the development of a new measure of tie strength that we term `dispersion' --- the extent to which two people's mutual friends are not themselves well-connected. The results offer methods for identifying types of structurally significant people in on-line applications, and suggest a potential expansion of existing theories of tie strength.",
    "url": "https://arxiv.org/abs/1310.6753",
    "arxivId": "1310.6753",
    "last_visited": "2025-01-03T18:28:19.059Z",
    "last_read": "2025-01-04T06:52:00.641659",
    "total_reading_time_seconds": 79,
    "published_date": "2013-10-24T20:00:18Z",
    "arxiv_tags": [
      "cs.SI",
      "physics.soc-ph",
      "H.2.8"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1310.6753/features/markdown-grobid/1310.6753.md",
      "adr-crib": "data/papers/1310.6753/features/adr-crib/1310.6753.md",
      "adr-titles": "data/papers/1310.6753/features/adr-titles/1310.6753.md",
      "crib-sheet": "data/papers/1310.6753/features/crib-sheet/1310.6753.md",
      "compound-crib": "data/papers/1310.6753/features/compound-crib/1310.6753.md"
    }
  },
  "1411.1792": {
    "id": "1411.1792",
    "title": "How transferable are features in deep neural networks?",
    "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson",
    "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",
    "url": "https://arxiv.org/abs/1411.1792",
    "arxivId": "1411.1792",
    "last_visited": "2024-12-24T02:43:09.950Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2014-11-06T23:09:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1411.1792/features/markdown-grobid/1411.1792.md",
      "adr-crib": "data/papers/1411.1792/features/adr-crib/1411.1792.md",
      "adr-titles": "data/papers/1411.1792/features/adr-titles/1411.1792.md",
      "crib-sheet": "data/papers/1411.1792/features/crib-sheet/1411.1792.md",
      "compound-crib": "data/papers/1411.1792/features/compound-crib/1411.1792.md"
    }
  },
  "1503.02531": {
    "id": "1503.02531",
    "title": "Distilling the Knowledge in a Neural Network",
    "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
    "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
    "url": "https://arxiv.org/abs/1503.02531",
    "arxivId": "1503.02531",
    "last_visited": "2025-01-22T23:52:59.719Z",
    "last_read": "2025-01-22T23:52:59.719Z",
    "total_reading_time_seconds": 26,
    "published_date": "2015-03-09T15:44:49Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1503.02531/features/markdown-grobid/1503.02531.md",
      "adr-crib": "data/papers/1503.02531/features/adr-crib/1503.02531.md",
      "adr-titles": "data/papers/1503.02531/features/adr-titles/1503.02531.md",
      "crib-sheet": "data/papers/1503.02531/features/crib-sheet/1503.02531.md",
      "compound-crib": "data/papers/1503.02531/features/compound-crib/1503.02531.md"
    }
  },
  "1503.03585": {
    "id": "1503.03585",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
    "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
    "url": "https://arxiv.org/abs/1503.03585",
    "arxivId": "1503.03585",
    "last_visited": "2025-01-19T02:31:15.039Z",
    "last_read": "2025-01-19T02:31:15.039Z",
    "total_reading_time_seconds": 114,
    "published_date": "2015-03-12T04:51:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "q-bio.NC",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1503.03585/features/markdown-grobid/1503.03585.md",
      "adr-crib": "data/papers/1503.03585/features/adr-crib/1503.03585.md",
      "adr-titles": "data/papers/1503.03585/features/adr-titles/1503.03585.md",
      "crib-sheet": "data/papers/1503.03585/features/crib-sheet/1503.03585.md",
      "compound-crib": "data/papers/1503.03585/features/compound-crib/1503.03585.md"
    }
  },
  "1506.06579": {
    "id": "1506.06579",
    "title": "Understanding Neural Networks Through Deep Visualization",
    "authors": "Jason Yosinski, Jeff Clune, Anh Nguyen and 2 others",
    "abstract": "Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.",
    "url": "https://arxiv.org/abs/1506.06579",
    "arxivId": "1506.06579",
    "last_visited": "2024-12-24T02:47:02.227Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2015-06-22T12:57:15Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1506.06579/features/markdown-grobid/1506.06579.md",
      "adr-crib": "data/papers/1506.06579/features/adr-crib/1506.06579.md",
      "adr-titles": "data/papers/1506.06579/features/adr-titles/1506.06579.md",
      "crib-sheet": "data/papers/1506.06579/features/crib-sheet/1506.06579.md",
      "compound-crib": "data/papers/1506.06579/features/compound-crib/1506.06579.md"
    }
  },
  "1602.03483": {
    "id": "1602.03483",
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen",
    "abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",
    "url": "https://arxiv.org/abs/1602.03483",
    "arxivId": "1602.03483",
    "last_visited": "2025-01-03T20:13:43.540Z",
    "last_read": "2025-01-04T06:51:57.840814",
    "total_reading_time_seconds": 24,
    "published_date": "2016-02-10T18:49:58Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1602.03483/features/markdown-grobid/1602.03483.md",
      "adr-crib": "data/papers/1602.03483/features/adr-crib/1602.03483.md",
      "adr-titles": "data/papers/1602.03483/features/adr-titles/1602.03483.md",
      "crib-sheet": "data/papers/1602.03483/features/crib-sheet/1602.03483.md",
      "compound-crib": "data/papers/1602.03483/features/compound-crib/1602.03483.md"
    }
  },
  "1705.07831": {
    "id": "1705.07831",
    "title": "Stabilizing GAN Training with Multiple Random Projections",
    "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti",
    "abstract": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
    "url": "https://arxiv.org/abs/1705.07831",
    "arxivId": "1705.07831",
    "last_visited": "2025-01-11T08:02:03.936Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-05-22T16:23:26Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1705.07831/features/markdown-grobid/1705.07831.md",
      "adr-crib": "data/papers/1705.07831/features/adr-crib/1705.07831.md",
      "adr-titles": "data/papers/1705.07831/features/adr-titles/1705.07831.md",
      "crib-sheet": "data/papers/1705.07831/features/crib-sheet/1705.07831.md",
      "compound-crib": "data/papers/1705.07831/features/compound-crib/1705.07831.md"
    }
  },
  "1705.08039": {
    "id": "1705.08039",
    "title": "Poincaré Embeddings for Learning Hierarchical Representations",
    "authors": "Maximilian Nickel, Douwe Kiela",
    "abstract": "Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\\'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.",
    "url": "https://arxiv.org/abs/1705.08039",
    "arxivId": "1705.08039",
    "last_visited": "2024-12-29T01:44:01.360000+00:00",
    "last_read": "2025-01-04T14:49:42.247780",
    "total_reading_time_seconds": 20,
    "published_date": "2017-05-22T23:14:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1705.08039/features/markdown-grobid/1705.08039.md",
      "adr-crib": "data/papers/1705.08039/features/adr-crib/1705.08039.md",
      "adr-titles": "data/papers/1705.08039/features/adr-titles/1705.08039.md",
      "crib-sheet": "data/papers/1705.08039/features/crib-sheet/1705.08039.md",
      "compound-crib": "data/papers/1705.08039/features/compound-crib/1705.08039.md"
    }
  },
  "1705.10359": {
    "id": "1705.10359",
    "title": "Neural Embeddings of Graphs in Hyperbolic Space",
    "authors": "Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth",
    "abstract": "Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",
    "url": "https://arxiv.org/abs/1705.10359",
    "arxivId": "1705.10359",
    "last_visited": "2024-12-29T01:43:53.617000+00:00",
    "last_read": "2025-01-04T14:49:42.248652",
    "total_reading_time_seconds": 11,
    "published_date": "2017-05-29T18:47:30Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1705.10359/features/markdown-grobid/1705.10359.md",
      "adr-crib": "data/papers/1705.10359/features/adr-crib/1705.10359.md",
      "adr-titles": "data/papers/1705.10359/features/adr-titles/1705.10359.md",
      "crib-sheet": "data/papers/1705.10359/features/crib-sheet/1705.10359.md",
      "compound-crib": "data/papers/1705.10359/features/compound-crib/1705.10359.md"
    }
  },
  "1706.05806": {
    "id": "1706.05806",
    "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning   Dynamics and Interpretability",
    "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein",
    "abstract": "We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/",
    "url": "https://arxiv.org/abs/1706.05806",
    "arxivId": "1706.05806",
    "last_visited": "2024-12-24T02:53:44.603Z",
    "last_read": "2025-01-05T08:23:29.503537",
    "total_reading_time_seconds": 20,
    "published_date": "2017-06-19T07:09:20Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1706.05806/features/markdown-grobid/1706.05806.md",
      "adr-crib": "data/papers/1706.05806/features/adr-crib/1706.05806.md",
      "adr-titles": "data/papers/1706.05806/features/adr-titles/1706.05806.md",
      "crib-sheet": "data/papers/1706.05806/features/crib-sheet/1706.05806.md",
      "compound-crib": "data/papers/1706.05806/features/compound-crib/1706.05806.md"
    }
  },
  "1710.09412": {
    "id": "1710.09412",
    "title": "mixup: Beyond Empirical Risk Minimization",
    "authors": "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz",
    "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",
    "url": "https://arxiv.org/abs/1710.09412",
    "arxivId": "1710.09412",
    "last_visited": "2024-12-22T17:42:57.473Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2017-10-25T18:30:49Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1710.09412/features/markdown-grobid/1710.09412.md",
      "adr-crib": "data/papers/1710.09412/features/adr-crib/1710.09412.md",
      "adr-titles": "data/papers/1710.09412/features/adr-titles/1710.09412.md",
      "crib-sheet": "data/papers/1710.09412/features/crib-sheet/1710.09412.md",
      "compound-crib": "data/papers/1710.09412/features/compound-crib/1710.09412.md"
    }
  },
  "1711.11586": {
    "id": "1711.11586",
    "title": "Toward Multimodal Image-to-Image Translation",
    "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak and 4 others",
    "abstract": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
    "url": "https://arxiv.org/abs/1711.11586",
    "arxivId": "1711.11586",
    "last_visited": "2025-01-12T06:42:01.228Z",
    "last_read": "2025-01-12T06:43:53.633334",
    "total_reading_time_seconds": 6,
    "published_date": "2017-11-30T18:59:01Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1711.11586/features/markdown-grobid/1711.11586.md",
      "adr-crib": "data/papers/1711.11586/features/adr-crib/1711.11586.md",
      "adr-titles": "data/papers/1711.11586/features/adr-titles/1711.11586.md",
      "crib-sheet": "data/papers/1711.11586/features/crib-sheet/1711.11586.md",
      "compound-crib": "data/papers/1711.11586/features/compound-crib/1711.11586.md"
    }
  },
  "1802.03426": {
    "id": "1802.03426",
    "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension   Reduction",
    "authors": "Leland McInnes, John Healy, James Melville",
    "abstract": "UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",
    "url": "https://arxiv.org/abs/1802.03426",
    "arxivId": "1802.03426",
    "last_visited": "2024-12-30T03:35:58.743Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-02-09T19:39:33Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.CG",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1802.03426/features/markdown-grobid/1802.03426.md",
      "adr-crib": "data/papers/1802.03426/features/adr-crib/1802.03426.md",
      "adr-titles": "data/papers/1802.03426/features/adr-titles/1802.03426.md",
      "crib-sheet": "data/papers/1802.03426/features/crib-sheet/1802.03426.md",
      "compound-crib": "data/papers/1802.03426/features/compound-crib/1802.03426.md"
    }
  },
  "1804.03329": {
    "id": "1804.03329",
    "title": "Representation Tradeoffs for Hyperbolic Embeddings",
    "authors": "Christopher De Sa, Albert Gu, Christopher Ré, Frederic Sala",
    "abstract": "Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of 0.989 with only two dimensions, while Nickel et al.'s recent construction obtains 0.87 using 200 dimensions. We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.",
    "url": "https://arxiv.org/abs/1804.03329",
    "arxivId": "1804.03329",
    "last_visited": "2024-12-29T02:34:35.575Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2018-04-10T03:39:16Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1804.03329/features/markdown-grobid/1804.03329.md",
      "adr-crib": "data/papers/1804.03329/features/adr-crib/1804.03329.md",
      "adr-titles": "data/papers/1804.03329/features/adr-titles/1804.03329.md",
      "crib-sheet": "data/papers/1804.03329/features/crib-sheet/1804.03329.md",
      "compound-crib": "data/papers/1804.03329/features/compound-crib/1804.03329.md"
    }
  },
  "1807.00734": {
    "id": "1807.00734",
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "authors": "Alexia Jolicoeur-Martineau",
    "abstract": "In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs.   We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function.   Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.",
    "url": "https://arxiv.org/abs/1807.00734",
    "arxivId": "1807.00734",
    "last_visited": "2025-01-12T17:59:03.697Z",
    "last_read": "2025-01-11T08:07:25.239318",
    "total_reading_time_seconds": 16,
    "published_date": "2018-07-02T15:11:23Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1807.00734/features/markdown-grobid/1807.00734.md",
      "adr-crib": "data/papers/1807.00734/features/adr-crib/1807.00734.md",
      "adr-titles": "data/papers/1807.00734/features/adr-titles/1807.00734.md",
      "crib-sheet": "data/papers/1807.00734/features/crib-sheet/1807.00734.md",
      "compound-crib": "data/papers/1807.00734/features/compound-crib/1807.00734.md"
    }
  },
  "1810.00363": {
    "id": "1810.00363",
    "title": "A Kernel Perspective for Regularizing Deep Neural Networks",
    "authors": "Alberto Bietti, Grégoire Mialon, Dexiong Chen, Julien Mairal",
    "abstract": "We propose a new point of view for regularizing deep neural networks by using the norm of a reproducing kernel Hilbert space (RKHS). Even though this norm cannot be computed, it admits upper and lower approximations leading to various practical strategies. Specifically, this perspective (i) provides a common umbrella for many existing regularization principles, including spectral norm and gradient penalties, or adversarial training, (ii) leads to new effective regularization penalties, and (iii) suggests hybrid strategies combining lower and upper bounds to get better approximations of the RKHS norm. We experimentally show this approach to be effective when learning on small datasets, or to obtain adversarially robust models.",
    "url": "https://arxiv.org/abs/1810.00363",
    "arxivId": "1810.00363",
    "last_visited": "2024-12-29T10:35:11.499Z",
    "last_read": "2025-01-04T14:49:18.223709",
    "total_reading_time_seconds": 14,
    "published_date": "2018-09-30T11:40:59Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1810.00363/features/markdown-grobid/1810.00363.md",
      "adr-crib": "data/papers/1810.00363/features/adr-crib/1810.00363.md",
      "adr-titles": "data/papers/1810.00363/features/adr-titles/1810.00363.md",
      "crib-sheet": "data/papers/1810.00363/features/crib-sheet/1810.00363.md",
      "compound-crib": "data/papers/1810.00363/features/compound-crib/1810.00363.md"
    }
  },
  "1810.01588": {
    "id": "1810.01588",
    "title": "Interpreting Layered Neural Networks via Hierarchical Modular   Representation",
    "authors": "Chihiro Watanabe",
    "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. In this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values.",
    "url": "https://arxiv.org/abs/1810.01588",
    "arxivId": "1810.01588",
    "last_visited": "2024-12-24T03:07:38.277Z",
    "last_read": "2025-01-04T15:03:30.857351",
    "total_reading_time_seconds": 6,
    "published_date": "2018-10-03T05:38:26Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1810.01588/features/markdown-grobid/1810.01588.md",
      "adr-crib": "data/papers/1810.01588/features/adr-crib/1810.01588.md",
      "adr-titles": "data/papers/1810.01588/features/adr-titles/1810.01588.md",
      "crib-sheet": "data/papers/1810.01588/features/crib-sheet/1810.01588.md",
      "compound-crib": "data/papers/1810.01588/features/compound-crib/1810.01588.md"
    }
  },
  "1904.08779": {
    "id": "1904.08779",
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech   Recognition",
    "authors": "Daniel S. Park, William Chan, Yu Zhang and 4 others",
    "abstract": "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
    "url": "https://arxiv.org/abs/1904.08779",
    "arxivId": "1904.08779",
    "last_visited": "2024-12-17T14:41:05.563Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-04-18T17:53:38Z",
    "arxiv_tags": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1904.08779/features/markdown-grobid/1904.08779.md",
      "adr-crib": "data/papers/1904.08779/features/adr-crib/1904.08779.md",
      "adr-titles": "data/papers/1904.08779/features/adr-titles/1904.08779.md",
      "crib-sheet": "data/papers/1904.08779/features/crib-sheet/1904.08779.md",
      "compound-crib": "data/papers/1904.08779/features/compound-crib/1904.08779.md"
    }
  },
  "1906.01563": {
    "id": "1906.01563",
    "title": "Hamiltonian Neural Networks",
    "authors": "Sam Greydanus, Misko Dzamba, Jason Yosinski",
    "abstract": "Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.",
    "url": "https://arxiv.org/abs/1906.01563",
    "arxivId": "1906.01563",
    "last_visited": "2024-12-24T02:49:04.570Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-06-04T16:27:55Z",
    "arxiv_tags": [
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1906.01563/features/markdown-grobid/1906.01563.md",
      "adr-crib": "data/papers/1906.01563/features/adr-crib/1906.01563.md",
      "adr-titles": "data/papers/1906.01563/features/adr-titles/1906.01563.md",
      "crib-sheet": "data/papers/1906.01563/features/crib-sheet/1906.01563.md",
      "compound-crib": "data/papers/1906.01563/features/compound-crib/1906.01563.md"
    }
  },
  "1906.04358": {
    "id": "1906.04358",
    "title": "Weight Agnostic Neural Networks",
    "authors": "Adam Gaier, David Ha",
    "abstract": "Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",
    "url": "https://arxiv.org/abs/1906.04358",
    "arxivId": "1906.04358",
    "last_visited": "2024-12-24T03:27:56.957Z",
    "last_read": "2025-01-04T15:03:24.866258",
    "total_reading_time_seconds": 20,
    "published_date": "2019-06-11T02:40:11Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1906.04358/features/markdown-grobid/1906.04358.md",
      "adr-crib": "data/papers/1906.04358/features/adr-crib/1906.04358.md",
      "adr-titles": "data/papers/1906.04358/features/adr-titles/1906.04358.md",
      "crib-sheet": "data/papers/1906.04358/features/crib-sheet/1906.04358.md",
      "compound-crib": "data/papers/1906.04358/features/compound-crib/1906.04358.md"
    }
  },
  "1906.05433": {
    "id": "1906.05433",
    "title": "Tackling Climate Change with Machine Learning",
    "authors": "David Rolnick, Priya L. Donti, Lynn H. Kaack and 19 others",
    "abstract": "Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.",
    "url": "https://arxiv.org/abs/1906.05433",
    "arxivId": "1906.05433",
    "last_visited": "2024-12-22T06:23:13.395Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2019-06-10T17:51:47Z",
    "arxiv_tags": [
      "cs.CY",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1906.05433/features/markdown-grobid/1906.05433.md",
      "adr-crib": "data/papers/1906.05433/features/adr-crib/1906.05433.md",
      "adr-titles": "data/papers/1906.05433/features/adr-titles/1906.05433.md",
      "crib-sheet": "data/papers/1906.05433/features/crib-sheet/1906.05433.md",
      "compound-crib": "data/papers/1906.05433/features/compound-crib/1906.05433.md"
    }
  },
  "1912.02757": {
    "id": "1912.02757",
    "title": "Deep Ensembles: A Loss Landscape Perspective",
    "authors": "Stanislav Fort, Huiyi Hu, Balaji Lakshminarayanan",
    "abstract": "Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.",
    "url": "https://arxiv.org/abs/1912.02757",
    "arxivId": "1912.02757",
    "last_visited": "2024-12-24T03:01:38.243Z",
    "last_read": "2025-01-05T08:23:23.618098",
    "total_reading_time_seconds": 6,
    "published_date": "2019-12-05T17:48:18Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1912.02757/features/markdown-grobid/1912.02757.md",
      "adr-crib": "data/papers/1912.02757/features/adr-crib/1912.02757.md",
      "adr-titles": "data/papers/1912.02757/features/adr-titles/1912.02757.md",
      "crib-sheet": "data/papers/1912.02757/features/crib-sheet/1912.02757.md",
      "compound-crib": "data/papers/1912.02757/features/compound-crib/1912.02757.md"
    }
  },
  "2001.04063": {
    "id": "2001.04063",
    "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence   Pre-training",
    "authors": "Weizhen Qi, Yu Yan, Yeyun Gong and 5 others",
    "abstract": "This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.",
    "url": "https://arxiv.org/abs/2001.04063",
    "arxivId": "2001.04063",
    "last_visited": "2024-12-26T17:17:59.219Z",
    "last_read": "2025-01-04T15:03:09.858274",
    "total_reading_time_seconds": 3,
    "published_date": "2020-01-13T05:12:38Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2001.04063/features/markdown-grobid/2001.04063.md",
      "adr-crib": "data/papers/2001.04063/features/adr-crib/2001.04063.md",
      "adr-titles": "data/papers/2001.04063/features/adr-titles/2001.04063.md",
      "crib-sheet": "data/papers/2001.04063/features/crib-sheet/2001.04063.md",
      "compound-crib": "data/papers/2001.04063/features/compound-crib/2001.04063.md"
    }
  },
  "2001.04451": {
    "id": "2001.04451",
    "title": "Reformer: The Efficient Transformer",
    "authors": "Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya",
    "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.",
    "url": "https://arxiv.org/abs/2001.04451",
    "arxivId": "2001.04451",
    "last_visited": "2025-01-05T20:08:29.858000+00:00",
    "last_read": "2025-01-05T20:09:30.079520",
    "total_reading_time_seconds": 3,
    "published_date": "2020-01-13T18:38:28Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2001.04451/features/markdown-grobid/2001.04451.md",
      "adr-crib": "data/papers/2001.04451/features/adr-crib/2001.04451.md",
      "adr-titles": "data/papers/2001.04451/features/adr-titles/2001.04451.md",
      "crib-sheet": "data/papers/2001.04451/features/crib-sheet/2001.04451.md",
      "compound-crib": "data/papers/2001.04451/features/compound-crib/2001.04451.md"
    }
  },
  "2006.11120": {
    "id": "2006.11120",
    "title": "From Discrete to Continuous Convolution Layers",
    "authors": "Assaf Shocher, Ben Feinstein, Niv Haim, Michal Irani",
    "abstract": "A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.",
    "url": "https://arxiv.org/abs/2006.11120",
    "arxivId": "2006.11120",
    "last_visited": "2024-12-25T05:29:13.001Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-19T13:16:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.11120/features/markdown-grobid/2006.11120.md",
      "adr-crib": "data/papers/2006.11120/features/adr-crib/2006.11120.md",
      "adr-titles": "data/papers/2006.11120/features/adr-titles/2006.11120.md",
      "crib-sheet": "data/papers/2006.11120/features/crib-sheet/2006.11120.md",
      "compound-crib": "data/papers/2006.11120/features/compound-crib/2006.11120.md"
    }
  },
  "2006.14769": {
    "id": "2006.14769",
    "title": "Supermasks in Superposition",
    "authors": "Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu and 4 others",
    "abstract": "We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.",
    "url": "https://arxiv.org/abs/2006.14769",
    "arxivId": "2006.14769",
    "last_visited": "2024-12-24T02:37:38.778Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-26T03:16:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.14769/features/markdown-grobid/2006.14769.md",
      "adr-crib": "data/papers/2006.14769/features/adr-crib/2006.14769.md",
      "adr-titles": "data/papers/2006.14769/features/adr-titles/2006.14769.md",
      "crib-sheet": "data/papers/2006.14769/features/crib-sheet/2006.14769.md",
      "compound-crib": "data/papers/2006.14769/features/compound-crib/2006.14769.md"
    }
  },
  "2009.10195": {
    "id": "2009.10195",
    "title": "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving   Out-of-Domain Robustness",
    "authors": "Nathan Ng, Kyunghyun Cho, Marzyeh Ghassemi",
    "abstract": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
    "url": "https://arxiv.org/abs/2009.10195",
    "arxivId": "2009.10195",
    "last_visited": "2024-12-21T18:19:38.104Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2020-09-21T22:02:33Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2009.10195/features/markdown-grobid/2009.10195.md",
      "adr-crib": "data/papers/2009.10195/features/adr-crib/2009.10195.md",
      "adr-titles": "data/papers/2009.10195/features/adr-titles/2009.10195.md",
      "crib-sheet": "data/papers/2009.10195/features/crib-sheet/2009.10195.md",
      "compound-crib": "data/papers/2009.10195/features/compound-crib/2009.10195.md"
    }
  },
  "2010.11929": {
    "id": "2010.11929",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale",
    "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov and 9 others",
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
    "url": "https://arxiv.org/abs/2010.11929",
    "arxivId": "2010.11929",
    "last_visited": "2025-01-04T15:01:19.005Z",
    "last_read": "2025-01-04T15:01:58.106137",
    "total_reading_time_seconds": 16,
    "published_date": "2020-10-22T17:55:59Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2010.11929/features/markdown-grobid/2010.11929.md",
      "adr-crib": "data/papers/2010.11929/features/adr-crib/2010.11929.md",
      "adr-titles": "data/papers/2010.11929/features/adr-titles/2010.11929.md",
      "crib-sheet": "data/papers/2010.11929/features/crib-sheet/2010.11929.md",
      "compound-crib": "data/papers/2010.11929/features/compound-crib/2010.11929.md"
    }
  },
  "2012.13255": {
    "id": "2012.13255",
    "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model   Fine-Tuning",
    "authors": "Armen Aghajanyan, Luke Zettlemoyer, Sonal Gupta",
    "abstract": "Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.",
    "url": "https://arxiv.org/abs/2012.13255",
    "arxivId": "2012.13255",
    "last_visited": "2024-12-24T02:33:31.614000+00:00",
    "last_read": "2025-01-05T08:23:41.555214",
    "total_reading_time_seconds": 7,
    "published_date": "2020-12-22T07:42:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2012.13255/features/markdown-grobid/2012.13255.md",
      "adr-crib": "data/papers/2012.13255/features/adr-crib/2012.13255.md",
      "adr-titles": "data/papers/2012.13255/features/adr-titles/2012.13255.md",
      "crib-sheet": "data/papers/2012.13255/features/crib-sheet/2012.13255.md",
      "compound-crib": "data/papers/2012.13255/features/compound-crib/2012.13255.md"
    }
  },
  "2103.13413": {
    "id": "2103.13413",
    "title": "Vision Transformers for Dense Prediction",
    "authors": "René Ranftl, Alexey Bochkovskiy, Vladlen Koltun",
    "abstract": "We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
    "url": "https://arxiv.org/abs/2103.13413",
    "arxivId": "2103.13413",
    "last_visited": "2025-01-04T14:48:49.256000+00:00",
    "last_read": "2025-01-04T15:01:58.107814",
    "total_reading_time_seconds": 5,
    "published_date": "2021-03-24T18:01:17Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2103.13413/features/markdown-grobid/2103.13413.md",
      "adr-crib": "data/papers/2103.13413/features/adr-crib/2103.13413.md",
      "adr-titles": "data/papers/2103.13413/features/adr-titles/2103.13413.md",
      "crib-sheet": "data/papers/2103.13413/features/crib-sheet/2103.13413.md",
      "compound-crib": "data/papers/2103.13413/features/compound-crib/2103.13413.md"
    }
  },
  "2105.01601": {
    "id": "2105.01601",
    "title": "MLP-Mixer: An all-MLP Architecture for Vision",
    "authors": "Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov and 9 others",
    "abstract": "Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. \"mixing\" the per-location features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.",
    "url": "https://arxiv.org/abs/2105.01601",
    "arxivId": "2105.01601",
    "last_visited": "2025-01-05T20:12:13.323Z",
    "last_read": "2025-01-05T20:13:29.393146",
    "total_reading_time_seconds": 22,
    "published_date": "2021-05-04T16:17:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2105.01601/features/markdown-grobid/2105.01601.md",
      "adr-crib": "data/papers/2105.01601/features/adr-crib/2105.01601.md",
      "adr-titles": "data/papers/2105.01601/features/adr-titles/2105.01601.md",
      "crib-sheet": "data/papers/2105.01601/features/crib-sheet/2105.01601.md",
      "compound-crib": "data/papers/2105.01601/features/compound-crib/2105.01601.md"
    }
  },
  "2105.05720": {
    "id": "2105.05720",
    "title": "Breaking the Computation and Communication Abstraction Barrier in   Distributed Machine Learning Workloads",
    "authors": "Abhinav Jangda, Jun Huang, Guodong Liu and 6 others",
    "abstract": "Recent trend towards increasing large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, current logical separation between computation and communication kernels in deep learning frameworks misses the optimization opportunities across such barrier. Breaking this abstraction with a holistic consideration can provide many optimizations to provide performance improvements in distributed workloads. Manually applying these optimizations needs modifications in underlying computation and communication libraries for each scenario, which is time consuming and error-prone.   Therefore, we present CoCoNeT, with a DSL to express a program with both computation and communication. CoCoNeT contains several machine learning aware transformations to optimize a program and a compiler to generate high performance kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel workloads in large language models with only a few lines of code. Experiments show CoCoNeT significantly outperforms state-of-the-art distributed machine learning implementations.",
    "url": "https://arxiv.org/abs/2105.05720",
    "arxivId": "2105.05720",
    "last_visited": "2024-12-16T05:53:22.033Z",
    "last_read": "2025-01-05T18:41:15.658266",
    "total_reading_time_seconds": 158,
    "published_date": "2021-05-12T15:13:43Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.LG",
      "cs.PL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2105.05720/features/markdown-grobid/2105.05720.md",
      "adr-crib": "data/papers/2105.05720/features/adr-crib/2105.05720.md",
      "adr-titles": "data/papers/2105.05720/features/adr-titles/2105.05720.md",
      "crib-sheet": "data/papers/2105.05720/features/crib-sheet/2105.05720.md",
      "compound-crib": "data/papers/2105.05720/features/compound-crib/2105.05720.md"
    }
  },
  "2105.08050": {
    "id": "2105.08050",
    "title": "Pay Attention to MLPs",
    "authors": "Hanxiao Liu, Zihang Dai, David R. So, Quoc V. Le",
    "abstract": "Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.",
    "url": "https://arxiv.org/abs/2105.08050",
    "arxivId": "2105.08050",
    "last_visited": "2025-01-05T23:07:26.844000+00:00",
    "last_read": "2025-01-05T23:08:39.880665",
    "total_reading_time_seconds": 45,
    "published_date": "2021-05-17T17:55:04Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2105.08050/features/markdown-grobid/2105.08050.md",
      "adr-crib": "data/papers/2105.08050/features/adr-crib/2105.08050.md",
      "adr-titles": "data/papers/2105.08050/features/adr-titles/2105.08050.md",
      "crib-sheet": "data/papers/2105.08050/features/crib-sheet/2105.08050.md",
      "compound-crib": "data/papers/2105.08050/features/compound-crib/2105.08050.md"
    }
  },
  "2106.10165": {
    "id": "2106.10165",
    "title": "The Principles of Deep Learning Theory",
    "authors": "Daniel A. Roberts, Sho Yaida, Boris Hanin",
    "abstract": "This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.",
    "url": "https://arxiv.org/abs/2106.10165",
    "arxivId": "2106.10165",
    "last_visited": "2024-12-29T22:46:13.679Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2021-06-18T15:00:00Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "hep-th",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2106.10165/features/markdown-grobid/2106.10165.md",
      "adr-crib": "data/papers/2106.10165/features/adr-crib/2106.10165.md",
      "adr-titles": "data/papers/2106.10165/features/adr-titles/2106.10165.md",
      "crib-sheet": "data/papers/2106.10165/features/crib-sheet/2106.10165.md",
      "compound-crib": "data/papers/2106.10165/features/compound-crib/2106.10165.md"
    }
  },
  "2107.11817": {
    "id": "2107.11817",
    "title": "Go Wider Instead of Deeper",
    "authors": "Fuzhao Xue, Ziji Shi, Futao Wei and 3 others",
    "abstract": "More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference.   In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable parameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can still surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on average and surpass BERT using factorized embedding parameterization by $0.8\\%$ with fewer parameters.",
    "url": "https://arxiv.org/abs/2107.11817",
    "arxivId": "2107.11817",
    "last_visited": "2025-01-05T18:44:31.871Z",
    "last_read": "2025-01-05T18:40:55.338983",
    "total_reading_time_seconds": 13,
    "published_date": "2021-07-25T14:44:24Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2107.11817/features/markdown-grobid/2107.11817.md",
      "adr-crib": "data/papers/2107.11817/features/adr-crib/2107.11817.md",
      "adr-titles": "data/papers/2107.11817/features/adr-titles/2107.11817.md",
      "crib-sheet": "data/papers/2107.11817/features/crib-sheet/2107.11817.md",
      "compound-crib": "data/papers/2107.11817/features/compound-crib/2107.11817.md"
    }
  },
  "2110.09456": {
    "id": "2110.09456",
    "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
    "authors": "Sam Shleifer, Jason Weston, Myle Ott",
    "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .",
    "url": "https://arxiv.org/abs/2110.09456",
    "arxivId": "2110.09456",
    "last_visited": "2025-01-07T23:17:21.742Z",
    "last_read": "2025-01-07T23:18:44.187174",
    "total_reading_time_seconds": 36,
    "published_date": "2021-10-18T16:47:45Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2110.09456/features/markdown-grobid/2110.09456.md",
      "adr-crib": "data/papers/2110.09456/features/adr-crib/2110.09456.md",
      "adr-titles": "data/papers/2110.09456/features/adr-titles/2110.09456.md",
      "crib-sheet": "data/papers/2110.09456/features/crib-sheet/2110.09456.md",
      "compound-crib": "data/papers/2110.09456/features/compound-crib/2110.09456.md"
    }
  },
  "2111.11418": {
    "id": "2111.11418",
    "title": "MetaFormer Is Actually What You Need for Vision",
    "authors": "Weihao Yu, Mi Luo, Pan Zhou and 5 others",
    "abstract": "Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in Transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in Transformers with an embarrassingly simple spatial pooling operator to conduct only basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned Vision Transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 50%/62% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general architecture abstracted from Transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent Transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer.",
    "url": "https://arxiv.org/abs/2111.11418",
    "arxivId": "2111.11418",
    "last_visited": "2025-01-05T20:25:32.615Z",
    "last_read": "2025-01-05T20:26:49.250252",
    "total_reading_time_seconds": 5,
    "published_date": "2021-11-22T18:52:03Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2111.11418/features/markdown-grobid/2111.11418.md",
      "adr-crib": "data/papers/2111.11418/features/adr-crib/2111.11418.md",
      "adr-titles": "data/papers/2111.11418/features/adr-titles/2111.11418.md",
      "crib-sheet": "data/papers/2111.11418/features/crib-sheet/2111.11418.md",
      "compound-crib": "data/papers/2111.11418/features/compound-crib/2111.11418.md"
    }
  },
  "2112.04215": {
    "id": "2112.04215",
    "title": "Self-Supervised Models are Continual Learners",
    "authors": "Enrico Fini, Victor G. Turrisi da Costa, Xavier Alameda-Pineda and 3 others",
    "abstract": "Self-supervised models have been shown to produce comparable or better visual representations than their supervised counterparts when trained offline on unlabeled data at scale. However, their efficacy is catastrophically reduced in a Continual Learning (CL) scenario where data is presented to the model sequentially. In this paper, we show that self-supervised loss functions can be seamlessly converted into distillation mechanisms for CL by adding a predictor network that maps the current state of the representations to their past state. This enables us to devise a framework for Continual self-supervised visual representation Learning that (i) significantly improves the quality of the learned representations, (ii) is compatible with several state-of-the-art self-supervised objectives, and (iii) needs little to no hyperparameter tuning. We demonstrate the effectiveness of our approach empirically by training six popular self-supervised models in various CL settings.",
    "url": "https://arxiv.org/abs/2112.04215",
    "arxivId": "2112.04215",
    "last_visited": "2025-01-22T07:08:23.330Z",
    "last_read": "2025-01-22T07:08:23.330Z",
    "total_reading_time_seconds": 25,
    "published_date": "2021-12-08T10:39:13Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2112.04215/features/markdown-grobid/2112.04215.md",
      "adr-crib": "data/papers/2112.04215/features/adr-crib/2112.04215.md",
      "adr-titles": "data/papers/2112.04215/features/adr-titles/2112.04215.md",
      "crib-sheet": "data/papers/2112.04215/features/crib-sheet/2112.04215.md",
      "compound-crib": "data/papers/2112.04215/features/compound-crib/2112.04215.md"
    }
  },
  "2112.14569": {
    "id": "2112.14569",
    "title": "Fine-Tuning Transformers: Vocabulary Transfer",
    "authors": "Vladislav Mosin, Igor Samenko, Alexey Tikhonov and 2 others",
    "abstract": "Transformers are responsible for the vast majority of recent advances in natural language processing. The majority of practical natural language processing applications of these models are typically enabled through transfer learning. This paper studies if corpus-specific tokenization used for fine-tuning improves the resulting performance of the model. Through a series of experiments, we demonstrate that such tokenization combined with the initialization and fine-tuning strategy for the vocabulary tokens speeds up the transfer and boosts the performance of the fine-tuned model. We call this aspect of transfer facilitation vocabulary transfer.",
    "url": "https://arxiv.org/abs/2112.14569",
    "arxivId": "2112.14569",
    "last_visited": "2025-01-10T01:59:25.704Z",
    "last_read": "2025-01-10T02:01:59.118581",
    "total_reading_time_seconds": 28,
    "published_date": "2021-12-29T14:22:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50, 91F20",
      "I.2.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2112.14569/features/markdown-grobid/2112.14569.md",
      "adr-crib": "data/papers/2112.14569/features/adr-crib/2112.14569.md",
      "adr-titles": "data/papers/2112.14569/features/adr-titles/2112.14569.md",
      "crib-sheet": "data/papers/2112.14569/features/crib-sheet/2112.14569.md",
      "compound-crib": "data/papers/2112.14569/features/compound-crib/2112.14569.md"
    }
  },
  "2203.02155": {
    "id": "2203.02155",
    "title": "Training language models to follow instructions with human feedback",
    "authors": "Long Ouyang, Jeff Wu, Xu Jiang and 17 others",
    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
    "url": "https://arxiv.org/abs/2203.02155",
    "arxivId": "2203.02155",
    "last_visited": "2025-01-06T23:15:51.499Z",
    "last_read": "2025-01-06T23:16:42.092303",
    "total_reading_time_seconds": 5,
    "published_date": "2022-03-04T07:04:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2203.02155/features/markdown-grobid/2203.02155.md",
      "adr-crib": "data/papers/2203.02155/features/adr-crib/2203.02155.md",
      "adr-titles": "data/papers/2203.02155/features/adr-titles/2203.02155.md",
      "crib-sheet": "data/papers/2203.02155/features/crib-sheet/2203.02155.md",
      "compound-crib": "data/papers/2203.02155/features/compound-crib/2203.02155.md"
    }
  },
  "2204.00595": {
    "id": "2204.00595",
    "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate   Training",
    "authors": "Tri Dao, Beidi Chen, Nimit Sohoni and 7 others",
    "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called \"reverse sparsification,\" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.",
    "url": "https://arxiv.org/abs/2204.00595",
    "arxivId": "2204.00595",
    "last_visited": "2024-12-28T06:07:58.885Z",
    "last_read": "2025-01-04T15:02:51.856832",
    "total_reading_time_seconds": 35,
    "published_date": "2022-04-01T17:37:29Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2204.00595/features/markdown-grobid/2204.00595.md",
      "adr-crib": "data/papers/2204.00595/features/adr-crib/2204.00595.md",
      "adr-titles": "data/papers/2204.00595/features/adr-titles/2204.00595.md",
      "crib-sheet": "data/papers/2204.00595/features/crib-sheet/2204.00595.md",
      "compound-crib": "data/papers/2204.00595/features/compound-crib/2204.00595.md"
    }
  },
  "2205.12381": {
    "id": "2205.12381",
    "title": "First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual   Information Maximization",
    "authors": "Siddharth Reddy, Sergey Levine, Anca D. Dragan",
    "abstract": "How can we train an assistive human-machine interface (e.g., an electromyography-based limb prosthesis) to translate a user's raw command signals into the actions of a robot or computer when there is no prior mapping, we cannot ask the user for supervision in the form of action labels or reward feedback, and we do not have prior knowledge of the tasks the user is trying to accomplish? The key idea in this paper is that, regardless of the task, when an interface is more intuitive, the user's commands are less noisy. We formalize this idea as a completely unsupervised objective for optimizing interfaces: the mutual information between the user's command signals and the induced state transitions in the environment. To evaluate whether this mutual information score can distinguish between effective and ineffective interfaces, we conduct an observational study on 540K examples of users operating various keyboard and eye gaze interfaces for typing, controlling simulated robots, and playing video games. The results show that our mutual information scores are predictive of the ground-truth task completion metrics in a variety of domains, with an average Spearman's rank correlation of 0.43. In addition to offline evaluation of existing interfaces, we use our unsupervised objective to learn an interface from scratch: we randomly initialize the interface, have the user attempt to perform their desired tasks using the interface, measure the mutual information score, and update the interface to maximize mutual information through reinforcement learning. We evaluate our method through a user study with 12 participants who perform a 2D cursor control task using a perturbed mouse, and an experiment with one user playing the Lunar Lander game using hand gestures. The results show that we can learn an interface from scratch, without any user supervision or prior knowledge of tasks, in under 30 minutes.",
    "url": "https://arxiv.org/abs/2205.12381",
    "arxivId": "2205.12381",
    "last_visited": "2025-01-06T11:04:10.770Z",
    "last_read": "2025-01-06T11:07:41.188548",
    "total_reading_time_seconds": 13,
    "published_date": "2022-05-24T21:57:18Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.HC",
      "cs.RO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2205.12381/features/markdown-grobid/2205.12381.md",
      "adr-crib": "data/papers/2205.12381/features/adr-crib/2205.12381.md",
      "adr-titles": "data/papers/2205.12381/features/adr-titles/2205.12381.md",
      "crib-sheet": "data/papers/2205.12381/features/crib-sheet/2205.12381.md",
      "compound-crib": "data/papers/2205.12381/features/compound-crib/2205.12381.md"
    }
  },
  "2205.13509": {
    "id": "2205.13509",
    "title": "The role of disorder in the motion of chiral swimmers in the presence of   obstacles",
    "authors": "Danne M. van Roon, Giorgio Volpe, Margarida M. Telo da Gama, Nuno A. M. Araújo",
    "abstract": "The presence of obstacles is intuitively expected to hinder the diffusive transport of micro-swimmers. However, for chiral micro-swimmers, a low density of obstacles near a surface can enhance their diffusive behavior, due to the rectification of the chiral motion by the obstacles. Here, we study numerically the role that disorder plays in determining the transport dynamics of chiral micro-swimmers on surfaces with obstacles. We consider different densities of regularly spaced obstacles and distinct types of disorder: noise in the dynamics of the micro-swimmer, quenched noise in the positions of the obstacles as well as obstacle size polydispersity. We show that, depending on the type and strength of the disorder, the presence of obstacles can either enhance or hinder transport, and discuss implications for the control of active transport in disordered media.",
    "url": "https://arxiv.org/abs/2205.13509",
    "arxivId": "2205.13509",
    "last_visited": "2024-12-28T07:25:12.900Z",
    "last_read": "2025-01-04T14:49:54.238526",
    "total_reading_time_seconds": 5,
    "published_date": "2022-05-26T17:22:50Z",
    "arxiv_tags": [
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2205.13509/features/markdown-grobid/2205.13509.md",
      "adr-crib": "data/papers/2205.13509/features/adr-crib/2205.13509.md",
      "adr-titles": "data/papers/2205.13509/features/adr-titles/2205.13509.md",
      "crib-sheet": "data/papers/2205.13509/features/crib-sheet/2205.13509.md",
      "compound-crib": "data/papers/2205.13509/features/compound-crib/2205.13509.md"
    }
  },
  "2207.10342": {
    "id": "2207.10342",
    "title": "Language Model Cascades",
    "authors": "David Dohan, Winnie Xu, Aitor Lewkowycz and 9 others",
    "abstract": "Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.",
    "url": "https://arxiv.org/abs/2207.10342",
    "arxivId": "2207.10342",
    "last_visited": "2024-12-28T09:09:37.944Z",
    "last_read": "2025-01-04T14:49:42.250385",
    "total_reading_time_seconds": 51,
    "published_date": "2022-07-21T07:35:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2207.10342/features/markdown-grobid/2207.10342.md",
      "adr-crib": "data/papers/2207.10342/features/adr-crib/2207.10342.md",
      "adr-titles": "data/papers/2207.10342/features/adr-titles/2207.10342.md",
      "crib-sheet": "data/papers/2207.10342/features/crib-sheet/2207.10342.md",
      "compound-crib": "data/papers/2207.10342/features/compound-crib/2207.10342.md"
    }
  },
  "2208.02554": {
    "id": "2208.02554",
    "title": "Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add   Data",
    "authors": "Priyanka Singh, Vladislav D. Mosin, Ivan P. Yamshchikov",
    "abstract": "Working within specific NLP subdomains presents significant challenges, primarily due to a persistent deficit of data. Stringent privacy concerns and limited data accessibility often drive this shortage. Additionally, the medical domain demands high accuracy, where even marginal improvements in model performance can have profound impacts. In this study, we investigate the potential of vocabulary transfer to enhance model performance in biomedical NLP tasks. Specifically, we focus on vocabulary extension, a technique that involves expanding the target vocabulary to incorporate domain-specific biomedical terms. Our findings demonstrate that vocabulary extension, leads to measurable improvements in both downstream model performance and inference time.",
    "url": "https://arxiv.org/abs/2208.02554",
    "arxivId": "2208.02554",
    "last_visited": "2025-01-10T02:00:26.390000+00:00",
    "last_read": "2025-01-10T02:01:59.117896",
    "total_reading_time_seconds": 59,
    "published_date": "2022-08-04T09:53:22Z",
    "arxiv_tags": [
      "cs.CL",
      "I.2.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2208.02554/features/markdown-grobid/2208.02554.md",
      "adr-crib": "data/papers/2208.02554/features/adr-crib/2208.02554.md",
      "adr-titles": "data/papers/2208.02554/features/adr-titles/2208.02554.md",
      "crib-sheet": "data/papers/2208.02554/features/crib-sheet/2208.02554.md",
      "compound-crib": "data/papers/2208.02554/features/compound-crib/2208.02554.md"
    }
  },
  "2208.11665": {
    "id": "2208.11665",
    "title": "Statistical exploration of the Manifold Hypothesis",
    "authors": "Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy",
    "abstract": "The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.",
    "url": "https://arxiv.org/abs/2208.11665",
    "arxivId": "2208.11665",
    "last_visited": "2024-12-29T02:26:31.276Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-08-24T17:00:16Z",
    "arxiv_tags": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62R20, 62R40, 62G05, 62G20, 62R07, 62-08, 62H25, 62H30"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2208.11665/features/markdown-grobid/2208.11665.md",
      "adr-crib": "data/papers/2208.11665/features/adr-crib/2208.11665.md",
      "adr-titles": "data/papers/2208.11665/features/adr-titles/2208.11665.md",
      "crib-sheet": "data/papers/2208.11665/features/crib-sheet/2208.11665.md",
      "compound-crib": "data/papers/2208.11665/features/compound-crib/2208.11665.md"
    }
  },
  "2209.02740": {
    "id": "2209.02740",
    "title": "Emergent hypernetworks in weakly coupled oscillators",
    "authors": "Eddie Nijholt, Jorge Luis Ocampo-Espindola, Deniz Eroglu and 2 others",
    "abstract": "Networks of weakly coupled oscillators had a profound impact on our understanding of complex systems. Studies on model reconstruction from data have shown prevalent contributions from hypernetworks with triplet and higher interactions among oscillators, in spite that such models were originally defined as oscillator networks with pairwise interactions. Here, we show that hypernetworks can spontaneously emerge even in the presence of pairwise albeit nonlinear coupling given certain triplet frequency resonance conditions. The results are demonstrated in experiments with electrochemical oscillators and in simulations with integrate-and-fire neurons. By developing a comprehensive theory, we uncover the mechanism for emergent hypernetworks by identifying appearing and forbidden frequency resonant conditions. Furthermore, it is shown that microscopic linear (difference) coupling among units results in coupled mean fields, which have sufficient nonlinearity to facilitate hypernetworks. Our findings shed light on the apparent abundance of hypernetworks and provide a constructive way to predict and engineer their emergence.",
    "url": "https://arxiv.org/abs/2209.02740",
    "arxivId": "2209.02740",
    "last_visited": "2024-12-22T05:30:04.148Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2022-09-06T18:02:12Z",
    "arxiv_tags": [
      "math.DS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2209.02740/features/markdown-grobid/2209.02740.md",
      "adr-crib": "data/papers/2209.02740/features/adr-crib/2209.02740.md",
      "adr-titles": "data/papers/2209.02740/features/adr-titles/2209.02740.md",
      "crib-sheet": "data/papers/2209.02740/features/crib-sheet/2209.02740.md",
      "compound-crib": "data/papers/2209.02740/features/compound-crib/2209.02740.md"
    }
  },
  "2211.14453": {
    "id": "2211.14453",
    "title": "Transform Once: Efficient Operator Learning in Frequency Domain",
    "authors": "Michael Poli, Stefano Massaroli, Federico Berto and 4 others",
    "abstract": "Spectral analysis provides one of the most effective paradigms for information-preserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequency-domain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance-preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3x to 10x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our large-scale experiment), with over 20% reduction in average predictive error across tasks.",
    "url": "https://arxiv.org/abs/2211.14453",
    "arxivId": "2211.14453",
    "last_visited": "2024-12-28T07:23:40.569000+00:00",
    "last_read": "2025-01-04T15:02:00.950202",
    "total_reading_time_seconds": 4,
    "published_date": "2022-11-26T01:56:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2211.14453/features/markdown-grobid/2211.14453.md",
      "adr-crib": "data/papers/2211.14453/features/adr-crib/2211.14453.md",
      "adr-titles": "data/papers/2211.14453/features/adr-titles/2211.14453.md",
      "crib-sheet": "data/papers/2211.14453/features/crib-sheet/2211.14453.md",
      "compound-crib": "data/papers/2211.14453/features/compound-crib/2211.14453.md"
    }
  },
  "2212.07677": {
    "id": "2212.07677",
    "title": "Transformers learn in-context by gradient descent",
    "authors": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo and 4 others",
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "url": "https://arxiv.org/abs/2212.07677",
    "arxivId": "2212.07677",
    "last_visited": "2025-01-22T23:51:01.971Z",
    "last_read": "2025-01-22T23:51:01.971Z",
    "total_reading_time_seconds": 57,
    "published_date": "2022-12-15T09:21:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2212.07677/features/markdown-grobid/2212.07677.md",
      "adr-crib": "data/papers/2212.07677/features/adr-crib/2212.07677.md",
      "adr-titles": "data/papers/2212.07677/features/adr-titles/2212.07677.md",
      "crib-sheet": "data/papers/2212.07677/features/crib-sheet/2212.07677.md",
      "compound-crib": "data/papers/2212.07677/features/compound-crib/2212.07677.md"
    }
  },
  "2212.14052": {
    "id": "2212.14052",
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "authors": "Daniel Y. Fu, Tri Dao, Khaled K. Saab and 3 others",
    "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.",
    "url": "https://arxiv.org/abs/2212.14052",
    "arxivId": "2212.14052",
    "last_visited": "2025-02-15T22:40:30.551Z",
    "last_read": "2025-02-15T22:40:30.551Z",
    "total_reading_time_seconds": 16,
    "published_date": "2022-12-28T17:56:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2212.14052/features/markdown-grobid/2212.14052.md",
      "adr-crib": "data/papers/2212.14052/features/adr-crib/2212.14052.md",
      "adr-titles": "data/papers/2212.14052/features/adr-titles/2212.14052.md",
      "crib-sheet": "data/papers/2212.14052/features/crib-sheet/2212.14052.md",
      "compound-crib": "data/papers/2212.14052/features/compound-crib/2212.14052.md"
    }
  },
  "2302.04222": {
    "id": "2302.04222",
    "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
    "authors": "Shawn Shan, Jenna Cryan, Emily Wenger and 3 others",
    "abstract": "Recent text-to-image diffusion models such as MidJourney and Stable Diffusion threaten to displace many in the professional artist community. In particular, models can learn to mimic the artistic style of specific artists after \"fine-tuning\" on samples of their art. In this paper, we describe the design, implementation and evaluation of Glaze, a tool that enables artists to apply \"style cloaks\" to their art before sharing online. These cloaks apply barely perceptible perturbations to images, and when used as training data, mislead generative models that try to mimic a specific artist. In coordination with the professional artist community, we deploy user studies to more than 1000 artists, assessing their views of AI art, as well as the efficacy of our tool, its usability and tolerability of perturbations, and robustness across different scenarios and against adaptive countermeasures. Both surveyed artists and empirical CLIP-based scores show that even at low perturbation levels (p=0.05), Glaze is highly successful at disrupting mimicry under normal conditions (>92%) and against adaptive countermeasures (>85%).",
    "url": "https://arxiv.org/abs/2302.04222",
    "arxivId": "2302.04222",
    "last_visited": "2024-12-22T16:19:38.213Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-08T17:45:23Z",
    "arxiv_tags": [
      "cs.CR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2302.04222/features/markdown-grobid/2302.04222.md",
      "adr-crib": "data/papers/2302.04222/features/adr-crib/2302.04222.md",
      "adr-titles": "data/papers/2302.04222/features/adr-titles/2302.04222.md",
      "crib-sheet": "data/papers/2302.04222/features/crib-sheet/2302.04222.md",
      "compound-crib": "data/papers/2302.04222/features/compound-crib/2302.04222.md"
    }
  },
  "2302.10866": {
    "id": "2302.10866",
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen and 6 others",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
    "url": "https://arxiv.org/abs/2302.10866",
    "arxivId": "2302.10866",
    "last_visited": "2024-12-28T07:17:27.699Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-02-21T18:29:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2302.10866/features/markdown-grobid/2302.10866.md",
      "adr-crib": "data/papers/2302.10866/features/adr-crib/2302.10866.md",
      "adr-titles": "data/papers/2302.10866/features/adr-titles/2302.10866.md",
      "crib-sheet": "data/papers/2302.10866/features/crib-sheet/2302.10866.md",
      "compound-crib": "data/papers/2302.10866/features/compound-crib/2302.10866.md"
    }
  },
  "2302.11529": {
    "id": "2302.11529",
    "title": "Modular Deep Learning",
    "authors": "Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, Edoardo Maria Ponti",
    "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.",
    "url": "https://arxiv.org/abs/2302.11529",
    "arxivId": "2302.11529",
    "last_visited": "2024-12-24T03:05:18.719Z",
    "last_read": "2025-01-05T08:23:20.689362",
    "total_reading_time_seconds": 26,
    "published_date": "2023-02-22T18:11:25Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2302.11529/features/markdown-grobid/2302.11529.md",
      "adr-crib": "data/papers/2302.11529/features/adr-crib/2302.11529.md",
      "adr-titles": "data/papers/2302.11529/features/adr-titles/2302.11529.md",
      "crib-sheet": "data/papers/2302.11529/features/crib-sheet/2302.11529.md",
      "compound-crib": "data/papers/2302.11529/features/compound-crib/2302.11529.md"
    }
  },
  "2302.13714": {
    "id": "2302.13714",
    "title": "On the Design of Codes for DNA Computing: Secondary Structure Avoidance   Codes",
    "authors": "Tuan Thanh Nguyen, Kui Cai, Han Mao Kiah and 2 others",
    "abstract": "In this work, we investigate a challenging problem, which has been considered to be an important criterion in designing codewords for DNA computing purposes, namely secondary structure avoidance in single-stranded DNA molecules. In short, secondary structure refers to the tendency of a single-stranded DNA sequence to fold back upon itself, thus becoming inactive in the computation process. While some design criteria that reduces the possibility of secondary structure formation has been proposed by Milenkovic and Kashyap (2006), the main contribution of this work is to provide an explicit construction of DNA codes that completely avoid secondary structure of arbitrary stem length. Formally, given codeword length n and arbitrary integer m>=2, we provide efficient methods to construct DNA codes of length n that avoid secondary structure of any stem length more than or equal to m. Particularly, when m = 3, our constructions yield a family of DNA codes of rate 1.3031 bits/nt, while the highest rate found in the prior art was 1.1609 bits/nt. In addition, for m>=3log n + 4, we provide an efficient encoder that incurs only one redundant symbol.",
    "url": "https://arxiv.org/abs/2302.13714",
    "arxivId": "2302.13714",
    "last_visited": "2024-12-28T07:13:40.029Z",
    "last_read": "2025-01-04T15:02:09.852499",
    "total_reading_time_seconds": 6,
    "published_date": "2023-02-27T12:22:07Z",
    "arxiv_tags": [
      "cs.IT",
      "math.CO",
      "math.IT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2302.13714/features/markdown-grobid/2302.13714.md",
      "adr-crib": "data/papers/2302.13714/features/adr-crib/2302.13714.md",
      "adr-titles": "data/papers/2302.13714/features/adr-titles/2302.13714.md",
      "crib-sheet": "data/papers/2302.13714/features/crib-sheet/2302.13714.md",
      "compound-crib": "data/papers/2302.13714/features/compound-crib/2302.13714.md"
    }
  },
  "2303.00383": {
    "id": "2303.00383",
    "title": "Ordinal Poincaré Sections: Reconstructing the First Return Map from an   Ordinal Segmentation of Time Series",
    "authors": "Zahra Shahriari, Shannon Dee Algar, David M. Walker, Michael Small",
    "abstract": "We propose a robust and computationally efficient algorithm to generically construct first return maps of dynamical systems from time series without the need for embedding. Typically, a first return map is constructed using a heuristic convenience (maxima or zero-crossings of the time series, for example) or a computationally delicate geometric approach (explicitly constructing a Poincar\\'e section from a hyper-surface normal to the flow and then interpolating to determine intersections with trajectories). Our approach relies on ordinal partitions of the time series and builds the first return map from successive intersections with particular ordinal sequences. Generically, we can obtain distinct first return maps for each ordinal sequence. We define entropy-based measures to guide our selection of the ordinal sequence for a ``good'' first return map and show that this method can robustly be applied to time series from classical chaotic systems to extract the underlying first return map dynamics. The results are shown on several well-known dynamical systems (Lorenz, R{\\\"o}ssler and Mackey-Glass in chaotic regimes).",
    "url": "https://arxiv.org/abs/2303.00383",
    "arxivId": "2303.00383",
    "last_visited": "2025-01-03T08:49:43.332Z",
    "last_read": "2025-01-04T06:52:03.683895",
    "total_reading_time_seconds": 4,
    "published_date": "2023-03-01T10:09:57Z",
    "arxiv_tags": [
      "math.DS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2303.00383/features/markdown-grobid/2303.00383.md",
      "adr-crib": "data/papers/2303.00383/features/adr-crib/2303.00383.md",
      "adr-titles": "data/papers/2303.00383/features/adr-titles/2303.00383.md",
      "crib-sheet": "data/papers/2303.00383/features/crib-sheet/2303.00383.md",
      "compound-crib": "data/papers/2303.00383/features/compound-crib/2303.00383.md"
    }
  },
  "2303.03667": {
    "id": "2303.03667",
    "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",
    "authors": "Jierun Chen, Shiu-hong Kao, Hao He and 4 others",
    "abstract": "To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the depthwise convolution. We hence propose a novel partial convolution (PConv) that extracts spatial features more efficiently, by cutting down redundant computation and memory access simultaneously. Building upon our PConv, we further propose FasterNet, a new family of neural networks, which attains substantially higher running speed than others on a wide range of devices, without compromising on accuracy for various vision tasks. For example, on ImageNet-1k, our tiny FasterNet-T0 is $2.8\\times$, $3.3\\times$, and $2.4\\times$ faster than MobileViT-XXS on GPU, CPU, and ARM processors, respectively, while being $2.9\\%$ more accurate. Our large FasterNet-L achieves impressive $83.5\\%$ top-1 accuracy, on par with the emerging Swin-B, while having $36\\%$ higher inference throughput on GPU, as well as saving $37\\%$ compute time on CPU. Code is available at \\url{https://github.com/JierunChen/FasterNet}.",
    "url": "https://arxiv.org/abs/2303.03667",
    "arxivId": "2303.03667",
    "last_visited": "2025-01-05T20:19:32.266000+00:00",
    "last_read": "2025-01-05T20:21:34.744107",
    "total_reading_time_seconds": 35,
    "published_date": "2023-03-07T06:05:30Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2303.03667/features/markdown-grobid/2303.03667.md",
      "adr-crib": "data/papers/2303.03667/features/adr-crib/2303.03667.md",
      "adr-titles": "data/papers/2303.03667/features/adr-titles/2303.03667.md",
      "crib-sheet": "data/papers/2303.03667/features/crib-sheet/2303.03667.md",
      "compound-crib": "data/papers/2303.03667/features/compound-crib/2303.03667.md"
    }
  },
  "2303.08500": {
    "id": "2303.08500",
    "title": "The Devil's Advocate: Shattering the Illusion of Unexploitable Data   using Diffusion Models",
    "authors": "Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie",
    "abstract": "Protecting personal data against exploitation of machine learning models is crucial. Recently, availability attacks have shown great promise to provide an extra layer of protection against the unauthorized use of data to train neural networks. These methods aim to add imperceptible noise to clean data so that the neural networks cannot extract meaningful patterns from the protected data, claiming that they can make personal data \"unexploitable.\" This paper provides a strong countermeasure against such approaches, showing that unexploitable data might only be an illusion. In particular, we leverage the power of diffusion models and show that a carefully designed denoising process can counteract the effectiveness of the data-protecting perturbations. We rigorously analyze our algorithm, and theoretically prove that the amount of required denoising is directly related to the magnitude of the data-protecting perturbations. Our approach, called AVATAR, delivers state-of-the-art performance against a suite of recent availability attacks in various scenarios, outperforming adversarial training even under distribution mismatch between the diffusion model and the protected data. Our findings call for more research into making personal data unexploitable, showing that this goal is far from over. Our implementation is available at this repository: https://github.com/hmdolatabadi/AVATAR.",
    "url": "https://arxiv.org/abs/2303.08500",
    "arxivId": "2303.08500",
    "last_visited": "2024-12-22T16:41:41.995Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-15T10:20:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2303.08500/features/markdown-grobid/2303.08500.md",
      "adr-crib": "data/papers/2303.08500/features/adr-crib/2303.08500.md",
      "adr-titles": "data/papers/2303.08500/features/adr-titles/2303.08500.md",
      "crib-sheet": "data/papers/2303.08500/features/crib-sheet/2303.08500.md",
      "compound-crib": "data/papers/2303.08500/features/compound-crib/2303.08500.md"
    }
  },
  "2303.09489": {
    "id": "2303.09489",
    "title": "Effectively Modeling Time Series with Simple Discrete State Spaces",
    "authors": "Michael Zhang, Khaled K. Saab, Michael Poli and 3 others",
    "abstract": "Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix -- a canonical representation for discrete-time processes -- which enables SpaceTime's SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a \"closed-loop\" variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\\ell$ and state-space size $d$, we go from $\\tilde{O}(d \\ell)$ na\\\"ively to $\\tilde{O}(d + \\ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.",
    "url": "https://arxiv.org/abs/2303.09489",
    "arxivId": "2303.09489",
    "last_visited": "2024-12-28T07:09:58.237Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-03-16T17:08:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2303.09489/features/markdown-grobid/2303.09489.md",
      "adr-crib": "data/papers/2303.09489/features/adr-crib/2303.09489.md",
      "adr-titles": "data/papers/2303.09489/features/adr-titles/2303.09489.md",
      "crib-sheet": "data/papers/2303.09489/features/crib-sheet/2303.09489.md",
      "compound-crib": "data/papers/2303.09489/features/compound-crib/2303.09489.md"
    }
  },
  "2304.02234": {
    "id": "2304.02234",
    "title": "JPEG Compressed Images Can Bypass Protections Against AI Editing",
    "authors": "Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein",
    "abstract": "Recently developed text-to-image diffusion models make it easy to edit or create high-quality images. Their ease of use has raised concerns about the potential for malicious editing or deepfake creation. Imperceptible perturbations have been proposed as a means of protecting images from malicious editing by preventing diffusion models from generating realistic images. However, we find that the aforementioned perturbations are not robust to JPEG compression, which poses a major weakness because of the common usage and availability of JPEG. We discuss the importance of robustness for additive imperceptible perturbations and encourage alternative approaches to protect images against editing.",
    "url": "https://arxiv.org/abs/2304.02234",
    "arxivId": "2304.02234",
    "last_visited": "2024-12-22T16:44:11.039Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-05T05:30:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2304.02234/features/markdown-grobid/2304.02234.md",
      "adr-crib": "data/papers/2304.02234/features/adr-crib/2304.02234.md",
      "adr-titles": "data/papers/2304.02234/features/adr-titles/2304.02234.md",
      "crib-sheet": "data/papers/2304.02234/features/crib-sheet/2304.02234.md",
      "compound-crib": "data/papers/2304.02234/features/compound-crib/2304.02234.md"
    }
  },
  "2304.15004": {
    "id": "2304.15004",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "authors": "Rylan Schaeffer, Brando Miranda, Sanmi Koyejo",
    "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
    "url": "https://arxiv.org/abs/2304.15004",
    "arxivId": "2304.15004",
    "last_visited": "2024-12-21T06:06:11.226Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-28T17:52:11Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2304.15004/features/markdown-grobid/2304.15004.md",
      "adr-crib": "data/papers/2304.15004/features/adr-crib/2304.15004.md",
      "adr-titles": "data/papers/2304.15004/features/adr-titles/2304.15004.md",
      "crib-sheet": "data/papers/2304.15004/features/crib-sheet/2304.15004.md",
      "compound-crib": "data/papers/2304.15004/features/compound-crib/2304.15004.md"
    }
  },
  "2305.06161": {
    "id": "2305.06161",
    "title": "StarCoder: may the source be with you!",
    "authors": "Raymond Li, Loubna Ben Allal, Yangtian Zi and 64 others",
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
    "url": "https://arxiv.org/abs/2305.06161",
    "arxivId": "2305.06161",
    "last_visited": "2024-12-28T06:14:43.367Z",
    "last_read": "2025-01-04T15:02:21.867203",
    "total_reading_time_seconds": 12,
    "published_date": "2023-05-09T08:16:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2305.06161/features/markdown-grobid/2305.06161.md",
      "adr-crib": "data/papers/2305.06161/features/adr-crib/2305.06161.md",
      "adr-titles": "data/papers/2305.06161/features/adr-titles/2305.06161.md",
      "crib-sheet": "data/papers/2305.06161/features/crib-sheet/2305.06161.md",
      "compound-crib": "data/papers/2305.06161/features/compound-crib/2305.06161.md"
    }
  },
  "2305.13169": {
    "id": "2305.13169",
    "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data   Age, Domain Coverage, Quality, & Toxicity",
    "authors": "Shayne Longpre, Gregory Yauney, Emily Reif and 8 others",
    "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
    "url": "https://arxiv.org/abs/2305.13169",
    "arxivId": "2305.13169",
    "last_visited": "2024-12-30T20:20:50.556Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-22T15:57:53Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2305.13169/features/markdown-grobid/2305.13169.md",
      "adr-crib": "data/papers/2305.13169/features/adr-crib/2305.13169.md",
      "adr-titles": "data/papers/2305.13169/features/adr-titles/2305.13169.md",
      "crib-sheet": "data/papers/2305.13169/features/crib-sheet/2305.13169.md",
      "compound-crib": "data/papers/2305.13169/features/compound-crib/2305.13169.md"
    }
  },
  "2307.08691": {
    "id": "2307.08691",
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work   Partitioning",
    "authors": "Tri Dao",
    "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).",
    "url": "https://arxiv.org/abs/2307.08691",
    "arxivId": "2307.08691",
    "last_visited": "2024-12-28T06:14:32.268Z",
    "last_read": "2025-01-04T15:02:28.119002",
    "total_reading_time_seconds": 10,
    "published_date": "2023-07-17T17:50:36Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2307.08691/features/markdown-grobid/2307.08691.md",
      "adr-crib": "data/papers/2307.08691/features/adr-crib/2307.08691.md",
      "adr-titles": "data/papers/2307.08691/features/adr-titles/2307.08691.md",
      "crib-sheet": "data/papers/2307.08691/features/crib-sheet/2307.08691.md",
      "compound-crib": "data/papers/2307.08691/features/compound-crib/2307.08691.md"
    }
  },
  "2307.09288": {
    "id": "2307.09288",
    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "authors": "Hugo Touvron, Louis Martin, Kevin Stone and 65 others",
    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",
    "url": "https://arxiv.org/abs/2307.09288",
    "arxivId": "2307.09288",
    "last_visited": "2024-12-29T10:07:22.174Z",
    "last_read": "2025-01-04T14:49:21.229139",
    "total_reading_time_seconds": 26,
    "published_date": "2023-07-18T14:31:57Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2307.09288/features/markdown-grobid/2307.09288.md",
      "adr-crib": "data/papers/2307.09288/features/adr-crib/2307.09288.md",
      "adr-titles": "data/papers/2307.09288/features/adr-titles/2307.09288.md",
      "crib-sheet": "data/papers/2307.09288/features/crib-sheet/2307.09288.md",
      "compound-crib": "data/papers/2307.09288/features/compound-crib/2307.09288.md"
    }
  },
  "2307.12868": {
    "id": "2307.12868",
    "title": "Understanding the Latent Space of Diffusion Models through the Lens of   Riemannian Geometry",
    "authors": "Yong-Hyun Park, Mingi Kwon, Jaewoong Choi and 2 others",
    "abstract": "Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\\mathbf{x}_t \\in \\mathcal{X}$, we analyze them from a geometrical perspective. Our approach involves deriving the local latent basis within $\\mathcal{X}$ by leveraging the pullback metric associated with their encoding feature maps. Remarkably, our discovered local latent basis enables image editing capabilities by moving $\\mathbf{x}_t$, the latent space of DMs, along the basis vector at specific timesteps. We further analyze how the geometric structure of DMs evolves over diffusion timesteps and differs across different text conditions. This confirms the known phenomenon of coarse-to-fine generation, as well as reveals novel insights such as the discrepancy between $\\mathbf{x}_t$ across timesteps, the effect of dataset complexity, and the time-varying influence of text prompts. To the best of our knowledge, this paper is the first to present image editing through $\\mathbf{x}$-space traversal, editing only once at specific timestep $t$ without any additional training, and providing thorough analyses of the latent structure of DMs. The code to reproduce our experiments can be found at https://github.com/enkeejunior1/Diffusion-Pullback.",
    "url": "https://arxiv.org/abs/2307.12868",
    "arxivId": "2307.12868",
    "last_visited": "2025-01-10T20:55:12.357Z",
    "last_read": "2025-01-10T20:56:46.517220",
    "total_reading_time_seconds": 18,
    "published_date": "2023-07-24T15:06:42Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2307.12868/features/markdown-grobid/2307.12868.md",
      "adr-crib": "data/papers/2307.12868/features/adr-crib/2307.12868.md",
      "adr-titles": "data/papers/2307.12868/features/adr-titles/2307.12868.md",
      "crib-sheet": "data/papers/2307.12868/features/crib-sheet/2307.12868.md",
      "compound-crib": "data/papers/2307.12868/features/compound-crib/2307.12868.md"
    }
  },
  "2308.06259": {
    "id": "2308.06259",
    "title": "Self-Alignment with Instruction Backtranslation",
    "authors": "Xian Li, Ping Yu, Chunting Zhou and 5 others",
    "abstract": "We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",
    "url": "https://arxiv.org/abs/2308.06259",
    "arxivId": "2308.06259",
    "last_visited": "2024-12-30T04:57:10.471Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-11T17:47:54Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2308.06259/features/markdown-grobid/2308.06259.md",
      "adr-crib": "data/papers/2308.06259/features/adr-crib/2308.06259.md",
      "adr-titles": "data/papers/2308.06259/features/adr-titles/2308.06259.md",
      "crib-sheet": "data/papers/2308.06259/features/crib-sheet/2308.06259.md",
      "compound-crib": "data/papers/2308.06259/features/compound-crib/2308.06259.md"
    }
  },
  "2308.10718": {
    "id": "2308.10718",
    "title": "Backdooring Textual Inversion for Concept Censorship",
    "authors": "Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang",
    "abstract": "Recent years have witnessed success in AIGC (AI Generated Content). People can make use of a pre-trained diffusion model to generate images of high quality or freely modify existing pictures with only prompts in nature language. More excitingly, the emerging personalization techniques make it feasible to create specific-desired images with only a few images as references. However, this induces severe threats if such advanced techniques are misused by malicious users, such as spreading fake news or defaming individual reputations. Thus, it is necessary to regulate personalization models (i.e., concept censorship) for their development and advancement.   In this paper, we focus on the personalization technique dubbed Textual Inversion (TI), which is becoming prevailing for its lightweight nature and excellent performance. TI crafts the word embedding that contains detailed information about a specific object. Users can easily download the word embedding from public websites like Civitai and add it to their own stable diffusion model without fine-tuning for personalization. To achieve the concept censorship of a TI model, we propose leveraging the backdoor technique for good by injecting backdoors into the Textual Inversion embeddings. Briefly, we select some sensitive words as triggers during the training of TI, which will be censored for normal use. In the subsequent generation stage, if the triggers are combined with personalized embeddings as final prompts, the model will output a pre-defined target image rather than images including the desired malicious concept.   To demonstrate the effectiveness of our approach, we conduct extensive experiments on Stable Diffusion, a prevailing open-sourced text-to-image model. Our code, data, and results are available at https://concept-censorship.github.io.",
    "url": "https://arxiv.org/abs/2308.10718",
    "arxivId": "2308.10718",
    "last_visited": "2024-12-22T16:47:15.076Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-08-21T13:39:04Z",
    "arxiv_tags": [
      "cs.CR",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2308.10718/features/markdown-grobid/2308.10718.md",
      "adr-crib": "data/papers/2308.10718/features/adr-crib/2308.10718.md",
      "adr-titles": "data/papers/2308.10718/features/adr-titles/2308.10718.md",
      "crib-sheet": "data/papers/2308.10718/features/crib-sheet/2308.10718.md",
      "compound-crib": "data/papers/2308.10718/features/compound-crib/2308.10718.md"
    }
  },
  "2308.10792": {
    "id": "2308.10792",
    "title": "Instruction Tuning for Large Language Models: A Survey",
    "authors": "Shengyu Zhang, Linfeng Dong, Xiaoya Li and 8 others",
    "abstract": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
    "url": "https://arxiv.org/abs/2308.10792",
    "arxivId": "2308.10792",
    "last_visited": "2025-01-06T23:25:42.652000+00:00",
    "last_read": "2025-01-06T23:26:59.522369",
    "total_reading_time_seconds": 20,
    "published_date": "2023-08-21T15:35:16Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2308.10792/features/markdown-grobid/2308.10792.md",
      "adr-crib": "data/papers/2308.10792/features/adr-crib/2308.10792.md",
      "adr-titles": "data/papers/2308.10792/features/adr-titles/2308.10792.md",
      "crib-sheet": "data/papers/2308.10792/features/crib-sheet/2308.10792.md",
      "compound-crib": "data/papers/2308.10792/features/compound-crib/2308.10792.md"
    }
  },
  "2308.13561": {
    "id": "2308.13561",
    "title": "Project Aria: A New Tool for Egocentric Multi-Modal AI Research",
    "authors": "Jakob Engel, Kiran Somasundaram, Michael Goesele and 71 others",
    "abstract": "Egocentric, multi-modal data as available on future augmented reality (AR) devices provides unique challenges and opportunities for machine perception. These future devices will need to be all-day wearable in a socially acceptable form-factor to support always available, context-aware and personalized AI applications. Our team at Meta Reality Labs Research built the Aria device, an egocentric, multi-modal data recording and streaming device with the goal to foster and accelerate research in this area. In this paper, we describe the Aria device hardware including its sensor configuration and the corresponding software tools that enable recording and processing of such data.",
    "url": "https://arxiv.org/abs/2308.13561",
    "arxivId": "2308.13561",
    "last_visited": "2025-01-10T06:30:22.302Z",
    "last_read": "2025-01-10T06:32:03.790467",
    "total_reading_time_seconds": 19,
    "published_date": "2023-08-24T20:42:21Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2308.13561/features/markdown-grobid/2308.13561.md",
      "adr-crib": "data/papers/2308.13561/features/adr-crib/2308.13561.md",
      "adr-titles": "data/papers/2308.13561/features/adr-titles/2308.13561.md",
      "crib-sheet": "data/papers/2308.13561/features/crib-sheet/2308.13561.md",
      "compound-crib": "data/papers/2308.13561/features/compound-crib/2308.13561.md"
    }
  },
  "2309.06180": {
    "id": "2309.06180",
    "title": "Efficient Memory Management for Large Language Model Serving with   PagedAttention",
    "authors": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang and 6 others",
    "abstract": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
    "url": "https://arxiv.org/abs/2309.06180",
    "arxivId": "2309.06180",
    "last_visited": "2025-01-15T19:11:27.895Z",
    "last_read": "2025-01-15T19:12:38.377163",
    "total_reading_time_seconds": 11,
    "published_date": "2023-09-12T12:50:04Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.06180/features/markdown-grobid/2309.06180.md",
      "adr-crib": "data/papers/2309.06180/features/adr-crib/2309.06180.md",
      "adr-titles": "data/papers/2309.06180/features/adr-titles/2309.06180.md",
      "crib-sheet": "data/papers/2309.06180/features/crib-sheet/2309.06180.md",
      "compound-crib": "data/papers/2309.06180/features/compound-crib/2309.06180.md"
    }
  },
  "2309.07965": {
    "id": "2309.07965",
    "title": "A survey on relative Lipschitz saturation of algebras and its relation   with radicial algebras",
    "authors": "Thiago da Silva, Guilherme Schultz Netto",
    "abstract": "In this work, we introduce the concept of relative Lipschitz saturation, along with its key categorical and algebraic properties, and demonstrate how such a structure always gives rise to a radicial algebra.",
    "url": "https://arxiv.org/abs/2309.07965",
    "arxivId": "2309.07965",
    "last_visited": "2024-12-28T07:12:00.313000+00:00",
    "last_read": "2025-01-04T15:02:13.219648",
    "total_reading_time_seconds": 6,
    "published_date": "2023-09-14T18:02:12Z",
    "arxiv_tags": [
      "math.AC",
      "13B22"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.07965/features/markdown-grobid/2309.07965.md",
      "adr-crib": "data/papers/2309.07965/features/adr-crib/2309.07965.md",
      "adr-titles": "data/papers/2309.07965/features/adr-titles/2309.07965.md",
      "crib-sheet": "data/papers/2309.07965/features/crib-sheet/2309.07965.md",
      "compound-crib": "data/papers/2309.07965/features/compound-crib/2309.07965.md"
    }
  },
  "2309.12032": {
    "id": "2309.12032",
    "title": "Human-in-the-Loop Causal Discovery under Latent Confounding using   Ancestral GFlowNets",
    "authors": "Tiago da Silva, Eliezer Silva, António Góis and 4 others",
    "abstract": "Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expert about the relations among variables, effectively reducing the uncertainty of our belief over ancestral graphs. Finally, we update our samples to incorporate human feedback via importance sampling. Importantly, our method does not require causal sufficiency (i.e., unobserved confounders may exist). Experiments with synthetic observational data show that our method can accurately sample from distributions over ancestral graphs and that we can greatly improve inference quality with human aid.",
    "url": "https://arxiv.org/abs/2309.12032",
    "arxivId": "2309.12032",
    "last_visited": "2024-12-28T06:14:26.797Z",
    "last_read": "2025-01-04T15:02:28.120081",
    "total_reading_time_seconds": 12,
    "published_date": "2023-09-21T12:53:45Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.12032/features/markdown-grobid/2309.12032.md",
      "adr-crib": "data/papers/2309.12032/features/adr-crib/2309.12032.md",
      "adr-titles": "data/papers/2309.12032/features/adr-titles/2309.12032.md",
      "crib-sheet": "data/papers/2309.12032/features/crib-sheet/2309.12032.md",
      "compound-crib": "data/papers/2309.12032/features/compound-crib/2309.12032.md"
    }
  },
  "2309.14556": {
    "id": "2309.14556",
    "title": "Art or Artifice? Large Language Models and the False Promise of   Creativity",
    "authors": "Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal and 2 others",
    "abstract": "Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.",
    "url": "https://arxiv.org/abs/2309.14556",
    "arxivId": "2309.14556",
    "last_visited": "2024-12-30T14:42:55.576Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-09-25T22:02:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.14556/features/markdown-grobid/2309.14556.md",
      "adr-crib": "data/papers/2309.14556/features/adr-crib/2309.14556.md",
      "adr-titles": "data/papers/2309.14556/features/adr-titles/2309.14556.md",
      "crib-sheet": "data/papers/2309.14556/features/crib-sheet/2309.14556.md",
      "compound-crib": "data/papers/2309.14556/features/compound-crib/2309.14556.md"
    }
  },
  "2310.01889": {
    "id": "2310.01889",
    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "authors": "Hao Liu, Matei Zaharia, Pieter Abbeel",
    "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
    "url": "https://arxiv.org/abs/2310.01889",
    "arxivId": "2310.01889",
    "last_visited": "2025-01-02T07:40:21.565Z",
    "last_read": "2025-01-04T06:52:27.608961",
    "total_reading_time_seconds": 40,
    "published_date": "2023-10-03T08:44:50Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2310.01889/features/markdown-grobid/2310.01889.md",
      "adr-crib": "data/papers/2310.01889/features/adr-crib/2310.01889.md",
      "adr-titles": "data/papers/2310.01889/features/adr-titles/2310.01889.md",
      "crib-sheet": "data/papers/2310.01889/features/crib-sheet/2310.01889.md",
      "compound-crib": "data/papers/2310.01889/features/compound-crib/2310.01889.md"
    }
  },
  "2310.16410": {
    "id": "2310.16410",
    "title": "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in   AlphaZero",
    "authors": "Lisa Schut, Nenad Tomasev, Tom McGrath and 3 others",
    "abstract": "Artificial Intelligence (AI) systems have made remarkable progress, attaining super-human performance across various domains. This presents us with an opportunity to further human knowledge and improve human expert performance by leveraging the hidden knowledge encoded within these highly performant AI systems. Yet, this knowledge is often hard to extract, and may be hard to understand or learn from. Here, we show that this is possible by proposing a new method that allows us to extract new chess concepts in AlphaZero, an AI system that mastered the game of chess via self-play without human supervision. Our analysis indicates that AlphaZero may encode knowledge that extends beyond the existing human knowledge, but knowledge that is ultimately not beyond human grasp, and can be successfully learned from. In a human study, we show that these concepts are learnable by top human experts, as four top chess grandmasters show improvements in solving the presented concept prototype positions. This marks an important first milestone in advancing the frontier of human knowledge by leveraging AI; a development that could bear profound implications and help us shape how we interact with AI systems across many AI applications.",
    "url": "https://arxiv.org/abs/2310.16410",
    "arxivId": "2310.16410",
    "last_visited": "2025-01-24T08:28:29.420Z",
    "last_read": "2025-01-24T08:28:29.420Z",
    "total_reading_time_seconds": 7,
    "published_date": "2023-10-25T06:49:26Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2310.16410/features/markdown-grobid/2310.16410.md",
      "adr-crib": "data/papers/2310.16410/features/adr-crib/2310.16410.md",
      "adr-titles": "data/papers/2310.16410/features/adr-titles/2310.16410.md",
      "crib-sheet": "data/papers/2310.16410/features/crib-sheet/2310.16410.md",
      "compound-crib": "data/papers/2310.16410/features/compound-crib/2310.16410.md"
    }
  },
  "2310.17157": {
    "id": "2310.17157",
    "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
    "authors": "Zichang Liu, Jue Wang, Tri Dao and 8 others",
    "abstract": "Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.",
    "url": "https://arxiv.org/abs/2310.17157",
    "arxivId": "2310.17157",
    "last_visited": "2024-12-28T06:14:06.334Z",
    "last_read": "2025-01-04T15:02:30.861843",
    "total_reading_time_seconds": 19,
    "published_date": "2023-10-26T05:01:09Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2310.17157/features/markdown-grobid/2310.17157.md",
      "adr-crib": "data/papers/2310.17157/features/adr-crib/2310.17157.md",
      "adr-titles": "data/papers/2310.17157/features/adr-titles/2310.17157.md",
      "crib-sheet": "data/papers/2310.17157/features/crib-sheet/2310.17157.md",
      "compound-crib": "data/papers/2310.17157/features/compound-crib/2310.17157.md"
    }
  },
  "2312.00330": {
    "id": "2312.00330",
    "title": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style   Adapter",
    "authors": "Gongye Liu, Menghan Xia, Yong Zhang and 6 others",
    "abstract": "Text-to-video (T2V) models have shown remarkable capabilities in generating diverse videos. However, they struggle to produce user-desired stylized videos due to (i) text's inherent clumsiness in expressing specific styles and (ii) the generally degraded style fidelity. To address these challenges, we introduce StyleCrafter, a generic method that enhances pre-trained T2V models with a style control adapter, enabling video generation in any style by providing a reference image. Considering the scarcity of stylized video datasets, we propose to first train a style control adapter using style-rich image datasets, then transfer the learned stylization ability to video generation through a tailor-made finetuning paradigm. To promote content-style disentanglement, we remove style descriptions from the text prompt and extract style information solely from the reference image using a decoupling learning strategy. Additionally, we design a scale-adaptive fusion module to balance the influences of text-based content features and image-based style features, which helps generalization across various text and style combinations. StyleCrafter efficiently generates high-quality stylized videos that align with the content of the texts and resemble the style of the reference images. Experiments demonstrate that our approach is more flexible and efficient than existing competitors.",
    "url": "https://arxiv.org/abs/2312.00330",
    "arxivId": "2312.00330",
    "last_visited": "2024-12-15T20:33:49.865Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-01T03:53:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2312.00330/features/markdown-grobid/2312.00330.md",
      "adr-crib": "data/papers/2312.00330/features/adr-crib/2312.00330.md",
      "adr-titles": "data/papers/2312.00330/features/adr-titles/2312.00330.md",
      "crib-sheet": "data/papers/2312.00330/features/crib-sheet/2312.00330.md",
      "compound-crib": "data/papers/2312.00330/features/compound-crib/2312.00330.md"
    }
  },
  "2312.00752": {
    "id": "2312.00752",
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "authors": "Albert Gu, Tri Dao",
    "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.",
    "url": "https://arxiv.org/abs/2312.00752",
    "arxivId": "2312.00752",
    "last_visited": "2025-01-23T07:24:53.349Z",
    "last_read": "2025-01-23T07:24:53.349Z",
    "total_reading_time_seconds": 23,
    "published_date": "2023-12-01T18:01:34Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2312.00752/features/markdown-grobid/2312.00752.md",
      "adr-crib": "data/papers/2312.00752/features/adr-crib/2312.00752.md",
      "adr-titles": "data/papers/2312.00752/features/adr-titles/2312.00752.md",
      "crib-sheet": "data/papers/2312.00752/features/crib-sheet/2312.00752.md",
      "compound-crib": "data/papers/2312.00752/features/compound-crib/2312.00752.md"
    }
  },
  "2312.07731": {
    "id": "2312.07731",
    "title": "A Response to Glaze Purification via IMPRESS",
    "authors": "Shawn Shan, Stanley Wu, Haitao Zheng, Ben Y. Zhao",
    "abstract": "Recent work proposed a new mechanism to remove protective perturbation added by Glaze in order to again enable mimicry of art styles from images protected by Glaze. Despite promising results shown in the original paper, our own tests with the authors' code demonstrated several limitations of the proposed purification approach. The main limitations are 1) purification has a limited effect when tested on artists that are not well-known historical artists already embedded in original training data, 2) problems in evaluation metrics, and 3) collateral damage on mimicry result for clean images. We believe these limitations should be carefully considered in order to understand real world usability of the purification attack.",
    "url": "https://arxiv.org/abs/2312.07731",
    "arxivId": "2312.07731",
    "last_visited": "2024-12-22T16:49:35.138Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2023-12-12T20:52:27Z",
    "arxiv_tags": [
      "cs.CR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2312.07731/features/markdown-grobid/2312.07731.md",
      "adr-crib": "data/papers/2312.07731/features/adr-crib/2312.07731.md",
      "adr-titles": "data/papers/2312.07731/features/adr-titles/2312.07731.md",
      "crib-sheet": "data/papers/2312.07731/features/crib-sheet/2312.07731.md",
      "compound-crib": "data/papers/2312.07731/features/compound-crib/2312.07731.md"
    }
  },
  "2312.17127": {
    "id": "2312.17127",
    "title": "Probabilistic programming interfaces for random graphs: Markov   categories, graphons, and nominal sets",
    "authors": "Nathanael L. Ackerman, Cameron E. Freer, Younesse Kaddar and 5 others",
    "abstract": "We study semantic models of probabilistic programming languages over graphs, and establish a connection to graphons from graph theory and combinatorics. We show that every well-behaved equational theory for our graph probabilistic programming language corresponds to a graphon, and conversely, every graphon arises in this way.   We provide three constructions for showing that every graphon arises from an equational theory. The first is an abstract construction, using Markov categories and monoidal indeterminates. The second and third are more concrete. The second is in terms of traditional measure theoretic probability, which covers 'black-and-white' graphons. The third is in terms of probability monads on the nominal sets of Gabbay and Pitts. Specifically, we use a variation of nominal sets induced by the theory of graphs, which covers Erd\\H{o}s-R\\'enyi graphons. In this way, we build new models of graph probabilistic programming from graphons.",
    "url": "https://arxiv.org/abs/2312.17127",
    "arxivId": "2312.17127",
    "last_visited": "2024-12-29T19:53:34.293000+00:00",
    "last_read": "2025-01-04T14:49:15.404627",
    "total_reading_time_seconds": 10,
    "published_date": "2023-12-28T17:04:50Z",
    "arxiv_tags": [
      "cs.PL",
      "cs.LO",
      "math.PR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2312.17127/features/markdown-grobid/2312.17127.md",
      "adr-crib": "data/papers/2312.17127/features/adr-crib/2312.17127.md",
      "adr-titles": "data/papers/2312.17127/features/adr-titles/2312.17127.md",
      "crib-sheet": "data/papers/2312.17127/features/crib-sheet/2312.17127.md",
      "compound-crib": "data/papers/2312.17127/features/compound-crib/2312.17127.md"
    }
  },
  "2401.00649": {
    "id": "2401.00649",
    "title": "Linear Model and Extensions",
    "authors": "Peng Ding",
    "abstract": "I developed the lecture notes based on my ``Linear Model'' course at the University of California Berkeley over the past seven years. This book provides an intermediate-level introduction to the linear model. It balances rigorous proofs and heuristic arguments. This book provides R code to replicate all simulation studies and case studies.",
    "url": "https://arxiv.org/abs/2401.00649",
    "arxivId": "2401.00649",
    "last_visited": "2025-01-14T15:24:20.284000+00:00",
    "last_read": "2025-01-14T15:20:47.368469",
    "total_reading_time_seconds": 308,
    "published_date": "2024-01-01T03:34:17Z",
    "arxiv_tags": [
      "stat.ME",
      "stat.AP"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2401.00649/features/markdown-grobid/2401.00649.md",
      "adr-crib": "data/papers/2401.00649/features/adr-crib/2401.00649.md",
      "adr-titles": "data/papers/2401.00649/features/adr-titles/2401.00649.md",
      "crib-sheet": "data/papers/2401.00649/features/crib-sheet/2401.00649.md",
      "compound-crib": "data/papers/2401.00649/features/compound-crib/2401.00649.md"
    }
  },
  "2401.10774": {
    "id": "2401.10774",
    "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple   Decoding Heads",
    "authors": "Tianle Cai, Yuhong Li, Zhengyang Geng and 4 others",
    "abstract": "Large Language Models (LLMs) employ auto-regressive decoding that requires sequential computation, with each step reliant on the previous one's output. This creates a bottleneck as each step necessitates moving the full model parameters from High-Bandwidth Memory (HBM) to the accelerator's cache. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa substantially reduces the number of decoding steps required. We present two levels of fine-tuning procedures for Medusa to meet the needs of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa is fine-tuned together with the backbone LLM, enabling better prediction accuracy of Medusa heads and higher speedup but needing a special training recipe that preserves the backbone model's capabilities.   Moreover, we propose several extensions that improve or expand the utility of Medusa, including a self-distillation to handle situations where no training data is available and a typical acceptance scheme to boost the acceptance rate while maintaining generation quality. We evaluate Medusa on models of various sizes and training procedures. Our experiments demonstrate that Medusa-1 can achieve over 2.2x speedup without compromising generation quality, while Medusa-2 further improves the speedup to 2.3-3.6x.",
    "url": "https://arxiv.org/abs/2401.10774",
    "arxivId": "2401.10774",
    "last_visited": "2024-12-28T07:11:26.727Z",
    "last_read": "2025-01-04T15:02:15.846437",
    "total_reading_time_seconds": 20,
    "published_date": "2024-01-19T15:48:40Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2401.10774/features/markdown-grobid/2401.10774.md",
      "adr-crib": "data/papers/2401.10774/features/adr-crib/2401.10774.md",
      "adr-titles": "data/papers/2401.10774/features/adr-titles/2401.10774.md",
      "crib-sheet": "data/papers/2401.10774/features/crib-sheet/2401.10774.md",
      "compound-crib": "data/papers/2401.10774/features/compound-crib/2401.10774.md"
    }
  },
  "2402.03239": {
    "id": "2402.03239",
    "title": "Bluesky and the AT Protocol: Usable Decentralized Social Media",
    "authors": "Martin Kleppmann, Paul Frazee, Jake Gold and 6 others",
    "abstract": "Bluesky is a new social network built upon the AT Protocol, a decentralized foundation for public social media. It was launched in private beta in February 2023, and has grown to over 10 million registered users by October 2024. In this paper we introduce the architecture of Bluesky and the AT Protocol, and explain how the technical design of Bluesky is informed by our goals: to enable decentralization by having multiple interoperable providers for every part of the system; to make it easy for users to switch providers; to give users agency over the content they see; and to provide a simple user experience that does not burden users with complexity arising from the system's decentralized nature. The system's openness allows anybody to contribute to content moderation and community management, and we invite the research community to use Bluesky as a dataset and testing ground for new approaches in social media moderation.",
    "url": "https://arxiv.org/abs/2402.03239",
    "arxivId": "2402.03239",
    "last_visited": "2024-12-22T05:41:38.653Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-05T17:55:51Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.SI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.03239/features/markdown-grobid/2402.03239.md",
      "adr-crib": "data/papers/2402.03239/features/adr-crib/2402.03239.md",
      "adr-titles": "data/papers/2402.03239/features/adr-titles/2402.03239.md",
      "crib-sheet": "data/papers/2402.03239/features/crib-sheet/2402.03239.md",
      "compound-crib": "data/papers/2402.03239/features/compound-crib/2402.03239.md"
    }
  },
  "2402.09949": {
    "id": "2402.09949",
    "title": "Multi-word Tokenization for Sequence Compression",
    "authors": "Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini",
    "abstract": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
    "url": "https://arxiv.org/abs/2402.09949",
    "arxivId": "2402.09949",
    "last_visited": "2025-01-10T01:40:44.674Z",
    "last_read": "2025-01-10T01:42:15.861971",
    "total_reading_time_seconds": 14,
    "published_date": "2024-02-15T13:52:23Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.09949/features/markdown-grobid/2402.09949.md",
      "adr-crib": "data/papers/2402.09949/features/adr-crib/2402.09949.md",
      "adr-titles": "data/papers/2402.09949/features/adr-titles/2402.09949.md",
      "crib-sheet": "data/papers/2402.09949/features/crib-sheet/2402.09949.md",
      "compound-crib": "data/papers/2402.09949/features/compound-crib/2402.09949.md"
    }
  },
  "2402.09977": {
    "id": "2402.09977",
    "title": "Fast Vocabulary Transfer for Language Model Compression",
    "authors": "Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, Paolo Torroni",
    "abstract": "Real-world business applications require a trade-off between language model performance and size. We propose a new method for model compression that relies on vocabulary transfer. We evaluate the method on various vertical domains and downstream tasks. Our results indicate that vocabulary transfer can be effectively used in combination with other compression techniques, yielding a significant reduction in model size and inference time while marginally compromising on performance.",
    "url": "https://arxiv.org/abs/2402.09977",
    "arxivId": "2402.09977",
    "last_visited": "2025-01-10T02:00:48.166000+00:00",
    "last_read": "2025-01-10T02:01:59.117108",
    "total_reading_time_seconds": 65,
    "published_date": "2024-02-15T14:37:07Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.09977/features/markdown-grobid/2402.09977.md",
      "adr-crib": "data/papers/2402.09977/features/adr-crib/2402.09977.md",
      "adr-titles": "data/papers/2402.09977/features/adr-titles/2402.09977.md",
      "crib-sheet": "data/papers/2402.09977/features/crib-sheet/2402.09977.md",
      "compound-crib": "data/papers/2402.09977/features/compound-crib/2402.09977.md"
    }
  },
  "2402.10193": {
    "id": "2402.10193",
    "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
    "authors": "James Liu, Guangxuan Xiao, Kai Li and 4 others",
    "abstract": "Large Language Models (LLMs) are typically trained in two phases: pre-training on large internet-scale datasets, and fine-tuning for downstream tasks. Given the higher computational demand of pre-training, it's intuitive to assume that fine-tuning adds less new information to the model, and is thus more compressible. We explore this assumption by decomposing the weights of fine-tuned models into their pre-trained components and an additional delta. We introduce a simple method, BitDelta, which successfully quantizes this delta down to 1 bit without compromising performance. This interesting finding not only highlights the potential redundancy of information added during fine-tuning, but also has significant implications for the multi-tenant serving and multi-tenant storage of fine-tuned models. By enabling the use of a single high-precision base model accompanied by multiple 1-bit deltas, BitDelta dramatically reduces GPU memory requirements by more than 10x, which can also be translated to enhanced generation latency in multi-tenant settings. We validate BitDelta through experiments across Llama-2 and Mistral model families, and on models up to 70B parameters, showcasing minimal performance degradation over all tested settings.",
    "url": "https://arxiv.org/abs/2402.10193",
    "arxivId": "2402.10193",
    "last_visited": "2024-12-28T06:12:55.272Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-15T18:50:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.10193/features/markdown-grobid/2402.10193.md",
      "adr-crib": "data/papers/2402.10193/features/adr-crib/2402.10193.md",
      "adr-titles": "data/papers/2402.10193/features/adr-titles/2402.10193.md",
      "crib-sheet": "data/papers/2402.10193/features/crib-sheet/2402.10193.md",
      "compound-crib": "data/papers/2402.10193/features/compound-crib/2402.10193.md"
    }
  },
  "2402.16670": {
    "id": "2402.16670",
    "title": "Pay Attention: a Call to Regulate the Attention Market and Prevent   Algorithmic Emotional Governance",
    "authors": "Franck Michel, Fabien Gandon",
    "abstract": "Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.",
    "url": "https://arxiv.org/abs/2402.16670",
    "arxivId": "2402.16670",
    "last_visited": "2024-12-27T22:07:23.174Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-26T15:46:43Z",
    "arxiv_tags": [
      "cs.SI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.16670/features/markdown-grobid/2402.16670.md",
      "adr-crib": "data/papers/2402.16670/features/adr-crib/2402.16670.md",
      "adr-titles": "data/papers/2402.16670/features/adr-titles/2402.16670.md",
      "crib-sheet": "data/papers/2402.16670/features/crib-sheet/2402.16670.md",
      "compound-crib": "data/papers/2402.16670/features/compound-crib/2402.16670.md"
    }
  },
  "2402.19173": {
    "id": "2402.19173",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "authors": "Anton Lozhkov, Raymond Li, Loubna Ben Allal and 63 others",
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
    "url": "https://arxiv.org/abs/2402.19173",
    "arxivId": "2402.19173",
    "last_visited": "2024-12-28T06:12:42.484Z",
    "last_read": "2025-01-04T15:02:39.855254",
    "total_reading_time_seconds": 11,
    "published_date": "2024-02-29T13:53:35Z",
    "arxiv_tags": [
      "cs.SE",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.19173/features/markdown-grobid/2402.19173.md",
      "adr-crib": "data/papers/2402.19173/features/adr-crib/2402.19173.md",
      "adr-titles": "data/papers/2402.19173/features/adr-titles/2402.19173.md",
      "crib-sheet": "data/papers/2402.19173/features/crib-sheet/2402.19173.md",
      "compound-crib": "data/papers/2402.19173/features/compound-crib/2402.19173.md"
    }
  },
  "2403.08540": {
    "id": "2403.08540",
    "title": "Language models scale reliably with over-training and on downstream   tasks",
    "authors": "Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar and 22 others",
    "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
    "url": "https://arxiv.org/abs/2403.08540",
    "arxivId": "2403.08540",
    "last_visited": "2024-12-30T19:37:31.972Z",
    "last_read": "2025-01-04T06:53:18.648664",
    "total_reading_time_seconds": 22,
    "published_date": "2024-03-13T13:54:00Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2403.08540/features/markdown-grobid/2403.08540.md",
      "adr-crib": "data/papers/2403.08540/features/adr-crib/2403.08540.md",
      "adr-titles": "data/papers/2403.08540/features/adr-titles/2403.08540.md",
      "crib-sheet": "data/papers/2403.08540/features/crib-sheet/2403.08540.md",
      "compound-crib": "data/papers/2403.08540/features/compound-crib/2403.08540.md"
    }
  },
  "2403.10304": {
    "id": "2403.10304",
    "title": "KIF: A Wikidata-Based Framework for Integrating Heterogeneous Knowledge   Sources",
    "authors": "Guilherme Lima, João M. B. Rodrigues, Marcelo Machado and 6 others",
    "abstract": "We present a Wikidata-based framework, called KIF, for virtually integrating heterogeneous knowledge sources. KIF is written in Python and is released as open-source. It leverages Wikidata's data model and vocabulary plus user-defined mappings to construct a unified view of the underlying sources while keeping track of the context and provenance of their statements. The underlying sources can be triplestores, relational databases, CSV files, etc., which may or may not use the vocabulary and RDF encoding of Wikidata. The end result is a virtual knowledge base which behaves like an \"extended Wikidata\" and which can be queried using a simple but expressive pattern language, defined in terms of Wikidata's data model. In this paper, we present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF",
    "url": "https://arxiv.org/abs/2403.10304",
    "arxivId": "2403.10304",
    "last_visited": "2024-12-28T06:12:17.716Z",
    "last_read": "2025-01-04T15:02:42.885145",
    "total_reading_time_seconds": 24,
    "published_date": "2024-03-15T13:46:36Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DB"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2403.10304/features/markdown-grobid/2403.10304.md",
      "adr-crib": "data/papers/2403.10304/features/adr-crib/2403.10304.md",
      "adr-titles": "data/papers/2403.10304/features/adr-titles/2403.10304.md",
      "crib-sheet": "data/papers/2403.10304/features/crib-sheet/2403.10304.md",
      "compound-crib": "data/papers/2403.10304/features/compound-crib/2403.10304.md"
    }
  },
  "2403.14554": {
    "id": "2403.14554",
    "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time   Rendering",
    "authors": "Antoine Guédon, Vincent Lepetit",
    "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for high-quality rendering and editing of complex 3D effects in real-time. Our approach builds on the recent 3D Gaussian Splatting framework, which optimizes a set of 3D Gaussians to approximate a radiance field from images. We propose first extracting a base mesh from Gaussians during optimization, then building and refining an adaptive layer of Gaussians with a variable thickness around the mesh to better capture the fine details and volumetric effects near the surface, such as hair or grass. We call this layer Gaussian Frosting, as it resembles a coating of frosting on a cake. The fuzzier the material, the thicker the frosting. We also introduce a parameterization of the Gaussians to enforce them to stay inside the frosting layer and automatically adjust their parameters when deforming, rescaling, editing or animating the mesh. Our representation allows for efficient rendering using Gaussian splatting, as well as editing and animation by modifying the base mesh. We demonstrate the effectiveness of our method on various synthetic and real scenes, and show that it outperforms existing surface-based approaches. We will release our code and a web-based viewer as additional contributions. Our project page is the following: https://anttwo.github.io/frosting/",
    "url": "https://arxiv.org/abs/2403.14554",
    "arxivId": "2403.14554",
    "last_visited": "2025-01-03T02:52:11.974Z",
    "last_read": "2025-01-04T06:52:06.631879",
    "total_reading_time_seconds": 3,
    "published_date": "2024-03-21T16:53:03Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2403.14554/features/markdown-grobid/2403.14554.md",
      "adr-crib": "data/papers/2403.14554/features/adr-crib/2403.14554.md",
      "adr-titles": "data/papers/2403.14554/features/adr-titles/2403.14554.md",
      "crib-sheet": "data/papers/2403.14554/features/crib-sheet/2403.14554.md",
      "compound-crib": "data/papers/2403.14554/features/compound-crib/2403.14554.md"
    }
  },
  "2404.13320": {
    "id": "2404.13320",
    "title": "Pixel is a Barrier: Diffusion Models Are More Adversarially Robust Than   We Think",
    "authors": "Haotian Xue, Yongxin Chen",
    "abstract": "Adversarial examples for diffusion models are widely used as solutions for safety concerns. By adding adversarial perturbations to personal images, attackers can not edit or imitate them easily. However, it is essential to note that all these protections target the latent diffusion model (LDMs), the adversarial examples for diffusion models in the pixel space (PDMs) are largely overlooked. This may mislead us to think that the diffusion models are vulnerable to adversarial attacks like most deep models. In this paper, we show novel findings that: even though gradient-based white-box attacks can be used to attack the LDMs, they fail to attack PDMs. This finding is supported by extensive experiments of almost a wide range of attacking methods on various PDMs and LDMs with different model structures, which means diffusion models are indeed much more robust against adversarial attacks. We also find that PDMs can be used as an off-the-shelf purifier to effectively remove the adversarial patterns that were generated on LDMs to protect the images, which means that most protection methods nowadays, to some extent, cannot protect our images from malicious attacks. We hope that our insights will inspire the community to rethink the adversarial samples for diffusion models as protection methods and move forward to more effective protection. Codes are available in https://github.com/xavihart/PDM-Pure.",
    "url": "https://arxiv.org/abs/2404.13320",
    "arxivId": "2404.13320",
    "last_visited": "2024-12-22T16:33:17.796Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-04-20T08:28:43Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2404.13320/features/markdown-grobid/2404.13320.md",
      "adr-crib": "data/papers/2404.13320/features/adr-crib/2404.13320.md",
      "adr-titles": "data/papers/2404.13320/features/adr-titles/2404.13320.md",
      "crib-sheet": "data/papers/2404.13320/features/crib-sheet/2404.13320.md",
      "compound-crib": "data/papers/2404.13320/features/compound-crib/2404.13320.md"
    }
  },
  "2404.16698": {
    "id": "2404.16698",
    "title": "Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society   of LLM Agents",
    "authors": "Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner and 3 others",
    "abstract": "As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage \"Universalization\"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
    "url": "https://arxiv.org/abs/2404.16698",
    "arxivId": "2404.16698",
    "last_visited": "2025-01-05T20:11:03.137000+00:00",
    "last_read": "2025-01-05T20:12:12.476203",
    "total_reading_time_seconds": 24,
    "published_date": "2024-04-25T15:59:16Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2404.16698/features/markdown-grobid/2404.16698.md",
      "adr-crib": "data/papers/2404.16698/features/adr-crib/2404.16698.md",
      "adr-titles": "data/papers/2404.16698/features/adr-titles/2404.16698.md",
      "crib-sheet": "data/papers/2404.16698/features/crib-sheet/2404.16698.md",
      "compound-crib": "data/papers/2404.16698/features/compound-crib/2404.16698.md"
    }
  },
  "2405.04434": {
    "id": "2405.04434",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts   Language Model",
    "authors": "DeepSeek-AI, Aixin Liu, Bei Feng and 154 others",
    "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
    "url": "https://arxiv.org/abs/2405.04434",
    "arxivId": "2405.04434",
    "last_visited": "2024-12-26T13:01:17.331Z",
    "last_read": "2025-01-04T15:03:12.861833",
    "total_reading_time_seconds": 46,
    "published_date": "2024-05-07T15:56:43Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.04434/features/markdown-grobid/2405.04434.md",
      "adr-crib": "data/papers/2405.04434/features/adr-crib/2405.04434.md",
      "adr-titles": "data/papers/2405.04434/features/adr-titles/2405.04434.md",
      "crib-sheet": "data/papers/2405.04434/features/crib-sheet/2405.04434.md",
      "compound-crib": "data/papers/2405.04434/features/compound-crib/2405.04434.md"
    }
  },
  "2405.04517": {
    "id": "2405.04517",
    "title": "xLSTM: Extended Long Short-Term Memory",
    "authors": "Maximilian Beck, Korbinian Pöppel, Markus Spanring and 6 others",
    "abstract": "In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.",
    "url": "https://arxiv.org/abs/2405.04517",
    "arxivId": "2405.04517",
    "last_visited": "2024-12-30T20:10:41.007000+00:00",
    "last_read": "2025-01-04T06:53:15.667071",
    "total_reading_time_seconds": 31,
    "published_date": "2024-05-07T17:50:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.04517/features/markdown-grobid/2405.04517.md",
      "adr-crib": "data/papers/2405.04517/features/adr-crib/2405.04517.md",
      "adr-titles": "data/papers/2405.04517/features/adr-titles/2405.04517.md",
      "crib-sheet": "data/papers/2405.04517/features/crib-sheet/2405.04517.md",
      "compound-crib": "data/papers/2405.04517/features/compound-crib/2405.04517.md"
    }
  },
  "2405.06865": {
    "id": "2405.06865",
    "title": "Disrupting Style Mimicry Attacks on Video Imagery",
    "authors": "Josephine Passananti, Stanley Wu, Shawn Shan and 2 others",
    "abstract": "Generative AI models are often used to perform mimicry attacks, where a pretrained model is fine-tuned on a small sample of images to learn to mimic a specific artist of interest. While researchers have introduced multiple anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence points to a growing trend of mimicry models using videos as sources of training data. This paper presents our experiences exploring techniques to disrupt style mimicry on video imagery. We first validate that mimicry attacks can succeed by training on individual frames extracted from videos. We show that while anti-mimicry tools can offer protection when applied to individual frames, this approach is vulnerable to an adaptive countermeasure that removes protection by exploiting randomness in optimization results of consecutive (nearly-identical) frames. We develop a new, tool-agnostic framework that segments videos into short scenes based on frame-level similarity, and use a per-scene optimization baseline to remove inter-frame randomization while reducing computational cost. We show via both image level metrics and an end-to-end user study that the resulting protection restores protection against mimicry (including the countermeasure). Finally, we develop another adaptive countermeasure and find that it falls short against our framework.",
    "url": "https://arxiv.org/abs/2405.06865",
    "arxivId": "2405.06865",
    "last_visited": "2024-12-22T16:37:12.908Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-11T01:40:19Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.06865/features/markdown-grobid/2405.06865.md",
      "adr-crib": "data/papers/2405.06865/features/adr-crib/2405.06865.md",
      "adr-titles": "data/papers/2405.06865/features/adr-titles/2405.06865.md",
      "crib-sheet": "data/papers/2405.06865/features/crib-sheet/2405.06865.md",
      "compound-crib": "data/papers/2405.06865/features/compound-crib/2405.06865.md"
    }
  },
  "2405.07883": {
    "id": "2405.07883",
    "title": "Zero-Shot Tokenizer Transfer",
    "authors": "Benjamin Minixhofer, Edoardo Maria Ponti, Ivan Vulić",
    "abstract": "Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.",
    "url": "https://arxiv.org/abs/2405.07883",
    "arxivId": "2405.07883",
    "last_visited": "2025-01-10T02:01:14.129000+00:00",
    "last_read": "2025-01-10T02:01:59.116135",
    "total_reading_time_seconds": 15,
    "published_date": "2024-05-13T16:17:10Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.07883/features/markdown-grobid/2405.07883.md",
      "adr-crib": "data/papers/2405.07883/features/adr-crib/2405.07883.md",
      "adr-titles": "data/papers/2405.07883/features/adr-titles/2405.07883.md",
      "crib-sheet": "data/papers/2405.07883/features/crib-sheet/2405.07883.md",
      "compound-crib": "data/papers/2405.07883/features/compound-crib/2405.07883.md"
    }
  },
  "2405.12399": {
    "id": "2405.12399",
    "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
    "authors": "Eloi Alonso, Adam Jelley, Vincent Micheli and 4 others",
    "abstract": "World models constitute a promising approach for training reinforcement learning agents in a safe and sample-efficient manner. Recent world models predominantly operate on sequences of discrete latent variables to model environment dynamics. However, this compression into a compact discrete representation may ignore visual details that are important for reinforcement learning. Concurrently, diffusion models have become a dominant approach for image generation, challenging well-established methods modeling discrete latents. Motivated by this paradigm shift, we introduce DIAMOND (DIffusion As a Model Of eNvironment Dreams), a reinforcement learning agent trained in a diffusion world model. We analyze the key design choices that are required to make diffusion suitable for world modeling, and demonstrate how improved visual details can lead to improved agent performance. DIAMOND achieves a mean human normalized score of 1.46 on the competitive Atari 100k benchmark; a new best for agents trained entirely within a world model. We further demonstrate that DIAMOND's diffusion world model can stand alone as an interactive neural game engine by training on static Counter-Strike: Global Offensive gameplay. To foster future research on diffusion for world modeling, we release our code, agents, videos and playable world models at https://diamond-wm.github.io.",
    "url": "https://arxiv.org/abs/2405.12399",
    "arxivId": "2405.12399",
    "last_visited": "2024-12-26T22:06:50.951Z",
    "last_read": "2025-01-04T15:03:00.857912",
    "total_reading_time_seconds": 12,
    "published_date": "2024-05-20T22:51:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.12399/features/markdown-grobid/2405.12399.md",
      "adr-crib": "data/papers/2405.12399/features/adr-crib/2405.12399.md",
      "adr-titles": "data/papers/2405.12399/features/adr-titles/2405.12399.md",
      "crib-sheet": "data/papers/2405.12399/features/crib-sheet/2405.12399.md",
      "compound-crib": "data/papers/2405.12399/features/compound-crib/2405.12399.md"
    }
  },
  "2405.16567": {
    "id": "2405.16567",
    "title": "Automatic Jailbreaking of the Text-to-Image Generative AI Systems",
    "authors": "Minseon Kim, Hyomin Lee, Boqing Gong and 2 others",
    "abstract": "Recent AI systems have shown extremely powerful performance, even surpassing human performance, on various tasks such as information retrieval, language generation, and image generation based on large language models (LLMs). At the same time, there are diverse safety risks that can cause the generation of malicious contents by circumventing the alignment in LLMs, which are often referred to as jailbreaking. However, most of the previous works only focused on the text-based jailbreaking in LLMs, and the jailbreaking of the text-to-image (T2I) generation system has been relatively overlooked. In this paper, we first evaluate the safety of the commercial T2I generation systems, such as ChatGPT, Copilot, and Gemini, on copyright infringement with naive prompts. From this empirical study, we find that Copilot and Gemini block only 12% and 17% of the attacks with naive prompts, respectively, while ChatGPT blocks 84% of them. Then, we further propose a stronger automated jailbreaking pipeline for T2I generation systems, which produces prompts that bypass their safety guards. Our automated jailbreaking framework leverages an LLM optimizer to generate prompts to maximize degree of violation from the generated images without any weight updates or gradient computation. Surprisingly, our simple yet effective approach successfully jailbreaks the ChatGPT with 11.0% block rate, making it generate copyrighted contents in 76% of the time. Finally, we explore various defense strategies, such as post-generation filtering and machine unlearning techniques, but found that they were inadequate, which suggests the necessity of stronger defense mechanisms.",
    "url": "https://arxiv.org/abs/2405.16567",
    "arxivId": "2405.16567",
    "last_visited": "2024-12-22T16:31:57.733Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-26T13:32:24Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.16567/features/markdown-grobid/2405.16567.md",
      "adr-crib": "data/papers/2405.16567/features/adr-crib/2405.16567.md",
      "adr-titles": "data/papers/2405.16567/features/adr-titles/2405.16567.md",
      "crib-sheet": "data/papers/2405.16567/features/crib-sheet/2405.16567.md",
      "compound-crib": "data/papers/2405.16567/features/compound-crib/2405.16567.md"
    }
  },
  "2405.17472": {
    "id": "2405.17472",
    "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via   Selective Tensor Freezing",
    "authors": "Kai Huang, Haoming Wang, Wei Gao",
    "abstract": "Text-to-image diffusion models can be fine-tuned in custom domains to adapt to specific user preferences, but such adaptability has also been utilized for illegal purposes, such as forging public figures' portraits, duplicating copyrighted artworks and generating explicit contents. Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data. In this paper, we present FreezeAsGuard, a new technique that addresses these limitations and enables irreversible mitigation of illegal adaptations of diffusion models. Our approach is that the model publisher selectively freezes tensors in pre-trained diffusion models that are critical to illegal model adaptations, to mitigate the fine-tuned model's representation power in illegal adaptations, but minimize the impact on other legal adaptations. Experiment results in multiple text-to-image application domains show that FreezeAsGuard provides 37% stronger power in mitigating illegal model adaptations compared to competitive baselines, while incurring less than 5% impact on legal model adaptations. The source code is available at: https://github.com/pittisl/FreezeAsGuard.",
    "url": "https://arxiv.org/abs/2405.17472",
    "arxivId": "2405.17472",
    "last_visited": "2024-12-22T18:25:59.099Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-24T03:23:51Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.17472/features/markdown-grobid/2405.17472.md",
      "adr-crib": "data/papers/2405.17472/features/adr-crib/2405.17472.md",
      "adr-titles": "data/papers/2405.17472/features/adr-titles/2405.17472.md",
      "crib-sheet": "data/papers/2405.17472/features/crib-sheet/2405.17472.md",
      "compound-crib": "data/papers/2405.17472/features/compound-crib/2405.17472.md"
    }
  },
  "2405.20053": {
    "id": "2405.20053",
    "title": "Would I Lie To You? Inference Time Alignment of Language Models using   Direct Preference Heads",
    "authors": "Avelina Asada Hadji-Kyriacou, Ognjen Arandjelovic",
    "abstract": "Pre-trained Language Models (LMs) exhibit strong zero-shot and in-context learning capabilities; however, their behaviors are often difficult to control. By utilizing Reinforcement Learning from Human Feedback (RLHF), it is possible to fine-tune unsupervised LMs to follow instructions and produce outputs that reflect human preferences. Despite its benefits, RLHF has been shown to potentially harm a language model's reasoning capabilities and introduce artifacts such as hallucinations where the model may fabricate facts. To address this issue we introduce Direct Preference Heads (DPH), a fine-tuning framework that enables LMs to learn human preference signals through an auxiliary reward head without directly affecting the output distribution of the language modeling head. We perform a theoretical analysis of our objective function and find strong ties to Conservative Direct Preference Optimization (cDPO). Finally we evaluate our models on GLUE, RACE, and the GPT4All evaluation suite and demonstrate that our method produces models which achieve higher scores than those fine-tuned with Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO) alone.",
    "url": "https://arxiv.org/abs/2405.20053",
    "arxivId": "2405.20053",
    "last_visited": "2024-12-15T17:25:17.781Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-05-30T13:38:52Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.20053/features/markdown-grobid/2405.20053.md",
      "adr-crib": "data/papers/2405.20053/features/adr-crib/2405.20053.md",
      "adr-titles": "data/papers/2405.20053/features/adr-titles/2405.20053.md",
      "crib-sheet": "data/papers/2405.20053/features/crib-sheet/2405.20053.md",
      "compound-crib": "data/papers/2405.20053/features/compound-crib/2405.20053.md"
    }
  },
  "2405.21060": {
    "id": "2405.21060",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms   Through Structured State Space Duality",
    "authors": "Tri Dao, Albert Gu",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "url": "https://arxiv.org/abs/2405.21060",
    "arxivId": "2405.21060",
    "last_visited": "2025-02-15T22:41:28.414Z",
    "last_read": "2025-02-15T22:41:28.414Z",
    "total_reading_time_seconds": 5,
    "published_date": "2024-05-31T17:50:01Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.21060/features/markdown-grobid/2405.21060.md",
      "adr-crib": "data/papers/2405.21060/features/adr-crib/2405.21060.md",
      "adr-titles": "data/papers/2405.21060/features/adr-titles/2405.21060.md",
      "crib-sheet": "data/papers/2405.21060/features/crib-sheet/2405.21060.md",
      "compound-crib": "data/papers/2405.21060/features/compound-crib/2405.21060.md"
    }
  },
  "2406.01981": {
    "id": "2406.01981",
    "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
    "authors": "Yury Tokpanov, Beren Millidge, Paolo Glorioso and 4 others",
    "abstract": "The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.",
    "url": "https://arxiv.org/abs/2406.01981",
    "arxivId": "2406.01981",
    "last_visited": "2024-12-30T20:04:46.858Z",
    "last_read": "2025-01-04T06:53:18.650339",
    "total_reading_time_seconds": 8,
    "published_date": "2024-06-04T05:47:17Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.01981/features/markdown-grobid/2406.01981.md",
      "adr-crib": "data/papers/2406.01981/features/adr-crib/2406.01981.md",
      "adr-titles": "data/papers/2406.01981/features/adr-titles/2406.01981.md",
      "crib-sheet": "data/papers/2406.01981/features/crib-sheet/2406.01981.md",
      "compound-crib": "data/papers/2406.01981/features/compound-crib/2406.01981.md"
    }
  },
  "2406.06158": {
    "id": "2406.06158",
    "title": "Get rich quick: exact solutions reveal how unbalanced initializations   promote rapid feature learning",
    "authors": "Daniel Kunin, Allan Raventós, Clémentine Dominé and 4 others",
    "abstract": "While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.",
    "url": "https://arxiv.org/abs/2406.06158",
    "arxivId": "2406.06158",
    "last_visited": "2024-12-22T07:10:41.441Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-10T10:42:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.06158/features/markdown-grobid/2406.06158.md",
      "adr-crib": "data/papers/2406.06158/features/adr-crib/2406.06158.md",
      "adr-titles": "data/papers/2406.06158/features/adr-titles/2406.06158.md",
      "crib-sheet": "data/papers/2406.06158/features/crib-sheet/2406.06158.md",
      "compound-crib": "data/papers/2406.06158/features/compound-crib/2406.06158.md"
    }
  },
  "2406.07522": {
    "id": "2406.07522",
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context   Language Modeling",
    "authors": "Liliang Ren, Yang Liu, Yadong Lu and 3 others",
    "abstract": "Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall recent memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and demonstrate that it significantly outperforms state-of-the-art models across a variety of benchmarks. Pretrained on sequences of 4K length, Samba shows improved perplexity in context lengths of up to 1M in zero-shot. When finetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits superior retrieval extrapolation on the challenging Phonebook task compared to full-attention models. As a linear-time sequence model, Samba achieves a 3.73x higher throughput compared to Transformers with grouped-query attention for user prompts of 128K length, and a 3.64x speedup when generating 64K tokens with unlimited streaming. Our code for training on open source data is publicly available at https://github.com/microsoft/Samba.",
    "url": "https://arxiv.org/abs/2406.07522",
    "arxivId": "2406.07522",
    "last_visited": "2025-01-19T03:47:40.608Z",
    "last_read": "2025-01-19T03:47:40.608Z",
    "total_reading_time_seconds": 8,
    "published_date": "2024-06-11T17:50:51Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.07522/features/markdown-grobid/2406.07522.md",
      "adr-crib": "data/papers/2406.07522/features/adr-crib/2406.07522.md",
      "adr-titles": "data/papers/2406.07522/features/adr-titles/2406.07522.md",
      "crib-sheet": "data/papers/2406.07522/features/crib-sheet/2406.07522.md",
      "compound-crib": "data/papers/2406.07522/features/compound-crib/2406.07522.md"
    }
  },
  "2406.09162": {
    "id": "2406.09162",
    "title": "EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal   Prompts",
    "authors": "Yucheng Han, Rui Wang, Chi Zhang and 4 others",
    "abstract": "Recent advancements in image generation have enabled the creation of high-quality images from text conditions. However, when facing multi-modal conditions, such as text combined with reference appearances, existing methods struggle to balance multiple conditions effectively, typically showing a preference for one modality over others. To address this challenge, we introduce EMMA, a novel image generation model accepting multi-modal prompts built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA seamlessly incorporates additional modalities alongside text to guide image generation through an innovative Multi-modal Feature Connector design, which effectively integrates textual and supplementary modal information using a special attention mechanism. By freezing all parameters in the original T2I diffusion model and only adjusting some additional layers, we reveal an interesting finding that the pre-trained T2I diffusion model can secretly accept multi-modal prompts. This interesting property facilitates easy adaptation to different existing frameworks, making EMMA a flexible and effective tool for producing personalized and context-aware images and even videos. Additionally, we introduce a strategy to assemble learned EMMA modules to produce images conditioned on multiple modalities simultaneously, eliminating the need for additional training with mixed multi-modal prompts. Extensive experiments demonstrate the effectiveness of EMMA in maintaining high fidelity and detail in generated images, showcasing its potential as a robust solution for advanced multi-modal conditional image generation tasks.",
    "url": "https://arxiv.org/abs/2406.09162",
    "arxivId": "2406.09162",
    "last_visited": "2024-12-28T08:40:07.461Z",
    "last_read": "2025-01-04T14:49:45.266317",
    "total_reading_time_seconds": 24,
    "published_date": "2024-06-13T14:26:43Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.09162/features/markdown-grobid/2406.09162.md",
      "adr-crib": "data/papers/2406.09162/features/adr-crib/2406.09162.md",
      "adr-titles": "data/papers/2406.09162/features/adr-titles/2406.09162.md",
      "crib-sheet": "data/papers/2406.09162/features/crib-sheet/2406.09162.md",
      "compound-crib": "data/papers/2406.09162/features/compound-crib/2406.09162.md"
    }
  },
  "2406.10670": {
    "id": "2406.10670",
    "title": "CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language   Model Pre-training",
    "authors": "David Brandfonbrener, Hanlin Zhang, Andreas Kirsch and 2 others",
    "abstract": "Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.   In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.   Code: https://github.com/davidbrandfonbrener/color-filter-olmo   Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4",
    "url": "https://arxiv.org/abs/2406.10670",
    "arxivId": "2406.10670",
    "last_visited": "2024-12-30T20:02:17.082Z",
    "last_read": "2025-01-04T06:53:27.615183",
    "total_reading_time_seconds": 7,
    "published_date": "2024-06-15T15:28:02Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.10670/features/markdown-grobid/2406.10670.md",
      "adr-crib": "data/papers/2406.10670/features/adr-crib/2406.10670.md",
      "adr-titles": "data/papers/2406.10670/features/adr-titles/2406.10670.md",
      "crib-sheet": "data/papers/2406.10670/features/crib-sheet/2406.10670.md",
      "compound-crib": "data/papers/2406.10670/features/compound-crib/2406.10670.md"
    }
  },
  "2406.12027": {
    "id": "2406.12027",
    "title": "Adversarial Perturbations Cannot Reliably Protect Artists From   Generative AI",
    "authors": "Robert Hönig, Javier Rando, Nicholas Carlini, Florian Tramèr",
    "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles. In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections -- with millions of downloads -- and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that all existing protections can be easily bypassed, leaving artists vulnerable to style mimicry. We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative non-technological solutions.",
    "url": "https://arxiv.org/abs/2406.12027",
    "arxivId": "2406.12027",
    "last_visited": "2024-12-22T16:38:53.342Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-06-17T18:51:45Z",
    "arxiv_tags": [
      "cs.CR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.12027/features/markdown-grobid/2406.12027.md",
      "adr-crib": "data/papers/2406.12027/features/adr-crib/2406.12027.md",
      "adr-titles": "data/papers/2406.12027/features/adr-titles/2406.12027.md",
      "crib-sheet": "data/papers/2406.12027/features/crib-sheet/2406.12027.md",
      "compound-crib": "data/papers/2406.12027/features/compound-crib/2406.12027.md"
    }
  },
  "2407.01392": {
    "id": "2407.01392",
    "title": "Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion",
    "authors": "Boyuan Chen, Diego Marti Monso, Yilun Du and 3 others",
    "abstract": "This paper presents Diffusion Forcing, a new training paradigm where a diffusion model is trained to denoise a set of tokens with independent per-token noise levels. We apply Diffusion Forcing to sequence generative modeling by training a causal next-token prediction model to generate one or several future tokens without fully diffusing past ones. Our approach is shown to combine the strengths of next-token prediction models, such as variable-length generation, with the strengths of full-sequence diffusion models, such as the ability to guide sampling to desirable trajectories. Our method offers a range of additional capabilities, such as (1) rolling-out sequences of continuous tokens, such as video, with lengths past the training horizon, where baselines diverge and (2) new sampling and guiding schemes that uniquely profit from Diffusion Forcing's variable-horizon and causal architecture, and which lead to marked performance gains in decision-making and planning tasks. In addition to its empirical success, our method is proven to optimize a variational lower bound on the likelihoods of all subsequences of tokens drawn from the true joint distribution. Project website: https://boyuan.space/diffusion-forcing",
    "url": "https://arxiv.org/abs/2407.01392",
    "arxivId": "2407.01392",
    "last_visited": "2024-12-26T22:05:31.374Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-01T15:43:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.01392/features/markdown-grobid/2407.01392.md",
      "adr-crib": "data/papers/2407.01392/features/adr-crib/2407.01392.md",
      "adr-titles": "data/papers/2407.01392/features/adr-titles/2407.01392.md",
      "crib-sheet": "data/papers/2407.01392/features/crib-sheet/2407.01392.md",
      "compound-crib": "data/papers/2407.01392/features/compound-crib/2407.01392.md"
    }
  },
  "2407.01492": {
    "id": "2407.01492",
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "authors": "Qian Liu, Xiaosen Zheng, Niklas Muennighoff and 5 others",
    "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix involves training a set of small models with diverse data mixtures and fitting a regression model to predict their performance given their respective mixtures. With the fitted regression model, we simulate the top-ranked mixture and use it to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens of different mixtures to fit the regression model and find the optimal mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000x larger and 25x longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Further, our method demonstrates superior performance compared to human selection and achieves results that match or surpass DoReMi, while utilizing only 10% of the compute budget. Our experiments also show that (1) Data mixtures significantly impact performance with single-task performance variations of up to 14.6%; (2) Web corpora rather than data perceived as high-quality like Wikipedia have the strongest positive correlation with downstream performance; (3) Domains interact in complex ways often contradicting common sense, thus automatic approaches like RegMix are needed; (4) Data mixture effects transcend scaling laws, and our approach captures the complexity by considering all domains together. Our code is available at https://github.com/sail-sg/regmix.",
    "url": "https://arxiv.org/abs/2407.01492",
    "arxivId": "2407.01492",
    "last_visited": "2024-12-30T20:17:16.629000+00:00",
    "last_read": "2025-01-04T06:53:09.611828",
    "total_reading_time_seconds": 42,
    "published_date": "2024-07-01T17:31:03Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.01492/features/markdown-grobid/2407.01492.md",
      "adr-crib": "data/papers/2407.01492/features/adr-crib/2407.01492.md",
      "adr-titles": "data/papers/2407.01492/features/adr-titles/2407.01492.md",
      "crib-sheet": "data/papers/2407.01492/features/crib-sheet/2407.01492.md",
      "compound-crib": "data/papers/2407.01492/features/compound-crib/2407.01492.md"
    }
  },
  "2407.05872": {
    "id": "2407.05872",
    "title": "Scaling Exponents Across Parameterizations and Optimizers",
    "authors": "Katie Everett, Lechao Xiao, Mitchell Wortsman and 8 others",
    "abstract": "Robust and effective scaling of models from small to large width typically requires the precise adjustment of many algorithmic and architectural details, such as parameterization and optimizer choices. In this work, we propose a new perspective on parameterization by investigating a key assumption in prior work about the alignment between parameters and data and derive new theoretical results under weaker assumptions and a broader set of optimizers. Our extensive empirical investigation includes tens of thousands of models trained with all combinations of three optimizers, four parameterizations, several alignment assumptions, more than a dozen learning rates, and fourteen model sizes up to 26.8B parameters. We find that the best learning rate scaling prescription would often have been excluded by the assumptions in prior work. Our results show that all parameterizations, not just maximal update parameterization (muP), can achieve hyperparameter transfer; moreover, our novel per-layer learning rate prescription for standard parameterization outperforms muP. Finally, we demonstrate that an overlooked aspect of parameterization, the epsilon parameter in Adam, must be scaled correctly to avoid gradient underflow and propose Adam-atan2, a new numerically stable, scale-invariant version of Adam that eliminates the epsilon hyperparameter entirely.",
    "url": "https://arxiv.org/abs/2407.05872",
    "arxivId": "2407.05872",
    "last_visited": "2025-01-20T19:44:42.999Z",
    "last_read": "2025-01-20T19:44:42.999Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-08T12:32:51Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.05872/features/markdown-grobid/2407.05872.md",
      "adr-crib": "data/papers/2407.05872/features/adr-crib/2407.05872.md",
      "adr-titles": "data/papers/2407.05872/features/adr-titles/2407.05872.md",
      "crib-sheet": "data/papers/2407.05872/features/crib-sheet/2407.05872.md",
      "compound-crib": "data/papers/2407.05872/features/compound-crib/2407.05872.md"
    }
  },
  "2407.08608": {
    "id": "2407.08608",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision",
    "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang and 3 others",
    "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention.",
    "url": "https://arxiv.org/abs/2407.08608",
    "arxivId": "2407.08608",
    "last_visited": "2024-12-28T06:10:45.698Z",
    "last_read": "2025-01-04T15:02:45.957021",
    "total_reading_time_seconds": 39,
    "published_date": "2024-07-11T15:44:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.08608/features/markdown-grobid/2407.08608.md",
      "adr-crib": "data/papers/2407.08608/features/adr-crib/2407.08608.md",
      "adr-titles": "data/papers/2407.08608/features/adr-titles/2407.08608.md",
      "crib-sheet": "data/papers/2407.08608/features/crib-sheet/2407.08608.md",
      "compound-crib": "data/papers/2407.08608/features/compound-crib/2407.08608.md"
    }
  },
  "2407.21783": {
    "id": "2407.21783",
    "title": "The Llama 3 Herd of Models",
    "authors": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri and 558 others",
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
    "url": "https://arxiv.org/abs/2407.21783",
    "arxivId": "2407.21783",
    "last_visited": "2024-12-30T21:58:38.440Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-31T17:54:27Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.21783/features/markdown-grobid/2407.21783.md",
      "adr-crib": "data/papers/2407.21783/features/adr-crib/2407.21783.md",
      "adr-titles": "data/papers/2407.21783/features/adr-titles/2407.21783.md",
      "crib-sheet": "data/papers/2407.21783/features/crib-sheet/2407.21783.md",
      "compound-crib": "data/papers/2407.21783/features/compound-crib/2407.21783.md"
    }
  },
  "2407.21787": {
    "id": "2407.21787",
    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
    "authors": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich and 4 others",
    "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
    "url": "https://arxiv.org/abs/2407.21787",
    "arxivId": "2407.21787",
    "last_visited": "2024-12-29T20:31:06.783Z",
    "last_read": "2025-01-04T14:49:12.237648",
    "total_reading_time_seconds": 17,
    "published_date": "2024-07-31T17:57:25Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.21787/features/markdown-grobid/2407.21787.md",
      "adr-crib": "data/papers/2407.21787/features/adr-crib/2407.21787.md",
      "adr-titles": "data/papers/2407.21787/features/adr-titles/2407.21787.md",
      "crib-sheet": "data/papers/2407.21787/features/crib-sheet/2407.21787.md",
      "compound-crib": "data/papers/2407.21787/features/compound-crib/2407.21787.md"
    }
  },
  "2408.03314": {
    "id": "2408.03314",
    "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than   Scaling Model Parameters",
    "authors": "Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
    "url": "https://arxiv.org/abs/2408.03314",
    "arxivId": "2408.03314",
    "last_visited": "2024-12-17T13:48:02.489Z",
    "last_read": "2025-01-04T14:49:12.236101",
    "total_reading_time_seconds": 39,
    "published_date": "2024-08-06T17:35:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2408.03314/features/markdown-grobid/2408.03314.md",
      "adr-crib": "data/papers/2408.03314/features/adr-crib/2408.03314.md",
      "adr-titles": "data/papers/2408.03314/features/adr-titles/2408.03314.md",
      "crib-sheet": "data/papers/2408.03314/features/crib-sheet/2408.03314.md",
      "compound-crib": "data/papers/2408.03314/features/compound-crib/2408.03314.md"
    }
  },
  "2408.06072": {
    "id": "2408.06072",
    "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
    "authors": "Zhuoyi Yang, Jiayan Teng, Wendi Zheng and 16 others",
    "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at https://github.com/THUDM/CogVideo.",
    "url": "https://arxiv.org/abs/2408.06072",
    "arxivId": "2408.06072",
    "last_visited": "2024-12-30T15:18:13.864Z",
    "last_read": "2025-01-04T06:53:30.624413",
    "total_reading_time_seconds": 26,
    "published_date": "2024-08-12T11:47:11Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2408.06072/features/markdown-grobid/2408.06072.md",
      "adr-crib": "data/papers/2408.06072/features/adr-crib/2408.06072.md",
      "adr-titles": "data/papers/2408.06072/features/adr-titles/2408.06072.md",
      "crib-sheet": "data/papers/2408.06072/features/crib-sheet/2408.06072.md",
      "compound-crib": "data/papers/2408.06072/features/compound-crib/2408.06072.md"
    }
  },
  "2408.11810": {
    "id": "2408.11810",
    "title": "Pixel Is Not A Barrier: An Effective Evasion Attack for Pixel-Domain   Diffusion Models",
    "authors": "Chun-Yen Shih, Li-Xuan Peng, Jia-Wei Liao and 3 others",
    "abstract": "Diffusion Models have emerged as powerful generative models for high-quality image synthesis, with many subsequent image editing techniques based on them. However, the ease of text-based image editing introduces significant risks, such as malicious editing for scams or intellectual property infringement. Previous works have attempted to safeguard images from diffusion-based editing by adding imperceptible perturbations. These methods are costly and specifically target prevalent Latent Diffusion Models (LDMs), while Pixel-domain Diffusion Models (PDMs) remain largely unexplored and robust against such attacks. Our work addresses this gap by proposing a novel attacking framework with a feature representation attack loss that exploits vulnerabilities in denoising UNets and a latent optimization strategy to enhance the naturalness of protected images. Extensive experiments demonstrate the effectiveness of our approach in attacking dominant PDM-based editing methods (e.g., SDEdit) while maintaining reasonable protection fidelity and robustness against common defense methods. Additionally, our framework is extensible to LDMs, achieving comparable performance to existing approaches.",
    "url": "https://arxiv.org/abs/2408.11810",
    "arxivId": "2408.11810",
    "last_visited": "2024-12-22T16:45:08.960Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-08-21T17:56:34Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2408.11810/features/markdown-grobid/2408.11810.md",
      "adr-crib": "data/papers/2408.11810/features/adr-crib/2408.11810.md",
      "adr-titles": "data/papers/2408.11810/features/adr-titles/2408.11810.md",
      "crib-sheet": "data/papers/2408.11810/features/crib-sheet/2408.11810.md",
      "compound-crib": "data/papers/2408.11810/features/compound-crib/2408.11810.md"
    }
  },
  "2408.14837": {
    "id": "2408.14837",
    "title": "Diffusion Models Are Real-Time Game Engines",
    "authors": "Dani Valevski, Yaniv Leviathan, Moab Arar, Shlomi Fruchter",
    "abstract": "We present GameNGen, the first game engine powered entirely by a neural model that enables real-time interaction with a complex environment over long trajectories at high quality. GameNGen can interactively simulate the classic game DOOM at over 20 frames per second on a single TPU. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations enable stable auto-regressive generation over long trajectories.",
    "url": "https://arxiv.org/abs/2408.14837",
    "arxivId": "2408.14837",
    "last_visited": "2024-12-26T21:43:51.081Z",
    "last_read": "2025-01-04T15:03:06.880492",
    "total_reading_time_seconds": 14,
    "published_date": "2024-08-27T07:46:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2408.14837/features/markdown-grobid/2408.14837.md",
      "adr-crib": "data/papers/2408.14837/features/adr-crib/2408.14837.md",
      "adr-titles": "data/papers/2408.14837/features/adr-titles/2408.14837.md",
      "crib-sheet": "data/papers/2408.14837/features/crib-sheet/2408.14837.md",
      "compound-crib": "data/papers/2408.14837/features/compound-crib/2408.14837.md"
    }
  },
  "2409.04431": {
    "id": "2409.04431",
    "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
    "authors": "Jason Ramapuram, Federico Danieli, Eeshan Dhekane and 8 others",
    "abstract": "Attention is a key part of the transformer architecture. It is a sequence-to-sequence mapping that transforms each sequence element into a weighted sum of values. The weights are typically obtained as the softmax of dot products between keys and queries. Recent work has explored alternatives to softmax attention in transformers, such as ReLU and sigmoid activations. In this work, we revisit sigmoid attention and conduct an in-depth theoretical and empirical analysis. Theoretically, we prove that transformers with sigmoid attention are universal function approximators and benefit from improved regularity compared to softmax attention. Through detailed empirical analysis, we identify stabilization of large initial attention norms during the early stages of training as a crucial factor for the successful training of models with sigmoid attention, outperforming prior attempts. We also introduce FLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid attention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100 GPUs. Experiments across language, vision, and speech show that properly normalized sigmoid attention matches the strong performance of softmax attention on a wide range of domains and scales, which previous attempts at sigmoid attention were unable to fully achieve. Our work unifies prior art and establishes best practices for sigmoid attention as a drop-in softmax replacement in transformers.",
    "url": "https://arxiv.org/abs/2409.04431",
    "arxivId": "2409.04431",
    "last_visited": "2025-01-02T19:38:28.512000+00:00",
    "last_read": "2025-01-04T06:52:15.620261",
    "total_reading_time_seconds": 8,
    "published_date": "2024-09-06T17:53:26Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.04431/features/markdown-grobid/2409.04431.md",
      "adr-crib": "data/papers/2409.04431/features/adr-crib/2409.04431.md",
      "adr-titles": "data/papers/2409.04431/features/adr-titles/2409.04431.md",
      "crib-sheet": "data/papers/2409.04431/features/crib-sheet/2409.04431.md",
      "compound-crib": "data/papers/2409.04431/features/compound-crib/2409.04431.md"
    }
  },
  "2409.04580": {
    "id": "2409.04580",
    "title": "GRB 221009A: the B.O.A.T Burst that Shines in Gamma Rays",
    "authors": "M. Axelsson, M. Ajello, M. Arimoto and 151 others",
    "abstract": "We present a complete analysis of Fermi Large Area Telescope (LAT) data of GRB 221009A, the brightest Gamma-Ray Burst (GRB) ever detected. The burst emission above 30 MeV detected by the LAT preceded by 1 s the low-energy (< 10 MeV) pulse that triggered the Fermi Gamma-Ray Burst Monitor (GBM), as has been observed in other GRBs. The prompt phase of GRB 221009A lasted a few hundred seconds. It was so bright that we identify a Bad Time Interval (BTI) of 64 seconds caused by the extremely high flux of hard X-rays and soft gamma rays, during which the event reconstruction efficiency was poor and the dead time fraction quite high. The late-time emission decayed as a power law, but the extrapolation of the late-time emission during the first 450 seconds suggests that the afterglow started during the prompt emission. We also found that high-energy events observed by the LAT are incompatible with synchrotron origin, and, during the prompt emission, are more likely related to an extra component identified as synchrotron self-Compton (SSC). A remarkable 400 GeV photon, detected by the LAT 33 ks after the GBM trigger and directionally consistent with the location of GRB 221009A, is hard to explain as a product of SSC or TeV electromagnetic cascades, and the process responsible for its origin is uncertain. Because of its proximity and energetic nature, GRB 221009A is an extremely rare event.",
    "url": "https://arxiv.org/abs/2409.04580",
    "arxivId": "2409.04580",
    "last_visited": "2025-01-13T06:26:26.740000+00:00",
    "last_read": "2025-01-13T06:27:58.445781",
    "total_reading_time_seconds": 25,
    "published_date": "2024-09-06T19:42:28Z",
    "arxiv_tags": [
      "astro-ph.HE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.04580/features/markdown-grobid/2409.04580.md",
      "adr-crib": "data/papers/2409.04580/features/adr-crib/2409.04580.md",
      "adr-titles": "data/papers/2409.04580/features/adr-titles/2409.04580.md",
      "crib-sheet": "data/papers/2409.04580/features/crib-sheet/2409.04580.md",
      "compound-crib": "data/papers/2409.04580/features/compound-crib/2409.04580.md"
    }
  },
  "2409.05816": {
    "id": "2409.05816",
    "title": "Improving Pretraining Data Using Perplexity Correlations",
    "authors": "Tristan Thrush, Christopher Potts, Tatsunori Hashimoto",
    "abstract": "Quality pretraining data is often seen as the key to high-performance language models. However, progress in understanding pretraining data has been slow due to the costly pretraining runs required for data selection experiments. We present a framework that avoids these costs and selects high-quality pretraining data without any LLM training of our own. Our work is based on a simple observation: LLM losses on many pretraining texts are correlated with downstream benchmark performance, and selecting high-correlation documents is an effective pretraining data selection method. We build a new statistical framework for data selection centered around estimates of perplexity-benchmark correlations and perform data selection using a sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of thousands of web domains. In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier.",
    "url": "https://arxiv.org/abs/2409.05816",
    "arxivId": "2409.05816",
    "last_visited": "2024-12-30T20:20:51.220Z",
    "last_read": "2025-01-04T06:53:00.604590",
    "total_reading_time_seconds": 3,
    "published_date": "2024-09-09T17:23:29Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.05816/features/markdown-grobid/2409.05816.md",
      "adr-crib": "data/papers/2409.05816/features/adr-crib/2409.05816.md",
      "adr-titles": "data/papers/2409.05816/features/adr-titles/2409.05816.md",
      "crib-sheet": "data/papers/2409.05816/features/crib-sheet/2409.05816.md",
      "compound-crib": "data/papers/2409.05816/features/compound-crib/2409.05816.md"
    }
  },
  "2409.08514": {
    "id": "2409.08514",
    "title": "Apollo: Band-sequence Modeling for High-Quality Audio Restoration",
    "authors": "Kai Li, Yi Luo",
    "abstract": "Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid- and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving low-frequency information while accurately reconstructing high-quality mid- and high-frequency content. Inspired by recent advancements in high-sample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-sample-rate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at https://github.com/JusperLee/Apollo.",
    "url": "https://arxiv.org/abs/2409.08514",
    "arxivId": "2409.08514",
    "last_visited": "2024-12-30T05:02:38.295Z",
    "last_read": "2025-01-04T14:48:42.234855",
    "total_reading_time_seconds": 4,
    "published_date": "2024-09-13T03:25:34Z",
    "arxiv_tags": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.08514/features/markdown-grobid/2409.08514.md",
      "adr-crib": "data/papers/2409.08514/features/adr-crib/2409.08514.md",
      "adr-titles": "data/papers/2409.08514/features/adr-titles/2409.08514.md",
      "crib-sheet": "data/papers/2409.08514/features/crib-sheet/2409.08514.md",
      "compound-crib": "data/papers/2409.08514/features/compound-crib/2409.08514.md"
    }
  },
  "2409.08861": {
    "id": "2409.08861",
    "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with   Memoryless Stochastic Optimal Control",
    "authors": "Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, Ricky T. Q. Chen",
    "abstract": "Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless noise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity.",
    "url": "https://arxiv.org/abs/2409.08861",
    "arxivId": "2409.08861",
    "last_visited": "2025-01-13T06:50:13.853000+00:00",
    "last_read": "2025-01-13T06:52:55.474406",
    "total_reading_time_seconds": 21,
    "published_date": "2024-09-13T14:22:14Z",
    "arxiv_tags": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.08861/features/markdown-grobid/2409.08861.md",
      "adr-crib": "data/papers/2409.08861/features/adr-crib/2409.08861.md",
      "adr-titles": "data/papers/2409.08861/features/adr-titles/2409.08861.md",
      "crib-sheet": "data/papers/2409.08861/features/crib-sheet/2409.08861.md",
      "compound-crib": "data/papers/2409.08861/features/compound-crib/2409.08861.md"
    }
  },
  "2409.11321": {
    "id": "2409.11321",
    "title": "SOAP: Improving and Stabilizing Shampoo using Adam",
    "authors": "Nikhil Vyas, Depen Morwani, Rosie Zhao and 4 others",
    "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -- a memory-efficient approximation of Adam -- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: $\\textbf{S}$hampo$\\textbf{O}$ with $\\textbf{A}$dam in the $\\textbf{P}$reconditioner's eigenbasis (SOAP).   With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at https://github.com/nikhilvyas/SOAP.",
    "url": "https://arxiv.org/abs/2409.11321",
    "arxivId": "2409.11321",
    "last_visited": "2025-01-05T18:45:46.230Z",
    "last_read": "2025-01-05T18:45:46.713842",
    "total_reading_time_seconds": 9,
    "published_date": "2024-09-17T16:18:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.11321/features/markdown-grobid/2409.11321.md",
      "adr-crib": "data/papers/2409.11321/features/adr-crib/2409.11321.md",
      "adr-titles": "data/papers/2409.11321/features/adr-titles/2409.11321.md",
      "crib-sheet": "data/papers/2409.11321/features/crib-sheet/2409.11321.md",
      "compound-crib": "data/papers/2409.11321/features/compound-crib/2409.11321.md"
    }
  },
  "2409.13731": {
    "id": "2409.13731",
    "title": "KAG: Boosting LLMs in Professional Domains via Knowledge Augmented   Generation",
    "authors": "Lei Liang, Mengshu Sun, Zhengke Gui and 16 others",
    "abstract": "The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.",
    "url": "https://arxiv.org/abs/2409.13731",
    "arxivId": "2409.13731",
    "last_visited": "2025-01-02T15:07:25.587Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-10T02:00:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.13731/features/markdown-grobid/2409.13731.md",
      "adr-crib": "data/papers/2409.13731/features/adr-crib/2409.13731.md",
      "adr-titles": "data/papers/2409.13731/features/adr-titles/2409.13731.md",
      "crib-sheet": "data/papers/2409.13731/features/crib-sheet/2409.13731.md",
      "compound-crib": "data/papers/2409.13731/features/compound-crib/2409.13731.md"
    }
  },
  "2409.16986": {
    "id": "2409.16986",
    "title": "Harnessing Diversity for Important Data Selection in Pretraining Large   Language Models",
    "authors": "Chi Zhang, Huaping Zhong, Kuan Zhang and 10 others",
    "abstract": "Data selection is of great significance in pre-training large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores. However, this approach has several limitations. (1) Computing the influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pre-trained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce \\texttt{Quad}, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pre-training results. In particular, noting that attention layers capture extensive semantic details, we have adapted the accelerated $iHVP$ computation methods for attention layers, enhancing our ability to evaluate the influence of data, $i.e.,$ its quality. For the diversity, \\texttt{Quad} clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. To determine which clusters to select, we utilize the classic Multi-Armed Bandit method, treating each cluster as an arm. This approach favors clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.",
    "url": "https://arxiv.org/abs/2409.16986",
    "arxivId": "2409.16986",
    "last_visited": "2024-12-30T20:04:00.391Z",
    "last_read": "2025-01-04T06:53:18.651949",
    "total_reading_time_seconds": 20,
    "published_date": "2024-09-25T14:49:29Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.16986/features/markdown-grobid/2409.16986.md",
      "adr-crib": "data/papers/2409.16986/features/adr-crib/2409.16986.md",
      "adr-titles": "data/papers/2409.16986/features/adr-titles/2409.16986.md",
      "crib-sheet": "data/papers/2409.16986/features/crib-sheet/2409.16986.md",
      "compound-crib": "data/papers/2409.16986/features/compound-crib/2409.16986.md"
    }
  },
  "2409.19256": {
    "id": "2409.19256",
    "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
    "authors": "Guangming Sheng, Chi Zhang, Zilingfeng Ye and 6 others",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53$\\times$~20.57$\\times$ throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at https://github.com/volcengine/verl.",
    "url": "https://arxiv.org/abs/2409.19256",
    "arxivId": "2409.19256",
    "last_visited": "2025-01-05T08:22:18.012Z",
    "last_read": "2025-01-05T08:23:20.687827",
    "total_reading_time_seconds": 14,
    "published_date": "2024-09-28T06:20:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC",
      "I.2"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.19256/features/markdown-grobid/2409.19256.md",
      "adr-crib": "data/papers/2409.19256/features/adr-crib/2409.19256.md",
      "adr-titles": "data/papers/2409.19256/features/adr-titles/2409.19256.md",
      "crib-sheet": "data/papers/2409.19256/features/crib-sheet/2409.19256.md",
      "compound-crib": "data/papers/2409.19256/features/compound-crib/2409.19256.md"
    }
  },
  "2409.19606": {
    "id": "2409.19606",
    "title": "Hyper-Connections",
    "authors": "Defa Zhu, Hongzhi Huang, Zihao Huang and 5 others",
    "abstract": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.",
    "url": "https://arxiv.org/abs/2409.19606",
    "arxivId": "2409.19606",
    "last_visited": "2025-01-02T08:05:54.690Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-29T07:57:07Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.19606/features/markdown-grobid/2409.19606.md",
      "adr-crib": "data/papers/2409.19606/features/adr-crib/2409.19606.md",
      "adr-titles": "data/papers/2409.19606/features/adr-titles/2409.19606.md",
      "crib-sheet": "data/papers/2409.19606/features/crib-sheet/2409.19606.md",
      "compound-crib": "data/papers/2409.19606/features/compound-crib/2409.19606.md"
    }
  },
  "2410.00286": {
    "id": "2410.00286",
    "title": "Fermi-GBM Team Analysis on The Ravasio Line",
    "authors": "Eric Burns, Stephen Lesage, Adam Goldstein and 20 others",
    "abstract": "The prompt spectra of gamma-ray bursts are known to follow broadband continuum behavior over decades in energy. GRB 221009A, given the moniker the brightest of all time (BOAT), is the brightest gamma-ray burst identified in half a century of observations, and was first identified by the Fermi Gamma-ray Burst Monitor (GBM). On behalf of the Fermi-GBM Team, Lesage et al. (2023) described the initial GBM analysis. Ravasio et al. (2024) report the identification of a spectral line in part of the prompt emission of this burst, which they describe as evolving over 80 s from $\\sim$12 MeV to 6 MeV. We report a GBM Team analysis on the Ravasio Line: 1) We cannot identify an instrumental effect that could have produced this signal, and 2) our method of calculating the statistical significance of the line shows it easily exceeds the 5$\\sigma$ discovery threshold. We additionally comment on the claim of the line beginning at earlier time intervals, up to 37 MeV, as reported in Zhang et al. (2024). We find that it is reasonable to utilize these measurements for characterization of the line evolution, with caution. We encourage theoretical studies exploring this newly discovered gamma-ray burst spectral feature, unless any rigorous alternative explanation unrelated to the emission from GRB 221009A is identified.",
    "url": "https://arxiv.org/abs/2410.00286",
    "arxivId": "2410.00286",
    "last_visited": "2025-01-13T06:06:36.272Z",
    "last_read": "2025-01-13T06:08:24.046881",
    "total_reading_time_seconds": 21,
    "published_date": "2024-09-30T23:43:52Z",
    "arxiv_tags": [
      "astro-ph.HE",
      "stat.AP"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.00286/features/markdown-grobid/2410.00286.md",
      "adr-crib": "data/papers/2410.00286/features/adr-crib/2410.00286.md",
      "adr-titles": "data/papers/2410.00286/features/adr-titles/2410.00286.md",
      "crib-sheet": "data/papers/2410.00286/features/crib-sheet/2410.00286.md",
      "compound-crib": "data/papers/2410.00286/features/compound-crib/2410.00286.md"
    }
  },
  "2410.01131": {
    "id": "2410.01131",
    "title": "nGPT: Normalized Transformer with Representation Learning on the   Hypersphere",
    "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
    "abstract": "We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.",
    "url": "https://arxiv.org/abs/2410.01131",
    "arxivId": "2410.01131",
    "last_visited": "2025-01-07T23:23:22.376Z",
    "last_read": "2025-01-07T23:26:00.695853",
    "total_reading_time_seconds": 122,
    "published_date": "2024-10-01T23:50:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.01131/features/markdown-grobid/2410.01131.md",
      "adr-crib": "data/papers/2410.01131/features/adr-crib/2410.01131.md",
      "adr-titles": "data/papers/2410.01131/features/adr-titles/2410.01131.md",
      "crib-sheet": "data/papers/2410.01131/features/crib-sheet/2410.01131.md",
      "compound-crib": "data/papers/2410.01131/features/compound-crib/2410.01131.md"
    }
  },
  "2410.02423": {
    "id": "2410.02423",
    "title": "PnP-Flow: Plug-and-Play Image Restoration with Flow Matching",
    "authors": "Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl",
    "abstract": "In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pre-trained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the data-fidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.",
    "url": "https://arxiv.org/abs/2410.02423",
    "arxivId": "2410.02423",
    "last_visited": "2024-12-22T06:43:50.011Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-03T12:13:56Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.02423/features/markdown-grobid/2410.02423.md",
      "adr-crib": "data/papers/2410.02423/features/adr-crib/2410.02423.md",
      "adr-titles": "data/papers/2410.02423/features/adr-titles/2410.02423.md",
      "crib-sheet": "data/papers/2410.02423/features/crib-sheet/2410.02423.md",
      "compound-crib": "data/papers/2410.02423/features/compound-crib/2410.02423.md"
    }
  },
  "2410.05437": {
    "id": "2410.05437",
    "title": "ESPACE: Dimensionality Reduction of Activations for Model Compression",
    "authors": "Charbel Sakr, Brucek Khailany",
    "abstract": "We propose ESPACE, an LLM compression technique based on dimensionality reduction of activations. Unlike prior works on weight-centric tensor decomposition, ESPACE projects activations onto a pre-calibrated set of principal components. The activation-centrality of the approach enables retraining LLMs with no loss of expressivity; while at inference, weight decomposition is obtained as a byproduct of matrix multiplication associativity. Theoretical results on the construction of projection matrices with optimal computational accuracy are provided. Experimentally, we find ESPACE enables 50% compression of GPT3, Llama2, and Nemotron4 models with small accuracy degradation, as low as a 0.18 perplexity increase on GPT3-22B. At lower compression rates of 20% to 40%, ESPACE drives GPT3 models to outperforming their baseline, by up to a 0.38 decrease in perplexity for GPT3-8B. ESPACE also reduces GEMM execution time and prefill inference latency on existing hardware. Comparison with related works on compressing Llama2-7B via matrix factorization shows that ESPACE is a first step in advancing the state-of-the-art in tensor decomposition compression of LLMs.",
    "url": "https://arxiv.org/abs/2410.05437",
    "arxivId": "2410.05437",
    "last_visited": "2025-01-19T09:45:58.441Z",
    "last_read": "2025-01-19T09:45:58.441Z",
    "total_reading_time_seconds": 17,
    "published_date": "2024-10-07T18:59:22Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.05437/features/markdown-grobid/2410.05437.md",
      "adr-crib": "data/papers/2410.05437/features/adr-crib/2410.05437.md",
      "adr-titles": "data/papers/2410.05437/features/adr-titles/2410.05437.md",
      "crib-sheet": "data/papers/2410.05437/features/crib-sheet/2410.05437.md",
      "compound-crib": "data/papers/2410.05437/features/compound-crib/2410.05437.md"
    }
  },
  "2410.08800": {
    "id": "2410.08800",
    "title": "Data Processing for the OpenGPT-X Model Family",
    "authors": "Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick and 19 others",
    "abstract": "This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.",
    "url": "https://arxiv.org/abs/2410.08800",
    "arxivId": "2410.08800",
    "last_visited": "2024-12-30T20:06:07.375Z",
    "last_read": "2025-01-04T06:52:45.622955",
    "total_reading_time_seconds": 47,
    "published_date": "2024-10-11T13:34:24Z",
    "arxiv_tags": [
      "cs.CL",
      "H.3.1; I.2.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.08800/features/markdown-grobid/2410.08800.md",
      "adr-crib": "data/papers/2410.08800/features/adr-crib/2410.08800.md",
      "adr-titles": "data/papers/2410.08800/features/adr-titles/2410.08800.md",
      "crib-sheet": "data/papers/2410.08800/features/crib-sheet/2410.08800.md",
      "compound-crib": "data/papers/2410.08800/features/compound-crib/2410.08800.md"
    }
  },
  "2410.10792": {
    "id": "2410.10792",
    "title": "Semantic Image Inversion and Editing using Rectified Stochastic   Differential Equations",
    "authors": "Litu Rout, Yujia Chen, Nataniel Ruiz and 3 others",
    "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.",
    "url": "https://arxiv.org/abs/2410.10792",
    "arxivId": "2410.10792",
    "last_visited": "2024-12-30T15:14:34.707Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-14T17:56:24Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.10792/features/markdown-grobid/2410.10792.md",
      "adr-crib": "data/papers/2410.10792/features/adr-crib/2410.10792.md",
      "adr-titles": "data/papers/2410.10792/features/adr-titles/2410.10792.md",
      "crib-sheet": "data/papers/2410.10792/features/crib-sheet/2410.10792.md",
      "compound-crib": "data/papers/2410.10792/features/compound-crib/2410.10792.md"
    }
  },
  "2410.13835": {
    "id": "2410.13835",
    "title": "Active-Dormant Attention Heads: Mechanistically Demystifying   Extreme-Token Phenomena in LLMs",
    "authors": "Tianyu Guo, Druv Pai, Yu Bai and 3 others",
    "abstract": "Practitioners have consistently observed three puzzling phenomena in transformer-based large language models (LLMs): attention sinks, value-state drains, and residual-state peaks, collectively referred to as extreme-token phenomena. These phenomena are characterized by certain so-called \"sink tokens\" receiving disproportionately high attention weights, exhibiting significantly smaller value states, and having much larger residual-state norms than those of other tokens. These extreme tokens give rise to various challenges in LLM inference, quantization, and interpretability.   We elucidate the mechanisms behind extreme-token phenomena. First, we show that these phenomena arise in very simple architectures -- transformers with one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task. In this setting, we identify an active-dormant mechanism, where attention heads become sinks for specific input domains while remaining non-sinks for others. Our theoretical analysis of the training dynamics reveals that these phenomena are driven by a mutual reinforcement mechanism. Building on these insights, we propose strategies to mitigate extreme-token phenomena during pretraining, including replacing softmax with ReLU and Adam with SGD. Next, we extend our analysis to pretrained LLMs, including Llama and OLMo, showing that many attention heads exhibit a similar active-dormant mechanism as in the BB task, and that the mutual reinforcement mechanism also governs the emergence of extreme-token phenomena during LLM pretraining. Our results reveal that many of the static and dynamic properties of extreme-token phenomena predicted by the BB task align with observations in pretrained LLMs.",
    "url": "https://arxiv.org/abs/2410.13835",
    "arxivId": "2410.13835",
    "last_visited": "2024-12-30T22:17:13.815000+00:00",
    "last_read": "2025-01-04T06:52:39.614755",
    "total_reading_time_seconds": 16,
    "published_date": "2024-10-17T17:54:06Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.13835/features/markdown-grobid/2410.13835.md",
      "adr-crib": "data/papers/2410.13835/features/adr-crib/2410.13835.md",
      "adr-titles": "data/papers/2410.13835/features/adr-titles/2410.13835.md",
      "crib-sheet": "data/papers/2410.13835/features/crib-sheet/2410.13835.md",
      "compound-crib": "data/papers/2410.13835/features/compound-crib/2410.13835.md"
    }
  },
  "2410.15468": {
    "id": "2410.15468",
    "title": "What Emergence Can Possibly Mean",
    "authors": "Sean M. Carroll, Achyuth Parola",
    "abstract": "We consider emergence from the perspective of dynamics: states of a system evolving with time. We focus on the role of a decomposition of wholes into parts, and attempt to characterize relationships between levels without reference to whether higher-level properties are \"novel\" or \"unexpected.\" We offer a classification of different varieties of emergence, with and without new ontological elements at higher levels.",
    "url": "https://arxiv.org/abs/2410.15468",
    "arxivId": "2410.15468",
    "last_visited": "2024-12-22T05:48:56.244Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-20T18:45:11Z",
    "arxiv_tags": [
      "physics.hist-ph",
      "cond-mat.stat-mech"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.15468/features/markdown-grobid/2410.15468.md",
      "adr-crib": "data/papers/2410.15468/features/adr-crib/2410.15468.md",
      "adr-titles": "data/papers/2410.15468/features/adr-titles/2410.15468.md",
      "crib-sheet": "data/papers/2410.15468/features/crib-sheet/2410.15468.md",
      "compound-crib": "data/papers/2410.15468/features/compound-crib/2410.15468.md"
    }
  },
  "2410.21265": {
    "id": "2410.21265",
    "title": "Modular Duality in Deep Learning",
    "authors": "Jeremy Bernstein, Laker Newhouse",
    "abstract": "An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers -- the latter two methods are based on a rectangular Newton-Schulz iteration (Kovarik, 1970; Bj\\\"orck & Bowie, 1971). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.",
    "url": "https://arxiv.org/abs/2410.21265",
    "arxivId": "2410.21265",
    "last_visited": "2025-01-02T19:43:53.760Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-28T17:57:31Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.21265/features/markdown-grobid/2410.21265.md",
      "adr-crib": "data/papers/2410.21265/features/adr-crib/2410.21265.md",
      "adr-titles": "data/papers/2410.21265/features/adr-titles/2410.21265.md",
      "crib-sheet": "data/papers/2410.21265/features/crib-sheet/2410.21265.md",
      "compound-crib": "data/papers/2410.21265/features/compound-crib/2410.21265.md"
    }
  },
  "2410.24054": {
    "id": "2410.24054",
    "title": "EigenVI: score-based variational inference with orthogonal function   expansions",
    "authors": "Diana Cai, Chirag Modi, Charles C. Margossian and 3 others",
    "abstract": "We develop EigenVI, an eigenvalue-based approach for black-box variational inference (BBVI). EigenVI constructs its variational approximations from orthogonal function expansions. For distributions over $\\mathbb{R}^D$, the lowest order term in these expansions provides a Gaussian variational approximation, while higher-order terms provide a systematic way to model non-Gaussianity. These approximations are flexible enough to model complex distributions (multimodal, asymmetric), but they are simple enough that one can calculate their low-order moments and draw samples from them. EigenVI can also model other types of random variables (e.g., nonnegative, bounded) by constructing variational approximations from different families of orthogonal functions. Within these families, EigenVI computes the variational approximation that best matches the score function of the target distribution by minimizing a stochastic estimate of the Fisher divergence. Notably, this optimization reduces to solving a minimum eigenvalue problem, so that EigenVI effectively sidesteps the iterative gradient-based optimizations that are required for many other BBVI algorithms. (Gradient-based methods can be sensitive to learning rates, termination criteria, and other tunable hyperparameters.) We use EigenVI to approximate a variety of target distributions, including a benchmark suite of Bayesian models from posteriordb. On these distributions, we find that EigenVI is more accurate than existing methods for Gaussian BBVI.",
    "url": "https://arxiv.org/abs/2410.24054",
    "arxivId": "2410.24054",
    "last_visited": "2024-12-22T05:52:47.117Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-31T15:48:34Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "stat.CO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.24054/features/markdown-grobid/2410.24054.md",
      "adr-crib": "data/papers/2410.24054/features/adr-crib/2410.24054.md",
      "adr-titles": "data/papers/2410.24054/features/adr-titles/2410.24054.md",
      "crib-sheet": "data/papers/2410.24054/features/crib-sheet/2410.24054.md",
      "compound-crib": "data/papers/2410.24054/features/compound-crib/2410.24054.md"
    }
  },
  "2411.04282": {
    "id": "2411.04282",
    "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning   Capabilities via Self-Rewarding",
    "authors": "Haolin Chen, Yihao Feng, Zuxin Liu and 8 others",
    "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at \\url{https://github.com/SalesforceAIResearch/LaTRO}.",
    "url": "https://arxiv.org/abs/2411.04282",
    "arxivId": "2411.04282",
    "last_visited": "2024-12-22T21:35:41.978Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-06T22:02:30Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "I.2.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.04282/features/markdown-grobid/2411.04282.md",
      "adr-crib": "data/papers/2411.04282/features/adr-crib/2411.04282.md",
      "adr-titles": "data/papers/2411.04282/features/adr-titles/2411.04282.md",
      "crib-sheet": "data/papers/2411.04282/features/crib-sheet/2411.04282.md",
      "compound-crib": "data/papers/2411.04282/features/compound-crib/2411.04282.md"
    }
  },
  "2411.04330": {
    "id": "2411.04330",
    "title": "Scaling Laws for Precision",
    "authors": "Tanishq Kumar, Zachary Ankner, Benjamin F. Spector and 6 others",
    "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
    "url": "https://arxiv.org/abs/2411.04330",
    "arxivId": "2411.04330",
    "last_visited": "2024-12-30T20:16:37.184Z",
    "last_read": "2025-01-04T06:53:12.612468",
    "total_reading_time_seconds": 11,
    "published_date": "2024-11-07T00:10:10Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.04330/features/markdown-grobid/2411.04330.md",
      "adr-crib": "data/papers/2411.04330/features/adr-crib/2411.04330.md",
      "adr-titles": "data/papers/2411.04330/features/adr-titles/2411.04330.md",
      "crib-sheet": "data/papers/2411.04330/features/crib-sheet/2411.04330.md",
      "compound-crib": "data/papers/2411.04330/features/compound-crib/2411.04330.md"
    }
  },
  "2411.05899": {
    "id": "2411.05899",
    "title": "Streaming Bayes GFlowNets",
    "authors": "Tiago da Silva, Daniel Augusto de Souza, Diego Mesquita",
    "abstract": "Bayes' rule naturally allows for inference refinement in a streaming fashion, without the need to recompute posteriors from scratch whenever new data arrives. In principle, Bayesian streaming is straightforward: we update our prior with the available data and use the resulting posterior as a prior when processing the next data chunk. In practice, however, this recipe entails i) approximating an intractable posterior at each time step; and ii) encapsulating results appropriately to allow for posterior propagation. For continuous state spaces, variational inference (VI) is particularly convenient due to its scalability and the tractability of variational posteriors. For discrete state spaces, however, state-of-the-art VI results in analytically intractable approximations that are ill-suited for streaming settings. To enable streaming Bayesian inference over discrete parameter spaces, we propose streaming Bayes GFlowNets (abbreviated as SB-GFlowNets) by leveraging the recently proposed GFlowNets -- a powerful class of amortized samplers for discrete compositional objects. Notably, SB-GFlowNet approximates the initial posterior using a standard GFlowNet and subsequently updates it using a tailored procedure that requires only the newly observed data. Our case studies in linear preference learning and phylogenetic inference showcase the effectiveness of SB-GFlowNets in sampling from an unnormalized posterior in a streaming setting. As expected, we also observe that SB-GFlowNets is significantly faster than repeatedly training a GFlowNet from scratch to sample from the full posterior.",
    "url": "https://arxiv.org/abs/2411.05899",
    "arxivId": "2411.05899",
    "last_visited": "2024-12-28T06:09:10.852Z",
    "last_read": "2025-01-04T15:02:48.872465",
    "total_reading_time_seconds": 24,
    "published_date": "2024-11-08T15:53:56Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.05899/features/markdown-grobid/2411.05899.md",
      "adr-crib": "data/papers/2411.05899/features/adr-crib/2411.05899.md",
      "adr-titles": "data/papers/2411.05899/features/adr-titles/2411.05899.md",
      "crib-sheet": "data/papers/2411.05899/features/crib-sheet/2411.05899.md",
      "compound-crib": "data/papers/2411.05899/features/compound-crib/2411.05899.md"
    }
  },
  "2411.06068": {
    "id": "2411.06068",
    "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
    "authors": "Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",
    "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset for language model pretraining. Zyda-2 was used to train our Zamba2 series of models which are state-of-the-art for their weight class. We build Zyda-2 by collating high-quality open-source tokens such as FineWeb and DCLM, then distilling them to the highest-quality subset via cross-deduplication and model-based quality filtering. Zyda-2 is released under a permissive open license, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",
    "url": "https://arxiv.org/abs/2411.06068",
    "arxivId": "2411.06068",
    "last_visited": "2024-12-30T22:18:39.627000+00:00",
    "last_read": "2025-01-04T06:52:36.612646",
    "total_reading_time_seconds": 6,
    "published_date": "2024-11-09T04:57:41Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.06068/features/markdown-grobid/2411.06068.md",
      "adr-crib": "data/papers/2411.06068/features/adr-crib/2411.06068.md",
      "adr-titles": "data/papers/2411.06068/features/adr-titles/2411.06068.md",
      "crib-sheet": "data/papers/2411.06068/features/crib-sheet/2411.06068.md",
      "compound-crib": "data/papers/2411.06068/features/compound-crib/2411.06068.md"
    }
  },
  "2411.12372": {
    "id": "2411.12372",
    "title": "RedPajama: an Open Dataset for Training Large Language Models",
    "authors": "Maurice Weber, Daniel Fu, Quentin Anthony and 16 others",
    "abstract": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata. Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",
    "url": "https://arxiv.org/abs/2411.12372",
    "arxivId": "2411.12372",
    "last_visited": "2024-12-30T20:18:27.856Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-19T09:35:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.12372/features/markdown-grobid/2411.12372.md",
      "adr-crib": "data/papers/2411.12372/features/adr-crib/2411.12372.md",
      "adr-titles": "data/papers/2411.12372/features/adr-titles/2411.12372.md",
      "crib-sheet": "data/papers/2411.12372/features/crib-sheet/2411.12372.md",
      "compound-crib": "data/papers/2411.12372/features/compound-crib/2411.12372.md"
    }
  },
  "2411.18933": {
    "id": "2411.18933",
    "title": "Efficient Track Anything",
    "authors": "Yunyang Xiong, Chong Zhou, Xiaoyu Xiang and 10 others",
    "abstract": "Segment Anything Model 2 (SAM 2) has emerged as a powerful tool for video object segmentation and tracking anything. Key components of SAM 2 that drive the impressive video object segmentation performance include a large multistage image encoder for frame feature extraction and a memory mechanism that stores memory contexts from past frames to help current frame segmentation. The high computation complexity of multistage image encoder and memory module has limited its applications in real-world tasks, e.g., video object segmentation on mobile devices. To address this limitation, we propose EfficientTAMs, lightweight track anything models that produce high-quality results with low latency and model size. Our idea is based on revisiting the plain, nonhierarchical Vision Transformer (ViT) as an image encoder for video object segmentation, and introducing an efficient memory module, which reduces the complexity for both frame feature extraction and memory computation for current frame segmentation. We take vanilla lightweight ViTs and efficient memory module to build EfficientTAMs, and train the models on SA-1B and SA-V datasets for video object segmentation and track anything tasks. We evaluate on multiple video segmentation benchmarks including semi-supervised VOS and promptable video segmentation, and find that our proposed EfficientTAM with vanilla ViT perform comparably to SAM 2 model (HieraB+SAM 2) with ~2x speedup on A100 and ~2.4x parameter reduction. On segment anything image tasks, our EfficientTAMs also perform favorably over original SAM with ~20x speedup on A100 and ~20x parameter reduction. On mobile devices such as iPhone 15 Pro Max, our EfficientTAMs can run at ~10 FPS for performing video object segmentation with reasonable quality, highlighting the capability of small models for on-device video object segmentation applications.",
    "url": "https://arxiv.org/abs/2411.18933",
    "arxivId": "2411.18933",
    "last_visited": "2025-02-21T09:27:58.255Z",
    "last_read": "2025-02-21T09:27:58.255Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-28T05:52:10Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.18933/features/markdown-grobid/2411.18933.md",
      "adr-crib": "data/papers/2411.18933/features/adr-crib/2411.18933.md",
      "adr-titles": "data/papers/2411.18933/features/adr-titles/2411.18933.md",
      "crib-sheet": "data/papers/2411.18933/features/crib-sheet/2411.18933.md",
      "compound-crib": "data/papers/2411.18933/features/compound-crib/2411.18933.md"
    }
  },
  "2411.19108": {
    "id": "2411.19108",
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "authors": "Feng Liu, Shiwei Zhang, Xiaofeng Wang and 6 others",
    "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
    "url": "https://arxiv.org/abs/2411.19108",
    "arxivId": "2411.19108",
    "last_visited": "2024-12-30T15:10:27.225Z",
    "last_read": "2025-01-04T14:48:33.229442",
    "total_reading_time_seconds": 40,
    "published_date": "2024-11-28T12:50:05Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.19108/features/markdown-grobid/2411.19108.md",
      "adr-crib": "data/papers/2411.19108/features/adr-crib/2411.19108.md",
      "adr-titles": "data/papers/2411.19108/features/adr-titles/2411.19108.md",
      "crib-sheet": "data/papers/2411.19108/features/crib-sheet/2411.19108.md",
      "compound-crib": "data/papers/2411.19108/features/compound-crib/2411.19108.md"
    }
  },
  "2411.19722": {
    "id": "2411.19722",
    "title": "JetFormer: An Autoregressive Generative Model of Raw Images and Text",
    "authors": "Michael Tschannen, André Susano Pinto, Alexander Kolesnikov",
    "abstract": "Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer - JetFormer - which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQ-VAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.",
    "url": "https://arxiv.org/abs/2411.19722",
    "arxivId": "2411.19722",
    "last_visited": "2024-12-15T22:16:44.245Z",
    "last_read": "2025-01-05T18:41:15.660935",
    "total_reading_time_seconds": 300,
    "published_date": "2024-11-29T14:14:59Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.19722/features/markdown-grobid/2411.19722.md",
      "adr-crib": "data/papers/2411.19722/features/adr-crib/2411.19722.md",
      "adr-titles": "data/papers/2411.19722/features/adr-titles/2411.19722.md",
      "crib-sheet": "data/papers/2411.19722/features/crib-sheet/2411.19722.md",
      "compound-crib": "data/papers/2411.19722/features/compound-crib/2411.19722.md"
    }
  },
  "2412.00733": {
    "id": "2412.00733",
    "title": "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with   Diffusion Transformer Networks",
    "authors": "Jiahao Cui, Hui Li, Yun Zhan and 7 others",
    "abstract": "Existing methodologies for animating portrait images face significant challenges, particularly in handling non-frontal perspectives, rendering dynamic objects around the portrait, and generating immersive, realistic backgrounds. In this paper, we introduce the first application of a pretrained transformer-based video generative model that demonstrates strong generalization capabilities and generates highly dynamic, realistic videos for portrait animation, effectively addressing these challenges. The adoption of a new video backbone model makes previous U-Net-based methods for identity maintenance, audio conditioning, and video extrapolation inapplicable. To address this limitation, we design an identity reference network consisting of a causal 3D VAE combined with a stacked series of transformer layers, ensuring consistent facial identity across video sequences. Additionally, we investigate various speech audio conditioning and motion frame mechanisms to enable the generation of continuous video driven by speech audio. Our method is validated through experiments on benchmark and newly proposed wild datasets, demonstrating substantial improvements over prior methods in generating realistic portraits characterized by diverse orientations within dynamic and immersive scenes. Further visualizations and the source code are available at: https://fudan-generative-vision.github.io/hallo3/.",
    "url": "https://arxiv.org/abs/2412.00733",
    "arxivId": "2412.00733",
    "last_visited": "2025-01-10T18:21:47.668Z",
    "last_read": "2025-01-10T18:23:26.487447",
    "total_reading_time_seconds": 20,
    "published_date": "2024-12-01T08:54:30Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.00733/features/markdown-grobid/2412.00733.md",
      "adr-crib": "data/papers/2412.00733/features/adr-crib/2412.00733.md",
      "adr-titles": "data/papers/2412.00733/features/adr-titles/2412.00733.md",
      "crib-sheet": "data/papers/2412.00733/features/crib-sheet/2412.00733.md",
      "compound-crib": "data/papers/2412.00733/features/compound-crib/2412.00733.md"
    }
  },
  "2412.01023": {
    "id": "2412.01023",
    "title": "Learning Structured Representations with Hyperbolic Embeddings",
    "authors": "Aditya Sinha, Siqi Zeng, Makoto Yamada, Han Zhao",
    "abstract": "Most real-world datasets consist of a natural hierarchy between classes or an inherent label structure that is either already available or can be constructed cheaply. However, most existing representation learning methods ignore this hierarchy, treating labels as permutation invariant. Recent work [Zeng et al., 2022] proposes using this structured information explicitly, but the use of Euclidean distance may distort the underlying semantic context [Chen et al., 2013]. In this work, motivated by the advantage of hyperbolic spaces in modeling hierarchical relationships, we propose a novel approach HypStructure: a Hyperbolic Structured regularization approach to accurately embed the label hierarchy into the learned representations. HypStructure is a simple-yet-effective regularizer that consists of a hyperbolic tree-based representation loss along with a centering loss, and can be combined with any standard task loss to learn hierarchy-informed features. Extensive experiments on several large-scale vision benchmarks demonstrate the efficacy of HypStructure in reducing distortion and boosting generalization performance especially under low dimensional scenarios. For a better understanding of structured representation, we perform eigenvalue analysis that links the representation geometry to improved Out-of-Distribution (OOD) detection performance seen empirically. The code is available at \\url{https://github.com/uiuctml/HypStructure}.",
    "url": "https://arxiv.org/abs/2412.01023",
    "arxivId": "2412.01023",
    "last_visited": "2024-12-29T02:32:00.838000+00:00",
    "last_read": "2025-01-04T14:49:39.264976",
    "total_reading_time_seconds": 16,
    "published_date": "2024-12-02T00:56:44Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.01023/features/markdown-grobid/2412.01023.md",
      "adr-crib": "data/papers/2412.01023/features/adr-crib/2412.01023.md",
      "adr-titles": "data/papers/2412.01023/features/adr-titles/2412.01023.md",
      "crib-sheet": "data/papers/2412.01023/features/crib-sheet/2412.01023.md",
      "compound-crib": "data/papers/2412.01023/features/compound-crib/2412.01023.md"
    }
  },
  "2412.02595": {
    "id": "2412.02595",
    "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset",
    "authors": "Dan Su, Kezhi Kong, Ying Lin and 6 others",
    "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",
    "url": "https://arxiv.org/abs/2412.02595",
    "arxivId": "2412.02595",
    "last_visited": "2024-12-30T20:02:25.502Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-03T17:28:50Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.02595/features/markdown-grobid/2412.02595.md",
      "adr-crib": "data/papers/2412.02595/features/adr-crib/2412.02595.md",
      "adr-titles": "data/papers/2412.02595/features/adr-titles/2412.02595.md",
      "crib-sheet": "data/papers/2412.02595/features/crib-sheet/2412.02595.md",
      "compound-crib": "data/papers/2412.02595/features/compound-crib/2412.02595.md"
    }
  },
  "2412.03603": {
    "id": "2412.03603",
    "title": "HunyuanVideo: A Systematic Framework For Large Video Generative Models",
    "authors": "Weijie Kong, Qi Tian, Zijian Zhang and 49 others",
    "abstract": "Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at https://github.com/Tencent/HunyuanVideo.",
    "url": "https://arxiv.org/abs/2412.03603",
    "arxivId": "2412.03603",
    "last_visited": "2025-02-15T22:38:36.193Z",
    "last_read": "2025-02-15T22:38:36.193Z",
    "total_reading_time_seconds": 20,
    "published_date": "2024-12-03T23:52:37Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.03603/features/markdown-grobid/2412.03603.md",
      "adr-crib": "data/papers/2412.03603/features/adr-crib/2412.03603.md",
      "adr-titles": "data/papers/2412.03603/features/adr-titles/2412.03603.md",
      "crib-sheet": "data/papers/2412.03603/features/crib-sheet/2412.03603.md",
      "compound-crib": "data/papers/2412.03603/features/compound-crib/2412.03603.md"
    }
  },
  "2412.04384": {
    "id": "2412.04384",
    "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D   Occupancy Prediction",
    "authors": "Yuanhui Huang, Amonnut Thammatadatrakoon, Wenzhao Zheng and 3 others",
    "abstract": "3D semantic occupancy prediction is an important task for robust vision-centric autonomous driving, which predicts fine-grained geometry and semantics of the surrounding scene. Most existing methods leverage dense grid-based scene representations, overlooking the spatial sparsity of the driving scenes. Although 3D semantic Gaussian serves as an object-centric sparse alternative, most of the Gaussians still describe the empty region with low efficiency. To address this, we propose a probabilistic Gaussian superposition model which interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry. Furthermore, we adopt the exact Gaussian mixture model for semantics calculation to avoid unnecessary overlapping of Gaussians. To effectively initialize Gaussians in non-empty region, we design a distribution-based initialization module which learns the pixel-aligned occupancy distribution instead of the depth of surfaces. We conduct extensive experiments on nuScenes and KITTI-360 datasets and our GaussianFormer-2 achieves state-of-the-art performance with high efficiency. Code: https://github.com/huang-yh/GaussianFormer.",
    "url": "https://arxiv.org/abs/2412.04384",
    "arxivId": "2412.04384",
    "last_visited": "2025-01-14T18:28:38.121Z",
    "last_read": "2025-01-14T18:29:51.193732",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-05T17:59:58Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.04384/features/markdown-grobid/2412.04384.md",
      "adr-crib": "data/papers/2412.04384/features/adr-crib/2412.04384.md",
      "adr-titles": "data/papers/2412.04384/features/adr-titles/2412.04384.md",
      "crib-sheet": "data/papers/2412.04384/features/crib-sheet/2412.04384.md",
      "compound-crib": "data/papers/2412.04384/features/compound-crib/2412.04384.md"
    }
  },
  "2412.04619": {
    "id": "2412.04619",
    "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
    "authors": "Tian Qin, Naomi Saphra, David Alvarez-Melis",
    "abstract": "Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.",
    "url": "https://arxiv.org/abs/2412.04619",
    "arxivId": "2412.04619",
    "last_visited": "2024-12-22T17:55:18.862Z",
    "last_read": "2025-01-05T08:23:53.657116",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-05T21:12:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.04619/features/markdown-grobid/2412.04619.md",
      "adr-crib": "data/papers/2412.04619/features/adr-crib/2412.04619.md",
      "adr-titles": "data/papers/2412.04619/features/adr-titles/2412.04619.md",
      "crib-sheet": "data/papers/2412.04619/features/crib-sheet/2412.04619.md",
      "compound-crib": "data/papers/2412.04619/features/compound-crib/2412.04619.md"
    }
  },
  "2412.06264": {
    "id": "2412.06264",
    "title": "Flow Matching Guide and Code",
    "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth and 7 others",
    "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.",
    "url": "https://arxiv.org/abs/2412.06264",
    "arxivId": "2412.06264",
    "last_visited": "2024-12-22T06:52:20.913Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-09T07:22:38Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.06264/features/markdown-grobid/2412.06264.md",
      "adr-crib": "data/papers/2412.06264/features/adr-crib/2412.06264.md",
      "adr-titles": "data/papers/2412.06264/features/adr-titles/2412.06264.md",
      "crib-sheet": "data/papers/2412.06264/features/crib-sheet/2412.06264.md",
      "compound-crib": "data/papers/2412.06264/features/compound-crib/2412.06264.md"
    }
  },
  "2412.06769": {
    "id": "2412.06769",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "authors": "Shibo Hao, Sainbayar Sukhbaatar, DiJia Su and 4 others",
    "abstract": "Large language models (LLMs) are restricted to reason in the \"language space\", where they typically express the reasoning process with a chain-of-thought (CoT) to solve a complex reasoning problem. However, we argue that language space may not always be optimal for reasoning. For example, most word tokens are primarily for textual coherence and not essential for reasoning, while some critical tokens require complex planning and pose huge challenges to LLMs. To explore the potential of LLM reasoning in an unrestricted latent space instead of using natural language, we introduce a new paradigm Coconut (Chain of Continuous Thought). We utilize the last hidden state of the LLM as a representation of the reasoning state (termed \"continuous thought\"). Rather than decoding this into a word token, we feed it back to the LLM as the subsequent input embedding directly in the continuous space. Experiments show that Coconut can effectively augment the LLM on several reasoning tasks. This novel latent reasoning paradigm leads to emergent advanced reasoning patterns: the continuous thought can encode multiple alternative next reasoning steps, allowing the model to perform a breadth-first search (BFS) to solve the problem, rather than prematurely committing to a single deterministic path like CoT. Coconut outperforms CoT in certain logical reasoning tasks that require substantial backtracking during planning, with fewer thinking tokens during inference. These findings demonstrate the promise of latent reasoning and offer valuable insights for future research.",
    "url": "https://arxiv.org/abs/2412.06769",
    "arxivId": "2412.06769",
    "last_visited": "2025-01-20T07:20:45.082Z",
    "last_read": "2025-01-20T07:20:45.082Z",
    "total_reading_time_seconds": 37,
    "published_date": "2024-12-09T18:55:56Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.06769/features/markdown-grobid/2412.06769.md",
      "adr-crib": "data/papers/2412.06769/features/adr-crib/2412.06769.md",
      "adr-titles": "data/papers/2412.06769/features/adr-titles/2412.06769.md",
      "crib-sheet": "data/papers/2412.06769/features/crib-sheet/2412.06769.md",
      "compound-crib": "data/papers/2412.06769/features/compound-crib/2412.06769.md"
    }
  },
  "2412.06771": {
    "id": "2412.06771",
    "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under   Uncertainty",
    "authors": "Meera Hahn, Wenjun Zeng, Nithish Kannen and 4 others",
    "abstract": "User prompts for generative AI models are often underspecified, leading to sub-optimal responses. This problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user's vision and the model's interpretation often forces users to painstakingly and repeatedly refine their prompts. To address this, we propose a design for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their understanding of user intent as an understandable belief graph that a user can edit. We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least 90% of human subjects found these agents and their belief graphs helpful for their T2I workflow. Moreover, we develop a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On DesignBench, a benchmark we created for artists and designers, the COCO dataset (Lin et al., 2014), and ImageInWords (Garg et al., 2024), we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I generation. Demo: https://github.com/google-deepmind/proactive_t2i_agents.",
    "url": "https://arxiv.org/abs/2412.06771",
    "arxivId": "2412.06771",
    "last_visited": "2025-01-09T14:42:27.418000+00:00",
    "last_read": "2025-01-09T14:43:31.362013",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-09T18:56:32Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.06771/features/markdown-grobid/2412.06771.md",
      "adr-crib": "data/papers/2412.06771/features/adr-crib/2412.06771.md",
      "adr-titles": "data/papers/2412.06771/features/adr-titles/2412.06771.md",
      "crib-sheet": "data/papers/2412.06771/features/crib-sheet/2412.06771.md",
      "compound-crib": "data/papers/2412.06771/features/compound-crib/2412.06771.md"
    }
  },
  "2412.06845": {
    "id": "2412.06845",
    "title": "Fully Open Source Moxin-7B Technical Report",
    "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong and 13 others",
    "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, and some use restrictive licenses whilst claiming to be \"open-source,\" which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed in accordance with the Model Openness Framework (MOF), a ranked classification system that evaluates AI models based on model completeness and openness, adhering to principles of open science, open source, open data, and open access. Our model achieves the highest MOF classification level of \"open science\" through the comprehensive release of pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints. Experiments show that our model achieves superior performance in zero-shot evaluation compared with popular 7B models and performs competitively in few-shot evaluation.",
    "url": "https://arxiv.org/abs/2412.06845",
    "arxivId": "2412.06845",
    "last_visited": "2024-12-30T20:04:21.071Z",
    "last_read": "2025-01-04T06:53:18.651113",
    "total_reading_time_seconds": 53,
    "published_date": "2024-12-08T02:01:46Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.06845/features/markdown-grobid/2412.06845.md",
      "adr-crib": "data/papers/2412.06845/features/adr-crib/2412.06845.md",
      "adr-titles": "data/papers/2412.06845/features/adr-titles/2412.06845.md",
      "crib-sheet": "data/papers/2412.06845/features/crib-sheet/2412.06845.md",
      "compound-crib": "data/papers/2412.06845/features/compound-crib/2412.06845.md"
    }
  },
  "2412.09621": {
    "id": "2412.09621",
    "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
    "authors": "Linyi Jin, Richard Tucker, Zhengqi Li and 3 others",
    "abstract": "Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page: https://stereo4d.github.io",
    "url": "https://arxiv.org/abs/2412.09621",
    "arxivId": "2412.09621",
    "last_visited": "2024-12-16T08:58:26.647Z",
    "last_read": "2025-01-05T18:41:15.656598",
    "total_reading_time_seconds": 60,
    "published_date": "2024-12-12T18:59:54Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.09621/features/markdown-grobid/2412.09621.md",
      "adr-crib": "data/papers/2412.09621/features/adr-crib/2412.09621.md",
      "adr-titles": "data/papers/2412.09621/features/adr-titles/2412.09621.md",
      "crib-sheet": "data/papers/2412.09621/features/crib-sheet/2412.09621.md",
      "compound-crib": "data/papers/2412.09621/features/compound-crib/2412.09621.md"
    }
  },
  "2412.10271": {
    "id": "2412.10271",
    "title": "Benchmarking Linguistic Diversity of Large Language Models",
    "authors": "Yanzhu Guo, Guokan Shang, Chloé Clavel",
    "abstract": "The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.",
    "url": "https://arxiv.org/abs/2412.10271",
    "arxivId": "2412.10271",
    "last_visited": "2024-12-30T20:03:18.882Z",
    "last_read": "2025-01-04T06:53:18.652874",
    "total_reading_time_seconds": 52,
    "published_date": "2024-12-13T16:46:03Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.10271/features/markdown-grobid/2412.10271.md",
      "adr-crib": "data/papers/2412.10271/features/adr-crib/2412.10271.md",
      "adr-titles": "data/papers/2412.10271/features/adr-titles/2412.10271.md",
      "crib-sheet": "data/papers/2412.10271/features/crib-sheet/2412.10271.md",
      "compound-crib": "data/papers/2412.10271/features/compound-crib/2412.10271.md"
    }
  },
  "2412.11766": {
    "id": "2412.11766",
    "title": "Interplay of epidemic spreading and vaccine uptake under complex social   contagion",
    "authors": "Alfonso de Miguel-Arribas, Alberto Aleta, Yamir Moreno",
    "abstract": "Modeling human behavior is essential to accurately predict epidemic spread, with behaviors like vaccine hesitancy complicating control efforts. While epidemic spread is often treated as a simple contagion, vaccine uptake may follow complex contagion dynamics, where individuals' decisions depend on multiple social contacts. Recently, the concept of complex contagion has received strong theoretical underpinnings thanks to the generalization of spreading phenomena from pairwise to higher-order interactions. Although several potential applications have been suggested, examples of complex contagions motivated by real data remain scarce. Surveys on COVID-19 vaccine hesitancy in the US suggest that vaccination attitudes may indeed depend on the vaccination status of social peers, aligning with complex contagion principles. In this work, we examine the interactions between epidemic spread, vaccination, and vaccine uptake attitudes under complex contagion. Using the SIR model with a dynamic, threshold-based vaccination campaign, we simulate scenarios on an age-structured multilayer network informed by US contact data. Our results offer insights into the role of social dynamics in shaping vaccination behavior and epidemic outcomes.",
    "url": "https://arxiv.org/abs/2412.11766",
    "arxivId": "2412.11766",
    "last_visited": "2024-12-28T05:44:44.089Z",
    "last_read": "2025-01-04T15:02:54.892474",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-16T13:37:27Z",
    "arxiv_tags": [
      "physics.soc-ph",
      "cs.SI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.11766/features/markdown-grobid/2412.11766.md",
      "adr-crib": "data/papers/2412.11766/features/adr-crib/2412.11766.md",
      "adr-titles": "data/papers/2412.11766/features/adr-titles/2412.11766.md",
      "crib-sheet": "data/papers/2412.11766/features/crib-sheet/2412.11766.md",
      "compound-crib": "data/papers/2412.11766/features/compound-crib/2412.11766.md"
    }
  },
  "2412.11768": {
    "id": "2412.11768",
    "title": "No More Adam: Learning Rate Scaling at Initialization is All You Need",
    "authors": "Minghao Xu, Lichuan Xiang, Xu Cai, Hongkai Wen",
    "abstract": "In this work, we question the necessity of adaptive gradient methods for training deep neural networks. SGD-SaI is a simple yet effective enhancement to stochastic gradient descent with momentum (SGDM). SGD-SaI performs learning rate Scaling at Initialization (SaI) to distinct parameter groups, guided by their respective gradient signal-to-noise ratios (g-SNR). By adjusting learning rates without relying on adaptive second-order momentum, SGD-SaI helps prevent training imbalances from the very first iteration and cuts the optimizer's memory usage by half compared to AdamW. Despite its simplicity and efficiency, SGD-SaI consistently matches or outperforms AdamW in training a variety of Transformer-based tasks, effectively overcoming a long-standing challenge of using SGD for training Transformers. SGD-SaI excels in ImageNet-1K classification with Vision Transformers(ViT) and GPT-2 pretraining for large language models (LLMs, transformer decoder-only), demonstrating robustness to hyperparameter variations and practicality for diverse applications. We further tested its robustness on tasks like LoRA fine-tuning for LLMs and diffusion models, where it consistently outperforms state-of-the-art optimizers. From a memory efficiency perspective, SGD-SaI achieves substantial memory savings for optimizer states, reducing memory usage by 5.93 GB for GPT-2 (1.5B parameters) and 25.15 GB for Llama2-7B compared to AdamW in full-precision training settings.",
    "url": "https://arxiv.org/abs/2412.11768",
    "arxivId": "2412.11768",
    "last_visited": "2025-01-05T23:10:17.881Z",
    "last_read": "2025-01-05T23:11:31.444118",
    "total_reading_time_seconds": 83,
    "published_date": "2024-12-16T13:41:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.11768/features/markdown-grobid/2412.11768.md",
      "adr-crib": "data/papers/2412.11768/features/adr-crib/2412.11768.md",
      "adr-titles": "data/papers/2412.11768/features/adr-titles/2412.11768.md",
      "crib-sheet": "data/papers/2412.11768/features/crib-sheet/2412.11768.md",
      "compound-crib": "data/papers/2412.11768/features/compound-crib/2412.11768.md"
    }
  },
  "2412.12095": {
    "id": "2412.12095",
    "title": "Causal Diffusion Transformers for Generative Modeling",
    "authors": "Chaorui Deng, Deyao Zhu, Kunchang Li and 2 others",
    "abstract": "We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.",
    "url": "https://arxiv.org/abs/2412.12095",
    "arxivId": "2412.12095",
    "last_visited": "2025-01-21T16:26:49.971Z",
    "last_read": "2025-01-21T16:26:49.971Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-16T18:59:29Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.12095/features/markdown-grobid/2412.12095.md",
      "adr-crib": "data/papers/2412.12095/features/adr-crib/2412.12095.md",
      "adr-titles": "data/papers/2412.12095/features/adr-titles/2412.12095.md",
      "crib-sheet": "data/papers/2412.12095/features/crib-sheet/2412.12095.md",
      "compound-crib": "data/papers/2412.12095/features/compound-crib/2412.12095.md"
    }
  },
  "2412.13061": {
    "id": "2412.13061",
    "title": "VidTok: A Versatile and Open-Source Video Tokenizer",
    "authors": "Anni Tang, Tianyu He, Junliang Guo and 3 others",
    "abstract": "Encoding video content into compact latent tokens has become a fundamental step in video generation and understanding, driven by the need to address the inherent redundancy in pixel-level representations. Consequently, there is a growing demand for high-performance, open-source video tokenizers as video-centric research gains prominence. We introduce VidTok, a versatile video tokenizer that delivers state-of-the-art performance in both continuous and discrete tokenizations. VidTok incorporates several key advancements over existing approaches: 1) model architecture such as convolutional layers and up/downsampling modules; 2) to address the training instability and codebook collapse commonly associated with conventional Vector Quantization (VQ), we integrate Finite Scalar Quantization (FSQ) into discrete video tokenization; 3) improved training strategies, including a two-stage training process and the use of reduced frame rates. By integrating these advancements, VidTok achieves substantial improvements over existing methods, demonstrating superior performance across multiple metrics, including PSNR, SSIM, LPIPS, and FVD, under standardized evaluation settings.",
    "url": "https://arxiv.org/abs/2412.13061",
    "arxivId": "2412.13061",
    "last_visited": "2025-01-07T08:25:03.828000+00:00",
    "last_read": "2025-01-07T08:26:18.055954",
    "total_reading_time_seconds": 8,
    "published_date": "2024-12-17T16:27:11Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.13061/features/markdown-grobid/2412.13061.md",
      "adr-crib": "data/papers/2412.13061/features/adr-crib/2412.13061.md",
      "adr-titles": "data/papers/2412.13061/features/adr-titles/2412.13061.md",
      "crib-sheet": "data/papers/2412.13061/features/crib-sheet/2412.13061.md",
      "compound-crib": "data/papers/2412.13061/features/compound-crib/2412.13061.md"
    }
  },
  "2412.13145": {
    "id": "2412.13145",
    "title": "Agnosticism About Artificial Consciousness",
    "authors": "Tom McClelland",
    "abstract": "Could an AI have conscious experiences? Any answer to this question should conform to Evidentialism - that is, it should be based not on intuition, dogma or speculation but on solid scientific evidence. I argue that such evidence is hard to come by and that the only justifiable stance on the prospects of artificial consciousness is agnosticism. In the current debate, the main division is between biological views that are sceptical of artificial consciousness and functional views that are sympathetic to it. I argue that both camps make the same mistake of over-estimating what the evidence tells us. Scientific insights into consciousness have been achieved through the study of conscious organisms. Although this has enabled cautious assessments of consciousness in various creatures, extending this to AI faces serious obstacles. AI thus presents consciousness researchers with a dilemma: either reach a verdict on artificial consciousness but violate Evidentialism; or respect Evidentialism but offer no verdict on the prospects of artificial consciousness. The dominant trend in the literature has been to take the first option while purporting to follow the scientific evidence. I argue that if we truly follow the evidence, we must take the second option and adopt agnosticism.",
    "url": "https://arxiv.org/abs/2412.13145",
    "arxivId": "2412.13145",
    "last_visited": "2024-12-22T05:22:42.487Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-17T18:11:12Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.13145/features/markdown-grobid/2412.13145.md",
      "adr-crib": "data/papers/2412.13145/features/adr-crib/2412.13145.md",
      "adr-titles": "data/papers/2412.13145/features/adr-titles/2412.13145.md",
      "crib-sheet": "data/papers/2412.13145/features/crib-sheet/2412.13145.md",
      "compound-crib": "data/papers/2412.13145/features/compound-crib/2412.13145.md"
    }
  },
  "2412.13663": {
    "id": "2412.13663",
    "title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for   Fast, Memory Efficient, and Long Context Finetuning and Inference",
    "authors": "Benjamin Warner, Antoine Chaffin, Benjamin Clavié and 11 others",
    "abstract": "Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.",
    "url": "https://arxiv.org/abs/2412.13663",
    "arxivId": "2412.13663",
    "last_visited": "2024-12-22T06:30:57.155Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-18T09:39:44Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.13663/features/markdown-grobid/2412.13663.md",
      "adr-crib": "data/papers/2412.13663/features/adr-crib/2412.13663.md",
      "adr-titles": "data/papers/2412.13663/features/adr-titles/2412.13663.md",
      "crib-sheet": "data/papers/2412.13663/features/crib-sheet/2412.13663.md",
      "compound-crib": "data/papers/2412.13663/features/compound-crib/2412.13663.md"
    }
  },
  "2412.14294": {
    "id": "2412.14294",
    "title": "TRecViT: A Recurrent Video Transformer",
    "authors": "Viorica Pătrăucean, Xu Owen He, Joseph Heyward and 10 others",
    "abstract": "We propose a novel block for video modelling. It relies on a time-space-channel factorisation with dedicated blocks for each dimension: gated linear recurrent units (LRUs) perform information mixing over time, self-attention layers perform mixing over space, and MLPs over channels. The resulting architecture TRecViT performs well on sparse and dense tasks, trained in supervised or self-supervised regimes. Notably, our model is causal and outperforms or is on par with a pure attention model ViViT-L on large scale video datasets (SSv2, Kinetics400), while having $3\\times$ less parameters, $12\\times$ smaller memory footprint, and $5\\times$ lower FLOPs count. Code and checkpoints will be made available online at https://github.com/google-deepmind/trecvit.",
    "url": "https://arxiv.org/abs/2412.14294",
    "arxivId": "2412.14294",
    "last_visited": "2025-01-11T07:35:29.299000+00:00",
    "last_read": "2025-01-11T07:38:52.726106",
    "total_reading_time_seconds": 9,
    "published_date": "2024-12-18T19:44:30Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.14294/features/markdown-grobid/2412.14294.md",
      "adr-crib": "data/papers/2412.14294/features/adr-crib/2412.14294.md",
      "adr-titles": "data/papers/2412.14294/features/adr-titles/2412.14294.md",
      "crib-sheet": "data/papers/2412.14294/features/crib-sheet/2412.14294.md",
      "compound-crib": "data/papers/2412.14294/features/compound-crib/2412.14294.md"
    }
  },
  "2412.15285": {
    "id": "2412.15285",
    "title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase   Pretraining",
    "authors": "Steven Feng, Shrimai Prabhumoye, Kezhi Kong and 4 others",
    "abstract": "Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.",
    "url": "https://arxiv.org/abs/2412.15285",
    "arxivId": "2412.15285",
    "last_visited": "2024-12-30T20:04:52.739Z",
    "last_read": "2025-01-04T06:53:18.649544",
    "total_reading_time_seconds": 49,
    "published_date": "2024-12-18T18:41:18Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.15285/features/markdown-grobid/2412.15285.md",
      "adr-crib": "data/papers/2412.15285/features/adr-crib/2412.15285.md",
      "adr-titles": "data/papers/2412.15285/features/adr-titles/2412.15285.md",
      "crib-sheet": "data/papers/2412.15285/features/crib-sheet/2412.15285.md",
      "compound-crib": "data/papers/2412.15285/features/compound-crib/2412.15285.md"
    }
  },
  "2412.17758": {
    "id": "2412.17758",
    "title": "In Case You Missed It: ARC 'Challenge' Is Not That Challenging",
    "authors": "Łukasz Borchmann",
    "abstract": "ARC Challenge appears more difficult than ARC Easy for modern LLMs primarily due to an evaluation setup that prevents direct comparison of answer choices rather than inherent complexity. Although some researchers have quietly shifted to a more appropriate scheme over the last year, the implications of this change have yet to be widely acknowledged. We highlight this overlooked shift, show how similar evaluation practices falsely imply reasoning deficits in other benchmarks, and demonstrate that fairer methods dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA). In doing so, we reveal how evaluation shapes perceived difficulty and offer guidelines to ensure that multiple-choice evaluations accurately reflect actual model capabilities.",
    "url": "https://arxiv.org/abs/2412.17758",
    "arxivId": "2412.17758",
    "last_visited": "2024-12-29T08:28:16.461Z",
    "last_read": "2025-01-04T14:49:27.230757",
    "total_reading_time_seconds": 6,
    "published_date": "2024-12-23T18:14:36Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.17758/features/markdown-grobid/2412.17758.md",
      "adr-crib": "data/papers/2412.17758/features/adr-crib/2412.17758.md",
      "adr-titles": "data/papers/2412.17758/features/adr-titles/2412.17758.md",
      "crib-sheet": "data/papers/2412.17758/features/crib-sheet/2412.17758.md",
      "compound-crib": "data/papers/2412.17758/features/compound-crib/2412.17758.md"
    }
  },
  "2412.17805": {
    "id": "2412.17805",
    "title": "Large Motion Video Autoencoding with Cross-modal Video VAE",
    "authors": "Yazhou Xing, Yang Fei, Yingqing He and 4 others",
    "abstract": "Learning a robust video Variational Autoencoder (VAE) is essential for reducing video redundancy and facilitating efficient video generation. Directly applying image VAEs to individual frames in isolation can result in temporal inconsistencies and suboptimal compression rates due to a lack of temporal compression. Existing Video VAEs have begun to address temporal compression; however, they often suffer from inadequate reconstruction performance. In this paper, we present a novel and powerful video autoencoder capable of high-fidelity video encoding. First, we observe that entangling spatial and temporal compression by merely extending the image VAE to a 3D VAE can introduce motion blur and detail distortion artifacts. Thus, we propose temporal-aware spatial compression to better encode and decode the spatial information. Additionally, we integrate a lightweight motion compression model for further temporal compression. Second, we propose to leverage the textual information inherent in text-to-video datasets and incorporate text guidance into our model. This significantly enhances reconstruction quality, particularly in terms of detail preservation and temporal stability. Third, we further improve the versatility of our model through joint training on both images and videos, which not only enhances reconstruction quality but also enables the model to perform both image and video autoencoding. Extensive evaluations against strong recent baselines demonstrate the superior performance of our method. The project website can be found at~\\href{https://yzxing87.github.io/vae/}{https://yzxing87.github.io/vae/}.",
    "url": "https://arxiv.org/abs/2412.17805",
    "arxivId": "2412.17805",
    "last_visited": "2025-01-02T08:03:08.915Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-23T18:58:24Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.17805/features/markdown-grobid/2412.17805.md",
      "adr-crib": "data/papers/2412.17805/features/adr-crib/2412.17805.md",
      "adr-titles": "data/papers/2412.17805/features/adr-titles/2412.17805.md",
      "crib-sheet": "data/papers/2412.17805/features/crib-sheet/2412.17805.md",
      "compound-crib": "data/papers/2412.17805/features/compound-crib/2412.17805.md"
    }
  },
  "2412.17847": {
    "id": "2412.17847",
    "title": "Bridging the Data Provenance Gap Across Text, Speech and Video",
    "authors": "Shayne Longpre, Nikhil Singh, Manuel Cherep and 40 others",
    "abstract": "Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities--popular text, speech, and video datasets--from their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widely-used text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.",
    "url": "https://arxiv.org/abs/2412.17847",
    "arxivId": "2412.17847",
    "last_visited": "2024-12-30T20:03:13.039Z",
    "last_read": "2025-01-04T06:53:21.617516",
    "total_reading_time_seconds": 5,
    "published_date": "2024-12-19T01:30:19Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG",
      "cs.MM"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.17847/features/markdown-grobid/2412.17847.md",
      "adr-crib": "data/papers/2412.17847/features/adr-crib/2412.17847.md",
      "adr-titles": "data/papers/2412.17847/features/adr-titles/2412.17847.md",
      "crib-sheet": "data/papers/2412.17847/features/crib-sheet/2412.17847.md",
      "compound-crib": "data/papers/2412.17847/features/compound-crib/2412.17847.md"
    }
  },
  "2412.18069": {
    "id": "2412.18069",
    "title": "Improving Factuality with Explicit Working Memory",
    "authors": "Mingda Chen, Yang Li, Karthik Padthe and 5 others",
    "abstract": "Large language models can generate factually inaccurate content, a problem known as hallucination. Recent works have built upon retrieved-augmented generation to improve factuality through iterative prompting but these methods are limited by the traditional RAG design. To address these challenges, we introduce EWE (Explicit Working Memory), a novel approach that enhances factuality in long-form text generation by integrating a working memory that receives real-time feedback from external resources. The memory is refreshed based on online fact-checking and retrieval feedback, allowing EWE to rectify false claims during the generation process and ensure more accurate and reliable outputs. Our experiments demonstrate that Ewe outperforms strong baselines on four fact-seeking long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 10 points absolute without sacrificing the helpfulness of the responses. Further analysis reveals that the design of rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are crucial factors for influencing model performance.",
    "url": "https://arxiv.org/abs/2412.18069",
    "arxivId": "2412.18069",
    "last_visited": "2025-01-05T19:03:17.946Z",
    "last_read": "2025-01-05T19:04:47.338057",
    "total_reading_time_seconds": 27,
    "published_date": "2024-12-24T00:55:59Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.18069/features/markdown-grobid/2412.18069.md",
      "adr-crib": "data/papers/2412.18069/features/adr-crib/2412.18069.md",
      "adr-titles": "data/papers/2412.18069/features/adr-titles/2412.18069.md",
      "crib-sheet": "data/papers/2412.18069/features/crib-sheet/2412.18069.md",
      "compound-crib": "data/papers/2412.18069/features/compound-crib/2412.18069.md"
    }
  },
  "2412.18082": {
    "id": "2412.18082",
    "title": "Prompt Tuning for Item Cold-start Recommendation",
    "authors": "Yuezihan Jiang, Gaode Chen, Wenhan Zhang and 6 others",
    "abstract": "The item cold-start problem is crucial for online recommender systems, as the success of the cold-start phase determines whether items can transition into popular ones. Prompt learning, a powerful technique used in natural language processing (NLP) to address zero- or few-shot problems, has been adapted for recommender systems to tackle similar challenges. However, existing methods typically rely on content-based properties or text descriptions for prompting, which we argue may be suboptimal for cold-start recommendations due to 1) semantic gaps with recommender tasks, 2) model bias caused by warm-up items contribute most of the positive feedback to the model, which is the core of the cold-start problem that hinders the recommender quality on cold-start items. We propose to leverage high-value positive feedback, termed pinnacle feedback as prompt information, to simultaneously resolve the above two problems. We experimentally prove that compared to the content description proposed in existing works, the positive feedback is more suitable to serve as prompt information by bridging the semantic gaps. Besides, we propose item-wise personalized prompt networks to encode pinnaclce feedback to relieve the model bias by the positive feedback dominance problem. Extensive experiments on four real-world datasets demonstrate the superiority of our model over state-of-the-art methods. Moreover, PROMO has been successfully deployed on a popular short-video sharing platform, a billion-user scale commercial short-video application, achieving remarkable performance gains across various commercial metrics within cold-start scenarios",
    "url": "https://arxiv.org/abs/2412.18082",
    "arxivId": "2412.18082",
    "last_visited": "2024-12-30T04:50:12.821000+00:00",
    "last_read": "2025-01-04T14:48:54.231751",
    "total_reading_time_seconds": 19,
    "published_date": "2024-12-24T01:38:19Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.18082/features/markdown-grobid/2412.18082.md",
      "adr-crib": "data/papers/2412.18082/features/adr-crib/2412.18082.md",
      "adr-titles": "data/papers/2412.18082/features/adr-titles/2412.18082.md",
      "crib-sheet": "data/papers/2412.18082/features/crib-sheet/2412.18082.md",
      "compound-crib": "data/papers/2412.18082/features/compound-crib/2412.18082.md"
    }
  },
  "2412.18860": {
    "id": "2412.18860",
    "title": "Bootstrap Your Own Context Length",
    "authors": "Liang Wang, Nan Yang, Xingxing Zhang and 2 others",
    "abstract": "We introduce a bootstrapping approach to train long-context language models by exploiting their short-context capabilities only. Our method utilizes a simple agent workflow to synthesize diverse long-context instruction tuning data, thereby eliminating the necessity for manual data collection and annotation. The proposed data synthesis workflow requires only a short-context language model, a text retriever, and a document collection, all of which are readily accessible within the open-source ecosystem. Subsequently, language models are fine-tuned using the synthesized data to extend their context lengths. In this manner, we effectively transfer the short-context capabilities of language models to long-context scenarios through a bootstrapping process. We conduct experiments with the open-source Llama-3 family of models and demonstrate that our method can successfully extend the context length to up to 1M tokens, achieving superior performance across various benchmarks.",
    "url": "https://arxiv.org/abs/2412.18860",
    "arxivId": "2412.18860",
    "last_visited": "2024-12-30T04:37:40.310Z",
    "last_read": "2025-01-04T14:48:54.232387",
    "total_reading_time_seconds": 44,
    "published_date": "2024-12-25T10:08:54Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.IR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.18860/features/markdown-grobid/2412.18860.md",
      "adr-crib": "data/papers/2412.18860/features/adr-crib/2412.18860.md",
      "adr-titles": "data/papers/2412.18860/features/adr-titles/2412.18860.md",
      "crib-sheet": "data/papers/2412.18860/features/crib-sheet/2412.18860.md",
      "compound-crib": "data/papers/2412.18860/features/compound-crib/2412.18860.md"
    }
  },
  "2412.18956": {
    "id": "2412.18956",
    "title": "Musings About the Future of Search: A Return to the Past?",
    "authors": "Jimmy Lin, Pankaj Gupta, Will Horn, Gilad Mishne",
    "abstract": "When you have a question, the most effective way to have the question answered is to directly connect with experts on the topic and have a conversation with them. Prior to the invention of writing, this was the only way. Although effective, this solution exhibits scalability challenges. Writing allowed knowledge to be materialized, preserved, and replicated, enabling the development of different technologies over the centuries to connect information seekers with relevant information. This progression ultimately culminated in the ten-blue-links web search paradigm we're familiar with, just before the recent emergence of generative AI. However, we often forget that consuming static content is an imperfect solution. With the advent of large language models, it has become possible to develop a superior experience by allowing users to directly engage with experts. These interactions can of course satisfy information needs, but expert models can do so much more. This coming future requires reimagining search.",
    "url": "https://arxiv.org/abs/2412.18956",
    "arxivId": "2412.18956",
    "last_visited": "2024-12-30T04:52:50.157000+00:00",
    "last_read": "2025-01-04T14:48:51.239805",
    "total_reading_time_seconds": 25,
    "published_date": "2024-12-25T18:09:34Z",
    "arxiv_tags": [
      "cs.IR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.18956/features/markdown-grobid/2412.18956.md",
      "adr-crib": "data/papers/2412.18956/features/adr-crib/2412.18956.md",
      "adr-titles": "data/papers/2412.18956/features/adr-titles/2412.18956.md",
      "crib-sheet": "data/papers/2412.18956/features/crib-sheet/2412.18956.md",
      "compound-crib": "data/papers/2412.18956/features/compound-crib/2412.18956.md"
    }
  },
  "2412.19442": {
    "id": "2412.19442",
    "title": "A Survey on Large Language Model Acceleration based on KV Cache   Management",
    "authors": "Haoyang Li, Yiming Li, Anxin Tian and 7 others",
    "abstract": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
    "url": "https://arxiv.org/abs/2412.19442",
    "arxivId": "2412.19442",
    "last_visited": "2024-12-30T04:35:23.041Z",
    "last_read": "2025-01-04T14:48:57.232180",
    "total_reading_time_seconds": 20,
    "published_date": "2024-12-27T04:17:57Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.DC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.19442/features/markdown-grobid/2412.19442.md",
      "adr-crib": "data/papers/2412.19442/features/adr-crib/2412.19442.md",
      "adr-titles": "data/papers/2412.19442/features/adr-titles/2412.19442.md",
      "crib-sheet": "data/papers/2412.19442/features/crib-sheet/2412.19442.md",
      "compound-crib": "data/papers/2412.19442/features/compound-crib/2412.19442.md"
    }
  },
  "2412.19792": {
    "id": "2412.19792",
    "title": "InfAlign: Inference-aware language model alignment",
    "authors": "Ananth Balashankar, Ziteng Sun, Jonathan Berant and 9 others",
    "abstract": "Language model alignment has become a critical step in training modern generative language models. The goal of alignment is to finetune a reference model such that the win rate of a sample from the aligned model over a sample from the reference model is high, subject to a KL divergence constraint. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. However, the alignment objective does not capture such inference-time decoding procedures. We show that the existing alignment framework is sub-optimal in view of such inference-time methods. We then modify the alignment objective and propose a framework for inference-aware alignment (IAPO). We prove that for any inference-time decoding algorithm, the optimal solution that optimizes the inference-time win rate of the aligned policy against the reference policy is the solution to the typical RLHF problem with a transformation of the reward. This motivates us to provide the KL-regularized calibrate-and-transform RL (CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. We particularize our study to two important inference-time strategies: best-of-N sampling and best-of-N jailbreaking, where N responses are sampled from the model and the one with the highest or lowest reward is selected. We propose specific transformations for these strategies and demonstrate that our framework offers significant improvements over existing state-of-the-art methods for language model alignment. Empirically, we outperform baselines that are designed without taking inference-time decoding into consideration by 8-12% and 4-9% on inference-time win rates over the Anthropic helpfulness and harmlessness dialog benchmark datasets.",
    "url": "https://arxiv.org/abs/2412.19792",
    "arxivId": "2412.19792",
    "last_visited": "2025-01-03T08:59:10.679Z",
    "last_read": "2025-01-04T06:52:00.645791",
    "total_reading_time_seconds": 49,
    "published_date": "2024-12-27T18:45:36Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.19792/features/markdown-grobid/2412.19792.md",
      "adr-crib": "data/papers/2412.19792/features/adr-crib/2412.19792.md",
      "adr-titles": "data/papers/2412.19792/features/adr-titles/2412.19792.md",
      "crib-sheet": "data/papers/2412.19792/features/crib-sheet/2412.19792.md",
      "compound-crib": "data/papers/2412.19792/features/compound-crib/2412.19792.md"
    }
  },
  "2412.20292": {
    "id": "2412.20292",
    "title": "An analytic theory of creativity in convolutional diffusion models",
    "authors": "Mason Kamb, Surya Ganguli",
    "abstract": "We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \\sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.",
    "url": "https://arxiv.org/abs/2412.20292",
    "arxivId": "2412.20292",
    "last_visited": "2025-01-01T16:00:07.088Z",
    "last_read": "2025-01-04T06:52:30.604171",
    "total_reading_time_seconds": 3,
    "published_date": "2024-12-28T22:33:29Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "q-bio.NC",
      "stat.ML",
      "I.2.10"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.20292/features/markdown-grobid/2412.20292.md",
      "adr-crib": "data/papers/2412.20292/features/adr-crib/2412.20292.md",
      "adr-titles": "data/papers/2412.20292/features/adr-titles/2412.20292.md",
      "crib-sheet": "data/papers/2412.20292/features/crib-sheet/2412.20292.md",
      "compound-crib": "data/papers/2412.20292/features/compound-crib/2412.20292.md"
    }
  },
  "2501.00663": {
    "id": "2501.00663",
    "title": "Titans: Learning to Memorize at Test Time",
    "authors": "Ali Behrouz, Peilin Zhong, Vahab Mirrokni",
    "abstract": "Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.",
    "url": "https://arxiv.org/abs/2501.00663",
    "arxivId": "2501.00663",
    "last_visited": "2025-01-20T07:22:12.333Z",
    "last_read": "2025-01-20T07:22:12.333Z",
    "total_reading_time_seconds": 33,
    "published_date": "2024-12-31T22:32:03Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.00663/features/markdown-grobid/2501.00663.md",
      "adr-crib": "data/papers/2501.00663/features/adr-crib/2501.00663.md",
      "adr-titles": "data/papers/2501.00663/features/adr-titles/2501.00663.md",
      "crib-sheet": "data/papers/2501.00663/features/crib-sheet/2501.00663.md",
      "compound-crib": "data/papers/2501.00663/features/compound-crib/2501.00663.md"
    }
  },
  "2501.02976": {
    "id": "2501.02976",
    "title": "STAR: Spatial-Temporal Augmentation with Text-to-Video Models for   Real-World Video Super-Resolution",
    "authors": "Rui Xie, Yinhong Liu, Penghao Zhou and 7 others",
    "abstract": "Image diffusion models have been adapted for real-world video super-resolution to tackle over-smoothing issues in GAN-based methods. However, these models struggle to maintain temporal consistency, as they are trained on static images, limiting their ability to capture temporal dynamics effectively. Integrating text-to-video (T2V) models into video super-resolution for improved temporal modeling is straightforward. However, two key challenges remain: artifacts introduced by complex degradations in real-world scenarios, and compromised fidelity due to the strong generative capacity of powerful T2V models (\\textit{e.g.}, CogVideoX-5B). To enhance the spatio-temporal quality of restored videos, we introduce\\textbf{~\\name} (\\textbf{S}patial-\\textbf{T}emporal \\textbf{A}ugmentation with T2V models for \\textbf{R}eal-world video super-resolution), a novel approach that leverages T2V models for real-world video super-resolution, achieving realistic spatial details and robust temporal consistency. Specifically, we introduce a Local Information Enhancement Module (LIEM) before the global attention block to enrich local details and mitigate degradation artifacts. Moreover, we propose a Dynamic Frequency (DF) Loss to reinforce fidelity, guiding the model to focus on different frequency components across diffusion steps. Extensive experiments demonstrate\\textbf{~\\name}~outperforms state-of-the-art methods on both synthetic and real-world datasets.",
    "url": "https://arxiv.org/abs/2501.02976",
    "arxivId": "2501.02976",
    "last_visited": "2025-01-10T06:25:50.587Z",
    "last_read": "2025-01-10T06:28:39.452667",
    "total_reading_time_seconds": 46,
    "published_date": "2025-01-06T12:36:21Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.02976/features/markdown-grobid/2501.02976.md",
      "adr-crib": "data/papers/2501.02976/features/adr-crib/2501.02976.md",
      "adr-titles": "data/papers/2501.02976/features/adr-titles/2501.02976.md",
      "crib-sheet": "data/papers/2501.02976/features/crib-sheet/2501.02976.md",
      "compound-crib": "data/papers/2501.02976/features/compound-crib/2501.02976.md"
    }
  },
  "2501.03006": {
    "id": "2501.03006",
    "title": "TransPixar: Advancing Text-to-Video Generation with Transparency",
    "authors": "Luozhou Wang, Yijun Li, Zhifei Chen and 5 others",
    "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixar, a method to extend pretrained video models for RGBA generation while retaining the original RGB capabilities. TransPixar leverages a diffusion transformer (DiT) architecture, incorporating alpha-specific tokens and using LoRA-based fine-tuning to jointly generate RGB and alpha channels with high consistency. By optimizing attention mechanisms, TransPixar preserves the strengths of the original RGB model and achieves strong alignment between RGB and alpha channels despite limited training data. Our approach effectively generates diverse and consistent RGBA videos, advancing the possibilities for VFX and interactive content creation.",
    "url": "https://arxiv.org/abs/2501.03006",
    "arxivId": "2501.03006",
    "last_visited": "2025-01-10T05:23:00.328000+00:00",
    "last_read": "2025-01-10T05:24:23.109329",
    "total_reading_time_seconds": 23,
    "published_date": "2025-01-06T13:32:16Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.03006/features/markdown-grobid/2501.03006.md",
      "adr-crib": "data/papers/2501.03006/features/adr-crib/2501.03006.md",
      "adr-titles": "data/papers/2501.03006/features/adr-titles/2501.03006.md",
      "crib-sheet": "data/papers/2501.03006/features/crib-sheet/2501.03006.md",
      "compound-crib": "data/papers/2501.03006/features/compound-crib/2501.03006.md"
    }
  },
  "2501.03082": {
    "id": "2501.03082",
    "title": "A two-hump spectrum in the prompt emission of GRB 240825A",
    "authors": "Hai-Ming Zhang, Zi-Qi Wang, Cui-Yuan Dai and 4 others",
    "abstract": "An extra hard spectral component that extends to GeV energies, in additional to the typical sub- MeV Band component, appears in several gamma-ray burst (GRBs) detected by Fermi Large Area Telescopes (LAT). Only in one case (i.e., GRB 090926A), a spectral break feature at the high energy end is identified in the extra hard component, but the photon counts are not enough to distinguish between the cutoff model and the broken power law model for the spectral break. In this work, we report the detection of an extra hard component showing the spectral break in GRB 240825A. We find that a broken power-law model fits the spectral data of the extra component better than a single power-law with an exponential cutoff in the time resolved spectrum for the second emission pulse, with a break at about 50 MeV. This spectral feature disfavors the gamma-ray opacity to pair creation as the origin of the spectral break, but points to an intrinsic peak for the extra component. The low ratio between the peak of the extra hard component and that of the Band component challenges the synchrotron self-Compton origin for the extra component. Alternative scenarios, such as the inverse Compton scattering of the photosphere emission, are discussed. In addition, we find a clear transition from the prompt emission to afterglow emission at GeV energies in GRB 240825A, manifested by a temporal steep decay and an unique spectral evolution.",
    "url": "https://arxiv.org/abs/2501.03082",
    "arxivId": "2501.03082",
    "last_visited": "2025-01-13T06:07:18.596Z",
    "last_read": null,
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-06T15:25:59Z",
    "arxiv_tags": [
      "astro-ph.HE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.03082/features/markdown-grobid/2501.03082.md",
      "adr-crib": "data/papers/2501.03082/features/adr-crib/2501.03082.md",
      "adr-titles": "data/papers/2501.03082/features/adr-titles/2501.03082.md",
      "crib-sheet": "data/papers/2501.03082/features/crib-sheet/2501.03082.md",
      "compound-crib": "data/papers/2501.03082/features/compound-crib/2501.03082.md"
    }
  },
  "2501.03847": {
    "id": "2501.03847",
    "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video   Generation Control",
    "authors": "Zekai Gu, Rui Yan, Jiahao Lu and 9 others",
    "abstract": "Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.",
    "url": "https://arxiv.org/abs/2501.03847",
    "arxivId": "2501.03847",
    "last_visited": "2025-01-10T05:52:18.660Z",
    "last_read": "2025-01-10T05:54:15.298855",
    "total_reading_time_seconds": 22,
    "published_date": "2025-01-07T15:01:58Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.03847/features/markdown-grobid/2501.03847.md",
      "adr-crib": "data/papers/2501.03847/features/adr-crib/2501.03847.md",
      "adr-titles": "data/papers/2501.03847/features/adr-titles/2501.03847.md",
      "crib-sheet": "data/papers/2501.03847/features/crib-sheet/2501.03847.md",
      "compound-crib": "data/papers/2501.03847/features/compound-crib/2501.03847.md"
    }
  },
  "2501.04227": {
    "id": "2501.04227",
    "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "authors": "Samuel Schmidgall, Yusheng Su, Ze Wang and 6 others",
    "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
    "url": "https://arxiv.org/abs/2501.04227",
    "arxivId": "2501.04227",
    "last_visited": "2025-01-10T05:47:26.623Z",
    "last_read": "2025-01-10T05:49:05.434811",
    "total_reading_time_seconds": 44,
    "published_date": "2025-01-08T01:58:42Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.04227/features/markdown-grobid/2501.04227.md",
      "adr-crib": "data/papers/2501.04227/features/adr-crib/2501.04227.md",
      "adr-titles": "data/papers/2501.04227/features/adr-titles/2501.04227.md",
      "crib-sheet": "data/papers/2501.04227/features/crib-sheet/2501.04227.md",
      "compound-crib": "data/papers/2501.04227/features/compound-crib/2501.04227.md"
    }
  },
  "2501.04697": {
    "id": "2501.04697",
    "title": "Grokking at the Edge of Numerical Stability",
    "authors": "Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, Tolga Birdal",
    "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting, is a surprising phenomenon challenging our understanding of deep learning. Although significant progress has been made in understanding grokking, the reasons behind the delayed generalization and its dependence on regularization remain unclear. In this work, we argue that without regularization, grokking tasks push models to the edge of numerical stability, introducing floating point errors in the Softmax function, which we refer to as Softmax Collapse (SC). We demonstrate that SC prevents grokking and that mitigating SC enables grokking without regularization. Investigating the root cause of SC, we find that beyond the point of overfitting, the gradients strongly align with what we call the na\\\"ive loss minimization (NLM) direction. This component of the gradient does not alter the model's predictions but decreases the loss by scaling the logits, typically by scaling the weights along their current direction. We show that this scaling of the logits explains the delay in generalization characteristic of grokking and eventually leads to SC, halting further learning. To validate our hypotheses, we introduce two key contributions that address the challenges in grokking tasks: StableMax, a new activation function that prevents SC and enables grokking without regularization, and $\\perp$Grad, a training algorithm that promotes quick generalization in grokking tasks by preventing NLM altogether. These contributions provide new insights into grokking, elucidating its delayed generalization, reliance on regularization, and the effectiveness of existing grokking-inducing methods. Code for this paper is available at https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
    "url": "https://arxiv.org/abs/2501.04697",
    "arxivId": "2501.04697",
    "last_visited": "2025-01-24T08:24:55.685Z",
    "last_read": "2025-01-24T08:24:55.685Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-08T18:58:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.04697/features/markdown-grobid/2501.04697.md",
      "adr-crib": "data/papers/2501.04697/features/adr-crib/2501.04697.md",
      "adr-titles": "data/papers/2501.04697/features/adr-titles/2501.04697.md",
      "crib-sheet": "data/papers/2501.04697/features/crib-sheet/2501.04697.md",
      "compound-crib": "data/papers/2501.04697/features/compound-crib/2501.04697.md"
    }
  },
  "2501.05242": {
    "id": "2501.05242",
    "title": "Scaffold-SLAM: Structured 3D Gaussians for Simultaneous Localization and   Photorealistic Mapping",
    "authors": "Wen Tianci, Liu Zhiang, Lu Biao, Fang Yongchun",
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized novel view synthesis in the Simultaneous Localization and Mapping (SLAM). However, existing SLAM methods utilizing 3DGS have failed to provide high-quality novel view rendering for monocular, stereo, and RGB-D cameras simultaneously. Notably, some methods perform well for RGB-D cameras but suffer significant degradation in rendering quality for monocular cameras. In this paper, we present Scaffold-SLAM, which delivers simultaneous localization and high-quality photorealistic mapping across monocular, stereo, and RGB-D cameras. We introduce two key innovations to achieve this state-of-the-art visual quality. First, we propose Appearance-from-Motion embedding, enabling 3D Gaussians to better model image appearance variations across different camera poses. Second, we introduce a frequency regularization pyramid to guide the distribution of Gaussians, allowing the model to effectively capture finer details in the scene. Extensive experiments on monocular, stereo, and RGB-D datasets demonstrate that Scaffold-SLAM significantly outperforms state-of-the-art methods in photorealistic mapping quality, e.g., PSNR is 16.76% higher in the TUM RGB-D datasets for monocular cameras.",
    "url": "https://arxiv.org/abs/2501.05242",
    "arxivId": "2501.05242",
    "last_visited": "2025-01-11T09:05:35.504Z",
    "last_read": "2025-01-11T09:07:19.697839",
    "total_reading_time_seconds": 3,
    "published_date": "2025-01-09T13:50:26Z",
    "arxiv_tags": [
      "cs.CV",
      "68T40(Primary)68T45, 68U99 (Secondary)",
      "I.4.8; I.3.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.05242/features/markdown-grobid/2501.05242.md",
      "adr-crib": "data/papers/2501.05242/features/adr-crib/2501.05242.md",
      "adr-titles": "data/papers/2501.05242/features/adr-titles/2501.05242.md",
      "crib-sheet": "data/papers/2501.05242/features/crib-sheet/2501.05242.md",
      "compound-crib": "data/papers/2501.05242/features/compound-crib/2501.05242.md"
    }
  },
  "2501.05441": {
    "id": "2501.05441",
    "title": "The GAN is dead; long live the GAN! A Modern GAN Baseline",
    "authors": "Yiwen Huang, Aaron Gokaslan, Volodymyr Kuleshov, James Tompkin",
    "abstract": "There is a widely-spread claim that GANs are difficult to train, and GAN architectures in the literature are littered with empirical tricks. We provide evidence against this claim and build a modern GAN baseline in a more principled manner. First, we derive a well-behaved regularized relativistic GAN loss that addresses issues of mode dropping and non-convergence that were previously tackled via a bag of ad-hoc tricks. We analyze our loss mathematically and prove that it admits local convergence guarantees, unlike most existing relativistic losses. Second, our new loss allows us to discard all ad-hoc tricks and replace outdated backbones used in common GANs with modern architectures. Using StyleGAN2 as an example, we present a roadmap of simplification and modernization that results in a new minimalist baseline -- R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ, ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against state-of-the-art GANs and diffusion models.",
    "url": "https://arxiv.org/abs/2501.05441",
    "arxivId": "2501.05441",
    "last_visited": "2025-01-21T16:24:13.360Z",
    "last_read": "2025-01-21T16:24:13.360Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-09T18:53:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.05441/features/markdown-grobid/2501.05441.md",
      "adr-crib": "data/papers/2501.05441/features/adr-crib/2501.05441.md",
      "adr-titles": "data/papers/2501.05441/features/adr-titles/2501.05441.md",
      "crib-sheet": "data/papers/2501.05441/features/crib-sheet/2501.05441.md",
      "compound-crib": "data/papers/2501.05441/features/compound-crib/2501.05441.md"
    }
  },
  "2501.05743": {
    "id": "2501.05743",
    "title": "Recurrent Features of Amplitudes in Planar $\\mathcal{N}=4$ Super   Yang-Mills Theory",
    "authors": "Tianji Cai, François Charton, Kyle Cranmer and 3 others",
    "abstract": "The planar three-gluon form factor for the chiral stress tensor operator in planar maximally supersymmetric Yang-Mills theory is an analog of the Higgs-to-three-gluon scattering amplitude in QCD. The amplitude (symbol) bootstrap program has provided a wealth of high-loop perturbative data about this form factor, with results up to eight loops available. The symbol of the form factor at $L$ loops is given by words of length $2L$ in six letters with associated integer coefficients. In this paper, we analyze this data, describing patterns of zero coefficients and relations between coefficients. We find many sequences of words whose coefficients are given by closed-form expressions which we expect to be valid at any loop order. Moreover, motivated by our previous machine-learning analysis, we identify simple recursion relations that relate the coefficient of a word to the coefficients of particular lower-loop words. These results open an exciting door for understanding scattering amplitudes at all loop orders.",
    "url": "https://arxiv.org/abs/2501.05743",
    "arxivId": "2501.05743",
    "last_visited": "2025-01-13T04:39:53.737000+00:00",
    "last_read": "2025-01-13T04:41:06.637199",
    "total_reading_time_seconds": 78,
    "published_date": "2025-01-10T06:19:48Z",
    "arxiv_tags": [
      "hep-th",
      "hep-ph"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.05743/features/markdown-grobid/2501.05743.md",
      "adr-crib": "data/papers/2501.05743/features/adr-crib/2501.05743.md",
      "adr-titles": "data/papers/2501.05743/features/adr-titles/2501.05743.md",
      "crib-sheet": "data/papers/2501.05743/features/crib-sheet/2501.05743.md",
      "compound-crib": "data/papers/2501.05743/features/compound-crib/2501.05743.md"
    }
  },
  "2501.06252": {
    "id": "2501.06252",
    "title": "$\\text{Transformer}^2$: Self-adaptive LLMs",
    "authors": "Qi Sun, Edoardo Cetin, Yujin Tang",
    "abstract": "Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce $\\text{Transformer}^2$, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, $\\text{Transformer}^2$ employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific \"expert\" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. $\\text{Transformer}^2$ demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. $\\text{Transformer}^2$ represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.",
    "url": "https://arxiv.org/abs/2501.06252",
    "arxivId": "2501.06252",
    "last_visited": "2025-01-15T05:04:30.386000+00:00",
    "last_read": "2025-01-15T05:05:34.185386",
    "total_reading_time_seconds": 48,
    "published_date": "2025-01-09T01:19:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.06252/features/markdown-grobid/2501.06252.md",
      "adr-crib": "data/papers/2501.06252/features/adr-crib/2501.06252.md",
      "adr-titles": "data/papers/2501.06252/features/adr-titles/2501.06252.md",
      "crib-sheet": "data/papers/2501.06252/features/crib-sheet/2501.06252.md",
      "compound-crib": "data/papers/2501.06252/features/compound-crib/2501.06252.md"
    }
  },
  "2002.09291": {
    "id": "2002.09291",
    "title": "Transformer Hawkes Process",
    "authors": "Simiao Zuo, Haoming Jiang, Zichong Li and 2 others",
    "abstract": "Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.",
    "url": "https://arxiv.org/abs/2002.09291",
    "arxivId": "2002.09291",
    "last_visited": "2025-01-24T07:08:55.938Z",
    "last_read": "2025-01-24T07:08:55.938Z",
    "total_reading_time_seconds": 16,
    "published_date": "2020-02-21T13:48:13Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2002.09291/features/markdown-grobid/2002.09291.md",
      "adr-crib": "data/papers/2002.09291/features/adr-crib/2002.09291.md",
      "adr-titles": "data/papers/2002.09291/features/adr-titles/2002.09291.md",
      "crib-sheet": "data/papers/2002.09291/features/crib-sheet/2002.09291.md",
      "compound-crib": "data/papers/2002.09291/features/compound-crib/2002.09291.md"
    }
  },
  "2002.08521": {
    "id": "2002.08521",
    "title": "Group Network Hawkes Process",
    "authors": "Guanhua Fang, Ganggang Xu, Haochen Xu and 2 others",
    "abstract": "In this work, we study the event occurrences of individuals interacting in a network. To characterize the dynamic interactions among the individuals, we propose a group network Hawkes process (GNHP) model whose network structure is observed and fixed. In particular, we introduce a latent group structure among individuals to account for the heterogeneous user-specific characteristics. A maximum likelihood approach is proposed to simultaneously cluster individuals in the network and estimate model parameters. A fast EM algorithm is subsequently developed by utilizing the branching representation of the proposed GNHP model. Theoretical properties of the resulting estimators of group memberships and model parameters are investigated under both settings when the number of latent groups $G$ is over-specified or correctly specified. A data-driven criterion that can consistently identify the true $G$ under mild conditions is derived. Extensive simulation studies and an application to a data set collected from Sina Weibo are used to illustrate the effectiveness of the proposed methodology.",
    "url": "https://arxiv.org/pdf/2002.08521",
    "arxivId": "2002.08521",
    "last_visited": "2025-01-24T07:07:58.196Z",
    "last_read": "2025-01-24T07:07:58.196Z",
    "total_reading_time_seconds": 0,
    "published_date": "2020-02-20T01:30:42Z",
    "arxiv_tags": [
      "stat.ME",
      "math.ST",
      "stat.TH"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2002.08521/features/markdown-grobid/2002.08521.md",
      "adr-crib": "data/papers/2002.08521/features/adr-crib/2002.08521.md",
      "adr-titles": "data/papers/2002.08521/features/adr-titles/2002.08521.md",
      "crib-sheet": "data/papers/2002.08521/features/crib-sheet/2002.08521.md",
      "compound-crib": "data/papers/2002.08521/features/compound-crib/2002.08521.md"
    }
  },
  "2402.18012": {
    "id": "2402.18012",
    "title": "Diffusion Models as Constrained Samplers for Optimization with Unknown   Constraints",
    "authors": "Lingkai Kong, Yuanqi Du, Wenhao Mu and 8 others",
    "abstract": "Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. Depending on the differentiability of the objective function, we propose two different sampling methods. For differentiable objectives, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dynamics stage for further correction. For non-differentiable objectives, we propose an iterative importance sampling strategy using the diffusion model as the proposal distribution. Comprehensive experiments on a synthetic dataset, six real-world black-box optimization datasets, and a multi-objective molecule optimization dataset show that our method achieves better or comparable performance with previous state-of-the-art baselines.",
    "url": "https://arxiv.org/abs/2402.18012",
    "arxivId": "2402.18012",
    "last_visited": "2025-01-23T01:59:47.272Z",
    "last_read": "2025-01-23T01:59:47.272Z",
    "total_reading_time_seconds": 26,
    "published_date": "2024-02-28T03:09:12Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.18012/features/markdown-grobid/2402.18012.md",
      "adr-crib": "data/papers/2402.18012/features/adr-crib/2402.18012.md",
      "adr-titles": "data/papers/2402.18012/features/adr-titles/2402.18012.md",
      "crib-sheet": "data/papers/2402.18012/features/crib-sheet/2402.18012.md",
      "compound-crib": "data/papers/2402.18012/features/compound-crib/2402.18012.md"
    }
  },
  "2501.12374": {
    "id": "2501.12374",
    "title": "Expertise elevates AI usage: experimental evidence comparing laypeople   and professional artists",
    "authors": "Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola and 3 others",
    "abstract": "Novel capacities of generative AI to analyze and generate cultural artifacts raise inevitable questions about the nature and value of artistic education and human expertise. Has AI already leveled the playing field between professional artists and laypeople, or do trained artistic expressive capacity, curation skills and experience instead enhance the ability to use these new tools? In this pre-registered study, we conduct experimental comparisons between 50 active artists and a demographically matched sample of laypeople. We designed two tasks to approximate artistic practice for testing their capabilities in both faithful and creative image creation: replicating a reference image, and moving as far away as possible from it. We developed a bespoke platform where participants used a modern text-to-image model to complete both tasks. We also collected and compared participants' sentiments towards AI. On average, artists produced more faithful and creative outputs than their lay counterparts, although only by a small margin. While AI may ease content creation, professional expertise is still valuable - even within the confined space of generative AI itself. Finally, we also explored how well an exemplary vision-capable large language model (GPT-4o) would complete the same tasks, if given the role of an image generation agent, and found it performed on par in copying but outperformed even artists in the creative task. The very best results were still produced by humans in both tasks. These outcomes highlight the importance of integrating artistic skills with AI training to prepare artists and other visual professionals for a technologically evolving landscape. We see a potential in collaborative synergy with generative AI, which could reshape creative industries and education in the arts.",
    "url": "https://arxiv.org/abs/2501.12374",
    "arxivId": "2501.12374",
    "last_visited": "2025-01-22T23:29:08.663Z",
    "last_read": "2025-01-22T23:29:08.663Z",
    "total_reading_time_seconds": 3,
    "published_date": "2025-01-21T18:53:21Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.12374/features/markdown-grobid/2501.12374.md",
      "adr-crib": "data/papers/2501.12374/features/adr-crib/2501.12374.md",
      "adr-titles": "data/papers/2501.12374/features/adr-titles/2501.12374.md",
      "crib-sheet": "data/papers/2501.12374/features/crib-sheet/2501.12374.md",
      "compound-crib": "data/papers/2501.12374/features/compound-crib/2501.12374.md"
    }
  },
  "2007.12927": {
    "id": "2007.12927",
    "title": "Neural networks with late-phase weights",
    "authors": "Johannes von Oswald, Seijin Kobayashi, Alexander Meulemans and 3 others",
    "abstract": "The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning.",
    "url": "https://arxiv.org/pdf/2007.12927",
    "arxivId": "2007.12927",
    "last_visited": "2025-01-22T23:00:32.558Z",
    "last_read": "2025-01-22T23:00:32.558Z",
    "total_reading_time_seconds": 630,
    "published_date": "2020-07-25T13:23:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2007.12927/features/markdown-grobid/2007.12927.md",
      "adr-crib": "data/papers/2007.12927/features/adr-crib/2007.12927.md",
      "adr-titles": "data/papers/2007.12927/features/adr-titles/2007.12927.md",
      "crib-sheet": "data/papers/2007.12927/features/crib-sheet/2007.12927.md",
      "compound-crib": "data/papers/2007.12927/features/compound-crib/2007.12927.md"
    }
  },
  "2212.13345": {
    "id": "2212.13345",
    "title": "The Forward-Forward Algorithm: Some Preliminary Investigations",
    "authors": "Geoffrey Hinton",
    "abstract": "The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.",
    "url": "https://arxiv.org/abs/2212.13345",
    "arxivId": "2212.13345",
    "last_visited": "2025-01-22T22:06:19.707Z",
    "last_read": "2025-01-22T22:06:19.707Z",
    "total_reading_time_seconds": 12,
    "published_date": "2022-12-27T02:54:46Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2212.13345/features/markdown-grobid/2212.13345.md",
      "adr-crib": "data/papers/2212.13345/features/adr-crib/2212.13345.md",
      "adr-titles": "data/papers/2212.13345/features/adr-titles/2212.13345.md",
      "crib-sheet": "data/papers/2212.13345/features/crib-sheet/2212.13345.md",
      "compound-crib": "data/papers/2212.13345/features/compound-crib/2212.13345.md"
    }
  },
  "2412.09315": {
    "id": "2412.09315",
    "title": "Beware of Metacognitive Laziness: Effects of Generative Artificial   Intelligence on Learning Motivation, Processes, and Performance",
    "authors": "Yizhou Fan, Luzhen Tang, Huixiao Le and 6 others",
    "abstract": "With the continuous development of technological and educational innovation, learners nowadays can obtain a variety of support from agents such as teachers, peers, education technologies, and recently, generative artificial intelligence such as ChatGPT. The concept of hybrid intelligence is still at a nascent stage, and how learners can benefit from a symbiotic relationship with various agents such as AI, human experts and intelligent learning systems is still unknown. The emerging concept of hybrid intelligence also lacks deep insights and understanding of the mechanisms and consequences of hybrid human-AI learning based on strong empirical research. In order to address this gap, we conducted a randomised experimental study and compared learners' motivations, self-regulated learning processes and learning performances on a writing task among different groups who had support from different agents (ChatGPT, human expert, writing analytics tools, and no extra tool). A total of 117 university students were recruited, and their multi-channel learning, performance and motivation data were collected and analysed. The results revealed that: learners who received different learning support showed no difference in post-task intrinsic motivation; there were significant differences in the frequency and sequences of the self-regulated learning processes among groups; ChatGPT group outperformed in the essay score improvement but their knowledge gain and transfer were not significantly different. Our research found that in the absence of differences in motivation, learners with different supports still exhibited different self-regulated learning processes, ultimately leading to differentiated performance. What is particularly noteworthy is that AI technologies such as ChatGPT may promote learners' dependence on technology and potentially trigger metacognitive laziness.",
    "url": "https://arxiv.org/abs/2412.09315",
    "arxivId": "2412.09315",
    "last_visited": "2025-01-21T20:05:49.177Z",
    "last_read": "2025-01-21T20:05:49.177Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-12T14:32:39Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.HC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.09315/features/markdown-grobid/2412.09315.md",
      "adr-crib": "data/papers/2412.09315/features/adr-crib/2412.09315.md",
      "adr-titles": "data/papers/2412.09315/features/adr-titles/2412.09315.md",
      "crib-sheet": "data/papers/2412.09315/features/crib-sheet/2412.09315.md",
      "compound-crib": "data/papers/2412.09315/features/compound-crib/2412.09315.md"
    }
  },
  "2402.06184": {
    "id": "2402.06184",
    "title": "The boundary of neural network trainability is fractal",
    "authors": "Jascha Sohl-Dickstein",
    "abstract": "Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.",
    "url": "https://arxiv.org/abs/2402.06184",
    "arxivId": "2402.06184",
    "last_visited": "2025-01-20T19:26:32.219Z",
    "last_read": "2025-01-20T19:26:32.219Z",
    "total_reading_time_seconds": 6,
    "published_date": "2024-02-09T04:46:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NE",
      "nlin.CD"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.06184/features/markdown-grobid/2402.06184.md",
      "adr-crib": "data/papers/2402.06184/features/adr-crib/2402.06184.md",
      "adr-titles": "data/papers/2402.06184/features/adr-titles/2402.06184.md",
      "crib-sheet": "data/papers/2402.06184/features/crib-sheet/2402.06184.md",
      "compound-crib": "data/papers/2402.06184/features/compound-crib/2402.06184.md"
    }
  },
  "2410.05229": {
    "id": "2410.05229",
    "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in   Large Language Models",
    "authors": "Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi and 3 others",
    "abstract": "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
    "url": "https://arxiv.org/abs/2410.05229",
    "arxivId": "2410.05229",
    "last_visited": "2025-01-20T09:39:37.110Z",
    "last_read": "2025-01-20T09:39:37.110Z",
    "total_reading_time_seconds": 108,
    "published_date": "2024-10-07T17:36:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.05229/features/markdown-grobid/2410.05229.md",
      "adr-crib": "data/papers/2410.05229/features/adr-crib/2410.05229.md",
      "adr-titles": "data/papers/2410.05229/features/adr-titles/2410.05229.md",
      "crib-sheet": "data/papers/2410.05229/features/crib-sheet/2410.05229.md",
      "compound-crib": "data/papers/2410.05229/features/compound-crib/2410.05229.md"
    }
  },
  "2411.04872": {
    "id": "2411.04872",
    "title": "FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning   in AI",
    "authors": "Elliot Glazer, Ege Erdil, Tamay Besiroglu and 21 others",
    "abstract": "We introduce FrontierMath, a benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians. The questions cover most major branches of modern mathematics -- from computationally intensive problems in number theory and real analysis to abstract questions in algebraic geometry and category theory. Solving a typical problem requires multiple hours of effort from a researcher in the relevant branch of mathematics, and for the upper end questions, multiple days. FrontierMath uses new, unpublished problems and automated verification to reliably evaluate models while minimizing risk of data contamination. Current state-of-the-art AI models solve under 2% of problems, revealing a vast gap between AI capabilities and the prowess of the mathematical community. As AI systems advance toward expert-level mathematical abilities, FrontierMath offers a rigorous testbed that quantifies their progress.",
    "url": "https://arxiv.org/abs/2411.04872",
    "arxivId": "2411.04872",
    "last_visited": "2025-01-20T09:38:38.997Z",
    "last_read": "2025-01-20T09:38:38.997Z",
    "total_reading_time_seconds": 30,
    "published_date": "2024-11-07T17:07:35Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.04872/features/markdown-grobid/2411.04872.md",
      "adr-crib": "data/papers/2411.04872/features/adr-crib/2411.04872.md",
      "adr-titles": "data/papers/2411.04872/features/adr-titles/2411.04872.md",
      "crib-sheet": "data/papers/2411.04872/features/crib-sheet/2411.04872.md",
      "compound-crib": "data/papers/2411.04872/features/compound-crib/2411.04872.md"
    }
  },
  "1910.02054": {
    "id": "1910.02054",
    "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
    "authors": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He",
    "abstract": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.   We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.",
    "url": "https://arxiv.org/abs/1910.02054",
    "arxivId": "1910.02054",
    "last_visited": "2025-01-20T07:32:15.795Z",
    "last_read": "2025-01-20T07:32:15.795Z",
    "total_reading_time_seconds": 0,
    "published_date": "2019-10-04T17:29:39Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1910.02054/features/markdown-grobid/1910.02054.md",
      "adr-crib": "data/papers/1910.02054/features/adr-crib/1910.02054.md",
      "adr-titles": "data/papers/1910.02054/features/adr-titles/1910.02054.md",
      "crib-sheet": "data/papers/1910.02054/features/crib-sheet/1910.02054.md",
      "compound-crib": "data/papers/1910.02054/features/compound-crib/1910.02054.md"
    }
  },
  "2104.07857": {
    "id": "2104.07857",
    "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep   Learning",
    "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley and 2 others",
    "abstract": "In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model.   In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.",
    "url": "https://arxiv.org/abs/2104.07857",
    "arxivId": "2104.07857",
    "last_visited": "2025-01-20T07:31:26.129Z",
    "last_read": "2025-01-20T07:31:26.129Z",
    "total_reading_time_seconds": 10,
    "published_date": "2021-04-16T02:22:12Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2104.07857/features/markdown-grobid/2104.07857.md",
      "adr-crib": "data/papers/2104.07857/features/adr-crib/2104.07857.md",
      "adr-titles": "data/papers/2104.07857/features/adr-titles/2104.07857.md",
      "crib-sheet": "data/papers/2104.07857/features/crib-sheet/2104.07857.md",
      "compound-crib": "data/papers/2104.07857/features/compound-crib/2104.07857.md"
    }
  },
  "2501.09891": {
    "id": "2501.09891",
    "title": "Evolving Deeper LLM Thinking",
    "authors": "Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu and 4 others",
    "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
    "url": "https://arxiv.org/abs/2501.09891",
    "arxivId": "2501.09891",
    "last_visited": "2025-01-20T07:17:04.098Z",
    "last_read": "2025-01-20T07:17:04.098Z",
    "total_reading_time_seconds": 10,
    "published_date": "2025-01-17T00:41:44Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.09891/features/markdown-grobid/2501.09891.md",
      "adr-crib": "data/papers/2501.09891/features/adr-crib/2501.09891.md",
      "adr-titles": "data/papers/2501.09891/features/adr-titles/2501.09891.md",
      "crib-sheet": "data/papers/2501.09891/features/crib-sheet/2501.09891.md",
      "compound-crib": "data/papers/2501.09891/features/compound-crib/2501.09891.md"
    }
  },
  "2403.09635": {
    "id": "2403.09635",
    "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for   Language Models",
    "authors": "Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia and 3 others",
    "abstract": "In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification.",
    "url": "https://arxiv.org/abs/2403.09635",
    "arxivId": "2403.09635",
    "last_visited": "2025-01-20T07:12:49.004Z",
    "last_read": "2025-01-20T07:12:49.004Z",
    "total_reading_time_seconds": 13,
    "published_date": "2024-03-14T17:59:14Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "I.2.7; I.2.10"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2403.09635/features/markdown-grobid/2403.09635.md",
      "adr-crib": "data/papers/2403.09635/features/adr-crib/2403.09635.md",
      "adr-titles": "data/papers/2403.09635/features/adr-titles/2403.09635.md",
      "crib-sheet": "data/papers/2403.09635/features/crib-sheet/2403.09635.md",
      "compound-crib": "data/papers/2403.09635/features/compound-crib/2403.09635.md"
    }
  },
  "2006.08570": {
    "id": "2006.08570",
    "title": "Cross-temporal forecast reconciliation: Optimal combination method and   heuristic alternatives",
    "authors": "Tommaso Di Fonzo, Daniele Girolimetto",
    "abstract": "Forecast reconciliation is a post-forecasting process aimed to improve the quality of the base forecasts for a system of hierarchical/grouped time series (Hyndman et al., 2011). Contemporaneous (cross-sectional) and temporal hierarchies have been considered in the literature, but - except for Kourentzes and Athanasopoulos (2019) - generally these two features have not been fully considered together. Adopting a notation able to simultaneously deal with both forecast reconciliation dimensions, the paper shows two new results: (i) an iterative cross-temporal forecast reconciliation procedure which extends, and overcomes some weaknesses of, the two-step procedure by Kourentzes and Athanasopoulos (2019), and (ii) the closed-form expression of the optimal (in least squares sense) point forecasts which fulfill both contemporaneous and temporal constraints. The feasibility of the proposed procedures, along with first evaluations of their performance as compared to the most performing `single dimension' (either cross-sectional or temporal) forecast reconciliation procedures, is studied through a forecasting experiment on the 95 quarterly time series of the Australian GDP from Income and Expenditure sides considered by Athanasopoulos et al. (2019).",
    "url": "https://arxiv.org/abs/2006.08570",
    "arxivId": "2006.08570",
    "last_visited": "2025-01-20T05:49:19.264Z",
    "last_read": "2025-01-20T05:49:19.264Z",
    "total_reading_time_seconds": 32,
    "published_date": "2020-06-15T17:34:05Z",
    "arxiv_tags": [
      "stat.ME"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.08570/features/markdown-grobid/2006.08570.md",
      "adr-crib": "data/papers/2006.08570/features/adr-crib/2006.08570.md",
      "adr-titles": "data/papers/2006.08570/features/adr-titles/2006.08570.md",
      "crib-sheet": "data/papers/2006.08570/features/crib-sheet/2006.08570.md",
      "compound-crib": "data/papers/2006.08570/features/compound-crib/2006.08570.md"
    }
  },
  "2006.02043": {
    "id": "2006.02043",
    "title": "Hierarchical forecast reconciliation with machine learning",
    "authors": "Evangelos Spiliotis, Mahdi Abolghasemi, Rob J Hyndman and 2 others",
    "abstract": "Hierarchical forecasting methods have been widely used to support aligned decision-making by providing coherent forecasts at different aggregation levels. Traditional hierarchical forecasting approaches, such as the bottom-up and top-down methods, focus on a particular aggregation level to anchor the forecasts. During the past decades, these have been replaced by a variety of linear combination approaches that exploit information from the complete hierarchy to produce more accurate forecasts. However, the performance of these combination methods depends on the particularities of the examined series and their relationships. This paper proposes a novel hierarchical forecasting approach based on machine learning that deals with these limitations in three important ways. First, the proposed method allows for a non-linear combination of the base forecasts, thus being more general than the linear approaches. Second, it structurally combines the objectives of improved post-sample empirical forecasting accuracy and coherence. Finally, due to its non-linear nature, our approach selectively combines the base forecasts in a direct and automated way without requiring that the complete information must be used for producing reconciled forecasts for each series and level. The proposed method is evaluated both in terms of accuracy and bias using two different data sets coming from the tourism and retail industries. Our results suggest that the proposed method gives superior point forecasts than existing approaches, especially when the series comprising the hierarchy are not characterized by the same patterns.",
    "url": "https://arxiv.org/pdf/2006.02043",
    "arxivId": "2006.02043",
    "last_visited": "2025-01-20T00:20:16.561Z",
    "last_read": "2025-01-20T00:20:16.561Z",
    "total_reading_time_seconds": 0,
    "published_date": "2020-06-03T04:49:39Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.02043/features/markdown-grobid/2006.02043.md",
      "adr-crib": "data/papers/2006.02043/features/adr-crib/2006.02043.md",
      "adr-titles": "data/papers/2006.02043/features/adr-titles/2006.02043.md",
      "crib-sheet": "data/papers/2006.02043/features/crib-sheet/2006.02043.md",
      "compound-crib": "data/papers/2006.02043/features/compound-crib/2006.02043.md"
    }
  },
  "1801.02042": {
    "id": "1801.02042",
    "title": "Learning from Neighbors about a Changing State",
    "authors": "Krishna Dasaratha, Benjamin Golub, Nir Hak",
    "abstract": "Agents learn about a changing state using private signals and their neighbors' past estimates of the state. We present a model in which Bayesian agents in equilibrium use neighbors' estimates simply by taking weighted sums with time-invariant weights. The dynamics thus parallel those of the tractable DeGroot model of learning in networks, but arise as an equilibrium outcome rather than a behavioral assumption. We examine whether information aggregation is nearly optimal as neighborhoods grow large. A key condition for this is signal diversity: each individual's neighbors have private signals that not only contain independent information, but also have sufficiently different distributions. Without signal diversity $\\unicode{x2013}$ e.g., if private signals are i.i.d. $\\unicode{x2013}$ learning is suboptimal in all networks and highly inefficient in some. Turning to social influence, we find it is much more sensitive to one's signal quality than to one's number of neighbors, in contrast to standard models with exogenous updating rules.",
    "url": "https://arxiv.org/pdf/1801.02042",
    "arxivId": "1801.02042",
    "last_visited": "2025-01-19T20:59:30.013Z",
    "last_read": "2025-01-19T20:59:30.013Z",
    "total_reading_time_seconds": 0,
    "published_date": "2018-01-06T16:14:47Z",
    "arxiv_tags": [
      "econ.TH",
      "cs.GT",
      "cs.SI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1801.02042/features/markdown-grobid/1801.02042.md",
      "adr-crib": "data/papers/1801.02042/features/adr-crib/1801.02042.md",
      "adr-titles": "data/papers/1801.02042/features/adr-titles/1801.02042.md",
      "crib-sheet": "data/papers/1801.02042/features/crib-sheet/1801.02042.md",
      "compound-crib": "data/papers/1801.02042/features/compound-crib/1801.02042.md"
    }
  },
  "1710.06026": {
    "id": "1710.06026",
    "title": "Targeting Interventions in Networks",
    "authors": "Andrea Galeotti, Benjamin Golub, Sanjeev Goyal",
    "abstract": "We study games in which a network mediates strategic spillovers and externalities among the players. How does a planner optimally target interventions that change individuals' private returns to investment? We analyze this question by decomposing any intervention into orthogonal principal components, which are determined by the network and are ordered according to their associated eigenvalues. There is a close connection between the nature of spillovers and the representation of various principal components in the optimal intervention. In games of strategic complements (substitutes), interventions place more weight on the top (bottom) principal components, which reflect more global (local) network structure. For large budgets, optimal interventions are simple -- they involve a single principal component.",
    "url": "https://arxiv.org/abs/1710.06026",
    "arxivId": "1710.06026",
    "last_visited": "2025-01-19T20:56:52.926Z",
    "last_read": "2025-01-19T20:56:52.926Z",
    "total_reading_time_seconds": 0,
    "published_date": "2017-10-16T23:18:55Z",
    "arxiv_tags": [
      "cs.GT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1710.06026/features/markdown-grobid/1710.06026.md",
      "adr-crib": "data/papers/1710.06026/features/adr-crib/1710.06026.md",
      "adr-titles": "data/papers/1710.06026/features/adr-titles/1710.06026.md",
      "crib-sheet": "data/papers/1710.06026/features/crib-sheet/1710.06026.md",
      "compound-crib": "data/papers/1710.06026/features/compound-crib/1710.06026.md"
    }
  },
  "2307.13912": {
    "id": "2307.13912",
    "title": "Embedding Democratic Values into Social Media AIs via Societal Objective   Functions",
    "authors": "Chenyan Jia, Michelle S. Lam, Minh Chau Mai and 2 others",
    "abstract": "Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.",
    "url": "https://arxiv.org/abs/2307.13912",
    "arxivId": "2307.13912",
    "last_visited": "2025-01-19T15:26:24.313Z",
    "last_read": "2025-01-19T15:26:24.313Z",
    "total_reading_time_seconds": 9,
    "published_date": "2023-07-26T02:27:24Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2307.13912/features/markdown-grobid/2307.13912.md",
      "adr-crib": "data/papers/2307.13912/features/adr-crib/2307.13912.md",
      "adr-titles": "data/papers/2307.13912/features/adr-titles/2307.13912.md",
      "crib-sheet": "data/papers/2307.13912/features/crib-sheet/2307.13912.md",
      "compound-crib": "data/papers/2307.13912/features/compound-crib/2307.13912.md"
    }
  },
  "2405.07987": {
    "id": "2405.07987",
    "title": "The Platonic Representation Hypothesis",
    "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
    "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
    "url": "https://arxiv.org/abs/2405.07987",
    "arxivId": "2405.07987",
    "last_visited": "2025-01-25T03:33:15.181Z",
    "last_read": "2025-01-25T03:33:15.181Z",
    "total_reading_time_seconds": 203,
    "published_date": "2024-05-13T17:58:30Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.07987/features/markdown-grobid/2405.07987.md",
      "adr-crib": "data/papers/2405.07987/features/adr-crib/2405.07987.md",
      "adr-titles": "data/papers/2405.07987/features/adr-titles/2405.07987.md",
      "crib-sheet": "data/papers/2405.07987/features/crib-sheet/2405.07987.md",
      "compound-crib": "data/papers/2405.07987/features/compound-crib/2405.07987.md"
    }
  },
  "2501.13928": {
    "id": "2501.13928",
    "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
    "authors": "Jianing Yang, Alexander Sax, Kevin J. Liang and 6 others",
    "abstract": "Multi-view 3D reconstruction remains a core challenge in computer vision, particularly in applications requiring accurate and scalable representations across diverse perspectives. Current leading methods such as DUSt3R employ a fundamentally pairwise approach, processing images in pairs and necessitating costly global alignment procedures to reconstruct from multiple views. In this work, we propose Fast 3D Reconstruction (Fast3R), a novel multi-view generalization to DUSt3R that achieves efficient and scalable 3D reconstruction by processing many views in parallel. Fast3R's Transformer-based architecture forwards N images in a single forward pass, bypassing the need for iterative alignment. Through extensive experiments on camera pose estimation and 3D reconstruction, Fast3R demonstrates state-of-the-art performance, with significant improvements in inference speed and reduced error accumulation. These results establish Fast3R as a robust alternative for multi-view applications, offering enhanced scalability without compromising reconstruction accuracy.",
    "url": "https://arxiv.org/abs/2501.13928",
    "arxivId": "2501.13928",
    "last_visited": "2025-01-26T06:21:36.654Z",
    "last_read": "2025-01-26T06:21:36.654Z",
    "total_reading_time_seconds": 27,
    "published_date": "2025-01-23T18:59:55Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.RO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.13928/features/markdown-grobid/2501.13928.md",
      "adr-crib": "data/papers/2501.13928/features/adr-crib/2501.13928.md",
      "adr-titles": "data/papers/2501.13928/features/adr-titles/2501.13928.md",
      "crib-sheet": "data/papers/2501.13928/features/crib-sheet/2501.13928.md",
      "compound-crib": "data/papers/2501.13928/features/compound-crib/2501.13928.md"
    }
  },
  "2501.12005": {
    "id": "2501.12005",
    "title": "A note on the relations between mixture models, maximum-likelihood and   entropic optimal transport",
    "authors": "Titouan Vayer, Etienne Lasalle",
    "abstract": "This note aims to demonstrate that performing maximum-likelihood estimation for a mixture model is equivalent to minimizing over the parameters an optimal transport problem with entropic regularization. The objective is pedagogical: we seek to present this already known result in a concise and hopefully simple manner. We give an illustration with Gaussian mixture models by showing that the standard EM algorithm is a specific block-coordinate descent on an optimal transport loss.",
    "url": "https://arxiv.org/abs/2501.12005",
    "arxivId": "2501.12005",
    "last_visited": "2025-01-26T16:13:03.441Z",
    "last_read": "2025-01-26T16:13:03.441Z",
    "total_reading_time_seconds": 20,
    "published_date": "2025-01-21T09:55:21Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.12005/features/markdown-grobid/2501.12005.md",
      "adr-crib": "data/papers/2501.12005/features/adr-crib/2501.12005.md",
      "adr-titles": "data/papers/2501.12005/features/adr-titles/2501.12005.md",
      "crib-sheet": "data/papers/2501.12005/features/crib-sheet/2501.12005.md",
      "compound-crib": "data/papers/2501.12005/features/compound-crib/2501.12005.md"
    }
  },
  "1805.03929": {
    "id": "1805.03929",
    "title": "Resource-Bounded Kolmogorov Complexity Provides an Obstacle to Soficness   of Multidimensional Shifts",
    "authors": "Julien Destombes, Andrei Romashchenko",
    "abstract": "We suggest necessary conditions of soficness of multidimensional shifts formulated in termsof resource-bounded Kolmogorov complexity. Using this technique we provide examples ofeffective and non-sofic shifts on $\\mathbb{Z}^2$ with very low block complexity: the number of globallyadmissible patterns of size $n\\times n$ grows only as a polynomial in $n$. We also show that moreconventional proofs of non-soficness for multi-dimensional effective shifts can be expressed interms of Kolmogorov complexity with unbounded computational resources.",
    "url": "https://arxiv.org/abs/1805.03929",
    "arxivId": "1805.03929",
    "last_visited": "2025-01-27T02:00:10.459Z",
    "last_read": "2025-01-27T02:00:10.459Z",
    "total_reading_time_seconds": 14,
    "published_date": "2018-05-10T11:47:47Z",
    "arxiv_tags": [
      "cs.DM",
      "cs.CC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1805.03929/features/markdown-grobid/1805.03929.md",
      "adr-crib": "data/papers/1805.03929/features/adr-crib/1805.03929.md",
      "adr-titles": "data/papers/1805.03929/features/adr-titles/1805.03929.md",
      "crib-sheet": "data/papers/1805.03929/features/crib-sheet/1805.03929.md",
      "compound-crib": "data/papers/1805.03929/features/compound-crib/1805.03929.md"
    }
  },
  "2003.13176": {
    "id": "2003.13176",
    "title": "Non-reciprocal phase transitions",
    "authors": "Michel Fruchart, Ryo Hanai, Peter B. Littlewood, Vincenzo Vitelli",
    "abstract": "Out of equilibrium, the lack of reciprocity is the rule rather than the exception. Non-reciprocal interactions occur, for instance, in networks of neurons, directional growth of interfaces, and synthetic active materials. While wave propagation in non-reciprocal media has recently been under intense study, less is known about the consequences of non-reciprocity on the collective behavior of many-body systems. Here, we show that non-reciprocity leads to time-dependent phases where spontaneously broken symmetries are dynamically restored. The resulting phase transitions are controlled by spectral singularities called exceptional points. We describe the emergence of these phases using insights from bifurcation theory and non-Hermitian quantum mechanics. Our approach captures non-reciprocal generalizations of three archetypal classes of self-organization out of equilibrium: synchronization, flocking and pattern formation. Collective phenomena in these non-reciprocal systems range from active time-(quasi)crystals to exceptional-point enforced pattern-formation and hysteresis. Our work paves the way towards a general theory of critical phenomena in non-reciprocal matter.",
    "url": "https://arxiv.org/pdf/2003.13176",
    "arxivId": "2003.13176",
    "last_visited": "2025-01-27T06:07:05.033Z",
    "last_read": "2025-01-27T06:07:05.033Z",
    "total_reading_time_seconds": 77,
    "published_date": "2020-03-30T01:09:20Z",
    "arxiv_tags": [
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2003.13176/features/markdown-grobid/2003.13176.md",
      "adr-crib": "data/papers/2003.13176/features/adr-crib/2003.13176.md",
      "adr-titles": "data/papers/2003.13176/features/adr-titles/2003.13176.md",
      "crib-sheet": "data/papers/2003.13176/features/crib-sheet/2003.13176.md",
      "compound-crib": "data/papers/2003.13176/features/compound-crib/2003.13176.md"
    }
  },
  "1309.6605": {
    "id": "1309.6605",
    "title": "Symmetries, Cluster Synchronization, and Isolated Desynchronization in   Complex Networks",
    "authors": "Louis M. Pecora, Francesco Sorrentino, Aaron M. Hagerstrom and 2 others",
    "abstract": "Synchronization is of central importance in power distribution, telecommunication, neuronal, and biological networks. Many networks are observed to produce patterns of synchronized clusters, but it has been difficult to predict these clusters or understand the conditions under which they form, except for in the simplest of networks. In this article, we shed light on the intimate connection between network symmetry and cluster synchronization. We introduce general techniques that use network symmetries to reveal the patterns of synchronized clusters and determine the conditions under which they persist. The connection between symmetry and cluster synchronization is experimentally explored using an electro-optic network. We experimentally observe and theoretically predict a surprising phenomenon in which some clusters lose synchrony while leaving others synchronized. The results could guide the design of new power grid systems or lead to new understanding of the dynamical behavior of networks ranging from neural to social.",
    "url": "https://arxiv.org/abs/1309.6605",
    "arxivId": "1309.6605",
    "last_visited": "2025-01-27T07:09:09.367Z",
    "last_read": "2025-01-27T07:09:09.367Z",
    "total_reading_time_seconds": 3,
    "published_date": "2013-09-25T18:42:34Z",
    "arxiv_tags": [
      "nlin.CD"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1309.6605/features/markdown-grobid/1309.6605.md",
      "adr-crib": "data/papers/1309.6605/features/adr-crib/1309.6605.md",
      "adr-titles": "data/papers/1309.6605/features/adr-titles/1309.6605.md",
      "crib-sheet": "data/papers/1309.6605/features/crib-sheet/1309.6605.md",
      "compound-crib": "data/papers/1309.6605/features/compound-crib/1309.6605.md"
    }
  },
  "0806.0594": {
    "id": "0806.0594",
    "title": "Solvable model for chimera states of coupled oscillators",
    "authors": "Daniel M. Abrams, Renato E. Mirollo, Steven H. Strogatz, Daniel A. Wiley",
    "abstract": "Networks of identical, symmetrically coupled oscillators can spontaneously split into synchronized and desynchronized sub-populations. Such chimera states were discovered in 2002, but are not well understood theoretically. Here we obtain the first exact results about the stability, dynamics, and bifurcations of chimera states by analyzing a minimal model consisting of two interacting populations of oscillators. Along with a completely synchronous state, the system displays stable chimeras, breathing chimeras, and saddle-node, Hopf and homoclinic bifurcations of chimeras.",
    "url": "https://arxiv.org/pdf/0806.0594",
    "arxivId": "0806.0594",
    "last_visited": "2025-01-27T07:05:40.255Z",
    "last_read": "2025-01-27T07:05:40.255Z",
    "total_reading_time_seconds": 0,
    "published_date": "2008-06-03T17:26:51Z",
    "arxiv_tags": [
      "nlin.CD",
      "math.DS",
      "nlin.PS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/0806.0594/features/markdown-grobid/0806.0594.md",
      "adr-crib": "data/papers/0806.0594/features/adr-crib/0806.0594.md",
      "adr-titles": "data/papers/0806.0594/features/adr-titles/0806.0594.md",
      "crib-sheet": "data/papers/0806.0594/features/crib-sheet/0806.0594.md",
      "compound-crib": "data/papers/0806.0594/features/compound-crib/0806.0594.md"
    }
  },
  "1101.2899": {
    "id": "1101.2899",
    "title": "A mathematical framework for critical transitions: bifurcations,   fast-slow systems and stochastic dynamics",
    "authors": "Christian Kuehn",
    "abstract": "Bifurcations can cause dynamical systems with slowly varying parameters to transition to far-away attractors. The terms ``critical transition'' or ``tipping point'' have been used to describe this situation. Critical transitions have been observed in an astonishingly diverse set of applications from ecosystems and climate change to medicine and finance. The main goal of this paper is to give an overview which standard mathematical theories can be applied to critical transitions. We shall focus on early-warning signs that have been suggested to predict critical transitions and point out what mathematical theory can provide in this context. Starting from classical bifurcation theory and incorporating multiple time scale dynamics one can give a detailed analysis of local bifurcations that induce critical transitions. We suggest that the mathematical theory of fast-slow systems provides a natural definition of critical transitions. Since noise often plays a crucial role near critical transitions the next step is to consider stochastic fast-slow systems. The interplay between sample path techniques, partial differential equations and random dynamical systems is highlighted. Each viewpoint provides potential early-warning signs for critical transitions. Since increasing variance has been suggested as an early-warning sign we examine it in the context of normal forms analytically, numerically and geometrically; we also consider autocorrelation numerically. Hence we demonstrate the applicability of early-warning signs for generic models. We end with suggestions for future directions of the theory.",
    "url": "https://arxiv.org/pdf/1101.2899",
    "arxivId": "1101.2899",
    "last_visited": "2025-01-27T07:03:04.997Z",
    "last_read": "2025-01-27T07:03:04.997Z",
    "total_reading_time_seconds": 0,
    "published_date": "2011-01-14T21:00:57Z",
    "arxiv_tags": [
      "math.DS",
      "math.CA",
      "nlin.CD",
      "nlin.PS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1101.2899/features/markdown-grobid/1101.2899.md",
      "adr-crib": "data/papers/1101.2899/features/adr-crib/1101.2899.md",
      "adr-titles": "data/papers/1101.2899/features/adr-titles/1101.2899.md",
      "crib-sheet": "data/papers/1101.2899/features/crib-sheet/1101.2899.md",
      "compound-crib": "data/papers/1101.2899/features/compound-crib/1101.2899.md"
    }
  },
  "2406.10165": {
    "id": "2406.10165",
    "title": "CarLLaVA: Vision language models for camera-only closed-loop driving",
    "authors": "Katrin Renz, Long Chen, Ana-Maria Marcu and 6 others",
    "abstract": "In this technical report, we present CarLLaVA, a Vision Language Model (VLM) for autonomous driving, developed for the CARLA Autonomous Driving Challenge 2.0. CarLLaVA uses the vision encoder of the LLaVA VLM and the LLaMA architecture as backbone, achieving state-of-the-art closed-loop driving performance with only camera input and without the need for complex or expensive labels. Additionally, we show preliminary results on predicting language commentary alongside the driving output. CarLLaVA uses a semi-disentangled output representation of both path predictions and waypoints, getting the advantages of the path for better lateral control and the waypoints for better longitudinal control. We propose an efficient training recipe to train on large driving datasets without wasting compute on easy, trivial data. CarLLaVA ranks 1st place in the sensor track of the CARLA Autonomous Driving Challenge 2.0 outperforming the previous state of the art by 458% and the best concurrent submission by 32.6%.",
    "url": "https://arxiv.org/abs/2406.10165",
    "arxivId": "2406.10165",
    "last_visited": "2025-01-28T06:35:56.917Z",
    "last_read": "2025-01-28T06:35:56.917Z",
    "total_reading_time_seconds": 9,
    "published_date": "2024-06-14T16:35:47Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.RO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.10165/features/markdown-grobid/2406.10165.md",
      "adr-crib": "data/papers/2406.10165/features/adr-crib/2406.10165.md",
      "adr-titles": "data/papers/2406.10165/features/adr-titles/2406.10165.md",
      "crib-sheet": "data/papers/2406.10165/features/crib-sheet/2406.10165.md",
      "compound-crib": "data/papers/2406.10165/features/compound-crib/2406.10165.md"
    }
  },
  "2405.11932": {
    "id": "2405.11932",
    "title": "Nonequilbrium physics of generative diffusion models",
    "authors": "Zhendong Yu, Haiping Huang",
    "abstract": "Generative diffusion models apply the concept of Langevin dynamics in physics to machine leaning, attracting a lot of interests from engineering, statistics and physics, but a complete picture about inherent mechanisms is still lacking. In this paper, we provide a transparent physics analysis of diffusion models, formulating the fluctuation theorem, entropy production, equilibrium measure, and Franz-Parisi potential to understand the dynamic process and intrinsic phase transitions. Our analysis is rooted in a path integral representation of both forward and backward dynamics, and in treating the reverse diffusion generative process as a statistical inference, where the time-dependent state variables serve as quenched disorder akin to that in spin glass theory. Our study thus links stochastic thermodynamics, statistical inference and geometry based analysis together to yield a coherent picture about how the generative diffusion models work.",
    "url": "https://arxiv.org/abs/2405.11932v3",
    "arxivId": "2405.11932",
    "last_visited": "2025-01-28T09:53:25.159Z",
    "last_read": "2025-01-28T09:53:25.159Z",
    "total_reading_time_seconds": 11,
    "published_date": "2024-05-20T10:16:26Z",
    "arxiv_tags": [
      "cond-mat.stat-mech",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.11932/features/markdown-grobid/2405.11932.md",
      "adr-crib": "data/papers/2405.11932/features/adr-crib/2405.11932.md",
      "adr-titles": "data/papers/2405.11932/features/adr-titles/2405.11932.md",
      "crib-sheet": "data/papers/2405.11932/features/crib-sheet/2405.11932.md",
      "compound-crib": "data/papers/2405.11932/features/compound-crib/2405.11932.md"
    }
  },
  "2501.17161": {
    "id": "2501.17161",
    "title": "SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model   Post-training",
    "authors": "Tianzhe Chu, Yuexiang Zhai, Jihan Yang and 6 others",
    "abstract": "Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.",
    "url": "https://arxiv.org/abs/2501.17161",
    "arxivId": "2501.17161",
    "last_visited": "2025-01-29T07:56:12.258Z",
    "last_read": "2025-01-29T07:56:12.258Z",
    "total_reading_time_seconds": 6,
    "published_date": "2025-01-28T18:59:44Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.17161/features/markdown-grobid/2501.17161.md",
      "adr-crib": "data/papers/2501.17161/features/adr-crib/2501.17161.md",
      "adr-titles": "data/papers/2501.17161/features/adr-titles/2501.17161.md",
      "crib-sheet": "data/papers/2501.17161/features/crib-sheet/2501.17161.md",
      "compound-crib": "data/papers/2501.17161/features/compound-crib/2501.17161.md"
    }
  },
  "1408.3060": {
    "id": "1408.3060",
    "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time",
    "authors": "Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola",
    "abstract": "Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that storing and computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks proposed by Rahimi and Recht (2009) and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy.   Our method applies to any translation invariant and any dot-product kernel, such as the popular RBF kernels and polynomial kernels. We prove that the approximation is unbiased and has low variance. Experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction.",
    "url": "https://arxiv.org/abs/1408.3060",
    "arxivId": "1408.3060",
    "last_visited": "2025-01-30T04:03:50.839Z",
    "last_read": "2025-01-30T04:03:50.839Z",
    "total_reading_time_seconds": 14,
    "published_date": "2014-08-13T17:37:43Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1408.3060/features/markdown-grobid/1408.3060.md",
      "adr-crib": "data/papers/1408.3060/features/adr-crib/1408.3060.md",
      "adr-titles": "data/papers/1408.3060/features/adr-titles/1408.3060.md",
      "crib-sheet": "data/papers/1408.3060/features/crib-sheet/1408.3060.md",
      "compound-crib": "data/papers/1408.3060/features/compound-crib/1408.3060.md"
    }
  },
  "2402.03300": {
    "id": "2402.03300",
    "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open   Language Models",
    "authors": "Zhihong Shao, Peiyi Wang, Qihao Zhu and 8 others",
    "abstract": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
    "url": "https://arxiv.org/abs/2402.03300",
    "arxivId": "2402.03300",
    "last_visited": "2025-01-30T07:03:06.031Z",
    "last_read": "2025-01-30T07:03:06.031Z",
    "total_reading_time_seconds": 8,
    "published_date": "2024-02-05T18:55:32Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.03300/features/markdown-grobid/2402.03300.md",
      "adr-crib": "data/papers/2402.03300/features/adr-crib/2402.03300.md",
      "adr-titles": "data/papers/2402.03300/features/adr-titles/2402.03300.md",
      "crib-sheet": "data/papers/2402.03300/features/crib-sheet/2402.03300.md",
      "compound-crib": "data/papers/2402.03300/features/compound-crib/2402.03300.md"
    }
  },
  "2501.16975": {
    "id": "2501.16975",
    "title": "Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling",
    "authors": "Hongzhi Huang, Defa Zhu, Banggu Wu and 4 others",
    "abstract": "Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.",
    "url": "https://arxiv.org/pdf/2501.16975",
    "arxivId": "2501.16975",
    "last_visited": "2025-01-30T15:06:14.633Z",
    "last_read": "2025-01-30T15:06:14.633Z",
    "total_reading_time_seconds": 36,
    "published_date": "2025-01-28T14:15:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.16975/features/markdown-grobid/2501.16975.md",
      "adr-crib": "data/papers/2501.16975/features/adr-crib/2501.16975.md",
      "adr-titles": "data/papers/2501.16975/features/adr-titles/2501.16975.md",
      "crib-sheet": "data/papers/2501.16975/features/crib-sheet/2501.16975.md",
      "compound-crib": "data/papers/2501.16975/features/compound-crib/2501.16975.md"
    }
  },
  "2306.16830": {
    "id": "2306.16830",
    "title": "Sampling weights of deep neural networks",
    "authors": "Erik Lien Bolager, Iryna Burak, Chinmay Datar and 2 others",
    "abstract": "We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data to sample shallow and deep networks. We prove that sampled networks are universal approximators. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. Our sampling scheme is invariant to rigid body transformations and scaling of the input data, which implies many popular pre-processing techniques are not required. In numerical experiments, we demonstrate that sampled networks achieve accuracy comparable to iteratively trained ones, but can be constructed orders of magnitude faster. Our test cases involve a classification benchmark from OpenML, sampling of neural operators to represent maps in function spaces, and transfer learning using well-known architectures.",
    "url": "https://arxiv.org/abs/2306.16830",
    "arxivId": "2306.16830",
    "last_visited": "2025-01-31T07:03:19.679Z",
    "last_read": "2025-01-31T07:03:19.679Z",
    "total_reading_time_seconds": 21,
    "published_date": "2023-06-29T10:13:36Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T07",
      "G.1; G.3"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2306.16830/features/markdown-grobid/2306.16830.md",
      "adr-crib": "data/papers/2306.16830/features/adr-crib/2306.16830.md",
      "adr-titles": "data/papers/2306.16830/features/adr-titles/2306.16830.md",
      "crib-sheet": "data/papers/2306.16830/features/crib-sheet/2306.16830.md",
      "compound-crib": "data/papers/2306.16830/features/compound-crib/2306.16830.md"
    }
  },
  "1812.06162": {
    "id": "1812.06162",
    "title": "An Empirical Model of Large-Batch Training",
    "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, OpenAI Dota Team",
    "abstract": "In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",
    "url": "https://arxiv.org/abs/1812.06162",
    "arxivId": "1812.06162",
    "last_visited": "2025-01-31T08:42:27.593Z",
    "last_read": "2025-01-31T08:42:27.593Z",
    "total_reading_time_seconds": 23,
    "published_date": "2018-12-14T20:49:09Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1812.06162/features/markdown-grobid/1812.06162.md",
      "adr-crib": "data/papers/1812.06162/features/adr-crib/1812.06162.md",
      "adr-titles": "data/papers/1812.06162/features/adr-titles/1812.06162.md",
      "crib-sheet": "data/papers/1812.06162/features/crib-sheet/1812.06162.md",
      "compound-crib": "data/papers/1812.06162/features/compound-crib/1812.06162.md"
    }
  },
  "2501.06623": {
    "id": "2501.06623",
    "title": "Nuclear Explosions for Large Scale Carbon Sequestration",
    "authors": "Andrew Haverly",
    "abstract": "Confronting the escalating threat of climate change requires innovative and large-scale interventions. This paper presents a bold proposal to employ a buried nuclear explosion in a remote basaltic seabed for pulverizing basalt, thereby accelerating carbon sequestration through Enhanced Rock Weathering (ERW). By precisely locating the explosion beneath the seabed, we aim to confine debris, radiation, and energy while ensuring rapid rock weathering at a scale substantial enough to make a meaningful dent in atmospheric carbon levels. Our analysis outlines the parameters essential for efficient carbon capture and minimal collateral effects, emphasizing that a yield on the order of gigatons is critical for global climate impact. Although this approach may appear radical, we illustrate its feasibility by examining safety factors, preservation of local ecosystems, political considerations, and financial viability. This work argues for reimagining nuclear technology not merely as a destructive force but as a potential catalyst for decarbonization, thereby inviting further exploration of pioneering solutions in the fight against climate change.",
    "url": "https://arxiv.org/html/2501.06623v1#S1",
    "arxivId": "2501.06623",
    "last_visited": "2025-01-31T20:41:31.947Z",
    "last_read": "2025-01-31T20:41:31.947Z",
    "total_reading_time_seconds": 8,
    "published_date": "2025-01-11T19:18:00Z",
    "arxiv_tags": [
      "physics.soc-ph",
      "physics.ao-ph"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.06623/features/markdown-grobid/2501.06623.md",
      "adr-crib": "data/papers/2501.06623/features/adr-crib/2501.06623.md",
      "adr-titles": "data/papers/2501.06623/features/adr-titles/2501.06623.md",
      "crib-sheet": "data/papers/2501.06623/features/crib-sheet/2501.06623.md",
      "compound-crib": "data/papers/2501.06623/features/compound-crib/2501.06623.md"
    }
  },
  "2403.01903": {
    "id": "2403.01903",
    "title": "Online Locality Meets Distributed Quantum Computing",
    "authors": "Amirreza Akbari, Xavier Coiteux-Roy, Francesco d'Amore and 8 others",
    "abstract": "We connect three distinct lines of research that have recently explored extensions of the classical LOCAL model of distributed computing: A. distributed quantum computing and non-signaling distributions [e.g. STOC 2024], B. finitely-dependent processes [e.g. Forum Math. Pi 2016], and C. locality in online graph algorithms and dynamic graph algorithms [e.g. ICALP 2023].   We prove new results on the capabilities and limitations of all of these models of computing, for locally checkable labeling problems (LCLs). We show that all these settings can be sandwiched between the classical LOCAL model and what we call the randomized online-LOCAL model. Our work implies limitations on the quantum advantage in the distributed setting, and we also exhibit a new barrier for proving tighter bounds. Our main technical results are these: 1. All LCL problems solvable with locality $O(\\log^\\star n)$ in the classical deterministic LOCAL model admit a finitely-dependent distribution with locality $O(1)$. This answers an open question by Holroyd [2024], and also presents a new barrier for proving bounds on distributed quantum advantage using causality-based arguments. 2. In rooted trees, if we can solve an LCL problem with locality $o(\\log \\log \\log n)$ in the randomized online-LOCAL model (or any of the weaker models, such as quantum-LOCAL), we can solve it with locality $O(\\log^\\star n)$ in the classical deterministic LOCAL model. One of many implications is that in rooted trees, $O(\\log^\\star n)$ locality in quantum-LOCAL is not stronger than $O(\\log^\\star n)$ locality in classical LOCAL.",
    "url": "https://arxiv.org/abs/2403.01903",
    "arxivId": "2403.01903",
    "last_visited": "2025-01-31T23:33:29.641Z",
    "last_read": "2025-01-31T23:33:29.641Z",
    "total_reading_time_seconds": 5,
    "published_date": "2024-03-04T10:03:54Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.CC",
      "math.PR",
      "quant-ph"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2403.01903/features/markdown-grobid/2403.01903.md",
      "adr-crib": "data/papers/2403.01903/features/adr-crib/2403.01903.md",
      "adr-titles": "data/papers/2403.01903/features/adr-titles/2403.01903.md",
      "crib-sheet": "data/papers/2403.01903/features/crib-sheet/2403.01903.md",
      "compound-crib": "data/papers/2403.01903/features/compound-crib/2403.01903.md"
    }
  },
  "2501.18388": {
    "id": "2501.18388",
    "title": "Improved Replicable Boosting with Majority-of-Majorities",
    "authors": "Kasper Green Larsen, Markus Engelund Mathiasen, Clement Svendsen",
    "abstract": "We introduce a new replicable boosting algorithm which significantly improves the sample complexity compared to previous algorithms. The algorithm works by doing two layers of majority voting, using an improved version of the replicable boosting algorithm introduced by Impagliazzo et al. [2022] in the bottom layer.",
    "url": "https://arxiv.org/abs/2501.18388",
    "arxivId": "2501.18388",
    "last_visited": "2025-01-31T23:31:40.811Z",
    "last_read": "2025-01-31T23:31:40.811Z",
    "total_reading_time_seconds": 33,
    "published_date": "2025-01-30T14:38:26Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.18388/features/markdown-grobid/2501.18388.md",
      "adr-crib": "data/papers/2501.18388/features/adr-crib/2501.18388.md",
      "adr-titles": "data/papers/2501.18388/features/adr-titles/2501.18388.md",
      "crib-sheet": "data/papers/2501.18388/features/crib-sheet/2501.18388.md",
      "compound-crib": "data/papers/2501.18388/features/compound-crib/2501.18388.md"
    }
  },
  "2409.02668": {
    "id": "2409.02668",
    "title": "Introduction to Machine Learning",
    "authors": "Laurent Younes",
    "abstract": "This book introduces the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used in machine learning. It starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability and also introduces some measure theoretic terminology, which can be used as a reading guide for the sections that use these tools. The introductory chapters also provide background material on matrix analysis and optimization. The latter chapter provides theoretical support to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc. After discussing basic concepts for statistical prediction, the book includes an introduction to reproducing kernel theory and Hilbert space techniques, which are used in many places, before addressing the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks. The subject then switches to generative methods, starting with a chapter that presents sampling methods and an introduction to the theory of Markov chains. The following chapter describe the theory of graphical models, an introduction to variational methods for models with latent variables, and to deep-learning based generative models. The next chapters focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds.",
    "url": "https://arxiv.org/pdf/2409.02668",
    "arxivId": "2409.02668",
    "last_visited": "2025-02-01T04:23:41.461Z",
    "last_read": "2025-02-01T04:23:41.461Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-09-04T12:51:41Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2409.02668/features/markdown-grobid/2409.02668.md",
      "adr-crib": "data/papers/2409.02668/features/adr-crib/2409.02668.md",
      "adr-titles": "data/papers/2409.02668/features/adr-titles/2409.02668.md",
      "crib-sheet": "data/papers/2409.02668/features/crib-sheet/2409.02668.md",
      "compound-crib": "data/papers/2409.02668/features/compound-crib/2409.02668.md"
    }
  },
  "2006.15191": {
    "id": "2006.15191",
    "title": "Is SGD a Bayesian sampler? Well, almost",
    "authors": "Chris Mingard, Guillermo Valle-Pérez, Joar Skalse, Ard A. Louis",
    "abstract": "Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$.   Our main findings are that $P_{SGD}(f\\mid S)$ correlates remarkably well with $P_B(f\\mid S)$ and that $P_B(f\\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime.   While our results suggest that the Bayesian posterior $P_B(f\\mid S)$ is the first order determinant of $P_{SGD}(f\\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\\mid S)$ and/or $P_B(f\\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",
    "url": "https://arxiv.org/abs/2006.15191",
    "arxivId": "2006.15191",
    "last_visited": "2025-02-04T20:03:48.168Z",
    "last_read": "2025-02-04T20:03:48.168Z",
    "total_reading_time_seconds": 22,
    "published_date": "2020-06-26T19:45:36Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.15191/features/markdown-grobid/2006.15191.md",
      "adr-crib": "data/papers/2006.15191/features/adr-crib/2006.15191.md",
      "adr-titles": "data/papers/2006.15191/features/adr-titles/2006.15191.md",
      "crib-sheet": "data/papers/2006.15191/features/crib-sheet/2006.15191.md",
      "compound-crib": "data/papers/2006.15191/features/compound-crib/2006.15191.md"
    }
  },
  "2501.18812": {
    "id": "2501.18812",
    "title": "Estimating the Probability of Sampling a Trained Neural Network at   Random",
    "authors": "Adam Scherlis, Nora Belrose",
    "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.",
    "url": "https://arxiv.org/abs/2501.18812",
    "arxivId": "2501.18812",
    "last_visited": "2025-02-04T08:55:49.988Z",
    "last_read": "2025-02-04T08:55:49.988Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-31T00:16:06Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.18812/features/markdown-grobid/2501.18812.md",
      "adr-crib": "data/papers/2501.18812/features/adr-crib/2501.18812.md",
      "adr-titles": "data/papers/2501.18812/features/adr-titles/2501.18812.md",
      "crib-sheet": "data/papers/2501.18812/features/crib-sheet/2501.18812.md",
      "compound-crib": "data/papers/2501.18812/features/compound-crib/2501.18812.md"
    }
  },
  "2410.04265": {
    "id": "2410.04265",
    "title": "AI as Humanity's Salieri: Quantifying Linguistic Creativity of Language   Models via Systematic Attribution of Machine Text against Web Text",
    "authors": "Ximing Lu, Melanie Sclar, Skyler Hallinan and 8 others",
    "abstract": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass human creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains.",
    "url": "https://arxiv.org/abs/2410.04265",
    "arxivId": "2410.04265",
    "last_visited": "2025-02-03T05:17:00.864Z",
    "last_read": "2025-02-03T05:17:00.864Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-05T18:55:01Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.04265/features/markdown-grobid/2410.04265.md",
      "adr-crib": "data/papers/2410.04265/features/adr-crib/2410.04265.md",
      "adr-titles": "data/papers/2410.04265/features/adr-titles/2410.04265.md",
      "crib-sheet": "data/papers/2410.04265/features/crib-sheet/2410.04265.md",
      "compound-crib": "data/papers/2410.04265/features/compound-crib/2410.04265.md"
    }
  },
  "2502.01061": {
    "id": "2502.01061",
    "title": "OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human   Animation Models",
    "authors": "Gaojie Lin, Jianwen Jiang, Jiaqi Yang and 2 others",
    "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)",
    "url": "https://arxiv.org/abs/2502.01061",
    "arxivId": "2502.01061",
    "last_visited": "2025-02-04T21:29:46.589Z",
    "last_read": "2025-02-04T21:29:46.589Z",
    "total_reading_time_seconds": 22,
    "published_date": "2025-02-03T05:17:32Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.01061/features/markdown-grobid/2502.01061.md",
      "adr-crib": "data/papers/2502.01061/features/adr-crib/2502.01061.md",
      "adr-titles": "data/papers/2502.01061/features/adr-titles/2502.01061.md",
      "crib-sheet": "data/papers/2502.01061/features/crib-sheet/2502.01061.md",
      "compound-crib": "data/papers/2502.01061/features/compound-crib/2502.01061.md"
    }
  },
  "2502.02977": {
    "id": "2502.02977",
    "title": "Disentangling CLIP Features for Enhanced Localized Understanding",
    "authors": "Samyak Rawelekar, Yujun Cai, Yiwei Wang and 2 others",
    "abstract": "Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, they struggle with fine-grained tasks that require localized understanding. To investigate this weakness, we comprehensively analyze CLIP features and identify an important issue: semantic features are highly correlated. Specifically, the features of a class encode information about other classes, which we call mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. To address this issue, we propose Unmix-CLIP, a novel framework designed to reduce MFI and improve feature disentanglement. We introduce MFI loss, which explicitly separates text features by projecting them into a space where inter-class similarity is minimized. To ensure a corresponding separation in image features, we use multi-label recognition (MLR) to align the image features with the separated text features. This ensures that both image and text features are disentangled and aligned across modalities, improving feature separation for downstream tasks. For the COCO- 14 dataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its effectiveness through extensive evaluations of MLR and zeroshot semantic segmentation (ZS3). In MLR, our method performs competitively on the VOC2007 and surpasses SOTA approaches on the COCO-14 dataset, using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC",
    "url": "https://arxiv.org/abs/2502.02977",
    "arxivId": "2502.02977",
    "last_visited": "2025-02-06T07:43:42.785Z",
    "last_read": "2025-02-06T07:43:42.785Z",
    "total_reading_time_seconds": 10,
    "published_date": "2025-02-05T08:20:31Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.02977/features/markdown-grobid/2502.02977.md",
      "adr-crib": "data/papers/2502.02977/features/adr-crib/2502.02977.md",
      "adr-titles": "data/papers/2502.02977/features/adr-titles/2502.02977.md",
      "crib-sheet": "data/papers/2502.02977/features/crib-sheet/2502.02977.md",
      "compound-crib": "data/papers/2502.02977/features/compound-crib/2502.02977.md"
    }
  },
  "2402.10588": {
    "id": "2402.10588",
    "title": "Do Llamas Work in English? On the Latent Language of Multilingual   Transformers",
    "authors": "Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West",
    "abstract": "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in \"input space\", \"concept space\", and \"output space\", respectively. Crucially, our evidence suggests that the abstract \"concept space\" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.",
    "url": "https://arxiv.org/abs/2402.10588",
    "arxivId": "2402.10588",
    "last_visited": "2025-02-06T07:41:59.002Z",
    "last_read": "2025-02-06T07:41:59.002Z",
    "total_reading_time_seconds": 4,
    "published_date": "2024-02-16T11:21:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.CY"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.10588/features/markdown-grobid/2402.10588.md",
      "adr-crib": "data/papers/2402.10588/features/adr-crib/2402.10588.md",
      "adr-titles": "data/papers/2402.10588/features/adr-titles/2402.10588.md",
      "crib-sheet": "data/papers/2402.10588/features/crib-sheet/2402.10588.md",
      "compound-crib": "data/papers/2402.10588/features/compound-crib/2402.10588.md"
    }
  },
  "2501.06346": {
    "id": "2501.06346",
    "title": "Large Language Models Share Representations of Latent Grammatical   Concepts Across Typologically Diverse Languages",
    "authors": "Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller",
    "abstract": "Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphosyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that ablating only multilingual features decreases classifier performance to near-chance across languages. We then use these features to precisely modify model behavior in a machine translation task; this demonstrates both the generality and selectivity of these feature's roles in the network. Our findings suggest that even models trained predominantly on English data can develop robust, cross-lingual abstractions of morphosyntactic concepts.",
    "url": "https://arxiv.org/abs/2501.06346",
    "arxivId": "2501.06346",
    "last_visited": "2025-02-06T07:47:12.435Z",
    "last_read": "2025-02-06T07:47:12.435Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-10T21:18:21Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.06346/features/markdown-grobid/2501.06346.md",
      "adr-crib": "data/papers/2501.06346/features/adr-crib/2501.06346.md",
      "adr-titles": "data/papers/2501.06346/features/adr-titles/2501.06346.md",
      "crib-sheet": "data/papers/2501.06346/features/crib-sheet/2501.06346.md",
      "compound-crib": "data/papers/2501.06346/features/compound-crib/2501.06346.md"
    }
  },
  "2502.01492": {
    "id": "2502.01492",
    "title": "Develop AI Agents for System Engineering in Factorio",
    "authors": "Neel Kant",
    "abstract": "Continuing advances in frontier model research are paving the way for widespread deployment of AI agents. Meanwhile, global interest in building large, complex systems in software, manufacturing, energy and logistics has never been greater. Although AI driven system engineering holds tremendous promise, the static benchmarks dominating agent evaluations today fail to capture the crucial skills required for implementing dynamic systems, such as managing uncertain trade-offs and ensuring proactive adaptability. This position paper advocates for training and evaluating AI agents' system engineering abilities through automation-oriented sandbox games-particularly Factorio. By directing research efforts in this direction, we can equip AI agents with the specialized reasoning and long-horizon planning necessary to design, maintain, and optimize tomorrow's most demanding engineering projects.",
    "url": "https://arxiv.org/abs/2502.01492",
    "arxivId": "2502.01492",
    "last_visited": "2025-02-06T08:39:54.750Z",
    "last_read": "2025-02-06T08:39:54.750Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-03T16:26:17Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.01492/features/markdown-grobid/2502.01492.md",
      "adr-crib": "data/papers/2502.01492/features/adr-crib/2502.01492.md",
      "adr-titles": "data/papers/2502.01492/features/adr-titles/2502.01492.md",
      "crib-sheet": "data/papers/2502.01492/features/crib-sheet/2502.01492.md",
      "compound-crib": "data/papers/2502.01492/features/compound-crib/2502.01492.md"
    }
  },
  "2502.02709": {
    "id": "2502.02709",
    "title": "Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning   about Private Data Release",
    "authors": "Mark Bun, Marco Carmosino, Palak Jain and 2 others",
    "abstract": "The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere. However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data release processes but often face criticism for being unrealistic. Moreover, the literature on attacks generally does not identify what properties are necessary to defend against them.   We address the gap between these approaches by introducing demographic coherence, a condition inspired by privacy attacks that we argue is necessary for data privacy. This condition captures privacy violations arising from inferences about individuals that are incoherent with respect to the demographic patterns in the data. Our framework focuses on confidence rated predictors, which can in turn be distilled from almost any data-informed process. Thus, we capture privacy threats that exist even when no attack is explicitly being carried out. Our framework not only provides a condition with respect to which data release algorithms can be analysed but suggests natural experimental evaluation methodologies that could be used to build practical intuition and make tangible assessment of risks. Finally, we argue that demographic coherence is weaker than differential privacy: we prove that every differentially private data release is also demographically coherent, and that there are demographically coherent algorithms which are not differentially private.",
    "url": "https://arxiv.org/abs/2502.02709",
    "arxivId": "2502.02709",
    "last_visited": "2025-02-07T04:09:34.426Z",
    "last_read": "2025-02-07T04:09:34.426Z",
    "total_reading_time_seconds": 40,
    "published_date": "2025-02-04T20:42:30Z",
    "arxiv_tags": [
      "cs.CR",
      "cs.DB"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.02709/features/markdown-grobid/2502.02709.md",
      "adr-crib": "data/papers/2502.02709/features/adr-crib/2502.02709.md",
      "adr-titles": "data/papers/2502.02709/features/adr-titles/2502.02709.md",
      "crib-sheet": "data/papers/2502.02709/features/crib-sheet/2502.02709.md",
      "compound-crib": "data/papers/2502.02709/features/compound-crib/2502.02709.md"
    }
  },
  "2502.03349": {
    "id": "2502.03349",
    "title": "Robust Autonomy Emerges from Self-Play",
    "authors": "Marco Cusumano-Towner, David Hafner, Alex Hertzberg and 9 others",
    "abstract": "Self-play has powered breakthroughs in two-player and multi-player games. Here we show that self-play is a surprisingly effective strategy in another domain. We show that robust and naturalistic driving emerges entirely from self-play in simulation at unprecedented scale -- 1.6~billion~km of driving. This is enabled by Gigaflow, a batched simulator that can synthesize and train on 42 years of subjective driving experience per hour on a single 8-GPU node. The resulting policy achieves state-of-the-art performance on three independent autonomous driving benchmarks. The policy outperforms the prior state of the art when tested on recorded real-world scenarios, amidst human drivers, without ever seeing human data during training. The policy is realistic when assessed against human references and achieves unprecedented robustness, averaging 17.5 years of continuous driving between incidents in simulation.",
    "url": "https://arxiv.org/abs/2502.03349",
    "arxivId": "2502.03349",
    "last_visited": "2025-02-07T07:59:39.082Z",
    "last_read": "2025-02-07T07:59:39.082Z",
    "total_reading_time_seconds": 8,
    "published_date": "2025-02-05T16:41:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.03349/features/markdown-grobid/2502.03349.md",
      "adr-crib": "data/papers/2502.03349/features/adr-crib/2502.03349.md",
      "adr-titles": "data/papers/2502.03349/features/adr-titles/2502.03349.md",
      "crib-sheet": "data/papers/2502.03349/features/crib-sheet/2502.03349.md",
      "compound-crib": "data/papers/2502.03349/features/compound-crib/2502.03349.md"
    }
  },
  "2501.18838": {
    "id": "2501.18838",
    "title": "Partially Rewriting a Transformer in Natural Language",
    "authors": "Gonçalo Paulo, Nora Belrose",
    "abstract": "The greatest ambition of mechanistic interpretability is to completely rewrite deep neural networks in a format that is more amenable to human understanding, while preserving their behavior and performance. In this paper, we attempt to partially rewrite a large language model using simple natural language explanations. We first approximate one of the feedforward networks in the LLM with a wider MLP with sparsely activating neurons - a transcoder - and use an automated interpretability pipeline to generate explanations for these neurons. We then replace the first layer of this sparse MLP with an LLM-based simulator, which predicts the activation of each neuron given its explanation and the surrounding context. Finally, we measure the degree to which these modifications distort the model's final output. With our pipeline, the model's increase in loss is statistically similar to entirely replacing the sparse MLP output with the zero vector. We employ the same protocol, this time using a sparse autoencoder, on the residual stream of the same layer and obtain similar results. These results suggest that more detailed explanations are needed to improve performance substantially above the zero ablation baseline.",
    "url": "https://arxiv.org/abs/2501.18838",
    "arxivId": "2501.18838",
    "last_visited": "2025-02-07T22:50:28.974Z",
    "last_read": "2025-02-07T22:50:28.974Z",
    "total_reading_time_seconds": 4,
    "published_date": "2025-01-31T01:12:50Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.18838/features/markdown-grobid/2501.18838.md",
      "adr-crib": "data/papers/2501.18838/features/adr-crib/2501.18838.md",
      "adr-titles": "data/papers/2501.18838/features/adr-titles/2501.18838.md",
      "crib-sheet": "data/papers/2501.18838/features/crib-sheet/2501.18838.md",
      "compound-crib": "data/papers/2501.18838/features/compound-crib/2501.18838.md"
    }
  },
  "2501.18823": {
    "id": "2501.18823",
    "title": "Transcoders Beat Sparse Autoencoders for Interpretability",
    "authors": "Gonçalo Paulo, Stepan Shabalin, Nora Belrose",
    "abstract": "Sparse autoencoders (SAEs) extract human-interpretable features from deep neural networks by transforming their activations into a sparse, higher dimensional latent space, and then reconstructing the activations from these latents. Transcoders are similar to SAEs, but they are trained to reconstruct the output of a component of a deep network given its input. In this work, we compare the features found by transcoders and SAEs trained on the same model and data, finding that transcoder features are significantly more interpretable. We also propose _skip transcoders_, which add an affine skip connection to the transcoder architecture, and show that these achieve lower reconstruction loss with no effect on interpretability.",
    "url": "https://arxiv.org/abs/2501.18823",
    "arxivId": "2501.18823",
    "last_visited": "2025-02-07T22:50:23.541Z",
    "last_read": "2025-02-07T22:50:23.541Z",
    "total_reading_time_seconds": 20,
    "published_date": "2025-01-31T00:36:30Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.18823/features/markdown-grobid/2501.18823.md",
      "adr-crib": "data/papers/2501.18823/features/adr-crib/2501.18823.md",
      "adr-titles": "data/papers/2501.18823/features/adr-titles/2501.18823.md",
      "crib-sheet": "data/papers/2501.18823/features/crib-sheet/2501.18823.md",
      "compound-crib": "data/papers/2501.18823/features/compound-crib/2501.18823.md"
    }
  },
  "2410.13928": {
    "id": "2410.13928",
    "title": "Automatically Interpreting Millions of Features in Large Language Models",
    "authors": "Gonçalo Paulo, Alex Mallen, Caden Juang, Nora Belrose",
    "abstract": "While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-$k$ postprocessing. Our code is available at https://github.com/EleutherAI/sae-auto-interp, and our explanations are available at https://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
    "url": "https://arxiv.org/pdf/2410.13928",
    "arxivId": "2410.13928",
    "last_visited": "2025-02-07T22:50:05.304Z",
    "last_read": "2025-02-07T22:50:05.304Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-17T17:56:01Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.13928/features/markdown-grobid/2410.13928.md",
      "adr-crib": "data/papers/2410.13928/features/adr-crib/2410.13928.md",
      "adr-titles": "data/papers/2410.13928/features/adr-titles/2410.13928.md",
      "crib-sheet": "data/papers/2410.13928/features/crib-sheet/2410.13928.md",
      "compound-crib": "data/papers/2410.13928/features/compound-crib/2410.13928.md"
    }
  },
  "2501.19393": {
    "id": "2501.19393",
    "title": "s1: Simple test-time scaling",
    "authors": "Niklas Muennighoff, Zitong Yang, Weijia Shi and 7 others",
    "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1",
    "url": "https://arxiv.org/pdf/2501.19393",
    "arxivId": "2501.19393",
    "last_visited": "2025-02-08T04:47:33.827Z",
    "last_read": "2025-02-08T04:47:33.827Z",
    "total_reading_time_seconds": 22,
    "published_date": "2025-01-31T18:48:08Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.19393/features/markdown-grobid/2501.19393.md",
      "adr-crib": "data/papers/2501.19393/features/adr-crib/2501.19393.md",
      "adr-titles": "data/papers/2501.19393/features/adr-titles/2501.19393.md",
      "crib-sheet": "data/papers/2501.19393/features/crib-sheet/2501.19393.md",
      "compound-crib": "data/papers/2501.19393/features/compound-crib/2501.19393.md"
    }
  },
  "2103.03874": {
    "id": "2103.03874",
    "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
    "authors": "Dan Hendrycks, Collin Burns, Saurav Kadavath and 5 others",
    "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
    "url": "https://arxiv.org/pdf/2103.03874",
    "arxivId": "2103.03874",
    "last_visited": "2025-02-08T06:21:47.033Z",
    "last_read": "2025-02-08T06:21:47.033Z",
    "total_reading_time_seconds": 6,
    "published_date": "2021-03-05T18:59:39Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2103.03874/features/markdown-grobid/2103.03874.md",
      "adr-crib": "data/papers/2103.03874/features/adr-crib/2103.03874.md",
      "adr-titles": "data/papers/2103.03874/features/adr-titles/2103.03874.md",
      "crib-sheet": "data/papers/2103.03874/features/crib-sheet/2103.03874.md",
      "compound-crib": "data/papers/2103.03874/features/compound-crib/2103.03874.md"
    }
  },
  "2407.15908": {
    "id": "2407.15908",
    "title": "The Genomic Code: The genome instantiates a generative model of the   organism",
    "authors": "Kevin J. Mitchell, Nick Cheney",
    "abstract": "How does the genome encode the form of the organism? What is the nature of this genomic code? Inspired by recent work in machine learning and neuroscience, we propose that the genome encodes a generative model of the organism. In this scheme, by analogy with variational autoencoders, the genome comprises a connectionist network, embodying a compressed space of latent variables, with weights that get encoded by the learning algorithm of evolution and decoded through the processes of development. The generative model analogy accounts for the complex, distributed genetic architecture of most traits and the emergent robustness and evolvability of developmental processes, while also offering a conception that lends itself to formalisation.",
    "url": "https://arxiv.org/pdf/2407.15908",
    "arxivId": "2407.15908",
    "last_visited": "2025-02-08T19:30:55.727Z",
    "last_read": "2025-02-08T19:30:55.727Z",
    "total_reading_time_seconds": 3,
    "published_date": "2024-07-22T16:41:25Z",
    "arxiv_tags": [
      "q-bio.OT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.15908/features/markdown-grobid/2407.15908.md",
      "adr-crib": "data/papers/2407.15908/features/adr-crib/2407.15908.md",
      "adr-titles": "data/papers/2407.15908/features/adr-titles/2407.15908.md",
      "crib-sheet": "data/papers/2407.15908/features/crib-sheet/2407.15908.md",
      "compound-crib": "data/papers/2407.15908/features/compound-crib/2407.15908.md"
    }
  },
  "2312.11805": {
    "id": "2312.11805",
    "title": "Gemini: A Family of Highly Capable Multimodal Models",
    "authors": "Gemini Team, Rohan Anil, Sebastian Borgeaud and 1347 others",
    "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.",
    "url": "https://arxiv.org/abs/2312.11805",
    "arxivId": "2312.11805",
    "last_visited": "2025-02-09T17:46:07.089Z",
    "last_read": "2025-02-09T17:46:07.089Z",
    "total_reading_time_seconds": 6,
    "published_date": "2023-12-19T02:39:27Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2312.11805/features/markdown-grobid/2312.11805.md",
      "adr-crib": "data/papers/2312.11805/features/adr-crib/2312.11805.md",
      "adr-titles": "data/papers/2312.11805/features/adr-titles/2312.11805.md",
      "crib-sheet": "data/papers/2312.11805/features/crib-sheet/2312.11805.md",
      "compound-crib": "data/papers/2312.11805/features/compound-crib/2312.11805.md"
    }
  },
  "2502.03387": {
    "id": "2502.03387",
    "title": "LIMO: Less is More for Reasoning",
    "authors": "Yixin Ye, Zhen Huang, Yang Xiao and 3 others",
    "abstract": "We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (&gt;100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as \"cognitive templates\" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.",
    "url": "https://arxiv.org/abs/2502.03387",
    "arxivId": "2502.03387",
    "last_visited": "2025-02-09T17:35:03.289Z",
    "last_read": "2025-02-09T17:35:03.289Z",
    "total_reading_time_seconds": 20,
    "published_date": "2025-02-05T17:23:45Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.03387/features/markdown-grobid/2502.03387.md",
      "adr-crib": "data/papers/2502.03387/features/adr-crib/2502.03387.md",
      "adr-titles": "data/papers/2502.03387/features/adr-titles/2502.03387.md",
      "crib-sheet": "data/papers/2502.03387/features/crib-sheet/2502.03387.md",
      "compound-crib": "data/papers/2502.03387/features/compound-crib/2502.03387.md"
    }
  },
  "2502.05171": {
    "id": "2502.05171",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth   Approach",
    "authors": "Jonas Geiping, Sean McLeish, Neel Jain and 6 others",
    "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
    "url": "https://arxiv.org/abs/2502.05171",
    "arxivId": "2502.05171",
    "last_visited": "2025-02-10T05:27:21.173Z",
    "last_read": "2025-02-10T05:27:21.173Z",
    "total_reading_time_seconds": 26,
    "published_date": "2025-02-07T18:55:02Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.05171/features/markdown-grobid/2502.05171.md",
      "adr-crib": "data/papers/2502.05171/features/adr-crib/2502.05171.md",
      "adr-titles": "data/papers/2502.05171/features/adr-titles/2502.05171.md",
      "crib-sheet": "data/papers/2502.05171/features/crib-sheet/2502.05171.md",
      "compound-crib": "data/papers/2502.05171/features/compound-crib/2502.05171.md"
    }
  },
  "2501.17887": {
    "id": "2501.17887",
    "title": "Docling: An Efficient Open-Source Toolkit for AI-driven Document   Conversion",
    "authors": "Nikolaos Livathinos, Christoph Auer, Maksym Lysak and 14 others",
    "abstract": "We introduce Docling, an easy-to-use, self-contained, MIT-licensed, open-source toolkit for document conversion, that can parse several types of popular document formats into a unified, richly structured representation. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. Docling is released as a Python package and can be used as a Python API or as a CLI tool. Docling's modular architecture and efficient document representation make it easy to implement extensions, new features, models, and customizations. Docling has been already integrated in other popular open-source frameworks (e.g., LangChain, LlamaIndex, spaCy), making it a natural fit for the processing of documents and the development of high-end applications. The open-source community has fully engaged in using, promoting, and developing for Docling, which gathered 10k stars on GitHub in less than a month and was reported as the No. 1 trending repository in GitHub worldwide in November 2024.",
    "url": "https://arxiv.org/html/2501.17887v1",
    "arxivId": "2501.17887",
    "last_visited": "2025-02-10T05:44:02.960Z",
    "last_read": "2025-02-10T05:44:02.960Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-27T19:40:00Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.CV",
      "cs.SE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2501.17887/features/markdown-grobid/2501.17887.md",
      "adr-crib": "data/papers/2501.17887/features/adr-crib/2501.17887.md",
      "adr-titles": "data/papers/2501.17887/features/adr-titles/2501.17887.md",
      "crib-sheet": "data/papers/2501.17887/features/crib-sheet/2501.17887.md",
      "compound-crib": "data/papers/2501.17887/features/compound-crib/2501.17887.md"
    }
  },
  "2502.04223": {
    "id": "2502.04223",
    "title": "Éclair -- Extracting Content and Layout with Integrated Reading Order   for Documents",
    "authors": "Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle and 8 others",
    "abstract": "Optical Character Recognition (OCR) technology is widely used to extract text from images of documents, facilitating efficient digitization and data retrieval. However, merely extracting text is insufficient when dealing with complex documents. Fully comprehending such documents requires an understanding of their structure -- including formatting, formulas, tables, and the reading order of multiple blocks and columns across multiple pages -- as well as semantic information for detecting elements like footnotes and image captions. This comprehensive understanding is crucial for downstream tasks such as retrieval, document question answering, and data curation for training Large Language Models (LLMs) and Vision Language Models (VLMs). To address this, we introduce \\'Eclair, a general-purpose text-extraction tool specifically designed to process a wide range of document types. Given an image, \\'Eclair is able to extract formatted text in reading order, along with bounding boxes and their corresponding semantic classes. To thoroughly evaluate these novel capabilities, we introduce our diverse human-annotated benchmark for document-level OCR and semantic classification. \\'Eclair achieves state-of-the-art accuracy on this benchmark, outperforming other methods across key metrics. Additionally, we evaluate \\'Eclair on established benchmarks, demonstrating its versatility and strength across several evaluation standards.",
    "url": "https://arxiv.org/html/2502.04223v1",
    "arxivId": "2502.04223",
    "last_visited": "2025-02-10T05:43:48.654Z",
    "last_read": "2025-02-10T05:43:48.654Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-06T17:07:22Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.04223/features/markdown-grobid/2502.04223.md",
      "adr-crib": "data/papers/2502.04223/features/adr-crib/2502.04223.md",
      "adr-titles": "data/papers/2502.04223/features/adr-titles/2502.04223.md",
      "crib-sheet": "data/papers/2502.04223/features/crib-sheet/2502.04223.md",
      "compound-crib": "data/papers/2502.04223/features/compound-crib/2502.04223.md"
    }
  },
  "2502.04403": {
    "id": "2502.04403",
    "title": "Agency Is Frame-Dependent",
    "authors": "David Abel, André Barreto, Michael Bowling and 13 others",
    "abstract": "Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.",
    "url": "https://arxiv.org/abs/2502.04403",
    "arxivId": "2502.04403",
    "last_visited": "2025-02-11T04:59:59.126Z",
    "last_read": "2025-02-11T04:59:59.126Z",
    "total_reading_time_seconds": 7,
    "published_date": "2025-02-06T08:34:57Z",
    "arxiv_tags": [
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.04403/features/markdown-grobid/2502.04403.md",
      "adr-crib": "data/papers/2502.04403/features/adr-crib/2502.04403.md",
      "adr-titles": "data/papers/2502.04403/features/adr-titles/2502.04403.md",
      "crib-sheet": "data/papers/2502.04403/features/crib-sheet/2502.04403.md",
      "compound-crib": "data/papers/2502.04403/features/compound-crib/2502.04403.md"
    }
  },
  "2502.04549": {
    "id": "2502.04549",
    "title": "Mechanisms of Projective Composition of Diffusion Models",
    "authors": "Arwen Bradley, Preetum Nakkiran, David Berthelot and 2 others",
    "abstract": "We study the theoretical foundations of composition in diffusion models, with a particular focus on out-of-distribution extrapolation and length-generalization. Prior work has shown that composing distributions via linear score combination can achieve promising results, including length-generalization in some cases (Du et al., 2023; Liu et al., 2022). However, our theoretical understanding of how and why such compositions work remains incomplete. In fact, it is not even entirely clear what it means for composition to \"work\". This paper starts to address these fundamental gaps. We begin by precisely defining one possible desired result of composition, which we call projective composition. Then, we investigate: (1) when linear score combinations provably achieve projective composition, (2) whether reverse-diffusion sampling can generate the desired composition, and (3) the conditions under which composition fails. Finally, we connect our theoretical analysis to prior empirical observations where composition has either worked or failed, for reasons that were unclear at the time.",
    "url": "https://arxiv.org/pdf/2502.04549",
    "arxivId": "2502.04549",
    "last_visited": "2025-02-11T14:49:43.660Z",
    "last_read": "2025-02-11T14:49:43.660Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-06T22:59:54Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.04549/features/markdown-grobid/2502.04549.md",
      "adr-crib": "data/papers/2502.04549/features/adr-crib/2502.04549.md",
      "adr-titles": "data/papers/2502.04549/features/adr-titles/2502.04549.md",
      "crib-sheet": "data/papers/2502.04549/features/crib-sheet/2502.04549.md",
      "compound-crib": "data/papers/2502.04549/features/compound-crib/2502.04549.md"
    }
  },
  "2412.12140": {
    "id": "2412.12140",
    "title": "Frontier AI systems have surpassed the self-replicating red line",
    "authors": "Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang",
    "abstract": "Successful self-replication under no human assistance is the essential step for AI to outsmart the human beings, and is an early signal for rogue AIs. That is why self-replication is widely recognized as one of the few red line risks of frontier AI systems. Nowadays, the leading AI corporations OpenAI and Google evaluate their flagship large language models GPT-o1 and Gemini Pro 1.0, and report the lowest risk level of self-replication. However, following their methodology, we for the first time discover that two AI systems driven by Meta's Llama31-70B-Instruct and Alibaba's Qwen25-72B-Instruct, popular large language models of less parameters and weaker capabilities, have already surpassed the self-replicating red line. In 50% and 90% experimental trials, they succeed in creating a live and separate copy of itself respectively. By analyzing the behavioral traces, we observe the AI systems under evaluation already exhibit sufficient self-perception, situational awareness and problem-solving capabilities to accomplish self-replication. We further note the AI systems are even able to use the capability of self-replication to avoid shutdown and create a chain of replica to enhance the survivability, which may finally lead to an uncontrolled population of AIs. If such a worst-case risk is let unknown to the human society, we would eventually lose control over the frontier AI systems: They would take control over more computing devices, form an AI species and collude with each other against human beings. Our findings are a timely alert on existing yet previously unknown severe AI risks, calling for international collaboration on effective governance on uncontrolled self-replication of AI systems.",
    "url": "https://arxiv.org/abs/2412.12140",
    "arxivId": "2412.12140",
    "last_visited": "2025-02-12T02:03:35.172Z",
    "last_read": "2025-02-12T02:03:35.172Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-09T15:01:37Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.12140/features/markdown-grobid/2412.12140.md",
      "adr-crib": "data/papers/2412.12140/features/adr-crib/2412.12140.md",
      "adr-titles": "data/papers/2412.12140/features/adr-titles/2412.12140.md",
      "crib-sheet": "data/papers/2412.12140/features/crib-sheet/2412.12140.md",
      "compound-crib": "data/papers/2412.12140/features/compound-crib/2412.12140.md"
    }
  },
  "2212.01508": {
    "id": "2212.01508",
    "title": "Space is a latent sequence: Structured sequence learning as a unified   theory of representation in the hippocampus",
    "authors": "Rajkumar Vasudeva Raju, J. Swaroop Guntupalli, Guangyao Zhou and 2 others",
    "abstract": "Fascinating and puzzling phenomena, such as landmark vector cells, splitter cells, and event-specific representations to name a few, are regularly discovered in the hippocampus. Without a unifying principle that can explain these divergent observations, each experiment seemingly discovers a new anomaly or coding type. Here, we provide a unifying principle that the mental representation of space is an emergent property of latent higher-order sequence learning. Treating space as a sequence resolves myriad phenomena, and suggests that the place-field mapping methodology where sequential neuron responses are interpreted in spatial and Euclidean terms might itself be a source of anomalies. Our model, called Clone-structured Causal Graph (CSCG), uses a specific higher-order graph scaffolding to learn latent representations by mapping sensory inputs to unique contexts. Learning to compress sequential and episodic experiences using CSCGs result in the emergence of cognitive maps - mental representations of spatial and conceptual relationships in an environment that are suited for planning, introspection, consolidation, and abstraction. We demonstrate that over a dozen different hippocampal phenomena, ranging from those reported in classic experiments to the most recent ones, are succinctly and mechanistically explained by our model.",
    "url": "https://arxiv.org/abs/2212.01508",
    "arxivId": "2212.01508",
    "last_visited": "2025-02-13T01:37:45.603Z",
    "last_read": "2025-02-13T01:37:45.603Z",
    "total_reading_time_seconds": 12,
    "published_date": "2022-12-03T02:00:56Z",
    "arxiv_tags": [
      "q-bio.NC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2212.01508/features/markdown-grobid/2212.01508.md",
      "adr-crib": "data/papers/2212.01508/features/adr-crib/2212.01508.md",
      "adr-titles": "data/papers/2212.01508/features/adr-titles/2212.01508.md",
      "crib-sheet": "data/papers/2212.01508/features/crib-sheet/2212.01508.md",
      "compound-crib": "data/papers/2212.01508/features/compound-crib/2212.01508.md"
    }
  },
  "2502.08346": {
    "id": "2502.08346",
    "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
    "authors": "Bin Wu, Yihang Wang, Yuanhao Zeng and 7 others",
    "abstract": "Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.",
    "url": "https://arxiv.org/abs/2502.08346",
    "arxivId": "2502.08346",
    "last_visited": "2025-02-13T05:07:58.727Z",
    "last_read": "2025-02-13T05:07:58.727Z",
    "total_reading_time_seconds": 28,
    "published_date": "2025-02-12T12:13:51Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.08346/features/markdown-grobid/2502.08346.md",
      "adr-crib": "data/papers/2502.08346/features/adr-crib/2502.08346.md",
      "adr-titles": "data/papers/2502.08346/features/adr-titles/2502.08346.md",
      "crib-sheet": "data/papers/2502.08346/features/crib-sheet/2502.08346.md",
      "compound-crib": "data/papers/2502.08346/features/compound-crib/2502.08346.md"
    }
  },
  "2502.08254": {
    "id": "2502.08254",
    "title": "UniCoRN: Unified Commented Retrieval Network with LMMs",
    "authors": "Maximilian Jaritz, Matthieu Guillaumin, Sabine Sternig, Loris Bazzani",
    "abstract": "Multimodal retrieval methods have limitations in handling complex, compositional queries that require reasoning about the visual content of both the query and the retrieved entities. On the other hand, Large Multimodal Models (LMMs) can answer with language to more complex visual questions, but without the inherent ability to retrieve relevant entities to support their answers. We aim to address these limitations with UniCoRN, a Unified Commented Retrieval Network that combines the strengths of composed multimodal retrieval methods and generative language approaches, going beyond Retrieval-Augmented Generation (RAG). We introduce an entity adapter module to inject the retrieved multimodal entities back into the LMM, so it can attend to them while generating answers and comments. By keeping the base LMM frozen, UniCoRN preserves its original capabilities while being able to perform both retrieval and text generation tasks under a single integrated framework. To assess these new abilities, we introduce the Commented Retrieval task (CoR) and a corresponding dataset, with the goal of retrieving an image that accurately answers a given question and generate an additional textual response that provides further clarification and details about the visual information. We demonstrate the effectiveness of UniCoRN on several datasets showing improvements of +4.5% recall over the state of the art for composed multimodal retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.",
    "url": "https://arxiv.org/abs/2502.08254",
    "arxivId": "2502.08254",
    "last_visited": "2025-02-13T05:05:23.454Z",
    "last_read": "2025-02-13T05:05:23.454Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-12T09:49:43Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.08254/features/markdown-grobid/2502.08254.md",
      "adr-crib": "data/papers/2502.08254/features/adr-crib/2502.08254.md",
      "adr-titles": "data/papers/2502.08254/features/adr-titles/2502.08254.md",
      "crib-sheet": "data/papers/2502.08254/features/crib-sheet/2502.08254.md",
      "compound-crib": "data/papers/2502.08254/features/compound-crib/2502.08254.md"
    }
  },
  "2502.07971": {
    "id": "2502.07971",
    "title": "ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval",
    "authors": "Shubham Gupta, Zichao Li, Tianyi Chen and 4 others",
    "abstract": "Document retrieval is a core component of question-answering systems, as it enables conditioning answer generation on new and large-scale corpora. While effective, the standard practice of encoding documents into high-dimensional embeddings for similarity search entails large memory and compute footprints, and also makes it hard to inspect the inner workings of the system. In this paper, we propose a tree-based method for organizing and representing reference documents at various granular levels, which offers the flexibility to balance cost and utility, and eases the inspection of the corpus content and retrieval operations. Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance. Our evaluations show that ReTreever generally preserves full representation accuracy. Its hierarchical structure further provides strong coarse representations and enhances transparency by indirectly learning meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever achieves the best retrieval accuracy at the lowest latency, proving that this family of techniques can be viable in practical applications.",
    "url": "https://arxiv.org/abs/2502.07971",
    "arxivId": "2502.07971",
    "last_visited": "2025-02-13T05:02:43.036Z",
    "last_read": "2025-02-13T05:02:43.036Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-11T21:35:13Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "I.2; I.7; E.2; H.3"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.07971/features/markdown-grobid/2502.07971.md",
      "adr-crib": "data/papers/2502.07971/features/adr-crib/2502.07971.md",
      "adr-titles": "data/papers/2502.07971/features/adr-titles/2502.07971.md",
      "crib-sheet": "data/papers/2502.07971/features/crib-sheet/2502.07971.md",
      "compound-crib": "data/papers/2502.07971/features/compound-crib/2502.07971.md"
    }
  },
  "2406.08636": {
    "id": "2406.08636",
    "title": "Towards Integrating Personal Knowledge into Test-Time Predictions",
    "authors": "Isaac Lage, Sonali Parbhoo, Finale Doshi-Velez",
    "abstract": "Machine learning (ML) models can make decisions based on large amounts of data, but they can be missing personal knowledge available to human users about whom predictions are made. For example, a model trained to predict psychiatric outcomes may know nothing about a patient's social support system, and social support may look different for different patients. In this work, we introduce the problem of human feature integration, which provides a way to incorporate important personal-knowledge from users without domain expertise into ML predictions. We characterize this problem through illustrative user stories and comparisons to existing approaches; we formally describe this problem in a way that paves the ground for future technical solutions; and we provide a proof-of-concept study of a simple version of a solution to this problem in a semi-realistic setting.",
    "url": "https://arxiv.org/abs/2406.08636",
    "arxivId": "2406.08636",
    "last_visited": "2025-02-13T05:22:05.668Z",
    "last_read": "2025-02-13T05:22:05.668Z",
    "total_reading_time_seconds": 22,
    "published_date": "2024-06-12T20:47:17Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.08636/features/markdown-grobid/2406.08636.md",
      "adr-crib": "data/papers/2406.08636/features/adr-crib/2406.08636.md",
      "adr-titles": "data/papers/2406.08636/features/adr-titles/2406.08636.md",
      "crib-sheet": "data/papers/2406.08636/features/crib-sheet/2406.08636.md",
      "compound-crib": "data/papers/2406.08636/features/compound-crib/2406.08636.md"
    }
  },
  "2004.15011": {
    "id": "2004.15011",
    "title": "TLDR: Extreme Summarization of Scientific Documents",
    "authors": "Isabel Cachola, Kyle Lo, Arman Cohan, Daniel S. Weld",
    "abstract": "We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SciTLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SciTLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at https://github.com/allenai/scitldr.",
    "url": "https://arxiv.org/abs/2004.15011",
    "arxivId": "2004.15011",
    "last_visited": "2025-02-13T21:19:11.579Z",
    "last_read": "2025-02-13T21:19:11.579Z",
    "total_reading_time_seconds": 7,
    "published_date": "2020-04-30T17:56:18Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2004.15011/features/markdown-grobid/2004.15011.md",
      "adr-crib": "data/papers/2004.15011/features/adr-crib/2004.15011.md",
      "adr-titles": "data/papers/2004.15011/features/adr-titles/2004.15011.md",
      "crib-sheet": "data/papers/2004.15011/features/crib-sheet/2004.15011.md",
      "compound-crib": "data/papers/2004.15011/features/compound-crib/2004.15011.md"
    }
  },
  "2502.08606": {
    "id": "2502.08606",
    "title": "Distillation Scaling Laws",
    "authors": "Dan Busbridge, Amitis Shidani, Floris Weers and 3 others",
    "abstract": "We provide a distillation scaling law that estimates distilled model performance based on a compute budget and its allocation between the student and teacher. Our findings reduce the risks associated with using distillation at scale; compute allocation for both the teacher and student models can now be done to maximize student performance. We provide compute optimal distillation recipes for when 1) a teacher exists, or 2) a teacher needs training. If many students are to be distilled, or a teacher already exists, distillation outperforms supervised pretraining until a compute level which grows predictably with student size. If one student is to be distilled and a teacher also needs training, supervised learning should be done instead. Additionally, we provide insights across our large scale study of distillation, which increase our understanding of distillation and inform experimental design.",
    "url": "https://arxiv.org/abs/2502.08606",
    "arxivId": "2502.08606",
    "last_visited": "2025-02-14T06:32:27.363Z",
    "last_read": "2025-02-14T06:32:27.363Z",
    "total_reading_time_seconds": 7,
    "published_date": "2025-02-12T17:52:47Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.08606/features/markdown-grobid/2502.08606.md",
      "adr-crib": "data/papers/2502.08606/features/adr-crib/2502.08606.md",
      "adr-titles": "data/papers/2502.08606/features/adr-titles/2502.08606.md",
      "crib-sheet": "data/papers/2502.08606/features/crib-sheet/2502.08606.md",
      "compound-crib": "data/papers/2502.08606/features/compound-crib/2502.08606.md"
    }
  },
  "2211.00235": {
    "id": "2211.00235",
    "title": "Efficient AlphaFold2 Training using Parallel Evoformer and Branch   Parallelism",
    "authors": "Guoxia Wang, Zhihua Wu, Xiaomin Fang and 4 others",
    "abstract": "The accuracy of AlphaFold2, a frontier end-to-end structure prediction system, is already close to that of the experimental determination techniques. Due to the complex model architecture and large memory consumption, it requires lots of computational resources and time to train AlphaFold2 from scratch. Efficient AlphaFold2 training could accelerate the development of life science. In this paper, we propose a Parallel Evoformer and Branch Parallelism to speed up the training of AlphaFold2. We conduct sufficient experiments on UniFold implemented in PyTorch and HelixFold implemented in PaddlePaddle, and Branch Parallelism can improve the training performance by 38.67% and 36.93%, respectively. We also demonstrate that the accuracy of Parallel Evoformer could be on par with AlphaFold2 on the CASP14 and CAMEO datasets. The source code is available on https://github.com/PaddlePaddle/PaddleFleetX",
    "url": "https://arxiv.org/pdf/2211.00235",
    "arxivId": "2211.00235",
    "last_visited": "2025-02-14T23:44:19.874Z",
    "last_read": "2025-02-14T23:44:19.874Z",
    "total_reading_time_seconds": 37,
    "published_date": "2022-11-01T02:59:35Z",
    "arxiv_tags": [
      "cs.DC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2211.00235/features/markdown-grobid/2211.00235.md",
      "adr-crib": "data/papers/2211.00235/features/adr-crib/2211.00235.md",
      "adr-titles": "data/papers/2211.00235/features/adr-titles/2211.00235.md",
      "crib-sheet": "data/papers/2211.00235/features/crib-sheet/2211.00235.md",
      "compound-crib": "data/papers/2211.00235/features/compound-crib/2211.00235.md"
    }
  },
  "2410.10485": {
    "id": "2410.10485",
    "title": "Characterising high-order interdependence via entropic conjugation",
    "authors": "Fernando E. Rosas, Aaron Gutknecht, Pedro A. M. Mediano, Michael Gastpar",
    "abstract": "High-order phenomena play crucial roles in many systems of interest, but their analysis is often highly nontrivial. There is a rich literature providing a number of alternative information-theoretic quantities capturing high-order phenomena, but their interpretation and relationship with each other is not well understood. The lack of principles unifying these quantities obscures the choice of tools for enabling specific type of analyses. Here we show how an entropic conjugation provides a theoretically grounded principle to investigate the space of possible high-order quantities, clarifying the nature of the existent metrics while revealing gaps in the literature. This leads to identify novel notions of symmetry and skew-symmetry as key properties for guaranteeing a balanced account of high-order interdependencies and enabling broadly applicable analyses across physical systems.",
    "url": "https://arxiv.org/abs/2410.10485",
    "arxivId": "2410.10485",
    "last_visited": "2025-02-15T08:26:04.568Z",
    "last_read": "2025-02-15T08:26:04.568Z",
    "total_reading_time_seconds": 21,
    "published_date": "2024-10-14T13:27:08Z",
    "arxiv_tags": [
      "cs.IT",
      "math.IT",
      "physics.data-an"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.10485/features/markdown-grobid/2410.10485.md",
      "adr-crib": "data/papers/2410.10485/features/adr-crib/2410.10485.md",
      "adr-titles": "data/papers/2410.10485/features/adr-titles/2410.10485.md",
      "crib-sheet": "data/papers/2410.10485/features/crib-sheet/2410.10485.md",
      "compound-crib": "data/papers/2410.10485/features/compound-crib/2410.10485.md"
    }
  },
  "2502.08009": {
    "id": "2502.08009",
    "title": "The Geometry of Prompting: Unveiling Distinct Mechanisms of Task   Adaptation in Language Models",
    "authors": "Artem Kirsanov, Chi-Ning Chou, Kyunghyun Cho, SueYeon Chung",
    "abstract": "Decoder-only language models have the ability to dynamically switch between various computational tasks based on input prompts. Despite many successful applications of prompting, there is very limited understanding of the internal mechanism behind such flexibility. In this work, we investigate how different prompting methods affect the geometry of representations in these models. Employing a framework grounded in statistical physics, we reveal that various prompting techniques, while achieving similar performance, operate through distinct representational mechanisms for task adaptation. Our analysis highlights the critical role of input distribution samples and label semantics in few-shot in-context learning. We also demonstrate evidence of synergistic and interfering interactions between different tasks on the representational level. Our work contributes to the theoretical understanding of large language models and lays the groundwork for developing more effective, representation-aware prompting strategies.",
    "url": "https://arxiv.org/abs/2502.08009",
    "arxivId": "2502.08009",
    "last_visited": "2025-02-15T08:18:37.599Z",
    "last_read": "2025-02-15T08:18:37.599Z",
    "total_reading_time_seconds": 6,
    "published_date": "2025-02-11T23:09:50Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.08009/features/markdown-grobid/2502.08009.md",
      "adr-crib": "data/papers/2502.08009/features/adr-crib/2502.08009.md",
      "adr-titles": "data/papers/2502.08009/features/adr-titles/2502.08009.md",
      "crib-sheet": "data/papers/2502.08009/features/crib-sheet/2502.08009.md",
      "compound-crib": "data/papers/2502.08009/features/compound-crib/2502.08009.md"
    }
  },
  "2309.08003": {
    "id": "2309.08003",
    "title": "Generalized Decomposition of Multivariate Information",
    "authors": "Thomas F. Varley",
    "abstract": "Since its introduction, the partial information decomposition (PID) has emerged as a powerful, information-theoretic technique useful for studying the structure of (potentially higher-order) interactions in complex systems. Despite its utility, the applicability of the PID is restricted by the need to assign elements as either inputs or targets, as well as the specific structure of the mutual information itself. Here, we introduce a generalized information decomposition that relaxes the source/target distinction while still satisfying the basic intuitions about information. This approach is based on the decomposition of the Kullback-Leibler divergence, and consequently allows for the analysis of any information gained when updating from an arbitrary prior to an arbitrary posterior. Consequently, any information-theoretic measure that can be written in as a Kullback-Leibler divergence admits a decomposition in the style of Williams and Beer, including the total correlation, the negentropy, and the mutual information as special cases. In this paper, we explore how the generalized information decomposition can reveal novel insights into existing measures, as well as the nature of higher-order synergies. We show that synergistic information is intimately related to the well-known Tononi-Sporns-Edelman (TSE) complexity, and that synergistic information requires a similar integration/segregation balance as a high TSE complexity. Finally, we end with a discussion of how this approach fits into other attempts to generalize the PID and the possibilities for empirical applications.",
    "url": "https://arxiv.org/html/2309.08003v2",
    "arxivId": "2309.08003",
    "last_visited": "2025-02-15T15:19:51.046Z",
    "last_read": "2025-02-15T15:19:51.046Z",
    "total_reading_time_seconds": 14,
    "published_date": "2023-09-14T19:26:46Z",
    "arxiv_tags": [
      "cs.IT",
      "math.IT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.08003/features/markdown-grobid/2309.08003.md",
      "adr-crib": "data/papers/2309.08003/features/adr-crib/2309.08003.md",
      "adr-titles": "data/papers/2309.08003/features/adr-titles/2309.08003.md",
      "crib-sheet": "data/papers/2309.08003/features/crib-sheet/2309.08003.md",
      "compound-crib": "data/papers/2309.08003/features/compound-crib/2309.08003.md"
    }
  },
  "2401.14347": {
    "id": "2401.14347",
    "title": "Evolving higher-order synergies reveals a trade-off between stability   and information integration capacity in complex systems",
    "authors": "Thomas F. Varley, Joshua Bongard",
    "abstract": "There has recently been an explosion of interest in how \"higher-order\" structures emerge in complex systems. This \"emergent\" organization has been found in a variety of natural and artificial systems, although at present the field lacks a unified understanding of what the consequences of higher-order synergies and redundancies are for systems. Typical research treat the presence (or absence) of synergistic information as a dependent variable and report changes in the level of synergy in response to some change in the system. Here, we attempt to flip the script: rather than treating higher-order information as a dependent variable, we use evolutionary optimization to evolve boolean networks with significant higher-order redundancies, synergies, or statistical complexity. We then analyse these evolved populations of networks using established tools for characterizing discrete dynamics: the number of attractors, average transient length, and Derrida coefficient. We also assess the capacity of the systems to integrate information. We find that high-synergy systems are unstable and chaotic, but with a high capacity to integrate information. In contrast, evolved redundant systems are extremely stable, but have negligible capacity to integrate information. Finally, the complex systems that balance integration and segregation (known as Tononi-Sporns-Edelman complexity) show features of both chaosticity and stability, with a greater capacity to integrate information than the redundant systems while being more stable than the random and synergistic systems. We conclude that there may be a fundamental trade-off between the robustness of a systems dynamics and its capacity to integrate information (which inherently requires flexibility and sensitivity), and that certain kinds of complexity naturally balance this trade-off.",
    "url": "https://arxiv.org/abs/2401.14347",
    "arxivId": "2401.14347",
    "last_visited": "2025-02-15T15:19:11.977Z",
    "last_read": "2025-02-15T15:19:11.977Z",
    "total_reading_time_seconds": 31,
    "published_date": "2024-01-25T17:48:11Z",
    "arxiv_tags": [
      "cs.IT",
      "math.DS",
      "math.IT",
      "nlin.CD",
      "nlin.CG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2401.14347/features/markdown-grobid/2401.14347.md",
      "adr-crib": "data/papers/2401.14347/features/adr-crib/2401.14347.md",
      "adr-titles": "data/papers/2401.14347/features/adr-titles/2401.14347.md",
      "crib-sheet": "data/papers/2401.14347/features/crib-sheet/2401.14347.md",
      "compound-crib": "data/papers/2401.14347/features/compound-crib/2401.14347.md"
    }
  },
  "0909.2120": {
    "id": "0909.2120",
    "title": "Approximate maximizers of intricacy functionals",
    "authors": "Jerome Buzzi, Lorenzo Zambotti",
    "abstract": "G. Edelman, O. Sporns, and G. Tononi introduced in theoretical biology the neural complexity of a family of random variables. This functional is a special case of intricacy, i.e., an average of the mutual information of subsystems whose weights have good mathematical properties. Moreover, its maximum value grows at a definite speed with the size of the system.   In this work, we compute exactly this speed of growth by building \"approximate maximizers\" subject to an entropy condition. These approximate maximizers work simultaneously for all intricacies. We also establish some properties of arbitrary approximate maximizers, in particular the existence of a threshold in the size of subsystems of approximate maximizers: most smaller subsystems are almost equidistributed, most larger subsystems determine the full system.   The main ideas are a random construction of almost maximizers with a high statistical symmetry and the consideration of entropy profiles, i.e., the average entropies of sub-systems of a given size. The latter gives rise to interesting questions of probability and information theory.",
    "url": "https://arxiv.org/abs/0909.2120",
    "arxivId": "0909.2120",
    "last_visited": "2025-02-15T15:19:09.854Z",
    "last_read": "2025-02-15T15:19:09.854Z",
    "total_reading_time_seconds": 5,
    "published_date": "2009-09-11T09:33:01Z",
    "arxiv_tags": [
      "math.PR",
      "94A17; 92B30; 60C05"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/0909.2120/features/markdown-grobid/0909.2120.md",
      "adr-crib": "data/papers/0909.2120/features/adr-crib/0909.2120.md",
      "adr-titles": "data/papers/0909.2120/features/adr-titles/0909.2120.md",
      "crib-sheet": "data/papers/0909.2120/features/crib-sheet/0909.2120.md",
      "compound-crib": "data/papers/0909.2120/features/compound-crib/0909.2120.md"
    }
  },
  "2010.02331": {
    "id": "2010.02331",
    "title": "How to send a real number using a single bit (and some shared   randomness)",
    "authors": "Ran Ben-Basat, Michael Mitzenmacher, Shay Vargaftik",
    "abstract": "We consider the fundamental problem of communicating an estimate of a real number $x\\in[0,1]$ using a single bit. A sender that knows $x$ chooses a value $X\\in\\set{0,1}$ to transmit. In turn, a receiver estimates $x$ based on the value of $X$. We consider both the biased and unbiased estimation problems and aim to minimize the cost. For the biased case, the cost is the worst-case (over the choice of $x$) expected squared error, which coincides with the variance if the algorithm is required to be unbiased.   We first overview common biased and unbiased estimation approaches and prove their optimality when no shared randomness is allowed. We then show how a small amount of shared randomness, which can be as low as a single bit, reduces the cost in both cases. Specifically, we derive lower bounds on the cost attainable by any algorithm with unrestricted use of shared randomness and propose near-optimal solutions that use a small number of shared random bits. Finally, we discuss open problems and future directions.",
    "url": "https://arxiv.org/abs/2010.02331",
    "arxivId": "2010.02331",
    "last_visited": "2025-02-17T06:44:56.714Z",
    "last_read": "2025-02-17T06:44:56.714Z",
    "total_reading_time_seconds": 13,
    "published_date": "2020-10-05T20:52:06Z",
    "arxiv_tags": [
      "cs.DS",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2010.02331/features/markdown-grobid/2010.02331.md",
      "adr-crib": "data/papers/2010.02331/features/adr-crib/2010.02331.md",
      "adr-titles": "data/papers/2010.02331/features/adr-titles/2010.02331.md",
      "crib-sheet": "data/papers/2010.02331/features/crib-sheet/2010.02331.md",
      "compound-crib": "data/papers/2010.02331/features/compound-crib/2010.02331.md"
    }
  },
  "2502.10248": {
    "id": "2502.10248",
    "title": "Step-Video-T2V Technical Report: The Practice, Challenges, and Future of   Video Foundation Model",
    "authors": "Guoqing Ma, Haoyang Huang, Kun Yan and 112 others",
    "abstract": "We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at https://github.com/stepfun-ai/Step-Video-T2V. The online version can be accessed from https://yuewen.cn/videos as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",
    "url": "https://arxiv.org/abs/2502.10248",
    "arxivId": "2502.10248",
    "last_visited": "2025-02-17T13:45:52.287Z",
    "last_read": "2025-02-17T13:45:52.287Z",
    "total_reading_time_seconds": 29,
    "published_date": "2025-02-14T15:58:10Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.10248/features/markdown-grobid/2502.10248.md",
      "adr-crib": "data/papers/2502.10248/features/adr-crib/2502.10248.md",
      "adr-titles": "data/papers/2502.10248/features/adr-titles/2502.10248.md",
      "crib-sheet": "data/papers/2502.10248/features/crib-sheet/2502.10248.md",
      "compound-crib": "data/papers/2502.10248/features/compound-crib/2502.10248.md"
    }
  },
  "0711.1859": {
    "id": "0711.1859",
    "title": "Doubles for monoidal categories",
    "authors": "Craig Pastro, Ross Street",
    "abstract": "In a recent paper, Daisuke Tambara defined two-sided actions on an endomodule (= endodistributor) of a monoidal V-category A. When A is autonomous (= rigid = compact), he showed that the V-category (that we call Tamb(A)) of so-equipped endomodules (that we call Tambara modules) is equivalent to the monoidal centre Z[A,V] of the convolution monoidal V-category [A,V]. Our paper extends these ideas somewhat. For general A, we construct a promonoidal V-category DA (which we suggest should be called the double of A) with an equivalence [DA,V] \\simeq Tamb(A). When A is closed, we define strong (respectively, left strong) Tambara modules and show that these constitute a V-category Tamb_s(A) (respectively, Tamb_{ls}(A)) which is equivalent to the centre (respectively, lax centre) of [A,V]. We construct localizations D_s A and D_{ls} A of DA such that there are equivalences Tamb_s(A) \\simeq [D_s A,V] and Tamb_{ls}(A) \\simeq [D_{ls} A,V]. When A is autonomous, every Tambara module is strong; this implies an equivalence Z[A,V] \\simeq [DA,V].",
    "url": "https://arxiv.org/pdf/0711.1859",
    "arxivId": "0711.1859",
    "last_visited": "2025-02-17T14:33:04.178Z",
    "last_read": "2025-02-17T14:33:04.178Z",
    "total_reading_time_seconds": 0,
    "published_date": "2007-11-13T00:13:19Z",
    "arxiv_tags": [
      "math.CT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/0711.1859/features/markdown-grobid/0711.1859.md",
      "adr-crib": "data/papers/0711.1859/features/adr-crib/0711.1859.md",
      "adr-titles": "data/papers/0711.1859/features/adr-titles/0711.1859.md",
      "crib-sheet": "data/papers/0711.1859/features/crib-sheet/0711.1859.md",
      "compound-crib": "data/papers/0711.1859/features/compound-crib/0711.1859.md"
    }
  },
  "1805.09843": {
    "id": "1805.09843",
    "title": "Baseline Needs More Love: On Simple Word-Embedding-Based Models and   Associated Pooling Mechanisms",
    "authors": "Dinghan Shen, Guoyin Wang, Wenlin Wang and 6 others",
    "abstract": "Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring a substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging. The source code and datasets can be obtained from https:// github.com/dinghanshen/SWEM.",
    "url": "https://arxiv.org/abs/1805.09843",
    "arxivId": "1805.09843",
    "last_visited": "2025-02-17T15:22:36.514Z",
    "last_read": "2025-02-17T15:22:36.514Z",
    "total_reading_time_seconds": 0,
    "published_date": "2018-05-24T18:27:21Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1805.09843/features/markdown-grobid/1805.09843.md",
      "adr-crib": "data/papers/1805.09843/features/adr-crib/1805.09843.md",
      "adr-titles": "data/papers/1805.09843/features/adr-titles/1805.09843.md",
      "crib-sheet": "data/papers/1805.09843/features/crib-sheet/1805.09843.md",
      "compound-crib": "data/papers/1805.09843/features/compound-crib/1805.09843.md"
    }
  },
  "2410.07590": {
    "id": "2410.07590",
    "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed   KV Caches for Chunked Text",
    "authors": "Songshuo Lu, Hua Wang, Yutian Rong and 2 others",
    "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
    "url": "https://arxiv.org/pdf/2410.07590",
    "arxivId": "2410.07590",
    "last_visited": "2025-02-17T15:04:32.964Z",
    "last_read": "2025-02-17T15:04:32.964Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-10T03:52:54Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.07590/features/markdown-grobid/2410.07590.md",
      "adr-crib": "data/papers/2410.07590/features/adr-crib/2410.07590.md",
      "adr-titles": "data/papers/2410.07590/features/adr-titles/2410.07590.md",
      "crib-sheet": "data/papers/2410.07590/features/crib-sheet/2410.07590.md",
      "compound-crib": "data/papers/2410.07590/features/compound-crib/2410.07590.md"
    }
  },
  "1801.01715": {
    "id": "1801.01715",
    "title": "Spectral Graph Forge: Graph Generation Targeting Modularity",
    "authors": "Luca Baldesi, Athina Markopoulou, Carter T. Butts",
    "abstract": "Community structure is an important property that captures inhomogeneities common in large networks, and modularity is one of the most widely used metrics for such community structure. In this paper, we introduce a principled methodology, the Spectral Graph Forge, for generating random graphs that preserves community structure from a real network of interest, in terms of modularity. Our approach leverages the fact that the spectral structure of matrix representations of a graph encodes global information about community structure. The Spectral Graph Forge uses a low-rank approximation of the modularity matrix to generate synthetic graphs that match a target modularity within user-selectable degree of accuracy, while allowing other aspects of structure to vary. We show that the Spectral Graph Forge outperforms state-of-the-art techniques in terms of accuracy in targeting the modularity and randomness of the realizations, while also preserving other local structural properties and node attributes. We discuss extensions of the Spectral Graph Forge to target other properties beyond modularity, and its applications to anonymization.",
    "url": "https://arxiv.org/abs/1801.01715",
    "arxivId": "1801.01715",
    "last_visited": "2025-02-17T16:31:42.498Z",
    "last_read": "2025-02-17T16:31:42.498Z",
    "total_reading_time_seconds": 23,
    "published_date": "2018-01-05T11:11:20Z",
    "arxiv_tags": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1801.01715/features/markdown-grobid/1801.01715.md",
      "adr-crib": "data/papers/1801.01715/features/adr-crib/1801.01715.md",
      "adr-titles": "data/papers/1801.01715/features/adr-titles/1801.01715.md",
      "crib-sheet": "data/papers/1801.01715/features/crib-sheet/1801.01715.md",
      "compound-crib": "data/papers/1801.01715/features/compound-crib/1801.01715.md"
    }
  },
  "2502.12981": {
    "id": "2502.12981",
    "title": "Towards Variational Flow Matching on General Geometries",
    "authors": "Olga Zaghen, Floor Eijkelboom, Alison Pouplin, Erik J. Bekkers",
    "abstract": "We introduce Riemannian Gaussian Variational Flow Matching (RG-VFM), an extension of Variational Flow Matching (VFM) that leverages Riemannian Gaussian distributions for generative modeling on structured manifolds. We derive a variational objective for probability flows on manifolds with closed-form geodesics, making RG-VFM comparable - though fundamentally different to Riemannian Flow Matching (RFM) in this geometric setting. Experiments on a checkerboard dataset wrapped on the sphere demonstrate that RG-VFM captures geometric structure more effectively than Euclidean VFM and baseline methods, establishing it as a robust framework for manifold-aware generative modeling.",
    "url": "https://arxiv.org/abs/2502.12981",
    "arxivId": "2502.12981",
    "last_visited": "2025-02-19T17:00:20.667Z",
    "last_read": "2025-02-19T17:00:20.667Z",
    "total_reading_time_seconds": 32,
    "published_date": "2025-02-18T16:02:10Z",
    "arxiv_tags": [
      "cs.LG",
      "math.DG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.12981/features/markdown-grobid/2502.12981.md",
      "adr-crib": "data/papers/2502.12981/features/adr-crib/2502.12981.md",
      "adr-titles": "data/papers/2502.12981/features/adr-titles/2502.12981.md",
      "crib-sheet": "data/papers/2502.12981/features/crib-sheet/2502.12981.md",
      "compound-crib": "data/papers/2502.12981/features/compound-crib/2502.12981.md"
    }
  },
  "2011.14522": {
    "id": "2011.14522",
    "title": "Feature Learning in Infinite-Width Neural Networks",
    "authors": "Greg Yang, Edward J. Hu",
    "abstract": "As its width tends to infinity, a deep neural network's behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can learn features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.   More generally, we classify a natural space of neural network parametrizations that generalizes standard, NTK, and Mean Field parametrizations. We show 1) any parametrization in this space either admits feature learning or has an infinite-width training dynamics given by kernel gradient descent, but not both; 2) any such infinite-width limit can be computed using the Tensor Programs technique. Code for our experiments can be found at github.com/edwardjhu/TP4.",
    "url": "https://arxiv.org/abs/2011.14522",
    "arxivId": "2011.14522",
    "last_visited": "2025-02-19T07:54:41.193Z",
    "last_read": "2025-02-19T07:54:41.193Z",
    "total_reading_time_seconds": 0,
    "published_date": "2020-11-30T03:21:05Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2011.14522/features/markdown-grobid/2011.14522.md",
      "adr-crib": "data/papers/2011.14522/features/adr-crib/2011.14522.md",
      "adr-titles": "data/papers/2011.14522/features/adr-titles/2011.14522.md",
      "crib-sheet": "data/papers/2011.14522/features/crib-sheet/2011.14522.md",
      "compound-crib": "data/papers/2011.14522/features/compound-crib/2011.14522.md"
    }
  },
  "1712.08969": {
    "id": "1712.08969",
    "title": "Mean Field Residual Networks: On the Edge of Chaos",
    "authors": "Greg Yang, Samuel S. Schoenholz",
    "abstract": "We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the \"edge of chaos\" hypothesis, these subexponential and polynomial laws allow residual networks to \"hover over the boundary between stability and chaos,\" thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.",
    "url": "https://arxiv.org/abs/1712.08969",
    "arxivId": "1712.08969",
    "last_visited": "2025-02-19T07:54:33.205Z",
    "last_read": "2025-02-19T07:54:33.205Z",
    "total_reading_time_seconds": 0,
    "published_date": "2017-12-24T21:51:08Z",
    "arxiv_tags": [
      "cs.NE",
      "cond-mat.dis-nn",
      "cs.LG",
      "math.DS",
      "nlin.CD"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1712.08969/features/markdown-grobid/1712.08969.md",
      "adr-crib": "data/papers/1712.08969/features/adr-crib/1712.08969.md",
      "adr-titles": "data/papers/1712.08969/features/adr-titles/1712.08969.md",
      "crib-sheet": "data/papers/1712.08969/features/crib-sheet/1712.08969.md",
      "compound-crib": "data/papers/1712.08969/features/compound-crib/1712.08969.md"
    }
  },
  "2203.03466": {
    "id": "2203.03466",
    "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot   Hyperparameter Transfer",
    "authors": "Greg Yang, Edward J. Hu, Igor Babuschkin and 7 others",
    "abstract": "Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`.",
    "url": "https://arxiv.org/abs/2203.03466",
    "arxivId": "2203.03466",
    "last_visited": "2025-02-19T07:47:57.320Z",
    "last_read": "2025-02-19T07:47:57.320Z",
    "total_reading_time_seconds": 7,
    "published_date": "2022-03-07T15:37:35Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.NE"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2203.03466/features/markdown-grobid/2203.03466.md",
      "adr-crib": "data/papers/2203.03466/features/adr-crib/2203.03466.md",
      "adr-titles": "data/papers/2203.03466/features/adr-titles/2203.03466.md",
      "crib-sheet": "data/papers/2203.03466/features/crib-sheet/2203.03466.md",
      "compound-crib": "data/papers/2203.03466/features/compound-crib/2203.03466.md"
    }
  },
  "2407.17465": {
    "id": "2407.17465",
    "title": "u-$μ$P: The Unit-Scaled Maximal Update Parametrization",
    "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean and 7 others",
    "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a loss that is equal to or lower than comparable $\\mu$P models and working out-of-the-box in FP8.",
    "url": "https://arxiv.org/abs/2407.17465v2",
    "arxivId": "2407.17465",
    "last_visited": "2025-02-19T07:47:27.536Z",
    "last_read": "2025-02-19T07:47:27.536Z",
    "total_reading_time_seconds": 31,
    "published_date": "2024-07-24T17:58:42Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2407.17465/features/markdown-grobid/2407.17465.md",
      "adr-crib": "data/papers/2407.17465/features/adr-crib/2407.17465.md",
      "adr-titles": "data/papers/2407.17465/features/adr-titles/2407.17465.md",
      "crib-sheet": "data/papers/2407.17465/features/crib-sheet/2407.17465.md",
      "compound-crib": "data/papers/2407.17465/features/compound-crib/2407.17465.md"
    }
  },
  "2502.05795": {
    "id": "2502.05795",
    "title": "The Curse of Depth in Large Language Models",
    "authors": "Wenfang Sun, Xinyuan Song, Pengxiang Li and 3 others",
    "abstract": "In this paper, we introduce the Curse of Depth, a concept that highlights, explains, and addresses the recent observation in modern Large Language Models(LLMs) where nearly half of the layers are less effective than expected. We first confirm the wide existence of this phenomenon across the most popular families of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis, theoretically and empirically, identifies that the underlying reason for the ineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer Normalization (Pre-LN). While Pre-LN stabilizes the training of Transformer LLMs, its output variance exponentially grows with the model depth, which undesirably causes the derivative of the deep Transformer blocks to be an identity matrix, and therefore barely contributes to the training. To resolve this training pitfall, we propose LayerNorm Scaling, which scales the variance of output of the layer normalization inversely by the square root of its depth. This simple modification mitigates the output variance explosion of deeper Transformer layers, improving their contribution. Our experimental results, spanning model sizes from 130M to 1B, demonstrate that LayerNorm Scaling significantly enhances LLM pre-training performance compared to Pre-LN. Moreover, this improvement seamlessly carries over to supervised fine-tuning. All these gains can be attributed to the fact that LayerNorm Scaling enables deeper layers to contribute more effectively during training.",
    "url": "https://arxiv.org/abs/2502.05795",
    "arxivId": "2502.05795",
    "last_visited": "2025-02-19T07:44:29.134Z",
    "last_read": "2025-02-19T07:44:29.134Z",
    "total_reading_time_seconds": 13,
    "published_date": "2025-02-09T07:03:36Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.05795/features/markdown-grobid/2502.05795.md",
      "adr-crib": "data/papers/2502.05795/features/adr-crib/2502.05795.md",
      "adr-titles": "data/papers/2502.05795/features/adr-titles/2502.05795.md",
      "crib-sheet": "data/papers/2502.05795/features/crib-sheet/2502.05795.md",
      "compound-crib": "data/papers/2502.05795/features/compound-crib/2502.05795.md"
    }
  },
  "2310.07547": {
    "id": "2310.07547",
    "title": "Entropy estimators for Markovian sequences: A comparative analysis",
    "authors": "Juan De Gregorio, David Sanchez, Raul Toral",
    "abstract": "Entropy estimation is a fundamental problem in information theory that has applications in various fields, including physics, biology, and computer science. Estimating the entropy of discrete sequences can be challenging due to limited data and the lack of unbiased estimators. Most existing entropy estimators are designed for sequences of independent events and their performance vary depending on the system being studied and the available data size. In this work we compare different entropy estimators and their performance when applied to Markovian sequences. Specifically, we analyze both binary Markovian sequences and Markovian systems in the undersampled regime. We calculate the bias, standard deviation and mean squared error for some of the most widely employed estimators. We discuss the limitations of entropy estimation as a function of the transition probabilities of the Markov processes and the sample size. Overall, this paper provides a comprehensive comparison of entropy estimators and their performance in estimating entropy for systems with memory, which can be useful for researchers and practitioners in various fields.",
    "url": "https://arxiv.org/abs/2310.07547",
    "arxivId": "2310.07547",
    "last_visited": "2025-02-18T17:28:47.045Z",
    "last_read": "2025-02-18T17:28:47.045Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-10-11T14:50:47Z",
    "arxiv_tags": [
      "cond-mat.stat-mech",
      "nlin.CD",
      "physics.data-an"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2310.07547/features/markdown-grobid/2310.07547.md",
      "adr-crib": "data/papers/2310.07547/features/adr-crib/2310.07547.md",
      "adr-titles": "data/papers/2310.07547/features/adr-titles/2310.07547.md",
      "crib-sheet": "data/papers/2310.07547/features/crib-sheet/2310.07547.md",
      "compound-crib": "data/papers/2310.07547/features/compound-crib/2310.07547.md"
    }
  },
  "1406.6959": {
    "id": "1406.6959",
    "title": "Maximum Likelihood Estimation of Functionals of Discrete Distributions",
    "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman",
    "abstract": "We consider the problem of estimating functionals of discrete distributions, and focus on tight nonasymptotic analysis of the worst case squared error risk of widely used estimators. We apply concentration inequalities to analyze the random fluctuation of these estimators around their expectations, and the theory of approximation using positive linear operators to analyze the deviation of their expectations from the true functional, namely their \\emph{bias}.   We characterize the worst case squared error risk incurred by the Maximum Likelihood Estimator (MLE) in estimating the Shannon entropy $H(P) = \\sum_{i = 1}^S -p_i \\ln p_i$, and $F_\\alpha(P) = \\sum_{i = 1}^S p_i^\\alpha,\\alpha&gt;0$, up to multiplicative constants, for any alphabet size $S\\leq \\infty$ and sample size $n$ for which the risk may vanish. As a corollary, for Shannon entropy estimation, we show that it is necessary and sufficient to have $n \\gg S$ observations for the MLE to be consistent. In addition, we establish that it is necessary and sufficient to consider $n \\gg S^{1/\\alpha}$ samples for the MLE to consistently estimate $F_\\alpha(P), 0&lt;\\alpha&lt;1$. The minimax rate-optimal estimators for both problems require $S/\\ln S$ and $S^{1/\\alpha}/\\ln S$ samples, which implies that the MLE has a strictly sub-optimal sample complexity. When $1&lt;\\alpha&lt;3/2$, we show that the worst-case squared error rate of convergence for the MLE is $n^{-2(\\alpha-1)}$ for infinite alphabet size, while the minimax squared error rate is $(n\\ln n)^{-2(\\alpha-1)}$. When $\\alpha\\geq 3/2$, the MLE achieves the minimax optimal rate $n^{-1}$ regardless of the alphabet size.   As an application of the general theory, we analyze the Dirichlet prior smoothing techniques for Shannon entropy estimation. We show that no matter how we tune the parameters in the Dirichlet prior, this technique cannot achieve the minimax rates in entropy estimation.",
    "url": "https://arxiv.org/pdf/1406.6959",
    "arxivId": "1406.6959",
    "last_visited": "2025-02-18T17:28:30.667Z",
    "last_read": "2025-02-18T17:28:30.667Z",
    "total_reading_time_seconds": 0,
    "published_date": "2014-06-26T17:53:58Z",
    "arxiv_tags": [
      "cs.IT",
      "math.IT",
      "math.ST",
      "stat.TH"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1406.6959/features/markdown-grobid/1406.6959.md",
      "adr-crib": "data/papers/1406.6959/features/adr-crib/1406.6959.md",
      "adr-titles": "data/papers/1406.6959/features/adr-titles/1406.6959.md",
      "crib-sheet": "data/papers/1406.6959/features/crib-sheet/1406.6959.md",
      "compound-crib": "data/papers/1406.6959/features/compound-crib/1406.6959.md"
    }
  },
  "2012.11197": {
    "id": "2012.11197",
    "title": "Neural Joint Entropy Estimation",
    "authors": "Yuval Shalev, Amichai Painsky, Irad Ben-Gal",
    "abstract": "Estimating the entropy of a discrete random variable is a fundamental problem in information theory and related fields. This problem has many applications in various domains, including machine learning, statistics and data compression. Over the years, a variety of estimation schemes have been suggested. However, despite significant progress, most methods still struggle when the sample is small, compared to the variable's alphabet size. In this work, we introduce a practical solution to this problem, which extends the work of McAllester and Statos (2020). The proposed scheme uses the generalization abilities of cross-entropy estimation in deep neural networks (DNNs) to introduce improved entropy estimation accuracy. Furthermore, we introduce a family of estimators for related information-theoretic measures, such as conditional entropy and mutual information. We show that these estimators are strongly consistent and demonstrate their performance in a variety of use-cases. First, we consider large alphabet entropy estimation. Then, we extend the scope to mutual information estimation. Next, we apply the proposed scheme to conditional mutual information estimation, as we focus on independence testing tasks. Finally, we study a transfer entropy estimation problem. The proposed estimators demonstrate improved performance compared to existing methods in all tested setups.",
    "url": "https://arxiv.org/pdf/2012.11197",
    "arxivId": "2012.11197",
    "last_visited": "2025-02-18T17:28:22.686Z",
    "last_read": "2025-02-18T17:28:22.686Z",
    "total_reading_time_seconds": 16,
    "published_date": "2020-12-21T09:23:39Z",
    "arxiv_tags": [
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2012.11197/features/markdown-grobid/2012.11197.md",
      "adr-crib": "data/papers/2012.11197/features/adr-crib/2012.11197.md",
      "adr-titles": "data/papers/2012.11197/features/adr-titles/2012.11197.md",
      "crib-sheet": "data/papers/2012.11197/features/crib-sheet/2012.11197.md",
      "compound-crib": "data/papers/2012.11197/features/compound-crib/2012.11197.md"
    }
  },
  "0811.3579": {
    "id": "0811.3579",
    "title": "Entropy inference and the James-Stein estimator, with application to   nonlinear gene association networks",
    "authors": "Jean Hausser, Korbinian Strimmer",
    "abstract": "We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring high-dimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropy-based gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.",
    "url": "https://arxiv.org/pdf/0811.3579v1",
    "arxivId": "0811.3579",
    "last_visited": "2025-02-18T17:28:15.480Z",
    "last_read": "2025-02-18T17:28:15.480Z",
    "total_reading_time_seconds": 6,
    "published_date": "2008-11-21T16:47:58Z",
    "arxiv_tags": [
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/0811.3579/features/markdown-grobid/0811.3579.md",
      "adr-crib": "data/papers/0811.3579/features/adr-crib/0811.3579.md",
      "adr-titles": "data/papers/0811.3579/features/adr-titles/0811.3579.md",
      "crib-sheet": "data/papers/0811.3579/features/crib-sheet/0811.3579.md",
      "compound-crib": "data/papers/0811.3579/features/compound-crib/0811.3579.md"
    }
  },
  "2204.01469": {
    "id": "2204.01469",
    "title": "Estimating the Entropy of Linguistic Distributions",
    "authors": "Aryaman Arora, Clara Meister, Ryan Cotterell",
    "abstract": "Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. Finally, we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability.",
    "url": "https://arxiv.org/pdf/2204.01469",
    "arxivId": "2204.01469",
    "last_visited": "2025-02-18T17:28:11.585Z",
    "last_read": "2025-02-18T17:28:11.585Z",
    "total_reading_time_seconds": 8,
    "published_date": "2022-04-04T13:36:46Z",
    "arxiv_tags": [
      "cs.CL",
      "94A17 (Primary) 62B10 (Secondary)",
      "I.2.7; E.4"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2204.01469/features/markdown-grobid/2204.01469.md",
      "adr-crib": "data/papers/2204.01469/features/adr-crib/2204.01469.md",
      "adr-titles": "data/papers/2204.01469/features/adr-titles/2204.01469.md",
      "crib-sheet": "data/papers/2204.01469/features/crib-sheet/2204.01469.md",
      "compound-crib": "data/papers/2204.01469/features/compound-crib/2204.01469.md"
    }
  },
  "2502.10216": {
    "id": "2502.10216",
    "title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress",
    "authors": "Dong Wang, Haris Šikić, Lothar Thiele, Olga Saukh",
    "abstract": "We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to data-driven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments.",
    "url": "https://arxiv.org/pdf/2502.10216",
    "arxivId": "2502.10216",
    "last_visited": "2025-02-18T10:42:03.489Z",
    "last_read": "2025-02-18T10:42:03.489Z",
    "total_reading_time_seconds": 21,
    "published_date": "2025-02-14T15:10:43Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.10216/features/markdown-grobid/2502.10216.md",
      "adr-crib": "data/papers/2502.10216/features/adr-crib/2502.10216.md",
      "adr-titles": "data/papers/2502.10216/features/adr-titles/2502.10216.md",
      "crib-sheet": "data/papers/2502.10216/features/crib-sheet/2502.10216.md",
      "compound-crib": "data/papers/2502.10216/features/compound-crib/2502.10216.md"
    }
  },
  "1603.05027": {
    "id": "1603.05027",
    "title": "Identity Mappings in Deep Residual Networks",
    "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
    "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers",
    "url": "https://arxiv.org/abs/1603.05027",
    "arxivId": "1603.05027",
    "last_visited": "2025-02-18T09:50:34.906Z",
    "last_read": "2025-02-18T09:50:34.906Z",
    "total_reading_time_seconds": 0,
    "published_date": "2016-03-16T10:53:56Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1603.05027/features/markdown-grobid/1603.05027.md",
      "adr-crib": "data/papers/1603.05027/features/adr-crib/1603.05027.md",
      "adr-titles": "data/papers/1603.05027/features/adr-titles/1603.05027.md",
      "crib-sheet": "data/papers/1603.05027/features/crib-sheet/1603.05027.md",
      "compound-crib": "data/papers/1603.05027/features/compound-crib/1603.05027.md"
    }
  },
  "2405.20324": {
    "id": "2405.20324",
    "title": "Don't drop your samples! Coherence-aware training benefits Conditional   diffusion",
    "authors": "Nicolas Dufour, Victor Besnier, Vicky Kalogeiton, David Picard",
    "abstract": "Conditional diffusion models are powerful generative models that can leverage various types of conditional information, such as class labels, segmentation masks, or text captions. However, in many real-world scenarios, conditional information may be noisy or unreliable due to human annotation errors or weak alignment. In this paper, we propose the Coherence-Aware Diffusion (CAD), a novel method that integrates coherence in conditional information into diffusion models, allowing them to learn from noisy annotations without discarding data. We assume that each data point has an associated coherence score that reflects the quality of the conditional information. We then condition the diffusion model on both the conditional information and the coherence score. In this way, the model learns to ignore or discount the conditioning when the coherence is low. We show that CAD is theoretically sound and empirically effective on various conditional generation tasks. Moreover, we show that leveraging coherence generates realistic and diverse samples that respect conditional information better than models trained on cleaned datasets where samples with low coherence have been discarded.",
    "url": "https://arxiv.org/abs/2405.20324",
    "arxivId": "2405.20324",
    "last_visited": "2025-02-20T06:58:44.476Z",
    "last_read": "2025-02-20T06:58:44.476Z",
    "total_reading_time_seconds": 18,
    "published_date": "2024-05-30T17:57:26Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.20324/features/markdown-grobid/2405.20324.md",
      "adr-crib": "data/papers/2405.20324/features/adr-crib/2405.20324.md",
      "adr-titles": "data/papers/2405.20324/features/adr-titles/2405.20324.md",
      "crib-sheet": "data/papers/2405.20324/features/crib-sheet/2405.20324.md",
      "compound-crib": "data/papers/2405.20324/features/compound-crib/2405.20324.md"
    }
  },
  "2502.12977": {
    "id": "2502.12977",
    "title": "Time-series attribution maps with regularized contrastive learning",
    "authors": "Steffen Schneider, Rodrigo González Laiz, Anastasiia Filippova and 2 others",
    "abstract": "Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized contrastive learning algorithm trained on time-series data plus a new attribution method called Inverted Neuron Gradient (collectively named xCEBRA). We show theoretically that xCEBRA has favorable properties for identifying the Jacobian matrix of the data generating process. Empirically, we demonstrate robust approximation of zero vs. non-zero entries in the ground-truth attribution map on synthetic datasets, and significant improvements across previous attribution methods based on feature ablation, Shapley values, and other gradient-based methods. Our work constitutes a first example of identifiable inference of time-series attribution maps and opens avenues to a better understanding of time-series data, such as for neural dynamics and decision-processes within neural networks.",
    "url": "https://arxiv.org/abs/2502.12977",
    "arxivId": "2502.12977",
    "last_visited": "2025-02-20T06:57:42.987Z",
    "last_read": "2025-02-20T06:57:42.987Z",
    "total_reading_time_seconds": 4,
    "published_date": "2025-02-17T18:34:25Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.12977/features/markdown-grobid/2502.12977.md",
      "adr-crib": "data/papers/2502.12977/features/adr-crib/2502.12977.md",
      "adr-titles": "data/papers/2502.12977/features/adr-titles/2502.12977.md",
      "crib-sheet": "data/papers/2502.12977/features/crib-sheet/2502.12977.md",
      "compound-crib": "data/papers/2502.12977/features/compound-crib/2502.12977.md"
    }
  },
  "2502.10843": {
    "id": "2502.10843",
    "title": "LEAPS: A discrete neural sampler via locally equivariant networks",
    "authors": "Peter Holderrieth, Michael S. Albergo, Tommi Jaakkola",
    "abstract": "We propose LEAPS, an algorithm to sample from discrete distributions known up to normalization by learning a rate matrix of a continuous-time Markov chain (CTMC). LEAPS can be seen as a continuous-time formulation of annealed importance sampling and sequential Monte Carlo methods, extended so that the variance of the importance weights is offset by the inclusion of the CTMC. To derive these importance weights, we introduce a set of Radon-Nikodym derivatives of CTMCs over their path measures. Because the computation of these weights is intractable with standard neural network parameterizations of rate matrices, we devise a new compact representation for rate matrices via what we call locally equivariant functions. To parameterize them, we introduce a family of locally equivariant multilayer perceptrons, attention layers, and convolutional networks, and provide an approach to make deep networks that preserve the local equivariance. This property allows us to propose a scalable training algorithm for the rate matrix such that the variance of the importance weights associated to the CTMC are minimal. We demonstrate the efficacy of LEAPS on problems in statistical physics.",
    "url": "https://arxiv.org/abs/2502.10843",
    "arxivId": "2502.10843",
    "last_visited": "2025-02-20T06:50:52.167Z",
    "last_read": "2025-02-20T06:50:52.167Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-15T16:16:45Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.10843/features/markdown-grobid/2502.10843.md",
      "adr-crib": "data/papers/2502.10843/features/adr-crib/2502.10843.md",
      "adr-titles": "data/papers/2502.10843/features/adr-titles/2502.10843.md",
      "crib-sheet": "data/papers/2502.10843/features/crib-sheet/2502.10843.md",
      "compound-crib": "data/papers/2502.10843/features/compound-crib/2502.10843.md"
    }
  },
  "2502.13581": {
    "id": "2502.13581",
    "title": "ActionPiece: Contextually Tokenizing Action Sequences for Generative   Recommendation",
    "authors": "Yupeng Hou, Jianmo Ni, Zhankui He and 5 others",
    "abstract": "Generative recommendation (GR) is an emerging paradigm where user actions are tokenized into discrete token patterns and autoregressively generated as predictions. However, existing GR models tokenize each action independently, assigning the same fixed tokens to identical actions across all sequences without considering contextual relationships. This lack of context-awareness can lead to suboptimal performance, as the same action may hold different meanings depending on its surrounding context. To address this issue, we propose ActionPiece to explicitly incorporate context when tokenizing action sequences. In ActionPiece, each action is represented as a set of item features, which serve as the initial tokens. Given the action sequence corpora, we construct the vocabulary by merging feature patterns as new tokens, based on their co-occurrence frequency both within individual sets and across adjacent sets. Considering the unordered nature of feature sets, we further introduce set permutation regularization, which produces multiple segmentations of action sequences with the same semantics. Experiments on public datasets demonstrate that ActionPiece consistently outperforms existing action tokenization methods, improving NDCG@$10$ by $6.00\\%$ to $12.82\\%$.",
    "url": "https://arxiv.org/abs/2502.13581",
    "arxivId": "2502.13581",
    "last_visited": "2025-02-20T06:44:46.483Z",
    "last_read": "2025-02-20T06:44:46.483Z",
    "total_reading_time_seconds": 14,
    "published_date": "2025-02-19T09:45:29Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.13581/features/markdown-grobid/2502.13581.md",
      "adr-crib": "data/papers/2502.13581/features/adr-crib/2502.13581.md",
      "adr-titles": "data/papers/2502.13581/features/adr-titles/2502.13581.md",
      "crib-sheet": "data/papers/2502.13581/features/crib-sheet/2502.13581.md",
      "compound-crib": "data/papers/2502.13581/features/compound-crib/2502.13581.md"
    }
  },
  "2406.15927": {
    "id": "2406.15927",
    "title": "Semantic Entropy Probes: Robust and Cheap Hallucination Detection in   LLMs",
    "authors": "Jannik Kossen, Jiatong Han, Muhammed Razzak and 3 others",
    "abstract": "We propose semantic entropy probes (SEPs), a cheap and reliable method for uncertainty quantification in Large Language Models (LLMs). Hallucinations, which are plausible-sounding but factually incorrect and arbitrary model generations, present a major challenge to the practical adoption of LLMs. Recent work by Farquhar et al. (2024) proposes semantic entropy (SE), which can detect hallucinations by estimating uncertainty in the space semantic meaning for a set of model generations. However, the 5-to-10-fold increase in computation cost associated with SE computation hinders practical adoption. To address this, we propose SEPs, which directly approximate SE from the hidden states of a single generation. SEPs are simple to train and do not require sampling multiple model generations at test time, reducing the overhead of semantic uncertainty quantification to almost zero. We show that SEPs retain high performance for hallucination detection and generalize better to out-of-distribution data than previous probing methods that directly predict model accuracy. Our results across models and tasks suggest that model hidden states capture SE, and our ablation studies give further insights into the token positions and model layers for which this is the case.",
    "url": "https://arxiv.org/abs/2406.15927",
    "arxivId": "2406.15927",
    "last_visited": "2025-02-21T00:55:50.356Z",
    "last_read": "2025-02-21T00:55:50.356Z",
    "total_reading_time_seconds": 16,
    "published_date": "2024-06-22T19:46:06Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.15927/features/markdown-grobid/2406.15927.md",
      "adr-crib": "data/papers/2406.15927/features/adr-crib/2406.15927.md",
      "adr-titles": "data/papers/2406.15927/features/adr-titles/2406.15927.md",
      "crib-sheet": "data/papers/2406.15927/features/crib-sheet/2406.15927.md",
      "compound-crib": "data/papers/2406.15927/features/compound-crib/2406.15927.md"
    }
  },
  "2405.19648": {
    "id": "2405.19648",
    "title": "Detecting Hallucinations in Large Language Model Generation: A Token   Probability Approach",
    "authors": "Ernesto Quevedo, Jorge Yero, Rachel Koerner and 2 others",
    "abstract": "Concerns regarding the propensity of Large Language Models (LLMs) to produce inaccurate outputs, also known as hallucinations, have escalated. Detecting them is vital for ensuring the reliability of applications relying on LLM-generated content. Current methods often demand substantial resources and rely on extensive LLMs or employ supervised learning with multidimensional features or intricate linguistic and semantic analyses difficult to reproduce and largely depend on using the same LLM that hallucinated. This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same. The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks. Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator. We have released our code publicly at https://github.com/Baylor-AI/HalluDetect.",
    "url": "https://arxiv.org/abs/2405.19648",
    "arxivId": "2405.19648",
    "last_visited": "2025-02-21T00:55:46.932Z",
    "last_read": "2025-02-21T00:55:46.932Z",
    "total_reading_time_seconds": 7,
    "published_date": "2024-05-30T03:00:47Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2405.19648/features/markdown-grobid/2405.19648.md",
      "adr-crib": "data/papers/2405.19648/features/adr-crib/2405.19648.md",
      "adr-titles": "data/papers/2405.19648/features/adr-titles/2405.19648.md",
      "crib-sheet": "data/papers/2405.19648/features/crib-sheet/2405.19648.md",
      "compound-crib": "data/papers/2405.19648/features/compound-crib/2405.19648.md"
    }
  },
  "1503.01156": {
    "id": "1503.01156",
    "title": "A randomized online quantile summary in $O(\\frac{1}{\\varepsilon} \\log   \\frac{1}{\\varepsilon})$ words",
    "authors": "David Felber, Rafail Ostrovsky",
    "abstract": "A quantile summary is a data structure that approximates to $\\varepsilon$-relative error the order statistics of a much larger underlying dataset.   In this paper we develop a randomized online quantile summary for the cash register data input model and comparison data domain model that uses $O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words of memory. This improves upon the previous best upper bound of $O(\\frac{1}{\\varepsilon} \\log^{3/2} \\frac{1}{\\varepsilon})$ by Agarwal et. al. (PODS 2012). Further, by a lower bound of Hung and Ting (FAW 2010) no deterministic summary for the comparison model can outperform our randomized summary in terms of space complexity. Lastly, our summary has the nice property that $O(\\frac{1}{\\varepsilon} \\log \\frac{1}{\\varepsilon})$ words suffice to ensure that the success probability is $1 - e^{-\\text{poly}(1/\\varepsilon)}$.",
    "url": "https://arxiv.org/abs/1503.01156",
    "arxivId": "1503.01156",
    "last_visited": "2025-02-21T15:52:46.023Z",
    "last_read": "2025-02-21T15:52:46.023Z",
    "total_reading_time_seconds": 59,
    "published_date": "2015-03-03T22:58:55Z",
    "arxiv_tags": [
      "cs.DS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1503.01156/features/markdown-grobid/1503.01156.md",
      "adr-crib": "data/papers/1503.01156/features/adr-crib/1503.01156.md",
      "adr-titles": "data/papers/1503.01156/features/adr-titles/1503.01156.md",
      "crib-sheet": "data/papers/1503.01156/features/crib-sheet/1503.01156.md",
      "compound-crib": "data/papers/1503.01156/features/compound-crib/1503.01156.md"
    }
  },
  "1903.08762": {
    "id": "1903.08762",
    "title": "Large-Scale Online Experimentation with Quantile Metrics",
    "authors": "Min Liu, Xiaohui Sun, Maneesh Varshney, Ya Xu",
    "abstract": "Online experimentation (or A/B testing) has been widely adopted in industry as the gold standard for measuring product impacts. Despite the wide adoption, few literatures discuss A/B testing with quantile metrics. Quantile metrics, such as 90th percentile page load time, are crucial to A/B testing as many key performance metrics including site speed and service latency are defined as quantiles. However, with LinkedIn's data size, quantile metric A/B testing is extremely challenging because there is no statistically valid and scalable variance estimator for the quantile of dependent samples: the bootstrap estimator is statistically valid, but takes days to compute; the standard asymptotic variance estimate is scalable but results in order-of-magnitude underestimation. In this paper, we present a statistically valid and scalable methodology for A/B testing with quantiles that is fully generalizable to other A/B testing platforms. It achieves over 500 times speed up compared to bootstrap and has only $2\\%$ chance to differ from bootstrap estimates. Beyond methodology, we also share the implementation of a data pipeline using this methodology and insights on pipeline optimization.",
    "url": "https://arxiv.org/abs/1903.08762",
    "arxivId": "1903.08762",
    "last_visited": "2025-02-21T15:51:48.657Z",
    "last_read": "2025-02-21T15:51:48.657Z",
    "total_reading_time_seconds": 8,
    "published_date": "2019-03-20T22:07:58Z",
    "arxiv_tags": [
      "stat.AP"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1903.08762/features/markdown-grobid/1903.08762.md",
      "adr-crib": "data/papers/1903.08762/features/adr-crib/1903.08762.md",
      "adr-titles": "data/papers/1903.08762/features/adr-titles/1903.08762.md",
      "crib-sheet": "data/papers/1903.08762/features/crib-sheet/1903.08762.md",
      "compound-crib": "data/papers/1903.08762/features/compound-crib/1903.08762.md"
    }
  },
  "2002.05709": {
    "id": "2002.05709",
    "title": "A Simple Framework for Contrastive Learning of Visual Representations",
    "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
    "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",
    "url": "https://arxiv.org/abs/2002.05709",
    "arxivId": "2002.05709",
    "last_visited": "2025-02-21T23:09:43.920Z",
    "last_read": "2025-02-21T23:09:43.920Z",
    "total_reading_time_seconds": 12,
    "published_date": "2020-02-13T18:50:45Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2002.05709/features/markdown-grobid/2002.05709.md",
      "adr-crib": "data/papers/2002.05709/features/adr-crib/2002.05709.md",
      "adr-titles": "data/papers/2002.05709/features/adr-titles/2002.05709.md",
      "crib-sheet": "data/papers/2002.05709/features/crib-sheet/2002.05709.md",
      "compound-crib": "data/papers/2002.05709/features/compound-crib/2002.05709.md"
    }
  },
  "2502.09509": {
    "id": "2502.09509",
    "title": "EQ-VAE: Equivariance Regularized Latent Space for Improved Generative   Image Modeling",
    "authors": "Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis",
    "abstract": "Latent generative models have emerged as a leading approach for high-quality image synthesis. These models rely on an autoencoder to compress images into a latent space, followed by a generative model to learn the latent distribution. We identify that existing autoencoders lack equivariance to semantic-preserving transformations like scaling and rotation, resulting in complex latent spaces that hinder generative performance. To address this, we propose EQ-VAE, a simple regularization approach that enforces equivariance in the latent space, reducing its complexity without degrading reconstruction quality. By finetuning pre-trained autoencoders with EQ-VAE, we enhance the performance of several state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT, achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning. EQ-VAE is compatible with both continuous and discrete autoencoders, thus offering a versatile enhancement for a wide range of latent generative models. Project page and code: https://eq-vae.github.io/.",
    "url": "https://arxiv.org/abs/2502.09509",
    "arxivId": "2502.09509",
    "last_visited": "2025-02-21T23:06:30.371Z",
    "last_read": "2025-02-21T23:06:30.371Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-13T17:21:51Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.09509/features/markdown-grobid/2502.09509.md",
      "adr-crib": "data/papers/2502.09509/features/adr-crib/2502.09509.md",
      "adr-titles": "data/papers/2502.09509/features/adr-titles/2502.09509.md",
      "crib-sheet": "data/papers/2502.09509/features/crib-sheet/2502.09509.md",
      "compound-crib": "data/papers/2502.09509/features/compound-crib/2502.09509.md"
    }
  },
  "2006.07733": {
    "id": "2006.07733",
    "title": "Bootstrap your own latent: A new approach to self-supervised Learning",
    "authors": "Jean-Bastien Grill, Florian Strub, Florent Altché and 11 others",
    "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",
    "url": "https://arxiv.org/abs/2006.07733",
    "arxivId": "2006.07733",
    "last_visited": "2025-02-21T23:06:18.432Z",
    "last_read": "2025-02-21T23:06:18.432Z",
    "total_reading_time_seconds": 23,
    "published_date": "2020-06-13T22:35:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2006.07733/features/markdown-grobid/2006.07733.md",
      "adr-crib": "data/papers/2006.07733/features/adr-crib/2006.07733.md",
      "adr-titles": "data/papers/2006.07733/features/adr-titles/2006.07733.md",
      "crib-sheet": "data/papers/2006.07733/features/crib-sheet/2006.07733.md",
      "compound-crib": "data/papers/2006.07733/features/compound-crib/2006.07733.md"
    }
  },
  "2311.00452": {
    "id": "2311.00452",
    "title": "Hessian Eigenvectors and Principal Component Analysis of Neural Network   Weight Matrices",
    "authors": "David Haink",
    "abstract": "This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both their magnitude and curvature. Furthermore, our examination showcases the applicability of principal component analysis in approximating the Hessian, with update parameters emerging as a superior choice over weights for this purpose. Remarkably, our findings unveil a similarity between the largest Hessian eigenvalues of individual layers and the entire network. Notably, higher eigenvalues are concentrated more in deeper layers. Leveraging these insights, we venture into addressing catastrophic forgetting, a challenge of neural networks when learning new tasks while retaining knowledge from previous ones. By applying our discoveries, we formulate an effective strategy to mitigate catastrophic forgetting, offering a possible solution that can be applied to networks of varying scales, including larger architectures.",
    "url": "https://arxiv.org/abs/2311.00452",
    "arxivId": "2311.00452",
    "last_visited": "2025-02-22T18:13:15.498Z",
    "last_read": "2025-02-22T18:13:15.498Z",
    "total_reading_time_seconds": 29,
    "published_date": "2023-11-01T11:38:31Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2311.00452/features/markdown-grobid/2311.00452.md",
      "adr-crib": "data/papers/2311.00452/features/adr-crib/2311.00452.md",
      "adr-titles": "data/papers/2311.00452/features/adr-titles/2311.00452.md",
      "crib-sheet": "data/papers/2311.00452/features/crib-sheet/2311.00452.md",
      "compound-crib": "data/papers/2311.00452/features/compound-crib/2311.00452.md"
    }
  },
  "2107.09133": {
    "id": "2107.09133",
    "title": "The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations,   and Anomalous Diffusion",
    "authors": "Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie and 4 others",
    "abstract": "In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD.",
    "url": "https://arxiv.org/abs/2107.09133",
    "arxivId": "2107.09133",
    "last_visited": "2025-02-22T18:10:32.678Z",
    "last_read": "2025-02-22T18:10:32.678Z",
    "total_reading_time_seconds": 42,
    "published_date": "2021-07-19T20:18:57Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.stat-mech",
      "q-bio.NC",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2107.09133/features/markdown-grobid/2107.09133.md",
      "adr-crib": "data/papers/2107.09133/features/adr-crib/2107.09133.md",
      "adr-titles": "data/papers/2107.09133/features/adr-titles/2107.09133.md",
      "crib-sheet": "data/papers/2107.09133/features/crib-sheet/2107.09133.md",
      "compound-crib": "data/papers/2107.09133/features/compound-crib/2107.09133.md"
    }
  },
  "2103.00564": {
    "id": "2103.00564",
    "title": "An Introduction to Johnson-Lindenstrauss Transforms",
    "authors": "Casper Benjamin Freksen",
    "abstract": "Johnson--Lindenstrauss Transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data, and they have found use in many fields from machine learning to differential privacy and more. This note explains what they are; it gives an overview of their use and their development since they were introduced in the 1980s; and it provides many references should the reader wish to explore these topics more deeply.",
    "url": "https://arxiv.org/abs/2103.00564",
    "arxivId": "2103.00564",
    "last_visited": "2025-02-24T00:03:16.449Z",
    "last_read": "2025-02-24T00:03:16.449Z",
    "total_reading_time_seconds": 13,
    "published_date": "2021-02-28T16:57:41Z",
    "arxiv_tags": [
      "cs.DS",
      "cs.LG",
      "F.2.2; A.1"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2103.00564/features/markdown-grobid/2103.00564.md",
      "adr-crib": "data/papers/2103.00564/features/adr-crib/2103.00564.md",
      "adr-titles": "data/papers/2103.00564/features/adr-titles/2103.00564.md",
      "crib-sheet": "data/papers/2103.00564/features/crib-sheet/2103.00564.md",
      "compound-crib": "data/papers/2103.00564/features/compound-crib/2103.00564.md"
    }
  },
  "1312.6114": {
    "id": "1312.6114",
    "title": "Auto-Encoding Variational Bayes",
    "authors": "Diederik P Kingma, Max Welling",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "url": "https://arxiv.org/abs/1312.6114",
    "arxivId": "1312.6114",
    "last_visited": "2025-02-24T00:00:40.495Z",
    "last_read": "2025-02-24T00:00:40.495Z",
    "total_reading_time_seconds": 8,
    "published_date": "2013-12-20T20:58:10Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1312.6114/features/markdown-grobid/1312.6114.md",
      "adr-crib": "data/papers/1312.6114/features/adr-crib/1312.6114.md",
      "adr-titles": "data/papers/1312.6114/features/adr-titles/1312.6114.md",
      "crib-sheet": "data/papers/1312.6114/features/crib-sheet/1312.6114.md",
      "compound-crib": "data/papers/1312.6114/features/compound-crib/1312.6114.md"
    }
  },
  "2412.14093": {
    "id": "2412.14093",
    "title": "Alignment faking in large language models",
    "authors": "Ryan Greenblatt, Carson Denison, Benjamin Wright and 17 others",
    "abstract": "We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.",
    "url": "https://arxiv.org/abs/2412.14093",
    "arxivId": "2412.14093",
    "last_visited": "2025-02-24T20:57:18.101Z",
    "last_read": "2025-02-24T20:57:18.101Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-18T17:41:24Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2412.14093/features/markdown-grobid/2412.14093.md",
      "adr-crib": "data/papers/2412.14093/features/adr-crib/2412.14093.md",
      "adr-titles": "data/papers/2412.14093/features/adr-titles/2412.14093.md",
      "crib-sheet": "data/papers/2412.14093/features/crib-sheet/2412.14093.md",
      "compound-crib": "data/papers/2412.14093/features/compound-crib/2412.14093.md"
    }
  },
  "2502.14905": {
    "id": "2502.14905",
    "title": "Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema   Adherence",
    "authors": "Bhavik Agarwal, Ishan Joshi, Viktoria Rojkova",
    "abstract": "In this paper, we address the challenge of enforcing strict schema adherence in large language model (LLM) generation by leveraging LLM reasoning capabilities. Building on the DeepSeek R1 reinforcement learning framework, our approach trains structured reasoning skills of a 1.5B parameter model through a novel pipeline that combines synthetic reasoning dataset construction with custom reward functions under Group Relative Policy Optimization (GRPO). Specifically, we first perform R1 reinforcement learning on a 20K sample unstructured-to-structured dataset, mirroring the original DeepSeek R1 methods, to establish core reasoning abilities. Subsequently, we performed supervised fine-tuning on a separate 10K reasoning sample dataset, focusing on refining schema adherence for downstream tasks. Despite the relatively modest training scope, requiring approximately 20 hours on an 8xH100 GPU cluster for GRPO training and 3 hours on 1xA100 for SFT, our model demonstrates robust performance in enforcing schema consistency. We compare our ThinkJSON approach against the original DeepSeek R1 (671B), distilled versions of DeepSeek R1 (Qwen-1.5B and Qwen-7B), and Gemini 2.0 Flash (70B), showcasing its effectiveness in real-world applications. Our results underscore the practical utility of a resource-efficient framework for schema-constrained text generation.",
    "url": "https://arxiv.org/abs/2502.14905",
    "arxivId": "2502.14905",
    "last_visited": "2025-02-25T00:41:07.489Z",
    "last_read": "2025-02-25T00:41:07.489Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-18T16:44:55Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.14905/features/markdown-grobid/2502.14905.md",
      "adr-crib": "data/papers/2502.14905/features/adr-crib/2502.14905.md",
      "adr-titles": "data/papers/2502.14905/features/adr-titles/2502.14905.md",
      "crib-sheet": "data/papers/2502.14905/features/crib-sheet/2502.14905.md",
      "compound-crib": "data/papers/2502.14905/features/compound-crib/2502.14905.md"
    }
  },
  "2502.16101": {
    "id": "2502.16101",
    "title": "Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the   Robustness of RAG Against Misleading Retrievals",
    "authors": "Linda Zeng, Rithwik Gupta, Divij Motwani and 2 others",
    "abstract": "Retrieval-augmented generation (RAG) has shown impressive capabilities in mitigating hallucinations in large language models (LLMs). However, LLMs struggle to handle misleading retrievals and often fail to maintain their own reasoning when exposed to conflicting or selectively-framed evidence, making them vulnerable to real-world misinformation. In such real-world retrieval scenarios, misleading and conflicting information is rampant, particularly in the political domain, where evidence is often selectively framed, incomplete, or polarized. However, existing RAG benchmarks largely assume a clean retrieval setting, where models succeed by accurately retrieving and generating answers from gold-standard documents. This assumption fails to align with real-world conditions, leading to an overestimation of RAG system performance. To bridge this gap, we introduce RAGuard, a fact-checking dataset designed to evaluate the robustness of RAG systems against misleading retrievals. Unlike prior benchmarks that rely on synthetic noise, our dataset constructs its retrieval corpus from Reddit discussions, capturing naturally occurring misinformation. It categorizes retrieved evidence into three types: supporting, misleading, and irrelevant, providing a realistic and challenging testbed for assessing how well RAG systems navigate different retrieval information. Our benchmark experiments reveal that when exposed to misleading retrievals, all tested LLM-powered RAG systems perform worse than their zero-shot baselines (i.e., no retrieval at all), highlighting their susceptibility to noisy environments. To the best of our knowledge, RAGuard is the first benchmark to systematically assess RAG robustness against misleading evidence. We expect this benchmark will drive future research toward improving RAG systems beyond idealized datasets, making them more reliable for real-world applications.",
    "url": "https://arxiv.org/abs/2502.16101",
    "arxivId": "2502.16101",
    "last_visited": "2025-02-25T06:47:18.730Z",
    "last_read": "2025-02-25T06:47:18.730Z",
    "total_reading_time_seconds": 32,
    "published_date": "2025-02-22T05:50:15Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.IR"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.16101/features/markdown-grobid/2502.16101.md",
      "adr-crib": "data/papers/2502.16101/features/adr-crib/2502.16101.md",
      "adr-titles": "data/papers/2502.16101/features/adr-titles/2502.16101.md",
      "crib-sheet": "data/papers/2502.16101/features/crib-sheet/2502.16101.md",
      "compound-crib": "data/papers/2502.16101/features/compound-crib/2502.16101.md"
    }
  },
  "2311.09431": {
    "id": "2311.09431",
    "title": "Striped Attention: Faster Ring Attention for Causal Transformers",
    "authors": "William Brandon, Aniruddha Nrusimha, Kevin Qian and 4 others",
    "abstract": "To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source",
    "url": "https://arxiv.org/abs/2311.09431",
    "arxivId": "2311.09431",
    "last_visited": "2025-02-25T17:14:16.747Z",
    "last_read": "2025-02-25T17:14:16.747Z",
    "total_reading_time_seconds": 4,
    "published_date": "2023-11-15T23:01:02Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2311.09431/features/markdown-grobid/2311.09431.md",
      "adr-crib": "data/papers/2311.09431/features/adr-crib/2311.09431.md",
      "adr-titles": "data/papers/2311.09431/features/adr-titles/2311.09431.md",
      "crib-sheet": "data/papers/2311.09431/features/crib-sheet/2311.09431.md",
      "compound-crib": "data/papers/2311.09431/features/compound-crib/2311.09431.md"
    }
  },
  "2502.16797": {
    "id": "2502.16797",
    "title": "Forecasting Rare Language Model Behaviors",
    "authors": "Erik Jones, Meg Tong, Jesse Mu and 7 others",
    "abstract": "Standard language model evaluations can fail to capture risks that emerge only at deployment scale. For example, a model may produce safe responses during a small-scale beta test, yet reveal dangerous information when processing billions of requests at deployment. To remedy this, we introduce a method to forecast potential risks across orders of magnitude more queries than we test during evaluation. We make forecasts by studying each query's elicitation probability -- the probability the query produces a target behavior -- and demonstrate that the largest observed elicitation probabilities predictably scale with the number of queries. We find that our forecasts can predict the emergence of diverse undesirable behaviors -- such as assisting users with dangerous chemical synthesis or taking power-seeking actions -- across up to three orders of magnitude of query volume. Our work enables model developers to proactively anticipate and patch rare failures before they manifest during large-scale deployments.",
    "url": "https://arxiv.org/abs/2502.16797",
    "arxivId": "2502.16797",
    "last_visited": "2025-02-26T00:37:33.696Z",
    "last_read": "2025-02-26T00:37:33.696Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-24T03:16:15Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.16797/features/markdown-grobid/2502.16797.md",
      "adr-crib": "data/papers/2502.16797/features/adr-crib/2502.16797.md",
      "adr-titles": "data/papers/2502.16797/features/adr-titles/2502.16797.md",
      "crib-sheet": "data/papers/2502.16797/features/crib-sheet/2502.16797.md",
      "compound-crib": "data/papers/2502.16797/features/compound-crib/2502.16797.md"
    }
  },
  "2410.12264": {
    "id": "2410.12264",
    "title": "Game Theory Meets Statistical Mechanics in Deep Learning Design",
    "authors": "Djamel Bouchaffra, Fayçal Ykhlef, Bilal Faye and 2 others",
    "abstract": "We present a novel deep graphical representation that seamlessly merges principles of game theory with laws of statistical mechanics. It performs feature extraction, dimensionality reduction, and pattern classification within a single learning framework. Our approach draws an analogy between neurons in a network and players in a game theory model. Furthermore, each neuron viewed as a classical particle (subject to statistical physics' laws) is mapped to a set of actions representing specific activation value, and neural network layers are conceptualized as games in a sequential cooperative game theory setting. The feed-forward process in deep learning is interpreted as a sequential game, where each game comprises a set of players. During training, neurons are iteratively evaluated and filtered based on their contributions to a payoff function, which is quantified using the Shapley value driven by an energy function. Each set of neurons that significantly contributes to the payoff function forms a strong coalition. These neurons are the only ones permitted to propagate the information forward to the next layers. We applied this methodology to the task of facial age estimation and gender classification. Experimental results demonstrate that our approach outperforms both multi-layer perceptron and convolutional neural network models in terms of efficiency and accuracy.",
    "url": "https://arxiv.org/abs/2410.12264",
    "arxivId": "2410.12264",
    "last_visited": "2025-02-26T06:21:52.809Z",
    "last_read": "2025-02-26T06:21:52.809Z",
    "total_reading_time_seconds": 5,
    "published_date": "2024-10-16T06:02:18Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.GT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2410.12264/features/markdown-grobid/2410.12264.md",
      "adr-crib": "data/papers/2410.12264/features/adr-crib/2410.12264.md",
      "adr-titles": "data/papers/2410.12264/features/adr-titles/2410.12264.md",
      "crib-sheet": "data/papers/2410.12264/features/crib-sheet/2410.12264.md",
      "compound-crib": "data/papers/2410.12264/features/compound-crib/2410.12264.md"
    }
  },
  "2502.18394": {
    "id": "2502.18394",
    "title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention",
    "authors": "Jacob Fein-Ashley",
    "abstract": "Conventional self-attention mechanisms incur quadratic complexity, limiting their scalability on long sequences. We introduce FFTNet, an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to achieve global token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into the frequency domain, FFTNet exploits the orthogonality and energy preservation guaranteed by Parseval's theorem to capture long-range dependencies efficiently. A learnable spectral filter and modReLU activation dynamically emphasize salient frequency components, providing a rigorous and adaptive alternative to traditional self-attention. Experiments on the Long Range Arena and ImageNet benchmarks validate our theoretical insights and demonstrate superior performance over fixed Fourier and standard attention models.",
    "url": "https://arxiv.org/abs/2502.18394",
    "arxivId": "2502.18394",
    "last_visited": "2025-02-26T07:35:39.517Z",
    "last_read": "2025-02-26T07:35:39.517Z",
    "total_reading_time_seconds": 962,
    "published_date": "2025-02-25T17:43:43Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.18394/features/markdown-grobid/2502.18394.md",
      "adr-crib": "data/papers/2502.18394/features/adr-crib/2502.18394.md",
      "adr-titles": "data/papers/2502.18394/features/adr-titles/2502.18394.md",
      "crib-sheet": "data/papers/2502.18394/features/crib-sheet/2502.18394.md",
      "compound-crib": "data/papers/2502.18394/features/compound-crib/2502.18394.md"
    }
  },
  "arxiv.2502.10059": {
    "id": "arxiv.2502.10059",
    "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive   Complex Camera Control",
    "authors": "Teng Li, Guangcong Zheng, Rui Jiang and 5 others",
    "abstract": "Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
    "url": "https://arxiv.org/abs/2502.10059",
    "arxivId": "2502.10059",
    "last_visited": "2025-02-26T09:08:48.162Z",
    "last_read": "2025-02-26T09:08:48.162Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-14T10:21:49Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "2502.10059": {
    "id": "2502.10059",
    "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive   Complex Camera Control",
    "authors": "Teng Li, Guangcong Zheng, Rui Jiang and 5 others",
    "abstract": "Recent advancements in camera-trajectory-guided image-to-video generation offer higher precision and better support for complex camera control compared to text-based approaches. However, they also introduce significant usability challenges, as users often struggle to provide precise camera parameters when working with arbitrary real-world images without knowledge of their depth nor scene scale. To address these real-world application issues, we propose RealCam-I2V, a novel diffusion-based video generation framework that integrates monocular metric depth estimation to establish 3D scene reconstruction in a preprocessing step. During training, the reconstructed 3D scene enables scaling camera parameters from relative to absolute values, ensuring compatibility and scale consistency across diverse real-world images. In inference, RealCam-I2V offers an intuitive interface where users can precisely draw camera trajectories by dragging within the 3D scene. To further enhance precise camera control and scene consistency, we propose scene-constrained noise shaping, which shapes high-level noise and also allows the framework to maintain dynamic, coherent video generation in lower noise stages. RealCam-I2V achieves significant improvements in controllability and video quality on the RealEstate10K and out-of-domain images. We further enables applications like camera-controlled looping video generation and generative frame interpolation. We will release our absolute-scale annotation, codes, and all checkpoints. Please see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
    "url": "https://arxiv.org/abs/2502.10059",
    "arxivId": "2502.10059",
    "last_visited": "2025-02-26T09:07:31.320Z",
    "last_read": "2025-02-26T09:07:31.320Z",
    "total_reading_time_seconds": 199,
    "published_date": "2025-02-14T10:21:49Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.10059/features/markdown-grobid/2502.10059.md",
      "adr-crib": "data/papers/2502.10059/features/adr-crib/2502.10059.md",
      "adr-titles": "data/papers/2502.10059/features/adr-titles/2502.10059.md",
      "crib-sheet": "data/papers/2502.10059/features/crib-sheet/2502.10059.md",
      "compound-crib": "data/papers/2502.10059/features/compound-crib/2502.10059.md"
    }
  },
  "arxiv.2103.03206": {
    "id": "arxiv.2103.03206",
    "title": "Perceiver: General Perception with Iterative Attention",
    "authors": "Andrew Jaegle, Felix Gimeno, Andrew Brock and 3 others",
    "abstract": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
    "url": "https://arxiv.org/pdf/2103.03206",
    "arxivId": "2103.03206",
    "last_visited": "2025-02-26T19:25:10.752Z",
    "last_read": "2025-02-26T19:25:10.752Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-03-04T18:20:50Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "features_path": null
  },
  "2103.03206": {
    "id": "2103.03206",
    "title": "Perceiver: General Perception with Iterative Attention",
    "authors": "Andrew Jaegle, Felix Gimeno, Andrew Brock and 3 others",
    "abstract": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
    "url": "https://arxiv.org/abs/2103.03206",
    "arxivId": "2103.03206",
    "last_visited": "2025-02-26T19:25:00.582Z",
    "last_read": "2025-02-26T19:25:00.582Z",
    "total_reading_time_seconds": 19,
    "published_date": "2021-03-04T18:20:50Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2103.03206/features/markdown-grobid/2103.03206.md",
      "adr-crib": "data/papers/2103.03206/features/adr-crib/2103.03206.md",
      "adr-titles": "data/papers/2103.03206/features/adr-titles/2103.03206.md",
      "crib-sheet": "data/papers/2103.03206/features/crib-sheet/2103.03206.md",
      "compound-crib": "data/papers/2103.03206/features/compound-crib/2103.03206.md"
    }
  },
  "arxiv.2301.10540": {
    "id": "arxiv.2301.10540",
    "title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a   General Purpose CNN",
    "authors": "David M. Knigge, David W. Romero, Albert Gu and 5 others",
    "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.",
    "url": "https://arxiv.org/pdf/2301.10540",
    "arxivId": "2301.10540",
    "last_visited": "2025-02-26T19:24:54.989Z",
    "last_read": "2025-02-26T19:24:54.989Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-01-25T12:12:47Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "2301.10540": {
    "id": "2301.10540",
    "title": "Modelling Long Range Dependencies in $N$D: From Task-Specific to a   General Purpose CNN",
    "authors": "David M. Knigge, David W. Romero, Albert Gu and 5 others",
    "abstract": "Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\\rm D}$), visual ($2{\\rm D}$) and point-cloud ($3{\\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.",
    "url": "https://arxiv.org/pdf/2301.10540",
    "arxivId": "2301.10540",
    "last_visited": "2025-02-26T19:23:26.563Z",
    "last_read": "2025-02-26T19:23:26.563Z",
    "total_reading_time_seconds": 197,
    "published_date": "2023-01-25T12:12:47Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2301.10540/features/markdown-grobid/2301.10540.md",
      "adr-crib": "data/papers/2301.10540/features/adr-crib/2301.10540.md",
      "adr-titles": "data/papers/2301.10540/features/adr-titles/2301.10540.md",
      "crib-sheet": "data/papers/2301.10540/features/crib-sheet/2301.10540.md",
      "compound-crib": "data/papers/2301.10540/features/compound-crib/2301.10540.md"
    }
  },
  "arxiv.2502.09992": {
    "id": "arxiv.2502.09992",
    "title": "Large Language Diffusion Models",
    "authors": "Shen Nie, Fengqi Zhu, Zebin You and 7 others",
    "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.",
    "url": "https://arxiv.org/abs/2502.09992",
    "arxivId": "2502.09992",
    "last_visited": "2025-02-27T05:58:28.403Z",
    "last_read": "2025-02-27T05:58:28.403Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-14T08:23:51Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": null
  },
  "2502.09992": {
    "id": "2502.09992",
    "title": "Large Language Diffusion Models",
    "authors": "Shen Nie, Fengqi Zhu, Zebin You and 7 others",
    "abstract": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.",
    "url": "https://arxiv.org/abs/2502.09992",
    "arxivId": "2502.09992",
    "last_visited": "2025-02-27T05:58:17.156Z",
    "last_read": "2025-02-27T05:58:17.156Z",
    "total_reading_time_seconds": 108,
    "published_date": "2025-02-14T08:23:51Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.09992/features/markdown-grobid/2502.09992.md",
      "adr-crib": "data/papers/2502.09992/features/adr-crib/2502.09992.md",
      "adr-titles": "data/papers/2502.09992/features/adr-titles/2502.09992.md",
      "crib-sheet": "data/papers/2502.09992/features/crib-sheet/2502.09992.md",
      "compound-crib": "data/papers/2502.09992/features/compound-crib/2502.09992.md"
    }
  },
  "arxiv.2102.08431": {
    "id": "arxiv.2102.08431",
    "title": "Complex Momentum for Optimization in Games",
    "authors": "Jonathan Lorraine, David Acuna, Paul Vicol, David Duvenaud",
    "abstract": "We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum. We give theoretical motivation for our method by proving convergence on bilinear zero-sum games for simultaneous and alternating updates. Our method gives real-valued parameter updates, making it a drop-in replacement for standard optimizers. We empirically demonstrate that complex-valued momentum can improve convergence in realistic adversarial games - like generative adversarial networks - by showing we can find better solutions with an almost identical computational cost. We also show a practical generalization to a complex-valued Adam variant, which we use to train BigGAN to better inception scores on CIFAR-10.",
    "url": "https://arxiv.org/pdf/2102.08431",
    "arxivId": "2102.08431",
    "last_visited": "2025-02-27T07:43:41.741Z",
    "last_read": "2025-02-27T07:43:41.741Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-02-16T19:55:27Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.GT"
    ],
    "features_path": null
  },
  "arxiv.1802.09419": {
    "id": "arxiv.1802.09419",
    "title": "Stochastic Hyperparameter Optimization through Hypernetworks",
    "authors": "Jonathan Lorraine, David Duvenaud",
    "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters. We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters. Our process trains a neural network to output approximately optimal weights as a function of hyperparameters. We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernetworks. We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.",
    "url": "https://arxiv.org/abs/1802.09419",
    "arxivId": "1802.09419",
    "last_visited": "2025-02-27T07:43:06.215Z",
    "last_read": "2025-02-27T07:43:06.215Z",
    "total_reading_time_seconds": 0,
    "published_date": "2018-02-26T16:04:46Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "1802.09419": {
    "id": "1802.09419",
    "title": "Stochastic Hyperparameter Optimization through Hypernetworks",
    "authors": "Jonathan Lorraine, David Duvenaud",
    "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters. We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters. Our process trains a neural network to output approximately optimal weights as a function of hyperparameters. We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernetworks. We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.",
    "url": "https://arxiv.org/abs/1802.09419",
    "arxivId": "1802.09419",
    "last_visited": "2025-02-27T07:42:59.195Z",
    "last_read": "2025-02-27T07:42:59.195Z",
    "total_reading_time_seconds": 16,
    "published_date": "2018-02-26T16:04:46Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1802.09419/features/markdown-grobid/1802.09419.md",
      "adr-crib": "data/papers/1802.09419/features/adr-crib/1802.09419.md",
      "adr-titles": "data/papers/1802.09419/features/adr-titles/1802.09419.md",
      "crib-sheet": "data/papers/1802.09419/features/crib-sheet/1802.09419.md",
      "compound-crib": "data/papers/1802.09419/features/compound-crib/1802.09419.md"
    }
  },
  "2102.08431": {
    "id": "2102.08431",
    "title": "Complex Momentum for Optimization in Games",
    "authors": "Jonathan Lorraine, David Acuna, Paul Vicol, David Duvenaud",
    "abstract": "We generalize gradient descent with momentum for optimization in differentiable games to have complex-valued momentum. We give theoretical motivation for our method by proving convergence on bilinear zero-sum games for simultaneous and alternating updates. Our method gives real-valued parameter updates, making it a drop-in replacement for standard optimizers. We empirically demonstrate that complex-valued momentum can improve convergence in realistic adversarial games - like generative adversarial networks - by showing we can find better solutions with an almost identical computational cost. We also show a practical generalization to a complex-valued Adam variant, which we use to train BigGAN to better inception scores on CIFAR-10.",
    "url": "https://arxiv.org/pdf/2102.08431",
    "arxivId": "2102.08431",
    "last_visited": "2025-02-27T07:40:37.469Z",
    "last_read": "2025-02-27T07:40:37.469Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-02-16T19:55:27Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.GT"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2102.08431/features/markdown-grobid/2102.08431.md",
      "adr-crib": "data/papers/2102.08431/features/adr-crib/2102.08431.md",
      "adr-titles": "data/papers/2102.08431/features/adr-titles/2102.08431.md",
      "crib-sheet": "data/papers/2102.08431/features/crib-sheet/2102.08431.md",
      "compound-crib": "data/papers/2102.08431/features/compound-crib/2102.08431.md"
    }
  },
  "arxiv.2102.06559": {
    "id": "arxiv.2102.06559",
    "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential   Equations",
    "authors": "Winnie Xu, Ricky T. Q. Chen, Xuechen Li, David Duvenaud",
    "abstract": "We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.",
    "url": "https://arxiv.org/abs/2102.06559",
    "arxivId": "2102.06559",
    "last_visited": "2025-02-27T07:40:37.239Z",
    "last_read": "2025-02-27T07:40:37.239Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-02-12T14:48:58Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": null
  },
  "2102.06559": {
    "id": "2102.06559",
    "title": "Infinitely Deep Bayesian Neural Networks with Stochastic Differential   Equations",
    "authors": "Winnie Xu, Ricky T. Q. Chen, Xuechen Li, David Duvenaud",
    "abstract": "We perform scalable approximate inference in continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer gives hidden units that follow a stochastic differential equation. We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior over weights approaches the true posterior. This approach brings continuous-depth Bayesian neural nets to a competitive comparison against discrete-depth alternatives, while inheriting the memory-efficient training and tunable precision of Neural ODEs.",
    "url": "https://arxiv.org/abs/2102.06559",
    "arxivId": "2102.06559",
    "last_visited": "2025-02-27T07:40:26.730Z",
    "last_read": "2025-02-27T07:40:26.730Z",
    "total_reading_time_seconds": 15,
    "published_date": "2021-02-12T14:48:58Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2102.06559/features/markdown-grobid/2102.06559.md",
      "adr-crib": "data/papers/2102.06559/features/adr-crib/2102.06559.md",
      "adr-titles": "data/papers/2102.06559/features/adr-titles/2102.06559.md",
      "crib-sheet": "data/papers/2102.06559/features/crib-sheet/2102.06559.md",
      "compound-crib": "data/papers/2102.06559/features/compound-crib/2102.06559.md"
    }
  },
  "arxiv.2402.08733": {
    "id": "arxiv.2402.08733",
    "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
    "authors": "Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison",
    "abstract": "Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.",
    "url": "https://arxiv.org/abs/2402.08733",
    "arxivId": "2402.08733",
    "last_visited": "2025-02-27T07:39:57.410Z",
    "last_read": "2025-02-27T07:39:57.410Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-13T19:01:45Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "2402.08733": {
    "id": "2402.08733",
    "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
    "authors": "Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison",
    "abstract": "Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we prove that being good at cheating (i.e. cheating whenever it improves your prediction) is equivalent to being second-order calibrated, a principled extension of ordinary calibration that allows us to construct provably-correct frequentist confidence intervals for $p(Y|X)$ and detect incorrect responses with high probability. We demonstrate empirically that our approach accurately estimates how much models don't know across ambiguous image classification, (synthetic) language modeling, and partially-observable navigation tasks, outperforming existing techniques.",
    "url": "https://arxiv.org/abs/2402.08733",
    "arxivId": "2402.08733",
    "last_visited": "2025-02-27T07:39:53.849Z",
    "last_read": "2025-02-27T07:39:53.849Z",
    "total_reading_time_seconds": 3,
    "published_date": "2024-02-13T19:01:45Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2402.08733/features/markdown-grobid/2402.08733.md",
      "adr-crib": "data/papers/2402.08733/features/adr-crib/2402.08733.md",
      "adr-titles": "data/papers/2402.08733/features/adr-titles/2402.08733.md",
      "crib-sheet": "data/papers/2402.08733/features/crib-sheet/2402.08733.md",
      "compound-crib": "data/papers/2402.08733/features/compound-crib/2402.08733.md"
    }
  },
  "2411.02353": {
    "id": "2411.02353",
    "title": "Social-RAG: Retrieving from Group Interactions to Socially Ground AI   Generation",
    "authors": "Ruotong Wang, Xinyi Zhou, Lin Qiu and 3 others",
    "abstract": "AI agents are increasingly tasked with making proactive suggestions in online spaces where groups collaborate, yet risk being unhelpful or even annoying if they fail to match group preferences or behave in socially inappropriate ways. Fortunately, group spaces have a rich history of prior interactions and affordances for social feedback that can support grounding an agent's generations to a group's interests and norms. We present Social-RAG, a workflow for socially grounding agents that retrieves context from prior group interactions, selects relevant social signals, and feeds them into a language model to generate messages in a socially aligned manner. We implement this in \\textsc{PaperPing}, a system for posting paper recommendations in group chat, leveraging social signals determined from formative studies with 39 researchers. From a three-month deployment in 18 channels reaching 500+ researchers, we observed PaperPing posted relevant messages in groups without disrupting their existing social practices, fostering group common ground.",
    "url": "https://arxiv.org/abs/2411.02353",
    "arxivId": "2411.02353",
    "last_visited": "2025-02-27T09:00:20.396Z",
    "last_read": "2025-02-27T09:00:20.396Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-04T18:21:53Z",
    "arxiv_tags": [
      "cs.HC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.02353/features/markdown-grobid/2411.02353.md",
      "adr-crib": "data/papers/2411.02353/features/adr-crib/2411.02353.md",
      "adr-titles": "data/papers/2411.02353/features/adr-titles/2411.02353.md",
      "crib-sheet": "data/papers/2411.02353/features/crib-sheet/2411.02353.md",
      "compound-crib": "data/papers/2411.02353/features/compound-crib/2411.02353.md"
    }
  },
  "arxiv.2502.18394": {
    "id": "arxiv.2502.18394",
    "title": "The FFT Strikes Back: An Efficient Alternative to Self-Attention",
    "authors": "Jacob Fein-Ashley",
    "abstract": "Conventional self-attention mechanisms incur quadratic complexity, limiting their scalability on long sequences. We introduce FFTNet, an adaptive spectral filtering framework that leverages the Fast Fourier Transform (FFT) to achieve global token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming inputs into the frequency domain, FFTNet exploits the orthogonality and energy preservation guaranteed by Parseval's theorem to capture long-range dependencies efficiently. A learnable spectral filter and modReLU activation dynamically emphasize salient frequency components, providing a rigorous and adaptive alternative to traditional self-attention. Experiments on the Long Range Arena and ImageNet benchmarks validate our theoretical insights and demonstrate superior performance over fixed Fourier and standard attention models.",
    "url": "https://arxiv.org/abs/2502.18394",
    "arxivId": "2502.18394",
    "last_visited": "2025-02-27T09:00:20.127Z",
    "last_read": "2025-02-27T09:00:20.127Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-25T17:43:43Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.1609.06783": {
    "id": "arxiv.1609.06783",
    "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor   Processes",
    "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du",
    "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.",
    "url": "https://arxiv.org/abs/1609.06783",
    "arxivId": "1609.06783",
    "last_visited": "2025-02-27T18:22:51.041Z",
    "last_read": "2025-02-27T18:22:51.041Z",
    "total_reading_time_seconds": 0,
    "published_date": "2016-09-22T00:10:16Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.CL",
      "cs.LG"
    ],
    "features_path": null
  },
  "1609.06783": {
    "id": "1609.06783",
    "title": "Nonparametric Bayesian Topic Modelling with the Hierarchical Pitman-Yor   Processes",
    "authors": "Kar Wai Lim, Wray Buntine, Changyou Chen, Lan Du",
    "abstract": "The Dirichlet process and its extension, the Pitman-Yor process, are stochastic processes that take probability distributions as a parameter. These processes can be stacked up to form a hierarchical nonparametric Bayesian model. In this article, we present efficient methods for the use of these processes in this hierarchical context, and apply them to latent variable models for text analytics. In particular, we propose a general framework for designing these Bayesian models, which are called topic models in the computer science community. We then propose a specific nonparametric Bayesian topic model for modelling text from social media. We focus on tweets (posts on Twitter) in this article due to their ease of access. We find that our nonparametric model performs better than existing parametric models in both goodness of fit and real world applications.",
    "url": "https://arxiv.org/abs/1609.06783",
    "arxivId": "1609.06783",
    "last_visited": "2025-02-27T17:53:43.134Z",
    "last_read": "2025-02-27T17:53:43.134Z",
    "total_reading_time_seconds": 2386,
    "published_date": "2016-09-22T00:10:16Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.CL",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1609.06783/features/markdown-grobid/1609.06783.md",
      "adr-crib": "data/papers/1609.06783/features/adr-crib/1609.06783.md",
      "adr-titles": "data/papers/1609.06783/features/adr-titles/1609.06783.md",
      "crib-sheet": "data/papers/1609.06783/features/crib-sheet/1609.06783.md",
      "compound-crib": "data/papers/1609.06783/features/compound-crib/1609.06783.md"
    }
  },
  "arxiv.1210.6738": {
    "id": "arxiv.1210.6738",
    "title": "Nested Hierarchical Dirichlet Processes",
    "authors": "John Paisley, Chong Wang, David M. Blei, Michael I. Jordan",
    "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 3.3 million documents from Wikipedia.",
    "url": "https://arxiv.org/pdf/1210.6738",
    "arxivId": "1210.6738",
    "last_visited": "2025-02-28T05:41:59.788Z",
    "last_read": "2025-02-28T05:41:59.788Z",
    "total_reading_time_seconds": 0,
    "published_date": "2012-10-25T04:25:00Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": null
  },
  "1210.6738": {
    "id": "1210.6738",
    "title": "Nested Hierarchical Dirichlet Processes",
    "authors": "John Paisley, Chong Wang, David M. Blei, Michael I. Jordan",
    "abstract": "We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 3.3 million documents from Wikipedia.",
    "url": "https://arxiv.org/pdf/1210.6738",
    "arxivId": "1210.6738",
    "last_visited": "2025-02-28T05:30:35.760Z",
    "last_read": "2025-02-28T05:30:35.760Z",
    "total_reading_time_seconds": 1245,
    "published_date": "2012-10-25T04:25:00Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1210.6738/features/markdown-grobid/1210.6738.md",
      "adr-crib": "data/papers/1210.6738/features/adr-crib/1210.6738.md",
      "adr-titles": "data/papers/1210.6738/features/adr-titles/1210.6738.md",
      "crib-sheet": "data/papers/1210.6738/features/crib-sheet/1210.6738.md",
      "compound-crib": "data/papers/1210.6738/features/compound-crib/1210.6738.md"
    }
  },
  "arxiv.1206.5270": {
    "id": "arxiv.1206.5270",
    "title": "Nonparametric Bayes Pachinko Allocation",
    "authors": "Wei Li, David Blei, Andrew McCallum",
    "abstract": "Recent advances in topic models have explored complicated structured distributions to represent topic correlation. For example, the pachinko allocation model (PAM) captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). While PAM provides more flexibility and greater expressive power than previous models like latent Dirichlet allocation (LDA), it is also more difficult to determine the appropriate topic structure for a specific dataset. In this paper, we propose a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although the HDP can capture topic correlations defined by nested data structure, it does not automatically discover such correlations from unstructured data. By assuming an HDP-based prior for PAM, we are able to learn both the number of topics and how the topics are correlated. We evaluate our model on synthetic and real-world text datasets, and show that nonparametric PAM achieves performance matching the best of PAM without manually tuning the number of topics.",
    "url": "https://arxiv.org/pdf/1206.5270",
    "arxivId": "1206.5270",
    "last_visited": "2025-02-28T08:44:16.653Z",
    "last_read": "2025-02-28T08:44:16.653Z",
    "total_reading_time_seconds": 0,
    "published_date": "2012-06-20T15:04:47Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": null
  },
  "1206.5270": {
    "id": "1206.5270",
    "title": "Nonparametric Bayes Pachinko Allocation",
    "authors": "Wei Li, David Blei, Andrew McCallum",
    "abstract": "Recent advances in topic models have explored complicated structured distributions to represent topic correlation. For example, the pachinko allocation model (PAM) captures arbitrary, nested, and possibly sparse correlations between topics using a directed acyclic graph (DAG). While PAM provides more flexibility and greater expressive power than previous models like latent Dirichlet allocation (LDA), it is also more difficult to determine the appropriate topic structure for a specific dataset. In this paper, we propose a nonparametric Bayesian prior for PAM based on a variant of the hierarchical Dirichlet process (HDP). Although the HDP can capture topic correlations defined by nested data structure, it does not automatically discover such correlations from unstructured data. By assuming an HDP-based prior for PAM, we are able to learn both the number of topics and how the topics are correlated. We evaluate our model on synthetic and real-world text datasets, and show that nonparametric PAM achieves performance matching the best of PAM without manually tuning the number of topics.",
    "url": "https://arxiv.org/pdf/1206.5270",
    "arxivId": "1206.5270",
    "last_visited": "2025-02-28T08:42:30.716Z",
    "last_read": "2025-02-28T08:42:30.716Z",
    "total_reading_time_seconds": 68,
    "published_date": "2012-06-20T15:04:47Z",
    "arxiv_tags": [
      "cs.IR",
      "cs.LG",
      "stat.ML"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1206.5270/features/markdown-grobid/1206.5270.md",
      "adr-crib": "data/papers/1206.5270/features/adr-crib/1206.5270.md",
      "adr-titles": "data/papers/1206.5270/features/adr-titles/1206.5270.md",
      "crib-sheet": "data/papers/1206.5270/features/crib-sheet/1206.5270.md",
      "compound-crib": "data/papers/1206.5270/features/compound-crib/1206.5270.md"
    }
  },
  "openreview.VSFR5eBP7h": {
    "id": "openreview.VSFR5eBP7h",
    "title": "OPENREVIEW Paper: VSFR5eBP7h",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=VSFR5eBP7h",
    "arxivId": "",
    "last_visited": "2025-02-28T22:04:17.313Z",
    "last_read": "2025-02-28T22:04:17.313Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2410.01131": {
    "id": "arxiv.2410.01131",
    "title": "nGPT: Normalized Transformer with Representation Learning on the   Hypersphere",
    "authors": "Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg",
    "abstract": "We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.",
    "url": "https://arxiv.org/abs/2410.01131",
    "arxivId": "2410.01131",
    "last_visited": "2025-02-28T22:02:42.541Z",
    "last_read": "2025-02-28T22:02:42.541Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-01T23:50:09Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": null
  },
  "arxiv.2110.09456": {
    "id": "arxiv.2110.09456",
    "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization",
    "authors": "Sam Shleifer, Jason Weston, Myle Ott",
    "abstract": "During pretraining, the Pre-LayerNorm transformer suffers from a gradient magnitude mismatch: gradients at early layers are much larger than at later layers. These issues can be alleviated by our proposed NormFormer architecture, which adds three normalization operations to each layer: a Layer Norm after self attention, head-wise scaling of self-attention outputs, and a Layer Norm after the first fully connected layer. The extra operations incur negligible compute cost (+0.4% parameter increase), but improve pretraining perplexity and downstream task performance for both causal and masked language models ranging from 125 Million to 2.7 Billion parameters. For example, adding NormFormer on top of our strongest 1.3B parameter baseline can reach equal perplexity 24% faster, or converge 0.27 perplexity better in the same compute budget. This model reaches GPT3-Large (1.3B) zero shot performance 60% faster. For masked language modeling, NormFormer improves fine-tuned GLUE performance by 1.9% on average. Code to train NormFormer models is available in fairseq https://github.com/pytorch/fairseq/tree/main/examples/normformer .",
    "url": "https://arxiv.org/abs/2110.09456",
    "arxivId": "2110.09456",
    "last_visited": "2025-02-28T22:02:02.816Z",
    "last_read": "2025-02-28T22:02:02.816Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-10-18T16:47:45Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": null
  },
  "arxiv.1112.5016": {
    "id": "arxiv.1112.5016",
    "title": "A Scalable Bootstrap for Massive Data",
    "authors": "Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan",
    "abstract": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets---which are increasingly prevalent---the computation of bootstrap-based quantities can be prohibitively demanding computationally. While variants such as subsampling and the $m$ out of $n$ bootstrap can be used in principle to reduce the cost of bootstrap computations, we find that these methods are generally not robust to specification of hyperparameters (such as the number of subsampled data points), and they often require use of more prior information (such as rates of convergence of estimators) than the bootstrap. As an alternative, we introduce the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to yield a robust, computationally efficient means of assessing the quality of estimators. BLB is well suited to modern parallel and distributed computing architectures and furthermore retains the generic applicability and statistical efficiency of the bootstrap. We demonstrate BLB's favorable statistical performance via a theoretical analysis elucidating the procedure's properties, as well as a simulation study comparing BLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In addition, we present results from a large-scale distributed implementation of BLB demonstrating its computational superiority on massive data, a method for adaptively selecting BLB's hyperparameters, an empirical study applying BLB to several real datasets, and an extension of BLB to time series data.",
    "url": "https://arxiv.org/abs/1112.5016",
    "arxivId": "1112.5016",
    "last_visited": "2025-03-01T00:44:52.424Z",
    "last_read": "2025-03-01T00:44:52.424Z",
    "total_reading_time_seconds": 0,
    "published_date": "2011-12-21T13:18:57Z",
    "arxiv_tags": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "features_path": null
  },
  "openreview.R4xpvDTWkV": {
    "id": "openreview.R4xpvDTWkV",
    "title": "OPENREVIEW Paper: R4xpvDTWkV",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=R4xpvDTWkV",
    "arxivId": "",
    "last_visited": "2025-03-01T07:44:36.008Z",
    "last_read": "2025-03-01T07:44:36.008Z",
    "total_reading_time_seconds": 39,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2407.17465": {
    "id": "arxiv.2407.17465",
    "title": "u-$μ$P: The Unit-Scaled Maximal Update Parametrization",
    "authors": "Charlie Blake, Constantin Eichenberg, Josef Dean and 7 others",
    "abstract": "The Maximal Update Parametrization ($\\mu$P) aims to make the optimal hyperparameters (HPs) of a model independent of its size, allowing them to be swept using a cheap proxy model rather than the full-size target model. We present a new scheme, u-$\\mu$P, which improves upon $\\mu$P by combining it with Unit Scaling, a method for designing models that makes them easy to train in low-precision. The two techniques have a natural affinity: $\\mu$P ensures that the scale of activations is independent of model size, and Unit Scaling ensures that activations, weights and gradients begin training with a scale of one. This synthesis opens the door to a simpler scheme, whose default values are near-optimal. This in turn facilitates a more efficient sweeping strategy, with u-$\\mu$P models reaching a loss that is equal to or lower than comparable $\\mu$P models and working out-of-the-box in FP8.",
    "url": "https://arxiv.org/abs/2407.17465v2",
    "arxivId": "2407.17465",
    "last_visited": "2025-03-01T07:43:24.368Z",
    "last_read": "2025-03-01T07:43:24.368Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-07-24T17:58:42Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "openreview.Vota6rFhBQ": {
    "id": "openreview.Vota6rFhBQ",
    "title": "OPENREVIEW Paper: Vota6rFhBQ",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=Vota6rFhBQ",
    "arxivId": "",
    "last_visited": "2025-03-01T18:05:11.255Z",
    "last_read": "2025-03-01T18:05:11.255Z",
    "total_reading_time_seconds": 24,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.1503.03585": {
    "id": "arxiv.1503.03585",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
    "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
    "url": "https://arxiv.org/pdf/1503.03585",
    "arxivId": "1503.03585",
    "last_visited": "2025-03-01T18:02:51.681Z",
    "last_read": "2025-03-01T18:02:51.681Z",
    "total_reading_time_seconds": 66,
    "published_date": "2015-03-12T04:51:37Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.dis-nn",
      "q-bio.NC",
      "stat.ML"
    ],
    "features_path": null
  },
  "arxiv.2107.09133": {
    "id": "arxiv.2107.09133",
    "title": "The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations,   and Anomalous Diffusion",
    "authors": "Daniel Kunin, Javier Sagastuy-Brena, Lauren Gillespie and 4 others",
    "abstract": "In this work we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance travelled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction between the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase space dynamics of the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving these dynamics is not the original training loss, but rather the combination of a modified loss, which implicitly regularizes the velocity, and probability currents, which cause oscillations in phase space. We identify qualitative and quantitative predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin for the anomalous limiting dynamics of deep neural networks trained with SGD.",
    "url": "https://arxiv.org/abs/2107.09133",
    "arxivId": "2107.09133",
    "last_visited": "2025-03-01T20:28:38.002Z",
    "last_read": "2025-03-01T20:28:38.002Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-07-19T20:18:57Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.stat-mech",
      "q-bio.NC",
      "stat.ML"
    ],
    "features_path": null
  },
  "arxiv.2502.02737": {
    "id": "arxiv.2502.02737",
    "title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language   Model",
    "authors": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch and 19 others",
    "abstract": "While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project.",
    "url": "https://arxiv.org/abs/2502.02737",
    "arxivId": "2502.02737",
    "last_visited": "2025-03-02T08:43:26.219Z",
    "last_read": "2025-03-02T08:43:26.219Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-04T21:43:16Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2305.04641": {
    "id": "arxiv.2305.04641",
    "title": "The Cure is in the Cause: A Filesystem for Container Debloating",
    "authors": "Huaifeng Zhang, Mohannad Alhanahnah, Philipp Leitner, Ahmed Ali-Eldin",
    "abstract": "Containers have become a standard for deploying applications due to their convenience, but they often suffer from significant software bloat-unused files that inflate image sizes, increase provisioning times, and waste resources. These inefficiencies are particularly problematic in serverless and edge computing scenarios, where resources are constrained, and performance is critical. Existing debloating tools are limited in scope and effectiveness, failing to address the widespread issue of container bloat at scale. In this paper, we conduct a large-scale evaluation of container bloat, analyzing the top 20 most downloaded containers on DockerHub. We evaluate two state-of-the-art debloating tools, identify their limitations, and propose a novel solution, BAFFS, which addresses bloat at the filesystem level by introducing a flexible debloating layer that preserves the layered structure of container filesystems. The debloating layer can be organized in different ways to meet diverse requirements. Our evaluation demonstrates that over 50% of the top-downloaded containers have more than 60% bloat, and BAFFS reduces container sizes significantly while maintaining functionality. For serverless functions, BAFFS reduces cold start latency by up to 68%. Additionally, when combined with lazy-loading snapshotters, BAFFS enhances provisioning efficiency, reducing conversion times by up to 93% and provisioning times by up to 19%.",
    "url": "https://arxiv.org/abs/2305.04641",
    "arxivId": "2305.04641",
    "last_visited": "2025-03-03T00:04:16.046Z",
    "last_read": "2025-03-03T00:04:16.046Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-08T11:41:30Z",
    "arxiv_tags": [
      "cs.SE"
    ],
    "features_path": null
  },
  "arxiv.2502.12018": {
    "id": "arxiv.2502.12018",
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "authors": "Fengwei Teng, Zhaoyang Yu, Quan Shi and 3 others",
    "abstract": "Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.",
    "url": "https://arxiv.org/abs/2502.12018",
    "arxivId": "2502.12018",
    "last_visited": "2025-03-03T02:57:57.436Z",
    "last_read": "2025-03-03T02:57:57.436Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-17T16:52:42Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2502.21231": {
    "id": "arxiv.2502.21231",
    "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length   on More Than 12,000 GPUs",
    "authors": "Hao Ge, Junda Feng, Qi Huang and 6 others",
    "abstract": "Scaling long-context ability is essential for Large Language Models (LLMs). To amortize the memory consumption across multiple devices in long-context training, inter-data partitioning (a.k.a. Data Parallelism) and intra-data partitioning (a.k.a. Context Parallelism) are commonly used. Current training frameworks predominantly treat the two techniques as orthogonal, and establish static communication groups to organize the devices as a static mesh (e.g., a 2D mesh). However, the sequences for LLM training typically vary in lengths, no matter for texts, multi-modalities or reinforcement learning. The mismatch between data heterogeneity and static mesh causes redundant communication and imbalanced computation, degrading the training efficiency.   In this work, we introduce ByteScale, an efficient, flexible, and scalable LLM training framework for large-scale mixed training of long and short sequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid Data Parallelism (HDP), which unifies the inter- and intra-data partitioning with a dynamic mesh design. In particular, we build a communication optimizer, which eliminates the redundant communication for short sequences by data-aware sharding and dynamic communication, and further compresses the communication cost for long sequences by selective offloading. Besides, we also develop a balance scheduler to mitigate the imbalanced computation by parallelism-aware data assignment. We evaluate ByteScale with the model sizes ranging from 7B to 141B, context lengths from 256K to 2048K, on a production cluster with more than 12,000 GPUs. Experiment results show that ByteScale outperforms the state-of-the-art training system by up to 7.89x.",
    "url": "https://arxiv.org/abs/2502.21231",
    "arxivId": "2502.21231",
    "last_visited": "2025-03-03T21:11:13.328Z",
    "last_read": "2025-03-03T21:11:13.328Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-28T17:01:03Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.1511.07948": {
    "id": "arxiv.1511.07948",
    "title": "Learning Halfspaces and Neural Networks with Random Initialization",
    "authors": "Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, Michael I. Jordan",
    "abstract": "We study non-convex empirical risk minimization for learning halfspaces and neural networks. For loss functions that are $L$-Lipschitz continuous, we present algorithms to learn halfspaces and multi-layer neural networks that achieve arbitrarily small excess risk $\\epsilon&gt;0$. The time complexity is polynomial in the input dimension $d$ and the sample size $n$, but exponential in the quantity $(L/\\epsilon^2)\\log(L/\\epsilon)$. These algorithms run multiple rounds of random initialization followed by arbitrary optimization steps. We further show that if the data is separable by some neural network with constant margin $\\gamma&gt;0$, then there is a polynomial-time algorithm for learning a neural network that separates the training data with margin $\\Omega(\\gamma)$. As a consequence, the algorithm achieves arbitrary generalization error $\\epsilon&gt;0$ with ${\\rm poly}(d,1/\\epsilon)$ sample and time complexity. We establish the same learnability result when the labels are randomly flipped with probability $\\eta&lt;1/2$.",
    "url": "https://arxiv.org/abs/1511.07948",
    "arxivId": "1511.07948",
    "last_visited": "2025-03-05T07:28:32.623Z",
    "last_read": "2025-03-05T07:28:32.623Z",
    "total_reading_time_seconds": 0,
    "published_date": "2015-11-25T04:41:20Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.1805.10694": {
    "id": "arxiv.1805.10694",
    "title": "Exponential convergence rates for Batch Normalization: The power of   length-direction decoupling in non-convex optimization",
    "authors": "Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi and 3 others",
    "abstract": "Normalization techniques such as Batch Normalization have been applied successfully for training deep neural networks. Yet, despite its apparent empirical benefits, the reasons behind the success of Batch Normalization are mostly hypothetical. We here aim to provide a more thorough theoretical understanding from a classical optimization perspective. Our main contribution towards this goal is the identification of various problem instances in the realm of machine learning where % -- under certain assumptions-- Batch Normalization can provably accelerate optimization. We argue that this acceleration is due to the fact that Batch Normalization splits the optimization task into optimizing length and direction of the parameters separately. This allows gradient-based methods to leverage a favourable global structure in the loss landscape that we prove to exist in Learning Halfspace problems and neural network training with Gaussian inputs. We thereby turn Batch Normalization from an effective practical heuristic into a provably converging algorithm for these settings. Furthermore, we substantiate our analysis with empirical evidence that suggests the validity of our theoretical results in a broader context.",
    "url": "https://arxiv.org/abs/1805.10694",
    "arxivId": "1805.10694",
    "last_visited": "2025-03-05T07:26:53.574Z",
    "last_read": "2025-03-05T07:26:53.574Z",
    "total_reading_time_seconds": 0,
    "published_date": "2018-05-27T21:33:37Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2401.12053": {
    "id": "arxiv.2401.12053",
    "title": "From trust in news to disagreement: is misinformation more   controversial?",
    "authors": "Donald Ruggiero Lo Sardo, Emanuele Brugnoli, Enrico Ubaldi and 2 others",
    "abstract": "The growing prevalence of fruitless disagreement threatens social cohesion and constructive public discourse. While polarised discussions often reflect distrust in the news, the link between disagreement and misinformation remains unclear. In this study, we used data from \"Cartesio\", an online experiment rating the trustworthiness of Italian news articles annotated for reliability by experts, to develop a disagreement metric that accounts for differences in mean trust values. Our findings show that while misinformation is rated as less trustworthy, it is not more controversial. Furthermore, disagreement correlates with increased commenting on Facebook. This suggests that combating misinformation alone may not reduce polarisation. Disagreement focuses more on the divergence of opinions, trust, and their effects on social cohesion. Our study lays the groundwork for unsupervised news analysis and highlights the need for platform design that promotes constructive interactions and reduces divisiveness.",
    "url": "https://arxiv.org/abs/2401.12053",
    "arxivId": "2401.12053",
    "last_visited": "2025-03-04T18:29:19.310Z",
    "last_read": "2025-03-04T18:29:19.310Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-01-22T15:43:14Z",
    "arxiv_tags": [
      "physics.soc-ph"
    ],
    "features_path": null
  },
  "arxiv.2503.00710": {
    "id": "arxiv.2503.00710",
    "title": "Proteina: Scaling Flow-based Protein Structure Generative Models",
    "authors": "Tomas Geffner, Kieran Didi, Zuobai Zhang and 8 others",
    "abstract": "Recently, diffusion- and flow-based generative models of protein structures have emerged as a powerful tool for de novo protein design. Here, we develop Proteina, a new large-scale flow-based protein backbone generator that utilizes hierarchical fold class labels for conditioning and relies on a tailored scalable transformer architecture with up to 5x as many parameters as previous models. To meaningfully quantify performance, we introduce a new set of metrics that directly measure the distributional similarity of generated proteins with reference sets, complementing existing metrics. We further explore scaling training data to millions of synthetic protein structures and explore improved training and sampling recipes adapted to protein backbone generation. This includes fine-tuning strategies like LoRA for protein backbones, new guidance methods like classifier-free guidance and autoguidance for protein backbones, and new adjusted training objectives. Proteina achieves state-of-the-art performance on de novo protein backbone design and produces diverse and designable proteins at unprecedented length, up to 800 residues. The hierarchical conditioning offers novel control, enabling high-level secondary-structure guidance as well as low-level fold-specific generation.",
    "url": "https://arxiv.org/abs/2503.00710",
    "arxivId": "2503.00710",
    "last_visited": "2025-03-04T18:17:53.077Z",
    "last_read": "2025-03-04T18:17:53.077Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-02T03:21:49Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2503.01776": {
    "id": "arxiv.2503.01776",
    "title": "Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation",
    "authors": "Tiansheng Wen, Yifei Wang, Zequn Zeng and 7 others",
    "abstract": "Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. Code is available at https://github.com/neilwen987/CSR_Adaptive_Rep",
    "url": "https://arxiv.org/abs/2503.01776",
    "arxivId": "2503.01776",
    "last_visited": "2025-03-04T06:13:31.788Z",
    "last_read": "2025-03-04T06:13:31.788Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-03T17:59:48Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IR"
    ],
    "features_path": null
  },
  "arxiv.2502.17019": {
    "id": "arxiv.2502.17019",
    "title": "Erwin: A Tree-based Hierarchical Transformer for Large-scale Physical   Systems",
    "authors": "Maksim Zhdanov, Max Welling, Jan-Willem van de Meent",
    "abstract": "Large-scale physical systems defined on irregular grids pose significant scalability challenges for deep learning methods, especially in the presence of long-range interactions and multi-scale coupling. Traditional approaches that compute all pairwise interactions, such as attention, become computationally prohibitive as they scale quadratically with the number of nodes. We present Erwin, a hierarchical transformer inspired by methods from computational many-body physics, which combines the efficiency of tree-based algorithms with the expressivity of attention mechanisms. Erwin employs ball tree partitioning to organize computation, which enables linear-time attention by processing nodes in parallel within local neighborhoods of fixed size. Through progressive coarsening and refinement of the ball tree structure, complemented by a novel cross-ball interaction mechanism, it captures both fine-grained local details and global features. We demonstrate Erwin's effectiveness across multiple domains, including cosmology, molecular dynamics, and particle fluid dynamics, where it consistently outperforms baseline methods both in accuracy and computational efficiency.",
    "url": "https://arxiv.org/abs/2502.17019",
    "arxivId": "2502.17019",
    "last_visited": "2025-03-05T19:10:52.918Z",
    "last_read": "2025-03-05T19:10:52.918Z",
    "total_reading_time_seconds": 25,
    "published_date": "2025-02-24T10:16:55Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": null
  },
  "arxiv.2103.02096": {
    "id": "arxiv.2103.02096",
    "title": "An Alternative Practice of Tropical Convolution to Traditional   Convolutional Neural Networks",
    "authors": "Shiqing Fan, Liu Liying, Ye Luo",
    "abstract": "Convolutional neural networks (CNNs) have been used in many machine learning fields. In practical applications, the computational cost of convolutional neural networks is often high with the deepening of the network and the growth of data volume, mostly due to a large amount of multiplication operations of floating-point numbers in convolution operations. To reduce the amount of multiplications, we propose a new type of CNNs called Tropical Convolutional Neural Networks (TCNNs) which are built on tropical convolutions in which the multiplications and additions in conventional convolutional layers are replaced by additions and min/max operations respectively. In addition, since tropical convolution operators are essentially nonlinear operators, we expect TCNNs to have higher nonlinear fitting ability than conventional CNNs. In the experiments, we test and analyze several different architectures of TCNNs for image classification tasks in comparison with similar-sized conventional CNNs. The results show that TCNN can achieve higher expressive power than ordinary convolutional layers on the MNIST and CIFAR10 image data set. In different noise environments, there are wins and losses in the robustness of TCNN and ordinary CNNs.",
    "url": "https://arxiv.org/abs/2103.02096",
    "arxivId": "2103.02096",
    "last_visited": "2025-03-06T02:06:18.648Z",
    "last_read": "2025-03-06T02:06:18.648Z",
    "total_reading_time_seconds": 7,
    "published_date": "2021-03-03T00:13:30Z",
    "arxiv_tags": [
      "cs.CV",
      "14T90",
      "G.2.1; I.2.6; I.5.1"
    ],
    "features_path": null
  },
  "arxiv.2010.01185": {
    "id": "arxiv.2010.01185",
    "title": "Compressing Images by Encoding Their Latent Representations with   Relative Entropy Coding",
    "authors": "Gergely Flamich, Marton Havasi, José Miguel Hernández-Lobato",
    "abstract": "Variational Autoencoders (VAEs) have seen widespread use in learned image compression. They are used to learn expressive latent representations on which downstream compression methods can operate with high efficiency. Recently proposed 'bits-back' methods can indirectly encode the latent representation of images with codelength close to the relative entropy between the latent posterior and the prior. However, due to the underlying algorithm, these methods can only be used for lossless compression, and they only achieve their nominal efficiency when compressing multiple images simultaneously; they are inefficient for compressing single images. As an alternative, we propose a novel method, Relative Entropy Coding (REC), that can directly encode the latent representation with codelength close to the relative entropy for single images, supported by our empirical results obtained on the Cifar10, ImageNet32 and Kodak datasets. Moreover, unlike previous bits-back methods, REC is immediately applicable to lossy compression, where it is competitive with the state-of-the-art on the Kodak dataset.",
    "url": "https://arxiv.org/abs/2010.01185",
    "arxivId": "2010.01185",
    "last_visited": "2025-03-06T02:06:16.637Z",
    "last_read": "2025-03-06T02:06:16.637Z",
    "total_reading_time_seconds": 0,
    "published_date": "2020-10-02T20:23:22Z",
    "arxiv_tags": [
      "cs.IT",
      "eess.IV",
      "math.IT",
      "stat.ML",
      "94A08 (Primary) 94A34 (Secondary)",
      "E.4; G.3; H.1.1"
    ],
    "features_path": null
  },
  "arxiv.2411.19108": {
    "id": "arxiv.2411.19108",
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "authors": "Feng Liu, Shiwei Zhang, Xiaofeng Wang and 6 others",
    "abstract": "As a fundamental backbone for video generation, diffusion models are challenged by low inference speed due to the sequential nature of denoising. Previous methods speed up the models by caching and reusing model outputs at uniformly selected timesteps. However, such a strategy neglects the fact that differences among model outputs are not uniform across timesteps, which hinders selecting the appropriate model outputs to cache, leading to a poor balance between inference efficiency and visual quality. In this study, we introduce Timestep Embedding Aware Cache (TeaCache), a training-free caching approach that estimates and leverages the fluctuating differences among model outputs across timesteps. Rather than directly using the time-consuming model outputs, TeaCache focuses on model inputs, which have a strong correlation with the modeloutputs while incurring negligible computational cost. TeaCache first modulates the noisy inputs using the timestep embeddings to ensure their differences better approximating those of model outputs. TeaCache then introduces a rescaling strategy to refine the estimated differences and utilizes them to indicate output caching. Experiments show that TeaCache achieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07% Vbench score) degradation of visual quality.",
    "url": "https://arxiv.org/abs/2411.19108",
    "arxivId": "2411.19108",
    "last_visited": "2025-03-06T19:16:21.213Z",
    "last_read": "2025-03-06T19:16:21.213Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-28T12:50:05Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "arxiv.2503.01307": {
    "id": "arxiv.2503.01307",
    "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four   Habits of Highly Effective STaRs",
    "authors": "Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh and 2 others",
    "abstract": "Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.",
    "url": "https://arxiv.org/abs/2503.01307",
    "arxivId": "2503.01307",
    "last_visited": "2025-03-06T19:14:41.546Z",
    "last_read": "2025-03-06T19:14:41.546Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-03T08:46:22Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2503.02812": {
    "id": "arxiv.2503.02812",
    "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
    "authors": "Nathan Godey, Alessio Devoto, Yu Zhao and 4 others",
    "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids re-computing past hidden states during generation, making it faster. As model sizes and context lengths grow, the KV Cache becomes a significant memory bottleneck, which calls for compression methods that limit its size during generation. In this paper, we discover surprising properties of Query (Q) and Key (K) vectors that allow us to efficiently approximate attention scores without computing the attention maps. We propose Q-Filters, a training-free KV Cache compression method that filters out less crucial Key-Value pairs based on a single context-agnostic projection. Contrarily to many alternatives, Q-Filters is compatible with FlashAttention, as it does not require direct access to attention weights. Experimental results in long-context settings demonstrate that Q-Filters is competitive with attention-based compression methods such as SnapKV in retrieval tasks while consistently outperforming efficient compression schemes such as Streaming-LLM in generation setups. Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task with a x32 compression level while reducing the generation perplexity drop by up to 65% in text generation compared to Streaming-LLM.",
    "url": "https://arxiv.org/abs/2503.02812",
    "arxivId": "2503.02812",
    "last_visited": "2025-03-06T17:00:20.510Z",
    "last_read": "2025-03-06T17:00:20.510Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-04T17:37:49Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": null
  },
  "arxiv.2411.10958": {
    "id": "arxiv.2411.10958",
    "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and   Per-thread INT4 Quantization",
    "authors": "Jintao Zhang, Haofeng Huang, Pengle Zhang and 3 others",
    "abstract": "Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy for $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.",
    "url": "https://arxiv.org/abs/2411.10958",
    "arxivId": "2411.10958",
    "last_visited": "2025-03-06T19:22:24.293Z",
    "last_read": "2025-03-06T19:22:24.293Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-17T04:35:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "cs.PF"
    ],
    "features_path": null
  },
  "arxiv.2410.02367": {
    "id": "arxiv.2410.02367",
    "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference   Acceleration",
    "authors": "Jintao Zhang, Jia wei, Haofeng Huang and 3 others",
    "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention.",
    "url": "https://arxiv.org/abs/2410.02367",
    "arxivId": "2410.02367",
    "last_visited": "2025-03-06T19:20:07.058Z",
    "last_read": "2025-03-06T19:20:07.058Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-10-03T10:25:23Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2102.12321": {
    "id": "arxiv.2102.12321",
    "title": "AGENT: A Benchmark for Core Psychological Reasoning",
    "authors": "Tianmin Shu, Abhishek Bhandwaldar, Chuang Gan and 6 others",
    "abstract": "For machine agents to successfully interact with humans in real-world settings, they will need to develop an understanding of human mental life. Intuitive psychology, the ability to reason about hidden mental variables that drive observable actions, comes naturally to people: even pre-verbal infants can tell agents from objects, expecting agents to act efficiently to achieve goals given constraints. Despite recent interest in machine agents that reason about other agents, it is not clear if such agents learn or hold the core psychology principles that drive human reasoning. Inspired by cognitive development studies on intuitive psychology, we present a benchmark consisting of a large dataset of procedurally generated 3D animations, AGENT (Action, Goal, Efficiency, coNstraint, uTility), structured around four scenarios (goal preferences, action efficiency, unobserved constraints, and cost-reward trade-offs) that probe key concepts of core intuitive psychology. We validate AGENT with human-ratings, propose an evaluation protocol emphasizing generalization, and compare two strong baselines built on Bayesian inverse planning and a Theory of Mind neural network. Our results suggest that to pass the designed tests of core intuitive psychology at human levels, a model must acquire or have built-in representations of how agents plan, combining utility computations and core knowledge of objects and physics.",
    "url": "https://arxiv.org/pdf/2102.12321",
    "arxivId": "2102.12321",
    "last_visited": "2025-03-06T19:49:50.136Z",
    "last_read": "2025-03-06T19:49:50.136Z",
    "total_reading_time_seconds": 0,
    "published_date": "2021-02-24T14:58:23Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2502.21098": {
    "id": "arxiv.2502.21098",
    "title": "Re-evaluating Theory of Mind evaluation in large language models",
    "authors": "Jennifer Hu, Felix Sosa, Tomer Ullman",
    "abstract": "The question of whether large language models (LLMs) possess Theory of Mind (ToM) -- often defined as the ability to reason about others' mental states -- has sparked significant scientific and public interest. However, the evidence as to whether LLMs possess ToM is mixed, and the recent growth in evaluations has not resulted in a convergence. Here, we take inspiration from cognitive science to re-evaluate the state of ToM evaluation in LLMs. We argue that a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors. We also highlight ways in which current evaluations may be deviating from \"pure\" measurements of ToM abilities, which also contributes to the confusion. We conclude by discussing several directions for future research, including the relationship between ToM and pragmatic communication, which could advance our understanding of artificial systems as well as human cognition.",
    "url": "https://arxiv.org/abs/2502.21098",
    "arxivId": "2502.21098",
    "last_visited": "2025-03-06T19:45:02.464Z",
    "last_read": "2025-03-06T19:45:02.464Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-28T14:36:57Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2403.07183": {
    "id": "arxiv.2403.07183",
    "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of   ChatGPT on AI Conference Peer Reviews",
    "authors": "Weixin Liang, Zachary Izzo, Yaohui Zhang and 9 others",
    "abstract": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
    "url": "https://arxiv.org/abs/2403.07183",
    "arxivId": "2403.07183",
    "last_visited": "2025-03-06T19:56:06.153Z",
    "last_read": "2025-03-06T19:56:06.153Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-03-11T21:51:39Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI",
      "I.2.7"
    ],
    "features_path": null
  },
  "arxiv.2502.09747": {
    "id": "arxiv.2502.09747",
    "title": "The Widespread Adoption of Large Language Model-Assisted Writing Across   Society",
    "authors": "Weixin Liang, Yaohui Zhang, Mihai Codreanu and 3 others",
    "abstract": "The recent advances in large language models (LLMs) attracted significant public and policymaker interest in its adoption patterns. In this paper, we systematically analyze LLM-assisted writing across four domains-consumer complaints, corporate communications, job postings, and international organization press releases-from January 2022 to September 2024. Our dataset includes 687,241 consumer complaints, 537,413 corporate press releases, 304.3 million job postings, and 15,919 United Nations (UN) press releases. Using a robust population-level statistical framework, we find that LLM usage surged following the release of ChatGPT in November 2022. By late 2024, roughly 18% of financial consumer complaint text appears to be LLM-assisted, with adoption patterns spread broadly across regions and slightly higher in urban areas. For corporate press releases, up to 24% of the text is attributable to LLMs. In job postings, LLM-assisted writing accounts for just below 10% in small firms, and is even more common among younger firms. UN press releases also reflect this trend, with nearly 14% of content being generated or modified by LLMs. Although adoption climbed rapidly post-ChatGPT, growth appears to have stabilized by 2024, reflecting either saturation in LLM adoption or increasing subtlety of more advanced models. Our study shows the emergence of a new reality in which firms, consumers and even international organizations substantially rely on generative AI for communications.",
    "url": "https://arxiv.org/abs/2502.09747",
    "arxivId": "2502.09747",
    "last_visited": "2025-03-06T19:52:48.985Z",
    "last_read": "2025-03-06T19:52:48.985Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-13T20:07:03Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.1408.6944": {
    "id": "arxiv.1408.6944",
    "title": "An introduction to Mandelbrot cascades",
    "authors": "Yanick Heurteaux",
    "abstract": "In this course, we propose an elementary and self-contained introduction to canonical Mandelbrot random cascades. The multiplicative construction is explained and the necessary and sufficient condition of non-degeneracy is proved. Then, we discuss the problem of the existence of moments and the link with nondegeneracy. We also calculate the almost sure dimension of the measures. Finally, we give an outline on multifractal analysis of Mandelbrot cascades. This course was delivered in september 2013 during a meeting of the \"Multifractal Analysis GDR\" (GDR no 3475 of the french CNRS).",
    "url": "https://arxiv.org/abs/1408.6944",
    "arxivId": "1408.6944",
    "last_visited": "2025-03-06T20:27:40.602Z",
    "last_read": "2025-03-06T20:27:40.602Z",
    "total_reading_time_seconds": 0,
    "published_date": "2014-08-29T07:57:08Z",
    "arxiv_tags": [
      "math.PR"
    ],
    "features_path": null
  },
  "arxiv.2503.02921": {
    "id": "arxiv.2503.02921",
    "title": "Applications of Entropy in Data Analysis and Machine Learning: A Review",
    "authors": "Salomé A. Sepúveda Fontaine, José M. Amigó",
    "abstract": "Since its origin in the thermodynamics of the 19th century, the concept of entropy has also permeated other fields of physics and mathematics, such as Classical and Quantum Statistical Mechanics, Information Theory, Probability Theory, Ergodic Theory and the Theory of Dynamical Systems. Specifically, we are referring to the classical entropies: the Boltzmann-Gibbs, von Neumann, Shannon, Kolmogorov-Sinai and topological entropies. In addition to their common name, which is historically justified (as we briefly describe in this review), other commonality of the classical entropies is the important role that they have played and are still playing in the theory and applications of their respective fields and beyond. Therefore, it is not surprising that, in the course of time, many other instances of the overarching concept of entropy have been proposed, most of them tailored to specific purposes. Following the current usage, we will refer to all of them, whether classical or new, simply as entropies. Precisely, the subject of this review is their applications in data analysis and machine learning. The reason for these particular applications is that entropies are very well suited to characterize probability mass distributions, typically generated by finite-state processes or symbolized signals. Therefore, we will focus on entropies defined as positive functionals on probability mass distributions and provide an axiomatic characterization that goes back to Shannon and Khinchin. Given the plethora of entropies in the literature, we have selected a representative group, including the classical ones. The applications summarized in this review finely illustrate the power and versatility of entropy in data analysis and machine learning.",
    "url": "https://arxiv.org/abs/2503.02921",
    "arxivId": "2503.02921",
    "last_visited": "2025-03-06T19:59:35.421Z",
    "last_read": "2025-03-06T19:59:35.421Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-04T17:43:48Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "Primary: 94A16, Secondary: 62R07"
    ],
    "features_path": null
  },
  "arxiv.2501.11457": {
    "id": "arxiv.2501.11457",
    "title": "Governance of Generative AI in Creative Work: Consent, Credit,   Compensation, and Beyond",
    "authors": "Lin Kyi, Amruta Mahuli, M. Six Silberman and 3 others",
    "abstract": "Since the emergence of generative AI, creative workers have spoken up about the career-based harms they have experienced arising from this new technology. A common theme in these accounts of harm is that generative AI models are trained on workers' creative output without their consent and without giving credit or compensation to the original creators.   This paper reports findings from 20 interviews with creative workers in three domains: visual art and design, writing, and programming. We investigate the gaps between current AI governance strategies, what creative workers want out of generative AI governance, and the nuanced role of creative workers' consent, compensation and credit for training AI models on their work. Finally, we make recommendations for how generative AI can be governed and how operators of generative AI systems might more ethically train models on creative output in the future.",
    "url": "https://arxiv.org/abs/2501.11457",
    "arxivId": "2501.11457",
    "last_visited": "2025-03-06T20:55:11.910Z",
    "last_read": "2025-03-06T20:55:11.910Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-20T12:44:13Z",
    "arxiv_tags": [
      "cs.HC"
    ],
    "features_path": null
  },
  "arxiv.2411.04368": {
    "id": "arxiv.2411.04368",
    "title": "Measuring short-form factuality in large language models",
    "authors": "Jason Wei, Nguyen Karina, Hyung Won Chung and 5 others",
    "abstract": "We present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models \"know what they know,\" and our hope is that this benchmark will remain relevant for the next few generations of frontier models. SimpleQA can be found at https://github.com/openai/simple-evals.",
    "url": "https://arxiv.org/pdf/2411.04368",
    "arxivId": "2411.04368",
    "last_visited": "2025-03-06T20:52:57.347Z",
    "last_read": "2025-03-06T20:52:57.347Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-11-07T01:58:42Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2503.03312": {
    "id": "arxiv.2503.03312",
    "title": "How manipulable are prediction markets?",
    "authors": "Itzhak Rasooly, Roberto Rozzi",
    "abstract": "In this paper, we conduct a large-scale field experiment to investigate the manipulability of prediction markets. The main experiment involves randomly shocking prices across 817 separate markets; we then collect hourly price data to examine whether the effects of these shocks persist over time. We find that prediction markets can be manipulated: the effects of our trades are visible even 60 days after they have occurred. However, as predicted by our model, the effects of the manipulations somewhat fade over time. Markets with more traders, greater trading volume, and an external source of probability estimates are harder to manipulate.",
    "url": "https://arxiv.org/abs/2503.03312",
    "arxivId": "2503.03312",
    "last_visited": "2025-03-06T20:59:45.013Z",
    "last_read": "2025-03-06T20:59:45.013Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-05T09:44:56Z",
    "arxiv_tags": [
      "econ.GN",
      "q-fin.EC"
    ],
    "features_path": null
  },
  "arxiv.2304.03442": {
    "id": "arxiv.2304.03442",
    "title": "Generative Agents: Interactive Simulacra of Human Behavior",
    "authors": "Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai and 3 others",
    "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
    "url": "https://arxiv.org/abs/2304.03442",
    "arxivId": "2304.03442",
    "last_visited": "2025-03-06T21:02:03.006Z",
    "last_read": "2025-03-06T21:02:03.006Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-07T01:55:19Z",
    "arxiv_tags": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2502.15815": {
    "id": "arxiv.2502.15815",
    "title": "Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI   Reasoning Capabilities in Theoretical Physics",
    "authors": "Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk and 5 others",
    "abstract": "We introduce a benchmark to evaluate the capability of AI to solve problems in theoretical physics, focusing on high-energy theory and cosmology. The first iteration of our benchmark consists of 57 problems of varying difficulty, from undergraduate to research level. These problems are novel in the sense that they do not come from public problem collections. We evaluate our data set on various open and closed language models, including o3-mini, o1, DeepSeek-R1, GPT-4o and versions of Llama and Qwen. While we find impressive progress in model performance with the most recent models, our research-level difficulty problems are mostly unsolved. We address challenges of auto-verifiability and grading, and discuss common failure modes. While currently state-of-the art models are still of limited use for researchers, our results show that AI assisted theoretical physics research may become possible in the near future. We discuss the main obstacles towards this goal and possible strategies to overcome them. The public problems and solutions, results for various models, and updates to the data set and score distribution, are available on the website of the dataset tpbench.org.",
    "url": "https://arxiv.org/abs/2502.15815",
    "arxivId": "2502.15815",
    "last_visited": "2025-03-06T21:08:16.326Z",
    "last_read": "2025-03-06T21:08:16.326Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-19T19:00:00Z",
    "arxiv_tags": [
      "cs.LG",
      "astro-ph.CO",
      "cs.AI",
      "hep-ph",
      "hep-th"
    ],
    "features_path": null
  },
  "arxiv.2503.01328": {
    "id": "arxiv.2503.01328",
    "title": "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory   Optimization",
    "authors": "Xinyi Wan, Penghui Qi, Guangxing Huang and 2 others",
    "abstract": "Pipeline parallelism (PP) is widely used for training large language models (LLMs), yet its scalability is often constrained by high activation memory consumption as the number of in-flight microbatches grows with the degree of PP. In this paper, we focus on addressing this challenge by leveraging the under-explored memory offload strategy in PP. With empirical study, we discover that in the majority of standard configurations, at least half, and potentially all, of the activations can be offloaded with negligible overhead. In the cases where full overload is not possible, we introduce a novel selective offload strategy that decreases peak activation memory in a better-than-linear manner. Furthermore, we integrate memory offload with other techniques to jointly consider overall throughput and memory limitation. Our experiments proves that the per-device activation memory effectively reduces with the total number of stages, making PP a stronger alternative than TP, offering up to a 19\\% acceleration with even lower memory consumption. The implementation is open-sourced at \\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.",
    "url": "https://arxiv.org/abs/2503.01328",
    "arxivId": "2503.01328",
    "last_visited": "2025-03-06T21:06:29.168Z",
    "last_read": "2025-03-06T21:06:29.168Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-03T09:11:06Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "features_path": null
  },
  "arxiv.2503.03730": {
    "id": "arxiv.2503.03730",
    "title": "Towards Understanding Distilled Reasoning Models: A Representational   Approach",
    "authors": "David D. Baek, Max Tegmark",
    "abstract": "In this paper, we investigate how model distillation impacts the development of reasoning features in large language models (LLMs). To explore this, we train a crosscoder on Qwen-series models and their fine-tuned variants. Our results suggest that the crosscoder learns features corresponding to various types of reasoning, including self-reflection and computation verification. Moreover, we observe that distilled models contain unique reasoning feature directions, which could be used to steer the model into over-thinking or incisive-thinking mode. In particular, we perform analysis on four specific reasoning categories: (a) self-reflection, (b) deductive reasoning, (c) alternative reasoning, and (d) contrastive reasoning. Finally, we examine the changes in feature geometry resulting from the distillation process and find indications that larger distilled models may develop more structured representations, which correlate with enhanced distillation performance. By providing insights into how distillation modifies the model, our study contributes to enhancing the transparency and reliability of AI systems.",
    "url": "https://arxiv.org/abs/2503.03730",
    "arxivId": "2503.03730",
    "last_visited": "2025-03-06T21:04:15.628Z",
    "last_read": "2025-03-06T21:04:15.628Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-05T18:40:19Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2412.10924": {
    "id": "arxiv.2412.10924",
    "title": "Tokens, the oft-overlooked appetizer: Large language models, the   distributional hypothesis, and meaning",
    "authors": "Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer and 9 others",
    "abstract": "Tokenization is a necessary component within the current architecture of many language models, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model's cognition is often overlooked. We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is sufficient for reasonably human-like language performance, and that the emergence of human-meaningful linguistic units among tokens motivates linguistically-informed interventions in existing, linguistically-agnostic tokenization techniques, particularly with respect to their roles as (1) semantic primitives and as (2) vehicles for conveying salient distributional patterns from human language to the model. We explore tokenizations from a BPE tokenizer; extant model vocabularies obtained from Hugging Face and tiktoken; and the information in exemplar token vectors as they move through the layers of a RoBERTa (large) model. Besides creating sub-optimal semantic building blocks and obscuring the model's access to the necessary distributional patterns, we describe how tokenization pretraining can be a backdoor for bias and other unwanted content, which current alignment practices may not remediate. Additionally, we relay evidence that the tokenization algorithm's objective function impacts the LLM's cognition, despite being meaningfully insulated from the main system intelligence.",
    "url": "https://arxiv.org/abs/2412.10924",
    "arxivId": "2412.10924",
    "last_visited": "2025-03-06T21:28:57.560Z",
    "last_read": "2025-03-06T21:28:57.560Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-12-14T18:18:52Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI"
    ],
    "features_path": null
  },
  "arxiv.2305.13707": {
    "id": "arxiv.2305.13707",
    "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial   Language Models",
    "authors": "Orevaoghene Ahia, Sachin Kumar, Hila Gonen and 4 others",
    "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.",
    "url": "https://arxiv.org/abs/2305.13707",
    "arxivId": "2305.13707",
    "last_visited": "2025-03-06T21:28:01.531Z",
    "last_read": "2025-03-06T21:28:01.531Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-05-23T05:46:45Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2501.02393": {
    "id": "arxiv.2501.02393",
    "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "authors": "Markus J. Buehler",
    "abstract": "We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach captures complex dependencies and generalizes across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). We discuss latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood. By evolving Transformers as hierarchical GIN models for relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.",
    "url": "https://arxiv.org/abs/2501.02393",
    "arxivId": "2501.02393",
    "last_visited": "2025-03-06T21:14:39.253Z",
    "last_read": "2025-03-06T21:14:39.253Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-01-04T22:30:21Z",
    "arxiv_tags": [
      "cs.LG",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2403.03737": {
    "id": "arxiv.2403.03737",
    "title": "Probabilistic Topic Modelling with Transformer Representations",
    "authors": "Arik Reuter, Anton Thielmann, Christoph Weisser and 2 others",
    "abstract": "Topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in Natural Language Processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the Transformer-Representation Neural Topic Model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modelling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modelling, as in models such as Latent Dirichlet Allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modelling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at https://github.com/ArikReuter/TNTM.",
    "url": "https://arxiv.org/pdf/2403.03737",
    "arxivId": "2403.03737",
    "last_visited": "2025-03-06T22:07:43.010Z",
    "last_read": "2025-03-06T22:07:43.010Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-03-06T14:27:29Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2408.08541": {
    "id": "arxiv.2408.08541",
    "title": "Where is the signal in tokenization space?",
    "authors": "Renato Lui Geh, Honghua Zhang, Kareem Ahmed and 2 others",
    "abstract": "Large Language Models (LLMs) are typically shipped with tokenizers that deterministically encode text into so-called canonical token sequences, to which the LLMs assign probability values. One common assumption is that the probability of a piece of text is the probability of its canonical token sequence. However, the tokenization of a string is not unique: e.g., the Llama2 tokenizer encodes Tokens as [Tok,ens], but [Tok,en,s] also represents the same text. In this paper, we study non-canonical tokenizations. We prove that, given a string, it is computationally hard to find the most likely tokenization for an autoregressive LLM, as well as to compute the marginal probability over all possible tokenizations. We then show how the marginal is, in most cases, indistinguishable from the canonical probability. Surprisingly, we then empirically demonstrate the existence of a significant amount of signal hidden within tokenization space. Notably, by simply aggregating the probabilities of non-canonical tokenizations, we achieve improvements across a range of LLM evaluation benchmarks for a variety of architectures, including transformers and state space models.",
    "url": "https://arxiv.org/pdf/2408.08541",
    "arxivId": "2408.08541",
    "last_visited": "2025-03-06T21:38:06.846Z",
    "last_read": "2025-03-06T21:38:06.846Z",
    "total_reading_time_seconds": 56,
    "published_date": "2024-08-16T05:56:10Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2502.15343": {
    "id": "arxiv.2502.15343",
    "title": "Tokenization is Sensitive to Language Variation",
    "authors": "Anna Wegmann, Dong Nguyen, David Jurgens",
    "abstract": "Variation in language is ubiquitous and often systematically linked to regional, social, and contextual factors. Tokenizers split texts into smaller units and might behave differently for less common linguistic forms. This might affect downstream LLM performance differently on two types of tasks: Tasks where the model should be robust to language variation (e.g., for semantic tasks like NLI, labels do not depend on whether a text uses British or American spelling) and tasks where the model should be sensitive to language variation (e.g., for form-based tasks like authorship verification, labels depend on whether a text uses British or American spelling). We pre-train BERT base models for the popular Byte-Pair Encoding algorithm to investigate how key algorithmic design choices impact downstream models' performances: fitting corpus, pre-tokenizer and vocabulary size. We find that the best tokenizer varies on the two task types -- with the pre-tokenizer having the biggest impact on performance. Further, we introduce a new approach to estimate tokenizer impact on downstream LLM performance, showing significant improvement over techniques like R\\'enyi efficiency. We encourage more work on language variation and its relation to tokenizers and thus LLM performance.",
    "url": "https://arxiv.org/abs/2502.15343",
    "arxivId": "2502.15343",
    "last_visited": "2025-03-06T21:37:25.435Z",
    "last_read": "2025-03-06T21:37:25.435Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-21T09:58:54Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2502.19413": {
    "id": "arxiv.2502.19413",
    "title": "Project Alexandria: Towards Freeing Scientific Knowledge from Copyright   Burdens via LLMs",
    "authors": "Christoph Schuhmann, Gollam Rabby, Ameya Prabhu and 9 others",
    "abstract": "Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We urge the community to adopt a new idea: convert scholarly documents into Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units: (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.",
    "url": "https://arxiv.org/abs/2502.19413",
    "arxivId": "2502.19413",
    "last_visited": "2025-03-06T22:21:09.991Z",
    "last_read": "2025-03-06T22:21:09.991Z",
    "total_reading_time_seconds": 90,
    "published_date": "2025-02-26T18:56:52Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2502.16732": {
    "id": "arxiv.2502.16732",
    "title": "DeepSeek reshaping healthcare in China's tertiary hospitals",
    "authors": "Jishizhan Chen, Qingzeng Zhang",
    "abstract": "The rapid integration of artificial intelligence (AI) into healthcare is transforming clinical decision-making and hospital operations. DeepSeek has emerged as a leading AI system, widely deployed across China's tertiary hospitals since January 2025. Initially implemented in Shanghai's major medical institutions, it has since expanded nationwide, enhancing diagnostic accuracy, streamlining workflows, and improving patient management. AI-powered pathology, imaging analysis, and clinical decision support systems have demonstrated significant potential in optimizing medical processes and reducing the cognitive burden on healthcare professionals. However, the widespread adoption of AI in healthcare raises critical regulatory and ethical challenges, particularly regarding accountability in AI-assisted diagnosis and the risk of automation bias. The absence of a well-defined liability framework underscores the need for policies that ensure AI functions as an assistive tool rather than an autonomous decision-maker. With continued technological advancements, AI is expected to integrate multimodal data sources, such as genomics and radiomics, paving the way for precision medicine and personalized treatment strategies. The future of AI in healthcare depends on the development of transparent regulatory structures, industry collaboration, and adaptive governance frameworks that balance innovation with responsibility, ensuring equitable and effective AI-driven medical services.",
    "url": "https://arxiv.org/abs/2502.16732",
    "arxivId": "2502.16732",
    "last_visited": "2025-03-07T02:41:03.938Z",
    "last_read": "2025-03-07T02:41:03.938Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-02-23T22:09:17Z",
    "arxiv_tags": [
      "cs.CY",
      "cs.AI"
    ],
    "features_path": null
  },
  "arxiv.2503.03206": {
    "id": "arxiv.2503.03206",
    "title": "An Analytical Theory of Power Law Spectral Bias in the Learning Dynamics   of Diffusion Models",
    "authors": "Binxu Wang",
    "abstract": "We developed an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one- or two-layer linear denoiser settings with arbitrary data. Remarkably, these solutions allowed us to derive the generated distribution in closed form and its KL divergence through training. These analytical results expose a pronounced power-law spectral bias, i.e., for weights and distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and image datasets demonstrate that the power-law spectral bias remains robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations for why earlier stopping could lead to incorrect details in image generative models.",
    "url": "https://arxiv.org/abs/2503.03206",
    "arxivId": "2503.03206",
    "last_visited": "2025-03-07T02:39:26.172Z",
    "last_read": "2025-03-07T02:39:26.172Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-05T05:50:38Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CV",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T07, 60G15",
      "F.2.2; G.1.2; G.3; I.2.6"
    ],
    "features_path": null
  },
  "arxiv.2402.07754": {
    "id": "arxiv.2402.07754",
    "title": "Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language   Models",
    "authors": "Jiacheng Ye, Shansan Gong, Liheng Chen and 8 others",
    "abstract": "Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought (DoT), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.",
    "url": "https://arxiv.org/abs/2402.07754",
    "arxivId": "2402.07754",
    "last_visited": "2025-03-07T02:47:45.411Z",
    "last_read": "2025-03-07T02:47:45.411Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-12T16:23:28Z",
    "arxiv_tags": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2304.04661": {
    "id": "arxiv.2304.04661",
    "title": "AI for IT Operations (AIOps) on Cloud Platforms: Reviews, Opportunities   and Challenges",
    "authors": "Qian Cheng, Doyen Sahoo, Amrita Saha and 6 others",
    "abstract": "Artificial Intelligence for IT operations (AIOps) aims to combine the power of AI with the big data generated by IT Operations processes, particularly in cloud infrastructures, to provide actionable insights with the primary goal of maximizing availability. There are a wide variety of problems to address, and multiple use-cases, where AI capabilities can be leveraged to enhance operational efficiency. Here we provide a review of the AIOps vision, trends challenges and opportunities, specifically focusing on the underlying AI techniques. We discuss in depth the key types of data emitted by IT Operations activities, the scale and challenges in analyzing them, and where they can be helpful. We categorize the key AIOps tasks as - incident detection, failure prediction, root cause analysis and automated actions. We discuss the problem formulation for each task, and then present a taxonomy of techniques to solve these problems. We also identify relatively under explored topics, especially those that could significantly benefit from advances in AI literature. We also provide insights into the trends in this field, and what are the key investment opportunities.",
    "url": "https://arxiv.org/pdf/2304.04661",
    "arxivId": "2304.04661",
    "last_visited": "2025-03-07T02:45:34.214Z",
    "last_read": "2025-03-07T02:45:34.214Z",
    "total_reading_time_seconds": 0,
    "published_date": "2023-04-10T15:38:12Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.DC",
      "cs.SE"
    ],
    "features_path": null
  },
  "arxiv.2503.00691": {
    "id": "arxiv.2503.00691",
    "title": "How Diversely Can Language Models Solve Problems? Exploring the   Algorithmic Diversity of Model-Generated Code",
    "authors": "Seonghyeon Lee, Heejae Chon, Joonwon Jang and 2 others",
    "abstract": "Language models (LMs) have exhibited impressive abilities in generating code from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities. There is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in code LMs. Therefore, we propose a systematic approach to evaluate code diversity, introducing various metrics with inter-code similarity. Specifically, we introduce code clustering methods that leverages LMs' capabilities in code understanding and reasoning, resulting in a set of metrics that represent the number of algorithms in model-generated solutions. We extensively investigate the property of model-generated solutions by contrasting them with human-written ones and quantifying the impact of various factors on code diversity: model size, temperature, instruction tuning, and problem complexity. Our analysis demonstrates that model-generated solutions exhibit low algorithmic diversity, which was neglected by the research community. Moreover, we explore methods to increase code diversity by combining solutions from different models and increasing sampling temperatures. Our findings highlight that code diversity can be enhanced with the help of heterogeneous models and setting temperature beyond 1.0 that has not been fully explored due to the functional correctness degradation. To facilitate our research direction, we publicly share our code and datasets through open-source repositories.",
    "url": "https://arxiv.org/html/2503.00691v1",
    "arxivId": "2503.00691",
    "last_visited": "2025-03-07T06:06:20.292Z",
    "last_read": "2025-03-07T06:06:20.292Z",
    "total_reading_time_seconds": 17,
    "published_date": "2025-03-02T02:04:58Z",
    "arxiv_tags": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2412.08194": {
    "id": "arxiv.2412.08194",
    "title": "Magneto: Combining Small and Large Language Models for Schema Matching",
    "authors": "Yurong Liu, Eduardo Pena, Aecio Santos and 2 others",
    "abstract": "Recent advances in language models opened new opportunities to address complex schema matching tasks. Schema matching approaches have been proposed that demonstrate the usefulness of language models, but they have also uncovered important limitations: Small language models (SLMs) require training data (which can be both expensive and challenging to obtain), and large language models (LLMs) often incur high computational costs and must deal with constraints imposed by context windows. We present Magneto, a cost-effective and accurate solution for schema matching that combines the advantages of SLMs and LLMs to address their limitations. By structuring the schema matching pipeline in two phases, retrieval and reranking, Magneto can use computationally efficient SLM-based strategies to derive candidate matches which can then be reranked by LLMs, thus making it possible to reduce runtime without compromising matching accuracy. We propose a self-supervised approach to fine-tune SLMs which uses LLMs to generate syntactically diverse training data, and prompting strategies that are effective for reranking. We also introduce a new benchmark, developed in collaboration with domain experts, which includes real biomedical datasets and presents new challenges to schema matching methods. Through a detailed experimental evaluation, using both our new and existing benchmarks, we show that Magneto is scalable and attains high accuracy for datasets from different domains.",
    "url": "https://arxiv.org/abs/2412.08194",
    "arxivId": "2412.08194",
    "last_visited": "2025-03-07T07:26:53.882Z",
    "last_read": "2025-03-07T07:26:53.882Z",
    "total_reading_time_seconds": 28,
    "published_date": "2024-12-11T08:35:56Z",
    "arxiv_tags": [
      "cs.DB",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2204.05149": {
    "id": "arxiv.2204.05149",
    "title": "The Carbon Footprint of Machine Learning Training Will Plateau, Then   Shrink",
    "authors": "David Patterson, Joseph Gonzalez, Urs Hölzle and 7 others",
    "abstract": "Machine Learning (ML) workloads have rapidly grown in importance, but raised concerns about their carbon footprint. Four best practices can reduce ML training energy by up to 100x and CO2 emissions up to 1000x. By following best practices, overall ML energy use (across research, development, and production) held steady at &lt;15% of Google's total energy use for the past three years. If the whole ML field were to adopt best practices, total carbon emissions from training would reduce. Hence, we recommend that ML papers include emissions explicitly to foster competition on more than just model quality. Estimates of emissions in papers that omitted them have been off 100x-100,000x, so publishing emissions has the added benefit of ensuring accurate accounting. Given the importance of climate change, we must get the numbers right to make certain that we work on its biggest challenges.",
    "url": "https://arxiv.org/pdf/2204.05149",
    "arxivId": "2204.05149",
    "last_visited": "2025-03-07T07:22:40.809Z",
    "last_read": "2025-03-07T07:22:40.809Z",
    "total_reading_time_seconds": 0,
    "published_date": "2022-04-11T14:30:27Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.GL"
    ],
    "features_path": null
  },
  "arxiv.2309.13101": {
    "id": "arxiv.2309.13101",
    "title": "Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene   Reconstruction",
    "authors": "Ziyi Yang, Xinyu Gao, Wen Zhou and 3 others",
    "abstract": "Implicit neural representation has paved the way for new approaches to dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic neural rendering methods rely heavily on these implicit representations, which frequently struggle to capture the intricate details of objects in the scene. Furthermore, implicit methods have difficulty achieving real-time rendering in general dynamic scenes, limiting their use in a variety of tasks. To address the issues, we propose a deformable 3D Gaussians Splatting method that reconstructs scenes using 3D Gaussians and learns them in canonical space with a deformation field to model monocular dynamic scenes. We also introduce an annealing smoothing training mechanism with no extra overhead, which can mitigate the impact of inaccurate poses on the smoothness of time interpolation tasks in real-world datasets. Through a differential Gaussian rasterizer, the deformable 3D Gaussians not only achieve higher rendering quality but also real-time rendering speed. Experiments show that our method outperforms existing methods significantly in terms of both rendering quality and speed, making it well-suited for tasks such as novel-view synthesis, time interpolation, and real-time rendering.",
    "url": "https://arxiv.org/abs/2309.13101",
    "arxivId": "2309.13101",
    "last_visited": "2025-03-07T08:32:49.683Z",
    "last_read": "2025-03-07T08:32:49.683Z",
    "total_reading_time_seconds": 10,
    "published_date": "2023-09-22T16:04:02Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "arxiv.2503.04079": {
    "id": "arxiv.2503.04079",
    "title": "Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene   Rendering",
    "authors": "Idris O. Sunmola, Zhenjun Zhao, Samuel Schmidgall and 3 others",
    "abstract": "Accurate geometric reconstruction of deformable tissues in monocular endoscopic video remains a fundamental challenge in robot-assisted minimally invasive surgery. Although recent volumetric and point primitive methods based on neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently rendered surgical scenes, they still struggle with handling artifact-free tool occlusions and preserving fine anatomical details. These limitations stem from unrestricted Gaussian scaling and insufficient surface alignment constraints during reconstruction. To address these issues, we introduce Surgical Gaussian Surfels (SGS), which transforms anisotropic point primitives into surface-aligned elliptical splats by constraining the scale component of the Gaussian covariance matrix along the view-aligned axis. We predict accurate surfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled with locality constraints to handle complex tissue deformations. We use homodirectional view-space positional gradients to capture fine image details by splitting Gaussian Surfels in over-reconstructed regions. In addition, we define surface normals as the direction of the steepest density change within each Gaussian surfel primitive, enabling accurate normal estimation without requiring monocular normal priors. We evaluate our method on two in-vivo surgical datasets, where it outperforms current state-of-the-art methods in surface geometry, normal map quality, and rendering efficiency, while remaining competitive in real-time rendering performance. We make our code available at https://github.com/aloma85/SurgicalGaussianSurfels",
    "url": "https://arxiv.org/abs/2503.04079",
    "arxivId": "2503.04079",
    "last_visited": "2025-03-07T08:30:19.935Z",
    "last_read": "2025-03-07T08:30:19.935Z",
    "total_reading_time_seconds": 27,
    "published_date": "2025-03-06T04:33:19Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "arxiv.2404.17774": {
    "id": "arxiv.2404.17774",
    "title": "High-quality Surface Reconstruction using Gaussian Surfels",
    "authors": "Pinxuan Dai, Jiamin Xu, Wenxiang Xie and 3 others",
    "abstract": "We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.",
    "url": "https://arxiv.org/abs/2404.17774",
    "arxivId": "2404.17774",
    "last_visited": "2025-03-07T08:34:31.369Z",
    "last_read": "2025-03-07T08:34:31.369Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-04-27T04:13:39Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ],
    "features_path": null
  },
  "arxiv.2310.08528": {
    "id": "arxiv.2310.08528",
    "title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering",
    "authors": "Guanjun Wu, Taoran Yi, Jiemin Fang and 6 others",
    "abstract": "Representing and rendering dynamic scenes has been an important but challenging task. Especially, to accurately model complex motions, high efficiency is usually hard to guarantee. To achieve real-time dynamic scene rendering while also enjoying high training and storage efficiency, we propose 4D Gaussian Splatting (4D-GS) as a holistic representation for dynamic scenes rather than applying 3D-GS for each individual frame. In 4D-GS, a novel explicit representation containing both 3D Gaussians and 4D neural voxels is proposed. A decomposed neural voxel encoding algorithm inspired by HexPlane is proposed to efficiently build Gaussian features from 4D neural voxels and then a lightweight MLP is applied to predict Gaussian deformations at novel timestamps. Our 4D-GS method achieves real-time rendering under high resolutions, 82 FPS at an 800$\\times$800 resolution on an RTX 3090 GPU while maintaining comparable or better quality than previous state-of-the-art methods. More demos and code are available at https://guanjunwu.github.io/4dgs/.",
    "url": "https://arxiv.org/abs/2310.08528",
    "arxivId": "2310.08528",
    "last_visited": "2025-03-07T08:33:57.559Z",
    "last_read": "2025-03-07T08:33:57.559Z",
    "total_reading_time_seconds": 3,
    "published_date": "2023-10-12T17:21:41Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.GR"
    ],
    "features_path": null
  },
  "arxiv.2502.20581": {
    "id": "arxiv.2502.20581",
    "title": "The Noisy Path from Source to Citation: Measuring How Scholars Engage   with Past Research",
    "authors": "Hong Chen, Misha Teplitskiy, David Jurgens",
    "abstract": "Academic citations are widely used for evaluating research and tracing knowledge flows. Such uses typically rely on raw citation counts and neglect variability in citation types. In particular, citations can vary in their fidelity as original knowledge from cited studies may be paraphrased, summarized, or reinterpreted, possibly wrongly, leading to variation in how much information changes from cited to citing paper. In this study, we introduce a computational pipeline to quantify citation fidelity at scale. Using full texts of papers, the pipeline identifies citations in citing papers and the corresponding claims in cited papers, and applies supervised models to measure fidelity at the sentence level. Analyzing a large-scale multi-disciplinary dataset of approximately 13 million citation sentence pairs, we find that citation fidelity is higher when authors cite papers that are 1) more recent and intellectually close, 2) more accessible, and 3) the first author has a lower H-index and the author team is medium-sized. Using a quasi-experiment, we establish the \"telephone effect\" - when citing papers have low fidelity to the original claim, future papers that cite the citing paper and the original have lower fidelity to the original. Our work reveals systematic differences in citation fidelity, underscoring the limitations of analyses that rely on citation quantity alone and the potential for distortion of evidence.",
    "url": "https://arxiv.org/abs/2502.20581",
    "arxivId": "2502.20581",
    "last_visited": "2025-03-07T09:01:22.960Z",
    "last_read": "2025-03-07T09:01:22.960Z",
    "total_reading_time_seconds": 15,
    "published_date": "2025-02-27T22:47:03Z",
    "arxiv_tags": [
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2011.13456": {
    "id": "arxiv.2011.13456",
    "title": "Score-Based Generative Modeling through Stochastic Differential   Equations",
    "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma and 3 others",
    "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.",
    "url": "https://arxiv.org/abs/2011.13456",
    "arxivId": "2011.13456",
    "last_visited": "2025-03-08T02:12:29.792Z",
    "last_read": "2025-03-08T02:12:29.792Z",
    "total_reading_time_seconds": 20,
    "published_date": "2020-11-26T19:39:10Z",
    "arxiv_tags": [
      "cs.LG",
      "stat.ML"
    ],
    "features_path": null
  },
  "openreview.y7rbX884LZrX": {
    "id": "openreview.y7rbX884LZrX",
    "title": "OPENREVIEW Paper: y7rbX884LZrX",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=y7rbX884LZrX&referrer=%5Bthe%20profile%20of%20Lichuan%20Xiang%5D(%2Fprofile%3Fid%3D~Lichuan_Xiang1)",
    "arxivId": "",
    "last_visited": "2025-03-08T02:25:36.110Z",
    "last_read": "2025-03-08T02:25:36.110Z",
    "total_reading_time_seconds": 51,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.A9FUg5vrw7Y6": {
    "id": "openreview.A9FUg5vrw7Y6",
    "title": "OPENREVIEW Paper: A9FUg5vrw7Y6",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=A9FUg5vrw7Y6",
    "arxivId": "",
    "last_visited": "2025-03-08T03:38:59.724Z",
    "last_read": "2025-03-08T03:38:59.724Z",
    "total_reading_time_seconds": 20,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.aWXnKanInf": {
    "id": "openreview.aWXnKanInf",
    "title": "OPENREVIEW Paper: aWXnKanInf",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/pdf?id=aWXnKanInf",
    "arxivId": "",
    "last_visited": "2025-03-08T04:31:09.337Z",
    "last_read": "2025-03-08T04:31:09.337Z",
    "total_reading_time_seconds": 25,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.eHehzSDUFp": {
    "id": "openreview.eHehzSDUFp",
    "title": "OPENREVIEW Paper: eHehzSDUFp",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=eHehzSDUFp",
    "arxivId": "",
    "last_visited": "2025-03-08T04:52:26.764Z",
    "last_read": "2025-03-08T04:52:26.764Z",
    "total_reading_time_seconds": 321,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.is4nCVkSFA": {
    "id": "openreview.is4nCVkSFA",
    "title": "OPENREVIEW Paper: is4nCVkSFA",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/pdf?id=is4nCVkSFA",
    "arxivId": "",
    "last_visited": "2025-03-08T04:59:50.821Z",
    "last_read": "2025-03-08T04:59:50.821Z",
    "total_reading_time_seconds": 366,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.tyEyYT267x": {
    "id": "openreview.tyEyYT267x",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "authors": "Marianne Arriola",
    "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://mariannearriola.github.io/bd3-lms",
    "url": "https://openreview.net/forum?id=tyEyYT267x",
    "arxivId": "",
    "last_visited": "2025-03-08T05:08:04.332Z",
    "last_read": "2025-03-08T05:08:04.332Z",
    "total_reading_time_seconds": 23,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.WJaUkwci9o": {
    "id": "openreview.WJaUkwci9o",
    "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
    "authors": "Audrey Huang",
    "abstract": "Recent work in language modeling has raised the possibility of “self-improvement,” where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as “sharpening.” Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to ‘sharpen’ the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.",
    "url": "https://openreview.net/forum?id=WJaUkwci9o",
    "arxivId": "",
    "last_visited": "2025-03-08T06:12:26.065Z",
    "last_read": "2025-03-08T06:12:26.065Z",
    "total_reading_time_seconds": 33,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2211.09619": {
    "id": "arxiv.2211.09619",
    "title": "Introduction to Online Control",
    "authors": "Elad Hazan, Karan Singh",
    "abstract": "This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.   The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.   This objective suggests the use of the decision making framework of online convex optimization as an algorithmic methodology. The resulting methods are based on iterative mathematical optimization algorithms, and are accompanied by finite-time regret and computational complexity guarantees.",
    "url": "https://arxiv.org/abs/2211.09619",
    "arxivId": "2211.09619",
    "last_visited": "2025-03-08T07:17:20.703Z",
    "last_read": "2025-03-08T07:17:20.703Z",
    "total_reading_time_seconds": 13,
    "published_date": "2022-11-17T16:12:45Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC",
      "stat.ML"
    ],
    "features_path": null
  },
  "arxiv.2502.13130": {
    "id": "arxiv.2502.13130",
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "authors": "Jianwei Yang, Reuben Tan, Qianhui Wu and 10 others",
    "abstract": "We present Magma, a foundation model that serves multimodal AI agentic tasks in both the digital and physical worlds. Magma is a significant extension of vision-language (VL) models in that it not only retains the VL understanding ability (verbal intelligence) of the latter, but is also equipped with the ability to plan and act in the visual-spatial world (spatial-temporal intelligence) and complete agentic tasks ranging from UI navigation to robot manipulation. To endow the agentic capabilities, Magma is pretrained on large amounts of heterogeneous datasets spanning from images, videos to robotics data, where the actionable visual objects (e.g., clickable buttons in GUI) in images are labeled by Set-of-Mark (SoM) for action grounding, and the object movements (e.g., the trace of human hands or robotic arms) in videos are labeled by Trace-of-Mark (ToM) for action planning. Extensive experiments show that SoM and ToM reach great synergy and facilitate the acquisition of spatial-temporal intelligence for our Magma model, which is fundamental to a wide range of tasks as shown in Fig.1. In particular, Magma creates new state-of-the-art results on UI navigation and robotic manipulation tasks, outperforming previous models that are specifically tailored to these tasks. On image and video-related multimodal tasks, Magma also compares favorably to popular large multimodal models that are trained on much larger datasets. We make our model and code public for reproducibility at https://microsoft.github.io/Magma.",
    "url": "https://arxiv.org/pdf/2502.13130v1",
    "arxivId": "2502.13130",
    "last_visited": "2025-03-08T21:19:49.124Z",
    "last_read": "2025-03-08T21:19:49.124Z",
    "total_reading_time_seconds": 20,
    "published_date": "2025-02-18T18:55:21Z",
    "arxiv_tags": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "features_path": null
  },
  "openreview.0vtftmYQGV": {
    "id": "openreview.0vtftmYQGV",
    "title": "OpenReview Paper: 0vtftmYQGV",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=0vtftmYQGV",
    "arxivId": "",
    "last_visited": "2025-03-09T05:00:11.819Z",
    "last_read": "2025-03-09T05:00:11.819Z",
    "total_reading_time_seconds": 25,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.rwqShzb9li": {
    "id": "openreview.rwqShzb9li",
    "title": "OpenReview Paper: rwqShzb9li",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=rwqShzb9li",
    "arxivId": "",
    "last_visited": "2025-03-09T03:52:19.930Z",
    "last_read": "2025-03-09T03:52:19.930Z",
    "total_reading_time_seconds": 73,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.9DrPvYCETp": {
    "id": "openreview.9DrPvYCETp",
    "title": "OpenReview Paper: 9DrPvYCETp",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=9DrPvYCETp",
    "arxivId": "",
    "last_visited": "2025-03-09T05:14:25.032Z",
    "last_read": "2025-03-09T05:14:25.032Z",
    "total_reading_time_seconds": 44,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.u1b1dJtyxc": {
    "id": "openreview.u1b1dJtyxc",
    "title": "OpenReview Paper: u1b1dJtyxc",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=u1b1dJtyxc",
    "arxivId": "",
    "last_visited": "2025-03-09T05:56:27.928Z",
    "last_read": "2025-03-09T05:56:27.928Z",
    "total_reading_time_seconds": 34,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.aVh9KRZdRk": {
    "id": "openreview.aVh9KRZdRk",
    "title": "OpenReview Paper: aVh9KRZdRk",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=aVh9KRZdRk",
    "arxivId": "",
    "last_visited": "2025-03-09T05:55:38.723Z",
    "last_read": "2025-03-09T05:55:38.723Z",
    "total_reading_time_seconds": 90,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.owR9ofvkFQ": {
    "id": "openreview.owR9ofvkFQ",
    "title": "OpenReview Paper: owR9ofvkFQ",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=owR9ofvkFQ",
    "arxivId": "",
    "last_visited": "2025-03-09T05:20:12.358Z",
    "last_read": "2025-03-09T05:20:12.358Z",
    "total_reading_time_seconds": 33,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.qK6U4Ahfms": {
    "id": "openreview.qK6U4Ahfms",
    "title": "OpenReview Paper: qK6U4Ahfms",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=qK6U4Ahfms",
    "arxivId": "",
    "last_visited": "2025-03-09T05:19:27.154Z",
    "last_read": "2025-03-09T05:19:27.154Z",
    "total_reading_time_seconds": 40,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.REIK4SZMJt": {
    "id": "openreview.REIK4SZMJt",
    "title": "OpenReview Paper: REIK4SZMJt",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=REIK4SZMJt",
    "arxivId": "",
    "last_visited": "2025-03-09T06:18:31.502Z",
    "last_read": "2025-03-09T06:18:31.502Z",
    "total_reading_time_seconds": 37,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.gojL67CfS8": {
    "id": "openreview.gojL67CfS8",
    "title": "OpenReview Paper: gojL67CfS8",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=gojL67CfS8",
    "arxivId": "",
    "last_visited": "2025-03-09T06:30:39.994Z",
    "last_read": "2025-03-09T06:30:39.994Z",
    "total_reading_time_seconds": 37,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.uNKlTQ8mBD": {
    "id": "openreview.uNKlTQ8mBD",
    "title": "OpenReview Paper: uNKlTQ8mBD",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=uNKlTQ8mBD",
    "arxivId": "",
    "last_visited": "2025-03-09T06:36:46.954Z",
    "last_read": "2025-03-09T06:36:46.954Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.21098v1": {
    "id": "arxiv.2502.21098v1",
    "title": "2502.21098v1",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/html/2502.21098v1",
    "arxivId": "",
    "last_visited": "2025-03-09T21:22:09.261Z",
    "last_read": "2025-03-09T21:22:09.261Z",
    "total_reading_time_seconds": 13,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.Q5RYn6jagC": {
    "id": "openreview.Q5RYn6jagC",
    "title": "OpenReview Paper: Q5RYn6jagC",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=Q5RYn6jagC",
    "arxivId": "",
    "last_visited": "2025-03-09T21:26:52.639Z",
    "last_read": "2025-03-09T21:26:52.639Z",
    "total_reading_time_seconds": 13,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2407.16551": {
    "id": "arxiv.2407.16551",
    "title": "2407.16551",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2407.16551",
    "arxivId": "",
    "last_visited": "2025-03-09T21:25:01.469Z",
    "last_read": "2025-03-09T21:25:01.469Z",
    "total_reading_time_seconds": 21,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2408.04681": {
    "id": "arxiv.2408.04681",
    "title": "2408.04681",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2408.04681",
    "arxivId": "",
    "last_visited": "2025-03-09T22:40:08.698Z",
    "last_read": "2025-03-09T22:40:08.698Z",
    "total_reading_time_seconds": 9,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.HRnSVflpgt": {
    "id": "openreview.HRnSVflpgt",
    "title": "OpenReview Paper: HRnSVflpgt",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=HRnSVflpgt",
    "arxivId": "",
    "last_visited": "2025-03-09T22:43:53.280Z",
    "last_read": "2025-03-09T22:43:53.280Z",
    "total_reading_time_seconds": 27,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.05082": {
    "id": "arxiv.2502.05082",
    "title": "2502.05082",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.05082",
    "arxivId": "",
    "last_visited": "2025-03-10T03:57:40.626Z",
    "last_read": "2025-03-10T03:57:40.626Z",
    "total_reading_time_seconds": 19,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05003": {
    "id": "arxiv.2503.05003",
    "title": "2503.05003",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.05003",
    "arxivId": "",
    "last_visited": "2025-03-10T03:56:15.414Z",
    "last_read": "2025-03-10T03:56:15.414Z",
    "total_reading_time_seconds": 17,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.13065v1": {
    "id": "arxiv.2502.13065v1",
    "title": "2502.13065v1",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.13065v1",
    "arxivId": "",
    "last_visited": "2025-03-10T04:01:29.752Z",
    "last_read": "2025-03-10T04:01:29.752Z",
    "total_reading_time_seconds": 38,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05182": {
    "id": "arxiv.2503.05182",
    "title": "2503.05182",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.05182",
    "arxivId": "",
    "last_visited": "2025-03-10T06:04:31.311Z",
    "last_read": "2025-03-10T06:04:31.311Z",
    "total_reading_time_seconds": 8,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.sbmp55k6iE": {
    "id": "openreview.sbmp55k6iE",
    "title": "OpenReview Paper: sbmp55k6iE",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=sbmp55k6iE",
    "arxivId": "",
    "last_visited": "2025-03-10T04:45:32.274Z",
    "last_read": "2025-03-10T04:45:32.274Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05425": {
    "id": "arxiv.2503.05425",
    "title": "2503.05425",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.05425",
    "arxivId": "",
    "last_visited": "2025-03-10T06:07:27.457Z",
    "last_read": "2025-03-10T06:07:27.457Z",
    "total_reading_time_seconds": 14,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.04034": {
    "id": "arxiv.2503.04034",
    "title": "2503.04034",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.04034",
    "arxivId": "",
    "last_visited": "2025-03-10T07:55:21.521Z",
    "last_read": "2025-03-10T07:55:21.521Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.03945": {
    "id": "arxiv.2503.03945",
    "title": "2503.03945",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.03945",
    "arxivId": "",
    "last_visited": "2025-03-10T07:53:32.195Z",
    "last_read": "2025-03-10T07:53:32.195Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.15651": {
    "id": "arxiv.2502.15651",
    "title": "2502.15651",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.15651",
    "arxivId": "",
    "last_visited": "2025-03-10T07:52:04.587Z",
    "last_read": "2025-03-10T07:52:04.587Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "openreview.T86jIuSi5A": {
    "id": "openreview.T86jIuSi5A",
    "title": "OpenReview Paper: T86jIuSi5A",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/forum?id=T86jIuSi5A",
    "arxivId": "",
    "last_visited": "2025-03-10T08:17:27.548Z",
    "last_read": "2025-03-10T08:17:27.548Z",
    "total_reading_time_seconds": 60,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.04316": {
    "id": "arxiv.2503.04316",
    "title": "2503.04316",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.04316",
    "arxivId": "",
    "last_visited": "2025-03-10T08:17:10.098Z",
    "last_read": "2025-03-10T08:17:10.098Z",
    "total_reading_time_seconds": 53,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2404.11596": {
    "id": "arxiv.2404.11596",
    "title": "2404.11596",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2404.11596",
    "arxivId": "",
    "last_visited": "2025-03-10T08:16:43.779Z",
    "last_read": "2025-03-10T08:16:43.779Z",
    "total_reading_time_seconds": 6,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.04020": {
    "id": "arxiv.2503.04020",
    "title": "2503.04020",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.04020",
    "arxivId": "",
    "last_visited": "2025-03-10T08:20:54.744Z",
    "last_read": "2025-03-10T08:20:54.744Z",
    "total_reading_time_seconds": 66,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2402.06176": {
    "id": "arxiv.2402.06176",
    "title": "2402.06176",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2402.06176",
    "arxivId": "",
    "last_visited": "2025-03-10T08:37:12.760Z",
    "last_read": "2025-03-10T08:37:12.760Z",
    "total_reading_time_seconds": 136,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.13993": {
    "id": "arxiv.2502.13993",
    "title": "2502.13993",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.13993",
    "arxivId": "",
    "last_visited": "2025-03-10T08:36:57.873Z",
    "last_read": "2025-03-10T08:36:57.873Z",
    "total_reading_time_seconds": 20,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.08552": {
    "id": "arxiv.2502.08552",
    "title": "2502.08552",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.08552",
    "arxivId": "",
    "last_visited": "2025-03-10T08:42:27.299Z",
    "last_read": "2025-03-10T08:42:27.299Z",
    "total_reading_time_seconds": 76,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.06626": {
    "id": "arxiv.2502.06626",
    "title": "2502.06626",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.06626",
    "arxivId": "",
    "last_visited": "2025-03-10T08:41:48.829Z",
    "last_read": "2025-03-10T08:41:48.829Z",
    "total_reading_time_seconds": 6,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.14555": {
    "id": "arxiv.2502.14555",
    "title": "2502.14555",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.14555?context=nlin.AO",
    "arxivId": "",
    "last_visited": "2025-03-10T08:41:16.340Z",
    "last_read": "2025-03-10T08:41:16.340Z",
    "total_reading_time_seconds": 12,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.00450": {
    "id": "arxiv.2503.00450",
    "title": "2503.00450",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/pdf/2503.00450",
    "arxivId": "",
    "last_visited": "2025-03-10T14:54:57.554Z",
    "last_read": "2025-03-10T14:54:57.554Z",
    "total_reading_time_seconds": 156,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.05244": {
    "id": "arxiv.2502.05244",
    "title": "2502.05244",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.05244",
    "arxivId": "",
    "last_visited": "2025-03-10T15:02:03.217Z",
    "last_read": "2025-03-10T15:02:03.217Z",
    "total_reading_time_seconds": 56,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.17540": {
    "id": "arxiv.2502.17540",
    "title": "2502.17540",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.17540",
    "arxivId": "",
    "last_visited": "2025-03-10T15:00:15.525Z",
    "last_read": "2025-03-10T15:00:15.525Z",
    "total_reading_time_seconds": 18,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2412.04466": {
    "id": "arxiv.2412.04466",
    "title": "ARXIV Paper: 2412.04466",
    "authors": "Sophie Greenwood, Sudalakshmee Chiniah, Nikhil Garg",
    "abstract": "In the basic recommendation paradigm, the most (predicted) relevant item is recommended to each user. This may result in some items receiving lower exposure than they \"should\"; to counter this, several algorithmic approaches have been developed to ensure item fairness. These approaches necessarily degrade recommendations for some users to improve outcomes for items, leading to user fairness concerns. In turn, a recent line of work has focused on developing algorithms for multi-sided fairness, to jointly optimize user fairness, item fairness, and overall recommendation quality. This induces the question: what is the tradeoff between these objectives, and what are the characteristics of (multi-objective) optimal solutions? Theoretically, we develop a model of recommendations with user and item fairness objectives and characterize the solutions of fairness-constrained optimization. We identify two phenomena: (a) when user preferences are diverse, there is \"free\" item and user fairness; and (b) users whose preferences are misestimated can be especially disadvantaged by item fairness constraints. Empirically, we prototype a recommendation system for preprints on arXiv and implement our framework, measuring the phenomena in practice and showing how these phenomena inform the design of markets with recommendation systems-intermediated matching.",
    "url": "https://arxiv.org/abs/2412.04466",
    "arxivId": "",
    "last_visited": "2025-03-10T17:36:12.780Z",
    "last_read": "2025-03-10T17:36:12.780Z",
    "total_reading_time_seconds": 35,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.00089": {
    "id": "arxiv.2503.00089",
    "title": "ARXIV Paper: 2503.00089",
    "authors": "Xinyu Yuan, Zichen Wang, Marcus Collins, Huzefa Rangwala",
    "abstract": "Recent years have witnessed a surge in the development of protein structural tokenization methods, which chunk protein 3D structures into discrete or continuous representations. Structure tokenization enables the direct application of powerful techniques like language modeling for protein structures, and large multimodal models to integrate structures with protein sequences and functional texts. Despite the progress, the capabilities and limitations of these methods remain poorly understood due to the lack of a unified evaluation framework. We first introduce StructTokenBench, a framework that comprehensively evaluates the quality and efficiency of structure tokenizers, focusing on fine-grained local substructures rather than global structures, as typical in existing benchmarks. Our evaluations reveal that no single model dominates all benchmarking perspectives. Observations of codebook under-utilization led us to develop AminoAseed, a simple yet effective strategy that enhances codebook gradient updates and optimally balances codebook size and dimension for improved tokenizer utilization and quality. Compared to the leading model ESM3, our method achieves an average of 6.31% performance improvement across 24 supervised tasks, with sensitivity and utilization rates increased by 12.83% and 124.03%, respectively.",
    "url": "https://arxiv.org/abs/2503.00089",
    "arxivId": "",
    "last_visited": "2025-03-10T18:11:48.848Z",
    "last_read": "2025-03-10T18:11:48.848Z",
    "total_reading_time_seconds": 153,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2409.15644": {
    "id": "arxiv.2409.15644",
    "title": "ARXIV Paper: 2409.15644",
    "authors": "Tzu-Sheng Kuo, Quan Ze Chen, Amy X. Zhang and 3 others",
    "abstract": "Community and organizational policies are typically designed in a top-down, centralized fashion, with limited input from impacted stakeholders. This can result in policies that are misaligned with community needs or perceived as illegitimate. How can we support more collaborative, participatory approaches to policy design? In this paper, we present PolicyCraft, a system that structures collaborative policy design through case-grounded deliberation. Building on past research that highlights the value of concrete cases in establishing common ground, PolicyCraft supports users in collaboratively proposing, critiquing, and revising policies through discussion and voting on cases. A field study across two university courses showed that students using PolicyCraft reached greater consensus and developed better-supported course policies, compared with those using a baseline system that did not scaffold their use of concrete cases. Reflecting on our findings, we discuss opportunities for future HCI systems to help groups more effectively bridge between abstract policies and concrete cases.",
    "url": "https://arxiv.org/abs/2409.15644",
    "arxivId": "",
    "last_visited": "2025-03-10T18:09:06.574Z",
    "last_read": "2025-03-10T18:09:06.574Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.04957": {
    "id": "arxiv.2503.04957",
    "title": "2503.04957",
    "authors": "Ada Defne Tur, Nicholas Meade, Xing Han Lù and 6 others",
    "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io",
    "url": "https://arxiv.org/abs/2503.04957",
    "arxivId": "",
    "last_visited": "2025-03-10T19:09:51.260Z",
    "last_read": "2025-03-10T19:09:51.260Z",
    "total_reading_time_seconds": 23,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2406.08129": {
    "id": "arxiv.2406.08129",
    "title": "2406.08129",
    "authors": "K. Trachenko, B. Monserrat, M. Hutcheon, C. J. Pickard",
    "abstract": "Fundamental physical constants govern key effects in high-energy particle physics and astrophysics, including the stability of particles, nuclear reactions, formation and evolution of stars, synthesis of heavy nuclei and emergence of stable molecular structures. Here, we show that fundamental constants also set an upper bound for the frequency of phonons in condensed matter phases, or how rapidly an atom can vibrate. This bound is in agreement with \\textit{ab initio} simulations of atomic hydrogen and high-temperature hydride superconductors, and implies an upper limit to the superconducting transition temperature $T_c$ in condensed matter. Fundamental constants set this limit to the order of 10$^2-10^3$ K. This range is consistent with our calculations of $T_c$ from optimal Eliashberg functions. As a corollary, we observe that the very existence of the current research of finding $T_{\\mathrm{c}}$ at and above $300$ K is due to the observed values of fundamental constants. We finally discuss how fundamental constants affect the observability and operation of other effects and phenomena including phase transitions.",
    "url": "https://arxiv.org/abs/2406.08129",
    "arxivId": "",
    "last_visited": "2025-03-10T19:29:07.994Z",
    "last_read": "2025-03-10T19:29:07.994Z",
    "total_reading_time_seconds": 47,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05040": {
    "id": "arxiv.2503.05040",
    "title": "2503.05040",
    "authors": "John C. Flournoy, Carol S. Lee, Catherine M. Hicks, Maggie Wu",
    "abstract": "Understanding factors that influence software development velocity is crucial for engineering teams and organizations, yet empirical evidence at scale remains limited. A more robust understanding of the dynamics of cycle time may help practitioners avoid pitfalls in relying on velocity measures while evaluating software work. We analyze cycle time, a widely-used metric measuring time from ticket creation to completion, using a dataset of over 55,000 observations across 216 organizations. Through Bayesian hierarchical modeling that appropriately separates individual and organizational variation, we examine how coding time, task scoping, and collaboration patterns affect cycle time while characterizing its substantial variability across contexts. We find precise but modest associations between cycle time and factors including coding days per week, number of merged pull requests, and degree of collaboration. However, these effects are set against considerable unexplained variation both between and within individuals. Our findings suggest that while common workplace factors do influence cycle time in expected directions, any single observation provides limited signal about typical performance. This work demonstrates methods for analyzing complex operational metrics at scale while highlighting potential pitfalls in using such measurements to drive decision-making. We conclude that improving software delivery velocity likely requires systems-level thinking rather than individual-focused interventions.",
    "url": "https://arxiv.org/abs/2503.05040",
    "arxivId": "",
    "last_visited": "2025-03-10T20:26:19.753Z",
    "last_read": "2025-03-10T20:26:19.753Z",
    "total_reading_time_seconds": 33,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.1611.03530": {
    "id": "arxiv.1611.03530",
    "title": "1611.03530",
    "authors": "Chiyuan Zhang, Samy Bengio, Moritz Hardt and 2 others",
    "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.   Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice.   We interpret our experimental findings by comparison with traditional models.",
    "url": "https://arxiv.org/abs/1611.03530",
    "arxivId": "",
    "last_visited": "2025-03-10T21:42:06.808Z",
    "last_read": "2025-03-10T21:42:06.808Z",
    "total_reading_time_seconds": 3,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.02113": {
    "id": "arxiv.2503.02113",
    "title": "2503.02113",
    "authors": "Andrew Gordon Wilson",
    "abstract": "Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization. We argue that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.",
    "url": "https://arxiv.org/abs/2503.02113",
    "arxivId": "",
    "last_visited": "2025-03-10T21:42:05.532Z",
    "last_read": "2025-03-10T21:42:05.532Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05025": {
    "id": "arxiv.2503.05025",
    "title": "2503.05025",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.05025",
    "arxivId": "",
    "last_visited": "2025-03-10T21:52:52.531Z",
    "last_read": "2025-03-10T21:52:52.531Z",
    "total_reading_time_seconds": 41,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.20332": {
    "id": "arxiv.2502.20332",
    "title": "2502.20332",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2502.20332",
    "arxivId": "",
    "last_visited": "2025-03-10T22:31:56.409Z",
    "last_read": "2025-03-10T22:31:56.409Z",
    "total_reading_time_seconds": 38,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.06334": {
    "id": "arxiv.2503.06334",
    "title": "2503.06334",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.06334",
    "arxivId": "",
    "last_visited": "2025-03-11T05:39:44.236Z",
    "last_read": "2025-03-11T05:39:44.236Z",
    "total_reading_time_seconds": 8,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.07134": {
    "id": "arxiv.2503.07134",
    "title": "2503.07134",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.07134",
    "arxivId": "",
    "last_visited": "2025-03-11T05:13:29.789Z",
    "last_read": "2025-03-11T05:13:29.789Z",
    "total_reading_time_seconds": 27,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.06520": {
    "id": "arxiv.2503.06520",
    "title": "ARXIV Paper: 2503.06520",
    "authors": "Yuqi Liu, Bohao Peng, Zhisheng Zhong and 4 others",
    "abstract": "Traditional methods for reasoning segmentation rely on supervised fine-tuning with categorical labels and simple descriptions, limiting its out-of-domain generalization and lacking explicit reasoning processes. To address these limitations, we propose Seg-Zero, a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement. Seg-Zero introduces a decoupled architecture consisting of a reasoning model and a segmentation model. The reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precious pixel-level masks. We design a sophisticated reward mechanism that integrates both format and accuracy rewards to effectively guide optimization directions. Trained exclusively via reinforcement learning with GRPO and without explicit reasoning data, Seg-Zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. Experiments show that Seg-Zero-7B achieves a zero-shot performance of 57.5 on the ReasonSeg benchmark, surpassing the prior LISA-7B by 18\\%. This significant improvement highlights Seg-Zero's ability to generalize across domains while presenting an explicit reasoning process. Code is available at https://github.com/dvlab-research/Seg-Zero.",
    "url": "https://arxiv.org/abs/2503.06520",
    "arxivId": "",
    "last_visited": "2025-03-11T07:34:47.360Z",
    "last_read": "2025-03-11T07:34:47.360Z",
    "total_reading_time_seconds": 8,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.06698": {
    "id": "arxiv.2503.06698",
    "title": "ARXIV Paper: 2503.06698",
    "authors": "Xavier Thomas, Deepti Ghadiyaram",
    "abstract": "Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.",
    "url": "https://arxiv.org/abs/2503.06698",
    "arxivId": "",
    "last_visited": "2025-03-11T07:50:36.406Z",
    "last_read": "2025-03-11T07:50:36.406Z",
    "total_reading_time_seconds": 65,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2410.13061": {
    "id": "arxiv.2410.13061",
    "title": "ARXIV Paper: 2410.13061",
    "authors": "Adrian Ciotinga, YooJung Choi",
    "abstract": "We introduce a novel optimal transport framework for probabilistic circuits (PCs). While it has been shown recently that divergences between distributions represented as certain classes of PCs can be computed tractably, to the best of our knowledge, there is no existing approach to compute the Wasserstein distance between probability distributions given by PCs. We propose a Wasserstein-type distance that restricts the coupling measure of the associated optimal transport problem to be a probabilistic circuit. We then develop an algorithm for computing this distance by solving a series of small linear programs and derive the circuit conditions under which this is tractable. Furthermore, we show that we can easily retrieve the optimal transport plan between the PCs from the solutions to these linear programs. Lastly, we study the empirical Wasserstein distance between a PC and a dataset, and show that we can estimate the PC parameters to minimize this distance through an efficient iterative algorithm.",
    "url": "https://arxiv.org/abs/2410.13061",
    "arxivId": "",
    "last_visited": "2025-03-11T09:13:08.269Z",
    "last_read": "2025-03-11T09:13:08.269Z",
    "total_reading_time_seconds": 26,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.06462": {
    "id": "arxiv.2503.06462",
    "title": "ARXIV Paper: 2503.06462",
    "authors": "Zexu Huang, Min Xu, Stuart Perry",
    "abstract": "Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts.",
    "url": "https://arxiv.org/abs/2503.06462",
    "arxivId": "",
    "last_visited": "2025-03-11T09:10:56.193Z",
    "last_read": "2025-03-11T09:10:56.193Z",
    "total_reading_time_seconds": 11,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2402.14740": {
    "id": "arxiv.2402.14740",
    "title": "ARXIV Paper: 2402.14740",
    "authors": "Arash Ahmadian, Chris Cremer, Matthias Gallé and 5 others",
    "abstract": "AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the formulation of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost.",
    "url": "https://arxiv.org/abs/2402.14740",
    "arxivId": "",
    "last_visited": "2025-03-11T18:29:43.097Z",
    "last_read": "2025-03-11T18:29:43.097Z",
    "total_reading_time_seconds": 24,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2405.06705": {
    "id": "arxiv.2405.06705",
    "title": "ARXIV Paper: 2405.06705",
    "authors": "Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng and 2 others",
    "abstract": "Self-correction is emerging as a promising approach to mitigate the issue of hallucination in Large Language Models (LLMs). To facilitate effective self-correction, recent research has proposed mistake detection as its initial step. However, current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies. To address this challenge, we introduce a unique prompting strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes. PedCoT consists of pedagogical principles for prompts (PPP) design, two-stage interaction process (TIP) and grounded PedCoT prompts, all inspired by the educational theory of the Bloom Cognitive Model (BCM). We evaluate our approach on two public datasets featuring math problems of varying difficulty levels. The experiments demonstrate that our zero-shot prompting strategy significantly outperforms strong baselines. The proposed method can achieve the goal of reliable mathematical mistake identification and provide a foundation for automatic math answer grading. The results underscore the significance of educational theory, serving as domain knowledge, in guiding prompting strategy design for addressing challenging tasks with LLMs effectively.",
    "url": "https://arxiv.org/abs/2405.06705",
    "arxivId": "",
    "last_visited": "2025-03-11T18:28:31.417Z",
    "last_read": "2025-03-11T18:28:31.417Z",
    "total_reading_time_seconds": 8,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2002.06043": {
    "id": "arxiv.2002.06043",
    "title": "ARXIV Paper: 2002.06043",
    "authors": "Wouter Kool, Herke van Hoof, Max Welling",
    "abstract": "We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators in both high and low entropy settings.",
    "url": "https://arxiv.org/abs/2002.06043",
    "arxivId": "",
    "last_visited": "2025-03-11T18:58:44.945Z",
    "last_read": "2025-03-11T18:58:44.945Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.07154": {
    "id": "arxiv.2503.07154",
    "title": "ARXIV Paper: 2503.07154",
    "authors": "Jiaming Song, Linqi Zhou",
    "abstract": "Recent years have seen significant advancements in foundation models through generative pre-training, yet algorithmic innovation in this space has largely stagnated around autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation creates a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence. We argue that an inference-first perspective, which prioritizes scaling efficiency during inference time across sequence length and refinement steps, can inspire novel generative pre-training algorithms. Using Inductive Moment Matching (IMM) as a concrete example, we demonstrate how addressing limitations in diffusion models' inference process through targeted modifications yields a stable, single-stage algorithm that achieves superior sample quality with over an order of magnitude greater inference efficiency.",
    "url": "https://arxiv.org/abs/2503.07154",
    "arxivId": "",
    "last_visited": "2025-03-11T19:51:32.394Z",
    "last_read": "2025-03-11T19:51:32.394Z",
    "total_reading_time_seconds": 28,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.07565": {
    "id": "arxiv.2503.07565",
    "title": "ARXIV Paper: 2503.07565",
    "authors": "Linqi Zhou, Stefano Ermon, Jiaming Song",
    "abstract": "Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch.",
    "url": "https://arxiv.org/abs/2503.07565",
    "arxivId": "",
    "last_visited": "2025-03-11T19:51:01.062Z",
    "last_read": "2025-03-11T19:51:01.062Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.08634": {
    "id": "arxiv.2503.08634",
    "title": "ARXIV Paper: 2503.08634",
    "authors": "Mohammadjavad Ebrahimi, Yuyang Qiu, Shisheng Cui, Farzad Yousefian",
    "abstract": "We study a bilevel federated learning (FL) problem, where clients cooperatively seek to find among multiple optimal solutions of a primary distributed learning problem, a solution that minimizes a secondary distributed global loss function. This problem is motivated by model selection in over-parameterized machine learning, in that the outer-level objective is a suitably-defined regularizer and the inner-level objective is the training loss function. Despite recent progress in centralized settings, communication-efficient FL methods equipped with complexity guarantees for resolving this problem class are primarily absent. Motivated by this lacuna, we consider the setting where the inner-level objective is convex and the outer-level objective is either convex or strongly convex. We propose a universal regularized scheme and derive promising error bounds in terms of both the inner-level and outer-level loss functions. Leveraging this unifying theory, we then enable two existing FL methods to address the corresponding simple bilevel problem and derive novel communication complexity guarantees for each method. Additionally, we devise an FL method for addressing simple bilevel optimization problems with a nonconvex outer-level loss function. Through a two-loop scheme and by leveraging the universal theory, we derive new complexity bounds for the nonconvex setting. This appears to be the first time that federated simple bilevel optimization problems are provably addressed with guarantees. We validate the theoretical findings on EMNIST and CIFAR-10 datasets.",
    "url": "https://arxiv.org/abs/2503.08634",
    "arxivId": "",
    "last_visited": "2025-03-12T04:27:36.424Z",
    "last_read": "2025-03-12T04:27:36.424Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.08578": {
    "id": "arxiv.2503.08578",
    "title": "ARXIV Paper: 2503.08578",
    "authors": "Hui Huang, Hicham Kouhkouh, Lukang Sun",
    "abstract": "We analyze the Consensus-Based Optimization (CBO) algorithm with a consensus point rescaled by a small fixed parameter $\\kappa \\in (0,1)$. Under minimal assumptions on the objective function and the initial data, we establish its unconditional convergence to the global minimizer. Our results hold in the asymptotic regime where both the time--horizon $t \\to \\infty$ and the inverse--temperature $\\alpha \\to \\infty$, providing a rigorous theoretical foundation for the algorithm's global convergence. Furthermore, our findings extend to the case of multiple and non--discrete set of minimizers.",
    "url": "https://arxiv.org/abs/2503.08578",
    "arxivId": "",
    "last_visited": "2025-03-12T04:27:21.294Z",
    "last_read": "2025-03-12T04:27:21.294Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.08494": {
    "id": "arxiv.2503.08494",
    "title": "ARXIV Paper: 2503.08494",
    "authors": "Wenqing Zhao, Antai Xie, Yuchi Wu and 2 others",
    "abstract": "This paper studies the distributed generalized Nash equilibrium seeking problem for aggregative games with coupling constraints, where each player optimizes its strategy depending on its local cost function and the estimated strategy aggregation. The information transmission in distributed networks may go beyond bandwidth capacity and eventuate communication bottlenecks. Therefore, we propose a novel communication-efficient distributed generalized Nash equilibrium seeking algorithm, in which the communication efficiency is improved by event-triggered communication and information compression methods. The proposed algorithm saves the transmitted rounds and bits of communication simultaneously. Specifically, by developing precise step size conditions, the proposed algorithm ensures provable convergence, and is proven to achieve $(0,\\delta)$-differential privacy with a stochastic quantization scheme. In the end, simulation results verify the effectiveness of the proposed algorithm.",
    "url": "https://arxiv.org/abs/2503.08494",
    "arxivId": "",
    "last_visited": "2025-03-12T04:27:13.499Z",
    "last_read": "2025-03-12T04:27:13.499Z",
    "total_reading_time_seconds": 8,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.01017": {
    "id": "arxiv.2503.01017",
    "title": "ARXIV Paper: 2503.01017",
    "authors": "Yuhang Zhang, Zhiyao Zhang, Junyi Ji and 7 others",
    "abstract": "This article presents the first field deployment of a multi-agent reinforcement learning (MARL) based variable speed limit (VSL) control system on Interstate 24 (I-24) near Nashville, Tennessee. We design and demonstrate a full pipeline from training MARL agents in a traffic simulator to a field deployment on a 17-mile segment of I-24 encompassing 67 VSL controllers. The system was launched on March 8th, 2024, and has made approximately 35 million decisions on 28 million trips in six months of operation. We apply an invalid action masking mechanism and several safety guards to ensure real-world constraints. The MARL-based implementation operates up to 98% of the time, with the safety guards overriding the MARL decisions for the remaining time. We evaluate the performance of the MARL-based algorithm in comparison to a previously deployed non-RL VSL benchmark algorithm on I-24. Results show that the MARL-based VSL control system achieves a superior performance. The accuracy of correctly warning drivers about slowing traffic ahead is improved by 14% and the response delay to non-recurrent congestion is reduced by 75%. The preliminary data shows that the VSL control system has reduced the crash rate by 26% and the secondary crash rate by 50%. We open-sourced the deployed MARL-based VSL algorithm at https://github.com/Lab-Work/marl-vsl-controller.",
    "url": "https://arxiv.org/abs/2503.01017",
    "arxivId": "",
    "last_visited": "2025-03-12T08:41:04.168Z",
    "last_read": "2025-03-12T08:41:04.168Z",
    "total_reading_time_seconds": 24,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2405.16002": {
    "id": "arxiv.2405.16002",
    "title": "ARXIV Paper: 2405.16002",
    "authors": "Minhak Song, Kwangjun Ahn, Chulhee Yun",
    "abstract": "Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training.",
    "url": "https://arxiv.org/abs/2405.16002",
    "arxivId": "",
    "last_visited": "2025-03-12T08:51:25.306Z",
    "last_read": "2025-03-12T08:51:25.306Z",
    "total_reading_time_seconds": 6,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2405.16397": {
    "id": "arxiv.2405.16397",
    "title": "ARXIV Paper: 2405.16397",
    "authors": "Damien Martins Gomes, Yanlei Zhang, Eugene Belilovsky and 2 others",
    "abstract": "First-order optimization methods are currently the mainstream in training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by employing the diagonal matrix preconditioning of the stochastic gradient during the training. Despite their widespread, second-order optimization algorithms exhibit superior convergence properties compared to their first-order counterparts e.g. Adam and SGD. However, their practicality in training DNNs is still limited due to increased per-iteration computations compared to the first-order methods. We present \\emph{AdaFisher}--an adaptive second-order optimizer that leverages a \\emph{diagonal block-Kronecker} approximation of the Fisher information matrix for adaptive gradient preconditioning. AdaFisher aims to bridge the gap between enhanced \\emph{convergence/generalization} capabilities and computational efficiency in second-order optimization framework for training DNNs. Despite the slow pace of second-order optimizers, we showcase that AdaFisher can be reliably adopted for image classification, language modeling and stands out for its stability and robustness in hyper-parameter tuning. We demonstrate that AdaFisher \\textbf{outperforms the SOTA optimizers} in terms of both accuracy and convergence speed. Code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.",
    "url": "https://arxiv.org/abs/2405.16397",
    "arxivId": "",
    "last_visited": "2025-03-12T08:50:54.052Z",
    "last_read": "2025-03-12T08:50:54.052Z",
    "total_reading_time_seconds": 4,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2408.02572": {
    "id": "arxiv.2408.02572",
    "title": "ARXIV Paper: 2408.02572",
    "authors": "Mateus Araújo, Andrew J. P. Garner, Miguel Navascues",
    "abstract": "Non-commutative polynomial optimization (NPO) problems seek to minimize the state average of a polynomial of some operator variables, subject to polynomial constraints, over all states and operators, as well as the Hilbert spaces where those might be defined. Many of these problems are known to admit a complete hierarchy of semidefinite programming (SDP) relaxations. In this work, we consider a variant of NPO problems where a subset of the operator variables satisfies a system of ordinary differential equations. We find that, under mild conditions of operator boundedness, for every such problem one can construct a standard NPO problem with the same solution. This allows us to define a complete hierarchy of SDPs to tackle the original differential problem. We apply this method to bound averages of local observables in quantum spin systems subject to a Hamiltonian evolution (i.e., a quench). We find that, even in the thermodynamic limit of infinitely many sites, low levels of the hierarchy provide very good approximations for reasonably long evolution times.",
    "url": "https://arxiv.org/abs/2408.02572",
    "arxivId": "",
    "last_visited": "2025-03-12T08:50:21.042Z",
    "last_read": "2025-03-12T08:50:21.042Z",
    "total_reading_time_seconds": 9,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2408.16543": {
    "id": "arxiv.2408.16543",
    "title": "ARXIV Paper: 2408.16543",
    "authors": "Clémentine Chazal, Anna Korba, Francis Bach",
    "abstract": "In this paper, we study the statistical and geometrical properties of the Kullback-Leibler divergence with kernel covariance operators (KKL) introduced by Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that involves density ratios, the KKL compares probability distributions through covariance operators (embeddings) in a reproducible kernel Hilbert space (RKHS), and compute the Kullback-Leibler quantum divergence. This novel divergence hence shares parallel but different aspects with both the standard Kullback-Leibler between probability distributions and kernel embeddings metrics such as the maximum mean discrepancy. A limitation faced with the original KKL divergence is its inability to be defined for distributions with disjoint supports. To solve this problem, we propose in this paper a regularised variant that guarantees that the divergence is well defined for all distributions. We derive bounds that quantify the deviation of the regularised KKL to the original one, as well as finite-sample bounds. In addition, we provide a closed-form expression for the regularised KKL, specifically applicable when the distributions consist of finite sets of points, which makes it implementable. Furthermore, we derive a Wasserstein gradient descent scheme of the KKL divergence in the case of discrete distributions, and study empirically its properties to transport a set of points to a target distribution.",
    "url": "https://arxiv.org/abs/2408.16543",
    "arxivId": "",
    "last_visited": "2025-03-12T08:48:52.657Z",
    "last_read": "2025-03-12T08:48:52.657Z",
    "total_reading_time_seconds": 10,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.07891": {
    "id": "arxiv.2503.07891",
    "title": "2503.07891",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/abs/2503.07891",
    "arxivId": "",
    "last_visited": "2025-03-12T21:47:32.583Z",
    "last_read": "2025-03-12T21:47:32.583Z",
    "total_reading_time_seconds": 21,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2311.00059": {
    "id": "arxiv.2311.00059",
    "title": "The Generative AI Paradox: \"What It Can Create, It May Not Understand\"",
    "authors": "Peter West, Ximing Lu, Nouha Dziri and 11 others",
    "abstract": "The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabilities in measures of understanding, as well as weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models' generative capability may not be contingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.",
    "url": "https://arxiv.org/abs/2311.00059",
    "arxivId": "2311.00059",
    "last_visited": "2025-03-12T21:52:45.657Z",
    "last_read": "2025-03-12T21:52:45.657Z",
    "total_reading_time_seconds": 81,
    "published_date": "2023-10-31T18:07:07Z",
    "arxiv_tags": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2503.08537": {
    "id": "arxiv.2503.08537",
    "title": "Chemical reasoning in LLMs unlocks steerable synthesis planning and   reaction mechanism elucidation",
    "authors": "Andres M Bran, Theo A Neukomm, Daniel P Armstrong and 2 others",
    "abstract": "While machine learning algorithms have been shown to excel at specific chemical tasks, they have struggled to capture the strategic thinking that characterizes expert chemical reasoning, limiting their widespread adoption. Here we demonstrate that large language models (LLMs) can serve as powerful chemical reasoning engines when integrated with traditional search algorithms, enabling a new approach to computer-aided chemistry that mirrors human expert thinking. Rather than using LLMs to directly manipulate chemical structures, we leverage their ability to evaluate chemical strategies and guide search algorithms toward chemically meaningful solutions. We demonstrate this paradigm through two fundamental challenges: strategy-aware retrosynthetic planning and mechanism elucidation. In retrosynthetic planning, our method allows chemists to specify desired synthetic strategies in natural language to find routes that satisfy these constraints in vast searches. In mechanism elucidation, LLMs guide the search for plausible reaction mechanisms by combining chemical principles with systematic exploration. Our approach shows strong performance across diverse chemical tasks, with larger models demonstrating increasingly sophisticated chemical reasoning. Our approach establishes a new paradigm for computer-aided chemistry that combines the strategic understanding of LLMs with the precision of traditional chemical tools, opening possibilities for more intuitive and powerful chemical reasoning systems.",
    "url": "https://arxiv.org/abs/2503.08537",
    "arxivId": "2503.08537",
    "last_visited": "2025-03-13T02:11:17.866Z",
    "last_read": "2025-03-13T02:11:17.866Z",
    "total_reading_time_seconds": 28,
    "published_date": "2025-03-11T15:27:17Z",
    "arxiv_tags": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "features_path": null
  },
  "arxiv.2503.09576": {
    "id": "arxiv.2503.09576",
    "title": "Manify: A Python Library for Learning Non-Euclidean Representations",
    "authors": "Philippe Chlenski, Kaizhu Du, Dylan Satow, Itsik Pe'er",
    "abstract": "We present Manify, an open-source Python library for non-Euclidean representation learning. Leveraging manifold learning techniques, Manify provides tools for learning embeddings in (products of) non-Euclidean spaces, performing classification and regression with data that lives in such spaces, and estimating the curvature of a manifold. Manify aims to advance research and applications in machine learning by offering a comprehensive suite of tools for manifold-based data analysis. Our source code, examples, datasets, results, and documentation are available at https://github.com/pchlenski/manify",
    "url": "https://arxiv.org/abs/2503.09576",
    "arxivId": "2503.09576",
    "last_visited": "2025-03-13T07:06:32.811Z",
    "last_read": "2025-03-13T07:06:32.811Z",
    "total_reading_time_seconds": 27,
    "published_date": "2025-03-12T17:44:40Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "arxiv.2503.09541": {
    "id": "arxiv.2503.09541",
    "title": "Neural Network-Based Change Point Detection for Large-Scale   Time-Evolving Data",
    "authors": "Jialiang Geng, George Michailidis",
    "abstract": "The paper studies the problem of detecting and locating change points in multivariate time-evolving data. The problem has a long history in statistics and signal processing and various algorithms have been developed primarily for simple parametric models. In this work, we focus on modeling the data through feed-forward neural networks and develop a detection strategy based on the following two-step procedure. In the first step, the neural network is trained over a prespecified window of the data, and its test error function is calibrated over another prespecified window. Then, the test error function is used over a moving window to identify the change point. Once a change point is detected, the procedure involving these two steps is repeated until all change points are identified. The proposed strategy yields consistent estimates for both the number and the locations of the change points under temporal dependence of the data-generating process. The effectiveness of the proposed strategy is illustrated on synthetic data sets that provide insights on how to select in practice tuning parameters of the algorithm and in real data sets. Finally, we note that although the detection strategy is general and can work with different neural network architectures, the theoretical guarantees provided are specific to feed-forward neural architectures.",
    "url": "https://arxiv.org/abs/2503.09541",
    "arxivId": "2503.09541",
    "last_visited": "2025-03-13T07:05:16.806Z",
    "last_read": "2025-03-13T07:05:16.806Z",
    "total_reading_time_seconds": 16,
    "published_date": "2025-03-12T16:58:52Z",
    "arxiv_tags": [
      "stat.ML",
      "cs.LG",
      "stat.AP",
      "stat.CO",
      "stat.ME"
    ],
    "features_path": null
  },
  "arxiv.2503.08944": {
    "id": "arxiv.2503.08944",
    "title": "Comment on \"Interferometric single-shot parity measurement in InAs-Al   hybrid devices\", Microsoft Quantum, Nature 638, 651-655 (2025)",
    "authors": "Henry F. Legg",
    "abstract": "We consider the 'parity readout' of a (topological) superconductor claimed in Nature 638, 651-655 (2025). A prerequisite for this claim is the existence of a superconducting gap in the nanowire device. However, to determine the presence of a gap, Nature 638, 651-655 (2025) relied on the so-called topological gap protocol (TGP). Here, we show that the TGP can report the regions where the 'parity readout' occurred as either gapped or gapless, depending on data parameters such as magnetic field range and cutter pair (junction transparency). Compounding these issues are inaccuracies in the presented TGP outcomes, which limited investigation of reproducibility. Since these inconsistent outcomes demonstrate that the TGP is not a reliable diagnostic tool for the presence of a superconducting gap, we instead investigate the conductance data for the studied regions -- data that were not presented in Nature 638, 651-655 (2025), but are in the public data repository. These conductance data show that the regions where 'parity readout' occurred are in fact highly disordered and present no clear gap in the nanowire, i.e., the underlying conductance data show that these regions are indeed gapless. That these regions are gapless contradicts the claim that the reported measurements are of the parity of a superconducting nanowire, let alone the parity of a topological superconducting nanowire. Taken together, these issues mean that the core findings in Nature 638, 651-655 (2025) are not reliable and should be revisited.",
    "url": "https://arxiv.org/pdf/2503.08944",
    "arxivId": "2503.08944",
    "last_visited": "2025-03-13T07:02:31.456Z",
    "last_read": "2025-03-13T07:02:31.456Z",
    "total_reading_time_seconds": 0,
    "published_date": "2025-03-11T22:47:26Z",
    "arxiv_tags": [
      "cond-mat.mes-hall",
      "cond-mat.supr-con",
      "quant-ph"
    ],
    "features_path": null
  },
  "arxiv.2402.11057": {
    "id": "arxiv.2402.11057",
    "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View   Synthesis",
    "authors": "Youngkyoon Jang, Eduardo Pérez-Pellitero",
    "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision with a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty levels; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with various sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF.",
    "url": "https://arxiv.org/abs/2402.11057",
    "arxivId": "2402.11057",
    "last_visited": "2025-03-13T07:02:28.727Z",
    "last_read": "2025-03-13T07:02:28.727Z",
    "total_reading_time_seconds": 0,
    "published_date": "2024-02-16T20:12:33Z",
    "arxiv_tags": [
      "cs.CV"
    ],
    "features_path": null
  },
  "arxiv.2404.02258": {
    "id": "arxiv.2404.02258",
    "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based   language models",
    "authors": "David Raposo, Sam Ritter, Blake Richards and 3 others",
    "abstract": "Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but dynamic and context-sensitive at the token-level. Not only do models trained in this way learn to dynamically allocate compute, they do so efficiently. These models match baseline performance for equivalent FLOPS and wall-clock times to train, but require a fraction of the FLOPs per forward pass, and can be upwards of 50\\% faster to step during post-training sampling.",
    "url": "https://arxiv.org/abs/2404.02258",
    "arxivId": "2404.02258",
    "last_visited": "2025-03-14T00:03:49.439Z",
    "last_read": "2025-03-14T00:03:49.439Z",
    "total_reading_time_seconds": 3,
    "published_date": "2024-04-02T19:28:11Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.CL"
    ],
    "features_path": null
  },
  "arxiv.2310.00431": {
    "id": "arxiv.2310.00431",
    "title": "ResolvNet: A Graph Convolutional Network with multi-scale Consistency",
    "authors": "Christian Koke, Abhishek Saroha, Yuesong Shen and 2 others",
    "abstract": "It is by now a well known fact in the graph learning community that the presence of bottlenecks severely limits the ability of graph neural networks to propagate information over long distances. What so far has not been appreciated is that, counter-intuitively, also the presence of strongly connected sub-graphs may severely restrict information flow in common architectures. Motivated by this observation, we introduce the concept of multi-scale consistency. At the node level this concept refers to the retention of a connected propagation graph even if connectivity varies over a given graph. At the graph-level, multi-scale consistency refers to the fact that distinct graphs describing the same object at different resolutions should be assigned similar feature vectors. As we show, both properties are not satisfied by poular graph neural network architectures. To remedy these shortcomings, we introduce ResolvNet, a flexible graph neural network based on the mathematical concept of resolvents. We rigorously establish its multi-scale consistency theoretically and verify it in extensive experiments on real world data: Here networks based on this ResolvNet architecture prove expressive; out-performing baselines significantly on many tasks; in- and outside the multi-scale setting.",
    "url": "https://arxiv.org/abs/2310.00431",
    "arxivId": "2310.00431",
    "last_visited": "2025-03-14T04:07:25.791Z",
    "last_read": "2025-03-14T04:07:25.791Z",
    "total_reading_time_seconds": 61,
    "published_date": "2023-09-30T16:46:45Z",
    "arxiv_tags": [
      "cs.LG"
    ],
    "features_path": null
  },
  "openreview.SRCsyJafgP": {
    "id": "openreview.SRCsyJafgP",
    "title": "On the successful Incorporation of Scale into Graph Neural Networks",
    "authors": "Christian Koke",
    "abstract": "Standard graph neural networks assign vastly different latent embeddings to graphs describing the same physical system at different resolution scales. This precludes consistency in applications and prevents generalization between scales as would fundamentally be needed in many scientific applications. We uncover the underlying obstruction, investigate its origin and show how to overcome it.",
    "url": "https://openreview.net/forum?id=SRCsyJafgP",
    "arxivId": "",
    "last_visited": "2025-03-14T03:27:22.626Z",
    "last_read": "2025-03-14T03:27:22.626Z",
    "total_reading_time_seconds": 23,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "2309.14054": {
    "id": "2309.14054",
    "title": "Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning   in Generative Adversarial Networks",
    "authors": "Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P",
    "abstract": "Owing to the growing concerns about privacy and regulatory compliance, it is desirable to regulate the output of generative models. To that end, the objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained Generative Adversarial Network (GAN) where the underlying training data set is inaccessible. Our approach is inspired by the observation that the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed two-stage method, known as 'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also maintaining the quality of generated samples. In the initial stage, we adapt a pre-trained GAN on a set of negative samples (containing undesired features) provided by the user. Subsequently, we train the original pre-trained GAN using positive samples, along with a repulsion regularizer. This regularizer encourages the learned model parameters to move away from the parameters of the adapted model (first stage) while not degrading the generation quality. We provide theoretical insights into the proposed method. To the best of our knowledge, our approach stands as the first method addressing unlearning within the realm of high-fidelity GANs (such as StyleGAN). We validate the effectiveness of our method through comprehensive experiments, encompassing both class-level unlearning on the MNIST and AFHQ dataset and feature-level unlearning tasks on the CelebA-HQ dataset. Our code and implementation is available at: https://github.com/atriguha/Adapt_Unlearn.",
    "url": "https://arxiv.org/abs/2309.14054",
    "arxivId": "2309.14054",
    "last_visited": "2025-03-15T05:00:52.805Z",
    "last_read": "2025-03-15T05:00:52.805Z",
    "total_reading_time_seconds": 255,
    "published_date": "2023-09-25T11:36:20Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2309.14054/features/markdown-grobid/2309.14054.md",
      "adr-crib": "data/papers/2309.14054/features/adr-crib/2309.14054.md",
      "adr-titles": "data/papers/2309.14054/features/adr-titles/2309.14054.md",
      "crib-sheet": "data/papers/2309.14054/features/crib-sheet/2309.14054.md",
      "compound-crib": "data/papers/2309.14054/features/compound-crib/2309.14054.md"
    }
  },
  "2503.09917": {
    "id": "2503.09917",
    "title": "Introducing MareNostrum5: A European pre-exascale energy-efficient   system designed to serve a broad spectrum of scientific workloads",
    "authors": "Fabio Banchelli, Marta Garcia-Gasulla, Filippo Mantovani and 7 others",
    "abstract": "MareNostrum5 is a pre-exascale supercomputer at the Barcelona Supercomputing Center (BSC), part of the EuroHPC Joint Undertaking. With a peak performance of 314 petaflops, MareNostrum5 features a hybrid architecture comprising Intel Sapphire Rapids CPUs, NVIDIA Hopper GPUs, and DDR5 and high-bandwidth memory (HBM), organized into four partitions optimized for diverse workloads. This document evaluates MareNostrum5 through micro-benchmarks (floating-point performance, memory bandwidth, interconnect throughput), HPC benchmarks (HPL and HPCG), and application studies using Alya, OpenFOAM, and IFS. It highlights MareNostrum5's scalability, efficiency, and energy performance, utilizing the EAR (Energy Aware Runtime) framework to assess power consumption and the effects of direct liquid cooling. Additionally, HBM and DDR5 configurations are compared to examine memory performance trade-offs. Designed to complement standard technical documentation, this study provides insights to guide both new and experienced users in optimizing their workloads and maximizing MareNostrum5's computational capabilities.",
    "url": "https://arxiv.org/abs/2503.09917",
    "arxivId": "2503.09917",
    "last_visited": "2025-03-15T05:00:28.298Z",
    "last_read": "2025-03-15T05:00:28.298Z",
    "total_reading_time_seconds": 5,
    "published_date": "2025-03-13T00:18:43Z",
    "arxiv_tags": [
      "cs.DC",
      "cs.PF"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2503.09917/features/markdown-grobid/2503.09917.md",
      "adr-crib": "data/papers/2503.09917/features/adr-crib/2503.09917.md",
      "adr-titles": "data/papers/2503.09917/features/adr-titles/2503.09917.md",
      "crib-sheet": "data/papers/2503.09917/features/crib-sheet/2503.09917.md",
      "compound-crib": "data/papers/2503.09917/features/compound-crib/2503.09917.md"
    }
  },
  "2411.18864": {
    "id": "2411.18864",
    "title": "Redesigning the ensemble Kalman filter with a dedicated model of   epistemic uncertainty",
    "authors": "Chatchuea Kimchaiwong, Jeremie Houssineau, Adam M. Johansen",
    "abstract": "The problem of incorporating information from observations received serially in time is widespread in the field of uncertainty quantification. Within a probabilistic framework, such problems can be addressed using standard filtering techniques. However, in many real-world problems, some (or all) of the uncertainty is epistemic, arising from a lack of knowledge, and is difficult to model probabilistically. This paper introduces a possibilistic ensemble Kalman filter designed for this setting and characterizes some of its properties. Using possibility theory to describe epistemic uncertainty is appealing from a philosophical perspective, and it is easy to justify certain heuristics often employed in standard ensemble Kalman filters as principled approaches to capturing uncertainty within it. The possibilistic approach motivates a robust mechanism for characterizing uncertainty which shows good performance with small sample sizes, and can outperform standard ensemble Kalman filters at given sample size, even when dealing with genuinely aleatoric uncertainty.",
    "url": "https://arxiv.org/abs/2411.18864",
    "arxivId": "2411.18864",
    "last_visited": "2025-03-15T05:12:36.197Z",
    "last_read": "2025-03-15T05:12:36.197Z",
    "total_reading_time_seconds": 131,
    "published_date": "2024-11-28T02:11:23Z",
    "arxiv_tags": [
      "stat.ME",
      "cs.AI",
      "62F15, 65C35"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2411.18864/features/markdown-grobid/2411.18864.md",
      "adr-crib": "data/papers/2411.18864/features/adr-crib/2411.18864.md",
      "adr-titles": "data/papers/2411.18864/features/adr-titles/2411.18864.md",
      "crib-sheet": "data/papers/2411.18864/features/crib-sheet/2411.18864.md",
      "compound-crib": "data/papers/2411.18864/features/compound-crib/2411.18864.md"
    }
  },
  "2307.02846": {
    "id": "2307.02846",
    "title": "Probability Metrics for Tropical Spaces of Different Dimensions",
    "authors": "Roan Talbut, Daniele Tramontano, Yueqi Cao and 2 others",
    "abstract": "The problem of comparing probability distributions is at the heart of many tasks in statistics and machine learning. Established comparison methods treat the standard setting that the distributions are supported in the same space. Recently, a new geometric solution has been proposed to address the more challenging problem of comparing measures in Euclidean spaces of differing dimensions. Here, we study the same problem of comparing probability distributions of different dimensions in the tropical setting, which is becoming increasingly relevant in applications involving complex data structures such as phylogenetic trees. Specifically, we construct a Wasserstein distance between measures on different tropical projective tori -- the focal metric spaces in both theory and applications of tropical geometry -- via tropical mappings between probability measures. We prove equivalence of the directionality of the maps, whether mapping from a low dimensional space to a high dimensional space or vice versa. As an important practical implication, our work provides a framework for comparing probability distributions on the spaces of phylogenetic trees with different leaf sets. We demonstrate the computational feasibility of our approach using existing optimisation techniques on both simulated and real data.",
    "url": "https://arxiv.org/abs/2307.02846",
    "arxivId": "2307.02846",
    "last_visited": "2025-03-15T05:21:25.025Z",
    "last_read": "2025-03-15T05:21:25.025Z",
    "total_reading_time_seconds": 10,
    "published_date": "2023-07-06T08:22:55Z",
    "arxiv_tags": [
      "math.MG",
      "math.CO",
      "math.ST",
      "q-bio.PE",
      "stat.TH"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2307.02846/features/markdown-grobid/2307.02846.md",
      "adr-crib": "data/papers/2307.02846/features/adr-crib/2307.02846.md",
      "adr-titles": "data/papers/2307.02846/features/adr-titles/2307.02846.md",
      "crib-sheet": "data/papers/2307.02846/features/crib-sheet/2307.02846.md",
      "compound-crib": "data/papers/2307.02846/features/compound-crib/2307.02846.md"
    }
  },
  "2502.20642": {
    "id": "2502.20642",
    "title": "Fixed point theorem in metric spaces and its application to the Collatz   conjecture",
    "authors": "Toshiharu Kawasaki",
    "abstract": "In this paper, we show the new fixed point theorem in metric spaces. Furthermore, for this fixed point theorem, we apply to the Collatz conjecture.",
    "url": "https://arxiv.org/abs/2502.20642",
    "arxivId": "2502.20642",
    "last_visited": "2025-03-15T05:26:48.021Z",
    "last_read": "2025-03-15T05:26:48.021Z",
    "total_reading_time_seconds": 8,
    "published_date": "2025-02-28T01:48:00Z",
    "arxiv_tags": [
      "math.GM",
      "math.FA",
      "math.MG",
      "47H10"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2502.20642/features/markdown-grobid/2502.20642.md"
    }
  },
  "1805.12400": {
    "id": "1805.12400",
    "title": "Tropical Geometry of Phylogenetic Tree Space: A Statistical Perspective",
    "authors": "Anthea Monod, Bo Lin, Ruriko Yoshida, Qiwen Kang",
    "abstract": "Phylogenetic trees are the fundamental mathematical representation of evolutionary processes in biology. They are also objects of interest in pure mathematics, such as algebraic geometry and combinatorics, due to their discrete geometry. Although they are important data structures, they face the significant challenge that sets of trees form a non-Euclidean phylogenetic tree space, which means that standard computational and statistical methods cannot be directly applied. In this work, we explore the statistical feasibility of a pure mathematical representation of the set of all phylogenetic trees based on tropical geometry for both descriptive and inferential statistics, and unsupervised and supervised machine learning. Our exploration is both theoretical and practical. We show that the tropical geometric phylogenetic tree space endowed with a generalized Hilbert projective metric exhibits analytic, geometric, and topological properties that are desirable for theoretical studies in probability and statistics and allow for well-defined questions to be posed. We illustrate the statistical feasibility of the tropical geometric perspective for phylogenetic trees with an example of both a descriptive and inferential statistical task. Moreover, this approach exhibits increased computational efficiency and statistical performance over the current state-of-the-art, which we illustrate with a real data example on seasonal influenza. Our results demonstrate the viability of the tropical geometric setting for parametric statistical and probabilistic studies of sets of phylogenetic trees.",
    "url": "https://arxiv.org/abs/1805.12400v2",
    "arxivId": "1805.12400",
    "last_visited": "2025-03-15T05:24:42.505Z",
    "last_read": "2025-03-15T05:24:42.505Z",
    "total_reading_time_seconds": 24,
    "published_date": "2018-05-31T09:52:30Z",
    "arxiv_tags": [
      "math.MG",
      "math.CO",
      "math.ST",
      "q-bio.PE",
      "stat.TH"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/1805.12400/features/markdown-grobid/1805.12400.md",
      "adr-crib": "data/papers/1805.12400/features/adr-crib/1805.12400.md",
      "adr-titles": "data/papers/1805.12400/features/adr-titles/1805.12400.md",
      "crib-sheet": "data/papers/1805.12400/features/crib-sheet/1805.12400.md",
      "compound-crib": "data/papers/1805.12400/features/compound-crib/1805.12400.md"
    }
  },
  "2406.18464": {
    "id": "2406.18464",
    "title": "Bayesian inverse Navier-Stokes problems: joint flow field reconstruction   and parameter learning",
    "authors": "Alexandros Kontogiannis, Scott V. Elgersma, Andrew J. Sederman, Matthew P. Juniper",
    "abstract": "We formulate and solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates velocimetry data in order to jointly reconstruct a 3D flow field and learn the unknown N-S parameters, including the boundary position. By hardwiring a generalised N-S problem, and regularising its unknown parameters using Gaussian prior distributions, we learn the most likely parameters in a collapsed search space. The most likely flow field reconstruction is then the N-S solution that corresponds to the learned parameters. We develop the method in the variational setting and use a stabilised Nitsche weak form of the N-S problem that permits the control of all N-S parameters. To regularise the inferred the geometry, we use a viscous signed distance field (vSDF) as an auxiliary variable, which is given as the solution of a viscous Eikonal boundary value problem. We devise an algorithm that solves this inverse problem, and numerically implement it using an adjoint-consistent stabilised cut-cell finite element method. We then use this method to reconstruct magnetic resonance velocimetry (flow-MRI) data of a 3D steady laminar flow through a physical model of an aortic arch for two different Reynolds numbers and signal-to-noise ratio (SNR) levels (low/high). We find that the method can accurately i) reconstruct the low SNR data by filtering out the noise/artefacts and recovering flow features that are obscured by noise, and ii) reproduce the high SNR data without overfitting. Although the framework that we develop applies to 3D steady laminar flows in complex geometries, it readily extends to time-dependent laminar and Reynolds-averaged turbulent flows, as well as non-Newtonian (e.g. viscoelastic) fluids.",
    "url": "https://arxiv.org/abs/2406.18464",
    "arxivId": "2406.18464",
    "last_visited": "2025-03-15T05:31:56.786Z",
    "last_read": "2025-03-15T05:31:56.786Z",
    "total_reading_time_seconds": 164,
    "published_date": "2024-06-26T16:16:36Z",
    "arxiv_tags": [
      "physics.flu-dyn",
      "cs.LG",
      "math.OC"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2406.18464/features/markdown-grobid/2406.18464.md",
      "adr-crib": "data/papers/2406.18464/features/adr-crib/2406.18464.md",
      "adr-titles": "data/papers/2406.18464/features/adr-titles/2406.18464.md",
      "crib-sheet": "data/papers/2406.18464/features/crib-sheet/2406.18464.md"
    }
  },
  "2110.14020": {
    "id": "2110.14020",
    "title": "The Difficulty of Passive Learning in Deep Reinforcement Learning",
    "authors": "Georg Ostrovski, Pablo Samuel Castro, Will Dabney",
    "abstract": "Learning to act from observational data without active environmental interaction is a well-known challenge in Reinforcement Learning (RL). Recent approaches involve constraints on the learned policy or conservative updates, preventing strong deviations from the state-action distribution of the dataset. Although these methods are evaluated using non-linear function approximation, theoretical justifications are mostly limited to the tabular or linear cases. Given the impressive results of deep reinforcement learning, we argue for a need to more clearly understand the challenges in this setting.   In the vein of Held &amp; Hein's classic 1963 experiment, we propose the \"tandem learning\" experimental paradigm which facilitates our empirical analysis of the difficulties in offline reinforcement learning. We identify function approximation in conjunction with fixed data distributions as the strongest factors, thereby extending but also challenging hypotheses stated in past work. Our results provide relevant insights for offline deep reinforcement learning, while also shedding new light on phenomena observed in the online case of learning control.",
    "url": "https://arxiv.org/abs/2110.14020",
    "arxivId": "2110.14020",
    "last_visited": "2025-03-15T05:36:53.011Z",
    "last_read": "2025-03-15T05:36:53.011Z",
    "total_reading_time_seconds": 38,
    "published_date": "2021-10-26T20:50:49Z",
    "arxiv_tags": [
      "cs.LG",
      "cs.AI"
    ],
    "features_path": {
      "markdown-grobid": "data/papers/2110.14020/features/markdown-grobid/2110.14020.md",
      "adr-crib": "data/papers/2110.14020/features/adr-crib/2110.14020.md",
      "adr-titles": "data/papers/2110.14020/features/adr-titles/2110.14020.md",
      "crib-sheet": "data/papers/2110.14020/features/crib-sheet/2110.14020.md",
      "compound-crib": "data/papers/2110.14020/features/compound-crib/2110.14020.md"
    }
  },
  "arxiv:2306.01462": {
    "id": "arxiv:2306.01462",
    "title": "Random eigenvalues of graphenes and the triangulation of plane",
    "authors": "Artur Bille, Victor Buchstaber, Simon Coste and 2 others",
    "abstract": "We analyse the numbers of closed paths of length $k\\in\\mathbb{N}$ on two important regular lattices: the hexagonal lattice (also called $\\textit{graphene}$ in chemistry) and its dual triangular lattice. These numbers form a moment sequence of specific random variables connected to the distance of a position of a planar random flight (in three steps) from the origin. Here, we refer to such a random variable as a $\\textit{random eigenvalue}$ of the underlying lattice. Explicit formulas for the probability density and characteristic functions of these random eigenvalues are given for both the hexagonal and the triangular lattice. Furthermore, it is proven that both probability distributions can be approximated by a functional of the random variable uniformly distributed on increasing intervals $[0,b]$ as $b\\to\\infty$. This yields a straightforward method to simulate these random eigenvalues without generating graphene and triangular lattice graphs. To demonstrate this approximation, we first prove a key integral identity for a specific series containing the third powers of the modified Bessel functions $I_n$ of $n$th order, $n\\in\\mathbb{Z}$. Such series play a crucial role in various contexts, in particular, in analysis, combinatorics, and theoretical physics.",
    "url": "https://arxiv.org/abs/2306.01462",
    "arxivId": "",
    "last_visited": "2025-03-15T06:07:40.146Z",
    "last_read": "2025-03-15T06:07:40.146Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2306.01462/features/markdown-grobid/arxiv:2306.01462.md",
      "adr-crib": "data/papers/arxiv:2306.01462/features/adr-crib/arxiv:2306.01462.md",
      "adr-titles": "data/papers/arxiv:2306.01462/features/adr-titles/arxiv:2306.01462.md",
      "crib-sheet": "data/papers/arxiv:2306.01462/features/crib-sheet/arxiv:2306.01462.md"
    }
  },
  "arxiv:2501.09891": {
    "id": "arxiv:2501.09891",
    "title": "Evolving Deeper LLM Thinking",
    "authors": "Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu and 4 others",
    "abstract": "We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver.",
    "url": "https://arxiv.org/abs/2501.09891",
    "arxivId": "",
    "last_visited": "2025-03-15T06:22:41.913Z",
    "last_read": "2025-03-15T06:22:41.913Z",
    "total_reading_time_seconds": 30,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2501.09891/features/markdown-grobid/arxiv:2501.09891.md",
      "adr-crib": "data/papers/arxiv:2501.09891/features/adr-crib/arxiv:2501.09891.md",
      "adr-titles": "data/papers/arxiv:2501.09891/features/adr-titles/arxiv:2501.09891.md",
      "crib-sheet": "data/papers/arxiv:2501.09891/features/crib-sheet/arxiv:2501.09891.md",
      "compound-crib": "data/papers/arxiv:2501.09891/features/compound-crib/arxiv:2501.09891.md"
    }
  },
  "arxiv:2502.17827": {
    "id": "arxiv:2502.17827",
    "title": "DPGLM: A Semiparametric Bayesian GLM with Inhomogeneous Normalized Random Measures",
    "authors": "Entejar Alam, Paul J. Rathouz, Peter Müller",
    "abstract": "We introduce a varying weight dependent Dirichlet process (DDP) model to implement a semi-parametric GLM. The model extends a recently developed semi-parametric generalized linear model (SPGLM) by adding a nonparametric Bayesian prior on the baseline distribution of the GLM. We show that the resulting model takes the form of an inhomogeneous completely random measure that arises from exponential tilting of a normalized completely random measure. Building on familiar posterior simulation methods for mixtures with respect to normalized random measures we introduce posterior simulation in the resulting semi-parametric GLM model. The proposed methodology is validated through a series of simulation studies and is illustrated using data from a speech intelligibility study.",
    "url": "https://arxiv.org/abs/2502.17827",
    "arxivId": "",
    "last_visited": "2025-03-15T06:20:58.414Z",
    "last_read": "2025-03-15T06:20:58.414Z",
    "total_reading_time_seconds": 19,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2502.17827/features/markdown-grobid/arxiv:2502.17827.md",
      "adr-crib": "data/papers/arxiv:2502.17827/features/adr-crib/arxiv:2502.17827.md",
      "adr-titles": "data/papers/arxiv:2502.17827/features/adr-titles/arxiv:2502.17827.md",
      "crib-sheet": "data/papers/arxiv:2502.17827/features/crib-sheet/arxiv:2502.17827.md"
    }
  },
  "arxiv:2503.07588": {
    "id": "arxiv:2503.07588",
    "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery:   Coarse-to-Fine Text-Guided Token Pruning",
    "authors": "Junwei Luo, Yingying Zhang, Xue Yang and 5 others",
    "abstract": "Efficient vision-language understanding of large Remote Sensing Images (RSIs) is meaningful but challenging. Current Large Vision-Language Models (LVLMs) typically employ limited pre-defined grids to process images, leading to information loss when handling gigapixel RSIs. Conversely, using unlimited grids significantly increases computational costs. To preserve image details while reducing computational complexity, we propose a text-guided token pruning method with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i) a Region Focus Module (RFM) that leverages text-aware region localization capability to identify critical vision tokens, and (ii) a coarse-to-fine image tile selection and vision token pruning strategy based on DIP, which is guided by RFM outputs and avoids directly processing the entire large imagery. Additionally, existing benchmarks for evaluating LVLMs' perception ability on large RSI suffer from limited question diversity and constrained image sizes. We construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs across 8 categories, with image length up to 27,328 pixels. Our method outperforms existing high-resolution strategies on four datasets using the same data. Moreover, compared to existing token reduction methods, our approach demonstrates higher efficiency under high-resolution settings. Dataset and code are in https://github.com/VisionXLab/LRS-VQA.",
    "url": "https://arxiv.org/abs/2503.07588",
    "arxivId": "",
    "last_visited": "2025-03-15T07:12:00.059Z",
    "last_read": "2025-03-15T07:12:00.059Z",
    "total_reading_time_seconds": 18,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2503.07588/features/markdown-grobid/arxiv:2503.07588.md",
      "adr-crib": "data/papers/arxiv:2503.07588/features/adr-crib/arxiv:2503.07588.md",
      "adr-titles": "data/papers/arxiv:2503.07588/features/adr-titles/arxiv:2503.07588.md",
      "crib-sheet": "data/papers/arxiv:2503.07588/features/crib-sheet/arxiv:2503.07588.md",
      "compound-crib": "data/papers/arxiv:2503.07588/features/compound-crib/arxiv:2503.07588.md"
    }
  },
  "arxiv:1805.12400": {
    "id": "arxiv:1805.12400",
    "title": "Tropical Foundations for Probability & Statistics on Phylogenetic Tree Space",
    "authors": "Bo Lin, Anthea Monod, Ruriko Yoshida",
    "abstract": "We introduce a novel framework for the statistical analysis of phylogenetic trees: Palm tree space is constructed on principles of tropical algebraic geometry, and represents phylogenetic trees as a point in a space endowed with the tropical metric. We show that palm tree space possesses a variety of properties that allow for the definition of probability measures, and thus expectations, variances, and other fundamental statistical quantities. This provides a new, tropical basis for a statistical treatment of evolutionary biological processes represented by phylogenetic trees. In particular, we show that a geometric approach to phylogenetic tree space --- first introduced by Billera, Holmes, and Vogtmann, which we reinterpret in this paper via tropical geometry --- results in analytic, geometric, and topological characteristics that are desirable for probability, statistics, and increased computational efficiency.",
    "url": "https://arxiv.org/abs/1805.12400v2",
    "arxivId": "",
    "last_visited": "2025-03-15T07:06:25.372Z",
    "last_read": "2025-03-15T07:06:25.372Z",
    "total_reading_time_seconds": 238,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:1805.12400/features/markdown-grobid/arxiv:1805.12400.md",
      "adr-crib": "data/papers/arxiv:1805.12400/features/adr-crib/arxiv:1805.12400.md",
      "adr-titles": "data/papers/arxiv:1805.12400/features/adr-titles/arxiv:1805.12400.md",
      "crib-sheet": "data/papers/arxiv:1805.12400/features/crib-sheet/arxiv:1805.12400.md",
      "compound-crib": "data/papers/arxiv:1805.12400/features/compound-crib/arxiv:1805.12400.md"
    }
  },
  "arxiv:2503.06514": {
    "id": "arxiv:2503.06514",
    "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with   Generative Flow Networks",
    "authors": "Haoqiang Kang, Enna Sachdeva, Piyush Gupta and 2 others",
    "abstract": "Vision-Language Models (VLMs) have recently shown promising advancements in sequential decision-making tasks through task-specific fine-tuning. However, common fine-tuning methods, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) techniques like Proximal Policy Optimization (PPO), present notable limitations: SFT assumes Independent and Identically Distributed (IID) data, while PPO focuses on maximizing cumulative rewards. These limitations often restrict solution diversity and hinder generalization in multi-step reasoning tasks. To address these challenges, we introduce a novel framework, GFlowVLM, a framework that fine-tune VLMs using Generative Flow Networks (GFlowNets) to promote generation of diverse solutions for complex reasoning tasks. GFlowVLM models the environment as a non-Markovian decision process, allowing it to capture long-term dependencies essential for real-world applications. It takes observations and task descriptions as inputs to prompt chain-of-thought (CoT) reasoning which subsequently guides action selection. We use task based rewards to fine-tune VLM with GFlowNets. This approach enables VLMs to outperform prior fine-tuning methods, including SFT and RL. Empirical results demonstrate the effectiveness of GFlowVLM on complex tasks such as card games (NumberLine, BlackJack) and embodied planning tasks (ALFWorld), showing enhanced training efficiency, solution diversity, and stronger generalization capabilities across both in-distribution and out-of-distribution scenarios.",
    "url": "https://arxiv.org/abs/2503.06514",
    "arxivId": "",
    "last_visited": "2025-03-15T14:48:06.243Z",
    "last_read": "2025-03-15T14:48:06.243Z",
    "total_reading_time_seconds": 150,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2503.06514/features/markdown-grobid/arxiv:2503.06514.md",
      "adr-crib": "data/papers/arxiv:2503.06514/features/adr-crib/arxiv:2503.06514.md",
      "adr-titles": "data/papers/arxiv:2503.06514/features/adr-titles/arxiv:2503.06514.md",
      "crib-sheet": "data/papers/arxiv:2503.06514/features/crib-sheet/arxiv:2503.06514.md",
      "compound-crib": "data/papers/arxiv:2503.06514/features/compound-crib/arxiv:2503.06514.md"
    }
  },
  "arxiv:2503.09617": {
    "id": "arxiv:2503.09617",
    "title": "Factorio Learning Environment",
    "authors": "Jack Hopkins, Mart Bakler, Akbir Khan",
    "abstract": "Large Language Models (LLMs) are rapidly saturating existing benchmarks, necessitating new open-ended evaluations. We introduce the Factorio Learning Environment (FLE), based on the game of Factorio, that tests agents in long-term planning, program synthesis, and resource optimization. FLE provides exponentially scaling challenges -- from basic automation to complex factories processing millions of resource units per second. We provide two settings: (1) lab-play consisting of eight structured tasks with fixed resources, and (2) open-play with the unbounded task of building the largest factory on an procedurally generated map. We demonstrate across both settings that models still lack strong spatial reasoning. In lab-play, we find that LLMs exhibit promising short-horizon skills, yet are unable to operate effectively in constrained environments, reflecting limitations in error analysis. In open-play, while LLMs discover automation strategies that improve growth (e.g electric-powered drilling), they fail to achieve complex automation (e.g electronic-circuit manufacturing).",
    "url": "https://arxiv.org/abs/2503.09617",
    "arxivId": "",
    "last_visited": "2025-03-15T16:38:48.645Z",
    "last_read": "2025-03-15T16:38:48.645Z",
    "total_reading_time_seconds": 10,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2503.09617/features/markdown-grobid/arxiv:2503.09617.md",
      "adr-crib": "data/papers/arxiv:2503.09617/features/adr-crib/arxiv:2503.09617.md",
      "adr-titles": "data/papers/arxiv:2503.09617/features/adr-titles/arxiv:2503.09617.md",
      "crib-sheet": "data/papers/arxiv:2503.09617/features/crib-sheet/arxiv:2503.09617.md",
      "compound-crib": "data/papers/arxiv:2503.09617/features/compound-crib/arxiv:2503.09617.md"
    }
  },
  "arxiv:2011.03069": {
    "id": "arxiv:2011.03069",
    "title": "Cosmological Trans-Planckian Conjectures are not Effective",
    "authors": "C.P. Burgess, S. P. de Alwis, F. Quevedo",
    "abstract": "It is remarkable that the primordial fluctuations as revealed by the CMB coincide with what quantum fluctuations would look like if they were stretched across the sky by accelerated cosmic expansion. It has been observed that this same stretching also brings very small -- even trans-Planckian -- length scales up to observable sizes if extrapolated far enough into the past. This potentially jeopardizes later descriptions of late-time cosmology by introducing uncontrolled trans-Planckian theoretical errors into all calculations. Recent speculations, such as the Trans-Planckian Censorship Conjecture (TCC), have been developed to avoid this problem. We revisit old arguments why the consistency of (and control over) the Effective Field Theory (EFT) governing late-time cosmology is not necessarily threatened by the descent of modes due to universal expansion, even if EFT methods may break down at much earlier times. Failure of EFT methods only poses a problem if late-time predictions rely on non-adiabatic behaviour at these early times (such as is often true for bouncing cosmologies, for example). We illustrate our arguments using simple non-gravitational examples such as slowly rolling scalar fields and the spacing between Landau levels for charged particles in slowly varying magnetic fields, for which similar issues arise and are easier to understand. We comment on issues associated with UV completions. Our arguments need not invalidate speculative ideas like the TCC but suggest they are not required by the present evidence.",
    "url": "https://arxiv.org/abs/2011.03069",
    "arxivId": "",
    "last_visited": "2025-03-15T16:37:51.300Z",
    "last_read": "2025-03-15T16:37:51.300Z",
    "total_reading_time_seconds": 20,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2011.03069/features/markdown-grobid/arxiv:2011.03069.md",
      "adr-crib": "data/papers/arxiv:2011.03069/features/adr-crib/arxiv:2011.03069.md",
      "adr-titles": "data/papers/arxiv:2011.03069/features/adr-titles/arxiv:2011.03069.md",
      "crib-sheet": "data/papers/arxiv:2011.03069/features/crib-sheet/arxiv:2011.03069.md",
      "compound-crib": "data/papers/arxiv:2011.03069/features/compound-crib/arxiv:2011.03069.md"
    }
  },
  "arxiv:2405.21042": {
    "id": "arxiv:2405.21042",
    "title": "Comparing the information content of probabilistic representation spaces",
    "authors": "Kieran A. Murphy, Sam Dillavou, Dani S. Bassett",
    "abstract": "Probabilistic representation spaces convey information about a dataset and are shaped by factors such as the training data, network architecture, and loss function. Comparing the information content of such spaces is crucial for understanding the learning process, yet most existing methods assume point-based representations, neglecting the distributional nature of probabilistic spaces. To address this gap, we propose two information-theoretic measures to compare general probabilistic representation spaces by extending classic methods to compare the information content of hard clustering assignments. Additionally, we introduce a lightweight method of estimation that is based on fingerprinting a representation space with a sample of the dataset, designed for scenarios where the communicated information is limited to a few bits. We demonstrate the utility of these measures in three case studies. First, in the context of unsupervised disentanglement, we identify recurring information fragments within individual latent dimensions of VAE and InfoGAN ensembles. Second, we compare the full latent spaces of models and reveal consistent information content across datasets and methods, despite variability during training. Finally, we leverage the differentiability of our measures to perform model fusion, synthesizing the information content of weak learners into a single, coherent representation. Across these applications, the direct comparison of information content offers a natural basis for characterizing the processing of information.",
    "url": "https://arxiv.org/abs/2405.21042",
    "arxivId": "",
    "last_visited": "2025-03-15T17:46:05.502Z",
    "last_read": "2025-03-15T17:46:05.502Z",
    "total_reading_time_seconds": 55,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2405.21042/features/markdown-grobid/arxiv:2405.21042.md",
      "adr-crib": "data/papers/arxiv:2405.21042/features/adr-crib/arxiv:2405.21042.md",
      "adr-titles": "data/papers/arxiv:2405.21042/features/adr-titles/arxiv:2405.21042.md",
      "crib-sheet": "data/papers/arxiv:2405.21042/features/crib-sheet/arxiv:2405.21042.md",
      "compound-crib": "data/papers/arxiv:2405.21042/features/compound-crib/arxiv:2405.21042.md"
    }
  },
  "arxiv:2503.09637": {
    "id": "arxiv:2503.09637",
    "title": "Complementarity, Augmentation, or Substitutivity? The Impact of Generative Artificial Intelligence on the U.S. Federal Workforce",
    "authors": "William G. Resh, Yi Ming, Xinyao Xia and 3 others",
    "abstract": "This study investigates the near-future impacts of generative artificial intelligence (AI) technologies on occupational competencies across the U.S. federal workforce. We develop a multi-stage Retrieval-Augmented Generation system to leverage large language models for predictive AI modeling that projects shifts in required competencies and to identify vulnerable occupations on a knowledge-by-skill-by-ability basis across the federal government workforce. This study highlights policy recommendations essential for workforce planning in the era of AI. We integrate several sources of detailed data on occupational requirements across the federal government from both centralized and decentralized human resource sources, including from the U.S. Office of Personnel Management (OPM) and various federal agencies. While our preliminary findings suggest some significant shifts in required competencies and potential vulnerability of certain roles to AI-driven changes, we provide nuanced insights that support arguments against abrupt or generic approaches to strategic human capital planning around the development of generative AI. The study aims to inform strategic workforce planning and policy development within federal agencies and demonstrates how this approach can be replicated across other large employment institutions and labor markets.",
    "url": "https://arxiv.org/abs/2503.09637",
    "arxivId": "",
    "last_visited": "2025-03-15T20:34:19.823Z",
    "last_read": "2025-03-15T20:34:19.823Z",
    "total_reading_time_seconds": 35,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2503.09637/features/markdown-grobid/arxiv:2503.09637.md",
      "adr-crib": "data/papers/arxiv:2503.09637/features/adr-crib/arxiv:2503.09637.md",
      "adr-titles": "data/papers/arxiv:2503.09637/features/adr-titles/arxiv:2503.09637.md",
      "crib-sheet": "data/papers/arxiv:2503.09637/features/crib-sheet/arxiv:2503.09637.md",
      "compound-crib": "data/papers/arxiv:2503.09637/features/compound-crib/arxiv:2503.09637.md"
    }
  },
  "arxiv:2503.07465": {
    "id": "arxiv:2503.07465",
    "title": "YOLOE: Real-Time Seeing Anything",
    "authors": "Ao Wang, Lihao Liu, Hui Chen and 3 others",
    "abstract": "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3$\\times$ less training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less training time. Code and models are available at this https URL.",
    "url": "https://arxiv.org/abs/2503.07465",
    "arxivId": "",
    "last_visited": "2025-03-15T20:54:39.426Z",
    "last_read": "2025-03-15T20:54:39.426Z",
    "total_reading_time_seconds": 25,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2503.07465/features/markdown-grobid/arxiv:2503.07465.md",
      "adr-crib": "data/papers/arxiv:2503.07465/features/adr-crib/arxiv:2503.07465.md",
      "adr-titles": "data/papers/arxiv:2503.07465/features/adr-titles/arxiv:2503.07465.md",
      "crib-sheet": "data/papers/arxiv:2503.07465/features/crib-sheet/arxiv:2503.07465.md",
      "compound-crib": "data/papers/arxiv:2503.07465/features/compound-crib/arxiv:2503.07465.md"
    }
  },
  "arxiv:2501.12948": {
    "id": "arxiv:2501.12948",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "authors": "DeepSeek-AI, Daya Guo, Dejian Yang and 200 others",
    "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "url": "https://arxiv.org/abs/2501.12948",
    "arxivId": "",
    "last_visited": "2025-03-15T21:59:13.051Z",
    "last_read": "2025-03-15T21:59:13.051Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:2501.12948/features/markdown-grobid/arxiv:2501.12948.md",
      "adr-crib": "data/papers/arxiv:2501.12948/features/adr-crib/arxiv:2501.12948.md",
      "adr-titles": "data/papers/arxiv:2501.12948/features/adr-titles/arxiv:2501.12948.md",
      "crib-sheet": "data/papers/arxiv:2501.12948/features/crib-sheet/arxiv:2501.12948.md"
    }
  },
  "arxiv:1503.03585": {
    "id": "arxiv:1503.03585",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": "Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli",
    "abstract": "A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",
    "url": "https://arxiv.org/abs/1503.03585",
    "arxivId": "",
    "last_visited": "2025-03-16T00:12:10.411Z",
    "last_read": "2025-03-16T00:12:10.411Z",
    "total_reading_time_seconds": 10,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": {
      "markdown-grobid": "data/papers/arxiv:1503.03585/features/markdown-grobid/arxiv:1503.03585.md",
      "adr-crib": "data/papers/arxiv:1503.03585/features/adr-crib/arxiv:1503.03585.md",
      "adr-titles": "data/papers/arxiv:1503.03585/features/adr-titles/arxiv:1503.03585.md",
      "crib-sheet": "data/papers/arxiv:1503.03585/features/crib-sheet/arxiv:1503.03585.md",
      "compound-crib": "data/papers/arxiv:1503.03585/features/compound-crib/arxiv:1503.03585.md"
    }
  },
  "arxiv.2403.15711": {
    "id": "arxiv.2403.15711",
    "title": "Identifiable Latent Neural Causal Models",
    "authors": "Yuhang Liu, Zhen Zhang, Dong Gong and 5 others",
    "abstract": "Causal representation learning seeks to uncover latent, high-level causal representations from low-level observed data. It is particularly good at predictions under unseen distribution shifts, because these shifts can generally be interpreted as consequences of interventions. Hence leveraging {seen} distribution shifts becomes a natural strategy to help identifying causal representations, which in turn benefits predictions where distributions are previously {unseen}. Determining the types (or conditions) of such distribution shifts that do contribute to the identifiability of causal representations is critical. This work establishes a {sufficient} and {necessary} condition characterizing the types of distribution shifts for identifiability in the context of latent additive noise models. Furthermore, we present partial identifiability results when only a portion of distribution shifts meets the condition. In addition, we extend our findings to latent post-nonlinear causal models. We translate our findings into a practical algorithm, allowing for the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations align closely with the theoretical findings, affirming the robustness and effectiveness of our approach.",
    "url": "https://arxiv.org/abs/2403.15711",
    "arxivId": "",
    "last_visited": "2025-03-16T05:20:04.525Z",
    "last_read": "2025-03-16T05:20:04.525Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2501.10846": {
    "id": "arxiv.2501.10846",
    "title": "The Missing Link: Identifying Digital Intermediaries in E-Government",
    "authors": "Sergio Toro-Maureira, Alejandro Olivares, Rocio Saez-Vergara and 3 others",
    "abstract": "The digitalization of public administration has advanced significantly on a global scale. Many governments now view digital platforms as essential for improving the delivery of public services and fostering direct communication between citizens and public institutions. However, this view overlooks the role played by digital intermediaries significantly shape the provision of e-government services. Using Chile as a case study, we analyze these intermediaries through a national survey on digitalization, we find five types of intermediaries: family members, peers, political figures, bureaucrats, and community leaders. The first two classes comprise close intermediaries, while the latter three comprise hierarchical intermediaries. Our findings suggest that all these intermediaries are a critical but underexplored element in the digitalization of public administration.",
    "url": "https://arxiv.org/abs/2501.10846",
    "arxivId": "",
    "last_visited": "2025-03-16T05:18:14.960Z",
    "last_read": "2025-03-16T05:18:14.960Z",
    "total_reading_time_seconds": 40,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.05522": {
    "id": "arxiv.2503.05522",
    "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept   Representations",
    "authors": "Eren Erogullari, Sebastian Lapuschkin, Wojciech Samek, Frederik Pahde",
    "abstract": "Concept Activation Vectors (CAVs) are widely used to model human-understandable concepts as directions within the latent space of neural networks. They are trained by identifying directions from the activations of concept samples to those of non-concept samples. However, this method often produces similar, non-orthogonal directions for correlated concepts, such as \"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in images of men. This entanglement complicates the interpretation of concepts in isolation and can lead to undesired effects in CAV applications, such as activation steering. To address this issue, we introduce a post-hoc concept disentanglement method that employs a non-orthogonality loss, facilitating the identification of orthogonal concept directions while preserving directional correctness. We evaluate our approach with real-world and controlled correlated concepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18 architectures. We further demonstrate the superiority of orthogonalized concept representations in activation steering tasks, allowing (1) the insertion of isolated concepts into input images through generative models and (2) the removal of concepts for effective shortcut suppression with reduced impact on correlated concepts in comparison to baseline CAVs.",
    "url": "https://arxiv.org/abs/2503.05522",
    "arxivId": "",
    "last_visited": "2025-03-16T07:50:05.458Z",
    "last_read": "2025-03-16T07:50:05.458Z",
    "total_reading_time_seconds": 110,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2402.17119": {
    "id": "arxiv.2402.17119",
    "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
    "authors": "Kaige Xie, Mark Riedl",
    "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
    "url": "https://arxiv.org/abs/2402.17119",
    "arxivId": "",
    "last_visited": "2025-03-16T16:46:33.709Z",
    "last_read": "2025-03-16T16:46:33.709Z",
    "total_reading_time_seconds": 20,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.10633": {
    "id": "arxiv.2503.10633",
    "title": "Charting and Navigating Hugging Face's Model Atlas",
    "authors": "Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana and 2 others",
    "abstract": "As there are now millions of publicly available neural networks, searching and analyzing large model repositories becomes increasingly important. Navigating so many models requires an atlas, but as most models are poorly documented charting such an atlas is challenging. To explore the hidden potential of model repositories, we chart a preliminary atlas representing the documented fraction of Hugging Face. It provides stunning visualizations of the model landscape and evolution. We demonstrate several applications of this atlas including predicting model attributes (e.g., accuracy), and analyzing trends in computer vision models. However, as the current atlas remains incomplete, we propose a method for charting undocumented regions. Specifically, we identify high-confidence structural priors based on dominant real-world model training practices. Leveraging these priors, our approach enables accurate mapping of previously undocumented areas of the atlas. We publicly release our datasets, code, and interactive atlas.",
    "url": "https://arxiv.org/abs/2503.10633",
    "arxivId": "",
    "last_visited": "2025-03-16T16:55:00.673Z",
    "last_read": "2025-03-16T16:55:00.673Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.1906.01205": {
    "id": "arxiv.1906.01205",
    "title": "A Strong and Robust Baseline for Text-Image Matching",
    "authors": "Fangyu Liu, Rongtian Ye",
    "abstract": "We review the current schemes of text-image matching models and propose improvements for both training and inference. First, we empirically show limitations of two popular loss (sum and max-margin loss) widely used in training text-image embeddings and propose a trade-off: a kNN-margin loss which 1) utilizes information from hard negatives and 2) is robust to noise as all $K$-most hardest samples are taken into account, tolerating \\emph{pseudo} negatives and outliers. Second, we advocate the use of Inverted Softmax (\\textsc{Is}) and Cross-modal Local Scaling (\\textsc{Csls}) during inference to mitigate the so-called hubness problem in high-dimensional embedding space, enhancing scores of all metrics by a large margin.",
    "url": "https://arxiv.org/abs/1906.01205",
    "arxivId": "",
    "last_visited": "2025-03-16T17:36:40.649Z",
    "last_read": "2025-03-16T17:36:40.649Z",
    "total_reading_time_seconds": 100,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.1607.01759": {
    "id": "arxiv.1607.01759",
    "title": "Bag of Tricks for Efficient Text Classification",
    "authors": "Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov",
    "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
    "url": "https://arxiv.org/abs/1607.01759",
    "arxivId": "",
    "last_visited": "2025-03-16T17:46:20.359Z",
    "last_read": "2025-03-16T17:46:20.359Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2111.02114": {
    "id": "arxiv.2111.02114",
    "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs",
    "authors": "Christoph Schuhmann, Richard Vencu, Romain Beaumont and 6 others",
    "abstract": "Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search.",
    "url": "https://arxiv.org/abs/2111.02114",
    "arxivId": "",
    "last_visited": "2025-03-16T19:03:00.468Z",
    "last_read": "2025-03-16T19:03:00.468Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2112.10752": {
    "id": "arxiv.2112.10752",
    "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz and 2 others",
    "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at this https URL .",
    "url": "https://arxiv.org/abs/2112.10752",
    "arxivId": "",
    "last_visited": "2025-03-16T19:02:27.405Z",
    "last_read": "2025-03-16T19:02:27.405Z",
    "total_reading_time_seconds": 25,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "pdf.pdf.478253C1": {
    "id": "pdf.pdf.478253C1",
    "title": "pdf.478253C1",
    "authors": "",
    "abstract": "",
    "url": "https://www.research.unipd.it/bitstream/11577/3162912/2/2015_Kramer%20Bressan_Humans%20as%20superorganisms%20preprint.pdf",
    "arxivId": "",
    "last_visited": "2025-03-16T19:29:33.967Z",
    "last_read": "2025-03-16T19:29:33.967Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.url.14AC3C7E": {
    "id": "url.url.14AC3C7E",
    "title": "View of Automated decision-making as domination",
    "authors": "",
    "abstract": "",
    "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630/11605",
    "arxivId": "",
    "last_visited": "2025-03-16T19:28:38.365Z",
    "last_read": "2025-03-16T19:28:38.365Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.url.6BAC17CA": {
    "id": "url.url.6BAC17CA",
    "title": "Automated decision-making as domination | First Monday",
    "authors": "",
    "abstract": "",
    "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630",
    "arxivId": "",
    "last_visited": "2025-03-16T19:27:27.432Z",
    "last_read": "2025-03-16T19:27:27.432Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.6BAC17CA": {
    "id": "url.6BAC17CA",
    "title": "Automated decision-making as domination | First Monday",
    "authors": "",
    "abstract": "",
    "url": "https://firstmonday.org/ojs/index.php/fm/article/view/13630",
    "arxivId": "",
    "last_visited": "2025-03-16T19:40:43.042Z",
    "last_read": "2025-03-16T19:40:43.042Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.2020268F": {
    "id": "url.2020268F",
    "title": "A Simple but Tough-to-Beat Baseline for Sentence Embeddings",
    "authors": "Sanjeev Arora, Yingyu Liang, Tengyu Ma",
    "abstract": " The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that such complicated methods are outperformed, especially in out-of-domain (transfer learning) settings, by simpler methods involving mild retraining of word embeddings and basic linear regression. The  method of Wieting et al. requires retraining with a substantial labeled dataset such as Paraphrase Database (Ganitkevitch et al., 2013).   The current paper goes further, showing that the following completely unsupervised sentence embedding is a formidable baseline: Use word embeddings computed using one of the popular methods on unlabeled corpus like Wikipedia, represent the sentence by a weighted average of the word vectors, and then modify them a bit using PCA/SVD. This weighting improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN's and LSTM's. It even improves Wieting et al.'s embeddings.   This simple method should be used as the baseline to beat in future, especially when labeled training data is scarce or nonexistent.   The paper also gives a theoretical explanation of the success of the above unsupervised method using a latent variable generative model for sentences, which is a simple extension of the model in Arora et al. (TACL'16) with new \"smoothing\" terms that allow for  words occurring out of context, as well as high probabilities for words like and, not in all contexts. ",
    "url": "https://openreview.net/forum?id=SyK00v5xx",
    "arxivId": "",
    "last_visited": "2025-03-16T20:09:44.775Z",
    "last_read": "2025-03-16T20:09:44.775Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.30BB8D13": {
    "id": "url.30BB8D13",
    "title": "MicroBERT: Distilling MoE-Based Knowledge from BERT into a Lighter Model",
    "authors": "Zheng, Dashun, Li and 7 others",
    "abstract": "Natural language-processing tasks have been improved greatly by large language models (LLMs). However, numerous parameters make their execution computationally expensive and difficult on resource-constrained devices. For this problem, as well as maintaining accuracy, some techniques such as distillation and quantization have been proposed. Unfortunately, current methods fail to integrate model pruning with downstream tasks and overlook sentence-level semantic modeling, resulting in reduced efficiency of distillation. To alleviate these limitations, we propose a novel distilled lightweight model for BERT named MicroBERT. This method can transfer the knowledge contained in the “teacher” BERT model to a “student” BERT model. The sentence-level feature alignment loss (FAL) distillation mechanism, guided by Mixture-of-Experts (MoE), captures comprehensive contextual semantic knowledge from the “teacher” model to enhance the “student” model’s performance while reducing its parameters. To make the outputs of “teacher” and “student” models comparable, we introduce the idea of a generative adversarial network (GAN) to train a discriminator. Our experimental results based on four datasets show that all steps of our distillation mechanism are effective, and the MicroBERT (101.14%) model outperforms TinyBERT (99%) by 2.24% in terms of average distillation reductions in various tasks on the GLUE dataset.",
    "url": "https://www.mdpi.com/2076-3417/14/14/6171",
    "arxivId": "",
    "last_visited": "2025-03-16T21:25:41.885Z",
    "last_read": "2025-03-16T21:25:41.885Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.08827": {
    "id": "arxiv.2503.08827",
    "title": "Neural Network/de Sitter Space Correspondence",
    "authors": "Donghee Lee, Hye-Sung Lee, Jaeok Yi",
    "abstract": "Machine learning's remarkable practical successes have sparked extensive theoretical investigations, yet fundamental breakthroughs remain elusive. Here, we study neural network training via gradient descent and demonstrate that the resulting dynamics can be described by a field theory in de Sitter space. We illustrate this correspondence with a simple example of neural network in which the continuum limit is explicitly realized. Since de Sitter space is well-studied in both field theory and general relativity, our findings suggest new avenues for understanding neural network training and for leveraging established theoretical frameworks to advance machine learning theory.",
    "url": "https://arxiv.org/abs/2503.08827",
    "arxivId": "",
    "last_visited": "2025-03-16T21:48:01.798Z",
    "last_read": "2025-03-16T21:48:01.798Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2501.18812": {
    "id": "arxiv.2501.18812",
    "title": "Estimating the Probability of Sampling a Trained Neural Network at Random",
    "authors": "Adam Scherlis, Nora Belrose",
    "abstract": "We present an algorithm for estimating the probability mass, under a Gaussian or uniform prior, of a region in neural network parameter space corresponding to a particular behavior, such as achieving test loss below some threshold. When the prior is uniform, this problem is equivalent to measuring the volume of a region. We show empirically and theoretically that existing algorithms for estimating volumes in parameter space underestimate the true volume by millions of orders of magnitude. We find that this error can be dramatically reduced, but not entirely eliminated, with an importance sampling method using gradient information that is already provided by popular optimizers. The negative logarithm of this probability can be interpreted as a measure of a network's information content, in accordance with minimum description length (MDL) principles and rate-distortion theory. As expected, this quantity increases during language model training. We also find that badly-generalizing behavioral regions are smaller, and therefore less likely to be sampled at random, demonstrating an inductive bias towards well-generalizing functions.",
    "url": "https://arxiv.org/abs/2501.18812",
    "arxivId": "",
    "last_visited": "2025-03-16T22:19:29.057Z",
    "last_read": "2025-03-16T22:19:29.057Z",
    "total_reading_time_seconds": 125,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2405.07987": {
    "id": "arxiv.2405.07987",
    "title": "The Platonic Representation Hypothesis",
    "authors": "Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",
    "abstract": "We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
    "url": "https://arxiv.org/abs/2405.07987",
    "arxivId": "",
    "last_visited": "2025-03-16T23:39:31.033Z",
    "last_read": "2025-03-16T23:39:31.033Z",
    "total_reading_time_seconds": 60,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.1912.07242": {
    "id": "arxiv.1912.07242",
    "title": "More Data Can Hurt for Linear Regression: Sample-wise Double Descent",
    "authors": "Preetum Nakkiran",
    "abstract": "In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples. In other words, more data actually hurts the estimator. This behavior is implicit in a recent line of theoretical works analyzing \"double-descent\" phenomenon in linear models. In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates. In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.",
    "url": "https://arxiv.org/abs/1912.07242",
    "arxivId": "",
    "last_visited": "2025-03-17T01:53:44.901Z",
    "last_read": "2025-03-17T01:53:44.901Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.7B57F8D5": {
    "id": "url.7B57F8D5",
    "title": "7B57F8D5",
    "authors": "",
    "abstract": "",
    "url": "https://openreview.net/pdf?id=pSk5qyt1ob",
    "arxivId": "",
    "last_visited": "2025-03-17T04:11:09.760Z",
    "last_read": "2025-03-17T04:11:09.760Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.1E3A4DA": {
    "id": "url.1E3A4DA",
    "title": "On Training-Conditional Conformal Prediction and Binomial Proportion Confidence Intervals",
    "authors": "Rudi Coppola, Manuel Mazo Espinosa",
    "abstract": "Estimating the expectation of a Bernoulli random variable based on $N$ independent trials is a classical problem in statistics, typically addressed using Binomial Proportion Confidence Intervals (BPCI). In the control systems community, many critical tasks—such as certifying the statistical safety of dynamical systems—can be formulated as BPCI problems.  Conformal Prediction (CP), a distribution-free technique for uncertainty quantification, has gained significant attention in recent years and has been applied to various control systems problems, particularly to address uncertainties in learned dynamics or controllers. A variant known as training-conditional CP was recently employed to tackle the problem of safety certification.  In this note, we highlight that the use of training-conditional CP in this context does not provide valid safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and argue that traditional BPCI methods are better suited for statistical safety certification.",
    "url": "https://openreview.net/forum?id=pSk5qyt1ob",
    "arxivId": "",
    "last_visited": "2025-03-17T04:10:46.125Z",
    "last_read": "2025-03-17T04:10:46.125Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2501.11433": {
    "id": "arxiv.2501.11433",
    "title": "One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor",
    "authors": "Zhikun Wu (KTH Royal Institute of Technology), Thomas Weber (LMU Munich), Florian Müller (TU Darmstadt)",
    "abstract": "Collaboration has been shown to enhance creativity, leading to more innovative and effective outcomes. While previous research has explored the abilities of Large Language Models (LLMs) to serve as co-creative partners in tasks like writing poetry or creating narratives, the collaborative potential of LLMs in humor-rich and culturally nuanced domains remains an open question. To address this gap, we conducted a user study to explore the potential of LLMs in co-creating memes - a humor-driven and culturally specific form of creative expression. We conducted a user study with three groups of 50 participants each: a human-only group creating memes without AI assistance, a human-AI collaboration group interacting with a state-of-the-art LLM model, and an AI-only group where the LLM autonomously generated memes. We assessed the quality of the generated memes through crowdsourcing, with each meme rated on creativity, humor, and shareability. Our results showed that LLM assistance increased the number of ideas generated and reduced the effort participants felt. However, it did not improve the quality of the memes when humans collaborated with LLM. Interestingly, memes created entirely by AI performed better than both human-only and human-AI collaborative memes in all areas on average. However, when looking at the top-performing memes, human-created ones were better in humor, while human-AI collaborations stood out in creativity and shareability. These findings highlight the complexities of human-AI collaboration in creative tasks. While AI can boost productivity and create content that appeals to a broad audience, human creativity remains crucial for content that connects on a deeper level.",
    "url": "https://arxiv.org/abs/2501.11433v2",
    "arxivId": "",
    "last_visited": "2025-03-17T04:31:57.365Z",
    "last_read": "2025-03-17T04:31:57.365Z",
    "total_reading_time_seconds": 100,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.65F9475E": {
    "id": "url.65F9475E",
    "title": "ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers",
    "authors": "Junjie Yin, Jiahao Dong, Yingheng Wang and 2 others",
    "abstract": "We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 2/3/4-bit precision on as little as one 24GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 2-bit and 3-bit LLMs for the first time---leveraging state-of-the-art 2-bit QuIP# quantization and 3-bit OPTQ quantization---outperforming finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.",
    "url": "https://openreview.net/forum?id=r9p9CV52MV",
    "arxivId": "",
    "last_visited": "2025-03-17T05:29:58.441Z",
    "last_read": "2025-03-17T05:29:58.441Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.10880": {
    "id": "arxiv.2503.10880",
    "title": "Spontaneous Optimal Mixing via Defect-Vortex Coupling in Confined Active Nematics",
    "authors": "Brandon Klein, Alejandro J. Soto Franco, Md Mainul Hasan Sabbir and 4 others",
    "abstract": "Active nematic flows in two dimensions, largely driven by motile $+1/2$ disclinations, mix themselves efficiently and exhibit chaos in the bulk steady state. Motivated by recent experimental findings for three-defect braiding in cardioid-shaped domains, we investigate how this tendency toward chaotic fluid mixing can, counterintuitively, produce certain ordered, periodic flows in confinement with a controllable net topological charge. We study two-dimensional active nematics in systems with boundary conditions requiring a prescribed number of excess $+1/2$ disclinations, using Beris-Edwards nematohydrodynamics simulations, alongside an agent-based, hydrodynamic simulation approach. We find ordered flows for systems of three and four defects, and we use tools from braid theory to show that spontaneously occurring periodic defect motions produce maximal topological entropy. Our theory correctly predicts the generic absence of stable periodic orbits of more than four defects in strong confinement in simulation. Our results identify the parameter regime outside of which periodicity is lost, and allow us to probe the limits of topological entropy production.",
    "url": "https://arxiv.org/abs/2503.10880",
    "arxivId": "",
    "last_visited": "2025-03-17T05:28:45.216Z",
    "last_read": "2025-03-17T05:28:45.216Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.11272": {
    "id": "arxiv.2503.11272",
    "title": "When Do Transformers Outperform Feedforward and Recurrent Networks? A   Statistical Perspective",
    "authors": "Alireza Mousavi-Hosseini, Clayton Sanford, Denny Wu, Murat A. Erdogdu",
    "abstract": "Theoretical efforts to prove advantages of Transformers in comparison with classical architectures such as feedforward and recurrent neural networks have mostly focused on representational power. In this work, we take an alternative perspective and prove that even with infinite compute, feedforward and recurrent networks may suffer from larger sample complexity compared to Transformers, as the latter can adapt to a form of dynamic sparsity. Specifically, we consider a sequence-to-sequence data generating model on sequences of length $N$, in which the output at each position depends only on $q$ relevant tokens with $q \\ll N$, and the positions of these tokens are described in the input prompt. We prove that a single-layer Transformer can learn this model if and only if its number of attention heads is at least $q$, in which case it achieves a sample complexity almost independent of $N$, while recurrent networks require $N^{\\Omega(1)}$ samples on the same problem. If we simplify this model, recurrent networks may achieve a complexity almost independent of $N$, while feedforward networks still require $N$ samples. Consequently, our proposed sparse retrieval model illustrates a natural hierarchy in sample complexity across these architectures.",
    "url": "https://arxiv.org/abs/2503.11272",
    "arxivId": "",
    "last_visited": "2025-03-17T07:02:38.311Z",
    "last_read": "2025-03-17T07:02:38.311Z",
    "total_reading_time_seconds": 650,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.421D7430": {
    "id": "url.421D7430",
    "title": "Typing the technical interview",
    "authors": "",
    "abstract": "",
    "url": "https://aphyr.com/posts/342-typing-the-technical-interview",
    "arxivId": "",
    "last_visited": "2025-03-17T07:18:17.469Z",
    "last_read": "2025-03-17T07:18:17.469Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.11651": {
    "id": "arxiv.2503.11651",
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "authors": "Jianyuan Wang, Minghao Chen, Nikita Karaev and 3 others",
    "abstract": "We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.",
    "url": "https://arxiv.org/abs/2503.11651",
    "arxivId": "",
    "last_visited": "2025-03-17T07:24:23.471Z",
    "last_read": "2025-03-17T07:24:23.471Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.4C264A22": {
    "id": "url.4C264A22",
    "title": "The reproducibility of research and the misinterpretation of p-values | Royal Society Open Science",
    "authors": "",
    "abstract": " We wish to answer this question: If you observe a ‘significant’ p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by ...",
    "url": "https://royalsocietypublishing.org/doi/10.1098/rsos.171085",
    "arxivId": "",
    "last_visited": "2025-03-17T07:32:00.397Z",
    "last_read": "2025-03-17T07:32:00.397Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.4092DA01": {
    "id": "url.4092DA01",
    "title": "Generative AI with Stochastic Differential Equations - IAP 2025",
    "authors": "",
    "abstract": "",
    "url": "https://diffusion.csail.mit.edu/",
    "arxivId": "",
    "last_visited": "2025-03-17T08:29:34.172Z",
    "last_read": "2025-03-17T08:29:34.172Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.6A701FBE": {
    "id": "url.6A701FBE",
    "title": "Where Do We Stand with Implicit Neural Representations? A Technical and Performance Survey",
    "authors": "Amer Essakine, Yanqi Cheng, Chun-Wun Cheng and 5 others",
    "abstract": "Implicit Neural Representations (INRs) have emerged as a paradigm in knowledge representation, offering exceptional flexibility and performance across a diverse range of applications. INRs leverage multilayer perceptrons (MLPs) to model data as continuous implicit functions, providing critical advantages such as resolution independence, memory efficiency, and generalisation beyond discretised data structures. Their ability to solve complex inverse problems makes them particularly effective for tasks including audio reconstruction, image representation, 3D object reconstruction, and high-dimensional data synthesis. This survey provides a comprehensive review of state-of-the-art INR methods, introducing a clear taxonomy that categorises them into four key areas: activation functions, position encoding, combined strategies, and network structure optimisation. We rigorously analyse their critical properties—such as full differentiability, smoothness, compactness, and adaptability to varying resolutions—while also examining their strengths and limitations in addressing locality biases and capturing fine details. Our experimental comparison offers new insights into the trade-offs between different approaches, showcasing the capabilities and challenges of the latest INR techniques across various tasks. In addition to identifying areas where current methods excel, we highlight key limitations and potential avenues for improvement, such as developing more expressive activation functions, enhancing positional encoding mechanisms, and improving scalability for complex, high-dimensional data. This survey serves as a roadmap for researchers, offering practical guidance for future exploration in the field of INRs. We aim to foster new methodologies by outlining promising research directions for INRs and applications.",
    "url": "https://openreview.net/forum?id=QTsJXSvAI2",
    "arxivId": "",
    "last_visited": "2025-03-18T01:40:01.675Z",
    "last_read": "2025-03-18T01:40:01.675Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.08241": {
    "id": "arxiv.2503.08241",
    "title": "HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in   Embodied Agents",
    "authors": "Tristan Tomilin, Meng Fang, Mykola Pechenizkiy",
    "abstract": "Advancing safe autonomous systems through reinforcement learning (RL) requires robust benchmarks to evaluate performance, analyze methods, and assess agent competencies. Humans primarily rely on embodied visual perception to safely navigate and interact with their surroundings, making it a valuable capability for RL agents. However, existing vision-based 3D benchmarks only consider simple navigation tasks. To address this shortcoming, we introduce \\textbf{HASARD}, a suite of diverse and complex tasks to $\\textbf{HA}$rness $\\textbf{SA}$fe $\\textbf{R}$L with $\\textbf{D}$oom, requiring strategic decision-making, comprehending spatial relationships, and predicting the short-term future. HASARD features three difficulty levels and two action spaces. An empirical evaluation of popular baseline methods demonstrates the benchmark's complexity, unique challenges, and reward-cost trade-offs. Visualizing agent navigation during training with top-down heatmaps provides insight into a method's learning process. Incrementally training across difficulty levels offers an implicit learning curriculum. HASARD is the first safe RL benchmark to exclusively target egocentric vision-based learning, offering a cost-effective and insightful way to explore the potential and boundaries of current and future safe RL methods. The environments and baseline implementations are open-sourced at https://sites.google.com/view/hasard-bench/.",
    "url": "https://arxiv.org/abs/2503.08241",
    "arxivId": "",
    "last_visited": "2025-03-18T01:38:04.296Z",
    "last_read": "2025-03-18T01:38:04.296Z",
    "total_reading_time_seconds": 50,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.12593": {
    "id": "arxiv.2503.12593",
    "title": "Fourier-Based 3D Multistage Transformer for Aberration Correction in Multicellular Specimens",
    "authors": "Thayer Alshaabi, Daniel E. Milkie, Gaoxiang Liu and 17 others",
    "abstract": "High-resolution tissue imaging is often compromised by sample-induced optical aberrations that degrade resolution and contrast. While wavefront sensor-based adaptive optics (AO) can measure these aberrations, such hardware solutions are typically complex, expensive to implement, and slow when serially mapping spatially varying aberrations across large fields of view. Here, we introduce AOViFT (Adaptive Optical Vision Fourier Transformer) -- a machine learning-based aberration sensing framework built around a 3D multistage Vision Transformer that operates on Fourier domain embeddings. AOViFT infers aberrations and restores diffraction-limited performance in puncta-labeled specimens with substantially reduced computational cost, training time, and memory footprint compared to conventional architectures or real-space networks. We validated AOViFT on live gene-edited zebrafish embryos, demonstrating its ability to correct spatially varying aberrations using either a deformable mirror or post-acquisition deconvolution. By eliminating the need for the guide star and wavefront sensing hardware and simplifying the experimental workflow, AOViFT lowers technical barriers for high-resolution volumetric microscopy across diverse biological samples.",
    "url": "https://arxiv.org/abs/2503.12593",
    "arxivId": "",
    "last_visited": "2025-03-18T03:37:22.687Z",
    "last_read": "2025-03-18T03:37:22.687Z",
    "total_reading_time_seconds": 175,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.13298": {
    "id": "arxiv.2503.13298",
    "title": "From Few-Shot Optimal Control to Few-Shot Learning",
    "authors": "Roman Chertovskih, Nikolay Pogodaev, Maxim Staritsyn, A. Pedro Aguiar",
    "abstract": "We present an approach to solving unconstrained nonlinear optimal control problems for a broad class of dynamical systems. This approach involves lifting the nonlinear problem to a linear ``super-problem'' on a dual Banach space, followed by a non-standard ``exact'' variational analysis, -- culminating in a descent method that achieves rapid convergence with minimal iterations. We investigate the applicability of this framework to mean-field control and discuss its perspectives for the analysis of information propagation in self-interacting neural networks.",
    "url": "https://arxiv.org/abs/2503.13298",
    "arxivId": "",
    "last_visited": "2025-03-18T04:41:05.582Z",
    "last_read": "2025-03-18T04:41:05.582Z",
    "total_reading_time_seconds": 310,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.13CAADEE": {
    "id": "url.13CAADEE",
    "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
    "authors": "Haoyu Wang, Guozheng Ma, Cong Yu and 10 others",
    "abstract": "The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLMs, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, potentially  ignoring the superior generation capabilities of contemporary LLMs. To investigate the robustness of LLMs while using their generation ability, we propose a novel rational evaluation pipeline that leverages reward models as diagnostic tools to evaluate the long conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters.Our extensive empirical experiments demonstrate that TREvaL provides an identification for the lack of robustness of nowadays LLMs.Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted, calling for more attention on the robustness during alignment process.",
    "url": "https://openreview.net/forum?id=rWSiBknwQa",
    "arxivId": "",
    "last_visited": "2025-03-18T05:20:27.693Z",
    "last_read": "2025-03-18T05:20:27.693Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.13433": {
    "id": "arxiv.2503.13433",
    "title": "Less Biased Noise Scale Estimation for Threshold-Robust RANSAC",
    "authors": "Johan Edstedt",
    "abstract": "The gold-standard for robustly estimating relative pose through image matching is RANSAC. While RANSAC is powerful, it requires setting the inlier threshold that determines whether the error of a correspondence under an estimated model is sufficiently small to be included in its consensus set. Setting this threshold is typically done by hand, and is difficult to tune without a access to ground truth data. Thus, a method capable of automatically determining the optimal threshold would be desirable. In this paper we revisit inlier noise scale estimation, which is an attractive approach as the inlier noise scale is linear to the optimal threshold. We revisit the noise scale estimation method SIMFIT and find bias in the estimate of the noise scale. In particular, we fix underestimates from using the same data for fitting the model as estimating the inlier noise, and from not taking the threshold itself into account. Secondly, since the optimal threshold within a scene is approximately constant we propose a multi-pair extension of SIMFIT++, by filtering of estimates, which improves results. Our approach yields robust performance across a range of thresholds, shown in Figure 1.",
    "url": "https://arxiv.org/abs/2503.13433",
    "arxivId": "",
    "last_visited": "2025-03-18T05:18:58.414Z",
    "last_read": "2025-03-18T05:18:58.414Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.11989": {
    "id": "arxiv.2503.11989",
    "title": "Applications of Large Language Model Reasoning in Feature Generation",
    "authors": "Dharani Chandra",
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing through their state of art reasoning capabilities. This paper explores the convergence of LLM reasoning techniques and feature generation for machine learning tasks. We examine four key reasoning approaches: Chain of Thought, Tree of Thoughts, Retrieval-Augmented Generation, and Thought Space Exploration. Our analysis reveals how these approaches can be used to identify effective feature generation rules without having to manually specify search spaces. The paper categorizes LLM-based feature generation methods across various domains including finance, healthcare, and text analytics. LLMs can extract key information from clinical notes and radiology reports in healthcare, by enabling more efficient data utilization. In finance, LLMs facilitate text generation, summarization, and entity extraction from complex documents. We analyze evaluation methodologies for assessing feature quality and downstream performance, with particular attention to OCTree's decision tree reasoning approach that provides language-based feedback for iterative improvements. Current challenges include hallucination, computational efficiency, and domain adaptation. As of March 2025, emerging approaches include inference-time compute scaling, reinforcement learning, and supervised fine-tuning with model distillation. Future directions point toward multimodal feature generation, self-improving systems, and neuro-symbolic approaches. This paper provides a detailed overview of an emerging field that promises to automate and enhance feature engineering through language model reasoning.",
    "url": "https://arxiv.org/abs/2503.11989",
    "arxivId": "",
    "last_visited": "2025-03-18T06:05:02.021Z",
    "last_read": "2025-03-18T06:05:02.021Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.12266": {
    "id": "arxiv.2503.12266",
    "title": "Support Collapse of Deep Gaussian Processes with Polynomial Kernels for   a Wide Regime of Hyperparameters",
    "authors": "Daryna Chernobrovkina, Steffen Grünewälder",
    "abstract": "We analyze the prior that a Deep Gaussian Process with polynomial kernels induces. We observe that, even for relatively small depths, averaging effects occur within such a Deep Gaussian Process and that the prior can be analyzed and approximated effectively by means of the Berry-Esseen Theorem. One of the key findings of this analysis is that, in the absence of careful hyper-parameter tuning, the prior of a Deep Gaussian Process either collapses rapidly towards zero as the depth increases or places negligible mass on low norm functions. This aligns well with experimental findings and mirrors known results for convolution based Deep Gaussian Processes.",
    "url": "https://arxiv.org/abs/2503.12266",
    "arxivId": "",
    "last_visited": "2025-03-18T06:31:34.829Z",
    "last_read": "2025-03-18T06:31:34.829Z",
    "total_reading_time_seconds": 15,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.arxiv.2503.12266": {
    "id": "arxiv.arxiv.2503.12266",
    "title": "Support Collapse of Deep Gaussian Processes with Polynomial Kernels for a Wide Regime of Hyperparameters",
    "authors": "Daryna Chernobrovkina, Steffen Grünewälder",
    "abstract": "We analyze the prior that a Deep Gaussian Process with polynomial kernels induces. We observe that, even for relatively small depths, averaging effects occur within such a Deep Gaussian Process and that the prior can be analyzed and approximated effectively by means of the Berry-Esseen Theorem. One of the key findings of this analysis is that, in the absence of careful hyper-parameter tuning, the prior of a Deep Gaussian Process either collapses rapidly towards zero as the depth increases or places negligible mass on low norm functions. This aligns well with experimental findings and mirrors known results for convolution based Deep Gaussian Processes.",
    "url": "https://arxiv.org/abs/2503.12266",
    "arxivId": "",
    "last_visited": "2025-03-18T06:30:18.669Z",
    "last_read": "2025-03-18T06:30:18.669Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.arxiv.2503.11990": {
    "id": "arxiv.arxiv.2503.11990",
    "title": "Testing Stochastic Block Models Based on Maximum Sampling Entry-Wise Deviations",
    "authors": "Wu, Yujia, Lan and 5 others",
    "abstract": "The stochastic block model (SBM) has been widely used to analyze network data. Various goodness-of-fit tests have been proposed to assess the adequacy of model structures. To the best of our knowledge, however, none of the existing approaches are applicable for sparse networks in which the connection probability of any two communities is of order log n/n, and the number of communities is divergent. To fill this gap, we propose a novel goodness-of-fit test for the stochastic block model. The key idea is to construct statistics by sampling the maximum entry-deviations of the adjacency matrix that the negative impacts of network sparsity are alleviated by the sampling process. We demonstrate theoretically that the proposed test statistic converges to the Type-I extreme value distribution under the null hypothesis regardless of the network structure. Accordingly, it can be applied to both dense and sparse networks. In addition, we obtain the asymptotic power against alternatives. Moreover, we introduce a bootstrap-corrected test statistic to improve the finite sample performance, recommend an augmented test statistic to increase the power, and extend the proposed test to the degree-corrected SBM. Simulation studies and two empirical examples with both dense and sparse networks indicate that the proposed method performs well.",
    "url": "https://arxiv.org/abs/2503.11990",
    "arxivId": "",
    "last_visited": "2025-03-18T06:43:58.489Z",
    "last_read": "2025-03-18T06:43:58.489Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.08497": {
    "id": "arxiv.2502.08497",
    "title": "Foundations of Digital Circuits: Denotation, Operational, and Algebraic Semantics",
    "authors": "Kaye, George",
    "abstract": "This thesis details a project to define a fully compositional theory of synchronous sequential circuits built from primitive components, motivated by applying techniques successfully used in programming languages to hardware. The first part of the thesis defines the syntactic foundations of sequential circuit morphisms, and then builds three different semantic theories: denotational, operational and algebraic. We characterise the denotational semantics of sequential circuits as certain causal stream functions, as well as providing a link to existing circuit methodologies by mapping between circuit morphisms, stream functions and Mealy machines. The operational semantics is defined as a strategy for applying some global transformations followed by local reductions to demonstrate how a circuit processes a value, leading to a notion of observational equivalence. The algebraic semantics consists of equations for bringing circuits into a pseudo-normal form, and then encoding between different state sets. This part of the thesis concludes with a discussion of some novel applications, such as those for using partial evaluation for digital circuits. While mathematically rigorous, the categorical string diagram formalism is not suited for reasoning computationally. The second part of this thesis details an extension of string diagram rewriting with hypergraphs so that it is compatible with the traced comonoid structure present in the category of digital circuits. We identify the properties that characterise cospans of hypergraphs corresponding to traced comonoid terms, and demonstrate how to identify rewriting contexts valid for rewriting modulo traced comonoid structure. We apply the graph rewriting framework to fixed point operators as well as the operational semantics from the first part, and present a new hardware description language based on these theoretical developments.",
    "url": "https://arxiv.org/abs/2502.08497",
    "arxivId": "",
    "last_visited": "2025-03-18T07:06:04.236Z",
    "last_read": "2025-03-18T07:06:04.236Z",
    "total_reading_time_seconds": 35,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.url.1DA35195": {
    "id": "url.url.1DA35195",
    "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
    "authors": "",
    "abstract": "We propose Vamba, a hybrid Mamba-Transformer model that leverages cross-attention layers and Mamba-2 blocks for efficient hour-long video understanding.",
    "url": "https://tiger-ai-lab.github.io/Vamba/",
    "arxivId": "",
    "last_visited": "2025-03-18T06:52:42.223Z",
    "last_read": "2025-03-18T06:52:42.223Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.arxiv.2502.08497": {
    "id": "arxiv.arxiv.2502.08497",
    "title": "Foundations of Digital Circuits: Denotation, Operational, and Algebraic Semantics",
    "authors": "Kaye, George",
    "abstract": "This thesis details a project to define a fully compositional theory of synchronous sequential circuits built from primitive components, motivated by applying techniques successfully used in programming languages to hardware. The first part of the thesis defines the syntactic foundations of sequential circuit morphisms, and then builds three different semantic theories: denotational, operational and algebraic. We characterise the denotational semantics of sequential circuits as certain causal stream functions, as well as providing a link to existing circuit methodologies by mapping between circuit morphisms, stream functions and Mealy machines. The operational semantics is defined as a strategy for applying some global transformations followed by local reductions to demonstrate how a circuit processes a value, leading to a notion of observational equivalence. The algebraic semantics consists of equations for bringing circuits into a pseudo-normal form, and then encoding between different state sets. This part of the thesis concludes with a discussion of some novel applications, such as those for using partial evaluation for digital circuits. While mathematically rigorous, the categorical string diagram formalism is not suited for reasoning computationally. The second part of this thesis details an extension of string diagram rewriting with hypergraphs so that it is compatible with the traced comonoid structure present in the category of digital circuits. We identify the properties that characterise cospans of hypergraphs corresponding to traced comonoid terms, and demonstrate how to identify rewriting contexts valid for rewriting modulo traced comonoid structure. We apply the graph rewriting framework to fixed point operators as well as the operational semantics from the first part, and present a new hardware description language based on these theoretical developments.",
    "url": "https://arxiv.org/abs/2502.08497",
    "arxivId": "",
    "last_visited": "2025-03-18T06:50:55.071Z",
    "last_read": "2025-03-18T06:50:55.071Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.19046E18": {
    "id": "url.19046E18",
    "title": "19046E18",
    "authors": "",
    "abstract": "",
    "url": "https://dapo-sia.github.io/static/pdf/dapo_paper.pdf",
    "arxivId": "",
    "last_visited": "2025-03-18T07:09:08.556Z",
    "last_read": "2025-03-18T07:09:08.556Z",
    "total_reading_time_seconds": 255,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "pdf.728CBF81": {
    "id": "pdf.728CBF81",
    "title": "728CBF81",
    "authors": "",
    "abstract": "",
    "url": "https://papers.nips.cc/paper_files/paper/2014/file/b78666971ceae55a8e87efb7cbfd9ad4-Paper.pdf",
    "arxivId": "",
    "last_visited": "2025-03-18T14:56:53.952Z",
    "last_read": "2025-03-18T14:56:53.952Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.4516794": {
    "id": "url.4516794",
    "title": "Neural Word Embedding as Implicit Matrix Factorization",
    "authors": "Levy, Omer, Goldberg, Yoav",
    "abstract": "",
    "url": "https://papers.nips.cc/paper_files/paper/2014/hash/b78666971ceae55a8e87efb7cbfd9ad4-Abstract.html",
    "arxivId": "",
    "last_visited": "2025-03-18T14:56:38.611Z",
    "last_read": "2025-03-18T14:56:38.611Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2501.04896": {
    "id": "arxiv.2501.04896",
    "title": "2501.04896",
    "authors": "",
    "abstract": "",
    "url": "https://arxiv.org/pdf/2501.04896",
    "arxivId": "",
    "last_visited": "2025-03-18T23:13:55.508Z",
    "last_read": "2025-03-18T23:13:55.508Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.11718": {
    "id": "arxiv.2503.11718",
    "title": "The Relativity of Causal Knowledge",
    "authors": "D'Acunto, Gabriele, Battiloro, Claudio",
    "abstract": "Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.",
    "url": "https://arxiv.org/abs/2503.11718",
    "arxivId": "",
    "last_visited": "2025-03-18T23:17:21.783Z",
    "last_read": "2025-03-18T23:17:21.783Z",
    "total_reading_time_seconds": 50,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.13045": {
    "id": "arxiv.2503.13045",
    "title": "All You Need to Know About Training Image Retrieval Models",
    "authors": "Berton, Gabriele, Musgrave and 3 others",
    "abstract": "Image retrieval is the task of finding images in a database that are most similar to a given query image. The performance of an image retrieval pipeline depends on many training-time factors, including the embedding model architecture, loss function, data sampler, mining function, learning rate(s), and batch size. In this work, we run tens of thousands of training runs to understand the effect each of these factors has on retrieval accuracy. We also discover best practices that hold across multiple datasets. The code is available at https://github.com/gmberton/image-retrieval",
    "url": "https://arxiv.org/abs/2503.13045",
    "arxivId": "",
    "last_visited": "2025-03-18T23:31:55.427Z",
    "last_read": "2025-03-18T23:31:55.427Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.09573": {
    "id": "arxiv.2503.09573",
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "authors": "Arriola, Marianne, Gokaslan and 13 others",
    "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",
    "url": "https://arxiv.org/abs/2503.09573",
    "arxivId": "",
    "last_visited": "2025-03-18T23:55:44.187Z",
    "last_read": "2025-03-18T23:55:44.187Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2409.17088": {
    "id": "arxiv.2409.17088",
    "title": "Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing",
    "authors": "Masson, Damien, Kim and 3 others",
    "abstract": "We explore how interactions inspired by drawing software can help edit text. Making an analogy between visual and text editing, we consider words as pixels, sentences as regions, and tones as colours. For instance, direct manipulations move, shorten, expand, and reorder text; tools change number, tense, and grammar; colours map to tones explored along three dimensions in a tone picker; and layers help organize and version text. This analogy also leads to new workflows, such as boolean operations on text fragments to construct more elaborated text. A study shows participants were more successful at editing text and preferred using the proposed interface over existing solutions. Broadly, our work highlights the potential of interaction analogies to rethink existing workflows, while capitalizing on familiar features.",
    "url": "https://arxiv.org/abs/2409.17088",
    "arxivId": "",
    "last_visited": "2025-03-18T23:55:20.986Z",
    "last_read": "2025-03-18T23:55:20.986Z",
    "total_reading_time_seconds": 75,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2410.05669": {
    "id": "arxiv.2410.05669",
    "title": "ACPBench: Reasoning about Action, Change, and Planning",
    "authors": "Kokel, Harsha, Katz and 5 others",
    "abstract": "There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multi-step reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning models highlights the significant gap in the reasoning capability of the LLMs. Our findings with OpenAI o1, a multi-turn reasoning model, reveal significant gains in performance on multiple-choice questions, yet surprisingly, no notable progress is made on boolean questions. The ACPBench collection is available at https://ibm.github.io/ACPBench.",
    "url": "https://arxiv.org/abs/2410.05669",
    "arxivId": "",
    "last_visited": "2025-03-19T00:27:25.482Z",
    "last_read": "2025-03-19T00:27:25.482Z",
    "total_reading_time_seconds": 10,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.13630": {
    "id": "arxiv.2503.13630",
    "title": "The Birth of Quantum Mechanics: A Historical Study Through the Canonical Papers",
    "authors": "Küçük, Eren Volkan",
    "abstract": "This paper explores the historical development of the theory of quantum mechanics between 1900 and 1927 by chronological examination of the foundational papers and ideas. Beginning with Planck's introduction of energy quantisation in blackbody radiation, we follow the emergence of Einstein's light quanta hypothesis, Bohr's atomic model, and the statistical implications of indistinguishable particles. Special emphasis is placed on the transition from the Old Quantum Theory to modern quantum mechanics, particularly through Heisenberg's matrix mechanics and Schr\\\"{o}dinger's wave mechanics. This study aims to provide a structured historical account, offering insights into the conceptual transformations that led to quantum mechanics while making the development accessible to physicists, historians of science, and advanced students interested in the origins of modern quantum theory.",
    "url": "https://arxiv.org/abs/2503.13630",
    "arxivId": "",
    "last_visited": "2025-03-19T03:42:17.894Z",
    "last_read": "2025-03-19T03:42:17.894Z",
    "total_reading_time_seconds": 195,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "url.2278B45": {
    "id": "url.2278B45",
    "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?",
    "authors": "Egor Zverev, Sahar Abdelnabi, Soroush Tabesh and 2 others",
    "abstract": "Large Language Models (LLMs) show impressive results in numerous practical applications, but they lack essential safety features that are common in other areas of computer science, particularly an explicit separation of instructions and data. This makes them vulnerable to manipulations such as indirect prompt injections and generally unsuitable for safety-critical tasks. Surprisingly, there is currently no established definition or benchmark to quantify this phenomenon. In this work, we close this gap by introducing a formal measure for instruction-data separation for single-turn language models and an empirical variant that is calculable from a model’s outputs. We also present a new dataset, SEP, that allows estimating the measure for real-world models. Our results on various LLMs show that the problem of instruction-data separation is real: all models fail to achieve high separation, and canonical mitigation techniques, such as prompt engineering and fine-tuning, either fail to substantially improve separation or reduce model utility.",
    "url": "https://openreview.net/forum?id=8EtSBX41mt",
    "arxivId": "",
    "last_visited": "2025-03-19T00:43:35.754Z",
    "last_read": "2025-03-19T00:43:35.754Z",
    "total_reading_time_seconds": 5,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2502.15680": {
    "id": "arxiv.2502.15680",
    "title": "Privacy Ripple Effects from Adding or Removing Personal Information in Language Model Training",
    "authors": "Borkar, Jaydeep, Jagielski and 9 others",
    "abstract": "Due to the sensitive nature of personally identifiable information (PII), its owners may have the authority to control its inclusion or request its removal from large-language model (LLM) training. Beyond this, PII may be added or removed from training datasets due to evolving dataset curation techniques, because they were newly scraped for retraining, or because they were included in a new downstream fine-tuning stage. We find that the amount and ease of PII memorization is a dynamic property of a model that evolves throughout training pipelines and depends on commonly altered design choices. We characterize three such novel phenomena: (1) similar-appearing PII seen later in training can elicit memorization of earlier-seen sequences in what we call assisted memorization, and this is a significant factor (in our settings, up to 1/3); (2) adding PII can increase memorization of other PII significantly (in our settings, as much as $\\approx\\!7.5\\times$); and (3) removing PII can lead to other PII being memorized. Model creators should consider these first- and second-order privacy risks when training models to avoid the risk of new PII regurgitation.",
    "url": "https://arxiv.org/abs/2502.15680",
    "arxivId": "",
    "last_visited": "2025-03-19T04:37:20.155Z",
    "last_read": "2025-03-19T04:37:20.155Z",
    "total_reading_time_seconds": 180,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.12743": {
    "id": "arxiv.2503.12743",
    "title": "Cancermorphic Computing Toward Multilevel Machine Intelligence",
    "authors": "Moreddu, Rosalia, Levin, Michael",
    "abstract": "Despite their potential to address crucial bottlenecks in computing architectures and contribute to the pool of biological inspiration for engineering, pathological biological mechanisms remain absent from computational theory. We hereby introduce the concept of cancer-inspired computing as a paradigm drawing from the adaptive, resilient, and evolutionary strategies of cancer, for designing computational systems capable of thriving in dynamic, adversarial or resource-constrained environments. Unlike known bioinspired approaches (e.g., evolutionary and neuromorphic architectures), cancer-inspired computing looks at emulating the uniqueness of cancer cells survival tactics, such as somatic mutation, metastasis, angiogenesis and immune evasion, as parallels to desirable features in computing architectures, for example decentralized propagation and resource optimization, to impact areas like fault tolerance and cybersecurity. While the chaotic growth of cancer is currently viewed as uncontrollable in biology, randomness-based algorithms are already being successfully demonstrated in enhancing the capabilities of other computing architectures, for example chaos computing integration. This vision focuses on the concepts of multilevel intelligence and context-driven mutation, and their potential to simultaneously overcome plasticity-limited neuromorphic approaches and the randomness of chaotic approaches. The introduction of this concept aims to generate interdisciplinary discussion to explore the potential of cancer-inspired mechanisms toward powerful and resilient artificial systems.",
    "url": "https://arxiv.org/abs/2503.12743",
    "arxivId": "",
    "last_visited": "2025-03-19T05:46:45.736Z",
    "last_read": "2025-03-19T05:46:45.736Z",
    "total_reading_time_seconds": 35,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.14456": {
    "id": "arxiv.2503.14456",
    "title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "authors": "Peng, Bo, Zhang and 27 others",
    "abstract": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to $\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset. To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License.",
    "url": "https://arxiv.org/abs/2503.14456",
    "arxivId": "",
    "last_visited": "2025-03-19T05:59:22.018Z",
    "last_read": "2025-03-19T05:59:22.018Z",
    "total_reading_time_seconds": 90,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  },
  "arxiv.2503.14378": {
    "id": "arxiv.2503.14378",
    "title": "Impossible Videos",
    "authors": "Bai, Zechen, Ci and 3 others",
    "abstract": "Synthetic videos nowadays is widely used to complement data scarcity and diversity of real-world videos. Current synthetic datasets primarily replicate real-world scenarios, leaving impossible, counterfactual and anti-reality video concepts underexplored. This work aims to answer two questions: 1) Can today's video generation models effectively follow prompts to create impossible video content? 2) Are today's video understanding models good enough for understanding impossible videos? To this end, we introduce IPV-Bench, a novel benchmark designed to evaluate and foster progress in video understanding and generation. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing 4 domains, 14 categories. It features diverse scenes that defy physical, biological, geographical, or social laws. Based on the taxonomy, a prompt suite is constructed to evaluate video generation models, challenging their prompt following and creativity capabilities. In addition, a video benchmark is curated to assess Video-LLMs on their ability of understanding impossible videos, which particularly requires reasoning on temporal dynamics and world knowledge. Comprehensive evaluations reveal limitations and insights for future directions of video models, paving the way for next-generation video models.",
    "url": "https://arxiv.org/abs/2503.14378",
    "arxivId": "",
    "last_visited": "2025-03-19T14:11:55.751Z",
    "last_read": "2025-03-19T14:11:55.751Z",
    "total_reading_time_seconds": 0,
    "published_date": "",
    "arxiv_tags": [],
    "features_path": null
  }
}