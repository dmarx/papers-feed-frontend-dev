{
  "arxivId": "2405.06865",
  "title": "Disrupting Style Mimicry Attacks on Video Imagery",
  "authors": "Josephine Passananti, Stanley Wu, Shawn Shan, Haitao Zheng, Ben Y. Zhao",
  "abstract": "Generative AI models are often used to perform mimicry attacks, where a\npretrained model is fine-tuned on a small sample of images to learn to mimic a\nspecific artist of interest. While researchers have introduced multiple\nanti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence\npoints to a growing trend of mimicry models using videos as sources of training\ndata. This paper presents our experiences exploring techniques to disrupt style\nmimicry on video imagery. We first validate that mimicry attacks can succeed by\ntraining on individual frames extracted from videos. We show that while\nanti-mimicry tools can offer protection when applied to individual frames, this\napproach is vulnerable to an adaptive countermeasure that removes protection by\nexploiting randomness in optimization results of consecutive (nearly-identical)\nframes. We develop a new, tool-agnostic framework that segments videos into\nshort scenes based on frame-level similarity, and use a per-scene optimization\nbaseline to remove inter-frame randomization while reducing computational cost.\nWe show via both image level metrics and an end-to-end user study that the\nresulting protection restores protection against mimicry (including the\ncountermeasure). Finally, we develop another adaptive countermeasure and find\nthat it falls short against our framework.",
  "url": "https://arxiv.org/abs/2405.06865",
  "issue_number": 153,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/153",
  "created_at": "2025-01-05T08:24:17.506837",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-22T16:37:12.908Z",
  "main_tex_file": null,
  "published_date": "2024-05-11T01:40:19Z",
  "arxiv_tags": [
    "cs.CV",
    "cs.CR"
  ]
}