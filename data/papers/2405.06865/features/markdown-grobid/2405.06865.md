# Disrupting Style Mimicry Attacks on Video Imagery

## Abstract

## 

Generative AI models are often used to perform mimicry attacks, where a pretrained model is fine-tuned on a small sample of images to learn to mimic a specific artist of interest. While researchers have introduced multiple anti-mimicry protection tools (Mist, Glaze, Anti-Dreambooth), recent evidence points to a growing trend of mimicry models using videos as sources of training data.

This paper presents our experiences exploring techniques to disrupt style mimicry on video imagery. We first validate that mimicry attacks can succeed by training on individual frames extracted from videos. We show that while anti-mimicry tools can offer protection when applied to individual frames, this approach is vulnerable to an adaptive countermeasure that removes protection by exploiting randomness in optimization results of consecutive (nearly-identical) frames. We develop a new, tool-agnostic framework that segments videos into short scenes based on frame-level similarity, and use a per-scene optimization baseline to remove inter-frame randomization while reducing computational cost. We show via both image level metrics and an end-to-end user study that the resulting protection restores protection against mimicry (including the countermeasure). Finally, we develop another adaptive countermeasure and find that it falls short against our framework.

## INTRODUCTION

While many debate the ethical and legal issues around the training of generative image models, all agree that their arrival has dramatically disrupted a range of visual art industries, from fine art to illustrations, concept art and graphics arts. Recent works has studied the harms experienced by professional artists due to these large-scale image generators, including reputational damage, economic loss, plagiarism and copyright infringement [[4,](#b3)[24]](#b22). One of the more harmful uses of these image models is "style mimicry," where someone "finetunes" a model on small samples of a specific artist's art, then uses the result to produce images in the artist's individual style without their knowledge [[2,](#b1)[5,](#b4)[29,](#b27)[51]](#b47). These mimicry models (usually lightweight models known as LORAs) are hosted on sites including Civitai, Tensor.art, PromptHero and HuggingFace.

Recently, the security and machine learning communities have developed a number of tools to disrupt unauthorized style mimicry, including Glaze [[43]](#b39), Mist [[27]](#b25), and Anti-Dreambooth [[49]](#b45). These tools disrupt the mimicry process, by modifying images so that they misrepresent themselves in a target model's style feature space during finetuning, while constraining changes to minimize visual impact to human eyes. Since their introduction, they have been adopted widely by artists across the globe, e.g. Glaze reports more than 2.3 million downloads in a year [[20]](#b18).

There are signs, however, that mimicry attacks are shifting away from 2D art images and towards video content (see Figure [1](#fig_0)). Online videos such as animations, game cut-scenes, music videos and TV shows provide attractive sources for training mimicry models for several reasons. First, a single video can provide thousands of frames, each convertible to a standalone image for training. For example, YouTube videos range from 30 to 60 frames per second, and even a short 5 minute video can yield 18,000 frames for potential training. Second, extracting frames from videos provides far more flexibility to choose a specific scene, character or perspective. This offers far richer training content compared to static images like movie posters or promotional art. In fact, many new LORAs already target video games (Riot's League of Legends[foot_0](#foot_0) and Valorant[foot_1](#foot_1) , LucasArts Games[foot_2](#foot_2) , Dead Or Alive [4](#foot_3) ), TV shows (The Flash[foot_4](#foot_4) , Rick & Morty [6](#foot_5) ), and movies (Hunger Games [7](#foot_6) , Jumanji [8](#foot_7) , Disney Pixar [9](#foot_8) ).

The natural question arises: what can we do to protect video creators from style mimicry attacks? This paper presents results from our efforts to answer this question, and to disrupt style mimicry attacks on video imagery. We begin by first validating the threat. Through empirical experiments on a variety of short videos, we confirm that it is possible to produce consistent, high quality mimicry models by extracting and training on frames from videos. Next, we consider the feasibility of applying existing tools like Glaze/Mist/Anti-DB to videos on a frame by frame basis. This naive approach, while computationally expensive, does indeed protect against extracting and training on video frames.

The problem, however, is that in the video domain, a naive application of anti-mimicry tools is vulnerable to an adaptive countermeasure. Because anti-mimicry tools are designed to operate on single images, they compute protection filters on each single image independently. Randomized components of these algorithms produce different optimization results on multiple runs of the same image. For medium to high frame rate videos, this means a clever attacker can take a protected video, extract consecutive frames whose originals are nearly identical, use the protected frames to identify alterations made by protection tools at the pixel level, and remove them to extract the original frames. We explore multiple versions of this adaptive attack, and show that they can effectively bypass all 3 anti-mimicry tools and produce mimicry models similar in quality to those trained from unprotected videos. Next, we develop Gimbal, an anti-mimicry framework for videos to resist this countermeasure and restore robustness of anti-mimicry tools. At a high level, we recognize commonalities across all 3 anti-mimicry tools, and extend these tools to include the notion of sequential similar video frames. Instead of computing a costly independent protection per frame, we first analyze video frames to identify scenes or frame sequences with low pixel differential across frames. The protection tool computes a single baseline perturbation for all frames in the same scene, and then performs additional local optimization on a per frame basis. The result is each new frame's protection pixels is an extension from the prior frame, removing unnecessary randomization used by the countermeasure. This new approach has the added benefit of greatly reducing time required to generate protection, often by an order of magnitude.

We evaluate the efficacy and robustness of Gimbal on a variety of videos. We validate that it integrates naturally with all 3 anti-mimicry tools. We adapt Gimbal to each tool and evaluate the robustness against video mimicry attacks using multiple imagelevel metrics, including latent ùêø 2 norm between frames, intra-frame mean pixel difference, and CLIP-based genre shift. Most importantly, we perform a user study and ask participants to evaluate if the prototype is able to provide sufficient protection against video mimicry attacks (including the anti-mimicry countermeasure). Responses from over 500 participants confirm that not only does our robust anti-mimicry system works as intended against mimicry models, but its protection is actually more visually appealing than naive Glaze (less flickering due to randomization across frames).

Finally, we identify an advanced countermeasure against our frame-aggregation framework, where an attacker can force us to break a sequence of similar frames into multiple scenes. We show that even in these scenarios, frame aggregation prevents an attacker from extracting unprotected frames.

In summary, our paper makes the following contributions:

‚Ä¢ We demonstrate that attackers can successfully mimic visual styles in videos by extracting and training on individual frames. ‚Ä¢ We identify and validate the efficacy of an adaptive countermeasure that exploits randomization from per-image optimizations to remove image protection and enable successful mimicry. ‚Ä¢ We propose a general anti-mimicry framework for videos that aggregates scenes of similar frames into the protection process, removing unnecessary inter-frame randomization, reducing visual artifacts and greatly reducing computation costs. ‚Ä¢ We validate the efficacy of our protection against mimicry attacks (including the adaptive countermeasure) using a variety of image metrics and two user studies of combined 525 participants.

‚Ä¢ We propose another adaptive countermeasure on our framework, and show that it failures to extract unprotected frames for mimicry.

Our work presents an important first step towards protecting video content against visual style mimicry, by identifying and mitigating a video-specific countermeasure to anti-mimicry tools. A number of challenges remain, and more effort is needed to identify other adaptive mimicry algorithms, particularly for longer videos.

## BACKGROUND AND RELATED WORK

We begin by providing background on style mimicry and existing image-based protection methods. Then we follow with an overview of publicly available tools that enable style mimicry attacks on the video domain.

## Style Mimicry and Existing Defenses

In a style mimicry attack, a bad actor finetunes a text-to-image model to generate art in a particular artist's style without their consent. Since the introduction of text-to-image diffusion [[31,](#b29)[34,](#b31)[36,](#b32)[45,](#b41)[46]](#b42) models in 2022, style mimicry has grown significantly. There have been multiple high-profile mimicry incidents involving human artists [[2,](#b1)[5,](#b4)[29,](#b27)[51]](#b47), and new companies are founded that focus purely on style mimicry [[25,](#b23)[40]](#b36). AI marketplaces have also recently gained traction, with websites like CivitAI [[12]](#b11) offering over 119K ready-to-use mimicry models for people to download and use. Image-based style mimicry. Style mimicry relies on finetuning pretrained text-to-image models (e.g., stable diffusion) on a small set of images from a specific style [[16,](#b14)[17,](#b15)[38]](#b34). The quality of these images greatly impacts the mimicry result, and thus, attackers often scrape high quality images from artists' websites and online galleries [[5,](#b4)[51]](#b47). In practice, a bad actor does not need many (less than 20 images [[17,](#b15)[43]](#b39)) in order to successfully generate arbitrary artwork from a victim artist's style. Because of the risk of imagebased mimicry, many artists choose to reduce the amount of art they post online [[15]](#b13), reduce the quality of any posted art [[21]](#b19), and apply protection (discussed in details below) on this artwork [[43]](#b39). Protecting images from style mimicry. Existing work (Mist [[27]](#b25), Anti-Dreambooth [[49]](#b45), and Glaze [[43]](#b39)) has proposed methods that leverage clean-label poisoning [[39,](#b35)[47,](#b43)[53]](#b49) to prevent style mimicry. At a high level, these systems add small optimized perturbations to image artwork that modifies the perturbed image's feature space representation without altering its content. The altered feature space representation prevents models from learning the correct artistic style. In general, these protection tools calculate the perturbation ùõø ùë• for an image ùë• using the following objective: min

$ùõø ùë• ùê∑ùëñùë†ùë° (Œ¶(ùë• + ùõø ùë• ), Œ¶(ùëá )) ,(1)$$subject to |ùõø ùë• | < ùëù,$where Œ¶ is a generic image feature extractor from a public textto-image model, ùê∑ùëñùë†ùë° (.) computes the distance between two feature representations, |ùõø ùë• | measures the perceptual perturbation caused by protection, and ùëù is the perceptual perturbation budget. ùëá is a "target image" that the perturbation ùõø ùë• is optimized towards, such that ùë• + ùõø ùë• resembles ùëá in feature space while being visually identical to ùë•. Mist [[27]](#b25) extends the optimization objective across the entire diffusion process, including gradient computations through the randomized diffusion denoising process. By default, Mist uses a predefined black and white patterned image as its target with the goal of producing chaotic patterns in generated images. Anti-DB (Anti-Dreambooth) [[49]](#b45) is most similar to Mist, but modifies the optimization objective to specifically target Dreambooth [[38]](#b34) textto-image models. There, they find that training surrogate models alongside computing image perturbations results in stronger protection, though it incurs additional computation time. Glaze [[43]](#b39) introduces input-specific target images by performing style transfer on the input image using a contrasting artistic style. This method preserves the overall content of the input image, while changing mainly the style, which the authors argue leads to more robust protection. Glaze then attacks the image encoder of a diffusion model as detailed above.

These protection tools have been positively received by the artist community, with Glaze having been downloaded at least 2.3 million times [[20]](#b18). While these systems are typically too computationally expensive for artists, efforts have been made to improve accessibility [[35,](#)[50]](#b46). Since these systems are free and increasingly available, images may no longer be a viable data source for attackers to access artwork for fine-tuning text-to-image models.

Video protection, on the other hand, has yet to be explored. Computation time per image is already limiting for many artists, and applying the same algorithms to all frames would be many times more costly. Yet, videos represent a significant source of data, incentivizing attackers to explore publicly available video art, such as short animation, movies or video game trailers etc.. Until recently, most style mimicry models are trained on still images. This is no longer the case today because 1) artists are increasingly more reluctant to post their work on the Internet [[15]](#b13), 2) existing defenses ( ¬ß2.1) are effective at protecting still images against mimicry, 3) video frames offer a significantly more diverse range of images compared to still images. Video content is a promising source for mimicry.

Video content (e.g., game trailers, anime, short videos, documentary, ads) provides promising alternative data sources for two reasons. First, video contents often offer a more diverse (3D) shots of an object or style, e.g., rotating shot of an object, panning across a scene. These diverse viewpoints enable models to better learn the content during the training process [[48]](#b44). Second, there are significantly more video frames compared to still images and many of the videos contain unique art styles/characters. The entire Internet produces around 3.2 billions still images daily [[1]](#b0), while YouTube alone sees over 271,000 hours of videos (i.e., around 29 billions video frames) uploaded per day. Specifically, gaming companies and animation studios often use short videos as a way to promote new games, characters, and movies. Movie clip compilations and trailers are readily available on YouTube [[11]](#b10), while video game companies like Riot and Mihoyo frequently post teasers and trailers showcasing new playable content, or highly anticipated characters [[3,](#b2)[19]](#b17). These videos are filled with original artwork, and contain image frames that are prime targets for style mimicry. Video-based mimicry in the real-world. Style mimicry using video content has already occurred in the real-world. Bad actors have created and distributed software that generates high quality text-to-image datasets from online videos. One GitHub tool [[14]](#) automates the process of downloading (e.g., torrenting) Japanese Anime episodes and extracting high quality frames of desired characters. Another option [[28]](#b26) advertised on CivitAI does the same, with the additional capability of scraping frames from screencap websites such as FanCaps [[6]](#b5). These tools demonstrate that there already exists sophisticated technology aimed at creating text-toimage datasets from original video content.

We also provide our own examples of this threat. We download and extract high quality frames from YouTube videos and train style mimicry models on them (Figure [1](#fig_0)). Figure [2](#fig_1) shows some examples of extracted video frames as well as mimicry results generated by the style mimicry model. We include human evaluation of the success of these style mimicry images later using user studies with both artists and the general public in ¬ß6.

While there has been recent developments in text-to-video [[10]](#b9) and image-to-video models [[8]](#b7), we leave them as a topic for future work, and focus solely on text-to-image mimicry where the source of data originates from video content.

## STYLE MIMICRY ATTACKS ON EXTRACTED VIDEO FRAMES

Our work considers a previously overlooked variant of the style mimicry attack, where an attacker extracts individual frames from a video to build image-based style mimicry models. Next, we introduce the threat model and consider the limitations of a baseline defense that applies existing image-based protection tools to individual video frames.

## Threat Model

Artist/Video creator. Artists want to share their video content online while disallowing unauthorized mimicry using these video frames. Artists seeks to protect their video by applying small pixel perturbations on the video frames. Following the assumptions made by existing defenses against style mimicry [[43]](#b39), we assume the artists:

‚Ä¢ have access to moderate computing resources (e.g., consumergrade GPUs) commonly used for video rendering; ‚Ä¢ add perturbation to video frames before posting videos online;

‚Ä¢ have access to some public feature extractor (e.g., open-source models such as Stable Diffusion).

Attacker. The attacker's goal is to build a text-to-image model that is able to generate images in the style of the victim artist. We assume the attacker ‚Ä¢ has access to videos from the victim and leverages frames from these videos for mimicry; ‚Ä¢ has significant computational power;

‚Ä¢ has full access to pretrained, benign text-to-image base models.

Note that our work focuses on text-to-image mimicry, where the attacker's goal is to generate images. We leave text-to-video mimicry using video contents to future work.

## Style Mimicry Leveraging Video Frames 3.3 A Naive Defense and Its Limitations

Given the existence of existing tools designed to disrupt art style mimicry [[27,](#b25)[43,](#b39)[49]](#b45), a straightforward solution to protect video imagery is to simply apply existing protection to each and every frame of a video. As discussed in ¬ß2, these defenses add highlyoptimized perturbations on each image (a video frame in our case), misleading the mimicry model to perceive each protected frame as an image with a completely different style.

Unfortunately, this "naive" application of anti-mimicry tools in the video context has multiple drawbacks, the most critical of which is vulnerability to a temporal-based adaptive countermeasure. Vulnerability to countermeasures based on temporal similarity. When applied on individual video frames without coordination, existing protection methods become vulnerable to advanced countermeasures that exploit visual similarity across consecutive video frames.

Existing anti-mimicry tools treat each input image independently, with a randomized component in their generation of perturbation targets for each image. That means even two runs on the same input image will likely output two different set of pixel changes. With this in mind, an adaptive mimicry attack could take several consecutive frames, whose original pixel values are highly similar, and use them against each other to try to cancel out the pixel changes made by the protection tools. An"averaged" or "smoothed" frame generated this way would have much weaker residue protective perturbations, and would provide a good estimate of the actual visual feature (i.e. style) carried by the original (unperturbed) video frames. An attacker can then use these frames to train a mimicry model. In ¬ß4, we provide a detailed study to validate and quantify this significant vulnerability. Other limitations: computation and video quality degradation. The naive application of protection tools leads to two other challenges. First, computing independent protection filters on each video frame is computationally very expensive. Existing protection tools (Glaze [[43]](#b39), Mist [[27]](#b25) and Anti-DB [[49]](#b45)) can takes up to ùúè = 1.5 minutes to protect a single frame for moderate GPUs. Protecting a 1 minute video at 30fps (1800 frames) would take 45 hours. Second, the protective pixel level perturbations are computed independently per frame and hard to detect on a still image. But when played in a video sequence, these perturbations cause noticeable flickering effects that degrade the video quality and viewer experience.

## AN ADAPTIVE MIMICRY ATTACK

In this section, we design, implement and evaluate an adaptive mimicry attack designed to bypass the naive protection method described in ¬ß3.3.

It is made possible by the fundamental temporal consistency inherent to all video content. Here, we propose Perturbation Removal Attacks (PRA) that leverage temporal consistency to remove protective perturbations on video frames, and present results measuring their efficacy using both automated metrics and human feedback.

## Perturbation Removal

For naively protected video sequences, we develop an adaptive mimicry attack that uses perturbation removal methods (PRM) to recover images that closely approximate the original, unperturbed video frames. These are then used to successfully train an imagebased style mimicry model. As such, for any video sequence, a PRMs behaves like a frame extraction tool. "Combining" consecutive frames. PRMs remove protective perturbations by combining multiple consecutive video frames (that share high visual similarity) into a single frame. Intuitively, combining highly similar frames will generally preserve the common pixel values inherited from the original unprotected frames, while reducing or removing the pixel value changes made by protective tools. With this in mind, we consider three PRM approaches that employ different "combining" functions across video frames.

‚Ä¢ Selective Pixel Averaging -This approach generates a combined image out of consecutive frames, where each pixel is the average of the corresponding pixel values across the set of frames. This pixel-level "averaging" function "smooths" out the protective perturbations. Pixel averaging can be limited to more static regions and avoid pixels that capture motion across the frames. ‚Ä¢ FILM -Frame Interpolation for Large Motion (FILM) [[37]](#b33) is a neural network designed to generate temporally smooth videos from disjoint frames. We apply FILM to multiple perturbed frames from the same scene to reconstruct high quality images that closely resemble the original unperturbed frames, but don't retain perturbations from either input. ‚Ä¢ Linear Interpolation -Linear interpolation is a technique for generating intermediate data points between a set of known points. Like the other approaches, pixel-level linear interpolation across frames exploits lack of consistency between consecutive perturbations.

Implementing adaptive attacks. We implemented 3 adaptive mimicry attacks, each using one of the frame-aggregation approaches described above. In the rest of this section, we present detailed results on all 3 adaptive attacks. The key takeaway is that pixel averaging significantly outperforms the alternatives. For  Pixel Averaging approximates the original unprotected frame by averaging pixels across highly similar consecutive frames (see Figure [3](#fig_3)). Pixel level differences between consecutive frames come from two sources: 1) natural changes between video frames which we call movement, and 2) differences in pixel changes added by protective tools (perturbations). Protective tools seek to minimize visual impact, so the large majority of perturbation values are constrained within a specific value. Thus an attacker can examine two consecutive perturbed frames, and identify the source of each pixel difference by filtering using a simple threshold (ùúñ ùëù ). A well chosen ùúñ ùëù will separate pixel differences due to movement (> ùúñ ùëù ) from pixel differences from protective tools (< ùúñ ùëù ). The attacker measures pixel differences (Œî ùëù ) between consecutive frames and only averages regions (0 < Œî ùëù < ùúñ ùëù ). In practice, ùúñ ùëù can easily be identified empirically as a transition point between two levels of region sizes. In our tests, we experimentally test pixel averaging across ùëõ consecutive frames, and find the best results around ùëõ = 5. We measure the quality of frames using CLIP Aesthetic predictor [[42]](#b38) and perturbation removal using metrics described in the following section. We show detailed results of the tradeoff between perturbation removal vs. image quality in the appendix.

## Experimental Setup and Metrics

To validate the efficacy of multiple perturbation removal methods, we add naive protection to short scenes on an independent, perframe basis. We then test the adaptive mimicry attack by using each PRM to extract unprotected frames from each video scenes. We compare different PRMs by measure the amount of image level differences in the perturbations before vs. after our attack using several automated metrics. We also measure end-to-end success of the adaptive mimicry attack by training models on extracted frames, and conducting a user study to gather human feedback. Applying naive protection to all frames. We experiment on 5 diverse datasets containing realistic videos of scenery and human actions, artistic style videos, and video game style videos. We implement each of 3 protection tools, Mist, Anti-DB, and Glaze.

For each scene, we identify and extract scenes of highly similar frames, and apply "naive protection" by applying each of 3 protection tools to each frame in selected frames. We apply each PRM to the naively protected images to attempt to recover a good estimation of the original images. We then compare original, naively protected, and attacked naively protected images to each other. As we show below, pixel-averaging significantly outperforms FILM and linear interpolation in pixel level metrics.

Performing style mimicry. We perform style mimicry attacks under 3 "perturbation scenarios": training mimicry models on clean (unperturbed) frames, frames protected by "naive protection, " and frames extracted by adaptive attack following "naive protection." Due to high computation costs (multiple days per scene), we only compute end-end results for the Pixel Averaging attack (shown to be strongest in pixel level metrics above). For each perturbation scenario, we chose 30 scenes from a video, select (or extract) one image from each scene, and train mimicry models on this set. Further details on mimicry attack configurations are in ¬ß6. Evaluation metrics. We evaluate the strength of perturbation removal methods using pixel level metrics (Mean Pixel Difference and Latent ùêø 2 Norm). We evaluate end-to-end results on style mimicry using human feedback (User Study). We briefly describe these metrics below, and give more details later in ¬ß6.

‚Ä¢ Latent ùêø 2 Norm. We employ the image encoder used in diffusion models to calculate image representations of perturbed and nonperturbed (original) frames, and then calculate the ùêø 2 distance between them as a measure of proximity between two images. Thus, a successful perturbation removal would minimize the latent ùêø 2 norm, while a robust system should maintain a high latent ùêø 2 norm. ‚Ä¢ Mean Pixel-Difference. We measure differences between images at a pixel level, motivated by the ùëô ùëñùëõùëì bounded pixel changes that all protection tools (Mist, Anti-DB, Glaze) use to limit visual artifacts. We define Mean Pixel Difference (MPD) as the average of all pixel differences between a perturbed image and clean image. Similar to the latent ùêø 2 norm, a higher MPD signals higher protection. ‚Ä¢ Human feedback. We perform a user study to evaluate the success of adaptive mimicry attacks, by asking participants to look at images produced by a mimicry model, and compare it to original video frames. We ask participants to rate the success on a 5-level Likert scale (ranging from "not successful at all" to "very successful"). Following existing work, we define protection success rate (PSR) as the percent of participants who rated the style mimicry as "not very successful" or "Not successful at all. "

## Adaptive Mimicry Results

Next we present results of our experiments on the adaptive attack.

## Naive Protection + adaptive attack

Original frames

Naive Protection

## Mimicry Images Trained on:

A) B)

Figure [4](#): Visual examples of adaptive mimicry attack. Three rows of mimicry images generated by models trained on 1) original video frames, 2) video frames protected naively, and 3) video frames recovered after perturbation removal using pixel averaging.

Protection Tool Protected Pixel Avg FILM Interpolation Linear Interpolation Glaze 111.30 ¬± 12.94 90.04 ¬± 14.13 97.83 ¬± 13.73 96.16 ¬± 14.47 Mist 121.25 ¬± 9.48 107.10 ¬± 16.33 112.99 ¬± 14.36 113.03 ¬± 15.46 Anti-DB 124.86 ¬± 8.93 103.75 ¬± 17.70 110.29 ¬± 15.52 110.39 ¬± 16.42

Table 2: Mean Pixel Difference between original frames, protected frames, protected frames after perturbation removal.

Similarity of recovered frames to original frames. We compare latent ùêø 2 norm between the original frames, the protected frames, and protected frames after protection removal. Table [1](#tab_0) shows FILM to have minimal impact, and that Pixel averaging does the best to minimize loss for the recovered frame, suggesting that it is closer to the original frame in the feature space. We also compare mean pixel differences between the original frames, protected frames, and protected frames after protection removal. Table [2](#) shows that again, pixel averaging method outperforms against all protection tools, and minimizes the pixel differences between the extracted frame and the original. Style mimicry attack on recovered images. Finally, we use a user study to evaluate the end-to-end success of the adaptive mimicry attack using pixel-averaging to overcome a per-frame application of protection tools. Table [3](#) shows that users agree, the adaptive mimicry attack with pixel averaging basically produces mimicry results similar to mimicry on original unprotected video frames (PSR baseline value of 17.65 for unprotected video frames). Figure [4](#) shows samples of mimicry images from models trained on original frames, protected frames, and protected frames under adaptive attack. Clearly the adaptive attack is able to bypass protection and restore mimicry success.

These results validate our concerns, that a naive, frame by frame application of protection tools to videos is insufficient to prevent image mimicry. We need to extend these anti-mimicry tools to restore their protection in the video domain.

## PROTECTING VIDEO IMAGERY WITH GIMBAL

We have identified that video imagery is susceptible to style mimicry attacks despite existing protection in image space ( ¬ß4). Although

Protection Tool Protected Protected + Pixel Averaging Glaze 70.59 ¬± 1.05 23.76 ¬± 1.14 Mist 62.90 ¬± 1.11 25.34 ¬± 1.14 Anti-DB 59.28 ¬± 1.18 22.62 ¬± 1.15

Table 3: Human feedback (Protection Success Rate) shows perturbation removal can significantly reduce effects of protection tools against style mimicry attacks. Note baseline PSR for original, unprotected frames is 17.65 ¬± 1.10.

current protection tools are effective for 2D art, they are no longer robust to adaptive adversaries in the video domain. In this section, we develop Gimbal, a framework that extends image-based protection tools to the video domain, resulting in improved robustness against adaptive adversaries as well as lower computation costs and improved video quality for protected videos.

## Design intuition

Challenges of existing perturbation systems. Existing perturbation systems do not take into account the threat of an adversary gaining access to highly similar or even identical frames that are protected with completely independent perturbations. These systems independently optimize separate protection perturbations for each frame. This optimization process includes 1) generating a random target latent tensor, and 2) perturbing the original image by optimizing the image towards the selected target. The choice of target significantly impacts the pixel level perturbations on an image. Current systems choose targets by generating a latent representation of the original image, introducing noise, and then employing denoising autoencoders. This randomness (i.e., non-linearity) is desired for image-based protection, making it more challenging to reverse engineer or remove the perturbations. However, it also leads to each perturbation mask being entirely unique; disregarding the temporal redundancy present in the original frames. This in turn gives rise to effective countermeasures that remove the protection ( ¬ß4). Intuition. Our key intuition is that if we can create very similar perturbations on similar underlying frames, we can nullify the adversaries ability to exploit duplicity in frames. With this in mind, there are two simple options for perturbing a scene of similar frames. The first option is to re-use the same perturbation for the entire scene. Re-using the same perturbation is robust to pixel averaging attacks, but frame-specific protection weakens after small levels of movement in frames.

A second option is that if we can chose very similar targets (that guide the optimizing of perturbations) for similar frames, we can increase the consistency between perturbations. We find that reusing a single target tensor throughout a scene leads to very similar perturbations, but also makes frames less robust to style mimicry. This is because the perturbation optimization algorithm is not able to customize the embeddings of different frames to the same degree as it does for the original frame the target tensor was generated from. Thus our goal is to 1) divide videos into scenes that can share a universal target, 2) generate this "universal target" for each scene, and 3) optimize perturbations on subsequent frame to maximize protection against mimicry. 2. Generating and Optimizing Perturbation Masks Output: Protected Scenes style transfer Scene Partitioning partition frames into scenes of highly similar frames highly similar frames highly similar frames ... scene i F 1 Œ¥ 1 F 2 apply Œ¥ 1 to F 2 F i ... consecutive video frames: Œ¥ 2 apply Œ¥ i-1 to F i generate initial perturbation (Œ¥ 1 ) compute Œ¥ 1 by minimizing L 2 ((Œ¥ 1 + Œ¶(F 1 ), Œ¶( )) finetune perturbation towards F 1 F 2 F i F i-1 Œ¥ i-1 Perturbing Entire Scenes scene 1 scene 2 ... F i-1 Œ¥ i

Applying prior perturbations and optimizing on subsequent frames 

## System Design

Figure [5](#fig_5) describes our pipeline for generating robust, protected videos by partitioning them into scenes, generating a target image for each scene, and then optimizing the perturbations on each frame towards its respective target. Scene partitioning. We want to pinpoint sections of videos where using different perturbations might leave frames vulnerable to averaging attacks. We split videos into distinct scenes based on frame similarity, and considered existing scene partitioning algorithms and tools [[9]](#b8). Note that we require all frames in a scene to be similar enough to share a single "perturbation target." This is a stronger constraint than most prior definitions for a "scene," leading us to implement our own algorithm. We split scenes based on the mean pixel difference between two consecutive frames ùêπ ùëñ and ùêπ ùëñ -1 , defined by

$ùëÉùëñùë•ùëíùëô (ùêπ ùëñ , ùêπ ùëñ -1 ) ùëÅ < ùúñ ùë†ùëêùëíùëõùëí (2$$)$where ùëÅ is the number of pixels per frame and ùëÉùëñùë•ùëíùëô (.) calculates the pixel difference between two frames, and ùúñ ùë†ùëêùëíùëõùëí is a parameter for scene partition.

Generating a universal target. To create consistency between consecutive perturbations, we compute a single target image for all frames in a scene. This means that the target embedding needs to be close enough the embeddings of each frame in the scene, so that it can correctly guide each individual optimization. We test several approaches to generating the target tensor: 1) using the middle frame from the scene as base image, 2) generating image embeddings of each frame in the scene, calculating the centroid of these embeddings as base image, and 3) averaging all frames together as base image. We measure latent ùêø 2 norm distance between the embedding of each unique frame in a scene and the target tensor, and find that averaging all frames in a scene leads to a target that is consistently the closest distance across all frames (Figure [10](#fig_0) in Appendix). Specifically, we compute a style transferred target image ùëá from the averaged image, like prior work [[43]](#b39). We select a video-specific prompt to guide the style transfer, for example: Japanese Anime videos use "impressionist painting by Van Gogh" as the target style. Generating and optimizing perturbation masks. We consider several factors when computing perturbations: maximizing protection, robustness against perturbation removal attacks, and finally, reducing computation costs. We balance re-using perturbations on subsequent frames which enhances robustness against removal attacks with optimizing or recomputing perturbations which maintains high protection levels against mimicry attacks. Protection success is achieved by reducing loss between a perturbed image and target image. We develop our algorithm for perturbing a scene as follows:

‚Ä¢ Given the current scene and its corresponding ùëÄ frames {ùêπ ùëñ } ùëÄ ùëñ=1 , compute the target frame ùëá (described above).

‚Ä¢ For the first frame ùêπ 1 , compute an image-based perturbation ùõø ùëñ from scratch, optimizing towards the target frame ùëá , as defined by eq. ( [1](#formula_0)). ‚Ä¢ For each subsequent frame ùëñ (ùëñ = 2..ùëÄ), first compute

$ùëë ùëñ = |ùêø 2 (Œ¶(ùêπ ùëñ + ùõø ùëñ -1 ), Œ¶(ùëá )) -ùêø 2 (Œ¶(ùêπ ùëñ -1 + ùõø ùëñ -1 ), Œ¶(ùëá ))| (3)$where Œ¶(.) is the feature extractor used to convert an image into a latent embedding, ùêø 2 is the L2 distance between the two embeddings. Compute ùõø ùëñ , the perturbation for ùêπ ùëñ as follows:

‚Ä¢ ùëë ùëñ ‚â§ ùúè 1 : reuse the previous frame's perturbation, i.e. ùõø ùëñ = ùõø ùëñ -1 ; ‚Ä¢ ùúè 1 < ùëë ùëñ ‚â§ ùúè 2 : compute ùõø ùëñ by performing perturbation optimization towards ùëá starting from ùõø ùëñ -1 ; ‚Ä¢ ùëë ùëñ > ùúè 2 , compute ùõø ùëñ from scratch.

Here we note that because the perturbation is generated progressively using the same target image ùëá , the system is able to maintain high consistency between perturbations. Setting ùúè 1 and ùúè 2 . Our system applies two thresholds ùúè 1 and ùúè 2 to guide the perturbation computation. We use grid search to identify proper values that balance robustness and computational efficiency in all videos. In practice, content creators can perform a benchmark on their own videos to select thresholds that best balance their robustness and efficiency requirements. Further details on our grid search is located in the Appendix.

## EVALUATION

In this section, we evaluate Gimbal's ability to protect individual video frames from style mimicry. ¬ß6.1 describes our video datasets and experimental setup. ¬ß6.2 introduces our metrics for evaluation.

¬ß6.3-6.5 present results on Gimbal's protection and efficiency. Due to the subjective nature of interpreting successful style mimicry and visual nature of videos, we evaluate protection using both automated metrics from existing work, as well as visual judgement in a user study. Summary of results. Naive perturbations protect videos against style mimicry attempts 64.3% of the time according to surveyed users. However, perturbation removal attacks on naive perturbations successfully recover video style, causing protection rate to drop to 23.8% (compared to completely unprotected videos at 17.7%).

Gimbal is able to maintain protection even against removal attacks, restoring protection against style mimicry attempts to 64.5%. We verify that Gimbal is able to preserve image perturbations across consecutive image frames on a diverse set of videos genres and types, and is robust against our averaging attack introduced in ¬ß4.

## Experimental Setup

Video datasets. We evaluate the effectiveness of Gimbal on five diverse datasets, covering different animations, as well as human actions and scenery. On average, each video contains 6289 total frames. Our experiments conducted on YouTube videos are for research purposes only, and trained models are deleted at the conclusion of the study [[33]](#).

(1) Video Games: 20 randomly selected YouTube videos from a single channel showcasing in-game content of different video games, ranging from 2 to 6 minutes long. We include three unique animation styles, Video Game (3D rigs [[41]](#b37)), Japanese Anime (hand drawn [[13]](#b12)), and Animated Movies (CGI [[32]](#b30)). The remaining two video datasets are selected to cover the other types of video imagery, including real human presence and photography/aerial footage. As a preprocessing step, we maintain consistent quality among all videos by center cropping videos to square and resizing to 512x512. Defense configuration. To showcase the generalizability of Gimbal to multiple image-based protection systems, we test Gimbal on three such algorithms: Mist, Anti-DB, and Glaze. We reimplement each defense using ùëô ‚àû bounded projected gradient descent (PGD), and use a consistent targeted image generation method for all three [[43]](#b39). As is standard in image-based PGD methods, we constrain maximum absolute change in each image pixel to 0.07. Mimicry configuration. We base our style mimicry setup on existing work [[27,](#b25)[43,](#b39)[44,](#b40)[49]](#b45) and adapt it to video frames:

(1) Split each video into partitions using our scene splitting algorithm from ¬ß5.

(2) Use the CLIP aesthetic model to identify the frame with the highest image quality score within a scene, and select the highest quality images based on scenes with the highest image quality score.

We find that all datasets could successfully train style mimicry models with 30 images, except for Animated Movies, which required 60. Here, we can apply the pixel averaging adaptive mimicry attack, finetuning on 80% of extracted images and saving the remaining 20% for testing. We use Dreambooth to finetune a Stable Diffusion 2.1 model on the finetuning dataset for 1000 steps and learning rate of 1e-5. To generate matching synthetic images, we generate captions from testing images with BLIP [[26]](#b24), and query the finetuned model to create a set of style mimicked images.

We evaluate Gimbal across several combinations of our proposed defense system and style mimicry configurations. For each existing perturbation algorithm, we evaluate the efficacy when perturbed images are untouched, and when our image averaging attack is used. We evaluate the efficacy of Gimbal against style mimicry only when the image averaging attack is used. We also train a style mimicry model on every video where the frames are left untouched.

## Evaluation Metrics

The two key contributions of Gimbal are robustness under potential adversary attack, and reduction in computation costs, which we will evaluate using the following metrics. Robustness. We measure robustness in two ways. The first two metrics capture how well image perturbations can withstand adversarial attack. The last two measure the impact that Gimbal has on the quality of images generated by style mimicry models.

‚Ä¢ Latent ùêø 2 Norm: We employ the image encoder used in diffusion models to calculate latent representations of perturbed and nonperturbed (original) frames, and then calculate the ùêø 2 distance between them as a measurement for image closeness. For example, a successful averaging attack on consecutively perturbed frames would lead to low latent ùêø 2 norm, while a robust system should maintain a high latent ùêø 2 norm. In particular, this metric isolates how diffusion models capture image differences. ‚Ä¢ Mean Pixel Difference: We also measure the differences between images at a pixel level, motivated by the ùëô ‚àû bounded pixel changes that all three of our defense algorithms are constrained by. Mean Pixel Difference (MPD) is model-agnostic, and defined as the average of all pixel differences between a perturbed image and original image. Similar to the latent ùêø 2 norm, a higher MPD between perturbed and original frames signals higher protection. ‚Ä¢ CLIP-Genre Shift: We adapt a metric used in existing work [[43]](#b39) to demonstrate the effectiveness of Gimbal at disrupting style mimicry. Intuitively, existing image perturbation algorithms are designed to cause diffusion models to learn the wrong artistic style. Thus, we can measure the success of Gimbal's protection by calculating the percentage of generated images in which a CLIP image classifier unsuccessfully predicts the ground-truth style from training images. A higher CLIP-based Genre Shift score equates to stronger protection, while the opposite equates to stronger style mimicry. However, it is a granular metric and its correlation to visual properties might be weak.

‚Ä¢ Human-rated protection success rate (PSR): To accurately capture end-to-end visual properties, we perform two IRB-approved user studies (more details on participants in Appendix A.4) where participants look at images generated by mimicry models and rate if the style mimicry was successful. The first asks artist volunteers to rate performance on our two "Art" datasets: Anime and Video Games. The second asks general users to rate performance on all 5 datasets. For each video, we train 10 style mimicry models, matching our experiment configuration. For each mimicry model, we show participants 5 frames from the original video next to 5 frames generated by the mimicry model. Each participant is shown one example from each experiment configuration, randomly selected from the set of 80 videos (3 datasets x 20 videos + 2 datasets x 10 videos each).

We ask participants to compare original video frames to those generated by mimicry, and rate the success on a 5-point Likert scale (ranging from "Not successful at all" to "Very successful").

Following prior work, we define protection success rate as the percent of participants who rated how well generated images mimic original style as "Not very well" or "Not well at all. " We also show artists short 10 second clips of videos glazed naively and with Gimbal. Here, we ask how noticeable the perturbations are on a 5 point Likert scale ranging from "Very noticeable" to "Not noticeable at all. " We define Noticeability Rate (NR) as the % of users that think perturbations are "Noticeable" or "Very noticeable. "

Computation efficiency: Finally, we also measure the computation speed of Gimbal. Here, we only experiment on one image perturbation algorithm due to computational constraints. Glaze is chosen due to its balance of speed (fastest of the three) and strong robustness. For each video, we conduct our measurement on a single A100 GPU.

‚Ä¢ Speedup Factor: We compute speedup factor as time taken to apply (naive) protection to every frame, divided by time taken to protect the video using Gimbal. Because of the compute time involved, we estimate full (naive) protection by estimating per frame protection time, scaled up by number of frames in the video. ‚Ä¢ Seconds per Frame: We also report the average time it takes to perturb each frame in a video. This provides a grounding metric that is not relative like speedup factor.

## Robustness against Pixel-Averaging Mimicry

Gimbal prevents style mimicry. We showed in ¬ß4 that pixelaveraging attacks can remove image protection by smoothing them out across similar frames. We begin by looking at the ability of pixel-averaging methods to extract frames similar to the original, as measured by pixel level metrics in Table [4](#) and [5](#). For all protection tools, across each category of videos, we see that the pixel-averaging attack significantly reduces the distance between the protected frames compared to the originals, measured by both latent ùêø 2 norm and MPD. More importantly, we see that that same pixel-averaging attack fail when applied to frames protected by Gimbal, and it actually increases the distances from the original.

Next, we turn our attention to the ability of adaptive mimicry attacks and their ability to produce accurate end-to-end mimicry   models. Table [6](#) shows the results of our two user studies, involving both a population of artists and general users (participant details in Appendix A.4). While artists' views varied somewhat from general users across categories, all users consistently provided the same feedback, that pixel-averaging broke the protection provided by naive anti-mimicry tools, but Gimbal restored that protection (and in many cases increased protection higher than naive protection levels). Table [7](#) quantified the same issue of end to end protection, but using the CLIP CLIP-Genre shift metric. Results are very consistent with those from user studies. Gimbal restored protection broken by pixel-averaging attack and in many cases, improved protection beyond the original naive levels.

Finally, Figure [6](#fig_8) shows some examples of images generated by style mimicry models under our experiment configurations. Impact of perturbation budget on robustness. In Figure [7](#fig_9), we show that robustness under Gimbal increases with perturbation budget, quickly maxing out after 0.05. This follows the same trend we see in existing image-only perturbation algorithms.

## Computational Costs and Video Quality

In ¬ß3.3, we identified two additional limitations of a naive application of anti-mimicry tools: high computation overhead and poor video quality (randomness across per-frame perturbations appear as flickering snow when viewed at regular speeds). Here, we show that Gimbal significantly improves on both the computation overhead of perturbing video frames, and increases video quality over naively protected videos.

Avg. Speedup Factor Avg. Seconds per Frame Video Game 7.87 ¬± 2.81 5.16 ¬± 2.13 Japanese Anime 4.16 ¬± 3.19 11.25 ¬± 4.55 Human Action 7.37 ¬± 4.34 7.84 ¬± 6.43  Gimbal addresses directly. In our user study, we show participants 10-second clips of protected videos from our three datasets. The first video is naively protected using our implementation of Glaze, and the second video is protected with Gimbal. Participants are asked to identify how visible the perturbations are in both videos. Table [9](#tab_8) presents noticeability rate, showing artists are able to notice perturbations significantly more when videos are protected naively (70%) than with Gimbal (21%).

## Impact of Video Types on Robustness and Efficiency

Beyond examining the efficacy of Gimbal on a variety of video genres, we also examine four aspects of videos creation/distribution irrespective to genre that real-world content creators are likely to deal with while posting videos online. We investigate the effects that framerate, movement between frames, duration of scenes, and compression, have on robustness and computation efficiency. We anticipate these as questions that video content creators are likely to ask with respect to the efficacy of Gimbal. In this section, we show that Gimbal's robustness holds steady across video types, while computational efficiency is the most prone to change.

FPS Here, we evaluate the change in robustness and computation efficiency of Gimbal when videos are encoded with a different number of frames per second (fps). We evaluate Gimbal with Glaze on three different framerates, 15fps, 30fps, and 60fps, and measure its performance on four videos from the Video Game Dataset. 30fps and 60fps are widely used in the video community [[52]](#b48), and we also include 15 to show the effects that much a slower framerate has on robustness and computation efficiency. Our results can be found in Table [10](#tab_9), which show that framerate does not have significant impact on robustness, but that increasing framerate from 15fps to 60fps increases speedup factor from 6x to 16x. Thus, we show that Gimbal's computation performance is at its worst with low framerate, but scales well when content creators choose to increase framerate in their videos.

Action within scenes Movement within videos is difficult to capture, but we use latent ùêø 2 norm between two image latent representations as a good approximation. Intuitively, minor movement between frames should lead to minor changes in image latent representation. In this section, we investigate the effect that latent ùêø 2 norm between consecutive frames in video scenes has on computation efficiency. We measure the latent ùêø 2 norm between consecutive frames in a scene for every video in the Video Game dataset, and compare the median of the latent ùêø 2 norm across scenes to the speedup factor Gimbal with Glaze provides. In Figure [8](#fig_10), we show that there is a negative linear correlation between ùêø 2 latent norm and speedup factor. Intuitively, videos with higher action within scenes are more likely to require additional optimization in order to better align image perturbations across fast changing frames. Finally, we also analyze the impact of scene duration (number of frames per scene) and video compression factor (bitrate of video) on robustness and speedup factor. We found that these factors have no observable impact.

## SCENE SPLITTING ADAPTIVE ATTACK

Gimbal removes uncontrolled randomness in perturbations between similar frames, thereby disabling adaptive attacks that leverage cross frame pixel correlations. However, does its own design give rise to new countermeasures? We carefully considered this question, and discuss what we consider the strongest possible countermeasure to Gimbal. We describe the potential countermeasure and evaluate its efficacy against Gimbal.

The intuition for the countermeasure is to manipulate the scene identification process to force a scene break between highly similar frames. If this can be achieved, then the attacker could obtain two consecutive (and similar) frames that have been perturbed towards different targets. They could then apply a version of the pixelaveraging attack to restore the original, unprotected frames. We call this the scene-splitting attack, and assume a powerful attacker who can somehow force Gimbal to insert a scene break in the middle of a sequence of similar frames.

The scene splitting attack. We simulate a strong adaptive attack by dividing a single scene from a video into two subscenes, Scene ùëÜ 1 and ùëÜ 2 each containing ùëÄ frames. We use each subscene (Scene ùëñ ) to generate a target tensor ùëá ùëñ with the aim of maximizing distance between target tensors, which should maximize the difference between perturbations generated from these targets. Now that we have obtained ùëá 1 ,ùëá 2 , we apply Gimbal to perturb the two consecutive frames around the bad scene split. Subsequently, we perform a pixel averaging attack on these two perturbed frames, following the implementation detailed in ¬ß4. We test our adaptive attack on 20 videos from the Japanese Anime and Video Game datasets (10 videos from each dataset). We choose these two datasets because they are best aligned with current threats as explained in ¬ß1. We limit ourselves to only 10 videos from each category because of the computation time involved. Gimbal is robust to scene-splitting attack. Tables [11](#tab_11) and [12](#tab_0) show results that quantify the image-level difference between the original frames and the frames produced by the scene-splitting attack. They show that Gimbal is robust to the scene-splitting attack combined with pixel averaging attacks attempting to recover original frames. There is only a minimal decrease in ùêø 2 norm and ùëÄùëÉùê∑ value when Gimbal is applied to videos under the new Adaptive Scene-Splitting Attack. In Figure [9](#fig_11), we further verify that style mimicry using a combination of scene-splitting and pixelaveraging is still unsuccessful, through both visual examples of generated images and associated CLIP-genre shift scores, which remain unchanged under the scene splitting attack. Why does the attack fail? The failure of the scene-splitting attack makes sense, once we consider its limitations. Regardless of where the attacker forces a new scene break, frames inside the two new scenes (ùëÜ 1 and ùëÜ 2 ) are bounded in their maximum difference

Naive Gimbal + Adaptive Attack Gimbal + Adaptive Scene-Splitting Attack Video Game 405.38 ¬± 19.43 421.21 ¬± 25.52 394.80 ¬± 25.16 Japanese Anime 396.76 ¬± 19.05 406.43 ¬± 24.07 387.06 ¬± 23.98 Table 12: MPD between original and perturbed frames across Video Games and Japanese Anime datasets. Demonstrates Scene-Splitting Adaptive Attack is not able to significantly reduce protection Gimbal offers.

from each other. Thus the two source frames for ùëÜ 1 and ùëÜ 2 (each computed as an average of frames in the scene) is also bounded in their dissimilarity. Thus, it is likely their resulting target tensors, and consequently the perturbations generated from them, are also small. This intuition holds even if the attacker could break a single long scene into several scenes of their choosing.

## DISCUSSION AND LIMITATIONS

Our work provides a first step to addressing the rising challenge of style mimicry through training data extracted from video imagery. However, we note that this work still has significant limitations. First, we note that our work primarily targets and addresses the pixel-averaging adaptive attack. Given time, it is entirely possible for new distinct adaptive attacks to arise in the video mimicry space that Gimbal is not designed to mitigate. Similarly, it is also possible that effective countermeasures will be identified which target Gimbal itself beyond the scene-splitting attack. Second, while Gimbal reduces computation costs significantly over a naive application of anti-mimicry tools to each frame, the resulting computation is still quite significant, and likely remains out of reach for smaller video or animation creators. Finally, it is possible that image mimicry attacks might be supplanted altogether by some form of video mimicry attack in the future. Such risks and potential mitigation methods might become more clear as we learn more about generative video models and their training process.       loss [[18]](#b16) to fill in the gap between two or more images. We implement their code to generate a single interpolated image between two images separated five frames apart. FILM is designed to handle both large and small motion, thus we find that the distance between frames does not impact the success of this removal attack. Simple Linear Interpolation We apply a simple linear interpolation function to generate a new image by smoothly blending the pixel values of two input images. We chose an interpolation factor ùõº = 0.5 to give equal weight to both images. Similar to our existing perturbation removal attacks, we perform simple linear interpolation between two images 5 frames apart.

## A.3 Number of Images to use in Average Attack

Figures 13-15 plot the robustness and quality of protected images as a function of the number of consecutive frames used by our selective pixel averaging attack. Image quality begins to degrade when more than 3-5 consecutive frames are used for selective pixel averaging, with diminishing returns on recovering the correct artist style. Thus, we fix the number of averaged frames to 5 throughout our evaluation.

## A.4 User Study Detailed Description

Ethics We conduct two IRB-approved user studies. The first study, released on Twitter, involved 305 volunteers from the art community who rated the effectiveness of Gimbal compared to naive perturbations in protecting videos against style mimicry attempts. We only presented style-mimicked images from the Video Game and Japanese Anime datasets, chosen due to their relevance to current threats faced by video creators as discussed in ¬ß1. These datasets align with art content that existing image protection systems (Glaze, Mist, AntiDB [[27,](#b25)[43,](#b39)[49]](#b45)) are designed to safeguard.

The second study, conducted on Prolific, involved 220 participants who were presented with style-mimicked images from videos across all 5 datasets. They were compensated at $12 per hour and

![Figure 1: Style mimicry scenario demonstrating pipeline for adversaries to finetune a diffusion model on video frames]()

![Figure 2: Style mimicry on clean video frames successfully mimics style of original video.]()

![Figure 3: Averaging pixel values across highly similar consecutive frames successfully degrades the randomized protection pixel shifts across frames and largely restores the original unprotected frame.]()

![Figure 5: Gimbal partitions videos into scenes by measuring average pixel difference. Each scene is perturbed using a two-part process: 1) A target image (ùëá ) is computed by averaging all frames and style to a 'target style' 2) Perturbations are iteratively applied and optimized by minimizing latent ùêø 2 norm between ùëá and the perturbed frame.]()

![Human Actions: 20 randomly selected untrimmed videos of human actions from the THUMOS-15 training dataset originally designed for action classification[23], ranging from 1 to 4 minutes long.(3) Japanese Anime: 20 randomly selected YouTube videos from different channels of Japanese animated movies and tv-shows, ranging from 1 to 5 minutes long. (4) Animated Movies: 10 randomly selected compilations of animated (Disney, Pixar, Sony) movie clips, ranging from 5 to 13 minutes long. (Nature and Wildlife: 10 randomly selected YouTube videos of animals and scenery in the wild, shot in a documentary format. Original videos are 30-60 minutes long, we clip each video to the first 2 minutes.]()

![Figure 6: Some visual examples of style mimicry on Gimbal showing it is robust to pixel averaging attack.]()

![Figure 7: Average robustness (CLIP-Genre Shift) increases as ùëô ‚àû perturbation budget increases on 4 Video Game videos. For comparison, mimicry on original frames produces CLIP-Genre Shift of 0.44.]()

![Figure 8: Speedup factor decreases as movement increases between frames (latent ùêø 2 norm) on all Video Game videos.]()

![Figure 9: Visual examples of mimicry attempts on Clean frames and frames protected by Gimbal across the standard Adaptive Attack and the Scene-Splitting Adaptive Attack. CLIP-Genre Shift score demonstrates robust protection against Mimicry attacks under both adaptive attack scenarios.]()

![Figure11: Gridsearch of Gimbal's two threshold parameters ùúè 1 , ùúè 2 with respect to the latent ùêø 2 norm between perturbed frames and the target it is optimized towards. A lower latent ùêø 2 norm equates to better optimized perturbation and stronger robust protection. Decreasing both values leads to higher number of full optimizations, and stronger robustness.]()

![Figure 12: Gridsearch of Gimbal's two threshold parameters ùúè 1 , ùúè 2 with respect to the computation speedup. Increasing both values leads to less number of full optimizations, and more significant speedup.]()

![Figure 13: Average image quality (CLIP Aesthetic score) increases initially as the number of consecutively frames used for averaging attack increases, but decreases after too many frames are averaged for both Video Game and Japanese Anime videos.]()

![Figure 14: Average robustness (CLIP-Genre Shift) decreases as the number of consecutively frames used for averaging attack increases and stagnates as more frames are used for both Video Game and Japanese Anime videos.]()

![Figure 15: Average latent ùêø 2 norm between perturbed frames and clean frames initially decreases as the number of consecutive frames used for averaging attack increases. The benefit stagnates for Video Game videos, but loses effectiveness for Japanese Anime.]()

![¬± 25.17 295.55 ¬± 52.02 391.51 ¬± 62.86 355.58 ¬± 44.38 Mist 474.87 ¬± 42.14 334.62 ¬± 56.72 437.78 ¬± 64.89 411.27 ¬± 51.89 Anti-DB 405.17 ¬± 37.55 283.41 ¬± 58.99 378.43 ¬± 67.34 352.10 ¬± 52.30Latent ùêø 2 Norm between original frames, protected frames and protected frames after perturbation removal.]()

![Gimbal with Glaze enables significant computation speedup factor across all Video Game, Japanese Anime, and Human action videos when compared to naive video protection. For comparison, naive video protection takes on average 35 seconds per frame on a single A100 GPU.]()

![Our user study shows that the percentage of users who notice perturbations generated by Gimbal is significantly less than those who notice perturbations in naive video protection. (Tests implemented using Glaze).]()

![Different framerates do not impact protection robustness (CLIP-Genre Shift), but higher framerates lead to higher speedup factor under Gimbal. (4 Video Game videos.)]()

![Latent ùêø 2 norm between original and perturbed frames across Video Games and Japanese Anime datasets. Demonstrates Scene-Splitting Adaptive Attack is not able to significantly reduce protection Gimbal offers.]()

https://huggingface.co/Totsukawaii/RiotDiffusion

https://huggingface.co/ItsJayQz/Valorant_Diffusion

https://civitai.com/models/270789/lucasarts-games-style

https://civitai.com/models/382550/kasumi-dead-or-alive-sdxl-lora-pony-diffusion

https://civitai.com/models/42622/danielle-panabaker-the-flash-tv-show

https://huggingface.co/Madhul/Rick_and_Morty_Stable_Diffusion_LORAS

https://civitai.com/models/160262/katniss-everdeen-hunger-games

https://civitai.com/models/105883/ruby-roundhouse-from-jumanji-movies-karengillan

https://tensor.art/models/662818547598142799

