<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-03-28">28 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
							<email>gregyang@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
							<email>edwardhu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">×</forename><forename type="middle">×</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Corporation</forename><forename type="middle">•</forename><surname>Openai</surname></persName>
						</author>
						<title level="a" type="main">Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-28">28 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">76AEDC789713D175694F0639CDD682A4</idno>
					<idno type="arXiv">arXiv:2203.03466v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (µP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call µTransfer: parametrize the target model in µP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify µTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via pip install mup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 Introduction 20 18 16 14 12 10 log2LearningRate 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 Training Loss optimum shifts Standard Practice Width 128 256 512 1024 2048 4096 8192 20 18 16 14 12 10 log2LearningRate optimum stable Our Work Conventionally and in contrast with our technique, different widths do not share the same optimal hyperparameter; wider networks do not always perform better than narrower ones; in fact they underperform the same-width networks in our technique even after tuning learning rate (see dashed line). See Sections 3 and 4 for experimental setup.</p><p>Hyperparameter (HP) tuning is critical to deep learning. Poorly chosen HPs result in subpar performance and training instability. Many published baselines are hard to compare to one another due to varying degrees of HP tuning. These issues are exacerbated when training extremely large deep learning models, since stateof-the-art networks with billions of parameters become prohibitively expensive to tune.</p><p>Recently, <ref type="bibr" target="#b57">[57]</ref> showed that different neural network parametrizations induce different infinitewidth limits and proposed the Maximal Update Parametrization (abbreviated µP) (summarized in Table <ref type="table" target="#tab_12">3</ref>) that enables "maximal" feature learning in the limit. Intuitively, it ensures that each layer is updated on the same order during training regardless of width. 2 In contrast, while the standard parametrization (SP) ensures activations are of unit order at initialization, it actually causes them to blow up in wide models during training <ref type="bibr" target="#b57">[57]</ref> essentially due to an imbalance of per-layer learning rate (also see Fig. <ref type="figure">5</ref>). We leverage µP to zero-shot transfer HPs from small models to large models in this work -that is, we obtain near optimal HPs on a large model without directly tuning it at all! While practitioners have always guessed HPs of large models from those of small models, the results are hit-or-miss at best because of incorrect parametrization. For example, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, in a Transformer, the optimal learning rate is stable with width in µP (right) but far from so in standard parametrization (left). In addition to width, we empirically verify that, with a few caveats, HPs can also be transferred across depth (in Section 6.1) as well as batch size, language model sequence length, and training time (in Appendix G.2.1). This reduces the tuning problem of an (arbitrarily) large model to that of a (fixed-sized) small model. Our overall procedure, which we call µTransfer, is summarized in Algorithm 1 and Fig. <ref type="figure" target="#fig_1">2</ref>, and the HPs we cover are summarized in Tables <ref type="table" target="#tab_1">1</ref> and <ref type="table" target="#tab_2">2</ref>. There are several benefits to our approach: 1. Better Performance: µTransfer is not just about predicting how the optimal learning rate scales in SP. In general, we expect the µTransferred model to outperform its SP counterpart with learning rate optimally tuned. For example, this is the case in Fig. <ref type="figure" target="#fig_0">1</ref> with the width-8192 Transformer. We discuss the reason for this in Section 5 and Appendix C. 2. Speedup: It provides massive speedup to the tuning of large models. For example, we are able to outperform published numbers of (350M) BERT-large <ref type="bibr" target="#b10">[11]</ref> purely by zero-shot HP transfer, with tuning cost approximately equal to 1 BERT-large pretraining. Likewise, we outperform the published numbers of the 6.7B GPT-3 model <ref type="bibr" target="#b6">[7]</ref> with tuning cost being only 7% of total pretraining cost. For models on this scale, HP tuning is not feasible at all without our approach. 3. Tune Once for Whole Family: For any fixed family of models with varying width and depth (such as the BERT family or the GPT-3 family), we only need to tune a single small model and can reuse its HPs for all models in the family. <ref type="foot" target="#foot_0">3</ref> For example, we will use this technique to tune BERT-base (110M parameters) and BERT-large (350M parameters) simultaneously by transferring from a 13M model. <ref type="bibr" target="#b3">4</ref>. Better Compute Utilization: While large model training needs to be distributed across many GPUs, the small model tuning can happen on individual GPUs, greatly increasing the level of parallelism for tuning (and in the context of organizational compute clusters, better scheduling and utilization ratio). 5. Painless Transition from Exploration to Scaling Up: Often, researchers explore new ideas on small models but, when scaling up, find their HPs optimized during exploration work poorly on large models. µTransfer would solve this problem.</p><p>In addition to the HP stability property, we find that wider is better throughout training in µP, in contrast to SP (Section 8). This increases the reliability of model scaling in deep learning.</p><p>In this work, we primarily focus on hyperparameter transfer with respect to training loss. In settings where regularization is not the bottleneck to test performance, as in all of our experiments here, this also translates to efficacy in terms of test loss. In other settings, such as finetuning of models on small datasets, µTransfer may not be sufficient, as we discuss in Section 6.1. • We demonstrate it is possible to zero-shot transfer near optimal HPs to a large model from a small version via the Maximal Update Parametrization (µP) from <ref type="bibr" target="#b57">[57]</ref>. • While <ref type="bibr" target="#b57">[57]</ref> only covered SGD, here we derive µP for Adam as well (Table <ref type="table" target="#tab_12">3</ref>).</p><p>• We propose a new HP tuning technique, µTransfer, for large neural networks based on this observation that provides massive speedup over conventional methods and covers both SGD and Adam training; • We thoroughly verify our method on machine translation and large language model pretraining (in Section 7.3) as well as image classification (in Appendix G.1); • We release a PyTorch <ref type="bibr" target="#b34">[35]</ref> package for implementing µTransfer painlessly. A sketch of this package is given in Appendix H.</p><p>Terminologies Sometimes, to be less ambiguous, we often refer to the "large model" as the target model, as it is the model we wish to ultimately tune, while we refer to the "small model" as the proxy model, as it proxies the HP tuning process. We follow standard notation d model , d head = d k , d v , n head , d f f n regarding dimensions in a Transformer; one can see Fig. <ref type="figure" target="#fig_15">11</ref> for a refresher.</p><p>Tensor Programs Series This paper is the 5th installment of the Tensor Programs series. While it is self-contained with the target audience being practitioners and empirical researchers, this paper presents the first major practical payoff of the theoretical foundation built in previous works <ref type="bibr" target="#b53">[53]</ref><ref type="bibr" target="#b54">[54]</ref><ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref><ref type="bibr" target="#b57">[57]</ref><ref type="bibr" target="#b58">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parametrization Matters: A Primer</head><p>In this section, we give a very basic primer on why the correct parametrization can allow HP transfer across width, but see Appendices J.1 to J.3 for more (mathematical) details.</p><p>The Central Limit Theorem (CLT) says that, if x 1 , . . . , x n are iid samples from a zero-mean, unitvariance distribution, then 1 √ n (x 1 + • • • + x n ) converges to a standard Gaussian N (0, 1) as n → ∞. Therefore, we can say that 1 √ n is the right order of scaling factor c n such that c n (x 1 + • • • + x n ) converges to something nontrivial. In contrast, if we set c n = 1/n, then c n (x</p><formula xml:id="formula_0">1 + • • • + x n ) → 0; or if c n = 1, then c n (x 1 + • • • + x n ) blows up in variance as n → ∞.</formula><p>Now suppose we would like to minimize the function Then for sufficiently large n, the optimal α * n def = arg min α G n (α) should be close to α * N for any N &gt; n, and indeed, for N = ∞ -this precisely means we can transfer the optimal c * n or α * n for a smaller problem (say F n ) to a larger problem (say F N ): G N is approximately minimized by α * n and F N is approximately minimized by c * n n/N . Because the transfer algorithm is simply copying α, we say the parametrization c = α/ √ n is the correct parametrization for this problem.</p><formula xml:id="formula_1">F n (c) def = E x1,...,xn f (c(x 1 + • • • + x n ))<label>(1</label></formula><p>In the scenario studied in this paper, x 1 , . . . , x n are akin to randomly initialized parameters of a width-n neural network, c is akin to a HP such as learning rate, and f is the test-set performance of the network after training, so that F n gives its expectation over random initializations. Just as in this example, if we parametrize the learning rate and other HPs correctly, then we can directly copy the optimal HPs for a narrower network into a wide network and expect approximately optimal performance -this is the (zero-shot) hyperparameter transfer we propose here. It turns out the Maximal Update Parametrization (µP) introduced in <ref type="bibr" target="#b57">[57]</ref> is correct (akin to the parametrization in α above), while the standard parametrization (SP) is incorrect (akin to the parametrization in c). We will review both parametrizations shortly. Theoretically, a µP network has a well-defined infinite-width limit -akin to (x 1 + • • • + x n )/ √ n having a N (0, 1) limit by CLT -while a SP network does not (the limit will blow up) <ref type="bibr" target="#b57">[57]</ref>. <ref type="foot" target="#foot_1">4</ref> In fact, based on the theoretical foundation laid in <ref type="bibr" target="#b57">[57]</ref>, we argue in Appendix J.3 that µP should also be the unique parametrization that allows HP transfer across width. For a more formal discussion of the terminologies parametrization and transfer, see Appendix A We emphasize that, to ensure transferability of any hyperparameter (such as learning rate), it's not sufficient to reparametrize only that hyperparameter, but rather, we need to identify and correctly reparametrize all hyperparameters in Table <ref type="table" target="#tab_2">2</ref>. For example, in Fig. <ref type="figure" target="#fig_0">1</ref>, the wide models in SP still underperform their counterparts in µP, even with learning rate tuned optimally. This is precisely because SP does not scale parameter multipliers and input/output layer learning rates correctly in contrast to µP (see Table <ref type="table" target="#tab_12">3</ref>). See Appendix C for more intuition via a continuation of our example here. We shall also explain this more concretely in the context of neural networks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hyperparameters Don't Transfer Conventionally</head><p>In the community there seem to be conflicting assumptions about HP stability. A priori, models of different sizes don't have any reason to share the optimal HPs. Indeed, papers aiming for stateof-the-art results often tune them separately. On the other hand, a nontrivial fraction of papers in deep learning fixes all HPs when comparing against baselines, which reflects an assumption that the optimal HPs should be stable -not only among the same model of different sizes but also among models of different designs -therefore, such comparisons are fair. Here, we demonstrate HP instability across width explicitly in MLP and Transformers in the standard parametrization. We will only look at training loss to exclude the effect of regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP with Standard Parametrization</head><p>We start with a 2-hidden-layer MLP with activation function φ, using the standard parametrization <ref type="foot" target="#foot_2">5</ref> with LeCun initialization <ref type="foot" target="#foot_3">6</ref> akin to the default in PyTorch: where</p><formula xml:id="formula_2">f (ξ) = W 3 φ(W 2 φ(W 1 ξ + b 1 ) + b 2 ) with init. W 1 ∼ N (0, 1 /din), W {2,3} ∼ N (0, 1 /n), b {1,2} = 0,<label>(2)</label></formula><formula xml:id="formula_3">W 1 ∈ R din×n , b 1 ∈ R n , W 2 ∈ R n×n , b 2 ∈ R n , W 3 ∈ R n×dout and</formula><p>d in , n, and d out are the input, hidden, and output dimensions. The particular MLP we use has φ = ReLU and a cross-entropy (xent) loss function. We define the width of MLP as the hidden size n, which is varied from 256 to 8192. The models are trained on CIFAR-10 for 20 epochs, which is more than enough to ensure convergence.</p><p>As shown on the left in Fig. <ref type="figure" target="#fig_3">3</ref>, the optimal learning rate shifts by roughly an order of magnitude as the width increases from 256 to 8192; using the optimal learning of the smallest model on the largest model gives very bad performance, if not divergence.</p><p>Transformer with Standard Parametrization This perhaps unsurprising observation holds for more complex architectures such as Transformer as well, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (left). We define width Table <ref type="table" target="#tab_12">3</ref>: µP <ref type="bibr" target="#b57">[57]</ref> and SP for General Neural Networks. Here, we emphasize the scaling with width (fan_in or fan_out); in practice, we may insert tunable multipliers in front of fan_in and fan_out as in Eq. ( <ref type="formula" target="#formula_7">4</ref>). The fan_out of a bias vector is its dimension (whereas fan_in is 1). Purple text highlights key differences from standard parametrization (SP); Gray text recalls the corresponding SP. SGD (resp. Adam) here can be replaced by variants such as SGD with momentum (resp. Adagrad, etc); see Appendix B.3 for other optimizers. In general, the three columns here can be interpreted as linear layers that have {finite, infinite, infinite} input dimension and {infinite, finite, infinite} output dimension in an infinite-width network; this description generalizes more readily to other parameters such as those of layernorm. Transformer µP requires one more modification (1/d attention instead of 1/ √ d); see Definition 4.1. This version of µP gets rid of parameter multipliers; for the version similar to that in <ref type="bibr" target="#b57">[57]</ref>, see Table <ref type="table" target="#tab_13">9</ref>. Also see Table <ref type="table">8</ref> for a µP formulation that is easier to implement (and compatible with input/output weight sharing). Further explanation of this table can be found in Appendix B. Its derivation can be found in Appendix J.</p><p>Input weights &amp; all biases Output weights Hidden weights</p><formula xml:id="formula_4">Init. Var. 1 /fan_in 1 /fan_in 2 ( 1 /fan_in) 1 /fan_in SGD LR fan_out (1) 1 /fan_in (1) 1 Adam LR 1 1 /fan_in (1) 1 /fan_in (1)</formula><p>as d model , with</p><formula xml:id="formula_5">d k = d q = d v = d model/n head and d f f n = 4d model .</formula><p>The models are trained on wikitext-2 for 5 epochs. In Fig. <ref type="figure" target="#fig_15">18</ref> in the appendix we also show the instability of initialization scale and other HPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Unlocking Zero-Shot Hyperparameter Transfer with µP</head><p>We show that µP solves the problems we see in Section 3.</p><p>MLP with µP For the MLP in Section 3, to switch to µP, we just need to modify Eq. ( <ref type="formula" target="#formula_2">2</ref>)'s initialization of the last layer and its learning rates of the first and last layer as well as of the biases. The basic form is <ref type="foot" target="#foot_4">7</ref>initialize</p><formula xml:id="formula_6">W 1 ∼ N (0, 1 /din), W 2 ∼ N (0, 1 /n), W 3 ∼ N (0, 1 /n 2 ), b {1,2} = 0 with SGD learning rates η W 1 = η b 1 = η b 2 = ηn, η W 2 = η, η W 3 = ηn -1 .<label>(3)</label></formula><p>Here, η specifies the "master" learning rate, and we highlighted in purple the differences in the two parametrizations. This basic form makes clear the scaling with width n of the parametrization, but in practice we will often insert (possibly tune-able) multiplicative constants in front of each appearance of n. For example, this is useful when we would like to be consistent with a SP MLP at a base width n 0 . Then we may insert constants as follows: For</p><formula xml:id="formula_7">ñ def = n/n 0 , initialize W 1 ∼ N (0, 1 /din), W 2 ∼ N (0, 1 /n), W 3 ∼ N (0, 1 /n•ñ), b {1,2} = 0 with SGD learning rates η W 1 = η b 1 = η b 2 = ηñ, η W 2 = η, η W 3 = ηñ -1 .<label>(4)</label></formula><p>Then at width n = n 0 , all purple factors above are 1, and the parametrization is identical to SP (Eq. ( <ref type="formula" target="#formula_2">2</ref>)) at width n 0 . Of course, as n increases from n 0 , then Eq. ( <ref type="formula" target="#formula_7">4</ref>) quickly deviates from Eq. ( <ref type="formula" target="#formula_2">2</ref>). In other words, for a particular n, µP and SP can be identical up to the choice of some constants (in this case n 0 ), but µP determines a different "set" of networks and optimization trajectory than SP as one varies n. As we will see empirically in the next section, this deviation is crucial for HP transfer.</p><p>Indeed, in Fig. <ref type="figure" target="#fig_3">3</ref>(right), we plot the CIFAR10 performances, over various learning rates and widths, of µP MLPs with n 0 = 128. In contrast to SP, the optimal learning rate under µP is stable. This means that, the best learning rate for a width-128 network is also best for a width-8192 network in µP -i.e. HP transfer works -but not for SP. In addition, we observe performance for a fixed learning rate always weakly improves with width in µP , but not in SP.</p><p>This MLP µP example can be generalized easily to general neural networks trained under SGD or Adam, as summarized in Table <ref type="table" target="#tab_12">3</ref>, which is derived in Appendix J. When not specified in the legend, the width used is 256, depth 2, batch size 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to each column, while fixing all others constant. See Section 6.1 for discussion of these results.</p><p>Transformers with µP We repeat the experiments with base width n 0 = 128 for Transformers: Definition 4.1. The Maximal Update Parametrization (µP) for a Transformer is given by Table <ref type="table" target="#tab_12">3</ref> and 1/d attention instead of 1/ √ d, i.e. the attention logit is calculated as q k/d instead of q k/ √ d where query q and key k have dimension d. <ref type="foot" target="#foot_5">8</ref>The results are shown on the right in Fig. <ref type="figure" target="#fig_0">1</ref>, where the optimal learning rate is stable, and the performance improves monotonically with width. See Appendix B for further explanation of µP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">The Defects of SP and How µP Fixes Them</head><p>The question of SP vs µP has already been studied at length in <ref type="bibr" target="#b57">[57]</ref>. Here we aim to recapitulate the key insights, with more explanations given in Appendix J.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Instructive Example</head><p>As shown in <ref type="bibr" target="#b57">[57]</ref> and Appendix J.3, in SP, the network output will blow up with width after 1 step of SGD. It's instructive to consider a 1-hidden-layer linear perceptron f (x) = V U x with scalar inputs and outputs, as well as weights</p><formula xml:id="formula_8">V, U ∈ R n×1 . In SP, V α ∼ N (0, 1/n) ad U α ∼ N (0, 1) for each α ∈ [n]</formula><p>. This sampling ensures that f (x) = Θ(|x|) at initialization. After 1 step of SGD with learning rate 1, the new weights are V ← V + θU, U ← U + θV , where θ is some scalar of size Θ(1) depending on the inputs, labels, and loss function. But now</p><formula xml:id="formula_9">f (x) = V U x = (V U + θU U + θV V + θ 2 U V )x<label>(5)</label></formula><p>blows up with width n because U U = Θ(n) by Law of Large Numbers. Now consider the same network in µP. According to Table <ref type="table" target="#tab_12">3</ref>, we now have V α ∼ N (0, 1/n 2 ) in contrast to SP, but U α ∼ N (0, 1) as before, with learning rates η V = 1/n, η U = n. After 1 step of SGD, we now have</p><formula xml:id="formula_10">f (x) = (V U + θn -1 U U + θnV V + θ 2 U V )x,</formula><p>0.0 0.5 1.0 1.5 SP std(xt x0) logits t 0 1 2 3 4 0 20 40 60 attn logits 0.0000 0.0005 0.0010 0.0015 0.0020 word embedding 0 2000 4000 width 0.00 0.05 0.10 0.15 P std(xt x0) 0 2000 4000 width 0.000 0.025 0.050 0.075 0.100 0.125 0 2000 4000 width 0.0000 0.0005 0.0010 0.0015 Figure 5: Logits and attention logits, but not word embeddings, of a Transformer blow up with width in SP after 1 step of training. In contrast, all three are well-behaved with width in µP. Here we measure how much different values change coordinatewise from initialization over 4 steps of Adam updates, as a function of width. Specifically, we plot the standard deviation of the coordinates of x t -x 0 , for t = 0, . . . , 4, and x ∈ {logits, attention logits, word embeddings}, where t = 0 indicates initialization. and one can verify this is Θ(1) and thus does not blow up with width.<ref type="foot" target="#foot_6">foot_6</ref> </p><p>Some Layers Update Too Fast, Others Too Slow One can observe the same behavior in more advanced architectures like Transformers and optimizers like Adam; in fact, in SP, other hidden quantities like attention logits will also blow up with width after 1 step, but in µP still remain bounded, as shown in Fig. <ref type="figure">5</ref>(middle).</p><p>One might think scaling down the learning rate with width can solve this problem in SP. However, other hidden activations like the word embedding (Fig. <ref type="figure">5</ref>(right)) in a Transformer update by a widthindependent amount for each step of training, so scaling down the learning rate will effectively mean the word embeddings are not learned in large width models. Similar conclusions apply to other models like ResNet (in fact, one can observe in the SP linear MLP example above, the input layer is updated much more slowly than the output layer). On the other hand, µP is designed so that all hidden activations update with the same speed in terms of width (see Appendix J.2 for why).</p><p>Performance Advantage of µP This is why a wide model tuned with µTransfer should in general outperform its SP counterpart with (global) learning rate tuned. For example, this is the case for the width-8192 Transformer in Fig. <ref type="figure" target="#fig_0">1</ref>, where, in SP, the optimal learning rate needs to mollify the blow-up in quantities like logits and attention logits, but this implies others like word embeddings do not learn appreciably. This performance advantage means µTransfer does more than just predicting the optimal learning rate of wide SP models. Relatedly, we observe, for any fixed HP combination, training performance never decreases with width in µP, in contrast to SP (e.g., the µP curves in Figs. 1, 3 and 16 do not cross, but the SP curves do; see also Section 8).</p><p>6 Which Hyperparameters Can Be µTransferred?</p><p>In this section, we explore how common HPs fit into our framework. In general, they can be divided into three kinds, summarized in Table <ref type="table" target="#tab_1">1:</ref> 1. those that can transfer from the small to the large model, such as learning rate (Table <ref type="table" target="#tab_2">2</ref>); 2. those that primarily control regularization and don't work well with our technique; and 3. those that define training scale, such as width as discussed above as well as others like depth and batch size, across which we transfer other HPs.</p><p>Those in the first category transfer across width, as theoretically justified above in Section 2. To push the practicality and generality of our technique, we empirically explore the transfer across the other dimensions in the third category. Note that µTransfer across width is quite general, e.g. it allows varying width ratio of different layers or number of attention heads in a Transformer; see Appendix E.2. This will be very useful in practice. For the second category, the amount of regularization (for the purpose of controlling overfitting) naturally depends on both the model size and data size, so we should not expect transfer to work if the parametrization only depends on model size. We discuss these HPs in more detail in Appendix E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Empirical Validation and Limitations</head><p>Our empirical investigations focus on Transformers (here) and ResNet (in Appendix G.1.1), the most popular backbones of deep learning models today. We train a 2-layer pre-layernorm µP <ref type="foot" target="#foot_7">10</ref>Transformer with 4 attention heads on Wikitext-2. We sweep one of four HPs (learning rate, output weight multiplier, initialization standard deviation, and learning rate schedule) while fixing the others and sweeping along width and depth (with additional results in Fig. <ref type="figure" target="#fig_15">19</ref> on transfer across batch size, sequence length, and training time). Fig. <ref type="figure" target="#fig_4">4</ref> shows the results averaged over 5 random seeds.</p><p>Empirically, we find that for language modeling on Transformers, HPs generally transfer across scale dimensions if some minimum width (e.g. 256), depth (e.g., 4), batch size (e.g., 32), sequence length (e.g., 128), and training steps (e.g., 5000) are met, and the target scale is within the "reasonable range" as in our experiments. Now, there are some caveats. While the exact optimum can shift slightly with increasing scale, this shift usually has very small impact on the loss, compared to SP (Figs. 1 and 3(left)). However, there are some caveats. For example, the best initialization standard deviation does not seem to transfer well across depth (2nd row, 3rd column), despite having a stabler optimum across width. In addition, while our results on width, batch size, sequence length, and training time still hold for post-layernorm (Fig. <ref type="figure" target="#fig_15">17</ref>), <ref type="foot" target="#foot_8">11</ref> the transfer across depth only works for pre-layernorm Transformer. Nevertheless, in practice (e.g. our results in Section 7.3) we find that fixing initialization standard deviation while tuning other HPs works well when transferring across depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Efficiency and Performance of µTransfer</head><p>Now that the plausibility of µTransfer has been established in toy settings, we turn to more realistic scenarios to see if one can achieve tangible gains. Specifically, we perform HP tuning only on a smaller proxy model, test the obtained HPs on the large target model directly, and compare against baselines tuned using the target model. We seek to answer the question: Can µTransfer make HP tuning more efficient while achieving performance on par with traditional tuning? As we shall see by the end of the section, the answer is positive. We focus on Transformers here, while experiments on ResNets on CIFAR10 and Imagenet can be found as well in Appendix G.1. All of our experiments are run on V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Transformer on IWSLT14 De-En</head><p>Setup IWSLT14 De-En is a well-known machine translation benchmark. We use the default IWSLT (post-layernorm) Transformer implemented in fairseq <ref type="bibr" target="#b32">[33]</ref> with 40M parameters, which we denote as the 1x model. <ref type="foot" target="#foot_9">12</ref> For µTransfer, we tune on a 0.25x model with 1/4 of the width, amounting to 4M parameters. For this experiment, we tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn . See the grid and other experimental details in Appendix F.1.</p><p>We compare transferring from the 0.25x model with tuning the 1x model while controlling the total tuning budget in FLOPs. <ref type="foot" target="#foot_10">13</ref> To improve the reproducibility of our result: 1) we repeat the entire HP search process (a trial) 25 times for each setup, with number of samples as indicated in Table <ref type="table">4</ref>, and report the 25th, 50th, 75th, and 100th percentiles in BLEU score; 2) we evaluate each selected HP combination using 5 random initializations and report the mean performance.</p><p>14 Table 4: Transformer on IWSLT14 De-En. 1x and 0.25x refers to scaling of width only. Compared to traditional tuning ("Tuning on 1x"), µTransfer from 0.25x provides better and more reliable outcome given fixed amount of compute. On the other hand, naive transfer (i.e. with SP instead of µP) fails completely. The percentiles are over independent trials, with each trial involving the entire tuning process with a new HP random search. Val. BLEU Percentiles Setup Total Compute #Samples 25 50 75 100 fairseq[33] default -----35.40 Tuning on 1x 1x 5 33.62 35.00 35.35 35.45 Naive transfer from 0.25x 1x 64 training diverged µTransfer from 0.25x (Ours) 1x 64 35.27 35.33 35.45 35.53</p><p>We pick the HP combination that achieves the lowest validation loss <ref type="foot" target="#foot_12">15</ref> for each trial. The reported best outcome is chosen according to the validation loss during tuning. We compare against the default in fairseq, which is presumably heavily tuned. The result is shown in Table <ref type="table">4</ref>.</p><p>4 3 2 1 0 1 2 3 log2Compute 34.4 34.6 34.8 35.0 35.2 35.4 BLEU Score Method Ours Conventional 10 20 30 40 50 60 # Samples Method Ours Conventional Figure 6: Efficiency-performance Pareto frontier of µTransfer compared to conventional tuning, on IWSLT Transformer, using random HP search as the base method. We plot the median BLEU score over 25 trials (Left) against relative compute budget in log scale and (Right) against number of HP samples taken. While with the same number of samples, µTransfer slightly underperforms conventional tuning, this gap vanishes with more samples, and in terms of compute, our Pareto frontier strongly and consistently dominates that of conventional tuning. Note that, in larger models (e.g. BERT or GPT-3, not shown here), we believe our efficiency advantage will only widen as our small proxy model can stay the same size while the target model grows. Performance Pareto Frontier The result above only describes a particular compute budget. Is µTransfer still preferable when we have a lot more (or less) compute? To answer this question, we produce the compute-performance Pareto frontier in Fig. 6(left), where we repeat the above experiment with different compute budgets. Evidently, our approach completely dominates conventional tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Quality of Proxy Model vs Target</head><p>Model The Pareto frontier in Fig. <ref type="figure">6</ref>(right) suggests that, given a fixed number of random samples from the HP space, 1) tuning the target model directly yields slightly better results than tuning the proxy model (while taking much more compute of course), but 2) this performance gap seems to vanish as more samples are taken. This can be explained by the intuition that the narrower proxy model is a "noisy estimator" of the wide target model <ref type="bibr" target="#b57">[57]</ref>.With few samples, this noise can distort the random HP search, but with more samples, this noise is suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Transformer on WMT14 En-De</head><p>We scale up to WMT14 En-De using the large (post-layernorm) Transformer from <ref type="bibr" target="#b50">[50]</ref> with 211M parameters. We tune on a proxy model with 15M parameters by shrinking d model , d f f n , and n head .</p><p>For this experiment, we tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid in Appendix F.2. The result is shown in Table <ref type="table" target="#tab_7">5</ref>: While random search with 3 HP samples far underperforms the fairseq default, we are able to match it via transfer using the same tuning budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">BERT</head><p>Finally, we consider large-scale language model pretraining where HP tuning is known to be challenging. Using Megatron (pre-layernorm) BERT <ref type="bibr" target="#b43">[43]</ref> as a baseline, we hope to recover the performance of the published HPs by only tuning a proxy model that has roughly 13M parameters, which we call BERT-prototype. While previous experiments scaled only width, here we will also scale depth, as discussed in Section 6 and validated in Fig. <ref type="figure" target="#fig_4">4</ref>. We use a batch size of 256 for all runs and follow the During HP tuning, we sample 256 combinations from the search space and train each combination on BERT-prototype for 10 5 steps. The total tuning cost measured in FLOPs is roughly the same as training 1 BERT-large for the full 10 6 steps; the exact calculation is shown in Appendix F.3. The results are shown in Table <ref type="table" target="#tab_8">6</ref>. Notice that on BERT-large, we obtain sizeable improvement over the well-tuned Megatron BERT-large baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">GPT-3</head><p>In order to further verify µTransfer at scale, we applied it to GPT-3 6.7B <ref type="bibr" target="#b6">[7]</ref> with relative attention. This target model consists of 32 residual blocks with width 4096. We form the small proxy model by shrinking width to 256, resulting in roughly 40 million trainable parameters, 168 times smaller than the target model. HPs were then determined by a random search on the proxy model. The total tuning cost was only 7% of total pretraining cost. Details of the HP sweep can be found in Appendix F. <ref type="bibr" target="#b3">4</ref>.</p><p>In order to exclude code difference as a possible confounder, we also re-trained GPT-3 6.7B from scratch using the original HPs from <ref type="bibr" target="#b6">[7]</ref>. Unfortunately, after we have finished all experiments, we found this baseline mistakenly used absolute attention (like models in <ref type="bibr" target="#b6">[7]</ref>) when it was supposed to use relative attention like the target model. In addition, during training of the µTransfer model we encountered numerical issues that lead to frequent divergences. In order to avoid them, the model was trained using FP32 precision, even though the original 6.7B model and our re-run were trained using FP16. 16 17 The resulting µTransfer model outperforms the 6.7B from <ref type="bibr" target="#b6">[7]</ref>, and is in fact comparable to the twice-as-large 13B model across our evaluation suite (see Table <ref type="table" target="#tab_15">11</ref>). Selected evaluation results can be found in Table <ref type="table">7</ref> and further details are given in Table <ref type="table" target="#tab_1">10</ref> and Appendix F.4. 16 While we are mainly focused on the efficacy of µTransfer regardless of precision, it would be interesting to ablate the effect of precision in our results, but we did not have enough resources to rerun the baseline in FP32 17 It is quite interesting that µTransfer identified a useful region of hyperparameters leading to much improved performance, which probably would be difficult to discover normally because 1) researchers usually change hyperparameters to accomodate precision and 2) there was no precise enough justification to go against this judgment until µTransfer.</p><p>Table <ref type="table">7</ref>: GPT-3 6.7B Pretraining. Selected evaluation results for the GPT-3 6.7B model tuned with µTransfer (transfered from a small proxy model of 40M parameters), compared to the results published in <ref type="bibr" target="#b6">[7]</ref> and a re-run with original HPs, as well as the 13B model in <ref type="bibr" target="#b6">[7]</ref> for reference. Note that the perplexities in this table are based on a custom tokenization and are not comparable to the literature. The validation loss refers to the loss achieved on a random held-out part of our dataset. Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in the context when performing the sampling-based evaluations. See Appendix F.4 for full evaluation. Task Metric 6.7B+µP 6.7B re-run 6.7B <ref type="bibr" target="#b6">[7]</ref> 13B <ref type="bibr" target="#b6">[7]</ref> Validation loss cross-entropy</p><p>1.98 2.03 --PTB perplexity 11.4 13.0 --WikiText-103 perplexity 8.56 9.13 --One Billion Words perplexity 20.5 21.7 --LAMBADA Zero-Shot accuracy 73.5 70.8 70.3 72.5 LAMBADA One-Shot accuracy 69.9 64.8 65.4 69.0 LAMBADA Few-Shot accuracy 74.7 77.1 79.1 81.3 HellaSwag Zero-Shot accuracy 72.0 66.7 67.4 70.9 HellaSwag One-Shot accuracy 71.1 65.9 66.5 70.0 HellaSwag Few-Shot accuracy 72.4 66.4 67.3 71.3 0 2000 4000 6000 8000 10000 Training Step 1 2 3 4 5 6 7 8 9 Training Loss P LR=0.001 Width 128 256 512 1024 2048 4096 0 2000 4000 6000 8000 10000 Training Step SP LR=0.001 0 2000 4000 6000 8000 10000 Training Step SP LR=0.00025 Here we trained a GPT-3 transformer with 4 layers and widths from 256 to 32,768. Modulo a brief period around 1e8 training tokens, wider is better throughout training.</p><p>In earlier plots like Figs. <ref type="figure" target="#fig_15">1</ref> and <ref type="figure" target="#fig_3">3</ref>, we saw that at the end of training, wider is always better in µP but not in SP. In fact, we find this to be true throughout training, as seen in Fig. <ref type="figure">7</ref>, modulo noise from random initialization and/or data ordering, and assuming the output layer is zero-initialized (which has no impact on performance as discussed in Appendix D.2). We then stress-tested this on a µP GPT-3 Transformer (on the GPT-3 training data) by scaling width from 256 to 32,768 using a fixed set of HPs (Fig. <ref type="figure">8</ref>). Wider models consistently match or outperform narrower models at each point in training (except a brief period around 1e8 training tokens, likely due to noise because we ran only 1 seed due to computational cost). Our observation suggests that wider models are strictly more data-efficient if scaled appropriately. By checking "wider-is-better" early in training, one can also cheaply debug a µP implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Useful Hyperparameter Transfer: A Theoretical Puzzle</head><p>We want to tune HPs on a small model with width N such that its HP landscape looks like that of a large model with width N . Our intuition in Section 2 and Appendices C and J leads us to µP. However, for this to be useful, we do not want the small model (as a function) after training to be close to that of the large model -otherwise there is no point in training the large model to begin with. So N 1) must be large enough so that the HP optimum converges, but 2) cannot be so large that the functional dynamics (and the loss) converges. The fact that such N exists, as demonstrated by our experiments, shows that: In some sense, the HP optimum is a "macroscopic" or "coarse" variable which converges quickly with width, while the neural network function (and its loss) is a very "microscopic" or "fine" detail that converges much more slowly with width. However, theoretically, it is unclear why this should happen, and where else we should expect such useful HP transfer. We leave an explanation to future work.</p><p>10 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Hyperparameter Tuning</head><p>Many have sought to speedup HP tuning beyond the simple grid or random search. Snoek et al. <ref type="bibr" target="#b45">[45]</ref> treated HP tuning as an optimization process and used Bayesian optimization by treating the performance of each HP combination as a sample from a Gaussian process (GP). Snoek et al. <ref type="bibr" target="#b46">[46]</ref> further improved the runtime by swapping the GP with a neural network. Another thread of work investigated how massively parallel infrasture can be used for efficient tuning under the multi-arm bandit problem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref>. There are also dedicated tools such as Optuna <ref type="bibr" target="#b3">[4]</ref> and Talos <ref type="bibr" target="#b2">[3]</ref> which integrate with existing deep learning frameworks and provide an easy way to apply more advanced tuning techniques.</p><p>Our approach is distinct from all of the above in that it does not work on the HP optimization process itself. Instead, it decouples the size of the target model from the tuning cost, which was not feasible prior to this work. This means that no matter how large the target model is, we can always use a fixed-sized proxy model to probe its HP landscape. Nevertheless, our method is complementary, as the above approaches can naturally be applied to the tuning of the proxy model; it is only for scientific reasons that we use either grid search or random search throughout this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Hyperparameter Transfer</head><p>Many previous works explored transfer learning of HP tuning (e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b62">62]</ref>). However, to the best of our knowledge, our work is the first to explore zero-shot HP transfer. In addition, we focus on transferring across model scale rather than between different tasks or datasets. Some algorithms like Hyperband <ref type="bibr" target="#b22">[23]</ref> can leverage cheap estimates of HP evaluations (like using a small model to proxy a large model) but they are not zero-shot algorithms, so would still be very expensive to apply to large model training. Nevertheless, all of the above methods are complementary to ours as they can be applied to the tuning of our proxy model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Previously Proposed Scaling Rules of Hyperparameters</head><p>(Learning Rate, Batch Size) Scaling <ref type="bibr" target="#b44">[44]</ref> proposed to scale learning rate with batch size while fixing the total epochs of training; <ref type="bibr" target="#b13">[14]</ref> proposed to scale learning rate as √ batchsize while fixing the total number of steps of training. However, <ref type="bibr" target="#b41">[41]</ref> showed that there's no consistent (learning rate, batch size) scaling law across a range of dataset and models. Later, <ref type="bibr" target="#b29">[30]</ref> studied the trade-off of training steps vs computation as a result of changing batch size. They proposed an equation of a/(1 + b/batchsize), where a and b are task-and model-specific constants, for the optimal learning rate (see their fig 3 and <ref type="figure">fig 5</ref>). This law suggests that for sufficiently large batch size, the optimal learning rate is roughly constant. <ref type="foot" target="#foot_13">18</ref> This supports our results here as well as the empirical results in <ref type="bibr">[41, fig 8]</ref>.</p><p>Learning Rate Scaling with Width Assuming that the optimal learning rate should scale with batch size following <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b33">[34]</ref> empirically investigated how the optimal "noise ratio" LR/batchsize scales with width for MLP and CNNs in NTK parametrization (NTP) or standard parametrization (SP) trained with SGD. They in particular focus on test loss in the regime of small batch size and training to convergence. In this regime, they claimed that in networks without batch normalization, the optimal noise ratio is constant in SP but scales like 1/width for NTP. However, they found this law breaks down for networks with normalization.</p><p>In contrast, here we focus on training loss, without training to convergence and with a range of batch sizes from small to very large (as is typical in large scale pretraining). Additionally, our work applies universally to 1) networks with normalization, along with 2) Adam and other adaptive optimizers; furthermore 3) we empirically validate transfer across depth and sequence length, and 4) explicitly validate tuning via µTransfer on large models like BERT-large and GPT-3.</p><p>Finally, as argued in <ref type="bibr" target="#b57">[57]</ref> and Appendix J.3, SP and NTP lead to bad infinite-width limits in contrast to µP and hence are suboptimal for wide neural networks. For example, sufficiently wide neural networks in SP and NTP would lose the ability to learn features, as concretely demonstrated on word2vec in <ref type="bibr" target="#b57">[57]</ref>.</p><p>Input Layer Parametrization The original formulation of µP in <ref type="bibr" target="#b57">[57]</ref> (see Table <ref type="table" target="#tab_13">9</ref>, which is equivalent to Table <ref type="table" target="#tab_12">3</ref>) uses a fan-out initialization for the input layer. This is atypical in vision models, but in language models where the input and output layers are shared (corresponding to word embeddings), it can actually be more natural to use a fan-out initialization (corresponding to fan-in initialization of the output layer). In fact, we found that fairseq <ref type="bibr" target="#b32">[33]</ref> by default actually implements both the fan-out initialization and the √ fan_out multiplier.</p><p>Other Scaling Rules Many previous works proposed different initialization or parametrizations with favorable properties, such as better stability for training deep neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b66">66]</ref>. Our work differs from these in that we focus on the transferability of optimal HPs from small models to large models in the same parametrization.</p><p>10.4 Infinite-Width Neural Networks: From Theory to Practice and Back <ref type="bibr" target="#b57">[57]</ref> introduced µP as the unique parametrization that enables all layers of a neural network to learn features in the infinite-width limit, especially in contrast to the NTK parametrization <ref type="bibr" target="#b16">[17]</ref> (which gives rise to the NTK limit) that does not learn features in the limit. Based on this theoretical insight, in Appendix J.3, we argue that µP should also be the unique parametrization (in the sense of <ref type="bibr" target="#b57">[57]</ref>) that allows HP transfer across width; in short this is because it both 1) preserves feature learning, so that performance on feature learning tasks (such as language model pretraining) does not become trivial in the limit, and 2) ensures each parameter tensor is not stuck at initialization in the large width limit, so that its learning rate does not become meaningless. At the same time, our results here suggest that µP is indeed the correct parametrization for wide neural networks and thus provide empirical motivation for the theoretical study of the infinite-width µP limit. Note, parametrization here refers to a rule to scale hyperparameters with width ("how should my initialization and learning rate change when my width doubles?"), which is coarser than a prescription for setting hyperparameters at any particular width ("how should I set my initialization and learning rate at width 1024?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>Leveraging the discovery of a feature learning neural network infinite-width limit, we hypothesized and verified that the HP landscape across NNs of different width is reasonably stable if parametrized according to Maximal Update Parametrization (µP). We further empirically showed that it's possible to transfer across depth, batch size, sequence length, and training time, with a few caveats. This allowed us to indirectly tune a very large network by tuning its smaller counterparts and transferring the HPs to the full model. Our results raise an interesting new theoretical question of how useful HP transfer is possible in neural networks in the first place.</p><p>Venues of Improvement Nevertheless, our method has plenty of room to improve. For example, initialization does not transfer well across depth, and depth transfer generally still does not work for post-layernorm Transformers. This begs the question whether a more principled parametrization in depth could solve these problems. Additionally, Fig. <ref type="figure" target="#fig_4">4</ref> shows that the optimal HP still shifts slightly for smaller models. Perhaps by considering finite-width corrections to µP one can fix this shift. Finally, it will be interesting to study if there's a way to transfer regularization HPs as a function of both the model size and data size, especially in the context of finetuning of pretrained models.</p><p>Contents 1 Introduction 2 Parametrization Matters: A Primer 3 Hyperparameters Don't Transfer Conventionally 4 Unlocking Zero-Shot Hyperparameter Transfer with µP 5 The Defects of SP and How µP Fixes Them 6 Which Hyperparameters Can Be µTransferred? 6.1 Empirical Validation and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . 7 Efficiency and Performance of µTransfer 7.1 Transformer on IWSLT14 De-En . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Transformer on WMT14 En-De . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Wider is Better in µP Throughout Training 9 Useful Hyperparameter Transfer: A Theoretical Puzzle 10 Related Works 10.1 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Hyperparameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Previously Proposed Scaling Rules of Hyperparameters . . . . . . . . . . . . . . . 10.4 Infinite-Width Neural Networks: From Theory to Practice and Back . . . . . . . . 11 Conclusion A Parametrization Terminologies B Further Explanations of the µP Tables B.1 Walkthrough of µP Implementation in a Transformer . . . . . . . . . . . . . . . . B.2 Other Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Optimizer Variants and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . C Parametrization Matters: A Primer for Multiple Hyperparameters D Practical Considerations D.1 Verifying µP Implementation via Coordinate Checking . . . . . . . . . . . . . . . D.2 Zero Initialization for Output Layers and Query Layers in Attention . . . . . . . . D.3 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Illustration of µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 SP vs µP for MLPs on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Empirical validation of the stability of four representative hyperparameters on pre-LN Transformers in µP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Activations blow up in SP but maintain a consistent scale in µP . . . . . . . . . . . 6 Efficiency-performance Pareto frontier of µTransfer . . . . . . . . . . . . . . . . 7 Wider is always better in training loss under µP, but not in SP, given the same HP . 8 Stress-testing "wider-is-better" in µP . . . . . . . . . . . . . . . . . . . . . . . . . 9 Squashing activation functions reduce transfer quality. . . . . . . . . . . . . . . . 10 Enlarging d k makes µTransfer more precise in Transformers . . . . . . . . . . . . 11 Schematics of each Transformer layer . . . . . . . . . . . . . . . . . . . . . . . . 12 Width ratio can be varied arbitrarily in µTransfer . . . . . . . . . . . . . . . . . . 13 µTransfer can handle increasing n head while fixing d head as well as increasing d head while fixing n head , or a mix of both . . . . . . . . . . . . . . . . . . . . . . . . . 14 Results of the random search over reduced-width GPT-3 proxy models . . . . . . . 15 The training curves of the GPT-3 6.7B model with µTransfer and a re-run with the original settings from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Verifying µP hyperparameter stability on ResNet . . . . . . . . . . . . . . . . . . 17 Verifying hyperparameter stability under µP for Post-LN Transformers . . . . . . . 18 µTransfer vs naive transfer for post-layernorm Transformers on Wikitext-2 . . . . . 19 Empirical validation of µTransfer across Batch Size, Sequence Length, and Training Time on pre-LN Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Learning rate landscape is highly unstable under standard parametrization in IWSLT 21 Replicating training instability issue on a small Transformer by reverse-µtransferring hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables 1 Hyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred Across 2 Examples of µTransferable Hyperparameters . . . . . . . . . . . . . . . . . . . . 3 µP[57] and SP for General Neural Networks . . . . . . . . . . . . . . . . . . . . . 4 µTransfer results for Transformer on IWSLT14 De-En . . . . . . . . . . . . . . . 5 µTransfer results for Transformer on WMT14 En-De . . . . . . . . . . . . . . . . 6 µTransfer results for BERT pretraining . . . . . . . . . . . . . . . . . . . . . . . . 7 µTransfer results for GPT-3 pretraining . . . . . . . . . . . . . . . . . . . . . . . 8 Alternative (Equivalent) µP Formulation for Easier Implementation . . . . . . . . 9 µP Formulation in the Style of [57] . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Full evaluation results of our GPT-3 6.7B models . . . . . . . . . . . . . . . . . . 11 Our µTransferred GPT-3 6.7B model performs comparably to the twice-as-large GPT-3 13B model from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 µTransfer results for ResNet on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . 13 µTransfer results for Wide ResNet on ImageNet . . . . . . . . . . . . . . . . . . . 14 Expected output size of matrix multiplication between different types of random matrices and a random vector, as preparation for deriving µP . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Parametrization Terminologies</head><p>This section seeks to make formal and clarify some of the notions regarding parametrization discussed informally in the main text. Definition A.1 (Multiplier and Parameter Multiplier). In a neural network, one may insert a "multiply by c" operation anywhere, where c is a non-learnable scalar hyperparameter. If c = 1, then this operation is a no-op. This c is called a multiplier.</p><p>Relatedly, for any parameter tensor W in a neural network, we may replace W with cW for some non-learnable scalar hyperparameter c. When c = 1, we recover the original formulation. This c is referred to as a parameter multiplier.</p><p>For example, in the attention logit calculation k, q / √ d head where q = W x, the 1/ √ d head factor is a multiplier. It may also be thought of as the parameter multiplier of W if we rewrite the attention logit as k, (W/ √ d head )x .</p><p>Note parameter multipliers cannot be absorbed into the initialization in general, since they affect backpropagation. Nevertheless, after training is done, parameter multipliers can always be absorbed into the weight. Definition A.2 (Parametrization). In this work, a parametrization is a rule for how to change hyperparameters when the widths of a neural network change, but note that it does not necessarily prescribes how to set the hyperparameters for any specific width. In particular, for any neural network, an abc-parametrization is a rule for how to scale a) the parameter multiplier, b) the initialization, and c) the learning rate individually for each parameter tensor as the widths of the network change, as well as any other multiplier in the network; all other hyperparameters are kept fixed with width.</p><p>For example, SP and µP are both abc-parametrizations. Again, we note that, in this sense, a parametrization does not prescribe, for example, that the initialization variance be 1/fan_in, but rather that it be halved when fan_in doubles. Definition A.</p><p>3 (Zero-Shot Hyperparameter Transfer). In this work, we say a parametrization admits zero-shot transfer of a set of hyperparameters H w.r.t. a metric L if the optimal combination of values of H w.r.t. L converges as width goes to infinity, i.e. it stays approximately optimal w.r.t. L under this parametrization as width increases.</p><p>Throughout this paper, we take L to be the training loss, but because regularization is not the bottleneck in our experiments (especially large scale pretraining with BERT and GPT-3), we nevertheless see high quality test performance in all of our results. We also remark that empirically, using training loss as the metric can be more robust to random seed compared to validation loss and especially BLEU score. See Table <ref type="table" target="#tab_1">1</ref>(left) for H. By our arguments in Appendix J.3 and our empirical results, µP is the unique abc-parametrization admitting zero-shot transfer for such H and L in this sense.</p><p>More generally, one may define a K-shot transfer algorithm of a set of hyperparameters H w.r.t. a metric L as one that 1) takes width values n and n and an approximately optimal combination of values of H w.r.t. L at a width n and 2) returns an approximately optimal combination of values of H w.r.t. L at width n , given 3) a budget of K evaluations of candidate hyperparameter combinations on models of width n . However, we will have no use for this definition in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Further Explanations of the µP Tables</head><p>In addition to Table <ref type="table" target="#tab_12">3</ref>, we provide Table <ref type="table">8</ref> as an equivalent µP formulation that is easier to implement, as well as Table <ref type="table" target="#tab_13">9</ref> for those more familiar with the original µP formulation in <ref type="bibr" target="#b57">[57]</ref>. Below, we provide some commentary on corner cases not well specified by the tables. Ultimately, by understanding Appendix J, one can derive µP for any architecture, new or old.</p><p>Matrix-Like, Vector-Like, Scalar-Like Parameters We can classify any dimension in a neural network as "infinite" if it scales with width, or "finite" otherwise. For example, in a Transformer, d model , d f f n , d head , n head are all infinite, but vocab size and context size are finite. Then we can categorize parameter tensors by how many infinite dimensions they have. If there are two such dimensions, then we say the parameter is matrix-like; if there is only one, then we say it is vector-like; if there is none, we say it is scalar-like. Then in Tables 3, 8 and 9, "input weights &amp; all biases" and "output weights" are all vector-like parameters, while hidden weights are matrix-like parameters. An Table <ref type="table">8</ref>: Alternative (Equivalent) µP Formulation for Easier Implementation. Same format as in Table <ref type="table" target="#tab_12">3</ref>. In contrast to the formulation in Table <ref type="table" target="#tab_12">3</ref>, here all "vector-like" parameters (i.e. those that have only one dimension tending to infinity), including input and output weights and biases, have the same width scaling for initialization variance and SGD/Adam LR (note the 1/fan_in for input weight/bias init. var. is Θ(1) in width). This has two benefits in practice: 1) implementation is unified and simplified for all "vector-like" parameters; 2) input and output weights can now be tied, in contrast to </p><formula xml:id="formula_11">/ √ fan_in (1) 1 SGD LR 1 1 1 Adam LR 1 / √ fan_out<label>(1) 1 /</label></formula><p>√ fan_in</p><p>(1)</p><formula xml:id="formula_12">1 /fan_in (1)</formula><p>advantage of Table <ref type="table">8</ref> is that it gives a uniform scaling rule of initialization and learning rate for all vector-like parameters. The multiplier rule in Table <ref type="table">8</ref> can be more interpreted more generally as the following: a multiplier of order 1/fan_in should accompany any weight that maps an infinite dimension to a finite one. This interpretation then nicely covers both the output logits and the attention logits (i.e. 1/d attention).</p><p>Scalar-like parameters are not as common as matrix-like and vector-like ones, but we will mention a few examples in Appendix B.2. The scaling rule for their initialization, learning rate (for both SGD and Adam), and multiplier is very simple: hold them constant with width.</p><p>Initialization Mean We did not specify the initialization mean in the tables, since most commonly the mean is just set to 0, but it can be nonzero for vector-like parameters (e.g., layernorm weights) and scalar-like parameters but must be 0 for matrix-like parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zero Initialization Variance</head><p>The initialization scaling rules in our tables can all be trivially satisfied if the initialization variance is set to 0. This can be useful in some settings (e.g., Appendix D.2) but detrimental in other settings (e.g., hidden weights).</p><p>What Are Considered Input Weights? Output Weights? Here, input weights very specifically refer to weights that map from an infinite dimension to a finite dimension. As a counterexample, in some architectures, the first layer can actually map from a finite dimension to another finite dimension, e.g., a PCA layer. Then this is not an "input weight"; if the next layer maps into an infinite dimension, then that's the input weight. A similar, symmetric discussion applies to output weights.</p><p>What Counts As a "Model"? Does the MLP in a Transformer Count As a "Model"? For our tables, a model is specifically a function that maps a finite dimension to another finite dimension, consistent with the discussion above. For example, for an image model on CIFAR10, it maps from 3 × 32 × 32 = 3072 dimensions to 10 dimensions, and these numbers are fixed regardless of the width of the model. Likewise, for an autoregressive Transformer model, the input and output dimension are both the vocab size, which is independent of the width. In contrast, an MLP inside a Transformer is not a "model" in this sense because its input and output dimension are both equal to the width of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Walkthrough of µP Implementation in a Transformer</head><p>To ground the abstract description in Tables 3, 8 and 9, we walk through the parameters of a typical Transformer and discuss concretely how to parametrize each.</p><p>We assume that the user wants to replicate SP when the model widths are equal to some base widths, for example, when Below, we introduce hyperparameters σ • , η • for each parameter tensor, as well as a few multipliers α • . One may always tie σ • (resp. η • ) across all parameter tensors, but in our experiments, we found it beneficial to at least distinguish the input and output layer initialization and learning rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Word Embeddings</head><p>The input word embedding matrix W wordemb has size d model × vocabsize, where vocabsize is the fan-in and d model is the fan-out. Follow the "input weight &amp; all biases" column in Tables 3, 8 and 9. For example, for Tables <ref type="table" target="#tab_12">3</ref> and <ref type="table">8</ref>,</p><formula xml:id="formula_13">W wordemb ∼ N (0, σ<label>2</label></formula><p>wordemb ), with Adam LR η wordemb Note here, because fan-in (vocabsize) here is independent of width (d model ), the "1/fan_in" for the initialization variance in these tables is equivalent to "1", i.e. the initialization variance can be anything fixed with width. In this case of the word embedding, setting the variance to 1, for example, is more natural than setting the variance to 1/fan_in, because the embedding is one-hot (1/fan_in would be more natural for image inputs).</p><p>Positional Embeddings The (absolute or relative) positional embedding matrix W posemb has size d model × contextsize, where contextsize is the fan-in and d model is the fan-out. With the same discussion as above for input word embeddings, follow the "input weight &amp; all biases" column in Tables 3, 8 and 9. For example, for Tables <ref type="table" target="#tab_12">3</ref> and <ref type="table">8</ref>,</p><formula xml:id="formula_14">W posemb ∼ N (0, σ 2 posemb ), with Adam LR η posemb</formula><p>Layernorm Weights and Biases Layernorm weights w LN and biases b LN both have shape d model and can be thought of "input weights" to the scalar input of 1. Hence one should follow the "input weight &amp; all biases" column in Tables 3, 8 and 9. In particular, the usual initialization of layernorm weights as all 1s and biases as all 0s suffice (where the initialization variance is 0). For example, for Tables <ref type="table" target="#tab_12">3</ref> and <ref type="table">8</ref>, Self-Attention There are 4 matrices, W q , W k ∈ R (d k n head )×d model , W v ∈ R (dvn head )×d model , and W o ∈ R d model ×(dvn head ) (where the shapes are R fan_out×fan_in ). Since d model , (d k n head ), and (d v n head ) all scale with width (where the latter two are commonly just set to d model ), all 4 matrices should be parametrized according to the "hidden weights" column in Tables 3, 8 and 9. For example, for Tables <ref type="table" target="#tab_12">3</ref> and <ref type="table">8</ref>,</p><formula xml:id="formula_15">w LN ← 1,</formula><formula xml:id="formula_16">W q ∼ N (0, σ 2 q /d model ), with Adam LR η q / dmodel W k ∼ N (0, σ 2 k /d model ), with Adam LR η k / dmodel W v ∼ N (0, σ 2 v /d model ), with Adam LR η v / dmodel W o ∼ N (0, σ 2 o /(d v n head )),</formula><p>with Adam LR η o /( dv ñhead ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Logit Scaling</head><p>We use 1/d instead of 1/ √ d attention. To be compatible with 1/ √ d attention when at a particular base d head = d head,0 , we set</p><formula xml:id="formula_17">AttnLogit = α attn d head,0 d head q k,</formula><p>where α attn is a tunable multiplier.</p><p>MLP There are 2 matrices, W 1 ∈ R d f f n ×d model , W 2 ∈ R d model ×d f f n (where the shapes are R fan_out×fan_in ), where d f f n is commonly set to 4d model . Since both d model , d f f n scale with width, both matrices are considered "hidden weights." For example, for Tables <ref type="table" target="#tab_12">3</ref> and <ref type="table">8</ref>,</p><formula xml:id="formula_18">W 1 ∼ N (0, σ 2 q /d model ), with Adam LR η q / dmodel W 2 ∼ N (0, σ 2 k /d f f n ), with Adam LR η k / dffn</formula><p>Word Unembeddings Symmetric to the discussion on input word embeddings, the output word unembeddings should be parametrized according to the "output weights" column of Tables <ref type="table" target="#tab_12">3, 8</ref> and <ref type="table" target="#tab_13">9</ref>.</p><p>Often, the unembeddings are tied with the embeddings, and Tables <ref type="table">8</ref> and <ref type="table" target="#tab_13">9</ref> allow for this as their initialization schemes are symmetric between input and output weights.</p><p>For example, for Table <ref type="table" target="#tab_12">3</ref>, we'd set</p><formula xml:id="formula_19">W unemb ∼ N (0, σ 2 unemb /(d model dmodel )), with Adam LR η unemb / dmodel .</formula><p>For Table <ref type="table">8</ref>, we would instead have</p><formula xml:id="formula_20">W unemb ∼ N (0, σ 2 unemb /d model,0</formula><p>), with Adam LR η unemb , (note d model,0 here is the base width and therefore is a constant) and the output is computed as</p><formula xml:id="formula_21">logits = α output dmodel W unemb z</formula><p>where z is the final layer embedding of a token, and α output is a tunable multiplier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Other Parameters</head><p>Learnable scalar multipliers For learnable scalar multipliers (e.g., softmax inverse temperature), one can initialize them to 1 and use a constant (in width) learning rate for both SGD and Adam. This is compatible with Tables 3, 8 and 9.</p><p>Positional Bias Some Transformers use positional bias (of size contextsize × contextsize, which are added to the attention logits). They are considered "scalar-like" in that it has no width dimension. One can initialize them to 0 and use a constant (in width) learning rate for both SGD and Adam. This is compatible with Tables 3, 8 and 9.</p><p>Spatial MLPs Recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b49">49]</ref> on MLP-only architectures in NLP and CV replace the self-attention layer in Transformers with MLPs across tokens or spatial locations. In our language here, such MLPs have finite input and output dimensions (the context size) and infinite hidden dimensions, so their input, output, and hidden weights should be parametrized via the corresponding columns in Tables 3, 8 and 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Optimizer Variants and Hyperparameters</head><p>AdamW Exactly the same as Adam in all of our tables, with the added benefit that weight decay is automatically scaled correctly in AdamW (but is incompatible with µP Adam). For this reason, we recommend using AdamW when weight decay is desired (which is consistent with current standard practice).</p><p>Frobenius Normalization LARS <ref type="bibr" target="#b63">[63]</ref>, Adafactor <ref type="bibr" target="#b42">[42]</ref>, Lamb <ref type="bibr" target="#b64">[64]</ref>, Layca <ref type="bibr" target="#b7">[8]</ref>, Fromage <ref type="bibr" target="#b5">[6]</ref>, Nero <ref type="bibr" target="#b27">[28]</ref> all involve a normalization step in which the update g (which may be obtained from SGD, Adam, or other optimzers) is normalized to have Frobenius norm equal to that of the parameter w: g ← w F g F g. They can be made compatible with µP in Table <ref type="table">8</ref> by scaling their learning rate for hidden weights like 1/ √ fan_in (for Table <ref type="table" target="#tab_12">3</ref>, the output weight learning rate should be likewise scaled). The intuitive reasoning (which can be formalized straightforwardly using Tensor Programs) is as follows.</p><p>This normalization implicitly encodes a width scaling: If one initializes a weight matrix with variance 1/fan_in, then an n × n matrix (e.g., a hidden weight matrix) has Frobenius norm √ n at initialization. Thus, in the first step and, by induction, in any step t, the normalized update to this n × n weight also has Frobenius norm Θ( √ n) (for any fixed t, as n → ∞). Heuristically, this means each entry of g is approximately of size Θ(1/ √ n). But, by the derivation of Appendix J, we want Θ(1/n) and this is Θ( √ n) too large! Thus, in wide enough networks, one should see a network blowup after one update, like demonstrated in Fig. <ref type="figure">5</ref>.</p><p>However, note that the Θ(1/ √ n) coordinate size induced by the normalization here is closer to the right size Θ(1/n) than Adam, whose update have coordinate size Θ(1). This may partially explain the apparent benefit of these optimizers. In particular, this may explain the observation that T5 <ref type="bibr" target="#b38">[38]</ref>, using Adafactor, was able to train its entire range of models from 220 million to 11 billion parameters with a fixed set of hyperparameters, while GPT-3 <ref type="bibr" target="#b6">[7]</ref>, using Adam, needed to decrease its learning rate with model size.</p><p>RAdam RAdam <ref type="bibr" target="#b24">[25]</ref> is a variant of Adam that uses SGD with momentum in an initial stage with learning rate warmup, followed by a second stage of Adam with a particular setting of learning rate with time. Thus, one can adapt RAdam to µP by individually scaling the learning rates of the initial SGD stage and the final Adam stage according to Table <ref type="table" target="#tab_12">3</ref>, Table <ref type="table">8</ref>, or Table <ref type="table" target="#tab_13">9</ref>.</p><p>Adagrad and RMSProp Exactly the same as Adam in all of our tables.</p><p>in Adam and Its Variants All of our derivations here assume is negligible in Adam. If it is set to a non-negligible number, then it needs to be scaled, for all parameters, like 1/fan_in 2 if it is added before the square root, or like 1/fan_in if it is added after the square root.</p><p>Gradient Clipping Gradient ( 2 -norm-wise) clipping is compatible with Table <ref type="table" target="#tab_12">3</ref> (as well as Tables <ref type="table">8</ref> and <ref type="table" target="#tab_13">9</ref>), for either SGD or Adam, if the clip value is held constant with respect to width.</p><p>Weight Decay Weight decay should be scaled independently of width in SGD and AdamW, for all of our tables. However, note it's not compatible with µP Adam.</p><p>Momentum Momentum should be scaled independently of width for all of our tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Parametrization Matters: A Primer for Multiple Hyperparameters</head><p>Here we give more intuition why we need to reparametrize all hyperparameters. In practice, neural networks have multitudes of hyperparameters all interacting together. In our example of Section 2, hyperparameter optimization would be akin to minimizing the function<ref type="foot" target="#foot_14">foot_14</ref> </p><formula xml:id="formula_22">F n (c 1 , . . . , c k ) def = E x1,...,xn f ((c 1 + • • • + c k )(x 1 + • • • + x n )).</formula><p>where x 1 , . . . , x n are as in Eq. ( <ref type="formula" target="#formula_1">1</ref>) and c 1 , . . . , c k are analogous to k hyperparameters. For the same reasoning in Section 2, the correct parametrization is in (α 1 , . . . , α k ) where</p><formula xml:id="formula_23">α i = c i √ n.</formula><p>While this is straightforward, in practice, researchers often fix some hyperparameters (e.g., they tune only learning rate but neglects to scale parameter multipliers or initialization correctly). For example, if we only partially reparametrize and optimize in α 1 while fixing c 2 , . . . , c k , then the optimal α 1 is</p><formula xml:id="formula_24">(α 1 ) * = α * -(c 1 + . . . + c k ) √</formula><p>n where α * is the optimal α for Eq. ( <ref type="formula" target="#formula_1">1</ref>). Thus, as n → ∞, (α 1 ) * still blows up even though we parametrized α 1 correctly. More generally, the incorrect parametrization of some hyperparameters forces other hyperparameters to increasingly compensate for it as width grows, distorting their optima, even if the latter are correctly parametrized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Practical Considerations</head><p>In this section, we outline several useful tips and tricks that can improve the quality of hyperparameter transfer in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Verifying µP Implementation via Coordinate Checking</head><p>Even though µP is neatly encapsulated by Table <ref type="table" target="#tab_12">3</ref>, implementing it correctly can in practice be error-prone, just like how implementing autograd by hand can be error-prone even though the math behind is just chain-rule. In the case of autograd, gradient checking is a simple way of verifying implementation correctness; similarly, we propose coordinate checking to verify the correctness of µP implementation: Exemplified by Fig. <ref type="figure">5</ref>, one calculates the average coordinate size of every (pre)activation vector in the network over a few steps of training, as width is varied over a large range. An incorrect implementation will see some activation vector blow up or shrink to zero with width (like in the top row of Fig. <ref type="figure">5</ref>). In the mup package we release with this paper, we include an easy-to-use method for coordinate checking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Zero Initialization for Output Layers and Query Layers in Attention</head><p>We find that the optimal hyperparameters of small and large width models match more closely when we initialize output layers at 0 (i.e. with variance σ 2 /fan_in where σ = 0 instead of positive σ). This is because the neural network in µP is approximately a Gaussian process (GP) at initialization with variance on the order Θ(σ 2 /width) (contrast this with SP networks, which approximates a GP with Θ(σ 2 ) variance) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b57">57]</ref>. Of course, when width is large, this variance vanishes, but this can be far from so in the small proxy model. This discrepancy in the initial GP can cause the training trajectory of the proxy model to be very different from the trajectory of the large target model, causing a mismatch in the optimal hyperparameters. By initializing the output layer at 0, we remove this mismatch in the initial GP. Empirically we do not find this modification to be detrimental to performance.</p><p>A similar consideration applies to the query layer in self-attention: At initialization, the attention logit q k/d head looks like a Gaussian with variance Θ(1/d head ) because q and k are almost independent and zero-mean. In the limit d head → ∞, the logit is exactly 0, which can be a large discrepancy compared to when d head is small in the small proxy model we want to tune. By initializing the query projection matrix W q to 0, q will also be 0, and hence the attention logit is always 0 at initialization regardless of width (but will generally become nonzero after a gradient step), resolving this discrepancy.</p><p>More generally, any layer or computation that goes from an "infinite" dimension (i.e. width) to a "finite" dimension (e.g. output dimension or sequence length) can exhibit this kind of discrepancy due to the initial GP. When d head → ∞ and n head is fixed, attention logit calculation can be viewed in the same vein as a function R seqlen×d model → R n head ×seqlen×seqlen , which "reduces to" R ∞ → R 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Activation Functions</head><p>14 12 10 8 6 log2LearningRate 0.50 0.75 1.00 1.25 1.50 1.75 Training Loss SP / tanh / xent 256 512 1024 2048 4096 8192 14 12 10 8 6 log2LearningRate 0.50 0.75 1.00 1.25 1.50 1.75 P / tanh / xent 15 10 5 0 log2LearningRate 0.02 0.04 0.06 0.08 0.10 SP / tanh / mse 15 10 5 0 log2LearningRate 0.02 0.04 0.06 0.08 0.10 P / tanh / mse Figure 9: Squashing activation functions reduce transfer quality. MLP of different hidden sizes with tanh activation trained for 20 epoch on CIFAR-10 using SGD. Left uses cross-entropy as loss function; right uses mean squared error; columns alternate between standard parametrization (SP) and maximal update parametrization (µP). Compared to ReLU, tanh exhibits slower convergence for µP, yet it still outperforms SP when width is increased</p><p>When the network is narrow, its approximation to the infinite-width behavior becomes crude, which is manifested as large fluctuations in preactivation coordinates. When using a squashing activation functions like softmax or tanh, this causes narrower networks to saturate the activation more than wider ones, which results in a systematic bias toward small gradients and therefore distorting the hyperparameter landscape. This can be seen in Fig. <ref type="figure">9</ref>, where we use tanh as the network activation function. Therefore, we recommend replacing non-essential squashing activation functions with ReLU, whose derivative depends only on the sign of the pre-activation. A similar reasoning can be applied to superlinear activation functions, where the distribution of activation values can have heavy tails, leading to slow convergence to the infinite-width limit. However, such activations are rarely used in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Enlarge d k</head><p>We find that small d head = d k can lead to a highly noisy HP landscape, as shown in Fig. <ref type="figure" target="#fig_8">10</ref>. This can significiantly decrease the quality of random HP search on the small proxy model. To solve this, we find it useful to decouple d </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Non-Gaussian vs Gaussian Initialization</head><p>We find non-Gaussian (e.g. uniform) initialization can sometimes cause wider models to perform worse than narrower models, whereas we do not find this behavior for Gaussian initialization. This is consistent with theory, since in the large width limit, one should expect non-Gaussian initialization to behave like Gaussian initializations anyway (essentially due to Central Limit Theorem, or more precisely, universality), but the non-Gaussianity slows down the convergence to this limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Using a Larger Sequence Length</head><p>For Transformers, we empirically find that we can better transfer initialization standard deviation from a narrower model (to a wide model) if we use a larger sequence length. It is not clear why this is the case. We leave an explanation to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.7 Tuning Per-Layer Hyperparameters</head><p>The techniques in this paper allow the transfer across width of (learning rate, initialization, multipliers) simultaneously for all parameter tensors. Thus, to get the best results, one should ideally tune all such hyperparameters. In practice, we find that just tuning the global learning rate and initialization, along with input, output, and attention multipliers, yield good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Which Hyperparameters Can Be Transferred? (Continued) E.1 Further Discussions on Hyperparameter Categories</head><p>Below, we discuss the reasoning behind each kind, which are supported by our empirical evidence collected in Fig. <ref type="figure" target="#fig_4">4</ref> on Transformers as well as those in Appendix G.1 on ResNet.</p><p>Transferable Hyperparameters In Table <ref type="table" target="#tab_2">2</ref>, we summarize which HPs can be transferred across training scale. The transfer across width, as explained in Section 2, is theoretically justified, while we present the transfer across the other dimensions as empirical results.</p><p>These cover most of the well-known and important HPs when the need for regularization is not paramount, e.g., during large scale language model pretraining. Parameter Multipliers are not wellknown HPs, yet we include them here as they serve a bridge between SP and µP and can impact model performance in practice. Concretely, any SP and µP neural networks of the same width can have their Parameter Multipliers tuned so that their training dynamics become identical.</p><p>Hyperparameters That Don't Transfer Well Not all HPs transfer well even if we use µP. In particular, those whose primary function is to regularize training to mitigate "overfitting" tend not to transfer well. Intuitively, regularization needs to be applied more heavily in larger models and when data is scarce, but µP does not know the data size so cannot adjust the regularization accordingly.</p><p>To the best of our knowledge, there is no strict separation between HPs that regularize and those that don't. However, conventional wisdom tells us that there exists a spectrum of how much regularizing effect a HP has. For example, dropout probability and weight decay are among those whose primary function is to regularize, whereas batch size and learning rate might regularize training in some cases but affect the dynamics more so in other ways. Our empirical exploration tells us that the former do not transfer well, while the latter do. Our subsequent discussion will focus on the latter; we leave to future works the expansion to the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters Transfered Across</head><p>We have left out a category of HPs that defines the training scale, or in practical terms, training cost. This includes 1) those that define how many operations a model's forward/backward pass takes, such as the model's width, depth, and in the case of language modeling, sequence length; and 2) those that define how many such passes are performed, such as batch size and number of training steps.</p><p>As recent works have shown <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">39]</ref>, improvements along any of these scale dimensions lead to apparently sustainable gain in performance; as a result, we are primarily interested in transferring other HPs across these dimensions that define scale, rather than finding the optimal scale. <ref type="foot" target="#foot_15">20</ref> This category of HPs is particularly crucial as one can speedup training by downsizing in one or multiple such dimensions. Indeed, it's very common for practitioners to implicitly transfer HPs across the number of training samples by tuning on only a subset of the full training data.</p><p>Our insights from the infinite-width limit inspired us to explore HP tranfer across width, which does not work under SP as we have shown earlier. Building upon our success with width, which is well explained theoretically, we hope to push the limit of compute-saving by investigating the other dimensions empirically. To the best of our knowledge, the transferability of optimal HPs across depth, batch size, sequence length, and training time has not been rigorously investigated previously, with the main exception of the literature on (learning rate, batch size) scaling <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b44">44]</ref> where our transferability result of learning rate across batch size recapitulates <ref type="bibr" target="#b29">[30]</ref>. <ref type="foot" target="#foot_16">21</ref> See Section 10.3 on how our results relate to prior works. We will primarily focus on the Transformer architecture in the main text with evidence for ResNet in Appendix G.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 On the Definitions of Width</head><p>Our theory allows more general notions of width. This is especially relevant in Transformers, where d model , d head = d k , d v , n head , d f f n (see Fig. <ref type="figure" target="#fig_15">11</ref>) can all be construed as measures of width.  We briefly discuss these here, with more theoretical justification in Appendix J.2.1 and empirical validation below.</p><p>Varying Width Ratio So far we have assumed that every hidden layer is widened by the same factor. But in fact we can widen different hidden layers differently. This is useful, for example, in a Transformer where we may want to use a smaller d f f n during tuning. If we are using Adam, as long as the width of every layer still tends to infinity, we still obtain approximately the same limit <ref type="foot" target="#foot_17">22</ref> , so the µTransfer remains theoretically justified.</p><p>See Fig. <ref type="figure" target="#fig_15">12</ref> for an empirical validation on IWSLT-14 using a Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Attention Heads</head><p>In attention-based models, one typically splits hidden size into multiple attention heads following d model = d head × n head . So far we have assumed d head and d model to be width, but it's possible and potentially advantageous to fix d head and treat n head as the width, or increasing both simultaneously. This allows our technique to handle many popular models, including GPT-3 <ref type="bibr" target="#b6">[7]</ref>, which scale up by fixing d head and increasing n head . See Fig. <ref type="figure" target="#fig_12">13</ref> for an empirical validation on Wikitext-2.</p><p>Varying Just the Width of Attention Heads A specific useful instance of varying width ratio is decoupling the key and value dimensions d k and d v and scaling d k differently from (typically larger √ d as is done commonly). When tuning on the small proxy model, if d k is too small, the HP landscape can be quite noisy. Keeping d k relatively large while shrinking all other dimensions solves this problem, while still obtaining significant speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Experimental Details</head><p>F.1 IWSLT IWSLT14 De-En is a well-known machine translation benchmark. We use a Transformer implemented in fairseq <ref type="bibr" target="#b32">[33]</ref> with a default</p><formula xml:id="formula_25">d model = 1 /4d f f n = 512 and d k = d q = d v = d model/n head = 128</formula><p>(amounting to 40M parameters), which we denote as the 1x model. For transfer, we tune on a proxy model with the same n head but with d model and other dimensions 4 times smaller; we will call this the 0.25x model (but it has 4M parameters). All models are trained with Adam for 100 epochs and validated at the end of every epoch. We tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid • η: 5 × 10 -4 × 2 z , where z ∈ {-1.5, -1.25, -1, ..., 1.25}</p><p>• α output : 2 z , where z ∈ {-8, -7, -6, ..., 7}</p><p>• α attn : 2 z , where z ∈ {-3, -2, -1, ..., 8}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 WMT</head><p>We scale up to WMT14 En-De using the large Transformer from <ref type="bibr" target="#b50">[50]</ref>, with a d model = 1 /4d f f n = 1024 and d q = d k = d v = d model/n head = 64. We use the exact same setup and reproduce their result as our baseline. Then, we build the proxy model by shrinking the target model's d model from the original 1024 to 256, d f f n from 4096 to 256 and n head from 16 to 4. This reduces the total parameter count from 211M to 15M. We then perform the HP search on the proxy model and take the best according to validation loss, before testing on the target model. We tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid</p><formula xml:id="formula_26">• η: 6 × 10 -4 × 2 z , where z ∈ {-1.5, -1.25, -1, ..., 1.25}</formula><p>• α output : 2 z , where z ∈ {-8, -7, -6, ..., 7}</p><p>• α attn : 2 z , where z ∈ {-3, -2, -1, ..., 8} F.3 BERT Details of BERT Prototype Our proxy model has 10 Transformer layers with d model = d f f n = 256. We also reduce the number of attention heads to 8 with a d head of 32. We call it BERT Prototype since we can increase its width and depth according to our definitions to recover both BERT Base and BERT Large, which enables us to sweep HPs once and use for both models. Overall, BERT Prototype has 13M trainable parameters, a fraction of the 110M in BERT Base and the 350M in BERT Large.</p><p>Hyperparameters Tuned for Pretraining We tune the following HPs for pretraining: Adam learning rate η, embedding learning rate η emb , output weight multiplier α output , attention logits multiplier α attn , layernorm gain multiplier α LNgain , and bias multiplier α bias .</p><p>We sample 256 combinations from the follow grid:</p><p>• η: 1 × 10 -4 × 2 z , where z ∈ {1.5, 2, 2.5, 3, 3.5}</p><p>• η emb : 1 × 10 -4 × 2 z , where z ∈ {-1, -0.5, 0, 0.5, 1} • α output : 2 z , where z ∈ {2, 4, 6} • α attn : 2 z , where z ∈ {3, 3.5, 4, ..., 7} • α LNgain : 2 z , where z ∈ {8.5, 9, 9.5, 10, 10.5}</p><p>• α bias : 2 z , where z ∈ {8.5, 9, 9.5, 10, 10.5}</p><p>The ranges are chosen to include the implicit choices of these HPs in SP BERT Large.</p><p>Finetuning Procedure and Hyperparameters We hand-pick the finetuning HPs after training the full-sized model. As regularization is an essential ingredient in successful finetuning, we do not transfer such HPs (at least via the suite of techniques presented in this work) (see Table <ref type="table" target="#tab_1">1</ref>). We focus on MNLI <ref type="bibr" target="#b52">[52]</ref> and QQP, which are two representative tasks from GLUE <ref type="bibr" target="#b51">[51]</ref>. Following <ref type="bibr" target="#b26">[27]</ref>, we used Adam <ref type="bibr" target="#b19">[20]</ref> with a learning rate of 5 × 10 -5 and a batch size of 64. The maximum number of epochs was set to 5. A linear learning rate decay schedule with warm-up of 0.1 was used. All the texts were tokenized using wordpieces and were chopped to spans no longer than 128 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 GPT-3</head><p>Baseline 6.7B GPT-3 Transformer As the GPT-3 codebase has evolved since the publication of <ref type="bibr" target="#b6">[7]</ref>, we re-trained the 6.7B model from scratch to remove changes in our codebase as a possible confounder. The main differences to <ref type="bibr" target="#b6">[7]</ref> are 1) a modified learning rate decay schedule, where the learning rate is decayed to zero at the end of training rather than being decayed to 0.1 of the initial value, and 2) use of relative attention in place of absolute attention. Unfortunately, after all experiments were finished, we found this re-run baseline used absolute attention instead of relative attention, while the µTransfer model still used relative attention.</p><p>Random Search using Reduced-Width Proxy Model In order to find a good set of hyperparameters for the µTransfer version of the 6.7B model, we performed a hyperparameter search over a reduced version of the model (i.e., the proxy model), where the width is set to 256 hidden units. This proxy model inherits changes from the evolved GPT-3 codebase: it uses relative <ref type="bibr" target="#b9">[10]</ref> (instead of absolute) position encoding. Early on, we noted that on the proxy model, linear learning rate decay outperformed the default cosine schedule, so all subsequent experiments for the proxy models use a linear decay schedule. By Fig. <ref type="figure" target="#fig_4">4</ref>, µTransferring this linear decay schedule to the full model should maintain such a performance advantage over the cosine schedule.</p><p>The hyperparameter search space consists of the following hyperparameters:</p><p>• learning rate: Sampled from 10 Uniform(-4,-1)</p><p>• initialization scale: All the parameters are multiplied -sampled from 10 Uniform(-1,1) • attention temperature: Reciprocal of the multiplier applied to the input to attention softmax. Sampled from 4 Uniform(-1,1) . • output temperature: Reciprocal of the multiplier applied to the input to softmax that produces the distribution over output tokens. Sampled from 4 Uniform(-1,1) . • embedding multiplier: Scalar by which we multiply the output of the embedding layer.</p><p>Sampled from 10 Uniform(-1,1) . • relative position embedding multiplier: Scalar by which we multiply vectors representing relative position. Sampled from 10 Uniform(-1,1) .</p><p>In order to make the search more efficient we reduced the total number of training tokens. We hypothesized that tuning hyperparameters on a reduced total number of tokens does not significantly affect optimal hyperparameters. To verify, we trained two different horizons and compared the results. As suspected, we observed that the results are well-aligned for both 4 and 16 billion tokens versions. We observe learning rate and initialization scale impact the results the most. Based on the results we chose 0.006 for the former and 2.5 for the latter. Since most other hyperparameters appear to have negligible effect on performance, they were kept at their default values of 1, the only exception being the embedding scale, where higher values seem to perform better and it was therefore set to 10.</p><p>Training the µTransfer Model We encountered frequent divergences in our initial attempt to train the µTransfer model. We traced the issue back to underflow of FP16 tensors in the backwards pass and therefore switched to training the model in FP32. This allowed us to finish the training run without divergences. We hypothesize that the divergence issue is related to µTransfer picking more aggressive hyperparameters, for example a higher learning rate on linear weight tensors compared to the original model. In order to exclude code differences as a possible confounder, we re-trained GPT-3 6.7B from scratch using the original hyperparameters. The only difference compared to the version published in <ref type="bibr" target="#b6">[7]</ref> is that the learning rate was decayed fully, whereas the learning rate of the model from <ref type="bibr" target="#b6">[7]</ref> was only decayed to 10% of its starting value. The retrained model performs slightly worse than the original published in <ref type="bibr" target="#b6">[7]</ref>. We suspect that this is because it made less progress during the last phase of training where the learning rate is close to zero. The training curves of the µTransfer model and the re-run of the original 6.7B can be seen in Fig. <ref type="figure" target="#fig_14">15</ref>. Detailed evaluation results can be found in Table <ref type="table" target="#tab_1">10</ref>   The training curves of the GPT-3 6.7B model with µTransfer (orange) and a re-run with the original settings from <ref type="bibr" target="#b6">[7]</ref> (blue). The µTransfer model uses relative attention while the re-run uses absolute attention. In addition, the former was trained using FP32 activations and weights after initially encountering stability issues with the hyperparameters computed using µP, while the re-run used the original FP16 training. The µTransfer model seems to underperform in the middle of training, but achieves a much better final validation loss once the learning rate is fully decayed. While the original model uses a cosine schedule, the µTransfer model uses a linear learning rate decay schedule transferred from the proxy model.</p><p>• T = 300 Billion is the number of training tokens for the 6.7B target model.</p><p>Here we are using the fact that the training FLOPs of a Transformer per token is roughly proportional to its number of parameters. Setup For this case we use Davidnet <ref type="bibr" target="#b1">[2]</ref>, a ResNet variant that trains quickly on CIFAR-10, so as to efficiently investigate its HP landscape. We train with SGD on CIFAR-10 for 10 epochs; all results are averaged over 15 random seeds. We use a width multiplier to identify models of different width, and a multiplier of 1 corresponds to the original model in <ref type="bibr" target="#b1">[2]</ref>. We look at validation accuracy here as the model barely overfits, and our observations will hold for the training accuracy as well. We first conduct a learning rate sweep for models of different widths using SP; the result is shown in Fig. <ref type="figure" target="#fig_15">16</ref>, on the left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Experiments</head><p>Hyperparameter Stability Note that the best model with a width multiplier of 8 under-performs that with a multiplier of 4. We run the same sweep with µP, along with a sweep of the output multiplier (α output ); the result is shown in Fig. <ref type="figure" target="#fig_15">16</ref>, on the right. We notice that wider models always perform better under µP and that the optimal learning rate η and α output are stable across width.</p><p>Table <ref type="table" target="#tab_1">10</ref>: Full evaluation results of our GPT-3 6.7B models: The new model tuned with µTransfer (marked µP), the original model from <ref type="bibr" target="#b6">[7]</ref>, and a re-training of this model from scratch with the original hyperparameter settings (marked re-run). The sampling-based evaluations shown here are a subset of the ones from <ref type="bibr" target="#b6">[7]</ref>. Since the sampling-based evaluations are subject to high variance, Wikitext 103 and the LM1B benchmark have been added to help distinguish the relative performance of the µP and non-µP model. Note that Wikitext-103 <ref type="bibr" target="#b31">[32]</ref> and the LM1B <ref type="bibr" target="#b8">[9]</ref> benchmarks overlap with the training dataset. Accuracies and F1 scores have been multiplied by 100. The perplexities reported in this table are based on a custom BPE encoding and are not comparable to other results in the literature. The number k of examples in the context for each task is identical to [7]. Note: Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in the context when performing the sampling-based evaluations, not the "shots" involved in hyperparameter transfer. Zero-shot One-shot Few-shot Task Split Metric µP [7] re-run µP [7] re-run µP [7] re-run Validation dataset valid ce 1.98 2.03 PTB test ppl 11.4 13.0 Wikitext 103 test ppl 8.56 9.13 LM1B test ppl 20.5 21.7 HellaSwag dev acc 72.0 67.4 66.7 71.1 66.5 65.9 72.4 67.3 66.4 LAMBADA test acc 73.5 70.3 70.8 69.9 65.4 64.8 74.7 79.1 77.1 StoryCloze test acc 79.4 77.7 77.3 80.6 78.7 78.3 84.2 81.2 81.1 NaturalQS test acc 9.86 5.79 7.20 14.7 9.78 10.6 20.2 17.0 15.7 TriviaQA dev acc 47.0 38.7 37.5 50.4 44.4 42.5 55.5 51.6 49.9 WebQS test acc 11.3 7.73 9.79 20.2 15.1 16.2 33.0 27.7 28.2 Ro→En 16 test BLEU-sb 26.9 8.75 13.7 36.5 34.2 33.5 38.2 36.2 35.6 En→Ro 16 test BLEU-sb 18.1 5.31 4.40 21.0 18.2 17.3 22.0 19.6 18.8 Fr→En 14 test BLEU-sb 29.8 15.5 19.6 31.7 31.6 30.1 38.0 36.4 36.5 En→Fr 14 test BLEU-sb 29.6 11.4 11.6 28.8 28.3 26.0 33.3 33.3 31.2 De→En 16 test BLEU-sb 31.7 18.2 21.7 33.3 31.9 31.1 38.9 36.5 36.2 En→De 16 test BLEU-sb 23.1 9.36 9.00 24.6 21.7 21.1 27.6 24.1 24.5 Winograd test acc 85.3 85.7 86.8 84.6 84.6 84.2 86.4 85.4 83.9 Winogrande dev acc 66.8 64.5 62.5 67.6 65.8 64.5 71.0 67.4 67.2 PIQA dev acc 79.1 78.0 78.0 77.3 76.3 76.9 79.2 77.8 77.7 ARC (Challenge) test acc 42.1 41.4 42.5 44.0 41.5 42.4 43.8 43.7 42.7 ARC (Easy) test acc 64.3 60.2 61.9 65.3 62.6 63.4 67.3 65.8 65.3 OpenBookQA test acc 54.4 50.4 52.6 56.4 53.0 52.8 58.4 55.2 54.4 Quac dev f1 41.8 36.1 38.2 43.1 39.0 39.5 44.0 39.9 39.9 RACE-h test acc 45.0 44.1 43.2 44.9 44.3 42.9 45.2 44.7 43.4 RACE-m test acc 58.4 54.4 54.0 57.9 54.7 53.8 58.6 55.4 55.4 SQuADv2 dev f1 59.9 52.7 50.9 64.9 57.1 54.7 68.9 62.1 58.4 CoQA dev f1 78.5 72.8 72.9 80.9 75.1 74.4 81.3 77.3 75.4 DROP dev f1 17.1 17.0 17.4 23.3 27.3 25.7 33.9 29.7 28.7 BoolQ dev acc 69.4 65.4 60.9 74.1 68.7 65.0 73.9 70.0 69.7 CB dev acc 21.4 28.6 37.5 60.7 33.9 32.1 62.5 60.7 66.1 Copa dev acc 82.0 80.0 77.0 81.0 82.0 81.0 88.0 83.0 82.0 RTE dev acc 55.2 55.2 46.2 61.0 54.9 58.8 52.7 49.5 59.9 WiC dev acc 0. 0. 0. 50.0 50.3 50.3 50.5 53.1 51.3 ANLI R1 test acc 33.7 32.3 33.4 32.4 31.6 31.7 30.9 33.1 30.7 ANLI R2 test acc 33.8 33.5 33.0 34.8 33.9 33.7 35.0 33.3 32.2 ANLI R3 test acc 32.7 34.8 33.4 34.8 33.1 33.3 36.9 33.9 32.3 Figure <ref type="figure" target="#fig_15">16</ref>: ResNet on CIFAR-10 for different widths (compared to a base network). On the left, the widest network SP underperforms; on the right, the µP network has a more consistent HP landscape and performs better. Both networks are tuned at the smallest width for the HP (η or α output ) not in the x-axis.</p><p>Hyperparameter Transfer Next, we perform a grid search for learning rate (η) and α output on the 0.5x model for both SP and µP. <ref type="foot" target="#foot_18">23</ref> Then, we take the best combination and test on the 8x model, simulating how a practitioner might use µTransfer. The result is shown in Table <ref type="table" target="#tab_18">12</ref>, where µP outperforms SP by 0.43% ± .001%. Hyperparameter Transfer We start with a proxy model with a width multiplier of 0.125 and tune several HPs using the following grid:</p><p>• η: 1 × 2.048 × 2 z , where z ∈ {-5, -4, -3, ..., 4}</p><p>• α output : 10 × 2 z , where z ∈ {-5, -4, -3, ..., 4}</p><p>• weight decay co-efficient γ: 3.05 × 10 -5 × 2 z , where z ∈ {-2, -1.5, -1, ..., 1.5}</p><p>• SGD momentum β: 0.875 × 2 z , where z ∈ {-2, -1.5, -1, ..., 1.5}</p><p>The grid is centered around the default HPs used by <ref type="bibr" target="#b0">[1]</ref> for ResNet-50; while not expected to be competitive for WRN, they represent a reasonable starting point for our experiment.</p><p>We randomly sample 64 HP combinations from the grid and train for 50 epochs, before selecting the one with the highest top-1 validation accuracy. Then, we scale up the model following both µP and SP and run with the same HPs we just selected. The result is shown in Table <ref type="table" target="#tab_19">13</ref>, where µP outperforms SP by 0.41% in terms of top-1 validation accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.2 Post-Layernorm Transformers</head><p>Fig. <ref type="figure" target="#fig_15">17</ref> shows the transferability of learning rate, α output , initialization standard deviation, and Adam β 2 across width, batch size, sequence length, and training steps for post-layernorm Transformers. However, in general, we find transfer across depth to be fragile.</p><p>What Happens in the mup Package Under the hood, mup implements the µP formulation in Table <ref type="table">8</ref>. By invoking set_base_shape(model, base_model), each parameter tensor p of model gets a p.infshape attribute that stores, for each of its dimensions, the corresponding base dimension and whether that dimension should be considered "infinite" (i.e. will be scaled up/down, e.g., d model of a Transformer) or "finite" (i.e. will be fixed, e.g., vocabulary size). This information is used in the initializers and optimizers to automatically scale the parameters or learning rates to be compliant with µP. For example, by Table <ref type="table">8</ref>, the Adam learning rate of hidden weights p is calculated as η/p.infshape.width_mult(), where p.infshape.width_mult() essentially calculates fan_in base_fan_in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Reverse-µTransfer for Diagnosing Training Instability in Large Models</head><p>Large Transformers are famously fickle to train <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b37">37]</ref>. We note that a possible source of this instability for larger transformers is the failure of naive hyperparameter transfer via the standard parametrization. This is certainly consistent with Fig. <ref type="figure" target="#fig_0">1</ref>, which shows that the optimal learning rate for small Transformers can lead to trivial performance in large Transformers. We support this hypothesis further by reverse-µTransferring the instability-inducing HPs from a large Transformer to a small one and replicating the training instability. This is shown in Fig. <ref type="figure" target="#fig_15">21</ref>.</p><p>Practically, this reverse-µTransfer technique can be used to diagnose or debug training instability problems of large models. We offer two case studies toward this claim.</p><p>1) When training transformers of width 8192 on Wikitext-2, we found certain HP combinations caused divergence in the middle of training. We reverse-µTransferred one such HP combination to a model of width 256 and replicated this divergence. By analyzing this small model's activations right before this divergence, we found that the cause is due to attention logits blowing up. Note this debugging session proceeded much more quickly than if we directly worked with the large model. Later we confirmed this is indeed the same cause of the width-8192 model's divergence.</p><p>2) A 6B-parameter language model (in standard parametrization) in a separate project experienced repeated blow-up in the middle of training. We reverse-µTransferred its hyperparameters to a smaller, 100M-parameter model and replicated the training instability. This was solved by a retuning of the small model via random search.</p><p>20 18 16 14 12 10 8 log2LearningRate 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Training Loss training instability Fix Hparam., Change Width Actual Width 256 512 1024 2048 4096 8192 20 18 16 14 12 10 8 log2LearningRate 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Training Loss training instability Fix Width, Change Hparam. Simulated Width 256 512 1024 2048 4096 8192 Figure 21: Replicating training instability on a small Transformer by reverse-µTransferring hyperparameters. These experiments concern 2-layer Transformers in Standard Parametrization (SP) on Wikitext-2, trained with Adam, where width is defined as d model = d f f n . (Left) LR-vsloss for wider and wider Transformers. (Right) Likewise for simulated width: Here each point (log 2 η, loss) for simulated width n indicates the loss from training a width-256 µP Transformer with base width n and LR η (i.e. loosely speaking, it's using LR transferred from η in a width-n SP <ref type="bibr">Transformer)</ref>. Takeaway: The overall shapes of the curves are identical between the left and right plots <ref type="foot" target="#foot_19">24</ref> ; in particular, a learning rate leads to instability in a wide model iff it does so when transferred back to a narrow model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J An Intuitive Introduction to the Theory of Maximal Update Parametrization</head><p>In what follows, we seek to describe useful intuitions and rules of thumb that would be helpful to practitioners and empirical researchers alike in figuring out what is the right neural network parametrization. The intuitions we shall describe regarding SGD can be made rigorous as in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref>; those regarding Adam are new, and their formalization will be done in an upcoming paper.</p><p>First, we write down the most basic intuition regarding sums of many random elements, which will underlie all of the calculations that follow.</p><p>Law of Large Numbers (LLN) If x 1 , . . . , x n , . . . "look like" random independent samples of a random variable X, then 1 n n i=1</p><p>x i → E[X], as n → ∞.</p><p>Central Limit Theorem (CLT) In the same scenario as above,</p><formula xml:id="formula_27">1 √ n n i=1 (x i -E[X]) → N (0, σ(X)), as n → ∞,</formula><p>where σ(X) is the standard deviation of the random variable X.</p><p>Of course, there are many subtleties one must resolve to make the statements above truly rigorous (e.g., what is the meaning of "look like"?), but as rules of thumb, they typically give the correct prediction. In particular, here we want to note the following basic intuition regarding the size of a sum of x i :</p><p>when n is large, n i=1</p><p>x i has typical size</p><formula xml:id="formula_28">Θ(n) if E[X] = 0 Θ( √ n) otherwise</formula><p>Here, "typical size" can be taken to mean the size 99% of time. Again, we stress that this is a good rule of thumb that yields the correct prediction in the cases we are concerned with here; the rigorous versions of this will come from the Tensor Programs framework (e.g., <ref type="bibr" target="#b56">[56]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 Behaviors of Gaussian Matrices vs Tensor Product Matrices</head><p>Central to the derivation of µP for any architecture are key insights on the behaviors of two kinds of random matrices: 1) iid Gaussian random matrix and 2) tensor product matrix (by which we mean a sum of outer products) and more generally what we call nonlinear tensor product matrix (see Eq. ( <ref type="formula">7</ref>)). For example, a neural network, randomly initialized in the typical way, will have each weight matrix look like the former. However, every step of training by gradient descent adds a sum of outer products to this initial matrix, so that the change in weights constitute a tensor product matrix. For Adam, the change in weights is not a tensor product but a more general nonlinear tensor product matrix (see Eq. ( <ref type="formula">7</ref>)). In this section, we will particularly focus on the right scaling for the entries of such matrices, leading to a discussion of the right neural network parametrization in the next section. We concentrate on the key heuristics but eschew burdensome rigor.</p><p>Key Insights Consider a random vector v ∈ R n with approximately iid entries and a random matrix A of either size n × n or 1 × n, both having entries of size Θ(1). <ref type="foot" target="#foot_20">25</ref> In the context of deep learning, v for example can be an activation vector in an MLP, a Gaussian A the hidden weights at initialization, a (nonlinear) tensor product A the change in hidden weights due to training, and a vector A the readout layer weights. Then Av corresponds to a part of the next layer preactivation or the network output. To make sure the preactivations and the output don't blow up, we thus need to understand the scale of Av, especially in the general case where A is correlated with v. <ref type="foot" target="#foot_21">26</ref> This is summarized in Table <ref type="table" target="#tab_21">14</ref>, with the derivations below. Intuitively, a (nonlinear) tensor product or vector A will interact with a correlated v via Law of Large Numbers, hence the n-scaling, while a Gaussian A interacts with v via Central Limit Theorem, hence the √ n-scaling.</p><p>In the derivations below, we answer a slightly different but equivalent question of "how to scale A such that Av has entry size Θ(1)?"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1.1 Preparation for the Derivations</head><p>By the results of <ref type="bibr" target="#b57">[57]</ref>, each (pre-)activation vector and its gradient vector in a multi-layer perceptron, at any time during training, have approximately iid coordinates in the large width limit, <ref type="foot" target="#foot_22">27</ref> and something similar can be said for more advanced networks such as ResNet and Transformers<ref type="foot" target="#foot_23">foot_23</ref> . Definition J.1. We say any such vector v ∈ R n has Θ(n a )-sized coordinates, or just Θ(n a )coordinates for short, if v 2 /n = Θ(n 2a ) as n → ∞. Because, by the above discussion, the coordinates are roughly iid when n is large, this intuitively means that each entry of v has "typical size" Θ(n a ). We make similar definitions with Θ replaced by O and Ω.</p><p>2. Neural network output should be O(1).</p><p>3. All parameters should be updated as much as possible (in terms of scaling in width) without leading to divergence.</p><p>Let's briefly justify these desiderata. For the desideratum 1, if the coordinates are ω(1) or o(1), then for sufficiently wide networks their values will go out of floating point range. This problem is particularly acute for low-precision formats that are essential for training large models such as BERT or GPT. Moreover, a general nonlinearity is only well-behaved if its input is in a fixed range (although this is not a problem for homogeneous nonlinearities like relu). For example, for tanh nonlinearity, if the preactivation is vanishing o(1), then tanh is essentially linear; if the preactivation is exploding ω(1), then the tanh gradient vanishes.</p><p>For the desideratum 2, a similar justification applies to the numerical fidelity of the loss function and loss derivative. Note that, with desideratum 3, this means the network output should be Θ(1) after training (but it can go to zero at initialization).</p><p>Finally, desideratum 3 means that 1) we are doing "maximal feature learning" <ref type="bibr" target="#b57">[57]</ref> and 2) every parameter contribute meaningfully in the infinite-width limit. This ensures that learning rate "plays the same role" in the finite-width case as in the infinite-width limit. For example, it prevents the scenario where a weight matrix gets stuck at initialization in the limit for any learning rate (so learning rate does not matter) but evolves nontrivially in any finite-width network (so learning rate does matter).</p><p>These desiderata will essentially uniquely single out µP. More formally, µP is the unique parametrization that admits feature learning in all parameters of the neural network <ref type="bibr" target="#b57">[57]</ref>, and this property theoretically guarantees HP transfer across width (for sufficiently large width). However, for the sake of reaching a broader audience, we will focus more on the intuitive derivations from the desiderata rather than on this formal aspect.</p><p>Below, we first assume for simplicity that the width of every layer is n, and we focus only on dense weights. Later, we will discuss convolutions and varying the widths between layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2.1 µP Derivation From the Desiderata</head><p>Below, we will derive the µP formulation in Table <ref type="table" target="#tab_12">3</ref>. Tables <ref type="table">8</ref> and <ref type="table" target="#tab_13">9</ref> can be derived from Table <ref type="table" target="#tab_12">3</ref> via the following equivalences, which can be easily derived via some simple calculations. Lemma J.1. Let f t (ξ) denote the neural network function after t steps of training (using any fixed sequence of batches), evaluated on input ξ. Consider a parameter tensor W with learning rate C, initialized as W ∼ N (0, B 2 ), and with a multiplier A. Then for any θ &gt; 0, f t (ξ) stays fixed for all t and ξ if we set</p><p>• when the optimizer is SGD</p><formula xml:id="formula_29">A ← Aθ, B ← B/θ, C ← C/θ 2</formula><p>• when the optimizer is Adam,</p><formula xml:id="formula_30">A ← Aθ, B ← B/θ, C ← C/θ;</formula><p>For example, for output weights, Table <ref type="table" target="#tab_12">3</ref> has A = 1, B = 1/fan_in, C = η/fan_in for SGD and Adam. Then taking θ = 1/fan_in, we get the entries in Table <ref type="table">8</ref>, with A = 1/fan_in, B = 1, C = η • fan_in for SGD and C = η for Adam. Taking θ = 1/ √ fan_in instead, we get the entries in Table <ref type="table" target="#tab_13">9</ref>, with A = 1/ √ fan_in, B = 1/fan_in, C = η for SGD and η/ √ fan_in for Adam. Similar calculations hold for the input weights scaling in those tables, after taking into consideration that fan_in is considered a constant in terms of width for the input layer.</p><p>We proceed with the derivation of Table <ref type="table" target="#tab_12">3</ref> below. Recall the definitions of Θ(n a )-sized coordinates or Θ(n a )-coordinates from Definition J.1.</p><p>Output Weights Suppose W ∈ R 1×n is an output weight. By desideratum 1, the input x to W has Θ(1)-sized coordinates. Thus W should have Θ(1/n)-coordinates so that |W x| = O(1). We can initialize W with Θ(1/n)-coordinates and scale its (per-layer) LR so that ∆W has Θ(1/n)coordinates as well. This means initializing W αβ ∼ N (0, Θ(1/n 2 )) and use Θ(1/n) learning rate for both SGD and Adam. but this induces a non-maximal feature learning limit, which, as we argue below, cannot transfer hyperparameters in all situations. 2. For SGD, the gradient of R n×n weight has Θ(1/ √ n)-coordinates, so Θ(1) learning rate would make preactivation scale like Θ( √ n) and hence blow up. If we use Θ(1/width) learning rate, then blow-up does not occur. However, this infinite-width limit is in the kernel regime <ref type="bibr" target="#b57">[57]</ref> and thus does not allow HP transfer for the same reason that NTP below does not.</p><p>Neural Tangent Parametrization (NTP) We have concrete examples, e.g. Word2Vec in <ref type="bibr" target="#b57">[57]</ref>, where the NTK limit has trivial performance -so HPs have no effect at all -vastly outperformed by finite-width networks -where HPs matter. More importantly, wider does not always do better in NTP, especially in tasks where feature learning is crucial <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b61">61]</ref>. So in the context of modern deep learning e.g. large language model pretraining, NTP (or SP with Θ(1/width) LR) does not make sense for wide neural networks.</p><p>Other Parametrizations Recall the Dynamical Dichotomy Theorem proven in <ref type="bibr" target="#b57">[57]</ref>, which says that any nontrivial stable "natural parametrization" (formally, "abc-parametrization," <ref type="bibr" target="#b57">[57]</ref>) either admits a feature learning limit or a kernel limit, but not both.</p><p>Our argument above against SP and NTP will also work against any parametrization inducing a kernel limit. Therefore, it remains to ask, can other feature learning parametrizations transfer HPs?</p><p>We argue no. As shown in <ref type="bibr" target="#b57">[57]</ref>, any other feature learning parametrization differs from µP essentially only in that some parameters are not updated maximally. By <ref type="bibr" target="#b57">[57,</ref><ref type="bibr">Sec 6.4]</ref>, in the infinite-width limit, such parameters can be thought of as being fixed at initialization. Therefore, in such infinite-width limits, the learning rate of such parameters becomes useless. As such, we cannot hope for the HP landscape of the limit to reflect the HP landscape of finite-width neural networks. µP is the unique feature learning parametrization that updates all parameters maximally, so that the learning rate of each parameter plays approximately the same role in finite-width neural networks as in the infinite-width limit. Consequently, the HP landscape of the µP limit should reflect the HP landscape of finite-width neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training loss against learning rate on Transformers of varying d model trained with Adam.Conventionally and in contrast with our technique, different widths do not share the same optimal hyperparameter; wider networks do not always perform better than narrower ones; in fact they underperform the same-width networks in our technique even after tuning learning rate (see dashed line). See Sections 3 and 4 for experimental setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of µTransfer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>) over c ∈ R, for some bounded continuous functionf : R → R. If we reparametrize c = α/ √ n for α ∈ R, then by CLT, G n (α) def = F n (c) → E f (N (0, α2)) stabilizes into a function of α as n → ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: MLP width different hidden sizes trained for 20 epoch on CIFAR-10 using SGD. Left uses standard parametrization (SP); right uses maximal update parametrization (µP). µP networks exhibit better learning rate stability than their SP counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Empirical validation of the stability of four representative hyperparameters on pre-LN Transformers in µP: learning rate, last layer weight multiplier α output , weight initialization standard deviation, and learning rate schedule. We use the following learning rate schedules: (a) linear decay; (b) StepLR @ [5k, 8k] with a decay factor of 0.1; (c) StepLR @ [4k, 7k] with a decay factor of 0.3; (d) cosine annealing; (e) constant; (f) inverse square-root decay. All models are trained on wikitext-2 for 10k steps.When not specified in the legend, the width used is 256, depth 2, batch size 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to each column, while fixing all others constant. See Section 6.1 for discussion of these results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Wider is always better in training loss under µP, but not in SP, given the same HP. Learning curves for µP and SP with different learning rates, aggregated over 5 seeds. (Left) Wider µP models always achieve better training loss at any time in training. (Middle) If using a small learning rate, SP models can appear to do so up to some large width, at which point the pattern fails (at width 2048 in our plot). (Right) If using a large learning rate, SP model can strictly do worse with width; here the SP model is identical to the µP model in (Left) at width 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>d model = d model,0 = 128, d f f n = d f f n,0 = 512, etc, as in the MLP example in Section 4. For this purpose, it's useful to define dmodel = d model /d model,0 , dffn = d f f n /d f f n,0 , and so on. One can always take d model,0 = d f f n,0 = • • • = 1 for a "pure" µP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>with Adam LR η LN w , and b LN ← 0, with Adam LR η LN b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Enlarging d k makes µTransfer more precise. Here we plot all curves after subtracting their minima for easier visual comparison. Transformer on IWSLT 14 similar to the setup in Appendix F.1 where the d model = 512 for a width multiplier of 1, n head = 4, and d q = d k . (Left) We leave d q = d k = d model/n head , so d k = 8 for width-multiplier 0.0625. The optimum for the attention logit multiplier c attn is noisy and does not accurately transfer across width. (Right) We enlarge d q = d k to a minimum of 128. The HP landscape is much smoother than in (Left), and the optima align between narrow and wide models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>k from d model (so that d model = d k • n head ) and maintain a relatively large d k even as d model is shrunk in the proxy model. For example, pegging d k = 32 is generally effective. Training or inference speed are not usually affected much by the larger d k because of CUDA optimizations. By Appendix E.2, this decoupling of d k from d model is theoretically justified, and as shown in Fig. 10, it significantly denoises the HP landscape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Schematics of each Transformer layer. Commonly, the key and value dimensions d k and d v are both set to d model /n head , and this is referred to as d head .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: µTransfer across width when we fix d head and vary d model and n head . α output , α attn are multipliers for output and key weights, and σ is initialization standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Results of the random search over reduced-width GPT-3 proxy models trained on 4 (left) and 16 (right) billion tokens. Only the best performing runs are highlighted.</figDesc><graphic coords="33,108.00,72.00,197.96,141.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure15:The training curves of the GPT-3 6.7B model with µTransfer (orange) and a re-run with the original settings from<ref type="bibr" target="#b6">[7]</ref> (blue). The µTransfer model uses relative attention while the re-run uses absolute attention. In addition, the former was trained using FP32 activations and weights after initially encountering stability issues with the hyperparameters computed using µP, while the re-run used the original FP16 training. The µTransfer model seems to underperform in the middle of training, but achieves a much better final validation loss once the learning rate is fully decayed. While the original model uses a cosine schedule, the µTransfer model uses a linear learning rate decay schedule transferred from the proxy model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>G. 1</head><label>1</label><figDesc>Experiments on ResNetsG.1.1 ResNet on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="41,167.40,72.00,277.21,346.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Algorithm 1 Tuning a Large Target Model via µTransfer 1: Parametrize target model in Maximal Update Parametrization (µP) 2: Tune a smaller version (in width and/or depth) of target model 3: Copy tuned hyperparameters to target model Hyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred Across, with a few caveats discussed in Section 6.1. * means empirically validated only on Transformers, while all others additionally have theoretical justification.</figDesc><table><row><cell>µTransferable</cell><cell>Not µTransferable</cell><cell>µTransferred Across</cell></row><row><cell>optimization related, init,</cell><cell>regularization</cell><cell>width, depth*, batch size*,</cell></row><row><cell cols="3">parameter multipliers, etc (dropout, weight decay, etc) training time*, seq length*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Examples of µTransferable Hyperparameters. All of the below can also be specialized to per-layer hyperparameters.</figDesc><table><row><cell>Optimizer Related</cell><cell>Initialization</cell><cell>Parameter Multipliers</cell></row><row><cell>learning rate (LR), momentum,</cell><cell>per-layer</cell><cell>multiplicative constants after</cell></row><row><cell>Adam beta, LR schedule, etc</cell><cell>init. variance</cell><cell>weight/biases, etc</cell></row><row><cell>Our Contributions</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Transformers on WMT14 En-De. 1x and 0.25x refers to scaling of width only. We report BLEU fluctuation over 3 independent trials, i.e., 3 independent random HP searches.For more details on BERT-prototype, what HPs we tune, and how we finetune the trained models, see Appendix F.3.</figDesc><table><row><cell>Val. BLEU Percentiles</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>BERT</figDesc><table><row><cell>BERT base</cell><cell>Megatron Default</cell><cell>1x</cell><cell>1x</cell><cell>1.995</cell><cell>84.2/84.2</cell><cell>90.6</cell></row><row><cell>BERT base</cell><cell>Naive Transfer</cell><cell>4x</cell><cell>40x</cell><cell></cell><cell>training diverged</cell><cell></cell></row><row><cell>BERT base</cell><cell>µTransfer (Ours)</cell><cell>4x</cell><cell>40x</cell><cell>1.970</cell><cell>84.3/84.8</cell><cell>90.8</cell></row><row><cell cols="2">BERT large Megatron Default</cell><cell>1x</cell><cell>1x</cell><cell>1.731</cell><cell>86.3/86.2</cell><cell>90.9</cell></row><row><cell>BERT large</cell><cell>Naive Transfer</cell><cell>22x</cell><cell>220x</cell><cell></cell><cell>training diverged</cell><cell></cell></row><row><cell>BERT large</cell><cell>µTransfer (Ours)</cell><cell>22x</cell><cell>220x</cell><cell>1.683</cell><cell>87.0/86.5</cell><cell>91.4</cell></row></table><note><p>pretraining. HP transfer outperforms published baselines without tuning the full model directly at all. We tune BERT-base and BERT-large simultaneously via a single proxy model, BERT-prototype. The total tuning cost = the cost of pretraining a single BERT-large. Model speedup refers to the training speedup of BERT-prototype over BERT-base or BERT-large. Total speedup in addition includes time saving from transferring across training steps. Both speedups can be interpreted either as real-time speedup on V100s or as FLOPs speedup (which turn out to be empirically very similar in this case).</p><p>Model</p><p>Method Model Speedup Total Speedup Test loss MNLI (m/mm) QQP</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 3 ,</head><label>3</label><figDesc>which is a common design feature of Transformer models. Note that in this table, for biases, the fan_in is 1 (compare to PyTorch nn.Linear default initialization of biases, where fan_in refers to fan_in of the layer.) This table can be derived from Table3via Lemma J.1.</figDesc><table><row><cell cols="2">See Appendix B for further explanations.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Input weights &amp; all biases</cell><cell cols="2">Output weights</cell><cell>Hidden weights</cell></row><row><cell>Init. Var.</cell><cell>1 /fan_in</cell><cell>1</cell><cell>( 1 /fan_in)</cell><cell>1 /fan_in</cell></row><row><cell>Multiplier</cell><cell>1</cell><cell cols="2">1 /fan_in (1)</cell><cell>1</cell></row><row><cell>SGD LR</cell><cell>fan_out (1)</cell><cell cols="2">fan_in (1)</cell><cell>1</cell></row><row><cell>Adam LR</cell><cell>1</cell><cell>1</cell><cell></cell><cell>1 /fan_in (1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>µP Formulation in the Style of<ref type="bibr" target="#b57">[57]</ref>. This table can be derived from Table3via Lemma J.1.</figDesc><table><row><cell></cell><cell>Input weights &amp; all biases</cell><cell cols="2">Output weights</cell><cell>Hidden weights</cell></row><row><cell>Init. Var. Multiplier</cell><cell>1 /fan_out √ fan_out (1) ( 1 /fan_in)</cell><cell>1</cell><cell>1 /fan_in</cell><cell>1 /fan_in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 .</head><label>11</label><figDesc>and Ratio of Tuning Cost to Pretraining Cost in FLOPs can be approximated ass(t 1 N 1 + t 2 N 2 )Billion is number of parameters of the target model • t 1 = 4 Billion is the number of training tokens for the short horizon HP search, and N 1 = 350 is the corresponding number of random HP search trials.• t 2 = 16 Billion is the number of training tokens for the longer horizon HP search, and N 1 = 117 is the corresponding number of random HP search trials.</figDesc><table><row><cell>ST</cell><cell>≈ 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11 :</head><label>11</label><figDesc>Evaluation results comparing the GPT-3 6.7B model tuned with µTransfer against the twice-as-large GPT-3 13B model from<ref type="bibr" target="#b6">[7]</ref>. The two models have similar performance on most of the evaluation tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Zero-shot</cell><cell></cell><cell cols="2">One-shot</cell><cell></cell><cell cols="2">Few-shot</cell></row><row><cell cols="2">Task</cell><cell></cell><cell>Split Metric</cell><cell cols="10">6.7B+µP 13B[7] 6.7B+µP 13B[7] 6.7B+µP 13B[7]</cell></row><row><cell cols="2">HellaSwag</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>72.0</cell><cell></cell><cell cols="2">70.9</cell><cell>71.1</cell><cell cols="2">70.0</cell><cell>72.4</cell><cell>71.3</cell></row><row><cell cols="2">LAMBADA</cell><cell></cell><cell>test acc</cell><cell></cell><cell>73.5</cell><cell></cell><cell cols="2">72.5</cell><cell>69.9</cell><cell cols="2">69.0</cell><cell>74.7</cell><cell>81.3</cell></row><row><cell cols="2">StoryCloze</cell><cell></cell><cell>test acc</cell><cell></cell><cell>79.4</cell><cell></cell><cell cols="2">79.5</cell><cell>80.6</cell><cell cols="2">79.7</cell><cell>84.2</cell><cell>83.0</cell></row><row><cell cols="2">NaturalQS</cell><cell></cell><cell>test acc</cell><cell></cell><cell>9.86</cell><cell></cell><cell cols="2">7.84</cell><cell>14.7</cell><cell cols="2">13.7</cell><cell>20.2</cell><cell>21.0</cell></row><row><cell cols="2">TriviaQA</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>47.0</cell><cell></cell><cell cols="2">41.8</cell><cell>50.4</cell><cell cols="2">51.3</cell><cell>55.5</cell><cell>57.5</cell></row><row><cell cols="2">WebQS</cell><cell></cell><cell>test acc</cell><cell></cell><cell>11.3</cell><cell></cell><cell cols="2">8.22</cell><cell>20.2</cell><cell cols="2">19.0</cell><cell>33.0</cell><cell>33.5</cell></row><row><cell cols="2">Ro→En 16</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>26.9</cell><cell></cell><cell cols="2">20.8</cell><cell>36.5</cell><cell cols="2">36.7</cell><cell>38.2</cell><cell>38.4</cell></row><row><cell cols="2">En→Ro 16</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>18.1</cell><cell></cell><cell cols="2">6.43</cell><cell>21.0</cell><cell cols="2">20.8</cell><cell>22.0</cell><cell>21.8</cell></row><row><cell cols="2">Fr→En 14</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>29.8</cell><cell></cell><cell cols="2">22.4</cell><cell>31.7</cell><cell cols="2">31.4</cell><cell>38.0</cell><cell>38.3</cell></row><row><cell cols="2">En→Fr 14</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>29.6</cell><cell></cell><cell cols="2">15.3</cell><cell>28.8</cell><cell cols="2">30.1</cell><cell>33.3</cell><cell>35.5</cell></row><row><cell cols="2">De→En 16</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>31.7</cell><cell></cell><cell cols="2">24.4</cell><cell>33.3</cell><cell cols="2">34.5</cell><cell>38.9</cell><cell>39.1</cell></row><row><cell cols="2">En→De 16</cell><cell></cell><cell>test BLEU-sb</cell><cell></cell><cell>23.1</cell><cell></cell><cell cols="2">11.0</cell><cell>24.6</cell><cell cols="2">23.3</cell><cell>27.6</cell><cell>27.7</cell></row><row><cell cols="2">Winograd</cell><cell></cell><cell>test acc</cell><cell></cell><cell>85.3</cell><cell></cell><cell cols="2">87.9</cell><cell>84.6</cell><cell cols="2">86.1</cell><cell>86.4</cell><cell>82.4</cell></row><row><cell cols="2">Winogrande</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>66.8</cell><cell></cell><cell cols="2">67.9</cell><cell>67.6</cell><cell cols="2">66.9</cell><cell>71.0</cell><cell>70.0</cell></row><row><cell cols="2">PIQA</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>79.1</cell><cell></cell><cell cols="2">78.5</cell><cell>77.3</cell><cell cols="2">77.8</cell><cell>79.2</cell><cell>79.9</cell></row><row><cell cols="4">ARC (Challenge) test acc</cell><cell></cell><cell>42.1</cell><cell></cell><cell cols="2">43.7</cell><cell>44.0</cell><cell cols="2">43.1</cell><cell>43.8</cell><cell>44.8</cell></row><row><cell cols="2">ARC (Easy)</cell><cell></cell><cell>test acc</cell><cell></cell><cell>64.3</cell><cell></cell><cell cols="2">63.8</cell><cell>65.3</cell><cell cols="2">66.8</cell><cell>67.3</cell><cell>69.1</cell></row><row><cell cols="3">OpenBookQA</cell><cell>test acc</cell><cell></cell><cell>54.4</cell><cell></cell><cell cols="2">55.6</cell><cell>56.4</cell><cell cols="2">55.8</cell><cell>58.4</cell><cell>60.8</cell></row><row><cell cols="2">Quac</cell><cell></cell><cell>dev f1</cell><cell></cell><cell>41.8</cell><cell></cell><cell cols="2">38.4</cell><cell>43.1</cell><cell cols="2">40.6</cell><cell>44.0</cell><cell>40.9</cell></row><row><cell cols="2">RACE-h</cell><cell></cell><cell>test acc</cell><cell></cell><cell>45.0</cell><cell></cell><cell cols="2">44.6</cell><cell>44.9</cell><cell cols="2">44.6</cell><cell>45.2</cell><cell>45.1</cell></row><row><cell cols="2">RACE-m</cell><cell></cell><cell>test acc</cell><cell></cell><cell>58.4</cell><cell></cell><cell cols="2">56.7</cell><cell>57.9</cell><cell cols="2">56.9</cell><cell>58.6</cell><cell>58.1</cell></row><row><cell cols="2">SQuADv2</cell><cell></cell><cell>dev f1</cell><cell></cell><cell>59.9</cell><cell></cell><cell cols="2">56.3</cell><cell>64.9</cell><cell cols="2">61.8</cell><cell>68.9</cell><cell>67.7</cell></row><row><cell cols="2">CoQA</cell><cell></cell><cell>dev f1</cell><cell></cell><cell>78.5</cell><cell></cell><cell cols="2">76.3</cell><cell>80.9</cell><cell cols="2">77.9</cell><cell>81.3</cell><cell>79.9</cell></row><row><cell cols="2">DROP</cell><cell></cell><cell>dev f1</cell><cell></cell><cell>17.1</cell><cell></cell><cell cols="2">24.0</cell><cell>23.3</cell><cell cols="2">29.2</cell><cell>33.9</cell><cell>32.3</cell></row><row><cell cols="2">BoolQ</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>69.4</cell><cell></cell><cell cols="2">66.2</cell><cell>74.1</cell><cell cols="2">69.0</cell><cell>73.9</cell><cell>70.2</cell></row><row><cell>CB</cell><cell></cell><cell></cell><cell>dev acc</cell><cell></cell><cell>21.4</cell><cell></cell><cell cols="2">19.6</cell><cell>60.7</cell><cell cols="2">55.4</cell><cell>62.5</cell><cell>66.1</cell></row><row><cell cols="2">Copa</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>82.0</cell><cell></cell><cell cols="2">84.0</cell><cell>81.0</cell><cell cols="2">86.0</cell><cell>88.0</cell><cell>86.0</cell></row><row><cell>RTE</cell><cell></cell><cell></cell><cell>dev acc</cell><cell></cell><cell>55.2</cell><cell></cell><cell cols="2">62.8</cell><cell>61.0</cell><cell cols="2">56.3</cell><cell>52.7</cell><cell>60.6</cell></row><row><cell cols="2">WiC</cell><cell></cell><cell>dev acc</cell><cell></cell><cell>0.</cell><cell></cell><cell>0.</cell><cell></cell><cell>50.0</cell><cell cols="2">50.0</cell><cell>50.5</cell><cell>51.1</cell></row><row><cell cols="2">ANLI R1</cell><cell></cell><cell>test acc</cell><cell></cell><cell>33.7</cell><cell></cell><cell cols="2">33.2</cell><cell>32.4</cell><cell cols="2">32.7</cell><cell>30.9</cell><cell>33.3</cell></row><row><cell cols="2">ANLI R2</cell><cell></cell><cell>test acc</cell><cell></cell><cell>33.8</cell><cell></cell><cell cols="2">33.5</cell><cell>34.8</cell><cell cols="2">33.9</cell><cell>35.0</cell><cell>32.6</cell></row><row><cell cols="2">ANLI R3</cell><cell></cell><cell>test acc</cell><cell></cell><cell>32.7</cell><cell></cell><cell cols="2">34.4</cell><cell>34.8</cell><cell cols="2">32.5</cell><cell>36.9</cell><cell>34.5</cell></row><row><cell></cell><cell cols="3">Standard Parametrization</cell><cell></cell><cell></cell><cell></cell><cell cols="6">Max Update Parametrization ( P)</cell><cell></cell></row><row><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Validation Accuracy</cell><cell>0.90 0.91 0.92 0.93 0.94</cell><cell>2</cell><cell>0 Width mult. log 2 0.5 1.0 2.0 4.0 8.0</cell><cell>0.90 0.91 0.92 0.93 0.94</cell><cell>3</cell><cell>2</cell><cell>log 2</cell><cell>1</cell><cell>0</cell><cell>0.91 0.92 0.93 0.94</cell><cell>5</cell><cell>0 log 2 output</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>ResNet on CIFAR10: Transferring the best learning rate (η) and α output from widening factor 0.5 to 8; µP significantly outperforms SP given the same search grid. The best HPs are different as the models are parametrized to be identical at 1x width.23   For this case we use Wide-Resnet, or WRN<ref type="bibr" target="#b65">[65]</ref>, a ResNet variant with more channels per layer, to further showcase µTransfer across width, i.e., number of channels. We train with SGD on ImageNet for 50 epochs following standard data augmentation procedures. We use a width multiplier to identify models of different width, and a multiplier of 1 corresponds to the original WRN-50-2-bottleneck in<ref type="bibr" target="#b65">[65]</ref>.</figDesc><table><row><cell cols="5">Transfer Setup Best η Best αoutput Valid. Acc. (0.5x) Valid. Acc. (8x)</cell></row><row><cell>SP</cell><cell>0.707</cell><cell>4</cell><cell>92.82%</cell><cell>94.86%</cell></row><row><cell>µP</cell><cell>0.5</cell><cell>4</cell><cell>92.78%</cell><cell>95.29%</cell></row><row><cell cols="2">G.1.2 Wide ResNet on ImageNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>ResNet on ImageNet: Transferring the best learning rate (η), α output , γ, and β from widening factor 0.125 to 1; µP significantly outperforms SP given the same search grid.</figDesc><table><row><cell cols="3">Transfer Setup Best η Best αoutput</cell><cell>Best γ</cell><cell cols="3">Best β Valid. Acc. (0.125x) Valid. Acc. (1x)</cell></row><row><cell>SP</cell><cell>32.768</cell><cell>.625</cell><cell>.000015</cell><cell>.4375</cell><cell>58.12%</cell><cell>76.75%</cell></row><row><cell>µP</cell><cell>32.768</cell><cell>.625</cell><cell>.000015</cell><cell>.4375</cell><cell>58.12%</cell><cell>77.16%</cell></row><row><cell cols="3">G.2 Experiments on Transformers</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">G.2.1 Verifying Transfer across Batch Size, Sequence Length, and Training Time on</cell></row><row><cell cols="2">Wikitext-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>See Fig. 19.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Expected entry size of Av for different matrices A and vector v correlated with each other, both having entries of size Θ(1).</figDesc><table><row><cell></cell><cell cols="2">Standard Gaussian (Nonlinear) Tensor Product</cell><cell>Vector</cell></row><row><cell>Entry size of Av</cell><cell>A ∈ R n×n Θ( √ n)</cell><cell>A ∈ R n×n Θ(n)</cell><cell>A ∈ R 1×n Θ(n)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>but possibly not for different data and/or tasks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The more theoretically astute reader may observe that SP with a Θ(1/width) learning rate induces a well-defined infinite-width limit exists as well. Nevertheless, this does not allow HP transfer because this limit is in kernel regime as shown in<ref type="bibr" target="#b57">[57]</ref>. See Appendix J.3 for more discussions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>i.e. the default parametrization offered by common deep learning frameworks. See Table3for a review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>The key here is that the init. variance ∝ 1/fan_in, so the same insights here apply with e.g. He initialization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>While superficially different, this parametrization is equivalent to the µP defined in<ref type="bibr" target="#b57">[57]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>This is roughly because during training, q and k will be correlated so q k actually scales like d due to Law of Large Numbers, in contrast to the original motivation that q, k are uncorrelated at initialization so Central Limit applies instead. See Appendix J.2.1 for a more in-depth discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>Note in this example, Glorot initialization<ref type="bibr" target="#b12">[13]</ref> (i.e. with variance 1/(fan_in + fan_out)) would scale asymptotically the same as µP and thus is similarly well-behaved. However, if one adds layernorm or batchnorm, then Glorot will cause logit blowup like SP, but µP still will not.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>"2 layers" means the model has 2 self-attention blocks. To compare with SP Transformer, see Fig.18.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p>in fact, post-layernorm Transformers are much more sensitive to HPs than pre-layernorm, so our technique is more crucial for them, especially for transfer across width. Fig.1uses post-layernorm.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p>https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_10"><p>Ideally we would like to measure the wall clock time used for tuning. However, smaller models such as the proxy Transformer used for IWSLT are not efficient on GPUs, so wall clock time would not reflect the speedup for larger models like GPT-3. Thus, we measure in FLOPs, which is less dependent on hardware optimization.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11"><p>We do not report the standard deviation over random initializations to avoid confusion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12"><p>We find this provides more reliable result than selecting for the best BLEU score.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13"><p>while the optimal learning is roughly linear in batch size when the latter is small</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14"><p>Here, for simplicity of the example, we model the interaction between "hyperparameters" c 1 , . . . , c k as additive, but in real neural networks such interactions are usually much more complicated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_15"><p>In particular, we are not fixing the total training FLOPs when we scale, which requires understanding the tradeoff of different scale HPs. For example, when we transfer across batch size, we fix the number of steps of training (not the number of epochs), so that the total FLOPs scales linearly.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_16"><p>There's also a literature on the proper initialization for training deep networks effectively (e.g.<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b66">66]</ref>), but they do not study the transferability per se. SeeSection 10.3   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_17"><p>This also applies for SGD, but we need more involved scaling to keep the limit approximately the same.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="23" xml:id="foot_18"><p>Here we tune the 0.5x model instead of the 1x model to simulate the situation that one does "exploratory work" on the 1x model but, when scaling up, would like to tune faster by using a smaller proxy model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_19"><p>Note that the curves on the left are "lower" than curves on the right. This just reflects the increasing capacity of wider models able to fit the training data better, so is orthogonal to our point.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="25" xml:id="foot_20"><p>in the sense that the the variance of the entries are Θ<ref type="bibr" target="#b0">(1)</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_21"><p>Here "correlated" formally means v depends on W in a Tensor Program. This essentially captures all scenarios of "v correlated with W " that occurs in deep learning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_22"><p>Our intuition here is derived from the assumption that width is much larger than training time; of course, as illustrated by our myriad experiments, these intuition are very useful even when this is not the case, such as when training to convergence.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_23"><p>E.g. in a convnet, the (pre-)activations are iid across channels, but correlated across pixels</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_24"><p>In some corner cases when x is uncorrelated with v, then v x = Θ( √ n) by Central Limit, so actually Ax has Θ(1/ √ n)-coordinates. However, this case does not come up much in the context of training neural networks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_25"><p>Adam also has bias correction for the moving averages which can be accomodated easily, but for simplicity we omit them here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_26"><p>This is because every "reasonable" deep learning computation can be expressed in a Tensor Program.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_27"><p>In a convnet, a (pre-)activation vector corresponds to a single pixel across all channels; in general , we expect (pre-)activations are iid across channels, but correlated across pixels</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements In alphabetical order, we thank <rs type="person">Arthur Jacot</rs>, <rs type="person">Arturs Backurs</rs>, <rs type="person">Colin Raffel</rs>, <rs type="person">Denny Wu</rs>, <rs type="person">Di He</rs>, <rs type="person">Huishuai Zhang</rs>, <rs type="person">Ilya Sutskever</rs>, <rs type="person">James Martens</rs>, <rs type="person">Janardhan Kulkarni</rs>, <rs type="person">Jascha Sohl-Dickstein</rs>, <rs type="person">Jeremy Bernstein</rs>, <rs type="person">Lenaic Chizat</rs>, <rs type="person">Luke Metz</rs>, <rs type="person">Mark Chen</rs>, <rs type="person">Michael Santacroce</rs>, <rs type="person">Muhammad ElNokrashy</rs>, <rs type="person">Pengchuan Zhang</rs>, <rs type="person">Sam Schoenholz</rs>, <rs type="person">Sanjeev Arora</rs>, <rs type="person">Taco Cohen</rs>, <rs type="person">Yiping Lu</rs>, <rs type="person">Yisong Yue</rs>, and <rs type="person">Yoshua Bengio</rs> for discussion and help during our research.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Figures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2.3 Hyperparameter Instability of SP Transformers</head><p>Fig. <ref type="figure">18</ref> and Fig. <ref type="figure">20</ref> show the HP instability inherent in SP Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Implementing µTransfer in a Jiffy</head><p>As we have shown, one can enable µTransfer by just reparametrizing the desired model in Maximal Update Parametrization (µP). While conceptually simple, switching from Standard Parametrization (SP) to µP can be error-prone, as popular deep learning frameworks are built around SP. We strive to build a tool that fulfills two goals:</p><p>1. Minimize code changes when switching to µP;</p><p>2. Keep model behavior invariant, under this switch, at a given base model shape.</p><p>By model shape, we mean the collection of dimensions of all parameters of the model. The latter goal, which we call parametrization backward compatibility, ensures that any code base works exactly as before at the base model shape, similar to Eq. ( <ref type="formula">4</ref>), e.g. the loss at any time step remains exactly the same before and after the switch to µP. Of course, when widths start to differ from the base model shape, the model behavior necessarily changes so that HPs can be transferred.   There are two common approaches to setting the base model shape: 1) If one intends to tune a large target model, then the user can set the base model shape to be the shape of the target model (e.g. BERT-large or T5-large), so that the target model itself is in standard parametrization. Then one can tune a proxy model with e.g. width = 124 to obtain the optimal HPs for the target model. In addition, if one wishes to scale up further e.g. width = 1024, then these HPs remain optimal. 2) If one has done exploration on a new idea with a small model and now wishes to scale up, reusing the HP found during this exploration, then one can set the base model shape to be the shape of the exploratory small model. Of course, in both scenarios, depth, batch size, and sequence lengths can be scaled up and down as well according to Fig. <ref type="figure">19</ref> (though note that currently we require users to recreate the base model shape at new depths, since the number of parameters now change with depth).</p><p>The mup Package We provide our tool as a Python package called mup designed to work with PyTorch. The following example illustrates the usage of our package.</p><p>Furthermore, to each such vector v with Θ(1)-sized coordinates, we can associate a random variable Z v , independent of n, that represents the coordinate distribution of v, in such a way that: If vector u is correlated with v, then Z u will also be correlated with Z v , and</p><p>J.1.2 Linear Tensor Product Matrix (e.g. SGD Updates)</p><p>The case of (linear) tensor product matrix can be reduced to the outer product case by linearity. Given u, v, x ∈ R n having approximately iid coordinates (of size Θ(1)) like discussed above, we can form the outer product</p><p>which is the form of a single (batch size 1) gradient update to a weight matrix. Then, by Law of Large Numbers,</p><p>So Ax also has approximately iid coordinates, distributed like</p><p>Notice that each coordinate of A has size Θ(1/n). The above reasoning shows that, in order for Ax to have coordinate size Θ(1) (assuming x does), then Θ(1/n) is the right coordinate size for A, in the general case that v i and x are correlated (as is generically the case during gradient descent, with A = ∆W for some weights W and x being the previous activations). 29   J.1.3 Nonlinear Tensor Product Matrix (e.g. Adam Updates)</p><p>When using Adam or another adaptive optimizer that normalizes the gradient coordinatewise before applying them, we need to modify our argument slightly to obtain the right coordinate size scaling of the matrix. The gradient update A, after such normalization, will take the form of</p><p>, for some ψ : R 2k → R and vectors</p><p>We say a matrix of this form is a nonlinear tensor product matrix.</p><p>First, note the tensor product matrices (e.g. the form of SGD update) discussed previously (Eq. ( <ref type="formula">6</ref>)) already takes this form, with ψ(u</p><p>, so Eq. ( <ref type="formula">7</ref>) is a strict generalization of linear tensor products. Next, for the example of Adam, each gradient update is µ/σ where µ (resp. σ 2 ) is the moving average of previous (unnormalized) gradients (resp. the coordinatewise square of the same). 30 If these unnormalized gradients are the outer products u 1 ⊗ v 1 , . . . , u k ⊗ v k , then the update has coordinates</p><p>where γ i and ω i are the weights involved in the moving averages. Now suppose we have some A ∈ R n×n of the form Eq. ( <ref type="formula">7</ref>), where u i , v i ∈ R n have approximately iid coordinates (of size Θ(1)), and ψ = n -1 ψ where ψ doesn't depend on n (in terms of Adam where ψ corresponds to the ψ of Eq. ( <ref type="formula">8</ref>), this corresponds to using a learning rate of 1/n). Then for x ∈ R n having approximately iid coordinates of size Θ(1), by Law of Large Numbers,</p><p>Here we made the obvious definition</p><p>Thus Ax also has approximately iid coordinates (of size Θ(1)),</p><p>For example, in the SGD example with A = u ⊗ v/n and ψ(u α , v β ) = u α v β , this formula gives Z Ax = Ψ(Z u ) where Ψ(z) = z E Z v Z x , recovering the earlier derivation.</p><p>In any case, the point here is that A has coordinate size Θ(1/n), and this is the unique scaling that leads to Ax having coordinate size Θ(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1.4 Vector Case (e.g. Readout Layer)</head><p>The vector A case is similar to the tensor product cases above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1.5 Gaussian Matrix (e.g. Hidden Weights Initialization)</head><p>Now consider the case where A ∈ R n×n is random Gaussian matrix with A αβ ∼ N (0, 1/n) and x ∈ R n has approximately iid coordinates distributed like Z x . In the context of neural network training, A should be thought of as a randomly initialized weight matrix, and x for example can be taken to be an activation vector in the first forward pass.</p><p>A Quick Intuition By standard random matrix theory, A has Θ(1) operator norm with high probability. Thus, with high probability, for any "typical" vector x, we expect Ax = Θ( x ), even if x is correlated with A. If Ax's coordinates are "evenly distributed", then this would imply Ax has Θ(1)-coordinates if x does. However, this is not so clear. Below we provide intuitions for why this would be the case.</p><p>Intuition for Evenness of Coordinate Distribution If x is independent from A (or sufficiently uncorrelated), then each coordinate (Ax) α has variance E(Z x ) 2 = Θ(1) (so by definition has size Θ(1)). Thus, here A having Θ(1/ √ n)-coordinates leads to Ax having Θ(1)-coordinates, in contrast to the tensor product case above.</p><p>When x is correlated with A, it turns out the same scaling applies (Θ(1/ √ n) is the unique scaling for A's entries such so that Ax has Θ(1) entries), but the reasoning is much more subtle: In the context of neural network training, it turns out all scenario where x is correlated with A can be reduced to the case where x = φ(A y, . . .) for some coordinatewise nonlinearity φ and some other vector R n . 31 Let's consider a very simple example with x = A 1 for the all 1s vector 1 ∈ R n (which has coordinate size Θ(1) as can be checked easily). Then, for each index α ∈ [n], we can calculate</p><p>Since E A 2 αβ = 1/n, by the Law of Large Number, the first sum β A 2 αβ ≈ 1. On the other hand, there are n summands of the form γ =α A αβ A γβ , all iid with variance n-1 n 2 = Θ(1/n). Thus by the Central Limit Theorem, we expect β γ =α A αβ A γβ ≈ N (0, 1). Therefore, each coordinate of (AA 1) α looks like 1 + N (0, 1) = N (1, 1) and thus has size Θ(1); again this is caused by</p><p>This example can be generalized to more general x that is correlated with A, but the mathematics is quite involved. See <ref type="bibr" target="#b56">[56]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Deriving µP for Any Architecture</head><p>Armed with the insight from the last section, we now outline the key steps to derive µP in Table <ref type="table">3</ref> for any architecture. In practice, µP implies the following desiderata Desiderata J.1. At any time during training 1. Every (pre)activation vector in a network should have Θ(1)-sized coordinates 32  Hidden Weights Consider a square weight matrix W ∈ R n×n . Desiderata 1 guarantees that the input x to W has Θ(1)-sized coordinates. Generally, x will be correlated with W . By Table <ref type="table">14</ref>, we can immediately derive Initialization W should be randomly initialized with coordinate size Θ(1/ √ n) LR The learning rate should be scaled so that ∆W has coordinate size Θ(1/n) so that (W 0 + ∆W )x is Θ(1) if x is, inductively satisfying desideratum 1. With Adam, this just means the per-layer LR is Θ(1/n). With SGD and the scaling of output layers above, we can calculate that the gradient of W has Θ(1/n)-coordinates, so the Θ(1) SGD LR derived above suffices as well.</p><p>Input Weights Suppose W ∈ R n×d is an input weight. To satisfy desideratum 1 (i.e. for any input ξ, W ξ should have Θ(1)-coordinates), we want W to have Θ(1)-coordinates. We can initialize W with Θ(1)-coordinates and scale its (per-layer) LR so that ∆W has Θ(1)-coordinates as well. This implies initialization variance of Θ(1) (or Θ(1/fan_in) since fan_in = Θ(1) here) and Adam learning rate Θ(n). As above, we can calculate that the gradient of W has Θ(1/n)-coordinates, so we want SGD learning rate Θ(n).</p><p>Biases Biases follow the same reasoning as input weights (just think of it as an input weight with input 1).</p><p>Attention Suppose the key dimension d k is tending to infinity with width with number of heads n head fixed. Then the key-query contraction q k ∈ R scales like Θ(d k ) by Law of Large Numbers (instead of Central Limit Theorem because q and k are generally correlated) and desideratum 1, hence the 1/d k we propose rather than 1/ √ d k .</p><p>Now suppose instead that n head tends to infinity with width with d k fixed. Let K, Q ∈ R N ×d k ×n head , V ∈ R N ×dv×n head be keys, queries, and values across all heads and tokens. Thinking of N × d k as constants, we may view attention as a nonlinearity coordinatewise in the n head dimension. Then it's clear that our parametrization described above already works.</p><p>Finally, we may freely let d k and n head both tend to infinity, and the above reasoning shows that our parametrization still works.</p><p>Changing Width Ratios As noted above, at any time in training, every (pre-)activation vector will have approximately iid coordinates (of order Θ(1) by desideratum 1). Another desideratum for µP is to ensure that this coordinate distribution (at any particular time) stays roughly invariant as widths increases. When all layer widths are tied, this is automatic if the other desiderata are satisfied, hence why we did not list this above.</p><p>When width ratios vary, this is not automatic. In this case, we need to choose whether to replace each n with fan-in or fan-out (or some function of them). Making the wrong choices will let the coordinate distributions vary with width ratios.</p><p>Obviously, we should replace n with fan-in for the output layers and with fan-out for the input layers since they are the only dimension scaling with n. For the hidden weights, we replace n with fan-in so that the forward pass is preserved. When using Adam (and assuming the initialization of W is quickly dominated by the change in W ), this ensures that the (pre-)activation coordinate distributions are preserved at any time during training even if we vary widths in different layers differently. (For SGD this doesn't quite work in general because the varying width ratios change the gradient sizes of different layers differently, whereas Adam always normalizes the gradient coordinatewise).</p><p>Convolution A convolution weight tensor W ∈ R fan_out×fan_in×s1×s2 with kernel size s 1 × s 2 can be thought of just as a s 1 s 2 = Θ(1)-sized collection of fan_out × fan_in dense weights. Then all of our discussions above apply accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3 Why Other Parametrizations Cannot Admit Hyperparameter Transfer</head><p>Standard Parametrization (SP) SP doesn't work essentially because it leads to blow-up in the infinite-width limit.</p><p>1. For Adam with LR Θ(1), ∆W would have Θ(1)-coordinates, causing preactivations to blow up like Θ(n) by Desideratum 1 and Table <ref type="table">14</ref>. We can avoid this blowup with LR Θ(1/n),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia/Deeplearningexamples</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/DeepLearningExamples" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://github.com/davidcpage/cifar10-fast" />
		<title level="m">Davidnet, mit license</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://github.com/autonomio/talos" />
		<title level="m">Autonomio talos, mit license</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">ReZero is All You Need: Fast Convergence at Large Depth</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Huanru</surname></persName>
		</author>
		<author>
			<persName><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Garrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04887</idno>
		<ptr target="http://arxiv.org/abs/2003.04887" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03432</idno>
		<ptr target="http://arxiv.org/abs/2002.03432" />
		<title level="m">On the distance between two neural networks and the stability of learning</title>
		<imprint>
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Layer rotation: a surprisingly powerful indicator of generalization in deep networks?</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Carbonnelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><forename type="middle">De</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01603</idno>
		<ptr target="http://arxiv.org/abs/1806.01603" />
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunlong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01883</idno>
		<ptr target="http://arxiv.org/abs/2105.01883" />
		<title level="m">RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition</title>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08741</idno>
		<ptr target="http://arxiv.org/abs/1705.08741" />
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hyperparameter transfer learning with adaptive complexity</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Archambeau</surname></persName>
		</author>
		<idno>CoRR, abs/2102.12810</idno>
		<ptr target="https://arxiv.org/abs/2102.12810" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving Transformer Optimization Through Better Initialization</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Pérez</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<ptr target="http://arxiv.org/abs/1806.07572" />
		<title level="m">Neural Tangent Kernel: Convergence and Generalization in Neural Networks</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Non-stochastic best arm identification and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<ptr target="http://arxiv.org/abs/2001.08361" />
		<title level="m">Scaling Laws for Neural Language Models</title>
		<imprint>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Neural Networks as Gaussian Processes</title>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1EA-M-0Z" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<title level="m">A system for massively parallel hyperparameter tuning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</title>
		<author>
			<persName><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<ptr target="http://arxiv.org/abs/2105.08050" />
		<title level="m">Pay Attention to MLPs</title>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.463</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.463" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding the Difficulty of Training Transformers</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08249</idno>
		<ptr target="http://arxiv.org/abs/2004.08249" />
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning by Turning: Neural Architecture Aware Optimisation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07227</idno>
		<ptr target="http://arxiv.org/abs/2102.07227" />
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11271</idno>
		<idno>arXiv: 1804.11271</idno>
		<ptr target="http://arxiv.org/abs/1804.11271" />
		<title level="m">Gaussian Process Behaviour in Wide Deep Neural Networks</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Openai</forename><surname>Dota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06162</idno>
		<ptr target="http://arxiv.org/abs/1812.06162" />
		<title level="m">An Empirical Model of Large-Batch Training</title>
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02723</idno>
		<ptr target="http://arxiv.org/abs/2105.02723" />
		<title level="m">Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet</title>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling, mit license</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.03776v1" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>bsd-style license</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">In</forename><forename type="middle">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Valerio Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><forename type="middle">W</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedric</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><surname>Archambeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scalable Hyperparameter Transfer Learning. NeurIPS</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training Tips for the Transformer Model</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<idno type="DOI">10.2478/pralin-2018-0002</idno>
		<ptr target="http://content.sciendo.com/view/journals/pralin/110/1/article-p43.xml" />
	</analytic>
	<monogr>
		<title level="m">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="43" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<ptr target="http://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Unified Text-to-Text Transformer.</note>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A Constructive Prediction of the Generalization Error Across Scales</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">S</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12673</idno>
		<ptr target="http://arxiv.org/abs/1909.12673" />
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01232</idno>
		<ptr target="http://arxiv.org/abs/1611.01232" />
		<title level="m">Deep Information Propagation</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Measuring the Effects of Data Parallelism on Neural Network Training</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03600</idno>
		<ptr target="http://arxiv.org/abs/1811.03600" />
		<imprint>
			<date type="published" when="2018-11">November 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804.04235v1" />
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<ptr target="http://arxiv.org/abs/1909.08053" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Don&apos;t Decay the Learning Rate</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00489</idno>
		<ptr target="http://arxiv.org/abs/1711.00489" />
	</analytic>
	<monogr>
		<title level="m">Increase the Batch Size</title>
		<imprint>
			<date type="published" when="2017-11">November 2017</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Mostofa</forename><surname>Ali Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
		<title level="m">Scalable bayesian optimization using deep neural networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Danny</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Jörg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Selg</surname></persName>
		</author>
		<author>
			<persName><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WPO0vDYLXem" />
		<title level="m">Hyperparameter transfer across developer adjustments</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><surname>Mlp-Mixer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<ptr target="http://arxiv.org/abs/2105.01601" />
		<title level="m">An all-MLP Architecture for Vision</title>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><surname>Resmlp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<ptr target="http://arxiv.org/abs/2105.03404" />
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12478</idno>
		<ptr target="http://arxiv.org/abs/1910.12478" />
		<imprint>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, physics:math-ph</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04760</idno>
		<ptr target="http://arxiv.org/abs/1902.04760" />
		<imprint>
			<date type="published" when="2019-02">February 2019</date>
		</imprint>
	</monogr>
	<note>cond-mat, physics:math-ph, stat</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14548</idno>
		<ptr target="http://arxiv.org/abs/2006.14548" />
		<title level="m">Tensor Programs II: Neural Tangent Kernel for Any Architecture</title>
		<imprint>
			<date type="published" when="2020-08">August 2020</date>
		</imprint>
	</monogr>
	<note>cond-mat, stat</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10685</idno>
		<ptr target="http://arxiv.org/abs/2009.10685" />
		<title level="m">Tensor Programs III: Neural Matrix Laws</title>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feature learning in infinite-width neural networks</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03703</idno>
		<ptr target="http://arxiv.org/abs/2105.03703" />
		<title level="m">Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics</title>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deep Mean Field Theory: Layerwise Variance and Width Variation as Methods to Control Gradient Explosion</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJGY8GbR-" />
		<imprint>
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08969</idno>
		<ptr target="http://arxiv.org/abs/1712.08969" />
		<title level="m">Mean Field Residual Networks: On the Edge of Chaos</title>
		<imprint>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
	<note>cond-mat, physics:nlin</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Efficient computation of deep nonlinear infinite-width neural networks that learn features</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Santacroce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tUMr0Iox8XW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient Transfer Learning Method for Automatic Hyperparameter Tuning</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v33/yogatama14.html" />
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Large Batch Training of Convolutional Networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<ptr target="http://arxiv.org/abs/1708.03888" />
		<imprint>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<ptr target="http://arxiv.org/abs/1904.00962" />
		<title level="m">Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</title>
		<imprint>
			<date type="published" when="2020-01">January 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Residual Learning Without Normalization via Better Initialization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1gsz30cKX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">D.4 Enlarge d k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">D.5 Non-Gaussian vs Gaussian Initialization . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">D.6 Using a Larger Sequence Length . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">D.7 Tuning Per-Layer Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . E Which Hyperparameters Can Be Transferred</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">E.1 Further Discussions on Hyperparameter Categories . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">E</forename></persName>
			<affiliation>
				<orgName type="collaboration">2 On the Definitions of Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">F Experimental Details F.1 IWSLT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">F.2 WMT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">F.3 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">F.4 GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">G Additional Experiments G.1 Experiments on ResNets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">G.1.1 ResNet on CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename></persName>
			<affiliation>
				<orgName type="collaboration">2 Wide ResNet on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">G.2.1 Verifying Transfer across Batch Size, Sequence Length, and</title>
		<author>
			<orgName type="collaboration">G.2 Experiments on Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . ; Training Time on Wikitext-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">G.2.2 Post-Layernorm Transformers . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">H Implementing µTransfer in a Jiffy I Reverse-µTransfer for Diagnosing Training Instability in Large Models J An Intuitive Introduction to the Theory of Maximal Update Parametrization J.1 Behaviors of Gaussian Matrices vs</title>
		<author>
			<orgName type="collaboration">G.2.3 Hyperparameter Instability of SP Transformers . . . . . . . . . . . . . . ; Tensor Product Matrices . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Preparation for the Derivations . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">SGD Updates) . . . . . . . . . . . .</orgName>
		</author>
		<editor>J.1.2 Linear Tensor Product Matrix</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Nonlinear Tensor Product Matrix</title>
		<author>
			<orgName type="collaboration">Adam Updates) . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Readout Layer) . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<editor>J.1.4 Vector Case</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Hidden Weights Initialization) . . . . . . . . . . .</orgName>
		</author>
		<editor>J.1.5 Gaussian Matrix</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename></persName>
			<affiliation>
				<orgName type="collaboration">2 Deriving µP for Any Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . J.2.1 µP Derivation From the Desiderata . . . . . . . . . . . . . . . . . . . .</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Why Other Parametrizations Cannot Admit Hyperparameter Transfer</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
