- Decision to utilize Maximal Update Parametrization (µP) for hyperparameter tuning
- Choice of zero-shot hyperparameter transfer method (µTransfer)
- Selection of model architectures for verification (Transformer and ResNet)
- Determination of hyperparameters to be transferred (e.g., learning rate, batch size)
- Decision to focus on training loss for evaluating hyperparameter transfer efficacy
- Choice of experimental setup (e.g., model sizes, datasets)
- Decision to implement the technique in PyTorch
- Choice of performance metrics for comparison (e.g., training loss, test loss)
- Decision to release a public implementation of µTransfer
- Assumption regarding hyperparameter stability across model sizes
- Decision to explore the impact of model width on hyperparameter performance
- Choice to document the theoretical foundation of µP and its implications
- Decision to compare µTransfer against conventional hyperparameter tuning methods
- Assumption about the scalability of hyperparameters across different model architectures
- Decision to include a discussion on the limitations of hyperparameter transfer in certain scenarios
- Choice to analyze the compute utilization benefits of the proposed method
- Decision to structure the paper as part of the Tensor Programs series
- Choice to include appendices for detailed mathematical explanations and additional insights