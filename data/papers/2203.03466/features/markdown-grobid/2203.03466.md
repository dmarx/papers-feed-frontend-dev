# Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer

## Abstract

## 

Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (µP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call µTransfer: parametrize the target model in µP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify µTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via pip install mup.

## 

1 Introduction 20 18 16 14 12 10 log2LearningRate 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 Training Loss optimum shifts Standard Practice Width 128 256 512 1024 2048 4096 8192 20 18 16 14 12 10 log2LearningRate optimum stable Our Work Conventionally and in contrast with our technique, different widths do not share the same optimal hyperparameter; wider networks do not always perform better than narrower ones; in fact they underperform the same-width networks in our technique even after tuning learning rate (see dashed line). See Sections 3 and 4 for experimental setup.

Hyperparameter (HP) tuning is critical to deep learning. Poorly chosen HPs result in subpar performance and training instability. Many published baselines are hard to compare to one another due to varying degrees of HP tuning. These issues are exacerbated when training extremely large deep learning models, since stateof-the-art networks with billions of parameters become prohibitively expensive to tune.

Recently, [[57]](#b57) showed that different neural network parametrizations induce different infinitewidth limits and proposed the Maximal Update Parametrization (abbreviated µP) (summarized in Table [3](#tab_12)) that enables "maximal" feature learning in the limit. Intuitively, it ensures that each layer is updated on the same order during training regardless of width. 2 In contrast, while the standard parametrization (SP) ensures activations are of unit order at initialization, it actually causes them to blow up in wide models during training [[57]](#b57) essentially due to an imbalance of per-layer learning rate (also see Fig. [5](#)). We leverage µP to zero-shot transfer HPs from small models to large models in this work -that is, we obtain near optimal HPs on a large model without directly tuning it at all! While practitioners have always guessed HPs of large models from those of small models, the results are hit-or-miss at best because of incorrect parametrization. For example, as shown in Fig. [1](#fig_0), in a Transformer, the optimal learning rate is stable with width in µP (right) but far from so in standard parametrization (left). In addition to width, we empirically verify that, with a few caveats, HPs can also be transferred across depth (in Section 6.1) as well as batch size, language model sequence length, and training time (in Appendix G.2.1). This reduces the tuning problem of an (arbitrarily) large model to that of a (fixed-sized) small model. Our overall procedure, which we call µTransfer, is summarized in Algorithm 1 and Fig. [2](#fig_1), and the HPs we cover are summarized in Tables [1](#tab_1) and [2](#tab_2). There are several benefits to our approach: 1. Better Performance: µTransfer is not just about predicting how the optimal learning rate scales in SP. In general, we expect the µTransferred model to outperform its SP counterpart with learning rate optimally tuned. For example, this is the case in Fig. [1](#fig_0) with the width-8192 Transformer. We discuss the reason for this in Section 5 and Appendix C. 2. Speedup: It provides massive speedup to the tuning of large models. For example, we are able to outperform published numbers of (350M) BERT-large [[11]](#b10) purely by zero-shot HP transfer, with tuning cost approximately equal to 1 BERT-large pretraining. Likewise, we outperform the published numbers of the 6.7B GPT-3 model [[7]](#b6) with tuning cost being only 7% of total pretraining cost. For models on this scale, HP tuning is not feasible at all without our approach. 3. Tune Once for Whole Family: For any fixed family of models with varying width and depth (such as the BERT family or the GPT-3 family), we only need to tune a single small model and can reuse its HPs for all models in the family. [3](#foot_0) For example, we will use this technique to tune BERT-base (110M parameters) and BERT-large (350M parameters) simultaneously by transferring from a 13M model. [4](#b3). Better Compute Utilization: While large model training needs to be distributed across many GPUs, the small model tuning can happen on individual GPUs, greatly increasing the level of parallelism for tuning (and in the context of organizational compute clusters, better scheduling and utilization ratio). 5. Painless Transition from Exploration to Scaling Up: Often, researchers explore new ideas on small models but, when scaling up, find their HPs optimized during exploration work poorly on large models. µTransfer would solve this problem.

In addition to the HP stability property, we find that wider is better throughout training in µP, in contrast to SP (Section 8). This increases the reliability of model scaling in deep learning.

In this work, we primarily focus on hyperparameter transfer with respect to training loss. In settings where regularization is not the bottleneck to test performance, as in all of our experiments here, this also translates to efficacy in terms of test loss. In other settings, such as finetuning of models on small datasets, µTransfer may not be sufficient, as we discuss in Section 6.1. • We demonstrate it is possible to zero-shot transfer near optimal HPs to a large model from a small version via the Maximal Update Parametrization (µP) from [[57]](#b57). • While [[57]](#b57) only covered SGD, here we derive µP for Adam as well (Table [3](#tab_12)).

• We propose a new HP tuning technique, µTransfer, for large neural networks based on this observation that provides massive speedup over conventional methods and covers both SGD and Adam training; • We thoroughly verify our method on machine translation and large language model pretraining (in Section 7.3) as well as image classification (in Appendix G.1); • We release a PyTorch [[35]](#b34) package for implementing µTransfer painlessly. A sketch of this package is given in Appendix H.

Terminologies Sometimes, to be less ambiguous, we often refer to the "large model" as the target model, as it is the model we wish to ultimately tune, while we refer to the "small model" as the proxy model, as it proxies the HP tuning process. We follow standard notation d model , d head = d k , d v , n head , d f f n regarding dimensions in a Transformer; one can see Fig. [11](#fig_15) for a refresher.

Tensor Programs Series This paper is the 5th installment of the Tensor Programs series. While it is self-contained with the target audience being practitioners and empirical researchers, this paper presents the first major practical payoff of the theoretical foundation built in previous works [[53]](#b53)[[54]](#b54)[[55]](#b55)[[56]](#b56)[[57]](#b57)[[58]](#b58).

## Parametrization Matters: A Primer

In this section, we give a very basic primer on why the correct parametrization can allow HP transfer across width, but see Appendices J.1 to J.3 for more (mathematical) details.

The Central Limit Theorem (CLT) says that, if x 1 , . . . , x n are iid samples from a zero-mean, unitvariance distribution, then 1 √ n (x 1 + • • • + x n ) converges to a standard Gaussian N (0, 1) as n → ∞. Therefore, we can say that 1 √ n is the right order of scaling factor c n such that c n (x 1 + • • • + x n ) converges to something nontrivial. In contrast, if we set c n = 1/n, then c n (x

$1 + • • • + x n ) → 0; or if c n = 1, then c n (x 1 + • • • + x n ) blows up in variance as n → ∞.$Now suppose we would like to minimize the function Then for sufficiently large n, the optimal α * n def = arg min α G n (α) should be close to α * N for any N > n, and indeed, for N = ∞ -this precisely means we can transfer the optimal c * n or α * n for a smaller problem (say F n ) to a larger problem (say F N ): G N is approximately minimized by α * n and F N is approximately minimized by c * n n/N . Because the transfer algorithm is simply copying α, we say the parametrization c = α/ √ n is the correct parametrization for this problem.

$F n (c) def = E x1,...,xn f (c(x 1 + • • • + x n ))(1$In the scenario studied in this paper, x 1 , . . . , x n are akin to randomly initialized parameters of a width-n neural network, c is akin to a HP such as learning rate, and f is the test-set performance of the network after training, so that F n gives its expectation over random initializations. Just as in this example, if we parametrize the learning rate and other HPs correctly, then we can directly copy the optimal HPs for a narrower network into a wide network and expect approximately optimal performance -this is the (zero-shot) hyperparameter transfer we propose here. It turns out the Maximal Update Parametrization (µP) introduced in [[57]](#b57) is correct (akin to the parametrization in α above), while the standard parametrization (SP) is incorrect (akin to the parametrization in c). We will review both parametrizations shortly. Theoretically, a µP network has a well-defined infinite-width limit -akin to (x 1 + • • • + x n )/ √ n having a N (0, 1) limit by CLT -while a SP network does not (the limit will blow up) [[57]](#b57). [4](#foot_1) In fact, based on the theoretical foundation laid in [[57]](#b57), we argue in Appendix J.3 that µP should also be the unique parametrization that allows HP transfer across width. For a more formal discussion of the terminologies parametrization and transfer, see Appendix A We emphasize that, to ensure transferability of any hyperparameter (such as learning rate), it's not sufficient to reparametrize only that hyperparameter, but rather, we need to identify and correctly reparametrize all hyperparameters in Table [2](#tab_2). For example, in Fig. [1](#fig_0), the wide models in SP still underperform their counterparts in µP, even with learning rate tuned optimally. This is precisely because SP does not scale parameter multipliers and input/output layer learning rates correctly in contrast to µP (see Table [3](#tab_12)). See Appendix C for more intuition via a continuation of our example here. We shall also explain this more concretely in the context of neural networks in Section 5.

## Hyperparameters Don't Transfer Conventionally

In the community there seem to be conflicting assumptions about HP stability. A priori, models of different sizes don't have any reason to share the optimal HPs. Indeed, papers aiming for stateof-the-art results often tune them separately. On the other hand, a nontrivial fraction of papers in deep learning fixes all HPs when comparing against baselines, which reflects an assumption that the optimal HPs should be stable -not only among the same model of different sizes but also among models of different designs -therefore, such comparisons are fair. Here, we demonstrate HP instability across width explicitly in MLP and Transformers in the standard parametrization. We will only look at training loss to exclude the effect of regularization.

## MLP with Standard Parametrization

We start with a 2-hidden-layer MLP with activation function φ, using the standard parametrization [5](#foot_2) with LeCun initialization [6](#foot_3) akin to the default in PyTorch: where

$f (ξ) = W 3 φ(W 2 φ(W 1 ξ + b 1 ) + b 2 ) with init. W 1 ∼ N (0, 1 /din), W {2,3} ∼ N (0, 1 /n), b {1,2} = 0,(2)$$W 1 ∈ R din×n , b 1 ∈ R n , W 2 ∈ R n×n , b 2 ∈ R n , W 3 ∈ R n×dout and$d in , n, and d out are the input, hidden, and output dimensions. The particular MLP we use has φ = ReLU and a cross-entropy (xent) loss function. We define the width of MLP as the hidden size n, which is varied from 256 to 8192. The models are trained on CIFAR-10 for 20 epochs, which is more than enough to ensure convergence.

As shown on the left in Fig. [3](#fig_3), the optimal learning rate shifts by roughly an order of magnitude as the width increases from 256 to 8192; using the optimal learning of the smallest model on the largest model gives very bad performance, if not divergence.

Transformer with Standard Parametrization This perhaps unsurprising observation holds for more complex architectures such as Transformer as well, as shown in Fig. [1](#fig_0) (left). We define width Table [3](#tab_12): µP [[57]](#b57) and SP for General Neural Networks. Here, we emphasize the scaling with width (fan_in or fan_out); in practice, we may insert tunable multipliers in front of fan_in and fan_out as in Eq. ( [4](#formula_7)). The fan_out of a bias vector is its dimension (whereas fan_in is 1). Purple text highlights key differences from standard parametrization (SP); Gray text recalls the corresponding SP. SGD (resp. Adam) here can be replaced by variants such as SGD with momentum (resp. Adagrad, etc); see Appendix B.3 for other optimizers. In general, the three columns here can be interpreted as linear layers that have {finite, infinite, infinite} input dimension and {infinite, finite, infinite} output dimension in an infinite-width network; this description generalizes more readily to other parameters such as those of layernorm. Transformer µP requires one more modification (1/d attention instead of 1/ √ d); see Definition 4.1. This version of µP gets rid of parameter multipliers; for the version similar to that in [[57]](#b57), see Table [9](#tab_13). Also see Table [8](#) for a µP formulation that is easier to implement (and compatible with input/output weight sharing). Further explanation of this table can be found in Appendix B. Its derivation can be found in Appendix J.

Input weights & all biases Output weights Hidden weights

$Init. Var. 1 /fan_in 1 /fan_in 2 ( 1 /fan_in) 1 /fan_in SGD LR fan_out (1) 1 /fan_in (1) 1 Adam LR 1 1 /fan_in (1) 1 /fan_in (1)$as d model , with

$d k = d q = d v = d model/n head and d f f n = 4d model .$The models are trained on wikitext-2 for 5 epochs. In Fig. [18](#fig_15) in the appendix we also show the instability of initialization scale and other HPs.

## Unlocking Zero-Shot Hyperparameter Transfer with µP

We show that µP solves the problems we see in Section 3.

MLP with µP For the MLP in Section 3, to switch to µP, we just need to modify Eq. ( [2](#formula_2))'s initialization of the last layer and its learning rates of the first and last layer as well as of the biases. The basic form is [7](#foot_4)initialize

$W 1 ∼ N (0, 1 /din), W 2 ∼ N (0, 1 /n), W 3 ∼ N (0, 1 /n 2 ), b {1,2} = 0 with SGD learning rates η W 1 = η b 1 = η b 2 = ηn, η W 2 = η, η W 3 = ηn -1 .(3)$Here, η specifies the "master" learning rate, and we highlighted in purple the differences in the two parametrizations. This basic form makes clear the scaling with width n of the parametrization, but in practice we will often insert (possibly tune-able) multiplicative constants in front of each appearance of n. For example, this is useful when we would like to be consistent with a SP MLP at a base width n 0 . Then we may insert constants as follows: For

$ñ def = n/n 0 , initialize W 1 ∼ N (0, 1 /din), W 2 ∼ N (0, 1 /n), W 3 ∼ N (0, 1 /n•ñ), b {1,2} = 0 with SGD learning rates η W 1 = η b 1 = η b 2 = ηñ, η W 2 = η, η W 3 = ηñ -1 .(4)$Then at width n = n 0 , all purple factors above are 1, and the parametrization is identical to SP (Eq. ( [2](#formula_2))) at width n 0 . Of course, as n increases from n 0 , then Eq. ( [4](#formula_7)) quickly deviates from Eq. ( [2](#formula_2)). In other words, for a particular n, µP and SP can be identical up to the choice of some constants (in this case n 0 ), but µP determines a different "set" of networks and optimization trajectory than SP as one varies n. As we will see empirically in the next section, this deviation is crucial for HP transfer.

Indeed, in Fig. [3](#fig_3)(right), we plot the CIFAR10 performances, over various learning rates and widths, of µP MLPs with n 0 = 128. In contrast to SP, the optimal learning rate under µP is stable. This means that, the best learning rate for a width-128 network is also best for a width-8192 network in µP -i.e. HP transfer works -but not for SP. In addition, we observe performance for a fixed learning rate always weakly improves with width in µP , but not in SP.

This MLP µP example can be generalized easily to general neural networks trained under SGD or Adam, as summarized in Table [3](#tab_12), which is derived in Appendix J. When not specified in the legend, the width used is 256, depth 2, batch size 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to each column, while fixing all others constant. See Section 6.1 for discussion of these results.

Transformers with µP We repeat the experiments with base width n 0 = 128 for Transformers: Definition 4.1. The Maximal Update Parametrization (µP) for a Transformer is given by Table [3](#tab_12) and 1/d attention instead of 1/ √ d, i.e. the attention logit is calculated as q k/d instead of q k/ √ d where query q and key k have dimension d. [8](#foot_5)The results are shown on the right in Fig. [1](#fig_0), where the optimal learning rate is stable, and the performance improves monotonically with width. See Appendix B for further explanation of µP.

## The Defects of SP and How µP Fixes Them

The question of SP vs µP has already been studied at length in [[57]](#b57). Here we aim to recapitulate the key insights, with more explanations given in Appendix J.3.

## An Instructive Example

As shown in [[57]](#b57) and Appendix J.3, in SP, the network output will blow up with width after 1 step of SGD. It's instructive to consider a 1-hidden-layer linear perceptron f (x) = V U x with scalar inputs and outputs, as well as weights

$V, U ∈ R n×1 . In SP, V α ∼ N (0, 1/n) ad U α ∼ N (0, 1) for each α ∈ [n]$. This sampling ensures that f (x) = Θ(|x|) at initialization. After 1 step of SGD with learning rate 1, the new weights are V ← V + θU, U ← U + θV , where θ is some scalar of size Θ(1) depending on the inputs, labels, and loss function. But now

$f (x) = V U x = (V U + θU U + θV V + θ 2 U V )x(5)$blows up with width n because U U = Θ(n) by Law of Large Numbers. Now consider the same network in µP. According to Table [3](#tab_12), we now have V α ∼ N (0, 1/n 2 ) in contrast to SP, but U α ∼ N (0, 1) as before, with learning rates η V = 1/n, η U = n. After 1 step of SGD, we now have

$f (x) = (V U + θn -1 U U + θnV V + θ 2 U V )x,$0.0 0.5 1.0 1.5 SP std(xt x0) logits t 0 1 2 3 4 0 20 40 60 attn logits 0.0000 0.0005 0.0010 0.0015 0.0020 word embedding 0 2000 4000 width 0.00 0.05 0.10 0.15 P std(xt x0) 0 2000 4000 width 0.000 0.025 0.050 0.075 0.100 0.125 0 2000 4000 width 0.0000 0.0005 0.0010 0.0015 Figure 5: Logits and attention logits, but not word embeddings, of a Transformer blow up with width in SP after 1 step of training. In contrast, all three are well-behaved with width in µP. Here we measure how much different values change coordinatewise from initialization over 4 steps of Adam updates, as a function of width. Specifically, we plot the standard deviation of the coordinates of x t -x 0 , for t = 0, . . . , 4, and x ∈ {logits, attention logits, word embeddings}, where t = 0 indicates initialization. and one can verify this is Θ(1) and thus does not blow up with width.[foot_6](#foot_6)

Some Layers Update Too Fast, Others Too Slow One can observe the same behavior in more advanced architectures like Transformers and optimizers like Adam; in fact, in SP, other hidden quantities like attention logits will also blow up with width after 1 step, but in µP still remain bounded, as shown in Fig. [5](#)(middle).

One might think scaling down the learning rate with width can solve this problem in SP. However, other hidden activations like the word embedding (Fig. [5](#)(right)) in a Transformer update by a widthindependent amount for each step of training, so scaling down the learning rate will effectively mean the word embeddings are not learned in large width models. Similar conclusions apply to other models like ResNet (in fact, one can observe in the SP linear MLP example above, the input layer is updated much more slowly than the output layer). On the other hand, µP is designed so that all hidden activations update with the same speed in terms of width (see Appendix J.2 for why).

Performance Advantage of µP This is why a wide model tuned with µTransfer should in general outperform its SP counterpart with (global) learning rate tuned. For example, this is the case for the width-8192 Transformer in Fig. [1](#fig_0), where, in SP, the optimal learning rate needs to mollify the blow-up in quantities like logits and attention logits, but this implies others like word embeddings do not learn appreciably. This performance advantage means µTransfer does more than just predicting the optimal learning rate of wide SP models. Relatedly, we observe, for any fixed HP combination, training performance never decreases with width in µP, in contrast to SP (e.g., the µP curves in Figs. 1, 3 and 16 do not cross, but the SP curves do; see also Section 8).

6 Which Hyperparameters Can Be µTransferred?

In this section, we explore how common HPs fit into our framework. In general, they can be divided into three kinds, summarized in Table [1:](#tab_1) 1. those that can transfer from the small to the large model, such as learning rate (Table [2](#tab_2)); 2. those that primarily control regularization and don't work well with our technique; and 3. those that define training scale, such as width as discussed above as well as others like depth and batch size, across which we transfer other HPs.

Those in the first category transfer across width, as theoretically justified above in Section 2. To push the practicality and generality of our technique, we empirically explore the transfer across the other dimensions in the third category. Note that µTransfer across width is quite general, e.g. it allows varying width ratio of different layers or number of attention heads in a Transformer; see Appendix E.2. This will be very useful in practice. For the second category, the amount of regularization (for the purpose of controlling overfitting) naturally depends on both the model size and data size, so we should not expect transfer to work if the parametrization only depends on model size. We discuss these HPs in more detail in Appendix E.1.

## Empirical Validation and Limitations

Our empirical investigations focus on Transformers (here) and ResNet (in Appendix G.1.1), the most popular backbones of deep learning models today. We train a 2-layer pre-layernorm µP [10](#foot_7)Transformer with 4 attention heads on Wikitext-2. We sweep one of four HPs (learning rate, output weight multiplier, initialization standard deviation, and learning rate schedule) while fixing the others and sweeping along width and depth (with additional results in Fig. [19](#fig_15) on transfer across batch size, sequence length, and training time). Fig. [4](#fig_4) shows the results averaged over 5 random seeds.

Empirically, we find that for language modeling on Transformers, HPs generally transfer across scale dimensions if some minimum width (e.g. 256), depth (e.g., 4), batch size (e.g., 32), sequence length (e.g., 128), and training steps (e.g., 5000) are met, and the target scale is within the "reasonable range" as in our experiments. Now, there are some caveats. While the exact optimum can shift slightly with increasing scale, this shift usually has very small impact on the loss, compared to SP (Figs. 1 and 3(left)). However, there are some caveats. For example, the best initialization standard deviation does not seem to transfer well across depth (2nd row, 3rd column), despite having a stabler optimum across width. In addition, while our results on width, batch size, sequence length, and training time still hold for post-layernorm (Fig. [17](#fig_15)), [11](#foot_8) the transfer across depth only works for pre-layernorm Transformer. Nevertheless, in practice (e.g. our results in Section 7.3) we find that fixing initialization standard deviation while tuning other HPs works well when transferring across depth.

## Efficiency and Performance of µTransfer

Now that the plausibility of µTransfer has been established in toy settings, we turn to more realistic scenarios to see if one can achieve tangible gains. Specifically, we perform HP tuning only on a smaller proxy model, test the obtained HPs on the large target model directly, and compare against baselines tuned using the target model. We seek to answer the question: Can µTransfer make HP tuning more efficient while achieving performance on par with traditional tuning? As we shall see by the end of the section, the answer is positive. We focus on Transformers here, while experiments on ResNets on CIFAR10 and Imagenet can be found as well in Appendix G.1. All of our experiments are run on V100 GPUs.

## Transformer on IWSLT14 De-En

Setup IWSLT14 De-En is a well-known machine translation benchmark. We use the default IWSLT (post-layernorm) Transformer implemented in fairseq [[33]](#b32) with 40M parameters, which we denote as the 1x model. [12](#foot_9) For µTransfer, we tune on a 0.25x model with 1/4 of the width, amounting to 4M parameters. For this experiment, we tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn . See the grid and other experimental details in Appendix F.1.

We compare transferring from the 0.25x model with tuning the 1x model while controlling the total tuning budget in FLOPs. [13](#foot_10) To improve the reproducibility of our result: 1) we repeat the entire HP search process (a trial) 25 times for each setup, with number of samples as indicated in Table [4](#), and report the 25th, 50th, 75th, and 100th percentiles in BLEU score; 2) we evaluate each selected HP combination using 5 random initializations and report the mean performance.

14 Table 4: Transformer on IWSLT14 De-En. 1x and 0.25x refers to scaling of width only. Compared to traditional tuning ("Tuning on 1x"), µTransfer from 0.25x provides better and more reliable outcome given fixed amount of compute. On the other hand, naive transfer (i.e. with SP instead of µP) fails completely. The percentiles are over independent trials, with each trial involving the entire tuning process with a new HP random search. Val. BLEU Percentiles Setup Total Compute #Samples 25 50 75 100 fairseq[33] default -----35.40 Tuning on 1x 1x 5 33.62 35.00 35.35 35.45 Naive transfer from 0.25x 1x 64 training diverged µTransfer from 0.25x (Ours) 1x 64 35.27 35.33 35.45 35.53

We pick the HP combination that achieves the lowest validation loss [15](#foot_12) for each trial. The reported best outcome is chosen according to the validation loss during tuning. We compare against the default in fairseq, which is presumably heavily tuned. The result is shown in Table [4](#).

4 3 2 1 0 1 2 3 log2Compute 34.4 34.6 34.8 35.0 35.2 35.4 BLEU Score Method Ours Conventional 10 20 30 40 50 60 # Samples Method Ours Conventional Figure 6: Efficiency-performance Pareto frontier of µTransfer compared to conventional tuning, on IWSLT Transformer, using random HP search as the base method. We plot the median BLEU score over 25 trials (Left) against relative compute budget in log scale and (Right) against number of HP samples taken. While with the same number of samples, µTransfer slightly underperforms conventional tuning, this gap vanishes with more samples, and in terms of compute, our Pareto frontier strongly and consistently dominates that of conventional tuning. Note that, in larger models (e.g. BERT or GPT-3, not shown here), we believe our efficiency advantage will only widen as our small proxy model can stay the same size while the target model grows. Performance Pareto Frontier The result above only describes a particular compute budget. Is µTransfer still preferable when we have a lot more (or less) compute? To answer this question, we produce the compute-performance Pareto frontier in Fig. 6(left), where we repeat the above experiment with different compute budgets. Evidently, our approach completely dominates conventional tuning.

## Sample Quality of Proxy Model vs Target

Model The Pareto frontier in Fig. [6](#)(right) suggests that, given a fixed number of random samples from the HP space, 1) tuning the target model directly yields slightly better results than tuning the proxy model (while taking much more compute of course), but 2) this performance gap seems to vanish as more samples are taken. This can be explained by the intuition that the narrower proxy model is a "noisy estimator" of the wide target model [[57]](#b57).With few samples, this noise can distort the random HP search, but with more samples, this noise is suppressed.

## Transformer on WMT14 En-De

We scale up to WMT14 En-De using the large (post-layernorm) Transformer from [[50]](#b50) with 211M parameters. We tune on a proxy model with 15M parameters by shrinking d model , d f f n , and n head .

For this experiment, we tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid in Appendix F.2. The result is shown in Table [5](#tab_7): While random search with 3 HP samples far underperforms the fairseq default, we are able to match it via transfer using the same tuning budget.

## BERT

Finally, we consider large-scale language model pretraining where HP tuning is known to be challenging. Using Megatron (pre-layernorm) BERT [[43]](#b43) as a baseline, we hope to recover the performance of the published HPs by only tuning a proxy model that has roughly 13M parameters, which we call BERT-prototype. While previous experiments scaled only width, here we will also scale depth, as discussed in Section 6 and validated in Fig. [4](#fig_4). We use a batch size of 256 for all runs and follow the During HP tuning, we sample 256 combinations from the search space and train each combination on BERT-prototype for 10 5 steps. The total tuning cost measured in FLOPs is roughly the same as training 1 BERT-large for the full 10 6 steps; the exact calculation is shown in Appendix F.3. The results are shown in Table [6](#tab_8). Notice that on BERT-large, we obtain sizeable improvement over the well-tuned Megatron BERT-large baseline. 

## GPT-3

In order to further verify µTransfer at scale, we applied it to GPT-3 6.7B [[7]](#b6) with relative attention. This target model consists of 32 residual blocks with width 4096. We form the small proxy model by shrinking width to 256, resulting in roughly 40 million trainable parameters, 168 times smaller than the target model. HPs were then determined by a random search on the proxy model. The total tuning cost was only 7% of total pretraining cost. Details of the HP sweep can be found in Appendix F. [4](#b3).

In order to exclude code difference as a possible confounder, we also re-trained GPT-3 6.7B from scratch using the original HPs from [[7]](#b6). Unfortunately, after we have finished all experiments, we found this baseline mistakenly used absolute attention (like models in [[7]](#b6)) when it was supposed to use relative attention like the target model. In addition, during training of the µTransfer model we encountered numerical issues that lead to frequent divergences. In order to avoid them, the model was trained using FP32 precision, even though the original 6.7B model and our re-run were trained using FP16. 16 17 The resulting µTransfer model outperforms the 6.7B from [[7]](#b6), and is in fact comparable to the twice-as-large 13B model across our evaluation suite (see Table [11](#tab_15)). Selected evaluation results can be found in Table [7](#) and further details are given in Table [10](#tab_1) and Appendix F.4. 16 While we are mainly focused on the efficacy of µTransfer regardless of precision, it would be interesting to ablate the effect of precision in our results, but we did not have enough resources to rerun the baseline in FP32 17 It is quite interesting that µTransfer identified a useful region of hyperparameters leading to much improved performance, which probably would be difficult to discover normally because 1) researchers usually change hyperparameters to accomodate precision and 2) there was no precise enough justification to go against this judgment until µTransfer.

Table [7](#): GPT-3 6.7B Pretraining. Selected evaluation results for the GPT-3 6.7B model tuned with µTransfer (transfered from a small proxy model of 40M parameters), compared to the results published in [[7]](#b6) and a re-run with original HPs, as well as the 13B model in [[7]](#b6) for reference. Note that the perplexities in this table are based on a custom tokenization and are not comparable to the literature. The validation loss refers to the loss achieved on a random held-out part of our dataset. Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in the context when performing the sampling-based evaluations. See Appendix F.4 for full evaluation. Task Metric 6.7B+µP 6.7B re-run 6.7B [[7]](#b6) 13B [[7]](#b6) Validation loss cross-entropy

1.98 2.03 --PTB perplexity 11.4 13.0 --WikiText-103 perplexity 8.56 9.13 --One Billion Words perplexity 20.5 21.7 --LAMBADA Zero-Shot accuracy 73.5 70.8 70.3 72.5 LAMBADA One-Shot accuracy 69.9 64.8 65.4 69.0 LAMBADA Few-Shot accuracy 74.7 77.1 79.1 81.3 HellaSwag Zero-Shot accuracy 72.0 66.7 67.4 70.9 HellaSwag One-Shot accuracy 71.1 65.9 66.5 70.0 HellaSwag Few-Shot accuracy 72.4 66.4 67.3 71.3 0 2000 4000 6000 8000 10000 Training Step 1 2 3 4 5 6 7 8 9 Training Loss P LR=0.001 Width 128 256 512 1024 2048 4096 0 2000 4000 6000 8000 10000 Training Step SP LR=0.001 0 2000 4000 6000 8000 10000 Training Step SP LR=0.00025 Here we trained a GPT-3 transformer with 4 layers and widths from 256 to 32,768. Modulo a brief period around 1e8 training tokens, wider is better throughout training.

In earlier plots like Figs. [1](#fig_15) and [3](#fig_3), we saw that at the end of training, wider is always better in µP but not in SP. In fact, we find this to be true throughout training, as seen in Fig. [7](#), modulo noise from random initialization and/or data ordering, and assuming the output layer is zero-initialized (which has no impact on performance as discussed in Appendix D.2). We then stress-tested this on a µP GPT-3 Transformer (on the GPT-3 training data) by scaling width from 256 to 32,768 using a fixed set of HPs (Fig. [8](#)). Wider models consistently match or outperform narrower models at each point in training (except a brief period around 1e8 training tokens, likely due to noise because we ran only 1 seed due to computational cost). Our observation suggests that wider models are strictly more data-efficient if scaled appropriately. By checking "wider-is-better" early in training, one can also cheaply debug a µP implementation.

## Useful Hyperparameter Transfer: A Theoretical Puzzle

We want to tune HPs on a small model with width N such that its HP landscape looks like that of a large model with width N . Our intuition in Section 2 and Appendices C and J leads us to µP. However, for this to be useful, we do not want the small model (as a function) after training to be close to that of the large model -otherwise there is no point in training the large model to begin with. So N 1) must be large enough so that the HP optimum converges, but 2) cannot be so large that the functional dynamics (and the loss) converges. The fact that such N exists, as demonstrated by our experiments, shows that: In some sense, the HP optimum is a "macroscopic" or "coarse" variable which converges quickly with width, while the neural network function (and its loss) is a very "microscopic" or "fine" detail that converges much more slowly with width. However, theoretically, it is unclear why this should happen, and where else we should expect such useful HP transfer. We leave an explanation to future work.

10 Related Works

## Hyperparameter Tuning

Many have sought to speedup HP tuning beyond the simple grid or random search. Snoek et al. [[45]](#b45) treated HP tuning as an optimization process and used Bayesian optimization by treating the performance of each HP combination as a sample from a Gaussian process (GP). Snoek et al. [[46]](#b46) further improved the runtime by swapping the GP with a neural network. Another thread of work investigated how massively parallel infrasture can be used for efficient tuning under the multi-arm bandit problem [[18,](#b17)[22]](#b21). There are also dedicated tools such as Optuna [[4]](#b3) and Talos [[3]](#b2) which integrate with existing deep learning frameworks and provide an easy way to apply more advanced tuning techniques.

Our approach is distinct from all of the above in that it does not work on the HP optimization process itself. Instead, it decouples the size of the target model from the tuning cost, which was not feasible prior to this work. This means that no matter how large the target model is, we can always use a fixed-sized proxy model to probe its HP landscape. Nevertheless, our method is complementary, as the above approaches can naturally be applied to the tuning of the proxy model; it is only for scientific reasons that we use either grid search or random search throughout this work.

## Hyperparameter Transfer

Many previous works explored transfer learning of HP tuning (e.g. [[15,](#b14)[36,](#b36)[47,](#b47)[62]](#b62)). However, to the best of our knowledge, our work is the first to explore zero-shot HP transfer. In addition, we focus on transferring across model scale rather than between different tasks or datasets. Some algorithms like Hyperband [[23]](#b22) can leverage cheap estimates of HP evaluations (like using a small model to proxy a large model) but they are not zero-shot algorithms, so would still be very expensive to apply to large model training. Nevertheless, all of the above methods are complementary to ours as they can be applied to the tuning of our proxy model.

## Previously Proposed Scaling Rules of Hyperparameters

(Learning Rate, Batch Size) Scaling [[44]](#b44) proposed to scale learning rate with batch size while fixing the total epochs of training; [[14]](#b13) proposed to scale learning rate as √ batchsize while fixing the total number of steps of training. However, [[41]](#b41) showed that there's no consistent (learning rate, batch size) scaling law across a range of dataset and models. Later, [[30]](#b29) studied the trade-off of training steps vs computation as a result of changing batch size. They proposed an equation of a/(1 + b/batchsize), where a and b are task-and model-specific constants, for the optimal learning rate (see their fig 3 and [fig 5](#)). This law suggests that for sufficiently large batch size, the optimal learning rate is roughly constant. [18](#foot_13) This supports our results here as well as the empirical results in [[41, fig 8]](#).

Learning Rate Scaling with Width Assuming that the optimal learning rate should scale with batch size following [[44]](#b44), [[34]](#b33) empirically investigated how the optimal "noise ratio" LR/batchsize scales with width for MLP and CNNs in NTK parametrization (NTP) or standard parametrization (SP) trained with SGD. They in particular focus on test loss in the regime of small batch size and training to convergence. In this regime, they claimed that in networks without batch normalization, the optimal noise ratio is constant in SP but scales like 1/width for NTP. However, they found this law breaks down for networks with normalization.

In contrast, here we focus on training loss, without training to convergence and with a range of batch sizes from small to very large (as is typical in large scale pretraining). Additionally, our work applies universally to 1) networks with normalization, along with 2) Adam and other adaptive optimizers; furthermore 3) we empirically validate transfer across depth and sequence length, and 4) explicitly validate tuning via µTransfer on large models like BERT-large and GPT-3.

Finally, as argued in [[57]](#b57) and Appendix J.3, SP and NTP lead to bad infinite-width limits in contrast to µP and hence are suboptimal for wide neural networks. For example, sufficiently wide neural networks in SP and NTP would lose the ability to learn features, as concretely demonstrated on word2vec in [[57]](#b57).

Input Layer Parametrization The original formulation of µP in [[57]](#b57) (see Table [9](#tab_13), which is equivalent to Table [3](#tab_12)) uses a fan-out initialization for the input layer. This is atypical in vision models, but in language models where the input and output layers are shared (corresponding to word embeddings), it can actually be more natural to use a fan-out initialization (corresponding to fan-in initialization of the output layer). In fact, we found that fairseq [[33]](#b32) by default actually implements both the fan-out initialization and the √ fan_out multiplier.

Other Scaling Rules Many previous works proposed different initialization or parametrizations with favorable properties, such as better stability for training deep neural networks [[5,](#b4)[13,](#b12)[16,](#b15)[26,](#b25)[40,](#b40)[59,](#b59)[60,](#b60)[66]](#b66). Our work differs from these in that we focus on the transferability of optimal HPs from small models to large models in the same parametrization.

10.4 Infinite-Width Neural Networks: From Theory to Practice and Back [[57]](#b57) introduced µP as the unique parametrization that enables all layers of a neural network to learn features in the infinite-width limit, especially in contrast to the NTK parametrization [[17]](#b16) (which gives rise to the NTK limit) that does not learn features in the limit. Based on this theoretical insight, in Appendix J.3, we argue that µP should also be the unique parametrization (in the sense of [[57]](#b57)) that allows HP transfer across width; in short this is because it both 1) preserves feature learning, so that performance on feature learning tasks (such as language model pretraining) does not become trivial in the limit, and 2) ensures each parameter tensor is not stuck at initialization in the large width limit, so that its learning rate does not become meaningless. At the same time, our results here suggest that µP is indeed the correct parametrization for wide neural networks and thus provide empirical motivation for the theoretical study of the infinite-width µP limit. Note, parametrization here refers to a rule to scale hyperparameters with width ("how should my initialization and learning rate change when my width doubles?"), which is coarser than a prescription for setting hyperparameters at any particular width ("how should I set my initialization and learning rate at width 1024?").

## Conclusion

Leveraging the discovery of a feature learning neural network infinite-width limit, we hypothesized and verified that the HP landscape across NNs of different width is reasonably stable if parametrized according to Maximal Update Parametrization (µP). We further empirically showed that it's possible to transfer across depth, batch size, sequence length, and training time, with a few caveats. This allowed us to indirectly tune a very large network by tuning its smaller counterparts and transferring the HPs to the full model. Our results raise an interesting new theoretical question of how useful HP transfer is possible in neural networks in the first place.

Venues of Improvement Nevertheless, our method has plenty of room to improve. For example, initialization does not transfer well across depth, and depth transfer generally still does not work for post-layernorm Transformers. This begs the question whether a more principled parametrization in depth could solve these problems. Additionally, Fig. [4](#fig_4) shows that the optimal HP still shifts slightly for smaller models. Perhaps by considering finite-width corrections to µP one can fix this shift. Finally, it will be interesting to study if there's a way to transfer regularization HPs as a function of both the model size and data size, especially in the context of finetuning of pretrained models.

Contents 1 Introduction 2 Parametrization Matters: A Primer 3 Hyperparameters Don't Transfer Conventionally 4 Unlocking Zero-Shot Hyperparameter Transfer with µP 5 The Defects of SP and How µP Fixes Them 6 Which Hyperparameters Can Be µTransferred? 6.1 Empirical Validation and Limitations . . . . . . . . . . . . . . . . . . . . . . . . . 7 Efficiency and Performance of µTransfer 7.1 Transformer on IWSLT14 De-En . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Transformer on WMT14 En-De . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Wider is Better in µP Throughout Training 9 Useful Hyperparameter Transfer: A Theoretical Puzzle 10 Related Works 10.1 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Hyperparameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Previously Proposed Scaling Rules of Hyperparameters . . . . . . . . . . . . . . . 10.4 Infinite-Width Neural Networks: From Theory to Practice and Back . . . . . . . . 11 Conclusion A Parametrization Terminologies B Further Explanations of the µP Tables B.1 Walkthrough of µP Implementation in a Transformer . . . . . . . . . . . . . . . . B.2 Other Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Optimizer Variants and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . C Parametrization Matters: A Primer for Multiple Hyperparameters D Practical Considerations D.1 Verifying µP Implementation via Coordinate Checking . . . . . . . . . . . . . . . D.2 Zero Initialization for Output Layers and Query Layers in Attention . . . . . . . . D.3 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Illustration of µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 SP vs µP for MLPs on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Empirical validation of the stability of four representative hyperparameters on pre-LN Transformers in µP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Activations blow up in SP but maintain a consistent scale in µP . . . . . . . . . . . 6 Efficiency-performance Pareto frontier of µTransfer . . . . . . . . . . . . . . . . 7 Wider is always better in training loss under µP, but not in SP, given the same HP . 8 Stress-testing "wider-is-better" in µP . . . . . . . . . . . . . . . . . . . . . . . . . 9 Squashing activation functions reduce transfer quality. . . . . . . . . . . . . . . . 10 Enlarging d k makes µTransfer more precise in Transformers . . . . . . . . . . . . 11 Schematics of each Transformer layer . . . . . . . . . . . . . . . . . . . . . . . . 12 Width ratio can be varied arbitrarily in µTransfer . . . . . . . . . . . . . . . . . . 13 µTransfer can handle increasing n head while fixing d head as well as increasing d head while fixing n head , or a mix of both . . . . . . . . . . . . . . . . . . . . . . . . . 14 Results of the random search over reduced-width GPT-3 proxy models . . . . . . . 15 The training curves of the GPT-3 6.7B model with µTransfer and a re-run with the original settings from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Verifying µP hyperparameter stability on ResNet . . . . . . . . . . . . . . . . . . 17 Verifying hyperparameter stability under µP for Post-LN Transformers . . . . . . . 18 µTransfer vs naive transfer for post-layernorm Transformers on Wikitext-2 . . . . . 19 Empirical validation of µTransfer across Batch Size, Sequence Length, and Training Time on pre-LN Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Learning rate landscape is highly unstable under standard parametrization in IWSLT 21 Replicating training instability issue on a small Transformer by reverse-µtransferring hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . List of Tables 1 Hyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred Across 2 Examples of µTransferable Hyperparameters . . . . . . . . . . . . . . . . . . . . 3 µP[57] and SP for General Neural Networks . . . . . . . . . . . . . . . . . . . . . 4 µTransfer results for Transformer on IWSLT14 De-En . . . . . . . . . . . . . . . 5 µTransfer results for Transformer on WMT14 En-De . . . . . . . . . . . . . . . . 6 µTransfer results for BERT pretraining . . . . . . . . . . . . . . . . . . . . . . . . 7 µTransfer results for GPT-3 pretraining . . . . . . . . . . . . . . . . . . . . . . . 8 Alternative (Equivalent) µP Formulation for Easier Implementation . . . . . . . . 9 µP Formulation in the Style of [57] . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Full evaluation results of our GPT-3 6.7B models . . . . . . . . . . . . . . . . . . 11 Our µTransferred GPT-3 6.7B model performs comparably to the twice-as-large GPT-3 13B model from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 µTransfer results for ResNet on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . 13 µTransfer results for Wide ResNet on ImageNet . . . . . . . . . . . . . . . . . . . 14 Expected output size of matrix multiplication between different types of random matrices and a random vector, as preparation for deriving µP . . . . . . . . . . . .

## A Parametrization Terminologies

This section seeks to make formal and clarify some of the notions regarding parametrization discussed informally in the main text. Definition A.1 (Multiplier and Parameter Multiplier). In a neural network, one may insert a "multiply by c" operation anywhere, where c is a non-learnable scalar hyperparameter. If c = 1, then this operation is a no-op. This c is called a multiplier.

Relatedly, for any parameter tensor W in a neural network, we may replace W with cW for some non-learnable scalar hyperparameter c. When c = 1, we recover the original formulation. This c is referred to as a parameter multiplier.

For example, in the attention logit calculation k, q / √ d head where q = W x, the 1/ √ d head factor is a multiplier. It may also be thought of as the parameter multiplier of W if we rewrite the attention logit as k, (W/ √ d head )x .

Note parameter multipliers cannot be absorbed into the initialization in general, since they affect backpropagation. Nevertheless, after training is done, parameter multipliers can always be absorbed into the weight. Definition A.2 (Parametrization). In this work, a parametrization is a rule for how to change hyperparameters when the widths of a neural network change, but note that it does not necessarily prescribes how to set the hyperparameters for any specific width. In particular, for any neural network, an abc-parametrization is a rule for how to scale a) the parameter multiplier, b) the initialization, and c) the learning rate individually for each parameter tensor as the widths of the network change, as well as any other multiplier in the network; all other hyperparameters are kept fixed with width.

For example, SP and µP are both abc-parametrizations. Again, we note that, in this sense, a parametrization does not prescribe, for example, that the initialization variance be 1/fan_in, but rather that it be halved when fan_in doubles. Definition A.

3 (Zero-Shot Hyperparameter Transfer). In this work, we say a parametrization admits zero-shot transfer of a set of hyperparameters H w.r.t. a metric L if the optimal combination of values of H w.r.t. L converges as width goes to infinity, i.e. it stays approximately optimal w.r.t. L under this parametrization as width increases.

Throughout this paper, we take L to be the training loss, but because regularization is not the bottleneck in our experiments (especially large scale pretraining with BERT and GPT-3), we nevertheless see high quality test performance in all of our results. We also remark that empirically, using training loss as the metric can be more robust to random seed compared to validation loss and especially BLEU score. See Table [1](#tab_1)(left) for H. By our arguments in Appendix J.3 and our empirical results, µP is the unique abc-parametrization admitting zero-shot transfer for such H and L in this sense.

More generally, one may define a K-shot transfer algorithm of a set of hyperparameters H w.r.t. a metric L as one that 1) takes width values n and n and an approximately optimal combination of values of H w.r.t. L at a width n and 2) returns an approximately optimal combination of values of H w.r.t. L at width n , given 3) a budget of K evaluations of candidate hyperparameter combinations on models of width n . However, we will have no use for this definition in this paper.

## B Further Explanations of the µP Tables

In addition to Table [3](#tab_12), we provide Table [8](#) as an equivalent µP formulation that is easier to implement, as well as Table [9](#tab_13) for those more familiar with the original µP formulation in [[57]](#b57). Below, we provide some commentary on corner cases not well specified by the tables. Ultimately, by understanding Appendix J, one can derive µP for any architecture, new or old.

Matrix-Like, Vector-Like, Scalar-Like Parameters We can classify any dimension in a neural network as "infinite" if it scales with width, or "finite" otherwise. For example, in a Transformer, d model , d f f n , d head , n head are all infinite, but vocab size and context size are finite. Then we can categorize parameter tensors by how many infinite dimensions they have. If there are two such dimensions, then we say the parameter is matrix-like; if there is only one, then we say it is vector-like; if there is none, we say it is scalar-like. Then in Tables 3, 8 and 9, "input weights & all biases" and "output weights" are all vector-like parameters, while hidden weights are matrix-like parameters. An Table [8](#): Alternative (Equivalent) µP Formulation for Easier Implementation. Same format as in Table [3](#tab_12). In contrast to the formulation in Table [3](#tab_12), here all "vector-like" parameters (i.e. those that have only one dimension tending to infinity), including input and output weights and biases, have the same width scaling for initialization variance and SGD/Adam LR (note the 1/fan_in for input weight/bias init. var. is Θ(1) in width). This has two benefits in practice: 1) implementation is unified and simplified for all "vector-like" parameters; 2) input and output weights can now be tied, in contrast to 

$/ √ fan_in (1) 1 SGD LR 1 1 1 Adam LR 1 / √ fan_out(1) 1 /$√ fan_in

(1)

$1 /fan_in (1)$advantage of Table [8](#) is that it gives a uniform scaling rule of initialization and learning rate for all vector-like parameters. The multiplier rule in Table [8](#) can be more interpreted more generally as the following: a multiplier of order 1/fan_in should accompany any weight that maps an infinite dimension to a finite one. This interpretation then nicely covers both the output logits and the attention logits (i.e. 1/d attention).

Scalar-like parameters are not as common as matrix-like and vector-like ones, but we will mention a few examples in Appendix B.2. The scaling rule for their initialization, learning rate (for both SGD and Adam), and multiplier is very simple: hold them constant with width.

Initialization Mean We did not specify the initialization mean in the tables, since most commonly the mean is just set to 0, but it can be nonzero for vector-like parameters (e.g., layernorm weights) and scalar-like parameters but must be 0 for matrix-like parameters.

## Zero Initialization Variance

The initialization scaling rules in our tables can all be trivially satisfied if the initialization variance is set to 0. This can be useful in some settings (e.g., Appendix D.2) but detrimental in other settings (e.g., hidden weights).

What Are Considered Input Weights? Output Weights? Here, input weights very specifically refer to weights that map from an infinite dimension to a finite dimension. As a counterexample, in some architectures, the first layer can actually map from a finite dimension to another finite dimension, e.g., a PCA layer. Then this is not an "input weight"; if the next layer maps into an infinite dimension, then that's the input weight. A similar, symmetric discussion applies to output weights.

What Counts As a "Model"? Does the MLP in a Transformer Count As a "Model"? For our tables, a model is specifically a function that maps a finite dimension to another finite dimension, consistent with the discussion above. For example, for an image model on CIFAR10, it maps from 3 × 32 × 32 = 3072 dimensions to 10 dimensions, and these numbers are fixed regardless of the width of the model. Likewise, for an autoregressive Transformer model, the input and output dimension are both the vocab size, which is independent of the width. In contrast, an MLP inside a Transformer is not a "model" in this sense because its input and output dimension are both equal to the width of the Transformer.

## B.1 Walkthrough of µP Implementation in a Transformer

To ground the abstract description in Tables 3, 8 and 9, we walk through the parameters of a typical Transformer and discuss concretely how to parametrize each.

We assume that the user wants to replicate SP when the model widths are equal to some base widths, for example, when Below, we introduce hyperparameters σ • , η • for each parameter tensor, as well as a few multipliers α • . One may always tie σ • (resp. η • ) across all parameter tensors, but in our experiments, we found it beneficial to at least distinguish the input and output layer initialization and learning rates.

## Input Word Embeddings

The input word embedding matrix W wordemb has size d model × vocabsize, where vocabsize is the fan-in and d model is the fan-out. Follow the "input weight & all biases" column in Tables 3, 8 and 9. For example, for Tables [3](#tab_12) and [8](#),

$W wordemb ∼ N (0, σ2$wordemb ), with Adam LR η wordemb Note here, because fan-in (vocabsize) here is independent of width (d model ), the "1/fan_in" for the initialization variance in these tables is equivalent to "1", i.e. the initialization variance can be anything fixed with width. In this case of the word embedding, setting the variance to 1, for example, is more natural than setting the variance to 1/fan_in, because the embedding is one-hot (1/fan_in would be more natural for image inputs).

Positional Embeddings The (absolute or relative) positional embedding matrix W posemb has size d model × contextsize, where contextsize is the fan-in and d model is the fan-out. With the same discussion as above for input word embeddings, follow the "input weight & all biases" column in Tables 3, 8 and 9. For example, for Tables [3](#tab_12) and [8](#),

$W posemb ∼ N (0, σ 2 posemb ), with Adam LR η posemb$Layernorm Weights and Biases Layernorm weights w LN and biases b LN both have shape d model and can be thought of "input weights" to the scalar input of 1. Hence one should follow the "input weight & all biases" column in Tables 3, 8 and 9. In particular, the usual initialization of layernorm weights as all 1s and biases as all 0s suffice (where the initialization variance is 0). For example, for Tables [3](#tab_12) and [8](#), Self-Attention There are 4 matrices, W q , W k ∈ R (d k n head )×d model , W v ∈ R (dvn head )×d model , and W o ∈ R d model ×(dvn head ) (where the shapes are R fan_out×fan_in ). Since d model , (d k n head ), and (d v n head ) all scale with width (where the latter two are commonly just set to d model ), all 4 matrices should be parametrized according to the "hidden weights" column in Tables 3, 8 and 9. For example, for Tables [3](#tab_12) and [8](#),

$w LN ← 1,$$W q ∼ N (0, σ 2 q /d model ), with Adam LR η q / dmodel W k ∼ N (0, σ 2 k /d model ), with Adam LR η k / dmodel W v ∼ N (0, σ 2 v /d model ), with Adam LR η v / dmodel W o ∼ N (0, σ 2 o /(d v n head )),$with Adam LR η o /( dv ñhead ).

## Attention Logit Scaling

We use 1/d instead of 1/ √ d attention. To be compatible with 1/ √ d attention when at a particular base d head = d head,0 , we set

$AttnLogit = α attn d head,0 d head q k,$where α attn is a tunable multiplier.

MLP There are 2 matrices, W 1 ∈ R d f f n ×d model , W 2 ∈ R d model ×d f f n (where the shapes are R fan_out×fan_in ), where d f f n is commonly set to 4d model . Since both d model , d f f n scale with width, both matrices are considered "hidden weights." For example, for Tables [3](#tab_12) and [8](#),

$W 1 ∼ N (0, σ 2 q /d model ), with Adam LR η q / dmodel W 2 ∼ N (0, σ 2 k /d f f n ), with Adam LR η k / dffn$Word Unembeddings Symmetric to the discussion on input word embeddings, the output word unembeddings should be parametrized according to the "output weights" column of Tables [3, 8](#tab_12) and [9](#tab_13).

Often, the unembeddings are tied with the embeddings, and Tables [8](#) and [9](#tab_13) allow for this as their initialization schemes are symmetric between input and output weights.

For example, for Table [3](#tab_12), we'd set

$W unemb ∼ N (0, σ 2 unemb /(d model dmodel )), with Adam LR η unemb / dmodel .$For Table [8](#), we would instead have

$W unemb ∼ N (0, σ 2 unemb /d model,0$), with Adam LR η unemb , (note d model,0 here is the base width and therefore is a constant) and the output is computed as

$logits = α output dmodel W unemb z$where z is the final layer embedding of a token, and α output is a tunable multiplier.

## B.2 Other Parameters

Learnable scalar multipliers For learnable scalar multipliers (e.g., softmax inverse temperature), one can initialize them to 1 and use a constant (in width) learning rate for both SGD and Adam. This is compatible with Tables 3, 8 and 9.

Positional Bias Some Transformers use positional bias (of size contextsize × contextsize, which are added to the attention logits). They are considered "scalar-like" in that it has no width dimension. One can initialize them to 0 and use a constant (in width) learning rate for both SGD and Adam. This is compatible with Tables 3, 8 and 9.

Spatial MLPs Recent works [[12,](#b11)[24,](#b23)[31,](#b30)[48,](#b48)[49]](#b49) on MLP-only architectures in NLP and CV replace the self-attention layer in Transformers with MLPs across tokens or spatial locations. In our language here, such MLPs have finite input and output dimensions (the context size) and infinite hidden dimensions, so their input, output, and hidden weights should be parametrized via the corresponding columns in Tables 3, 8 and 9.

## B.3 Optimizer Variants and Hyperparameters

AdamW Exactly the same as Adam in all of our tables, with the added benefit that weight decay is automatically scaled correctly in AdamW (but is incompatible with µP Adam). For this reason, we recommend using AdamW when weight decay is desired (which is consistent with current standard practice).

Frobenius Normalization LARS [[63]](#b63), Adafactor [[42]](#b42), Lamb [[64]](#b64), Layca [[8]](#b7), Fromage [[6]](#b5), Nero [[28]](#b27) all involve a normalization step in which the update g (which may be obtained from SGD, Adam, or other optimzers) is normalized to have Frobenius norm equal to that of the parameter w: g ← w F g F g. They can be made compatible with µP in Table [8](#) by scaling their learning rate for hidden weights like 1/ √ fan_in (for Table [3](#tab_12), the output weight learning rate should be likewise scaled). The intuitive reasoning (which can be formalized straightforwardly using Tensor Programs) is as follows.

This normalization implicitly encodes a width scaling: If one initializes a weight matrix with variance 1/fan_in, then an n × n matrix (e.g., a hidden weight matrix) has Frobenius norm √ n at initialization. Thus, in the first step and, by induction, in any step t, the normalized update to this n × n weight also has Frobenius norm Θ( √ n) (for any fixed t, as n → ∞). Heuristically, this means each entry of g is approximately of size Θ(1/ √ n). But, by the derivation of Appendix J, we want Θ(1/n) and this is Θ( √ n) too large! Thus, in wide enough networks, one should see a network blowup after one update, like demonstrated in Fig. [5](#).

However, note that the Θ(1/ √ n) coordinate size induced by the normalization here is closer to the right size Θ(1/n) than Adam, whose update have coordinate size Θ(1). This may partially explain the apparent benefit of these optimizers. In particular, this may explain the observation that T5 [[38]](#b38), using Adafactor, was able to train its entire range of models from 220 million to 11 billion parameters with a fixed set of hyperparameters, while GPT-3 [[7]](#b6), using Adam, needed to decrease its learning rate with model size.

RAdam RAdam [[25]](#b24) is a variant of Adam that uses SGD with momentum in an initial stage with learning rate warmup, followed by a second stage of Adam with a particular setting of learning rate with time. Thus, one can adapt RAdam to µP by individually scaling the learning rates of the initial SGD stage and the final Adam stage according to Table [3](#tab_12), Table [8](#), or Table [9](#tab_13).

Adagrad and RMSProp Exactly the same as Adam in all of our tables.

in Adam and Its Variants All of our derivations here assume is negligible in Adam. If it is set to a non-negligible number, then it needs to be scaled, for all parameters, like 1/fan_in 2 if it is added before the square root, or like 1/fan_in if it is added after the square root.

Gradient Clipping Gradient ( 2 -norm-wise) clipping is compatible with Table [3](#tab_12) (as well as Tables [8](#) and [9](#tab_13)), for either SGD or Adam, if the clip value is held constant with respect to width.

Weight Decay Weight decay should be scaled independently of width in SGD and AdamW, for all of our tables. However, note it's not compatible with µP Adam.

Momentum Momentum should be scaled independently of width for all of our tables.

## C Parametrization Matters: A Primer for Multiple Hyperparameters

Here we give more intuition why we need to reparametrize all hyperparameters. In practice, neural networks have multitudes of hyperparameters all interacting together. In our example of Section 2, hyperparameter optimization would be akin to minimizing the function[foot_14](#foot_14)

$F n (c 1 , . . . , c k ) def = E x1,...,xn f ((c 1 + • • • + c k )(x 1 + • • • + x n )).$where x 1 , . . . , x n are as in Eq. ( [1](#formula_1)) and c 1 , . . . , c k are analogous to k hyperparameters. For the same reasoning in Section 2, the correct parametrization is in (α 1 , . . . , α k ) where

$α i = c i √ n.$While this is straightforward, in practice, researchers often fix some hyperparameters (e.g., they tune only learning rate but neglects to scale parameter multipliers or initialization correctly). For example, if we only partially reparametrize and optimize in α 1 while fixing c 2 , . . . , c k , then the optimal α 1 is

$(α 1 ) * = α * -(c 1 + . . . + c k ) √$n where α * is the optimal α for Eq. ( [1](#formula_1)). Thus, as n → ∞, (α 1 ) * still blows up even though we parametrized α 1 correctly. More generally, the incorrect parametrization of some hyperparameters forces other hyperparameters to increasingly compensate for it as width grows, distorting their optima, even if the latter are correctly parametrized.

## D Practical Considerations

In this section, we outline several useful tips and tricks that can improve the quality of hyperparameter transfer in practice.

## D.1 Verifying µP Implementation via Coordinate Checking

Even though µP is neatly encapsulated by Table [3](#tab_12), implementing it correctly can in practice be error-prone, just like how implementing autograd by hand can be error-prone even though the math behind is just chain-rule. In the case of autograd, gradient checking is a simple way of verifying implementation correctness; similarly, we propose coordinate checking to verify the correctness of µP implementation: Exemplified by Fig. [5](#), one calculates the average coordinate size of every (pre)activation vector in the network over a few steps of training, as width is varied over a large range. An incorrect implementation will see some activation vector blow up or shrink to zero with width (like in the top row of Fig. [5](#)). In the mup package we release with this paper, we include an easy-to-use method for coordinate checking.

## D.2 Zero Initialization for Output Layers and Query Layers in Attention

We find that the optimal hyperparameters of small and large width models match more closely when we initialize output layers at 0 (i.e. with variance σ 2 /fan_in where σ = 0 instead of positive σ). This is because the neural network in µP is approximately a Gaussian process (GP) at initialization with variance on the order Θ(σ 2 /width) (contrast this with SP networks, which approximates a GP with Θ(σ 2 ) variance) [[21,](#b20)[29,](#b28)[53,](#b53)[57]](#b57). Of course, when width is large, this variance vanishes, but this can be far from so in the small proxy model. This discrepancy in the initial GP can cause the training trajectory of the proxy model to be very different from the trajectory of the large target model, causing a mismatch in the optimal hyperparameters. By initializing the output layer at 0, we remove this mismatch in the initial GP. Empirically we do not find this modification to be detrimental to performance.

A similar consideration applies to the query layer in self-attention: At initialization, the attention logit q k/d head looks like a Gaussian with variance Θ(1/d head ) because q and k are almost independent and zero-mean. In the limit d head → ∞, the logit is exactly 0, which can be a large discrepancy compared to when d head is small in the small proxy model we want to tune. By initializing the query projection matrix W q to 0, q will also be 0, and hence the attention logit is always 0 at initialization regardless of width (but will generally become nonzero after a gradient step), resolving this discrepancy.

More generally, any layer or computation that goes from an "infinite" dimension (i.e. width) to a "finite" dimension (e.g. output dimension or sequence length) can exhibit this kind of discrepancy due to the initial GP. When d head → ∞ and n head is fixed, attention logit calculation can be viewed in the same vein as a function R seqlen×d model → R n head ×seqlen×seqlen , which "reduces to" R ∞ → R 1 .

## D.3 Activation Functions

14 12 10 8 6 log2LearningRate 0.50 0.75 1.00 1.25 1.50 1.75 Training Loss SP / tanh / xent 256 512 1024 2048 4096 8192 14 12 10 8 6 log2LearningRate 0.50 0.75 1.00 1.25 1.50 1.75 P / tanh / xent 15 10 5 0 log2LearningRate 0.02 0.04 0.06 0.08 0.10 SP / tanh / mse 15 10 5 0 log2LearningRate 0.02 0.04 0.06 0.08 0.10 P / tanh / mse Figure 9: Squashing activation functions reduce transfer quality. MLP of different hidden sizes with tanh activation trained for 20 epoch on CIFAR-10 using SGD. Left uses cross-entropy as loss function; right uses mean squared error; columns alternate between standard parametrization (SP) and maximal update parametrization (µP). Compared to ReLU, tanh exhibits slower convergence for µP, yet it still outperforms SP when width is increased

When the network is narrow, its approximation to the infinite-width behavior becomes crude, which is manifested as large fluctuations in preactivation coordinates. When using a squashing activation functions like softmax or tanh, this causes narrower networks to saturate the activation more than wider ones, which results in a systematic bias toward small gradients and therefore distorting the hyperparameter landscape. This can be seen in Fig. [9](#), where we use tanh as the network activation function. Therefore, we recommend replacing non-essential squashing activation functions with ReLU, whose derivative depends only on the sign of the pre-activation. A similar reasoning can be applied to superlinear activation functions, where the distribution of activation values can have heavy tails, leading to slow convergence to the infinite-width limit. However, such activations are rarely used in practice.

## D.4 Enlarge d k

We find that small d head = d k can lead to a highly noisy HP landscape, as shown in Fig. [10](#fig_8). This can significiantly decrease the quality of random HP search on the small proxy model. To solve this, we find it useful to decouple d 

## D.5 Non-Gaussian vs Gaussian Initialization

We find non-Gaussian (e.g. uniform) initialization can sometimes cause wider models to perform worse than narrower models, whereas we do not find this behavior for Gaussian initialization. This is consistent with theory, since in the large width limit, one should expect non-Gaussian initialization to behave like Gaussian initializations anyway (essentially due to Central Limit Theorem, or more precisely, universality), but the non-Gaussianity slows down the convergence to this limit.

## D.6 Using a Larger Sequence Length

For Transformers, we empirically find that we can better transfer initialization standard deviation from a narrower model (to a wide model) if we use a larger sequence length. It is not clear why this is the case. We leave an explanation to future work.

## D.7 Tuning Per-Layer Hyperparameters

The techniques in this paper allow the transfer across width of (learning rate, initialization, multipliers) simultaneously for all parameter tensors. Thus, to get the best results, one should ideally tune all such hyperparameters. In practice, we find that just tuning the global learning rate and initialization, along with input, output, and attention multipliers, yield good results.

## E Which Hyperparameters Can Be Transferred? (Continued) E.1 Further Discussions on Hyperparameter Categories

Below, we discuss the reasoning behind each kind, which are supported by our empirical evidence collected in Fig. [4](#fig_4) on Transformers as well as those in Appendix G.1 on ResNet.

Transferable Hyperparameters In Table [2](#tab_2), we summarize which HPs can be transferred across training scale. The transfer across width, as explained in Section 2, is theoretically justified, while we present the transfer across the other dimensions as empirical results.

These cover most of the well-known and important HPs when the need for regularization is not paramount, e.g., during large scale language model pretraining. Parameter Multipliers are not wellknown HPs, yet we include them here as they serve a bridge between SP and µP and can impact model performance in practice. Concretely, any SP and µP neural networks of the same width can have their Parameter Multipliers tuned so that their training dynamics become identical.

Hyperparameters That Don't Transfer Well Not all HPs transfer well even if we use µP. In particular, those whose primary function is to regularize training to mitigate "overfitting" tend not to transfer well. Intuitively, regularization needs to be applied more heavily in larger models and when data is scarce, but µP does not know the data size so cannot adjust the regularization accordingly.

To the best of our knowledge, there is no strict separation between HPs that regularize and those that don't. However, conventional wisdom tells us that there exists a spectrum of how much regularizing effect a HP has. For example, dropout probability and weight decay are among those whose primary function is to regularize, whereas batch size and learning rate might regularize training in some cases but affect the dynamics more so in other ways. Our empirical exploration tells us that the former do not transfer well, while the latter do. Our subsequent discussion will focus on the latter; we leave to future works the expansion to the former.

## Hyperparameters Transfered Across

We have left out a category of HPs that defines the training scale, or in practical terms, training cost. This includes 1) those that define how many operations a model's forward/backward pass takes, such as the model's width, depth, and in the case of language modeling, sequence length; and 2) those that define how many such passes are performed, such as batch size and number of training steps.

As recent works have shown [[7,](#b6)[19,](#b18)[39]](#b39), improvements along any of these scale dimensions lead to apparently sustainable gain in performance; as a result, we are primarily interested in transferring other HPs across these dimensions that define scale, rather than finding the optimal scale. [20](#foot_15) This category of HPs is particularly crucial as one can speedup training by downsizing in one or multiple such dimensions. Indeed, it's very common for practitioners to implicitly transfer HPs across the number of training samples by tuning on only a subset of the full training data.

Our insights from the infinite-width limit inspired us to explore HP tranfer across width, which does not work under SP as we have shown earlier. Building upon our success with width, which is well explained theoretically, we hope to push the limit of compute-saving by investigating the other dimensions empirically. To the best of our knowledge, the transferability of optimal HPs across depth, batch size, sequence length, and training time has not been rigorously investigated previously, with the main exception of the literature on (learning rate, batch size) scaling [[41,](#b41)[44]](#b44) where our transferability result of learning rate across batch size recapitulates [[30]](#b29). [21](#foot_16) See Section 10.3 on how our results relate to prior works. We will primarily focus on the Transformer architecture in the main text with evidence for ResNet in Appendix G.1.

## E.2 On the Definitions of Width

Our theory allows more general notions of width. This is especially relevant in Transformers, where d model , d head = d k , d v , n head , d f f n (see Fig. [11](#fig_15)) can all be construed as measures of width.  We briefly discuss these here, with more theoretical justification in Appendix J.2.1 and empirical validation below.

Varying Width Ratio So far we have assumed that every hidden layer is widened by the same factor. But in fact we can widen different hidden layers differently. This is useful, for example, in a Transformer where we may want to use a smaller d f f n during tuning. If we are using Adam, as long as the width of every layer still tends to infinity, we still obtain approximately the same limit [22](#foot_17) , so the µTransfer remains theoretically justified.

See Fig. [12](#fig_15) for an empirical validation on IWSLT-14 using a Transformer.

## Number of Attention Heads

In attention-based models, one typically splits hidden size into multiple attention heads following d model = d head × n head . So far we have assumed d head and d model to be width, but it's possible and potentially advantageous to fix d head and treat n head as the width, or increasing both simultaneously. This allows our technique to handle many popular models, including GPT-3 [[7]](#b6), which scale up by fixing d head and increasing n head . See Fig. [13](#fig_12) for an empirical validation on Wikitext-2.

Varying Just the Width of Attention Heads A specific useful instance of varying width ratio is decoupling the key and value dimensions d k and d v and scaling d k differently from (typically larger √ d as is done commonly). When tuning on the small proxy model, if d k is too small, the HP landscape can be quite noisy. Keeping d k relatively large while shrinking all other dimensions solves this problem, while still obtaining significant speedup.

## F Experimental Details

F.1 IWSLT IWSLT14 De-En is a well-known machine translation benchmark. We use a Transformer implemented in fairseq [[33]](#b32) with a default

$d model = 1 /4d f f n = 512 and d k = d q = d v = d model/n head = 128$(amounting to 40M parameters), which we denote as the 1x model. For transfer, we tune on a proxy model with the same n head but with d model and other dimensions 4 times smaller; we will call this the 0.25x model (but it has 4M parameters). All models are trained with Adam for 100 epochs and validated at the end of every epoch. We tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid • η: 5 × 10 -4 × 2 z , where z ∈ {-1.5, -1.25, -1, ..., 1.25}

• α output : 2 z , where z ∈ {-8, -7, -6, ..., 7}

• α attn : 2 z , where z ∈ {-3, -2, -1, ..., 8}

## F.2 WMT

We scale up to WMT14 En-De using the large Transformer from [[50]](#b50), with a d model = 1 /4d f f n = 1024 and d q = d k = d v = d model/n head = 64. We use the exact same setup and reproduce their result as our baseline. Then, we build the proxy model by shrinking the target model's d model from the original 1024 to 256, d f f n from 4096 to 256 and n head from 16 to 4. This reduces the total parameter count from 211M to 15M. We then perform the HP search on the proxy model and take the best according to validation loss, before testing on the target model. We tune via random search the learning rate η, the output layer parameter multiplier α output , and the attention key-projection weight multiplier α attn following the grid

$• η: 6 × 10 -4 × 2 z , where z ∈ {-1.5, -1.25, -1, ..., 1.25}$• α output : 2 z , where z ∈ {-8, -7, -6, ..., 7}

• α attn : 2 z , where z ∈ {-3, -2, -1, ..., 8} F.3 BERT Details of BERT Prototype Our proxy model has 10 Transformer layers with d model = d f f n = 256. We also reduce the number of attention heads to 8 with a d head of 32. We call it BERT Prototype since we can increase its width and depth according to our definitions to recover both BERT Base and BERT Large, which enables us to sweep HPs once and use for both models. Overall, BERT Prototype has 13M trainable parameters, a fraction of the 110M in BERT Base and the 350M in BERT Large.

Hyperparameters Tuned for Pretraining We tune the following HPs for pretraining: Adam learning rate η, embedding learning rate η emb , output weight multiplier α output , attention logits multiplier α attn , layernorm gain multiplier α LNgain , and bias multiplier α bias .

We sample 256 combinations from the follow grid:

• η: 1 × 10 -4 × 2 z , where z ∈ {1.5, 2, 2.5, 3, 3.5}

• η emb : 1 × 10 -4 × 2 z , where z ∈ {-1, -0.5, 0, 0.5, 1} • α output : 2 z , where z ∈ {2, 4, 6} • α attn : 2 z , where z ∈ {3, 3.5, 4, ..., 7} • α LNgain : 2 z , where z ∈ {8.5, 9, 9.5, 10, 10.5}

• α bias : 2 z , where z ∈ {8.5, 9, 9.5, 10, 10.5}

The ranges are chosen to include the implicit choices of these HPs in SP BERT Large.

Finetuning Procedure and Hyperparameters We hand-pick the finetuning HPs after training the full-sized model. As regularization is an essential ingredient in successful finetuning, we do not transfer such HPs (at least via the suite of techniques presented in this work) (see Table [1](#tab_1)). We focus on MNLI [[52]](#b52) and QQP, which are two representative tasks from GLUE [[51]](#b51). Following [[27]](#b26), we used Adam [[20]](#b19) with a learning rate of 5 × 10 -5 and a batch size of 64. The maximum number of epochs was set to 5. A linear learning rate decay schedule with warm-up of 0.1 was used. All the texts were tokenized using wordpieces and were chopped to spans no longer than 128 tokens.

## F.4 GPT-3

Baseline 6.7B GPT-3 Transformer As the GPT-3 codebase has evolved since the publication of [[7]](#b6), we re-trained the 6.7B model from scratch to remove changes in our codebase as a possible confounder. The main differences to [[7]](#b6) are 1) a modified learning rate decay schedule, where the learning rate is decayed to zero at the end of training rather than being decayed to 0.1 of the initial value, and 2) use of relative attention in place of absolute attention. Unfortunately, after all experiments were finished, we found this re-run baseline used absolute attention instead of relative attention, while the µTransfer model still used relative attention.

Random Search using Reduced-Width Proxy Model In order to find a good set of hyperparameters for the µTransfer version of the 6.7B model, we performed a hyperparameter search over a reduced version of the model (i.e., the proxy model), where the width is set to 256 hidden units. This proxy model inherits changes from the evolved GPT-3 codebase: it uses relative [[10]](#b9) (instead of absolute) position encoding. Early on, we noted that on the proxy model, linear learning rate decay outperformed the default cosine schedule, so all subsequent experiments for the proxy models use a linear decay schedule. By Fig. [4](#fig_4), µTransferring this linear decay schedule to the full model should maintain such a performance advantage over the cosine schedule.

The hyperparameter search space consists of the following hyperparameters:

• learning rate: Sampled from 10 Uniform(-4,-1)

• initialization scale: All the parameters are multiplied -sampled from 10 Uniform(-1,1) • attention temperature: Reciprocal of the multiplier applied to the input to attention softmax. Sampled from 4 Uniform(-1,1) . • output temperature: Reciprocal of the multiplier applied to the input to softmax that produces the distribution over output tokens. Sampled from 4 Uniform(-1,1) . • embedding multiplier: Scalar by which we multiply the output of the embedding layer.

Sampled from 10 Uniform(-1,1) . • relative position embedding multiplier: Scalar by which we multiply vectors representing relative position. Sampled from 10 Uniform(-1,1) .

In order to make the search more efficient we reduced the total number of training tokens. We hypothesized that tuning hyperparameters on a reduced total number of tokens does not significantly affect optimal hyperparameters. To verify, we trained two different horizons and compared the results. As suspected, we observed that the results are well-aligned for both 4 and 16 billion tokens versions. We observe learning rate and initialization scale impact the results the most. Based on the results we chose 0.006 for the former and 2.5 for the latter. Since most other hyperparameters appear to have negligible effect on performance, they were kept at their default values of 1, the only exception being the embedding scale, where higher values seem to perform better and it was therefore set to 10.

Training the µTransfer Model We encountered frequent divergences in our initial attempt to train the µTransfer model. We traced the issue back to underflow of FP16 tensors in the backwards pass and therefore switched to training the model in FP32. This allowed us to finish the training run without divergences. We hypothesize that the divergence issue is related to µTransfer picking more aggressive hyperparameters, for example a higher learning rate on linear weight tensors compared to the original model. In order to exclude code differences as a possible confounder, we re-trained GPT-3 6.7B from scratch using the original hyperparameters. The only difference compared to the version published in [[7]](#b6) is that the learning rate was decayed fully, whereas the learning rate of the model from [[7]](#b6) was only decayed to 10% of its starting value. The retrained model performs slightly worse than the original published in [[7]](#b6). We suspect that this is because it made less progress during the last phase of training where the learning rate is close to zero. The training curves of the µTransfer model and the re-run of the original 6.7B can be seen in Fig. [15](#fig_14). Detailed evaluation results can be found in Table [10](#tab_1)   The training curves of the GPT-3 6.7B model with µTransfer (orange) and a re-run with the original settings from [[7]](#b6) (blue). The µTransfer model uses relative attention while the re-run uses absolute attention. In addition, the former was trained using FP32 activations and weights after initially encountering stability issues with the hyperparameters computed using µP, while the re-run used the original FP16 training. The µTransfer model seems to underperform in the middle of training, but achieves a much better final validation loss once the learning rate is fully decayed. While the original model uses a cosine schedule, the µTransfer model uses a linear learning rate decay schedule transferred from the proxy model.

• T = 300 Billion is the number of training tokens for the 6.7B target model.

Here we are using the fact that the training FLOPs of a Transformer per token is roughly proportional to its number of parameters. Setup For this case we use Davidnet [[2]](#b1), a ResNet variant that trains quickly on CIFAR-10, so as to efficiently investigate its HP landscape. We train with SGD on CIFAR-10 for 10 epochs; all results are averaged over 15 random seeds. We use a width multiplier to identify models of different width, and a multiplier of 1 corresponds to the original model in [[2]](#b1). We look at validation accuracy here as the model barely overfits, and our observations will hold for the training accuracy as well. We first conduct a learning rate sweep for models of different widths using SP; the result is shown in Fig. [16](#fig_15), on the left.

## G Additional Experiments

Hyperparameter Stability Note that the best model with a width multiplier of 8 under-performs that with a multiplier of 4. We run the same sweep with µP, along with a sweep of the output multiplier (α output ); the result is shown in Fig. [16](#fig_15), on the right. We notice that wider models always perform better under µP and that the optimal learning rate η and α output are stable across width.

Table [10](#tab_1): Full evaluation results of our GPT-3 6.7B models: The new model tuned with µTransfer (marked µP), the original model from [[7]](#b6), and a re-training of this model from scratch with the original hyperparameter settings (marked re-run). The sampling-based evaluations shown here are a subset of the ones from [[7]](#b6). Since the sampling-based evaluations are subject to high variance, Wikitext 103 and the LM1B benchmark have been added to help distinguish the relative performance of the µP and non-µP model. Note that Wikitext-103 [[32]](#b31) and the LM1B [[9]](#b8) benchmarks overlap with the training dataset. Accuracies and F1 scores have been multiplied by 100. The perplexities reported in this table are based on a custom BPE encoding and are not comparable to other results in the literature. The number k of examples in the context for each task is identical to [7]. Note: Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in the context when performing the sampling-based evaluations, not the "shots" involved in hyperparameter transfer. Zero-shot One-shot Few-shot Task Split Metric µP [7] re-run µP [7] re-run µP [7] re-run Validation dataset valid ce 1.98 2.03 PTB test ppl 11.4 13.0 Wikitext 103 test ppl 8.56 9.13 LM1B test ppl 20.5 21.7 HellaSwag dev acc 72.0 67.4 66.7 71.1 66.5 65.9 72.4 67.3 66.4 LAMBADA test acc 73.5 70.3 70.8 69.9 65.4 64.8 74.7 79.1 77.1 StoryCloze test acc 79.4 77.7 77.3 80.6 78.7 78.3 84.2 81.2 81.1 NaturalQS test acc 9.86 5.79 7.20 14.7 9.78 10.6 20.2 17.0 15.7 TriviaQA dev acc 47.0 38.7 37.5 50.4 44.4 42.5 55.5 51.6 49.9 WebQS test acc 11.3 7.73 9.79 20.2 15.1 16.2 33.0 27.7 28.2 Ro→En 16 test BLEU-sb 26.9 8.75 13.7 36.5 34.2 33.5 38.2 36.2 35.6 En→Ro 16 test BLEU-sb 18.1 5.31 4.40 21.0 18.2 17.3 22.0 19.6 18.8 Fr→En 14 test BLEU-sb 29.8 15.5 19.6 31.7 31.6 30.1 38.0 36.4 36.5 En→Fr 14 test BLEU-sb 29.6 11.4 11.6 28.8 28.3 26.0 33.3 33.3 31.2 De→En 16 test BLEU-sb 31.7 18.2 21.7 33.3 31.9 31.1 38.9 36.5 36.2 En→De 16 test BLEU-sb 23.1 9.36 9.00 24.6 21.7 21.1 27.6 24.1 24.5 Winograd test acc 85.3 85.7 86.8 84.6 84.6 84.2 86.4 85.4 83.9 Winogrande dev acc 66.8 64.5 62.5 67.6 65.8 64.5 71.0 67.4 67.2 PIQA dev acc 79.1 78.0 78.0 77.3 76.3 76.9 79.2 77.8 77.7 ARC (Challenge) test acc 42.1 41.4 42.5 44.0 41.5 42.4 43.8 43.7 42.7 ARC (Easy) test acc 64.3 60.2 61.9 65.3 62.6 63.4 67.3 65.8 65.3 OpenBookQA test acc 54.4 50.4 52.6 56.4 53.0 52.8 58.4 55.2 54.4 Quac dev f1 41.8 36.1 38.2 43.1 39.0 39.5 44.0 39.9 39.9 RACE-h test acc 45.0 44.1 43.2 44.9 44.3 42.9 45.2 44.7 43.4 RACE-m test acc 58.4 54.4 54.0 57.9 54.7 53.8 58.6 55.4 55.4 SQuADv2 dev f1 59.9 52.7 50.9 64.9 57.1 54.7 68.9 62.1 58.4 CoQA dev f1 78.5 72.8 72.9 80.9 75.1 74.4 81.3 77.3 75.4 DROP dev f1 17.1 17.0 17.4 23.3 27.3 25.7 33.9 29.7 28.7 BoolQ dev acc 69.4 65.4 60.9 74.1 68.7 65.0 73.9 70.0 69.7 CB dev acc 21.4 28.6 37.5 60.7 33.9 32.1 62.5 60.7 66.1 Copa dev acc 82.0 80.0 77.0 81.0 82.0 81.0 88.0 83.0 82.0 RTE dev acc 55.2 55.2 46.2 61.0 54.9 58.8 52.7 49.5 59.9 WiC dev acc 0. 0. 0. 50.0 50.3 50.3 50.5 53.1 51.3 ANLI R1 test acc 33.7 32.3 33.4 32.4 31.6 31.7 30.9 33.1 30.7 ANLI R2 test acc 33.8 33.5 33.0 34.8 33.9 33.7 35.0 33.3 32.2 ANLI R3 test acc 32.7 34.8 33.4 34.8 33.1 33.3 36.9 33.9 32.3 Figure [16](#fig_15): ResNet on CIFAR-10 for different widths (compared to a base network). On the left, the widest network SP underperforms; on the right, the µP network has a more consistent HP landscape and performs better. Both networks are tuned at the smallest width for the HP (η or α output ) not in the x-axis.

Hyperparameter Transfer Next, we perform a grid search for learning rate (η) and α output on the 0.5x model for both SP and µP. [23](#foot_18) Then, we take the best combination and test on the 8x model, simulating how a practitioner might use µTransfer. The result is shown in Table [12](#tab_18), where µP outperforms SP by 0.43% ± .001%. Hyperparameter Transfer We start with a proxy model with a width multiplier of 0.125 and tune several HPs using the following grid:

• η: 1 × 2.048 × 2 z , where z ∈ {-5, -4, -3, ..., 4}

• α output : 10 × 2 z , where z ∈ {-5, -4, -3, ..., 4}

• weight decay co-efficient γ: 3.05 × 10 -5 × 2 z , where z ∈ {-2, -1.5, -1, ..., 1.5}

• SGD momentum β: 0.875 × 2 z , where z ∈ {-2, -1.5, -1, ..., 1.5}

The grid is centered around the default HPs used by [[1]](#b0) for ResNet-50; while not expected to be competitive for WRN, they represent a reasonable starting point for our experiment.

We randomly sample 64 HP combinations from the grid and train for 50 epochs, before selecting the one with the highest top-1 validation accuracy. Then, we scale up the model following both µP and SP and run with the same HPs we just selected. The result is shown in Table [13](#tab_19), where µP outperforms SP by 0.41% in terms of top-1 validation accuracy. 

## G.2.2 Post-Layernorm Transformers

Fig. [17](#fig_15) shows the transferability of learning rate, α output , initialization standard deviation, and Adam β 2 across width, batch size, sequence length, and training steps for post-layernorm Transformers. However, in general, we find transfer across depth to be fragile.

What Happens in the mup Package Under the hood, mup implements the µP formulation in Table [8](#). By invoking set_base_shape(model, base_model), each parameter tensor p of model gets a p.infshape attribute that stores, for each of its dimensions, the corresponding base dimension and whether that dimension should be considered "infinite" (i.e. will be scaled up/down, e.g., d model of a Transformer) or "finite" (i.e. will be fixed, e.g., vocabulary size). This information is used in the initializers and optimizers to automatically scale the parameters or learning rates to be compliant with µP. For example, by Table [8](#), the Adam learning rate of hidden weights p is calculated as η/p.infshape.width_mult(), where p.infshape.width_mult() essentially calculates fan_in base_fan_in .

## I Reverse-µTransfer for Diagnosing Training Instability in Large Models

Large Transformers are famously fickle to train [[25,](#b24)[37]](#b37). We note that a possible source of this instability for larger transformers is the failure of naive hyperparameter transfer via the standard parametrization. This is certainly consistent with Fig. [1](#fig_0), which shows that the optimal learning rate for small Transformers can lead to trivial performance in large Transformers. We support this hypothesis further by reverse-µTransferring the instability-inducing HPs from a large Transformer to a small one and replicating the training instability. This is shown in Fig. [21](#fig_15).

Practically, this reverse-µTransfer technique can be used to diagnose or debug training instability problems of large models. We offer two case studies toward this claim.

1) When training transformers of width 8192 on Wikitext-2, we found certain HP combinations caused divergence in the middle of training. We reverse-µTransferred one such HP combination to a model of width 256 and replicated this divergence. By analyzing this small model's activations right before this divergence, we found that the cause is due to attention logits blowing up. Note this debugging session proceeded much more quickly than if we directly worked with the large model. Later we confirmed this is indeed the same cause of the width-8192 model's divergence.

2) A 6B-parameter language model (in standard parametrization) in a separate project experienced repeated blow-up in the middle of training. We reverse-µTransferred its hyperparameters to a smaller, 100M-parameter model and replicated the training instability. This was solved by a retuning of the small model via random search.

20 18 16 14 12 10 8 log2LearningRate 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Training Loss training instability Fix Hparam., Change Width Actual Width 256 512 1024 2048 4096 8192 20 18 16 14 12 10 8 log2LearningRate 4.5 5.0 5.5 6.0 6.5 7.0 7.5 Training Loss training instability Fix Width, Change Hparam. Simulated Width 256 512 1024 2048 4096 8192 Figure 21: Replicating training instability on a small Transformer by reverse-µTransferring hyperparameters. These experiments concern 2-layer Transformers in Standard Parametrization (SP) on Wikitext-2, trained with Adam, where width is defined as d model = d f f n . (Left) LR-vsloss for wider and wider Transformers. (Right) Likewise for simulated width: Here each point (log 2 η, loss) for simulated width n indicates the loss from training a width-256 µP Transformer with base width n and LR η (i.e. loosely speaking, it's using LR transferred from η in a width-n SP [Transformer)](#). Takeaway: The overall shapes of the curves are identical between the left and right plots [24](#foot_19) ; in particular, a learning rate leads to instability in a wide model iff it does so when transferred back to a narrow model.

## J An Intuitive Introduction to the Theory of Maximal Update Parametrization

In what follows, we seek to describe useful intuitions and rules of thumb that would be helpful to practitioners and empirical researchers alike in figuring out what is the right neural network parametrization. The intuitions we shall describe regarding SGD can be made rigorous as in [[56,](#b56)[57]](#b57); those regarding Adam are new, and their formalization will be done in an upcoming paper.

First, we write down the most basic intuition regarding sums of many random elements, which will underlie all of the calculations that follow.

Law of Large Numbers (LLN) If x 1 , . . . , x n , . . . "look like" random independent samples of a random variable X, then 1 n n i=1

x i → E[X], as n → ∞.

Central Limit Theorem (CLT) In the same scenario as above,

$1 √ n n i=1 (x i -E[X]) → N (0, σ(X)), as n → ∞,$where σ(X) is the standard deviation of the random variable X.

Of course, there are many subtleties one must resolve to make the statements above truly rigorous (e.g., what is the meaning of "look like"?), but as rules of thumb, they typically give the correct prediction. In particular, here we want to note the following basic intuition regarding the size of a sum of x i :

when n is large, n i=1

x i has typical size

$Θ(n) if E[X] = 0 Θ( √ n) otherwise$Here, "typical size" can be taken to mean the size 99% of time. Again, we stress that this is a good rule of thumb that yields the correct prediction in the cases we are concerned with here; the rigorous versions of this will come from the Tensor Programs framework (e.g., [[56]](#b56)).

## J.1 Behaviors of Gaussian Matrices vs Tensor Product Matrices

Central to the derivation of µP for any architecture are key insights on the behaviors of two kinds of random matrices: 1) iid Gaussian random matrix and 2) tensor product matrix (by which we mean a sum of outer products) and more generally what we call nonlinear tensor product matrix (see Eq. ( [7](#))). For example, a neural network, randomly initialized in the typical way, will have each weight matrix look like the former. However, every step of training by gradient descent adds a sum of outer products to this initial matrix, so that the change in weights constitute a tensor product matrix. For Adam, the change in weights is not a tensor product but a more general nonlinear tensor product matrix (see Eq. ( [7](#))). In this section, we will particularly focus on the right scaling for the entries of such matrices, leading to a discussion of the right neural network parametrization in the next section. We concentrate on the key heuristics but eschew burdensome rigor.

Key Insights Consider a random vector v ∈ R n with approximately iid entries and a random matrix A of either size n × n or 1 × n, both having entries of size Θ(1). [25](#foot_20) In the context of deep learning, v for example can be an activation vector in an MLP, a Gaussian A the hidden weights at initialization, a (nonlinear) tensor product A the change in hidden weights due to training, and a vector A the readout layer weights. Then Av corresponds to a part of the next layer preactivation or the network output. To make sure the preactivations and the output don't blow up, we thus need to understand the scale of Av, especially in the general case where A is correlated with v. [26](#foot_21) This is summarized in Table [14](#tab_21), with the derivations below. Intuitively, a (nonlinear) tensor product or vector A will interact with a correlated v via Law of Large Numbers, hence the n-scaling, while a Gaussian A interacts with v via Central Limit Theorem, hence the √ n-scaling.

In the derivations below, we answer a slightly different but equivalent question of "how to scale A such that Av has entry size Θ(1)?"

## J.1.1 Preparation for the Derivations

By the results of [[57]](#b57), each (pre-)activation vector and its gradient vector in a multi-layer perceptron, at any time during training, have approximately iid coordinates in the large width limit, [27](#foot_22) and something similar can be said for more advanced networks such as ResNet and Transformers[foot_23](#foot_23) . Definition J.1. We say any such vector v ∈ R n has Θ(n a )-sized coordinates, or just Θ(n a )coordinates for short, if v 2 /n = Θ(n 2a ) as n → ∞. Because, by the above discussion, the coordinates are roughly iid when n is large, this intuitively means that each entry of v has "typical size" Θ(n a ). We make similar definitions with Θ replaced by O and Ω.

2. Neural network output should be O(1).

3. All parameters should be updated as much as possible (in terms of scaling in width) without leading to divergence.

Let's briefly justify these desiderata. For the desideratum 1, if the coordinates are ω(1) or o(1), then for sufficiently wide networks their values will go out of floating point range. This problem is particularly acute for low-precision formats that are essential for training large models such as BERT or GPT. Moreover, a general nonlinearity is only well-behaved if its input is in a fixed range (although this is not a problem for homogeneous nonlinearities like relu). For example, for tanh nonlinearity, if the preactivation is vanishing o(1), then tanh is essentially linear; if the preactivation is exploding ω(1), then the tanh gradient vanishes.

For the desideratum 2, a similar justification applies to the numerical fidelity of the loss function and loss derivative. Note that, with desideratum 3, this means the network output should be Θ(1) after training (but it can go to zero at initialization).

Finally, desideratum 3 means that 1) we are doing "maximal feature learning" [[57]](#b57) and 2) every parameter contribute meaningfully in the infinite-width limit. This ensures that learning rate "plays the same role" in the finite-width case as in the infinite-width limit. For example, it prevents the scenario where a weight matrix gets stuck at initialization in the limit for any learning rate (so learning rate does not matter) but evolves nontrivially in any finite-width network (so learning rate does matter).

These desiderata will essentially uniquely single out µP. More formally, µP is the unique parametrization that admits feature learning in all parameters of the neural network [[57]](#b57), and this property theoretically guarantees HP transfer across width (for sufficiently large width). However, for the sake of reaching a broader audience, we will focus more on the intuitive derivations from the desiderata rather than on this formal aspect.

Below, we first assume for simplicity that the width of every layer is n, and we focus only on dense weights. Later, we will discuss convolutions and varying the widths between layers.

## J.2.1 µP Derivation From the Desiderata

Below, we will derive the µP formulation in Table [3](#tab_12). Tables [8](#) and [9](#tab_13) can be derived from Table [3](#tab_12) via the following equivalences, which can be easily derived via some simple calculations. Lemma J.1. Let f t (ξ) denote the neural network function after t steps of training (using any fixed sequence of batches), evaluated on input ξ. Consider a parameter tensor W with learning rate C, initialized as W ∼ N (0, B 2 ), and with a multiplier A. Then for any θ > 0, f t (ξ) stays fixed for all t and ξ if we set

• when the optimizer is SGD

$A ← Aθ, B ← B/θ, C ← C/θ 2$• when the optimizer is Adam,

$A ← Aθ, B ← B/θ, C ← C/θ;$For example, for output weights, Table [3](#tab_12) has A = 1, B = 1/fan_in, C = η/fan_in for SGD and Adam. Then taking θ = 1/fan_in, we get the entries in Table [8](#), with A = 1/fan_in, B = 1, C = η • fan_in for SGD and C = η for Adam. Taking θ = 1/ √ fan_in instead, we get the entries in Table [9](#tab_13), with A = 1/ √ fan_in, B = 1/fan_in, C = η for SGD and η/ √ fan_in for Adam. Similar calculations hold for the input weights scaling in those tables, after taking into consideration that fan_in is considered a constant in terms of width for the input layer.

We proceed with the derivation of Table [3](#tab_12) below. Recall the definitions of Θ(n a )-sized coordinates or Θ(n a )-coordinates from Definition J.1.

Output Weights Suppose W ∈ R 1×n is an output weight. By desideratum 1, the input x to W has Θ(1)-sized coordinates. Thus W should have Θ(1/n)-coordinates so that |W x| = O(1). We can initialize W with Θ(1/n)-coordinates and scale its (per-layer) LR so that ∆W has Θ(1/n)coordinates as well. This means initializing W αβ ∼ N (0, Θ(1/n 2 )) and use Θ(1/n) learning rate for both SGD and Adam. but this induces a non-maximal feature learning limit, which, as we argue below, cannot transfer hyperparameters in all situations. 2. For SGD, the gradient of R n×n weight has Θ(1/ √ n)-coordinates, so Θ(1) learning rate would make preactivation scale like Θ( √ n) and hence blow up. If we use Θ(1/width) learning rate, then blow-up does not occur. However, this infinite-width limit is in the kernel regime [[57]](#b57) and thus does not allow HP transfer for the same reason that NTP below does not.

Neural Tangent Parametrization (NTP) We have concrete examples, e.g. Word2Vec in [[57]](#b57), where the NTK limit has trivial performance -so HPs have no effect at all -vastly outperformed by finite-width networks -where HPs matter. More importantly, wider does not always do better in NTP, especially in tasks where feature learning is crucial [[57,](#b57)[61]](#b61). So in the context of modern deep learning e.g. large language model pretraining, NTP (or SP with Θ(1/width) LR) does not make sense for wide neural networks.

Other Parametrizations Recall the Dynamical Dichotomy Theorem proven in [[57]](#b57), which says that any nontrivial stable "natural parametrization" (formally, "abc-parametrization," [[57]](#b57)) either admits a feature learning limit or a kernel limit, but not both.

Our argument above against SP and NTP will also work against any parametrization inducing a kernel limit. Therefore, it remains to ask, can other feature learning parametrizations transfer HPs?

We argue no. As shown in [[57]](#b57), any other feature learning parametrization differs from µP essentially only in that some parameters are not updated maximally. By [[57,](#b57)[Sec 6.4]](#), in the infinite-width limit, such parameters can be thought of as being fixed at initialization. Therefore, in such infinite-width limits, the learning rate of such parameters becomes useless. As such, we cannot hope for the HP landscape of the limit to reflect the HP landscape of finite-width neural networks. µP is the unique feature learning parametrization that updates all parameters maximally, so that the learning rate of each parameter plays approximately the same role in finite-width neural networks as in the infinite-width limit. Consequently, the HP landscape of the µP limit should reflect the HP landscape of finite-width neural networks.

![Figure 1: Training loss against learning rate on Transformers of varying d model trained with Adam.Conventionally and in contrast with our technique, different widths do not share the same optimal hyperparameter; wider networks do not always perform better than narrower ones; in fact they underperform the same-width networks in our technique even after tuning learning rate (see dashed line). See Sections 3 and 4 for experimental setup.]()

![Figure 2: Illustration of µTransfer]()

![) over c ∈ R, for some bounded continuous functionf : R → R. If we reparametrize c = α/ √ n for α ∈ R, then by CLT, G n (α) def = F n (c) → E f (N (0, α2)) stabilizes into a function of α as n → ∞.]()

![Figure 3: MLP width different hidden sizes trained for 20 epoch on CIFAR-10 using SGD. Left uses standard parametrization (SP); right uses maximal update parametrization (µP). µP networks exhibit better learning rate stability than their SP counterparts.]()

![Figure 4: Empirical validation of the stability of four representative hyperparameters on pre-LN Transformers in µP: learning rate, last layer weight multiplier α output , weight initialization standard deviation, and learning rate schedule. We use the following learning rate schedules: (a) linear decay; (b) StepLR @ [5k, 8k] with a decay factor of 0.1; (c) StepLR @ [4k, 7k] with a decay factor of 0.3; (d) cosine annealing; (e) constant; (f) inverse square-root decay. All models are trained on wikitext-2 for 10k steps.When not specified in the legend, the width used is 256, depth 2, batch size 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to each column, while fixing all others constant. See Section 6.1 for discussion of these results.]()

![Figure 7: Wider is always better in training loss under µP, but not in SP, given the same HP. Learning curves for µP and SP with different learning rates, aggregated over 5 seeds. (Left) Wider µP models always achieve better training loss at any time in training. (Middle) If using a small learning rate, SP models can appear to do so up to some large width, at which point the pattern fails (at width 2048 in our plot). (Right) If using a large learning rate, SP model can strictly do worse with width; here the SP model is identical to the µP model in (Left) at width 128.]()

![d model = d model,0 = 128, d f f n = d f f n,0 = 512, etc, as in the MLP example in Section 4. For this purpose, it's useful to define dmodel = d model /d model,0 , dffn = d f f n /d f f n,0 , and so on. One can always take d model,0 = d f f n,0 = • • • = 1 for a "pure" µP.]()

![with Adam LR η LN w , and b LN ← 0, with Adam LR η LN b]()

![Figure 10: Enlarging d k makes µTransfer more precise. Here we plot all curves after subtracting their minima for easier visual comparison. Transformer on IWSLT 14 similar to the setup in Appendix F.1 where the d model = 512 for a width multiplier of 1, n head = 4, and d q = d k . (Left) We leave d q = d k = d model/n head , so d k = 8 for width-multiplier 0.0625. The optimum for the attention logit multiplier c attn is noisy and does not accurately transfer across width. (Right) We enlarge d q = d k to a minimum of 128. The HP landscape is much smoother than in (Left), and the optima align between narrow and wide models.]()

![k from d model (so that d model = d k • n head ) and maintain a relatively large d k even as d model is shrunk in the proxy model. For example, pegging d k = 32 is generally effective. Training or inference speed are not usually affected much by the larger d k because of CUDA optimizations. By Appendix E.2, this decoupling of d k from d model is theoretically justified, and as shown in Fig. 10, it significantly denoises the HP landscape.]()

![Figure 11: Schematics of each Transformer layer. Commonly, the key and value dimensions d k and d v are both set to d model /n head , and this is referred to as d head .]()

![Figure 13: µTransfer across width when we fix d head and vary d model and n head . α output , α attn are multipliers for output and key weights, and σ is initialization standard deviation.]()

![Figure 14: Results of the random search over reduced-width GPT-3 proxy models trained on 4 (left) and 16 (right) billion tokens. Only the best performing runs are highlighted.]()

![Figure15:The training curves of the GPT-3 6.7B model with µTransfer (orange) and a re-run with the original settings from[7] (blue). The µTransfer model uses relative attention while the re-run uses absolute attention. In addition, the former was trained using FP32 activations and weights after initially encountering stability issues with the hyperparameters computed using µP, while the re-run used the original FP16 training. The µTransfer model seems to underperform in the middle of training, but achieves a much better final validation loss once the learning rate is fully decayed. While the original model uses a cosine schedule, the µTransfer model uses a linear learning rate decay schedule transferred from the proxy model.]()

![Experiments on ResNetsG.1.1 ResNet on CIFAR-10]()

![]()

![Algorithm 1 Tuning a Large Target Model via µTransfer 1: Parametrize target model in Maximal Update Parametrization (µP) 2: Tune a smaller version (in width and/or depth) of target model 3: Copy tuned hyperparameters to target model Hyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred Across, with a few caveats discussed in Section 6.1. * means empirically validated only on Transformers, while all others additionally have theoretical justification.]()

![Examples of µTransferable Hyperparameters. All of the below can also be specialized to per-layer hyperparameters.]()

![Transformers on WMT14 En-De. 1x and 0.25x refers to scaling of width only. We report BLEU fluctuation over 3 independent trials, i.e., 3 independent random HP searches.For more details on BERT-prototype, what HPs we tune, and how we finetune the trained models, see Appendix F.3.]()

![BERT]()

![which is a common design feature of Transformer models. Note that in this table, for biases, the fan_in is 1 (compare to PyTorch nn.Linear default initialization of biases, where fan_in refers to fan_in of the layer.) This table can be derived from Table3via Lemma J.1.]()

![µP Formulation in the Style of[57]. This table can be derived from Table3via Lemma J.1.]()

![and Ratio of Tuning Cost to Pretraining Cost in FLOPs can be approximated ass(t 1 N 1 + t 2 N 2 )Billion is number of parameters of the target model • t 1 = 4 Billion is the number of training tokens for the short horizon HP search, and N 1 = 350 is the corresponding number of random HP search trials.• t 2 = 16 Billion is the number of training tokens for the longer horizon HP search, and N 1 = 117 is the corresponding number of random HP search trials.]()

![Evaluation results comparing the GPT-3 6.7B model tuned with µTransfer against the twice-as-large GPT-3 13B model from[7]. The two models have similar performance on most of the evaluation tasks.]()

![ResNet on CIFAR10: Transferring the best learning rate (η) and α output from widening factor 0.5 to 8; µP significantly outperforms SP given the same search grid. The best HPs are different as the models are parametrized to be identical at 1x width.23 For this case we use Wide-Resnet, or WRN[65], a ResNet variant with more channels per layer, to further showcase µTransfer across width, i.e., number of channels. We train with SGD on ImageNet for 50 epochs following standard data augmentation procedures. We use a width multiplier to identify models of different width, and a multiplier of 1 corresponds to the original WRN-50-2-bottleneck in[65].]()

![ResNet on ImageNet: Transferring the best learning rate (η), α output , γ, and β from widening factor 0.125 to 1; µP significantly outperforms SP given the same search grid.]()

![Expected entry size of Av for different matrices A and vector v correlated with each other, both having entries of size Θ(1).]()

but possibly not for different data and/or tasks.

The more theoretically astute reader may observe that SP with a Θ(1/width) learning rate induces a well-defined infinite-width limit exists as well. Nevertheless, this does not allow HP transfer because this limit is in kernel regime as shown in[[57]](#b57). See Appendix J.3 for more discussions.

i.e. the default parametrization offered by common deep learning frameworks. See Table3for a review.

The key here is that the init. variance ∝ 1/fan_in, so the same insights here apply with e.g. He initialization.

While superficially different, this parametrization is equivalent to the µP defined in[[57]](#b57).

This is roughly because during training, q and k will be correlated so q k actually scales like d due to Law of Large Numbers, in contrast to the original motivation that q, k are uncorrelated at initialization so Central Limit applies instead. See Appendix J.2.1 for a more in-depth discussion.

Note in this example, Glorot initialization[[13]](#b12) (i.e. with variance 1/(fan_in + fan_out)) would scale asymptotically the same as µP and thus is similarly well-behaved. However, if one adds layernorm or batchnorm, then Glorot will cause logit blowup like SP, but µP still will not.

"2 layers" means the model has 2 self-attention blocks. To compare with SP Transformer, see Fig.18.

in fact, post-layernorm Transformers are much more sensitive to HPs than pre-layernorm, so our technique is more crucial for them, especially for transfer across width. Fig.1uses post-layernorm.

https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md.

Ideally we would like to measure the wall clock time used for tuning. However, smaller models such as the proxy Transformer used for IWSLT are not efficient on GPUs, so wall clock time would not reflect the speedup for larger models like GPT-3. Thus, we measure in FLOPs, which is less dependent on hardware optimization.

We do not report the standard deviation over random initializations to avoid confusion.

We find this provides more reliable result than selecting for the best BLEU score.

while the optimal learning is roughly linear in batch size when the latter is small

Here, for simplicity of the example, we model the interaction between "hyperparameters" c 1 , . . . , c k as additive, but in real neural networks such interactions are usually much more complicated.

In particular, we are not fixing the total training FLOPs when we scale, which requires understanding the tradeoff of different scale HPs. For example, when we transfer across batch size, we fix the number of steps of training (not the number of epochs), so that the total FLOPs scales linearly.

There's also a literature on the proper initialization for training deep networks effectively (e.g.[[5,](#b4)[16,](#b15)[26,](#b25)[40,](#b40)[59,](#b59)[60,](#b60)[66]](#b66)), but they do not study the transferability per se. SeeSection 10.3   

This also applies for SGD, but we need more involved scaling to keep the limit approximately the same.

Here we tune the 0.5x model instead of the 1x model to simulate the situation that one does "exploratory work" on the 1x model but, when scaling up, would like to tune faster by using a smaller proxy model.

Note that the curves on the left are "lower" than curves on the right. This just reflects the increasing capacity of wider models able to fit the training data better, so is orthogonal to our point.

in the sense that the the variance of the entries are Θ[(1)](#b0)

Here "correlated" formally means v depends on W in a Tensor Program. This essentially captures all scenarios of "v correlated with W " that occurs in deep learning.

Our intuition here is derived from the assumption that width is much larger than training time; of course, as illustrated by our myriad experiments, these intuition are very useful even when this is not the case, such as when training to convergence.

E.g. in a convnet, the (pre-)activations are iid across channels, but correlated across pixels

In some corner cases when x is uncorrelated with v, then v x = Θ( √ n) by Central Limit, so actually Ax has Θ(1/ √ n)-coordinates. However, this case does not come up much in the context of training neural networks.

Adam also has bias correction for the moving averages which can be accomodated easily, but for simplicity we omit them here.

This is because every "reasonable" deep learning computation can be expressed in a Tensor Program.

In a convnet, a (pre-)activation vector corresponds to a single pixel across all channels; in general , we expect (pre-)activations are iid across channels, but correlated across pixels

