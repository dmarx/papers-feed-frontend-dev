<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty</title>
				<funder ref="#_r7kcARa">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_zVNaG4x #_DK8TSBE">
					<orgName type="full">United Kingdom Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder ref="#_5DGQQHN">
					<orgName type="full">United Kingdom Research and Innovation</orgName>
					<orgName type="abbreviated">UKRI</orgName>
				</funder>
				<funder ref="#_QKUUABS">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-28">28 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chatchuea</forename><surname>Kimchaiwong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Warwick Mathematics Institute</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Division of Mathematical Sciences</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><forename type="middle">M</forename><surname>Johansen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-28">28 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AB44C6BC92F7F29E45D15C67CF620397</idno>
					<idno type="arXiv">arXiv:2411.18864v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-03-15T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bayesian inference</term>
					<term>State-Space Model</term>
					<term>Possibility Theory</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of incorporating information from observations received serially in time is widespread in the field of uncertainty quantification. Within a probabilistic framework, such problems can be addressed using standard filtering techniques. However, in many real-world problems, some (or all) of the uncertainty is epistemic, arising from a lack of knowledge, and is difficult to model probabilistically. This paper introduces a possibilistic ensemble Kalman filter designed for this setting and characterizes some of its properties. Using possibility theory to describe epistemic uncertainty is appealing from a philosophical perspective, and it is easy to justify certain heuristics often employed in standard ensemble Kalman filters as principled approaches to capturing uncertainty within it. The possibilistic approach motivates a robust mechanism for characterizing uncertainty which shows good performance with small sample sizes, and can outperform standard ensemble Kalman filters at given sample size, even when dealing with genuinely aleatoric uncertainty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dynamical real-world systems are typically represented as a hidden process in some state space, indirectly observed via a measurement process. The usual approach to characterizing the unknown state and uncertainty about it is to combine the observed information with given prior information in an approach known as filtering, or sometimes data assimilation <ref type="bibr" target="#b28">[29]</ref>. However, this is non-trivial due not only to imprecise measurements, but also to model misspecification <ref type="bibr" target="#b30">[31]</ref>.</p><p>Perhaps the best-known data-assimilation algorithm is the Kalman filter (KF) <ref type="bibr" target="#b21">[22]</ref>, which is the optimal filter for the linear Gaussian state-space model, as detailed, e.g., in <ref type="bibr" target="#b30">[31]</ref>. However, its application is limited by its high computational costs when dealing with large state spaces, and by the fact that it cannot be applied directly to nonlinear models <ref type="bibr" target="#b27">[28]</ref>. Several modifications of the KF have been developed to extend its scope. The extended Kalman Filter can deal with a mild level of nonlinearity by approximating the model with a first order Taylor series expansion, as described, e.g., in <ref type="bibr" target="#b33">[34]</ref>. The unscented Kalman Filter (UKF) approximates the distribution with Gaussian distribution using a set of sigma points and their weights, often giving better accuracy for highly nonlinear models <ref type="bibr" target="#b36">[37]</ref>. Still, the accuracy of the UKF depends upon the tuning of certain algorithmic parameters which determine the locations of the sigma points. Moreover, the number of sigma points (and hence the computational cost and accuracy) is fixed. Another widely used algorithm is the ensemble Kalman filter (EnKF) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>, which uses an ensemble to approximate the distributions of interest. The strengths of the EnKF are that it does not require the calculation of the variance (avoiding costly matrix inversions in high-dimensional problems) and can be used with a nonlinear model without approximating it with its linear tangent model <ref type="bibr" target="#b27">[28]</ref> as one simply needs to be able to sample from the transition kernel of the dynamic model-although this comes at the cost of further approximating a non-Gaussian distribution with a Gaussian one. The EnKF must be used carefully with high-dimensional problems as constraints on computing power typically lead to an ensemble size relatively small compared to the dimension of the state space, leading to problems such as spurious correlation and underestimated variance, affecting the algorithm's performance. Thus, the recent development in this field mainly focuses on reducing computational cost and increasing the forecast's accuracy.</p><p>Although there has been much recent development in the field of data assimilation, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23]</ref>, the opportunities offered by alternative representations of the uncertainty have not been fully explored. Yet, there is a clear motivation for doing so: In many situations in which the EnKF is used, the main sources of uncertainty are i) the initial state of the system, ii) unknown deviations in the model, e.g., unknown forcing terms, and iii) the lack of data. These sources of uncertainty can be argued to be of an epistemic nature and, given their predominance, it is important to model them as faithfully as possible. This makes appealing the use of a dedicated framework for epistemic uncertainty. Many different approaches have been suggested in the literature to deal with epistemic uncertainty, such as fiducial inference <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> and Dempster-Shafer theory <ref type="bibr" target="#b8">[9]</ref>. While fiducial inference gives a probabilistic statement about a parameter without relying on a prior probability distribution, it still represents the information a posteriori via probability distributions. On the other hand, Dempster-Shafer theory leverages (fuzzy) set-valued probabilities to represent both aleatoric and epistemic uncertainty <ref type="bibr" target="#b26">[27]</ref>. Yet, its formulation, based on sums over all subsets of the state space, restricts its application to discrete cases and, even then, does not scale easily to larger problems. Another approach, termed possibility theory <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>, proposes to model exclusively epistemic uncertainty by changing the algebra underpinning probability theory, i.e., summation/integration is replaced by maximisation. Many probabilistic concepts and analyses can be adapted to possibility theory, for instance, <ref type="bibr" target="#b19">[20]</ref> proves a Bernsteinvon Mises theorem for a possibilistic version of Bayesian inference. In real systems, some uncertainty will be well modelled as aleatoric and some will of an epistemic nature; <ref type="bibr" target="#b16">[17]</ref> shows that probability theory and possibility theory can be combined in this case, offering an extension of the probabilistic approach rather than an alternative. Possibility theory can be applied to complex problems, for instance <ref type="bibr" target="#b17">[18]</ref> introduces an analogue to spatial point processes, and <ref type="bibr" target="#b18">[19]</ref> shows that the Kalman filter can also be derived in this framework. These results highlight the potential of possibility theory for data assimilation, which we explore in this work via a possibilistic analogue of the EnKF. Related work includes <ref type="bibr" target="#b3">[4]</ref> in which a robust version of the Kalman filter is proposed based on sets of probability distributions, with a motivation close to the one for this work.</p><p>Our contributions are as follows:</p><p>1. We introduce a new notion of Gaussian fitting for possibility theory, and contrast it against the standard moment-matching procedure. We highlight the potential of this approach for providing principle grounds for standard heuristics used in the EnKF.</p><p>2. We derive a possibilistic analogue of the EnKF, adapting the initialisation and predictions steps to the assumed epistemic uncertainty in the model, and showing how the update step of existing versions of the EnKF remain valid in this context.</p><p>3. We assess the performance of our method against two versions of the EnKF and against the UKF, considering both linear and nonlinear dynamics as well as fully and partially observed processes.</p><p>The remainder of this paper is organised as follows. Section 2 starts with the state estimation problem for the state-space model before giving the filtering techniques used to estimate such states under the probabilistic framework. Then, the details of the possibilistic framework developed to tackle epistemic uncertainty are given, with some concepts defined analogously to the probabilistic framework. Next, the possibilistic EnKF is introduced in Section 3 to estimate the system's state in the presence of epistemic uncertainty. After that, the performance of our algorithm is assessed on simulated data in a range of situations and compared against standard baselines in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We first briefly review the standard EnKF and the underlying filtering problem in Section 2.1 before introducing, in Section 2.2, the framework based on which the proposed method will be derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">State estimation problem and filtering technique</head><p>The standard, probabilistic Gaussian state-space model describes the evolution of a hidden state process, (X k ) k&gt;0 , and its associated observation process, (Y k ) k&gt;0 , via</p><formula xml:id="formula_0">X k = F k (X k-1 ) + ϵ k where ϵ k ∼ N(0, U k ) (1a) Y k = H k (X k ) + ε k where ε k ∼ N(0, V k ),<label>(1b)</label></formula><p>with U k and V k positive definite matrices and with N(µ, Σ) denoting a Gaussian distribution of mean µ and covariance Σ. We consider the setting in which a stream of observations y 1 , y 2 , . . . arrives sequentially in time, and we want to obtain estimates of the hidden states as they arrive. Thus, in the Bayesian context, we characterise our knowledge of the state X k at time step k &gt; 0 given the realisations y 1:k . = (y 1 , y 2 , . . . , y k ) of the observations Y 1:k via the filtering density p(x k |y 1:k ) which can be computed recursively, see, e.g., <ref type="bibr" target="#b30">[31]</ref>, as</p><formula xml:id="formula_1">p(x k |y 1:k-1 ) = p(x k |x k-1 )p(x k-1 |y 1:k-1 )dx k-1 (prediction) p(x k |y 1:k ) = p(y k |x k )p(x k |y 1:k-1 ) p(y k |x k )p(x k |y 1:k-1 )dx k .<label>(update)</label></formula><p>The Kalman Filter (KF) provides a closed-form recursion for the prediction and update steps for linear Gaussian state-space models (see, e.g., <ref type="bibr" target="#b30">[31]</ref>). However, many models in this world are nonlinear, meaning that the KF is not directly applicable and, unfortunately, there are few other settings in which analytic recursions are available. This motivates the development of numerous alternatives, including the UKF and the EnKF.</p><p>While the UKF can provide good performance for moderately nonlinear models, which depends upon careful choice of several algorithmic tuning parameters, and, as the number of sigma points is fixed, there is little scope for adjusting computational cost or accuracy. The EnKF algorithm takes a different approach; it can be viewed as an approximation to the KF which employs an ensemble of samples {X i k } N i=1 to approximate the parameters of the Gaussian distribution at time k. In particular, the ensemble's mean and variance are used to approximate those of the filtering distribution. The EnKF algorithm then proceeds recursively via the prediction and update steps. The prediction step mirrors that of the KF but uses an ensemble approximation, which means that F k need not be linear as long as it can be evaluated pointwise as in Algorithm 1 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Prediction step of EnKF at time step k</head><formula xml:id="formula_2">Input: The posterior ensemble { Xi k-1 } N i=1 at time step k -1. Output: The predictive ensemble {X i k } N i=1 at time step k with mean µ k and variance Σ k . 1: for i ∈ {1, . . . , N } do 2: ϵ i k ∼ N(0, U k ) \\Sample noise 3: X i k = F k ( Xi k-1 ) + ϵ i k \\Predict i-th particle 4: end for 5: µ k ← 1 N N i=1 X i k \\Predictive mean 6: Σ k ← 1 N -1 N i=1 (X i k -µ k )(X i k -µ k ) ⊤ , \\Predictive<label>variance</label></formula><p>The update step of the EnKF also approximates that of the KF: the mean of the updated distribution is a convex combination of that of the predictive distribution and the observation using the so-called Kalman gain as a weight. However, using the traditional Kalman gain can lead to an underestimation in the posterior variance (as it essentially neglects the observation noise) and can lead to filter divergence <ref type="bibr" target="#b37">[38]</ref>. A number of variants of the EnKF update step have been developed to address this problem, including both the stochastic EnKF (StEnKF) and the square root EnKF (SqrtEnKF) <ref type="bibr" target="#b23">[24]</ref>. The latter updates the deviation of the ensemble by applying a matrix obtained from the square root of a certain matrix. Different variations of the SqrtEnKF exist, depending on how such a matrix is obtained, as summarised in <ref type="bibr" target="#b34">[35]</ref>. In this work, we consider the version of the SqrtEnKF presented in <ref type="bibr" target="#b37">[38]</ref>, which is particularly suitable as a basis for introducing a possibilistic version of the EnKF. The update of the SqrtEnKF for a linear observation model is detailed in Algorithm 2, where the notation A 1 /2 refers to the Cholesky factor of matrix A, and where we use the same notation for a linear function and for its matrix representation.</p><p>Algorithm 2 Update step of SqrtEnKF at time step k Input: The predictive ensemble {X i k } N i=1 at time step k with mean µ k and variance Σ k . Output: The posterior ensemble { Xi k } N i=1 at time step k. 1:</p><formula xml:id="formula_3">S k ← H k Σ k H ⊤ k + V k \\Covariance of the innovation from the ensemble 2: K k ← Σ k H ⊤ k S -1 k \\Standard Kalman gain 3: Kk ← Σ k H ⊤ k S 1 /2 k -⊤ S 1 /2 k + V 1 /2 k -1 \\Adjusted Kalman gain 4: μk ← µ k + K k (Y k -H k µ k ) \\Posterior mean 5: for i ∈ {1, . . . , N } do 6: êi k ← (I n -Kk H k )(X i k -µ k ) \\Posterior deviation 7:</formula><p>Xi k ← êi k + μk \\Updated particle 8: end for For the ensemble-based method, the accuracy of the state estimate depends highly on the ensemble size. The situation in which the ensemble is too small to adequately represent the system is termed undersampling and leads to four main problems which have been termed: inbreeding, spurious correlation, matrix singularity, and filter divergence <ref type="bibr" target="#b27">[28]</ref>. Two widely used techniques to negate the undersampling problems are inflation and localisation <ref type="bibr" target="#b27">[28]</ref>. The inflation technique increases the deviation between the sample and the predictive mean so that the predictive covariance from the ensemble is not underestimated and avoids the so-called inbreeding problem. On the other hand, the localisation techniques are implemented to eradicate the spurious correlation and singular matrix, examples being tapering using the Schur product to cut off the long-range correlation or domain localisation where assimilation is performed in a local domain, which is disjoint in a physical space <ref type="bibr" target="#b20">[21]</ref>. Each technique needs to be fine-tuned in a problem-specific manner to achieve good accuracy.</p><p>We have focused here on approaches directly relevant to the methodology developed below in Section 3; many other methods have been devised to address nonlinearity and non-Gaussianity-see, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Review of possibility theory</head><p>Possibility theory aims to directly capture epistemic uncertainty about a fixed but unknown element θ * in a set Θ, by focusing on the possibility of an event related to θ * rather than on its probability. A possibility of 1 corresponds to the absence of evidence against the event taking place not evidence that the event took place almost surely. If there is no evidence against an event, say E, then there cannot be any evidence against (E or E ′ ), with E ′ any other event, so that the possibility of (E or E ′ ) is also equal to 1. This behaviour shows the notion of possibility does not give an additive measure of uncertainty, and the simplest operation that corresponds to the possibility of a union of events is the maximum of their individual possibilities.</p><p>The analogue of a probability density is a so-called possibility function, f : Θ → [0, 1] which must have supremum 1. Just as probabilities are best described by measures, possibilities are naturally cast as outer measures, P and just as one may obtain a probability from a density one can obtain a possibility from a possibility function by taking its supremum, that is, for any A ⊆ Θ, P (A) = sup θ∈A f (θ). If it holds that f (θ) = 1 for some θ ∈ A, then we will indeed have P (A) = P (A ∪ B) = 1 for any B ⊆ Θ, as required.</p><p>Formally, the set function P is an outer measure verifying P (Θ) = 1; we therefore refer to such set functions as outer probability measures (o.p.m.s). If P (A) = 1 models the absence of opposing evidence, P (A) = 0 has essentially the same interpretation as with probabilities, i.e., that θ * ∈ A is impossible, which requires very strong evidence. In general, P (A) can be interpreted as an upper bound for subjective probabilities of the event θ * ∈ A, i.e., the maximum probability that we would be ready to assign to this event is P (A). Under this interpretation, the statement P (A) = 1 is uninformative: we could assign any probability in the interval [0, 1] to this event.</p><p>To formally define the quantities described above, we consider an analogue of a random variable, referred to as an uncertain variable, defined as a mapping θ : Ω → Θ, and interpreted as follows: if the true outcome in Ω is ω then θ(ω) is the true value of the parameter. The set Ω plays a similar role to the elementary event space in probability theory, except that it is not equipped with any probabilistic structure; instead, it contains a true element ω * and it holds by construction that θ(ω * ) = θ * . An event can now be defined as a subset of Ω, for instance {ω ∈ Ω : θ(ω) ∈ A} for some A ⊆ Θ. We will use the standard shortcut and write this event as θ ∈ A. An important difference between possibility theory and probability theory is that uncertain variables do not characterise possibility functions, instead possibility functions describe uncertain variables in a way that is not unique. For instance, if the available information is modelled by a possibility function f describing an uncertain variable θ, then any function g such that g ≥ f , i.e. g(θ) ≥ f (θ) for any θ ∈ Θ, also describes θ. This is because g discarded some evidence about θ, that is, g is less informative than f . In particular, we can consider the possibility function equal to 1 everywhere, denoted 1, which upper bounds all other possibility functions. The function 1 models the total absence of information.</p><p>To gain information from data, Bayesian inference can be performed within possibility theory in a way that is very similar to the standard approach: If Y is a random variable with probability distribution p(• | θ * ) belonging to a parameterised family of distributions<ref type="foot" target="#foot_0">foot_0</ref> {p(• | θ) : θ ∈ Θ}, if we observe a realisation y of Y , and if the information available about θ * a priori is modelled by the possibility function f , then the information available a posteriori can be modelled by the possibility function f (• | y) characterised by <ref type="bibr" target="#b16">[17]</ref> </p><formula xml:id="formula_4">f (θ | y) = p(y | θ)f (θ) sup θ ′ ∈Θ p(y | θ ′ )f (θ ′ ) , θ ∈ Θ.<label>(2)</label></formula><p>We borrow from the Bayesian nomenclature and simply refer to f and f (• | y) as the prior and the posterior. Since we can always start from the uninformative prior f = 1, it is easy to find posterior possibility functions by inserting different likelihoods; for instance, if Θ = R n and if the likelihood is a multivariate Gaussian distribution with mean θ and known variance Σ, i.e., p(y | θ) = N(y; θ, Σ), then</p><formula xml:id="formula_5">f (θ | y) = exp - 1 2 (θ -y) ⊤ Σ -1 (θ -y) . = N(θ; y, Σ).</formula><p>Such a Gaussian possibility function is a conjugate prior for the Gaussian likelihood as in the probabilistic case, and shares many of the properties of its probabilistic analogue. It can be advantageous to parameterise a Gaussian possibility function, say N(θ; µ, Σ), by the precision matrix Λ = Σ -1 ; indeed, the precision matrix does not need to be positive definite for the Gaussian possibility function to be well defined, with positive semi-definiteness being sufficient. In particular, this means that setting Λ to the 0 matrix is possible, with the Gaussian possibility function being equal to 1 in this case. A simple way to quantify the amount of epistemic uncertainty in a possibility function is to consider the integral f (θ)dθ as in <ref type="bibr" target="#b5">[6]</ref>, when defined. This notion of uncertainty is consistent with the partial order on possibility functions: if g is less informative than f then the integral of g will obviously be larger then that of f . In particular, the uncertainty of a Gaussian possibility function is |2πΣ|, with | • | denoting the determinant; a quantity often used to quantify how informative a given Gaussian distribution is. If θ and ψ are two uncertain variables, respectively on sets Θ and Ψ, jointly described by a possibility function f on Θ × Ψ, then Bayes' rule can also be expressed via a more standard notion of conditioning as <ref type="bibr" target="#b7">[8]</ref> </p><formula xml:id="formula_6">f (θ | ψ) = f (θ, ψ) sup θ ′ ∈Θ f (θ ′ , ψ) ,</formula><p>where sup θ ′ ∈Θ f (θ ′ , ψ) is the marginal possibility function describing ψ. The conditional possibility function f (θ | ψ) allows independence to be defined simply:</p><formula xml:id="formula_7">if f (θ | ψ) is equal to the marginal f (θ) = sup ψ ′ ∈Ψ f (θ, ψ ′ )</formula><p>for any θ ∈ Θ, then θ is said to be independent from ψ under f . As with most concepts in possibility theory, the notion of independence depends on the choice of possibility function; if θ and ψ are not independent under f , we could find another possibility function g such that g ≥ f and such that θ is independent of ψ under g. This will be key later on for simplifying high-dimensional Gaussian possibility functions in a principled way. When there is no ambiguity about the underlying possibility function, we will simply say that θ is independent of ψ.</p><p>Although the parameters µ and Σ of the Gaussian possibility functions are reminiscent of the probabilistic notions of expected value and variance, these notions have to be redefined in the context of possibility theory. Asymptotic considerations <ref type="bibr" target="#b19">[20]</ref> lead to a notion of expected value E * (•) based on the mode, that is E * (θ) = arg max θ∈Θ f (θ), for any uncertain variable θ described by f , and to a local notion of variance based on the curvature of f at E * (θ), assuming that the latter is a singleton, an assumption that we will make throughout this work. These quantities correspond to the approximation often referred to as Laplace/Gaussian approximation, and make sense in the context of epistemic uncertainty: without a notion of variability, we simply look at our best guess E * (θ) for the unknown θ * and at how confident we are in that guess, i.e., how fast the possibility decreases around it. Although the expected value and variance associated with the Gaussian possibility function N(µ, Σ) are indeed µ and Σ, they will differ in general from their probabilistic counterparts.</p><p>In order to make inference for dynamical systems simpler, we introduce a Markovian structure as follows: We consider a sequence of uncertain variables x 0 , x 1 , . . . in a set X and assume that x k is described by a possibility function f k for some given k ≥ 0. Following the standard approach, we assume that x k+1 is conditionally independent of x k-δ given x k , for any δ &gt; 0. The available information about x k+1 given a value x k of x k can then be modelled by a conditional possibility function f k+1|k (• | x k ), and prediction can be performed via</p><formula xml:id="formula_8">f k+1 (x k+1 ) = sup x k ∈X f k+1|k (x k+1 | x k )f k (x k ), x k+1 ∈ X.</formula><p>We will focus in particular on the case where an analogue of (1a) holds, that is</p><formula xml:id="formula_9">x k = F k (x k-1 ) + u k ,</formula><p>with u k an uncertain variable described by N(0, U k ). In this case, the term u k cannot be interpreted as noise and models instead deviations between the model and the true dynamics: the true states</p><formula xml:id="formula_10">x * k and x * k-1 are related via x * k = Fk (x * k-1 ) with Fk potentially different from F k ; the true value u * k of interest is therefore equal to the difference Fk (x * k-1 ) -F k (x * k-1</formula><p>) between the model and the true dynamics. In the case where the prediction is deterministic, that is x k+1 = F k (x k ) for some possibly non-linear mapping F k , we can use the change of variable formula <ref type="bibr" target="#b2">[3]</ref> to characterise the possibility function f k+1 as</p><formula xml:id="formula_11">f k+1 (x k+1 ) = sup f k (x k ) : x k ∈ F -1 k (x k+1 ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_12">F -1 k (x k+1</formula><p>) is the (possibly set-valued) pre-image of x k+1 via F k , and sup ∅ = 0 by convention. A result that is specific to possibility theory is that the expected values of x k+1 and x k are related via E * (x k+1 ) = F k (E * (x k )) without assumptions on F k . This result will be key in our approach since it allows to compute the expected value at time step k + 1 with a single application of F k , rather than averaging ensembles as in the EnKF. This result does not hold for the mode of probability densities because the corresponding change of variable formula include the Jacobian of the transformation, which shifts the mode in non-trivial ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Possibilistic EnKF</head><p>It is has been shown in <ref type="bibr" target="#b18">[19]</ref> that an analogue of the Kalman filter can be derived in the context of possibility theory, and that the corresponding expected value and variance are the same as in the probabilistic case. However, as with the probabilistic KF, its applicability is limited to linear models-so it is natural to develop extensions that accommodate nonlinearity. Since the probabilistic EnKF is flexible in computational cost and can be effectively applied to nonlinear models, we explore the use of EnKF-like ideas in the possibilistic setting. Here, we present a novel possibilistic EnKF (p-EnKF) and show that analogues of inflation and localisation arise naturally in the context of possibility theory rather than being imposed as heuristics to improve performance as in the standard setting.</p><p>For the p-EnKF, we use an ensemble of weighted particles to characterise the possibility function and follow a similar path to the standard EnKF by assuming that the underlying possibility function is Gaussian in order to proceed with a KF-like update. For this purpose, we need to define two operations: 1) how to approximate a given possibility function by weighted particles? This will be necessary for initialisation at time step k = 0, and 2) how to define a Gaussian possibility function based on weighted particles? This will be necessary to carry out the Kalman-like update. We start by answering the latter question in Section 3.2, before moving to the former in Section 3.3. Based on this, we detail the prediction and update mechanism of the p-EnKF in Sections 3.4 and 3.5 respectively, and consider some extensions in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ensemble approximation</head><p>We consider the problem of defining a set {(w i , x i )} N i=1 of N weighted particles in R n , which will allow the approximate solution of optimisation problems of the form max x∈R n φ(x)f (x), for some bounded function φ on R n and for a given possibility function on R n . Although our approach could be easily formulated on more general spaces than R n , Euclidean spaces are sufficient for our purpose in this work and help to simplify the presentation. As is common in Monte Carlo approaches, we do not want the particles or weights to depend on φ, so as to allow us to solve this problem for many different such functions using a single sample, we therefore focus on directly approximating f : placing particles in locations which allow a good characterization of f allows us to approximate the optimization for a broad class of regular φ. Following the principles of Monte Carlo optimization <ref type="bibr" target="#b29">[30,</ref><ref type="bibr">Chapter 5]</ref>, and assuming that f (x)dx &lt; +∞, we can sample from the probability distribution proportional to f to obtain the particles {x i } N i=1 and then weight these particles with w i = f (x i ), for any i ∈ {1, . . . , N }. We then obtain the approximation</p><formula xml:id="formula_13">max x∈R n φ(x)f (x) ≈ max i∈{1,...,N } w i φ(x i ).</formula><p>This approximation can be proved to converge when N → ∞ under mild regularity conditions on φ as long as the supports of φ and f are not disjoint. This is a simple default choice for the methods developed below, although more sophisticated approaches are possible.</p><p>Remark 1. One initially appealing idea is to consider a form of maximum entropy principle by identifying the constraints we want to impose on the probability distributions from which we may wish to sample, and to pick the distribution of maximum entropy subject to satisfying these constraints. This is a standard argument for specifying the "least informative" distribution with particular properties, e.g., given a distribution when the mean and variance the maximum entropy distribution is the Gaussian with these moments. In the considered context, the constraints comes from the interpretation of o.p.m.s as upper bounds for probability distributions. Specifically, for an o.p.m. P , we could draw an ensemble from the probability density p satisfying the following criteria: p has the maximum entropy amongst those distributions satisfying A p(x)dx ≤ P (A) for any A ⊆ R n . The distribution p would typically be more diffuse than the one proportional to f , which can be beneficial for optimizing functions φ taking large values in the "tails" of f . However, obtaining such a distribution is in general highly non-trivial beyond discrete or univariate problem.</p><p>As there are no constraints on the way in which particle locations, {x i } N i=1 are selected, deterministic schemes such as quasi Monte Carlo <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref> could also be considered. In Section 4, we will consider using the same approach as in the UKF to define particle locations, which will prove to be beneficial in some situations.</p><p>In the situations that we will consider, it will always be the case that f is maximised at a single known element x * ∈ R n . Therefore, it makes sense to add one particle x 0 = x * with weight w 0 = 1. This will prove beneficial when fitting a Gaussian possibility function to the ensemble {(w i , x i )} N i=0 , as detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Best-fitting Gaussian possibility function</head><p>We consider the situation in which the epistemic uncertainty is captured by a set {(w i , x i )} N i=0 of N + 1 weighted particles in R n , with w i = 1 if and only if i = 0. We denote by f the "empirical" possibility function defined based on the ensemble as f (x) = max i∈{0,...,N } w i 1 xi (x), with 1 xi the indicator of the point x i . The standard approach would be to simply compute the (weighted) mean and variance of the ensemble, but these notions do not apply directly to possibility functions, and the curvature-based possibilistic notion of variance is not defined for an empirical possibility function like f . Instead, we aim to fit a Gaussian possibility function N(µ, Σ) to this (weighted) ensemble and the considered variance will simply be the second parameter Σ of this fitted Gaussian.</p><p>To fit the Gaussian N(µ, Σ) to the ensemble, we need a notion of best fit. Based on the partial order between possibility functions described in Section 2.2, we can easily ensure that N(µ, Σ) does not introduce artificial information-at least at locations {x i } N i=0 -by requiring that N(µ, Σ) ≥ f . Since f (x) = 0 when x / ∈ {x i } N i=0 , we can simplify this condition to: N(x i ; µ, Σ) ≥ w i for all i ∈ {0, . . . , N }. The inequality N(µ, Σ) ≥ f forces the expected values of N(µ, Σ) and f to coincide, from which we can deduce that µ = x 0 .</p><p>For the variance, we aim to minimise the uncertainty in the possibility function N(µ, Σ), i.e., minimising N(x; µ, Σ)dx ∝ |Σ|, which is equivalent to maximising log |Λ| with Λ = Σ -1 the precision matrix, thanks to the properties of the determinant. The precision matrix is then most naturally defined as the one solving the constrained optimisation problem max</p><formula xml:id="formula_14">Λ∈S d + log |Λ| subject to N(x i ; µ, Σ) ≥ w i , 1 ≤ i ≤ N,<label>(4)</label></formula><p>where S d + is the cone of positive semi-definite d × d matrices. Since µ = x 0 , the corresponding constraint N(x 0 ; µ, Σ) ≥ w 0 is automatically satisfied and we only need to ensure our Gaussian possibility function upper bounds the ensemble at the other x i 's. Using the invariance of the trace to cyclic permutations allows these constraints to be rewritten as</p><formula xml:id="formula_15">(x i -µ) ⊤ Λ(x i -µ) = Tr (C i Λ) ≤ -2 log w i , 1 ≤ i ≤ N,<label>(5)</label></formula><p>where</p><formula xml:id="formula_16">C i = (x i -µ)(x i -µ) ⊤ ; this is more convenient for numerical optimization because Tr(C i Λ) is linear in Λ.</formula><p>Remark 2. In the one-dimensional (n = 1) case, this optimisation problem can be easily solved for any N ≥ 1: the i th constraint in (5) can be expressed directly for the (scalar) variance as σ 2 . = Σ ≤ -2 log w i /(x i -µ) 2 , and (5) reduces to a single constraint:</p><formula xml:id="formula_17">σ 2 ≤ min i∈{1,...,N } -2 log w i (x i -µ) 2 . (<label>6</label></formula><formula xml:id="formula_18">)</formula><p>In particular, setting σ 2 to the right hand side of this inequality will maximise the precision, hence solving the optimisation problem (4). In this case, if it happens that w i = f (x i ) with f a Gaussian possibility function, then the associated variance will be recovered exactly, even with N = 1. Although this result does not generalise easily to higher dimensions, it highlights the potential of this method for recovering the variance from an ensemble, as will be studied later in this section.</p><p>We will denote by Λ * {(w i , x i )} i the solution to (4), omitting the limits on i for concision. In addition to providing an estimate for the variance of the best-fitting Gaussian, this approach provides a measure of how far from Gaussian the ensemble is. Indeed, a notion of distance can be defined based on the gaps -Tr (C i Λ) -2 log w i , i ∈ {1, . . . , N }.</p><p>We will be interested in the relationship between the best Gaussian fit for a given ensemble and the one for a linear and invertible transformation of that ensemble, which motivates the following proposition.</p><p>Proposition 1. Let {(w i , x i )} i be an ensemble such that w i = 1 if and only if i = 0, and let M be an invertible linear map on R n , it holds that</p><formula xml:id="formula_19">Λ * {(w i , x i )} i = M ⊤ Λ * {(w i , M x i )} i M.</formula><p>Proof. The constraints (5) for the ensemble (w i , M x i ) can be rewritten as</p><formula xml:id="formula_20">∀i ∈ {1, . . . , N } : (M x i -M x 0 ) ⊤ Λ(M x i -M x 0 ) = (x i -x 0 ) ⊤ M ⊤ ΛM (x i -x 0 ) ≤ -2 log w i .</formula><p>Changing the variable of the optimisation problem from Λ to Λ = M ⊤ ΛM changes the objective function to log |M -⊤ ΛM -1 | = log | Λ| + constant. Therefore, the two optimisation problems are equivalent, and their solutions are related as stated.</p><p>From a practical viewpoint, the computational cost for calculating the ensemble's variance via ( <ref type="formula" target="#formula_14">4</ref>) is greater than that of the probabilistic framework due to the optimisation problem. Yet, Remark 2 hints at a potential for an efficient recovery of the true variance in the Gaussian case. This aspect is investigated in Figure <ref type="figure" target="#fig_1">1a</ref> which displays the average root mean squared error (RMSE) between the true variance and the ensemble's variance based on different sample sizes and for n ∈ {8, 16, 32}, averaged over 1000 repeats. The true n × n covariance matrix Σ is drawn from the inverse Wishart distribution with n 2 degrees of freedom and scale matrix nI n , with I n the identity matrix of dimension n × n, making its reconstruction appropriately challenging. Here, we use a sample x i ∼ N(0 n , Σ) for i = 1, . . . , N , with 0 n the zero vector of size n, to both estimate the probabilistic variance and to be used as the ensemble in (4) with weights w i = N(x i ; 0 n , Σ). The performance of each case is investigated from the minimum required sample size N = n up to N = 500. Figure <ref type="figure" target="#fig_1">1a</ref> shows that the covariance matrix obtained from (4) indeed converges to the true underlying variance faster than the probabilistic one for small dimensions. In particular, the associated RMSE appears to drop significantly around N = n 2 .  The computational cost of the probabilistic approach is very small for the considered range of sample sizes and remains around 0.1 ms even for n = 32 in our experiments, which is likely due to other operations dominating the computational cost in the considered settings. This is in contrast to the computational time of the proposed method, shown in Figure <ref type="figure" target="#fig_1">1b</ref>, which has a clear linear trend and is orders of magnitude larger than in the probabilistic case.</p><p>One advantage of defining an ensemble's variance via (4) is that the knowledge of dependency-or lack thereof-between components of the states x ∈ R n can be more easily integrated. For instance, if we want to impose conditional independence between some components, i.e., to impose that Λ i,j = 0 for any indices i and j ̸ = i in a given set I, then we can add a set of constraints in the optimisation problem <ref type="bibr" target="#b3">(4)</ref>. Adding constraints will reduce the precision, i.e., it will yield a Gaussian possibility function that is less informative than the less-constrained problem. This behaviour can be interpreted as follows: in the possibilistic framework, conditional independence can be obtained by sacrificing some information. This is a well-understood trade-off in data assimilation, where forcing components to be uncorrelated, a method known as localisation, is usually accompanied by an increase of the variance of the remaining components, a process known as inflation. The main difference between the standard inflation and the proposed approach is that inflation usually comes with additional parameters that need to be fine-tuned for different situations, whereas the required amount of precision loss is automatically determined via the optimisation problem (4) when adding the constraints on conditional independence, with no additional tuning parameters.</p><p>Apart from defining the ensemble's expected value and variance, three steps of the algorithm need to be developed: initialisation, prediction, and update. These will be analogous to those of the probabilistic EnKF, but significant changes are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initialisation</head><p>We consider a sequence of uncertain variables x 0 , x 1 , . . . , with x k representing the state of the system at time step k. We assume that there is prior knowledge about x 0 , encoded into a possibility function N(µ 0 , Σ 0 ). This possibility function is approximated by an ensemble {(w i , xi 0 )} N i=0 of N + 1 weighted particles, defined as in Section 3.1. The time index, 0 in this case, is omitted for the weights as these will in fact remain constant throughout the algorithm, which relies exclusively on transports of the associated particles. The initialisation step is detailed in Algorithm 3. w i ← N(x i 0 ; µ 0 , Σ 0 ) \\Calculate the corresponding weight 4: end for 5: x0 0 ← µ 0 \\Add a particle deterministically at the mode 6: w 0 ← 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction step</head><p>In the probabilistic framework, the standard way of obtaining the predictive ensemble at time step k is to i) apply the transition model F k to each particle, and ii) add a realisation of the transition noise. The first part of this process can be used as is, with some advantageous properties: a key result is the fact that</p><formula xml:id="formula_21">E * (F k (x k-1 )) = F k (E * (x k-1</formula><p>)) with no assumption on either F k or x k-1 . This allows to perform the prediction without recomputing the expected value, therefore stabilising the estimation through non-linear dynamics. In practice, this means that the particle with index 0 will always correspond to the expected value, hence its special treatment. In practice, given the ensemble {(w i , xi k-1 )} N i=0 at time step k -1, we can simply compute the image xi k = F k (x i k-1 ) of each particle to capture the information at time k in the absence of perturbations.</p><p>However, the second part of the standard approach is not appropriate in the possibilistic setting since we aim to model epistemic uncertainty which, in this context, corresponds to deviations between the model and the actual dynamics rather than real perturbations. Deterministic methods can however be adapted and we consider using transport maps to move the points of the ensemble as in <ref type="bibr" target="#b31">[32]</ref>. To construct such a map for our setting, we first characterize linear transformations of uncertain variables as follows.</p><p>Proposition 2. If x is an uncertain variable on R n described by N(µ, Σ) and z = Ax + b where A is an n × n invertible matrix and b is a constant vector of length n, then z is described by N(Aµ + b, AΣA ⊤ ).</p><p>Proof. Let f x denote the Gaussian possibility function N(µ, Σ). Applying (3), the possibility function of the uncertain variable z is</p><formula xml:id="formula_22">f z (z) = sup{f x (x) : x ∈ R n , z = Ax + b} = sup{f x (x) : x = A -1 (z -b)} = f x (A -1 (z -b)) = N A -1 (z -b); µ, Σ = N z; Aµ + b, AΣA ⊤ .</formula><p>According to Proposition 2, Gaussianity is preserved under linear and invertible transformations in the possibilistic framework. Furthermore, it is straightforward to obtain a map between two uncertain variables in R n described by Gaussian possibility functions: Proposition 3 (Mapping between Gaussian possibility functions). If x and z are two uncertain variables in R n described respectively by N(µ, Σ) and N(μ, Σ), then there exists a map M such that z = M (x), which is characterised by</p><formula xml:id="formula_23">M (x) = μ + T (x -µ), x ∈ R n</formula><p>where T = Σ1 /2 Σ 1 /2 -1 , with the notation A 1 /2 referring to the Cholesky factor of a matrix A.</p><p>Proof. First, we rewrite the mapping as follows</p><formula xml:id="formula_24">M (x) = μ + T (x -µ) = T x + (μ -T µ)</formula><p>As this is a linear and invertible transformation of x, Proposition 2 guarantees that z = M (x) is also described by a Gaussian possibility function, defined as</p><formula xml:id="formula_25">f z (z) = N T µ + (μ -T µ), T ΣT ⊤ = N μ, Σ1 /2 Σ 1 /2 -1 Σ 1 /2 Σ 1 /2 ⊤ Σ 1 /2 -⊤ Σ1 /2 ⊤ = N μ, Σ .</formula><p>Such a transport map can be used in the prediction step of the p-EnKF to add uncertainty from the transition model to the ensemble using deterministic mapping as follows: We fit a Gaussian possibility function N(µ k , Σk ) to the ensemble {(w i , xi k )} N i=0 , then, from the prediction of the possibilistic Kalman filter <ref type="bibr" target="#b18">[19]</ref>, we know that an additive Gaussian uncertainty of the form N(0 n , U k ) will yield a Gaussian predictive possibility function with expected value µ k and variance Σk + U k . Using Proposition 3, we can compute a map M k from N(µ k , Σk ) to N(µ k , Σk + U k ) and apply it to each particle xi k to obtain the predicted ensemble at time k. The prediction step of the p-EnKF is summarised in Algorithm 4. Although we assume the Gaussianity of the ensemble at a slightly earlier stage than the standard EnKF, U k is typically small compared to Σk so that the impact of this assumption is expected to be small. In addition, fitting a Gaussian possibility function to the predicted ensemble is unnecessary as the result is known to be N(µ k , Σ k ), with Σ k = Σk + U k , based on Proposition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Prediction step of the p-EnKF at time k</head><formula xml:id="formula_26">Input: The time k -1 posterior ensemble, {(w i , xi k-1 )} N i=0 . Output: The time k predictive ensemble, {(w i , x i k )} N i=0</formula><p>, expected value µ k , and variance Σ k 1: for i ∈ {1, . . . , N + 1} do 2:</p><formula xml:id="formula_27">xi k ← F k (x i k-1 )</formula><p>\\Apply dynamics to particles 3: end for 4: µ k ← x0 </p><formula xml:id="formula_28">T ← Σk + U k 1 /2 Λ1 /2 k</formula><p>\\Compute matrix for adding uncertainty 9:</p><formula xml:id="formula_29">x i k ← µ k + T (x i k -µ k )</formula><p>\\Transport each particle 10: end for 11: Σ k ← Σk + U k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Update step</head><p>We first need to specify how the observation equation (1b) will be adapted to the considered context. Since perturbations in sensors are often stochastic in nature, we continue to model the error in the observation with a random variable, that is</p><formula xml:id="formula_30">Y k = H k (x k ) + ε k , with ε k ∼ N(0, V k ) as before.</formula><p>The mechanism to update the information on x k accordingly is provided by <ref type="bibr" target="#b1">(2)</ref>. In some situations, the main source of uncertainty in the observation will be of an epistemic nature, yet, if the corresponding model errors are described by N(0, V k ), then the posterior possibility function will be the same; this follows from the likelihood principle since the two associated likelihoods will only differ by a multiplicative constant.</p><p>There are several variants of the probabilistic EnKF update step. Here, we follow the principles of the SqrtEnKF as it is well suited to the non-random setting of interest. In fact, we will show that our ensemble can be updated exactly in the same way as in the SqrtEnKF.</p><p>From the prediction step, we know that the best fitting Gaussian possibility function for the predicted sample is N(µ k , Σ k ). As is standard, we consider the deviations e i k = x i k -µ k and we verify that the correct updating formulas for the expected value and deviations are</p><formula xml:id="formula_31">μk =µ k + K k (y k -H k µ k ),</formula><p>and êi</p><formula xml:id="formula_32">k =e i k -Kk H k e i k ,</formula><p>where K k and Kk are as defined in the Kalman filter and SqrtEnKF, respectively. The updated particles obtained by adding the posterior expected value and deviations together are</p><formula xml:id="formula_33">xi k = Mk (x i k ) . = (I n -Kk H k )x i k + ( Kk H k µ k + K k y k -K k H k µ k ),</formula><p>which is a linear transformation of x i k . Defining x k as an uncertain variable described by N(µ k , Σ k ) and using Proposition 3, it follows that Mk (x k ) is described by</p><formula xml:id="formula_34">fk = N (I n -Kk H k )µ k + ( Kk H k µ k + K k y k -K k H k µ k ), (I n -Kk H k )Σ k (I n -Kk H k ) ⊤ = N (µ k + K k (y k -H k µ k ), (I n -K k H k )Σ k ) ,</formula><p>which matches the update mechanism of the KF, as required. The map Mk is therefore moving the particles in such a way that the best fitting Gaussian for {(w i , Mk (x i k ))} is the posterior of the KF with N(µ k , Σ k ) as a predicted possibility function. Thus, the update step of the p-EnKF is formally the same as that of the standard SqrtEnKF, as detailed in Algorithm 2, albeit with a difference in interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Extensions</head><p>We finish this section by collecting together some extensions to the p-EnKF, demonstrating its applicability in the context of nonlinear measurement models and providing a number of ways to improve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Nonlinear observation models</head><p>In the previous section, we have detailed the p-EnKF with a linear observation model. We now establish that, similarly to the EnKF <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref>, the p-EnKF could be adapted to nonlinear observation models. There are, in fact, two main approaches to dealing with nonlinearity in the observation model, which we consider in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model linearisation</head><p>The simplest way to handle a nonlinear observation model is to linearise it, i.e., to Taylor expand the observation model around the predictive expected value at time k as</p><formula xml:id="formula_35">H k (x k ) ≈ H k (µ k ) + J H k (µ k )(x k -µ k ), with J H k (µ k ) the Jacobian matrix of H k at µ k .</formula><p>Then, a new observation model can be defined based on the observation matrix J H k (µ k ) and on a non-zero mean H k (µ k ) -J H k (µ k )µ k for the observation noise ε k . After that, we can follow Algorithm 2 for the update, except that the term</p><formula xml:id="formula_36">y k -H k µ k is replaced by y k + J H k (µ k )µ k in step 4.</formula><p>Since linearisation does not depend on the chosen representation of uncertainty, it is equally applicable to the p-EnKF as it is to standard versions of the EnKF. The linearisation method can usually be improved by replacing the term H k Σ k H ⊤ k by the predictive variance of the observation based on the ensemble <ref type="bibr" target="#b32">[33]</ref>, which is also applicable to the p-EnKF. The term Σ k H ⊤ k can usually also be replaced by a ensemble-based approximation, yet, this is not straightforward in the p-EnKF since the precision matrix does not have the same properties as the covariance matrix: to compute the covariance matrix Σ x,H k (x) between the uncertain variables x and H k (x), one must first compute the precision matrix for (x, H k (x)), invert it, and then extract the block corresponding to Σ x,H k (x) . Such an extension of the state is however usual, as described next.</p><p>Extending the state space Another way to deal with a nonlinear observation model is by extending/augmenting the original state with a the corresponding predicted observation, which is then observed linearly. In this case, the state becomes z k = (x k , H k (x k )) and the extended observation matrix is Hk = 0 m×n I m , with 0 m×n the 0 matrix of size m × n. Algorithm 2 can be used as usual once a Gaussian is fitted to this extended state. The posterior ensemble can then be extracted by choosing the first n elements of the extended state for every particle. Care must be taken in practice as the precision matrix of the extended state can be close to singularity due to a strong correlation between the elements. For instance, if one of the components of the state that is observed independently becomes sufficiently well estimated at a given time step, then it might be that the nonlinear observation function is approximately linear from the viewpoint of the ensemble. Yet, this corresponds to cases where linearisation would be appropriate. It therefore appears that a hybrid technique would be the most suitable, with components of the observations being either linearised or included in the state depending on their observed degree of nonlinearity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Techniques to improve computational efficiency</head><p>As with any ensemble-based technique, the accuracy of the p-EnKF depends on ensemble size. As it is typically of interest to use small ensembles for computational reasons, it can be challenging to represent the state of interest adequately. An important aspect is that the p-EnKF requires a minimum sample size equal to the state's dimension plus one due to the computation of the variance; a sample size matching the dimension plus one is sufficient to ensure that the resulting covariance matrix is of full rank providing only that the collection of displacements from the expected value to each of the sample points are linearly independent whereas a smaller sample will lead to a rank-deficient covariance matrix. That a sample size equal to the dimension plus one suffices follows from the fact that a quadratic form bounded away from zero at a number of points separated from the centroid by linearly-independent vectors cannot vanish anywhere.</p><p>However, the required sample size can be reduced by using one technique in the following section. Despite this constraint on the sample size, some methods can be considered for improving the computational efficiency, we present two of them in what follows.</p><p>Conditional Independence For the p-EnKF, variance is computed via an optimisation problem. Thus, the number of variables in the precision matrix of the state x ∈ R n will be n(n + 1)/2. However, for many problems, there is a natural structure in the state variable, such as a conditional-independence structure, which can be exploited to reduce this number. Indeed, for sparsely dependent models, it is straightforward to reduce the number of nonzero variables in the precision matrix Λ using conditional independence as follows: The off-diagonal elements Λ ij that i ̸ = j are set to 0 if the variable x i and x j are to be modelled as conditionally independent. By setting some of the off-diagonal elements in the precision matrix to 0 during the computation of this matrix by optimisation, the other terms in the precision matrix will automatically adjust to these constraints, offering a systemic way to perform inflation that is tailored to the strength of the dependence being assumed away. However, exploiting this structure to gain in computational efficiency would require optimisation algorithms that are specifically tailored to sparse/band diagonal positive-definite matrices. The design of such algorithms is left for future work.</p><p>Nodal Numbering Scheme Apart from reducing the number of variables, the calculation and storage can be done more efficiently by reordering the variables to minimise the bandwidth of nonzero entries in the covariance matrix. One of the methods to achieve that is proposed in the literature is called the nodal numbering scheme <ref type="bibr" target="#b6">[7]</ref>. This method uses the graph to reorder the state's element so that the nonzero elements will be close to the diagonal element. Moreover, the nodal numbering scheme ensures that the permuted matrix will have a bandwidth no greater than the original matrix, making the computation involved with the precision matrix more efficient, see <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b24">[25]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Experiments</head><p>Here we show the performance of the p-EnKF using simulated data from two different models: a simple linear model and a modified Lorenz 96 model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generation</head><p>One convenient feature of standard probabilistic modelling is that simulation can be performed exactly according to the model: the variability of scenarios, necessary for a thorough performance assessment, can be obtained directly by sampling from the assumed probability distributions. This is no longer the case with possibility theory since embracing epistemic uncertainty means that sampling is no longer a natural operation. The ideal solution would be to obtain a sufficiently large collection of real datasets for which the ground truth is known, this is however not generally achievable for problems like data assimilation. Instead, we generate our simulated scenarios by sampling from the probability distributions assumed by the probabilistic baselines and align our possibilistic model with these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear model</head><p>We first consider a linear model since the performance can be clearly compared with the optimal filter, which can be computed in this instance via the KF. A simple linear model is considered so as to generalise easily to arbitrary dimensions. In particular, we consider that the i-th component at time k, i ∈ {2, . . . , n}, only depends on the i-th and (i-1)-th components at time k-1. This means that conditional independence can be imposed to reduce the computational cost of obtaining the precision matrix with a limited information loss. The model can be written as a state-space model (1), for k ∈ {1, . . . , 100}, with the following components:</p><p>1. The initial state X 0 is sampled from N(0 n , 10I n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The dynamic model is linear</head><formula xml:id="formula_37">: F k (X k-1 ) = F k X k-1 with F k =        1 λ 0 • • • 0 0 1 λ • • • 0 0 0 1 • • • 0 . . . . . . . . . . . . . . . 0 0 0 • • • 1       </formula><p>, where λ = 0.1 and the covariance matrix of the dynamical noise ϵ k is U k = 0.01I n .</p><p>3. The observation model is also linear, H k (X k ) = H k X k , with H k = I m 0 m×(n-m) and the covariance matrix of the observation noise</p><formula xml:id="formula_38">ε k is V k = 0.1I m .</formula><p>Based on all the shared properties between the Gaussian possibility function and the Gaussian distribution, the best way to align our possibilistic model with the assumed probabilistic one is to simply keep the expected value and variance parameters in our possibility functions. In particular, we assume that the initial state x 0 is described by the Gaussian possibility function N(0 n , 10I n ) and that the errors in the dynamical model are described by N(0 n , U k ).</p><p>The parameters of the considered methods are as follows: Unless otherwise stated, the parameter N is set to twice the state's dimension, i.e., N = 2n. For a given value of N , the actual number of samples for all methods is N + 1. The parameters of the UKF chosen in this paper are given by α = 0.25, κ = 130, λ = α 2 (n + κ) -n and β = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Performance assessment for various sample sizes and dimensions</head><p>Since the p-EnKF is an ensemble-based method, it is natural for us to investigate the performance based on different sample sizes and dimensions first. Figure <ref type="figure" target="#fig_7">2</ref> shows the performance of the p-EnKF with no localisation when the state is fully observed (n = m), comparing it to the SqrtEnKF and the UKF. The performance is measured in terms of average RMSE over 1000 realisations, except for n = 64 where we only consider 50 realisations due to a large computational time (more than 30 minutes per run). We aim to assess the performance after initialisation and thus focus on the estimation of the state X 100 at the last time step. The RMSE is computed i) for the posterior expected value with respect to (w.r.t.) the posterior mean of the KF, ii) for the posterior expected value w.r.t. the true state, and iii) for the posterior variance w.r.t. the posterior variance of the KF. The key aspects in Figure <ref type="figure" target="#fig_7">2</ref> are as follows:</p><p>1. As can be seen in Figure <ref type="figure" target="#fig_7">2a</ref>, despite the similarities between the p-EnKF and the SqrtEnKF, the latter improves on the former by at least 4 orders of magnitude in terms of RMSE w.r.t. the posterior mean of the KF. Although the SqrtEnKF could be used when N &lt; n, which is key in large dimensions, the performance would necessary be lower in this regime. The difference in performance is still visible when considering the RMSE w.r.t. the true state, as in Figure <ref type="figure" target="#fig_7">2b</ref>, however it is less pronounced due to the unavoidable error caused by the distance between the true state and the optimal estimator given by the mean of the KF.</p><p>2. Despite the fact that the capabilities in terms of variance recovery are almost indistinguishable in Figure <ref type="figure" target="#fig_1">1a</ref> between the possibilistic and probabilistic approach, Figure <ref type="figure" target="#fig_7">2c</ref> shows that the p-EnKF once again largely outperforms the SqrtEnKF in terms of RMSE w.r.t. the optimal variance given by the KF, with improvements by at least 4 orders of magnitude throughout once again. This is due to the fact that here, as opposed to Figure <ref type="figure" target="#fig_1">1a</ref>, the mean of the ensemble also needs to be estimated by the SqrtEnKF whereas the expected value for the p-EnkF is given by the particle with index 0 and thus does not need to be re-estimated.</p><p>The estimates of the UKF are closer to optimality than the ones of the p-EnKF in Figures <ref type="figure" target="#fig_7">2a</ref> and <ref type="figure" target="#fig_7">2c</ref>; this could be due to the difference in initialisation between the two algorithms, with the UKF placing points deterministically and with the p-EnKF relying on a random sample at the first time step. Yet, this source of randomness in the p-EnKF is not necessary and other initialisation schemes are considered in the next section. VDPSOHVL]H GLPHQVLRQ S(Q.) VDPSOHVL]H GLPHQVLRQ 6TUW(Q.) VDPSOHVL]H GLPHQVLRQ 8.)   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VDPSOHVL]H GLPHQVLRQ H H H H H H H H H H H</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VDPSOHVL]H GLPHQVLRQ H H H H H H H H H H H</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Alternatives schemes for particle initialisation</head><p>We now investigate the performance of the p-EnKF with different initialisation schemes inspired by the UKF. When N = 2n, we can simply consider exactly the σ-points of the UKF as initial point for the p-EnKF and we refer to this scheme as "UKF initialisation". To allow for setting N = n, we also arbitrarily consider only the σ-points which corresponds to increasing (resp. decreasing) one of the components of the mean vector and we refer to this scheme as "UKF initialisation +" (resp. "UKF initialisation -").</p><p>To better highlight the dependency on the initialisation scheme, we consider in this section a partially observed model with m = 1, i.e., only the first component of the state is observed. This is challenging in general, so only problems of small state dimension are considered. In Figure <ref type="figure" target="#fig_9">3</ref>, the performance assessment is carried out for n = 3 and n = 5, with all RMSEs being averaged over 1000 repeats. In Figures <ref type="figure" target="#fig_9">3a</ref> and <ref type="figure" target="#fig_9">3b</ref>, all the considered initialisation schemes are compared against the SqrtEnKF and the StEnKF with 2n + 1 samples, with the poor performance of the latter highlighting the difficulty of these inference problems despite the small dimension. There is a slight but consistent improvement in performance when using the UKF initialisation schemes in the p-EnKF, although this improvement vanishes after 30 to 50 time steps, depending on the state dimension. Figures <ref type="figure" target="#fig_9">3c</ref> and <ref type="figure" target="#fig_9">3d</ref> are restricted to the StEnkKF, StEnKF and p-EnKF with UKF initialisation for the sake of legibility, and show that although the interquartile range of the different methods overlap, the p-EnKF performs consistently well across different realisations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Banded vs. full precision matrix</head><p>To illustrate the capabilities of the p-EnKF to deal with localisation via a systematic form of inflation, we contrast its performance with a full precision matrix against the one with a banded precision matrix with a bandwidth of two, i.e., where all elements except the diagonal and elements adjacent to it are set to zero. We compare these results against the ones obtained with the standard versions of the EnKF for the fully-observed (n = m = 5) and partially-observed (n = 5, m = 1) cases. The performance is measured with three quantities i) the RMSE of the posterior expected value w.r.t. the true state, ii) the determinant of the posterior variance, and iii) the Mahalanobis distance between the posterior expected value and the true state. The Mahalanobis distance is used to capture how good the estimate is relative to its variance and, hence, assesses the calibration of the algorithms in terms of uncertainty quantification. It is defined as (x -µ) ⊤ Σ -1 (x -µ) where x is the true state, µ is a given posterior expected value, and Σ is the posterior variance.</p><p>Figure <ref type="figure" target="#fig_11">4</ref> shows the performance of the p-EnKF with full and banded precision matrix against the same baselines as before. All the algorithms except the KF use 11 samples, i.e., the ensemble size is N + 1 where N = 2n and the considered performance metrics are averaged over 1000 repeats. The important aspects in Figure <ref type="figure" target="#fig_11">4</ref> are as follows:</p><p>1. As can be seen in Figures <ref type="figure" target="#fig_11">4c</ref> and <ref type="figure" target="#fig_11">4d</ref>, forcing the precision matrix to be banded has little impact on the precision of the p-EnKF, despite correlations being crucial for a strong performance in the partially observed case (Figure <ref type="figure" target="#fig_11">4d</ref>). This is confirmed in Figures <ref type="figure" target="#fig_11">4c</ref> and <ref type="figure" target="#fig_11">4d</ref>, where the log-determinant of the posterior variance is mostly unaffected by localisation. A small but noticeable difference can be seen in Figure <ref type="figure" target="#fig_11">4d</ref>, but the change in variance is in the correct direction: the determinant of the variance was increased by localisation, i.e., some inflation has been automatically applied in order to compensate for the imposed conditional independence.</p><p>2. The Mahalanobis distance for the two versions of the the p-EnKF is nearly constant and close to the one of the KF for both considered scenario, as seen in Figures <ref type="figure" target="#fig_11">4e</ref> and <ref type="figure" target="#fig_11">4f</ref>. Conversely, the SqrtEnKF and StEnKF both display large Mahalanobis distances, with the one of the StEnKF even diverging in the partially-observed case; this is due to these algorithms having a larger RMSE than the KF (Figures <ref type="figure" target="#fig_11">4c</ref> and <ref type="figure" target="#fig_11">4d</ref>) but a smaller variance (Figures <ref type="figure" target="#fig_11">4c</ref> and <ref type="figure" target="#fig_11">4d</ref>).</p><p>In conclusion, the p-EnKF adopts a nearly optimal behaviour in this linear scenario despite partial observability and localisation. In contrast, the standard versions of the EnKF depart significantly from the behaviour of the KF and tend to be overly optimistic even in the considered small-dimensional inference problems, with well known adverse consequences for downstream tasks for which a reliable quantification of the uncertainty can be crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modified Lorenz 96 type model</head><p>In order to show that the strong performance of the p-EnKF observed in the previous section generalises beyond the linear case, we now consider a modified Lorenz 96 (LR96) model, which can be written as a state-space model (1), with k ∈ {1, . . . , 100}, with the following components:</p><p>1. The initial state X 0 is sampled from N(0 n , 10I n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The deterministic part of the dynamic model is characterised by</head><formula xml:id="formula_39">x = F k (x ′ ) with x 1 = x ′ 1 + ((x ′ 2 -c)c -x ′ 1 + F ) ∆t x 2 = x ′ 2 + ((x ′ 3 -c)x ′ 1 -x ′ 2 + F ) ∆t x i = x ′ i + (x ′ i+1 -x ′ i-2 )x ′ i-1 -x ′ i + F ∆t for 3 ≤ i ≤ n -1 x n = x ′ n + (c -x ′ n-2 )x ′ n-1 -x ′ n + F ∆t</formula><p>where "x i " refers to the i-th component of x and where F = 8, c = 1, and ∆t = 0.01.  Overall, the only difference with the linear model is the transition function F k . Throughout this section, the ensemble size will be set to 2n + 1, that is N = 2n, unless specified otherwise. The LR96 model is particularly convenient for performance assessment since the dimension can be easily adjusted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The covariance matrix of the dynamical noise ϵ</head><formula xml:id="formula_40">k is U k = 0.01I n . 4. The observation model is linear, H k (X k ) = H k X k ,</formula><p>As before, we start by investigating the performance of the p-EnKF with the full precision matrix based on different sample sizes and dimensions such that all elements of the state are observed (n = m) compared to the SqrtEnKF and the UKF. However, for the nonlinear model, we only examine the performance in terms of RMSE w.r.t. the true state at the last step since we can no longer obtain the optimal state estimate from the KF. The RMSE displayed in Figure <ref type="figure" target="#fig_13">5</ref> is averaged over 100 repeats, except for n = 64 where we only consider 50 realisations due to the computational cost. The results shown in Figure <ref type="figure" target="#fig_13">5</ref> are very close to the ones obtained in the linear case, with all three algorithms maintaining a similar level of performance despite the non-linearities.  Because of the similarities between the results with the linear model and the LR96 model, we only highlight where noticeable differences arise. Figure <ref type="figure" target="#fig_15">6a</ref> shows that, in the partially-observed case with m = 1 and n = 5, the SqrtEnKF has a RMSE that is now closer to the StEnKF than to the p-EnKF. Localisation in the p-EnKF still has a very mild effect, although the error increases around time step 70. This increase in error is however captured by the associated covariance matrix, so that the Mahalanobis distance remains constant throughout the scenario, as required. This behaviour suggests that the correlation increased around time step 70, forcing the p-EnKF with banded matrix to increase the amount of inflation to compensate for the loss of information caused by the imposed conditional independence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced the possibilistic ensemble Kalman filter, or p-EnKF, a data assimilation technique treating the state of a state-space model as a fixed quantity about which limited information is available. By using possibility theory to model this form of epistemic uncertainty, we found that much of the intuition behind the standard versions of the EnKF remains valid, with the differences between the theories of possibility and probability leading to key features in the p-EnKF. Specifically, the properties of the expected value and variance in possibility theory appeared to be beneficial for inference problems of small to moderate dimensions, with the p-EnKF closely approximating the Kalman filter in the linear-Gaussian case. These properties also allowed for localisation to be seamlessly applied with no parameter tuning required to compensate for the loss of information incurred by the imposed conditional independence.</p><p>In the current version of the p-EnKF, the computation of covariance matrices relies on solving a constrained optimisation problem, which is time-consuming and requires an ensemble size greater than the dimension of the state. Although beyond the scope of this work, lifting these constraints appears to be feasible through the use of specialised optimisation techniques and suitable regularisation.</p><p>There are also several possible avenues for further investigation. We highlight two directions that we think are immediately interesting: although we have found the algorithms performance to be robust to the specification of the initial ensemble, there is a potential for developing systematic approaches to specifying it which might lead to further improved performance with small ensembles; and, an open question is how can the update step be reformulated to operate directly in terms of the precision matrix to avoid the need for matrix inversion, which would be required to facilitate the use of the p-EnKF in high-dimension.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Computational time of sample variance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Analysis of the proposed procedure for Gaussian fitting when the underlying possibility function / probability distribution is Gaussian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 3 1 :</head><label>31</label><figDesc>Initial step of the p-EnKF Input: The prior Gaussian possibility function N(µ 0 , Σ 0 ) Output: The initial ensemble {(w i , xi 0 )} N i=0 for i ∈ {1, . . . , N } do 2:xi 0 ∼ N(µ 0 , Σ 0 ) \\Draw a particle 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>k 5 : 6 : 1 k 7 :</head><label>5617</label><figDesc>Λk ← Λ * {(w i , xi k-1 )} i \\Compute the precision matrix Σk ← Λfor i ∈ {1, . . . , N } do 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Average RMSE w.r.t. the posterior mean of the KF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Average RMSE w.r.t. the true state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Average RMSE w.r.t. the posterior variance of the KF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance assessment for the fully-observed linear model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>Average RMSE w.r.t. the true state for n = 3 Average RMSE w.r.t. the true state for n = 5 with UKF ini (c) Average RMSE with the interquartile range for n = 3 with UKF ini (d) Average RMSE with the interquartile range for n = 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average RMSE and the error range of different algorithms in the linear model. Left: state's dimension of 3; Right: state's dimension of 5, averaged over 1000 repeats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc>with H k = I m 0 m×(n-m) and the covariance matrix of the observation noise ε k is V k = 0.1I m .Mahalanobis distance between the estimate and the true state</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance assessment for the linear model with n = 5 when (left) fully observed, m = 5 and (right) partially observed, m = 1, averaged over 1000 repeats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of p-EnKF in terms of RMSE w.r.t. the true state, compared with the SqrtEnKF and the UKF for the LR96 model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>Mahalanobis distance between the estimate and the true state</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance for a partially-observed LR96 model (m = 1) when n = 5, averaged over 1000 repeats.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Although p(• | θ) is not formally a conditional probability distribution, it is useful to slightly abuse notations and write it as such.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>AMJ acknowledges the financial support of the <rs type="funder">United Kingdom Engineering and Physical Sciences Research Council</rs> (EPSRC; grants <rs type="grantNumber">EP/R034710/1</rs> and <rs type="grantNumber">EP/T004134/1</rs>) and by <rs type="funder">United Kingdom Research and Innovation (UKRI)</rs> via grant number <rs type="grantNumber">EP/Y014650/1</rs>, as part of the <rs type="funder">ERC</rs> <rs type="projectName">Synergy</rs> project <rs type="projectName">OCEAN</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zVNaG4x">
					<idno type="grant-number">EP/R034710/1</idno>
				</org>
				<org type="funding" xml:id="_DK8TSBE">
					<idno type="grant-number">EP/T004134/1</idno>
				</org>
				<org type="funding" xml:id="_5DGQQHN">
					<idno type="grant-number">EP/Y014650/1</idno>
				</org>
				<org type="funded-project" xml:id="_QKUUABS">
					<orgName type="project" subtype="full">Synergy</orgName>
				</org>
				<org type="funded-project" xml:id="_r7kcARa">
					<orgName type="project" subtype="full">OCEAN</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Rossella</forename><surname>Arcucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lamya</forename><surname>Moutiq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ke</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">Neural assimilation. In Computational Science-ICCS 2020: 20th International Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">June 3-5, 2020. 2020</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 20</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data assimilation: methods, algorithms, applications</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Asch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Bocquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maëlle</forename><surname>Nodet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Representing parametric probabilistic models tainted with imprecision. Fuzzy sets and systems</title>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Baudrit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Perrot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="1913" to="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust Kalman and Bayesian set-valued filtering and model validation for linear stochastic systems</title>
		<author>
			<persName><forename type="first">Adrian N</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moral</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM/ASA Journal on Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="425" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analysis scheme in the ensemble Kalman filter</title>
		<author>
			<persName><forename type="first">Gerrit</forename><surname>Burgers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geir</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><surname>Evensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1719" to="1724" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Observer control for bearings-only tracking using possibility functions</title>
		<author>
			<persName><forename type="first">Zhijin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branko</forename><surname>Ristic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Du</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing the bandwidth of sparse symmetric matrices</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Cuthill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1969 24th National Conference</title>
		<meeting>the 1969 24th National Conference</meeting>
		<imprint>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="157" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditioning in possibility theory with strict order norms</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>De Baets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Tsiporkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radko</forename><surname>Mesiar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Dempster-Shafer calculus for statisticians</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><surname>Dempster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Possibility theory, probability and fuzzy sets misunderstandings, bridges and gaps: Misunderstandings, bridges and gaps</title>
		<author>
			<persName><forename type="first">Didier</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Prade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of fuzzy sets</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="343" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Possibility theory and its applications: Where do we stand?</title>
		<author>
			<persName><forename type="first">Didier</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Prade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer Handbook of Computational Intelligence</publisher>
			<biblScope unit="page" from="31" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Particle filters and data assimilation</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fearnhead</surname></persName>
		</author>
		<author>
			<persName><surname>Hans R Künsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="421" to="449" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The fiducial argument in statistical inference</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Eugenics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="398" />
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ensemble Kalman filtering and generalizations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Frei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ETH Zurich</publisher>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A quasi-Monte Carlo method for optimal control under uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vesa</forename><surname>Guth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frances</forename><forename type="middle">Y</forename><surname>Kaarnioja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Schillings</surname></persName>
		</author>
		<author>
			<persName><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM/ASA Journal on Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="383" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized fiducial inference: A review and new results</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hannig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy Cs</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas Cm</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">515</biblScope>
			<biblScope unit="page" from="1346" to="1361" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Parameter estimation with a class of outer probability measures</title>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00569</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A linear algorithm for multi-target tracking in the context of possibility theory</title>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="2740" to="2751" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smoothing and filtering with a class of outer measures</title>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">N</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM/ASA Journal on Uncertainty Quantification</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="845" to="866" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Houssineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">K</forename><surname>Chada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Delande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04331</idno>
		<title level="m">Elements of asymptotic theory with outer probability measures</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On domain localization in ensemble-based Kalman filter algorithms</title>
		<author>
			<persName><forename type="first">Tijana</forename><surname>Janjić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Nerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberta</forename><surname>Albertella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Schröter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Skachko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2046" to="2060" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new approach to linear filtering and prediction problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kalman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Atmospheric modeling, data assimilation and predictability</title>
		<author>
			<persName><forename type="first">Eugenia</forename><surname>Kalnay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding the ensemble Kalman filter</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Katzfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K</forename><surname>Wikle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="350" to="357" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ensemble Kalman filtering with an alternative representation of uncertainty</title>
		<author>
			<persName><forename type="first">Chatchuea</forename><surname>Kimchaiwong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
		<respStmt>
			<orgName>University of Warwick</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Monte Carlo and quasi-Monte Carlo sampling</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Lemieux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reasoning with belief functions: An analysis of compatibility</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="363" to="389" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><surname>Ruth E Petrie</surname></persName>
		</author>
		<title level="m">Localization in the ensemble Kalman filter. MSc Atmosphere, Ocean and Climate University of Reading</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic forecasting and Bayesian data assimilation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cotter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bayesian filtering and smoothing</title>
		<author>
			<persName><forename type="first">Simo</forename><surname>Särkkä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An optimal transport formulation of the ensemble Kalman filter</title>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Taghvaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Prashant</surname></persName>
		</author>
		<author>
			<persName><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3052" to="3067" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear measurement function in the ensemble Kalman filter</title>
		<author>
			<persName><forename type="first">Youmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaison</forename><surname>Ambandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dake</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Atmospheric Sciences</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="558" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Extended Kalman filter tutorial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><surname>Terejanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>University at Buffalo</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ensemble square root filters</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Michael K Tippett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">H</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Hamill</surname></persName>
		</author>
		<author>
			<persName><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly weather review</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1485" to="1490" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Nonlinear data assimilation for highdimensional systems: -with geophysical applications</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Reich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The unscented Kalman filter for nonlinear estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolph</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Merwe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No. 00EX373)</title>
		<meeting>the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No. 00EX373)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ensemble data assimilation without perturbed observations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><surname>Hamill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Weather Review</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1913" to="1924" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
