# Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty

## Abstract

## 

The problem of incorporating information from observations received serially in time is widespread in the field of uncertainty quantification. Within a probabilistic framework, such problems can be addressed using standard filtering techniques. However, in many real-world problems, some (or all) of the uncertainty is epistemic, arising from a lack of knowledge, and is difficult to model probabilistically. This paper introduces a possibilistic ensemble Kalman filter designed for this setting and characterizes some of its properties. Using possibility theory to describe epistemic uncertainty is appealing from a philosophical perspective, and it is easy to justify certain heuristics often employed in standard ensemble Kalman filters as principled approaches to capturing uncertainty within it. The possibilistic approach motivates a robust mechanism for characterizing uncertainty which shows good performance with small sample sizes, and can outperform standard ensemble Kalman filters at given sample size, even when dealing with genuinely aleatoric uncertainty.

## Introduction

Dynamical real-world systems are typically represented as a hidden process in some state space, indirectly observed via a measurement process. The usual approach to characterizing the unknown state and uncertainty about it is to combine the observed information with given prior information in an approach known as filtering, or sometimes data assimilation [[29]](#b28). However, this is non-trivial due not only to imprecise measurements, but also to model misspecification [[31]](#b30).

Perhaps the best-known data-assimilation algorithm is the Kalman filter (KF) [[22]](#b21), which is the optimal filter for the linear Gaussian state-space model, as detailed, e.g., in [[31]](#b30). However, its application is limited by its high computational costs when dealing with large state spaces, and by the fact that it cannot be applied directly to nonlinear models [[28]](#b27). Several modifications of the KF have been developed to extend its scope. The extended Kalman Filter can deal with a mild level of nonlinearity by approximating the model with a first order Taylor series expansion, as described, e.g., in [[34]](#b33). The unscented Kalman Filter (UKF) approximates the distribution with Gaussian distribution using a set of sigma points and their weights, often giving better accuracy for highly nonlinear models [[37]](#b36). Still, the accuracy of the UKF depends upon the tuning of certain algorithmic parameters which determine the locations of the sigma points. Moreover, the number of sigma points (and hence the computational cost and accuracy) is fixed. Another widely used algorithm is the ensemble Kalman filter (EnKF) [[5,](#b4)[35]](#b34), which uses an ensemble to approximate the distributions of interest. The strengths of the EnKF are that it does not require the calculation of the variance (avoiding costly matrix inversions in high-dimensional problems) and can be used with a nonlinear model without approximating it with its linear tangent model [[28]](#b27) as one simply needs to be able to sample from the transition kernel of the dynamic model-although this comes at the cost of further approximating a non-Gaussian distribution with a Gaussian one. The EnKF must be used carefully with high-dimensional problems as constraints on computing power typically lead to an ensemble size relatively small compared to the dimension of the state space, leading to problems such as spurious correlation and underestimated variance, affecting the algorithm's performance. Thus, the recent development in this field mainly focuses on reducing computational cost and increasing the forecast's accuracy.

Although there has been much recent development in the field of data assimilation, e.g., [[1,](#b0)[2,](#b1)[23]](#b22), the opportunities offered by alternative representations of the uncertainty have not been fully explored. Yet, there is a clear motivation for doing so: In many situations in which the EnKF is used, the main sources of uncertainty are i) the initial state of the system, ii) unknown deviations in the model, e.g., unknown forcing terms, and iii) the lack of data. These sources of uncertainty can be argued to be of an epistemic nature and, given their predominance, it is important to model them as faithfully as possible. This makes appealing the use of a dedicated framework for epistemic uncertainty. Many different approaches have been suggested in the literature to deal with epistemic uncertainty, such as fiducial inference [[13,](#b12)[16]](#b15) and Dempster-Shafer theory [[9]](#b8). While fiducial inference gives a probabilistic statement about a parameter without relying on a prior probability distribution, it still represents the information a posteriori via probability distributions. On the other hand, Dempster-Shafer theory leverages (fuzzy) set-valued probabilities to represent both aleatoric and epistemic uncertainty [[27]](#b26). Yet, its formulation, based on sums over all subsets of the state space, restricts its application to discrete cases and, even then, does not scale easily to larger problems. Another approach, termed possibility theory [[8,](#b7)[11,](#b10)[10]](#b9), proposes to model exclusively epistemic uncertainty by changing the algebra underpinning probability theory, i.e., summation/integration is replaced by maximisation. Many probabilistic concepts and analyses can be adapted to possibility theory, for instance, [[20]](#b19) proves a Bernsteinvon Mises theorem for a possibilistic version of Bayesian inference. In real systems, some uncertainty will be well modelled as aleatoric and some will of an epistemic nature; [[17]](#b16) shows that probability theory and possibility theory can be combined in this case, offering an extension of the probabilistic approach rather than an alternative. Possibility theory can be applied to complex problems, for instance [[18]](#b17) introduces an analogue to spatial point processes, and [[19]](#b18) shows that the Kalman filter can also be derived in this framework. These results highlight the potential of possibility theory for data assimilation, which we explore in this work via a possibilistic analogue of the EnKF. Related work includes [[4]](#b3) in which a robust version of the Kalman filter is proposed based on sets of probability distributions, with a motivation close to the one for this work.

Our contributions are as follows:

1. We introduce a new notion of Gaussian fitting for possibility theory, and contrast it against the standard moment-matching procedure. We highlight the potential of this approach for providing principle grounds for standard heuristics used in the EnKF.

2. We derive a possibilistic analogue of the EnKF, adapting the initialisation and predictions steps to the assumed epistemic uncertainty in the model, and showing how the update step of existing versions of the EnKF remain valid in this context.

3. We assess the performance of our method against two versions of the EnKF and against the UKF, considering both linear and nonlinear dynamics as well as fully and partially observed processes.

The remainder of this paper is organised as follows. Section 2 starts with the state estimation problem for the state-space model before giving the filtering techniques used to estimate such states under the probabilistic framework. Then, the details of the possibilistic framework developed to tackle epistemic uncertainty are given, with some concepts defined analogously to the probabilistic framework. Next, the possibilistic EnKF is introduced in Section 3 to estimate the system's state in the presence of epistemic uncertainty. After that, the performance of our algorithm is assessed on simulated data in a range of situations and compared against standard baselines in Section 4.

## Background

We first briefly review the standard EnKF and the underlying filtering problem in Section 2.1 before introducing, in Section 2.2, the framework based on which the proposed method will be derived.

## State estimation problem and filtering technique

The standard, probabilistic Gaussian state-space model describes the evolution of a hidden state process, (X k ) k>0 , and its associated observation process, (Y k ) k>0 , via

$X k = F k (X k-1 ) + ϵ k where ϵ k ∼ N(0, U k ) (1a) Y k = H k (X k ) + ε k where ε k ∼ N(0, V k ),(1b)$with U k and V k positive definite matrices and with N(µ, Σ) denoting a Gaussian distribution of mean µ and covariance Σ. We consider the setting in which a stream of observations y 1 , y 2 , . . . arrives sequentially in time, and we want to obtain estimates of the hidden states as they arrive. Thus, in the Bayesian context, we characterise our knowledge of the state X k at time step k > 0 given the realisations y 1:k . = (y 1 , y 2 , . . . , y k ) of the observations Y 1:k via the filtering density p(x k |y 1:k ) which can be computed recursively, see, e.g., [[31]](#b30), as

$p(x k |y 1:k-1 ) = p(x k |x k-1 )p(x k-1 |y 1:k-1 )dx k-1 (prediction) p(x k |y 1:k ) = p(y k |x k )p(x k |y 1:k-1 ) p(y k |x k )p(x k |y 1:k-1 )dx k .(update)$The Kalman Filter (KF) provides a closed-form recursion for the prediction and update steps for linear Gaussian state-space models (see, e.g., [[31]](#b30)). However, many models in this world are nonlinear, meaning that the KF is not directly applicable and, unfortunately, there are few other settings in which analytic recursions are available. This motivates the development of numerous alternatives, including the UKF and the EnKF.

While the UKF can provide good performance for moderately nonlinear models, which depends upon careful choice of several algorithmic tuning parameters, and, as the number of sigma points is fixed, there is little scope for adjusting computational cost or accuracy. The EnKF algorithm takes a different approach; it can be viewed as an approximation to the KF which employs an ensemble of samples {X i k } N i=1 to approximate the parameters of the Gaussian distribution at time k. In particular, the ensemble's mean and variance are used to approximate those of the filtering distribution. The EnKF algorithm then proceeds recursively via the prediction and update steps. The prediction step mirrors that of the KF but uses an ensemble approximation, which means that F k need not be linear as long as it can be evaluated pointwise as in Algorithm 1 [[5,](#b4)[35]](#b34).

## Algorithm 1 Prediction step of EnKF at time step k

$Input: The posterior ensemble { Xi k-1 } N i=1 at time step k -1. Output: The predictive ensemble {X i k } N i=1 at time step k with mean µ k and variance Σ k . 1: for i ∈ {1, . . . , N } do 2: ϵ i k ∼ N(0, U k ) \\Sample noise 3: X i k = F k ( Xi k-1 ) + ϵ i k \\Predict i-th particle 4: end for 5: µ k ← 1 N N i=1 X i k \\Predictive mean 6: Σ k ← 1 N -1 N i=1 (X i k -µ k )(X i k -µ k ) ⊤ , \\Predictivevariance$The update step of the EnKF also approximates that of the KF: the mean of the updated distribution is a convex combination of that of the predictive distribution and the observation using the so-called Kalman gain as a weight. However, using the traditional Kalman gain can lead to an underestimation in the posterior variance (as it essentially neglects the observation noise) and can lead to filter divergence [[38]](#b37). A number of variants of the EnKF update step have been developed to address this problem, including both the stochastic EnKF (StEnKF) and the square root EnKF (SqrtEnKF) [[24]](#b23). The latter updates the deviation of the ensemble by applying a matrix obtained from the square root of a certain matrix. Different variations of the SqrtEnKF exist, depending on how such a matrix is obtained, as summarised in [[35]](#b34). In this work, we consider the version of the SqrtEnKF presented in [[38]](#b37), which is particularly suitable as a basis for introducing a possibilistic version of the EnKF. The update of the SqrtEnKF for a linear observation model is detailed in Algorithm 2, where the notation A 1 /2 refers to the Cholesky factor of matrix A, and where we use the same notation for a linear function and for its matrix representation.

Algorithm 2 Update step of SqrtEnKF at time step k Input: The predictive ensemble {X i k } N i=1 at time step k with mean µ k and variance Σ k . Output: The posterior ensemble { Xi k } N i=1 at time step k. 1:

$S k ← H k Σ k H ⊤ k + V k \\Covariance of the innovation from the ensemble 2: K k ← Σ k H ⊤ k S -1 k \\Standard Kalman gain 3: Kk ← Σ k H ⊤ k S 1 /2 k -⊤ S 1 /2 k + V 1 /2 k -1 \\Adjusted Kalman gain 4: μk ← µ k + K k (Y k -H k µ k ) \\Posterior mean 5: for i ∈ {1, . . . , N } do 6: êi k ← (I n -Kk H k )(X i k -µ k ) \\Posterior deviation 7:$Xi k ← êi k + μk \\Updated particle 8: end for For the ensemble-based method, the accuracy of the state estimate depends highly on the ensemble size. The situation in which the ensemble is too small to adequately represent the system is termed undersampling and leads to four main problems which have been termed: inbreeding, spurious correlation, matrix singularity, and filter divergence [[28]](#b27). Two widely used techniques to negate the undersampling problems are inflation and localisation [[28]](#b27). The inflation technique increases the deviation between the sample and the predictive mean so that the predictive covariance from the ensemble is not underestimated and avoids the so-called inbreeding problem. On the other hand, the localisation techniques are implemented to eradicate the spurious correlation and singular matrix, examples being tapering using the Schur product to cut off the long-range correlation or domain localisation where assimilation is performed in a local domain, which is disjoint in a physical space [[21]](#b20). Each technique needs to be fine-tuned in a problem-specific manner to achieve good accuracy.

We have focused here on approaches directly relevant to the methodology developed below in Section 3; many other methods have been devised to address nonlinearity and non-Gaussianity-see, e.g., [[2,](#b1)[12,](#b11)[36]](#b35).

## Review of possibility theory

Possibility theory aims to directly capture epistemic uncertainty about a fixed but unknown element θ * in a set Θ, by focusing on the possibility of an event related to θ * rather than on its probability. A possibility of 1 corresponds to the absence of evidence against the event taking place not evidence that the event took place almost surely. If there is no evidence against an event, say E, then there cannot be any evidence against (E or E ′ ), with E ′ any other event, so that the possibility of (E or E ′ ) is also equal to 1. This behaviour shows the notion of possibility does not give an additive measure of uncertainty, and the simplest operation that corresponds to the possibility of a union of events is the maximum of their individual possibilities.

The analogue of a probability density is a so-called possibility function, f : Θ → [0, 1] which must have supremum 1. Just as probabilities are best described by measures, possibilities are naturally cast as outer measures, P and just as one may obtain a probability from a density one can obtain a possibility from a possibility function by taking its supremum, that is, for any A ⊆ Θ, P (A) = sup θ∈A f (θ). If it holds that f (θ) = 1 for some θ ∈ A, then we will indeed have P (A) = P (A ∪ B) = 1 for any B ⊆ Θ, as required.

Formally, the set function P is an outer measure verifying P (Θ) = 1; we therefore refer to such set functions as outer probability measures (o.p.m.s). If P (A) = 1 models the absence of opposing evidence, P (A) = 0 has essentially the same interpretation as with probabilities, i.e., that θ * ∈ A is impossible, which requires very strong evidence. In general, P (A) can be interpreted as an upper bound for subjective probabilities of the event θ * ∈ A, i.e., the maximum probability that we would be ready to assign to this event is P (A). Under this interpretation, the statement P (A) = 1 is uninformative: we could assign any probability in the interval [0, 1] to this event.

To formally define the quantities described above, we consider an analogue of a random variable, referred to as an uncertain variable, defined as a mapping θ : Ω → Θ, and interpreted as follows: if the true outcome in Ω is ω then θ(ω) is the true value of the parameter. The set Ω plays a similar role to the elementary event space in probability theory, except that it is not equipped with any probabilistic structure; instead, it contains a true element ω * and it holds by construction that θ(ω * ) = θ * . An event can now be defined as a subset of Ω, for instance {ω ∈ Ω : θ(ω) ∈ A} for some A ⊆ Θ. We will use the standard shortcut and write this event as θ ∈ A. An important difference between possibility theory and probability theory is that uncertain variables do not characterise possibility functions, instead possibility functions describe uncertain variables in a way that is not unique. For instance, if the available information is modelled by a possibility function f describing an uncertain variable θ, then any function g such that g ≥ f , i.e. g(θ) ≥ f (θ) for any θ ∈ Θ, also describes θ. This is because g discarded some evidence about θ, that is, g is less informative than f . In particular, we can consider the possibility function equal to 1 everywhere, denoted 1, which upper bounds all other possibility functions. The function 1 models the total absence of information.

To gain information from data, Bayesian inference can be performed within possibility theory in a way that is very similar to the standard approach: If Y is a random variable with probability distribution p(• | θ * ) belonging to a parameterised family of distributions[foot_0](#foot_0) {p(• | θ) : θ ∈ Θ}, if we observe a realisation y of Y , and if the information available about θ * a priori is modelled by the possibility function f , then the information available a posteriori can be modelled by the possibility function f (• | y) characterised by [[17]](#b16)

$f (θ | y) = p(y | θ)f (θ) sup θ ′ ∈Θ p(y | θ ′ )f (θ ′ ) , θ ∈ Θ.(2)$We borrow from the Bayesian nomenclature and simply refer to f and f (• | y) as the prior and the posterior. Since we can always start from the uninformative prior f = 1, it is easy to find posterior possibility functions by inserting different likelihoods; for instance, if Θ = R n and if the likelihood is a multivariate Gaussian distribution with mean θ and known variance Σ, i.e., p(y | θ) = N(y; θ, Σ), then

$f (θ | y) = exp - 1 2 (θ -y) ⊤ Σ -1 (θ -y) . = N(θ; y, Σ).$Such a Gaussian possibility function is a conjugate prior for the Gaussian likelihood as in the probabilistic case, and shares many of the properties of its probabilistic analogue. It can be advantageous to parameterise a Gaussian possibility function, say N(θ; µ, Σ), by the precision matrix Λ = Σ -1 ; indeed, the precision matrix does not need to be positive definite for the Gaussian possibility function to be well defined, with positive semi-definiteness being sufficient. In particular, this means that setting Λ to the 0 matrix is possible, with the Gaussian possibility function being equal to 1 in this case. A simple way to quantify the amount of epistemic uncertainty in a possibility function is to consider the integral f (θ)dθ as in [[6]](#b5), when defined. This notion of uncertainty is consistent with the partial order on possibility functions: if g is less informative than f then the integral of g will obviously be larger then that of f . In particular, the uncertainty of a Gaussian possibility function is |2πΣ|, with | • | denoting the determinant; a quantity often used to quantify how informative a given Gaussian distribution is. If θ and ψ are two uncertain variables, respectively on sets Θ and Ψ, jointly described by a possibility function f on Θ × Ψ, then Bayes' rule can also be expressed via a more standard notion of conditioning as [[8]](#b7)

$f (θ | ψ) = f (θ, ψ) sup θ ′ ∈Θ f (θ ′ , ψ) ,$where sup θ ′ ∈Θ f (θ ′ , ψ) is the marginal possibility function describing ψ. The conditional possibility function f (θ | ψ) allows independence to be defined simply:

$if f (θ | ψ) is equal to the marginal f (θ) = sup ψ ′ ∈Ψ f (θ, ψ ′ )$for any θ ∈ Θ, then θ is said to be independent from ψ under f . As with most concepts in possibility theory, the notion of independence depends on the choice of possibility function; if θ and ψ are not independent under f , we could find another possibility function g such that g ≥ f and such that θ is independent of ψ under g. This will be key later on for simplifying high-dimensional Gaussian possibility functions in a principled way. When there is no ambiguity about the underlying possibility function, we will simply say that θ is independent of ψ.

Although the parameters µ and Σ of the Gaussian possibility functions are reminiscent of the probabilistic notions of expected value and variance, these notions have to be redefined in the context of possibility theory. Asymptotic considerations [[20]](#b19) lead to a notion of expected value E * (•) based on the mode, that is E * (θ) = arg max θ∈Θ f (θ), for any uncertain variable θ described by f , and to a local notion of variance based on the curvature of f at E * (θ), assuming that the latter is a singleton, an assumption that we will make throughout this work. These quantities correspond to the approximation often referred to as Laplace/Gaussian approximation, and make sense in the context of epistemic uncertainty: without a notion of variability, we simply look at our best guess E * (θ) for the unknown θ * and at how confident we are in that guess, i.e., how fast the possibility decreases around it. Although the expected value and variance associated with the Gaussian possibility function N(µ, Σ) are indeed µ and Σ, they will differ in general from their probabilistic counterparts.

In order to make inference for dynamical systems simpler, we introduce a Markovian structure as follows: We consider a sequence of uncertain variables x 0 , x 1 , . . . in a set X and assume that x k is described by a possibility function f k for some given k ≥ 0. Following the standard approach, we assume that x k+1 is conditionally independent of x k-δ given x k , for any δ > 0. The available information about x k+1 given a value x k of x k can then be modelled by a conditional possibility function f k+1|k (• | x k ), and prediction can be performed via

$f k+1 (x k+1 ) = sup x k ∈X f k+1|k (x k+1 | x k )f k (x k ), x k+1 ∈ X.$We will focus in particular on the case where an analogue of (1a) holds, that is

$x k = F k (x k-1 ) + u k ,$with u k an uncertain variable described by N(0, U k ). In this case, the term u k cannot be interpreted as noise and models instead deviations between the model and the true dynamics: the true states

$x * k and x * k-1 are related via x * k = Fk (x * k-1 ) with Fk potentially different from F k ; the true value u * k of interest is therefore equal to the difference Fk (x * k-1 ) -F k (x * k-1$) between the model and the true dynamics. In the case where the prediction is deterministic, that is x k+1 = F k (x k ) for some possibly non-linear mapping F k , we can use the change of variable formula [[3]](#b2) to characterise the possibility function f k+1 as

$f k+1 (x k+1 ) = sup f k (x k ) : x k ∈ F -1 k (x k+1 ) ,(3)$where

$F -1 k (x k+1$) is the (possibly set-valued) pre-image of x k+1 via F k , and sup ∅ = 0 by convention. A result that is specific to possibility theory is that the expected values of x k+1 and x k are related via E * (x k+1 ) = F k (E * (x k )) without assumptions on F k . This result will be key in our approach since it allows to compute the expected value at time step k + 1 with a single application of F k , rather than averaging ensembles as in the EnKF. This result does not hold for the mode of probability densities because the corresponding change of variable formula include the Jacobian of the transformation, which shifts the mode in non-trivial ways.

## Possibilistic EnKF

It is has been shown in [[19]](#b18) that an analogue of the Kalman filter can be derived in the context of possibility theory, and that the corresponding expected value and variance are the same as in the probabilistic case. However, as with the probabilistic KF, its applicability is limited to linear models-so it is natural to develop extensions that accommodate nonlinearity. Since the probabilistic EnKF is flexible in computational cost and can be effectively applied to nonlinear models, we explore the use of EnKF-like ideas in the possibilistic setting. Here, we present a novel possibilistic EnKF (p-EnKF) and show that analogues of inflation and localisation arise naturally in the context of possibility theory rather than being imposed as heuristics to improve performance as in the standard setting.

For the p-EnKF, we use an ensemble of weighted particles to characterise the possibility function and follow a similar path to the standard EnKF by assuming that the underlying possibility function is Gaussian in order to proceed with a KF-like update. For this purpose, we need to define two operations: 1) how to approximate a given possibility function by weighted particles? This will be necessary for initialisation at time step k = 0, and 2) how to define a Gaussian possibility function based on weighted particles? This will be necessary to carry out the Kalman-like update. We start by answering the latter question in Section 3.2, before moving to the former in Section 3.3. Based on this, we detail the prediction and update mechanism of the p-EnKF in Sections 3.4 and 3.5 respectively, and consider some extensions in Section 3.6.

## Ensemble approximation

We consider the problem of defining a set {(w i , x i )} N i=1 of N weighted particles in R n , which will allow the approximate solution of optimisation problems of the form max x∈R n φ(x)f (x), for some bounded function φ on R n and for a given possibility function on R n . Although our approach could be easily formulated on more general spaces than R n , Euclidean spaces are sufficient for our purpose in this work and help to simplify the presentation. As is common in Monte Carlo approaches, we do not want the particles or weights to depend on φ, so as to allow us to solve this problem for many different such functions using a single sample, we therefore focus on directly approximating f : placing particles in locations which allow a good characterization of f allows us to approximate the optimization for a broad class of regular φ. Following the principles of Monte Carlo optimization [[30,](#b29)[Chapter 5]](#), and assuming that f (x)dx < +∞, we can sample from the probability distribution proportional to f to obtain the particles {x i } N i=1 and then weight these particles with w i = f (x i ), for any i ∈ {1, . . . , N }. We then obtain the approximation

$max x∈R n φ(x)f (x) ≈ max i∈{1,...,N } w i φ(x i ).$This approximation can be proved to converge when N → ∞ under mild regularity conditions on φ as long as the supports of φ and f are not disjoint. This is a simple default choice for the methods developed below, although more sophisticated approaches are possible.

Remark 1. One initially appealing idea is to consider a form of maximum entropy principle by identifying the constraints we want to impose on the probability distributions from which we may wish to sample, and to pick the distribution of maximum entropy subject to satisfying these constraints. This is a standard argument for specifying the "least informative" distribution with particular properties, e.g., given a distribution when the mean and variance the maximum entropy distribution is the Gaussian with these moments. In the considered context, the constraints comes from the interpretation of o.p.m.s as upper bounds for probability distributions. Specifically, for an o.p.m. P , we could draw an ensemble from the probability density p satisfying the following criteria: p has the maximum entropy amongst those distributions satisfying A p(x)dx ≤ P (A) for any A ⊆ R n . The distribution p would typically be more diffuse than the one proportional to f , which can be beneficial for optimizing functions φ taking large values in the "tails" of f . However, obtaining such a distribution is in general highly non-trivial beyond discrete or univariate problem.

As there are no constraints on the way in which particle locations, {x i } N i=1 are selected, deterministic schemes such as quasi Monte Carlo [[26,](#b25)[15]](#b14) could also be considered. In Section 4, we will consider using the same approach as in the UKF to define particle locations, which will prove to be beneficial in some situations.

In the situations that we will consider, it will always be the case that f is maximised at a single known element x * ∈ R n . Therefore, it makes sense to add one particle x 0 = x * with weight w 0 = 1. This will prove beneficial when fitting a Gaussian possibility function to the ensemble {(w i , x i )} N i=0 , as detailed in the next section.

## Best-fitting Gaussian possibility function

We consider the situation in which the epistemic uncertainty is captured by a set {(w i , x i )} N i=0 of N + 1 weighted particles in R n , with w i = 1 if and only if i = 0. We denote by f the "empirical" possibility function defined based on the ensemble as f (x) = max i∈{0,...,N } w i 1 xi (x), with 1 xi the indicator of the point x i . The standard approach would be to simply compute the (weighted) mean and variance of the ensemble, but these notions do not apply directly to possibility functions, and the curvature-based possibilistic notion of variance is not defined for an empirical possibility function like f . Instead, we aim to fit a Gaussian possibility function N(µ, Σ) to this (weighted) ensemble and the considered variance will simply be the second parameter Σ of this fitted Gaussian.

To fit the Gaussian N(µ, Σ) to the ensemble, we need a notion of best fit. Based on the partial order between possibility functions described in Section 2.2, we can easily ensure that N(µ, Σ) does not introduce artificial information-at least at locations {x i } N i=0 -by requiring that N(µ, Σ) ≥ f . Since f (x) = 0 when x / ∈ {x i } N i=0 , we can simplify this condition to: N(x i ; µ, Σ) ≥ w i for all i ∈ {0, . . . , N }. The inequality N(µ, Σ) ≥ f forces the expected values of N(µ, Σ) and f to coincide, from which we can deduce that µ = x 0 .

For the variance, we aim to minimise the uncertainty in the possibility function N(µ, Σ), i.e., minimising N(x; µ, Σ)dx ∝ |Σ|, which is equivalent to maximising log |Λ| with Λ = Σ -1 the precision matrix, thanks to the properties of the determinant. The precision matrix is then most naturally defined as the one solving the constrained optimisation problem max

$Λ∈S d + log |Λ| subject to N(x i ; µ, Σ) ≥ w i , 1 ≤ i ≤ N,(4)$where S d + is the cone of positive semi-definite d × d matrices. Since µ = x 0 , the corresponding constraint N(x 0 ; µ, Σ) ≥ w 0 is automatically satisfied and we only need to ensure our Gaussian possibility function upper bounds the ensemble at the other x i 's. Using the invariance of the trace to cyclic permutations allows these constraints to be rewritten as

$(x i -µ) ⊤ Λ(x i -µ) = Tr (C i Λ) ≤ -2 log w i , 1 ≤ i ≤ N,(5)$where

$C i = (x i -µ)(x i -µ) ⊤ ; this is more convenient for numerical optimization because Tr(C i Λ) is linear in Λ.$Remark 2. In the one-dimensional (n = 1) case, this optimisation problem can be easily solved for any N ≥ 1: the i th constraint in (5) can be expressed directly for the (scalar) variance as σ 2 . = Σ ≤ -2 log w i /(x i -µ) 2 , and (5) reduces to a single constraint:

$σ 2 ≤ min i∈{1,...,N } -2 log w i (x i -µ) 2 . (6$$)$In particular, setting σ 2 to the right hand side of this inequality will maximise the precision, hence solving the optimisation problem (4). In this case, if it happens that w i = f (x i ) with f a Gaussian possibility function, then the associated variance will be recovered exactly, even with N = 1. Although this result does not generalise easily to higher dimensions, it highlights the potential of this method for recovering the variance from an ensemble, as will be studied later in this section.

We will denote by Λ * {(w i , x i )} i the solution to (4), omitting the limits on i for concision. In addition to providing an estimate for the variance of the best-fitting Gaussian, this approach provides a measure of how far from Gaussian the ensemble is. Indeed, a notion of distance can be defined based on the gaps -Tr (C i Λ) -2 log w i , i ∈ {1, . . . , N }.

We will be interested in the relationship between the best Gaussian fit for a given ensemble and the one for a linear and invertible transformation of that ensemble, which motivates the following proposition.

Proposition 1. Let {(w i , x i )} i be an ensemble such that w i = 1 if and only if i = 0, and let M be an invertible linear map on R n , it holds that

$Λ * {(w i , x i )} i = M ⊤ Λ * {(w i , M x i )} i M.$Proof. The constraints (5) for the ensemble (w i , M x i ) can be rewritten as

$∀i ∈ {1, . . . , N } : (M x i -M x 0 ) ⊤ Λ(M x i -M x 0 ) = (x i -x 0 ) ⊤ M ⊤ ΛM (x i -x 0 ) ≤ -2 log w i .$Changing the variable of the optimisation problem from Λ to Λ = M ⊤ ΛM changes the objective function to log |M -⊤ ΛM -1 | = log | Λ| + constant. Therefore, the two optimisation problems are equivalent, and their solutions are related as stated.

From a practical viewpoint, the computational cost for calculating the ensemble's variance via ( [4](#formula_14)) is greater than that of the probabilistic framework due to the optimisation problem. Yet, Remark 2 hints at a potential for an efficient recovery of the true variance in the Gaussian case. This aspect is investigated in Figure [1a](#fig_1) which displays the average root mean squared error (RMSE) between the true variance and the ensemble's variance based on different sample sizes and for n ∈ {8, 16, 32}, averaged over 1000 repeats. The true n × n covariance matrix Σ is drawn from the inverse Wishart distribution with n 2 degrees of freedom and scale matrix nI n , with I n the identity matrix of dimension n × n, making its reconstruction appropriately challenging. Here, we use a sample x i ∼ N(0 n , Σ) for i = 1, . . . , N , with 0 n the zero vector of size n, to both estimate the probabilistic variance and to be used as the ensemble in (4) with weights w i = N(x i ; 0 n , Σ). The performance of each case is investigated from the minimum required sample size N = n up to N = 500. Figure [1a](#fig_1) shows that the covariance matrix obtained from (4) indeed converges to the true underlying variance faster than the probabilistic one for small dimensions. In particular, the associated RMSE appears to drop significantly around N = n 2 .  The computational cost of the probabilistic approach is very small for the considered range of sample sizes and remains around 0.1 ms even for n = 32 in our experiments, which is likely due to other operations dominating the computational cost in the considered settings. This is in contrast to the computational time of the proposed method, shown in Figure [1b](#fig_1), which has a clear linear trend and is orders of magnitude larger than in the probabilistic case.

One advantage of defining an ensemble's variance via (4) is that the knowledge of dependency-or lack thereof-between components of the states x ∈ R n can be more easily integrated. For instance, if we want to impose conditional independence between some components, i.e., to impose that Λ i,j = 0 for any indices i and j ̸ = i in a given set I, then we can add a set of constraints in the optimisation problem [(4)](#b3). Adding constraints will reduce the precision, i.e., it will yield a Gaussian possibility function that is less informative than the less-constrained problem. This behaviour can be interpreted as follows: in the possibilistic framework, conditional independence can be obtained by sacrificing some information. This is a well-understood trade-off in data assimilation, where forcing components to be uncorrelated, a method known as localisation, is usually accompanied by an increase of the variance of the remaining components, a process known as inflation. The main difference between the standard inflation and the proposed approach is that inflation usually comes with additional parameters that need to be fine-tuned for different situations, whereas the required amount of precision loss is automatically determined via the optimisation problem (4) when adding the constraints on conditional independence, with no additional tuning parameters.

Apart from defining the ensemble's expected value and variance, three steps of the algorithm need to be developed: initialisation, prediction, and update. These will be analogous to those of the probabilistic EnKF, but significant changes are required.

## Initialisation

We consider a sequence of uncertain variables x 0 , x 1 , . . . , with x k representing the state of the system at time step k. We assume that there is prior knowledge about x 0 , encoded into a possibility function N(µ 0 , Σ 0 ). This possibility function is approximated by an ensemble {(w i , xi 0 )} N i=0 of N + 1 weighted particles, defined as in Section 3.1. The time index, 0 in this case, is omitted for the weights as these will in fact remain constant throughout the algorithm, which relies exclusively on transports of the associated particles. The initialisation step is detailed in Algorithm 3. w i ← N(x i 0 ; µ 0 , Σ 0 ) \\Calculate the corresponding weight 4: end for 5: x0 0 ← µ 0 \\Add a particle deterministically at the mode 6: w 0 ← 1

## Prediction step

In the probabilistic framework, the standard way of obtaining the predictive ensemble at time step k is to i) apply the transition model F k to each particle, and ii) add a realisation of the transition noise. The first part of this process can be used as is, with some advantageous properties: a key result is the fact that

$E * (F k (x k-1 )) = F k (E * (x k-1$)) with no assumption on either F k or x k-1 . This allows to perform the prediction without recomputing the expected value, therefore stabilising the estimation through non-linear dynamics. In practice, this means that the particle with index 0 will always correspond to the expected value, hence its special treatment. In practice, given the ensemble {(w i , xi k-1 )} N i=0 at time step k -1, we can simply compute the image xi k = F k (x i k-1 ) of each particle to capture the information at time k in the absence of perturbations.

However, the second part of the standard approach is not appropriate in the possibilistic setting since we aim to model epistemic uncertainty which, in this context, corresponds to deviations between the model and the actual dynamics rather than real perturbations. Deterministic methods can however be adapted and we consider using transport maps to move the points of the ensemble as in [[32]](#b31). To construct such a map for our setting, we first characterize linear transformations of uncertain variables as follows.

Proposition 2. If x is an uncertain variable on R n described by N(µ, Σ) and z = Ax + b where A is an n × n invertible matrix and b is a constant vector of length n, then z is described by N(Aµ + b, AΣA ⊤ ).

Proof. Let f x denote the Gaussian possibility function N(µ, Σ). Applying (3), the possibility function of the uncertain variable z is

$f z (z) = sup{f x (x) : x ∈ R n , z = Ax + b} = sup{f x (x) : x = A -1 (z -b)} = f x (A -1 (z -b)) = N A -1 (z -b); µ, Σ = N z; Aµ + b, AΣA ⊤ .$According to Proposition 2, Gaussianity is preserved under linear and invertible transformations in the possibilistic framework. Furthermore, it is straightforward to obtain a map between two uncertain variables in R n described by Gaussian possibility functions: Proposition 3 (Mapping between Gaussian possibility functions). If x and z are two uncertain variables in R n described respectively by N(µ, Σ) and N(μ, Σ), then there exists a map M such that z = M (x), which is characterised by

$M (x) = μ + T (x -µ), x ∈ R n$where T = Σ1 /2 Σ 1 /2 -1 , with the notation A 1 /2 referring to the Cholesky factor of a matrix A.

Proof. First, we rewrite the mapping as follows

$M (x) = μ + T (x -µ) = T x + (μ -T µ)$As this is a linear and invertible transformation of x, Proposition 2 guarantees that z = M (x) is also described by a Gaussian possibility function, defined as

$f z (z) = N T µ + (μ -T µ), T ΣT ⊤ = N μ, Σ1 /2 Σ 1 /2 -1 Σ 1 /2 Σ 1 /2 ⊤ Σ 1 /2 -⊤ Σ1 /2 ⊤ = N μ, Σ .$Such a transport map can be used in the prediction step of the p-EnKF to add uncertainty from the transition model to the ensemble using deterministic mapping as follows: We fit a Gaussian possibility function N(µ k , Σk ) to the ensemble {(w i , xi k )} N i=0 , then, from the prediction of the possibilistic Kalman filter [[19]](#b18), we know that an additive Gaussian uncertainty of the form N(0 n , U k ) will yield a Gaussian predictive possibility function with expected value µ k and variance Σk + U k . Using Proposition 3, we can compute a map M k from N(µ k , Σk ) to N(µ k , Σk + U k ) and apply it to each particle xi k to obtain the predicted ensemble at time k. The prediction step of the p-EnKF is summarised in Algorithm 4. Although we assume the Gaussianity of the ensemble at a slightly earlier stage than the standard EnKF, U k is typically small compared to Σk so that the impact of this assumption is expected to be small. In addition, fitting a Gaussian possibility function to the predicted ensemble is unnecessary as the result is known to be N(µ k , Σ k ), with Σ k = Σk + U k , based on Proposition 1.

## Algorithm 4 Prediction step of the p-EnKF at time k

$Input: The time k -1 posterior ensemble, {(w i , xi k-1 )} N i=0 . Output: The time k predictive ensemble, {(w i , x i k )} N i=0$, expected value µ k , and variance Σ k 1: for i ∈ {1, . . . , N + 1} do 2:

$xi k ← F k (x i k-1 )$\\Apply dynamics to particles 3: end for 4: µ k ← x0 

$T ← Σk + U k 1 /2 Λ1 /2 k$\\Compute matrix for adding uncertainty 9:

$x i k ← µ k + T (x i k -µ k )$\\Transport each particle 10: end for 11: Σ k ← Σk + U k

## Update step

We first need to specify how the observation equation (1b) will be adapted to the considered context. Since perturbations in sensors are often stochastic in nature, we continue to model the error in the observation with a random variable, that is

$Y k = H k (x k ) + ε k , with ε k ∼ N(0, V k ) as before.$The mechanism to update the information on x k accordingly is provided by [(2)](#b1). In some situations, the main source of uncertainty in the observation will be of an epistemic nature, yet, if the corresponding model errors are described by N(0, V k ), then the posterior possibility function will be the same; this follows from the likelihood principle since the two associated likelihoods will only differ by a multiplicative constant.

There are several variants of the probabilistic EnKF update step. Here, we follow the principles of the SqrtEnKF as it is well suited to the non-random setting of interest. In fact, we will show that our ensemble can be updated exactly in the same way as in the SqrtEnKF.

From the prediction step, we know that the best fitting Gaussian possibility function for the predicted sample is N(µ k , Σ k ). As is standard, we consider the deviations e i k = x i k -µ k and we verify that the correct updating formulas for the expected value and deviations are

$μk =µ k + K k (y k -H k µ k ),$and êi

$k =e i k -Kk H k e i k ,$where K k and Kk are as defined in the Kalman filter and SqrtEnKF, respectively. The updated particles obtained by adding the posterior expected value and deviations together are

$xi k = Mk (x i k ) . = (I n -Kk H k )x i k + ( Kk H k µ k + K k y k -K k H k µ k ),$which is a linear transformation of x i k . Defining x k as an uncertain variable described by N(µ k , Σ k ) and using Proposition 3, it follows that Mk (x k ) is described by

$fk = N (I n -Kk H k )µ k + ( Kk H k µ k + K k y k -K k H k µ k ), (I n -Kk H k )Σ k (I n -Kk H k ) ⊤ = N (µ k + K k (y k -H k µ k ), (I n -K k H k )Σ k ) ,$which matches the update mechanism of the KF, as required. The map Mk is therefore moving the particles in such a way that the best fitting Gaussian for {(w i , Mk (x i k ))} is the posterior of the KF with N(µ k , Σ k ) as a predicted possibility function. Thus, the update step of the p-EnKF is formally the same as that of the standard SqrtEnKF, as detailed in Algorithm 2, albeit with a difference in interpretation.

## Extensions

We finish this section by collecting together some extensions to the p-EnKF, demonstrating its applicability in the context of nonlinear measurement models and providing a number of ways to improve computational efficiency.

## Nonlinear observation models

In the previous section, we have detailed the p-EnKF with a linear observation model. We now establish that, similarly to the EnKF [[14,](#b13)[33]](#b32), the p-EnKF could be adapted to nonlinear observation models. There are, in fact, two main approaches to dealing with nonlinearity in the observation model, which we consider in turn.

## Model linearisation

The simplest way to handle a nonlinear observation model is to linearise it, i.e., to Taylor expand the observation model around the predictive expected value at time k as

$H k (x k ) ≈ H k (µ k ) + J H k (µ k )(x k -µ k ), with J H k (µ k ) the Jacobian matrix of H k at µ k .$Then, a new observation model can be defined based on the observation matrix J H k (µ k ) and on a non-zero mean H k (µ k ) -J H k (µ k )µ k for the observation noise ε k . After that, we can follow Algorithm 2 for the update, except that the term

$y k -H k µ k is replaced by y k + J H k (µ k )µ k in step 4.$Since linearisation does not depend on the chosen representation of uncertainty, it is equally applicable to the p-EnKF as it is to standard versions of the EnKF. The linearisation method can usually be improved by replacing the term H k Σ k H ⊤ k by the predictive variance of the observation based on the ensemble [[33]](#b32), which is also applicable to the p-EnKF. The term Σ k H ⊤ k can usually also be replaced by a ensemble-based approximation, yet, this is not straightforward in the p-EnKF since the precision matrix does not have the same properties as the covariance matrix: to compute the covariance matrix Σ x,H k (x) between the uncertain variables x and H k (x), one must first compute the precision matrix for (x, H k (x)), invert it, and then extract the block corresponding to Σ x,H k (x) . Such an extension of the state is however usual, as described next.

Extending the state space Another way to deal with a nonlinear observation model is by extending/augmenting the original state with a the corresponding predicted observation, which is then observed linearly. In this case, the state becomes z k = (x k , H k (x k )) and the extended observation matrix is Hk = 0 m×n I m , with 0 m×n the 0 matrix of size m × n. Algorithm 2 can be used as usual once a Gaussian is fitted to this extended state. The posterior ensemble can then be extracted by choosing the first n elements of the extended state for every particle. Care must be taken in practice as the precision matrix of the extended state can be close to singularity due to a strong correlation between the elements. For instance, if one of the components of the state that is observed independently becomes sufficiently well estimated at a given time step, then it might be that the nonlinear observation function is approximately linear from the viewpoint of the ensemble. Yet, this corresponds to cases where linearisation would be appropriate. It therefore appears that a hybrid technique would be the most suitable, with components of the observations being either linearised or included in the state depending on their observed degree of nonlinearity.

## Techniques to improve computational efficiency

As with any ensemble-based technique, the accuracy of the p-EnKF depends on ensemble size. As it is typically of interest to use small ensembles for computational reasons, it can be challenging to represent the state of interest adequately. An important aspect is that the p-EnKF requires a minimum sample size equal to the state's dimension plus one due to the computation of the variance; a sample size matching the dimension plus one is sufficient to ensure that the resulting covariance matrix is of full rank providing only that the collection of displacements from the expected value to each of the sample points are linearly independent whereas a smaller sample will lead to a rank-deficient covariance matrix. That a sample size equal to the dimension plus one suffices follows from the fact that a quadratic form bounded away from zero at a number of points separated from the centroid by linearly-independent vectors cannot vanish anywhere.

However, the required sample size can be reduced by using one technique in the following section. Despite this constraint on the sample size, some methods can be considered for improving the computational efficiency, we present two of them in what follows.

Conditional Independence For the p-EnKF, variance is computed via an optimisation problem. Thus, the number of variables in the precision matrix of the state x ∈ R n will be n(n + 1)/2. However, for many problems, there is a natural structure in the state variable, such as a conditional-independence structure, which can be exploited to reduce this number. Indeed, for sparsely dependent models, it is straightforward to reduce the number of nonzero variables in the precision matrix Λ using conditional independence as follows: The off-diagonal elements Λ ij that i ̸ = j are set to 0 if the variable x i and x j are to be modelled as conditionally independent. By setting some of the off-diagonal elements in the precision matrix to 0 during the computation of this matrix by optimisation, the other terms in the precision matrix will automatically adjust to these constraints, offering a systemic way to perform inflation that is tailored to the strength of the dependence being assumed away. However, exploiting this structure to gain in computational efficiency would require optimisation algorithms that are specifically tailored to sparse/band diagonal positive-definite matrices. The design of such algorithms is left for future work.

Nodal Numbering Scheme Apart from reducing the number of variables, the calculation and storage can be done more efficiently by reordering the variables to minimise the bandwidth of nonzero entries in the covariance matrix. One of the methods to achieve that is proposed in the literature is called the nodal numbering scheme [[7]](#b6). This method uses the graph to reorder the state's element so that the nonzero elements will be close to the diagonal element. Moreover, the nodal numbering scheme ensures that the permuted matrix will have a bandwidth no greater than the original matrix, making the computation involved with the precision matrix more efficient, see [[7]](#b6) and [[25]](#b24) for more details.

## Numerical Experiments

Here we show the performance of the p-EnKF using simulated data from two different models: a simple linear model and a modified Lorenz 96 model.

## Data Generation

One convenient feature of standard probabilistic modelling is that simulation can be performed exactly according to the model: the variability of scenarios, necessary for a thorough performance assessment, can be obtained directly by sampling from the assumed probability distributions. This is no longer the case with possibility theory since embracing epistemic uncertainty means that sampling is no longer a natural operation. The ideal solution would be to obtain a sufficiently large collection of real datasets for which the ground truth is known, this is however not generally achievable for problems like data assimilation. Instead, we generate our simulated scenarios by sampling from the probability distributions assumed by the probabilistic baselines and align our possibilistic model with these.

## Linear model

We first consider a linear model since the performance can be clearly compared with the optimal filter, which can be computed in this instance via the KF. A simple linear model is considered so as to generalise easily to arbitrary dimensions. In particular, we consider that the i-th component at time k, i ∈ {2, . . . , n}, only depends on the i-th and (i-1)-th components at time k-1. This means that conditional independence can be imposed to reduce the computational cost of obtaining the precision matrix with a limited information loss. The model can be written as a state-space model (1), for k ∈ {1, . . . , 100}, with the following components:

1. The initial state X 0 is sampled from N(0 n , 10I n ).

## The dynamic model is linear

$: F k (X k-1 ) = F k X k-1 with F k =        1 λ 0 • • • 0 0 1 λ • • • 0 0 0 1 • • • 0 . . . . . . . . . . . . . . . 0 0 0 • • • 1       $, where λ = 0.1 and the covariance matrix of the dynamical noise ϵ k is U k = 0.01I n .

3. The observation model is also linear, H k (X k ) = H k X k , with H k = I m 0 m×(n-m) and the covariance matrix of the observation noise

$ε k is V k = 0.1I m .$Based on all the shared properties between the Gaussian possibility function and the Gaussian distribution, the best way to align our possibilistic model with the assumed probabilistic one is to simply keep the expected value and variance parameters in our possibility functions. In particular, we assume that the initial state x 0 is described by the Gaussian possibility function N(0 n , 10I n ) and that the errors in the dynamical model are described by N(0 n , U k ).

The parameters of the considered methods are as follows: Unless otherwise stated, the parameter N is set to twice the state's dimension, i.e., N = 2n. For a given value of N , the actual number of samples for all methods is N + 1. The parameters of the UKF chosen in this paper are given by α = 0.25, κ = 130, λ = α 2 (n + κ) -n and β = 2.

## Performance assessment for various sample sizes and dimensions

Since the p-EnKF is an ensemble-based method, it is natural for us to investigate the performance based on different sample sizes and dimensions first. Figure [2](#fig_7) shows the performance of the p-EnKF with no localisation when the state is fully observed (n = m), comparing it to the SqrtEnKF and the UKF. The performance is measured in terms of average RMSE over 1000 realisations, except for n = 64 where we only consider 50 realisations due to a large computational time (more than 30 minutes per run). We aim to assess the performance after initialisation and thus focus on the estimation of the state X 100 at the last time step. The RMSE is computed i) for the posterior expected value with respect to (w.r.t.) the posterior mean of the KF, ii) for the posterior expected value w.r.t. the true state, and iii) for the posterior variance w.r.t. the posterior variance of the KF. The key aspects in Figure [2](#fig_7) are as follows:

1. As can be seen in Figure [2a](#fig_7), despite the similarities between the p-EnKF and the SqrtEnKF, the latter improves on the former by at least 4 orders of magnitude in terms of RMSE w.r.t. the posterior mean of the KF. Although the SqrtEnKF could be used when N < n, which is key in large dimensions, the performance would necessary be lower in this regime. The difference in performance is still visible when considering the RMSE w.r.t. the true state, as in Figure [2b](#fig_7), however it is less pronounced due to the unavoidable error caused by the distance between the true state and the optimal estimator given by the mean of the KF.

2. Despite the fact that the capabilities in terms of variance recovery are almost indistinguishable in Figure [1a](#fig_1) between the possibilistic and probabilistic approach, Figure [2c](#fig_7) shows that the p-EnKF once again largely outperforms the SqrtEnKF in terms of RMSE w.r.t. the optimal variance given by the KF, with improvements by at least 4 orders of magnitude throughout once again. This is due to the fact that here, as opposed to Figure [1a](#fig_1), the mean of the ensemble also needs to be estimated by the SqrtEnKF whereas the expected value for the p-EnkF is given by the particle with index 0 and thus does not need to be re-estimated.

The estimates of the UKF are closer to optimality than the ones of the p-EnKF in Figures [2a](#fig_7) and [2c](#fig_7); this could be due to the difference in initialisation between the two algorithms, with the UKF placing points deterministically and with the p-EnKF relying on a random sample at the first time step. Yet, this source of randomness in the p-EnKF is not necessary and other initialisation schemes are considered in the next section. VDPSOHVL]H GLPHQVLRQ S(Q.) VDPSOHVL]H GLPHQVLRQ 6TUW(Q.) VDPSOHVL]H GLPHQVLRQ 8.)   

## VDPSOHVL]H GLPHQVLRQ H H H H H H H H H H H

## VDPSOHVL]H GLPHQVLRQ H H H H H H H H H H H

## Alternatives schemes for particle initialisation

We now investigate the performance of the p-EnKF with different initialisation schemes inspired by the UKF. When N = 2n, we can simply consider exactly the σ-points of the UKF as initial point for the p-EnKF and we refer to this scheme as "UKF initialisation". To allow for setting N = n, we also arbitrarily consider only the σ-points which corresponds to increasing (resp. decreasing) one of the components of the mean vector and we refer to this scheme as "UKF initialisation +" (resp. "UKF initialisation -").

To better highlight the dependency on the initialisation scheme, we consider in this section a partially observed model with m = 1, i.e., only the first component of the state is observed. This is challenging in general, so only problems of small state dimension are considered. In Figure [3](#fig_9), the performance assessment is carried out for n = 3 and n = 5, with all RMSEs being averaged over 1000 repeats. In Figures [3a](#fig_9) and [3b](#fig_9), all the considered initialisation schemes are compared against the SqrtEnKF and the StEnKF with 2n + 1 samples, with the poor performance of the latter highlighting the difficulty of these inference problems despite the small dimension. There is a slight but consistent improvement in performance when using the UKF initialisation schemes in the p-EnKF, although this improvement vanishes after 30 to 50 time steps, depending on the state dimension. Figures [3c](#fig_9) and [3d](#fig_9) are restricted to the StEnkKF, StEnKF and p-EnKF with UKF initialisation for the sake of legibility, and show that although the interquartile range of the different methods overlap, the p-EnKF performs consistently well across different realisations.  

## Banded vs. full precision matrix

To illustrate the capabilities of the p-EnKF to deal with localisation via a systematic form of inflation, we contrast its performance with a full precision matrix against the one with a banded precision matrix with a bandwidth of two, i.e., where all elements except the diagonal and elements adjacent to it are set to zero. We compare these results against the ones obtained with the standard versions of the EnKF for the fully-observed (n = m = 5) and partially-observed (n = 5, m = 1) cases. The performance is measured with three quantities i) the RMSE of the posterior expected value w.r.t. the true state, ii) the determinant of the posterior variance, and iii) the Mahalanobis distance between the posterior expected value and the true state. The Mahalanobis distance is used to capture how good the estimate is relative to its variance and, hence, assesses the calibration of the algorithms in terms of uncertainty quantification. It is defined as (x -µ) ⊤ Σ -1 (x -µ) where x is the true state, µ is a given posterior expected value, and Σ is the posterior variance.

Figure [4](#fig_11) shows the performance of the p-EnKF with full and banded precision matrix against the same baselines as before. All the algorithms except the KF use 11 samples, i.e., the ensemble size is N + 1 where N = 2n and the considered performance metrics are averaged over 1000 repeats. The important aspects in Figure [4](#fig_11) are as follows:

1. As can be seen in Figures [4c](#fig_11) and [4d](#fig_11), forcing the precision matrix to be banded has little impact on the precision of the p-EnKF, despite correlations being crucial for a strong performance in the partially observed case (Figure [4d](#fig_11)). This is confirmed in Figures [4c](#fig_11) and [4d](#fig_11), where the log-determinant of the posterior variance is mostly unaffected by localisation. A small but noticeable difference can be seen in Figure [4d](#fig_11), but the change in variance is in the correct direction: the determinant of the variance was increased by localisation, i.e., some inflation has been automatically applied in order to compensate for the imposed conditional independence.

2. The Mahalanobis distance for the two versions of the the p-EnKF is nearly constant and close to the one of the KF for both considered scenario, as seen in Figures [4e](#fig_11) and [4f](#fig_11). Conversely, the SqrtEnKF and StEnKF both display large Mahalanobis distances, with the one of the StEnKF even diverging in the partially-observed case; this is due to these algorithms having a larger RMSE than the KF (Figures [4c](#fig_11) and [4d](#fig_11)) but a smaller variance (Figures [4c](#fig_11) and [4d](#fig_11)).

In conclusion, the p-EnKF adopts a nearly optimal behaviour in this linear scenario despite partial observability and localisation. In contrast, the standard versions of the EnKF depart significantly from the behaviour of the KF and tend to be overly optimistic even in the considered small-dimensional inference problems, with well known adverse consequences for downstream tasks for which a reliable quantification of the uncertainty can be crucial.

## Modified Lorenz 96 type model

In order to show that the strong performance of the p-EnKF observed in the previous section generalises beyond the linear case, we now consider a modified Lorenz 96 (LR96) model, which can be written as a state-space model (1), with k ∈ {1, . . . , 100}, with the following components:

1. The initial state X 0 is sampled from N(0 n , 10I n ).

## The deterministic part of the dynamic model is characterised by

$x = F k (x ′ ) with x 1 = x ′ 1 + ((x ′ 2 -c)c -x ′ 1 + F ) ∆t x 2 = x ′ 2 + ((x ′ 3 -c)x ′ 1 -x ′ 2 + F ) ∆t x i = x ′ i + (x ′ i+1 -x ′ i-2 )x ′ i-1 -x ′ i + F ∆t for 3 ≤ i ≤ n -1 x n = x ′ n + (c -x ′ n-2 )x ′ n-1 -x ′ n + F ∆t$where "x i " refers to the i-th component of x and where F = 8, c = 1, and ∆t = 0.01.  Overall, the only difference with the linear model is the transition function F k . Throughout this section, the ensemble size will be set to 2n + 1, that is N = 2n, unless specified otherwise. The LR96 model is particularly convenient for performance assessment since the dimension can be easily adjusted.

## The covariance matrix of the dynamical noise ϵ

$k is U k = 0.01I n . 4. The observation model is linear, H k (X k ) = H k X k ,$As before, we start by investigating the performance of the p-EnKF with the full precision matrix based on different sample sizes and dimensions such that all elements of the state are observed (n = m) compared to the SqrtEnKF and the UKF. However, for the nonlinear model, we only examine the performance in terms of RMSE w.r.t. the true state at the last step since we can no longer obtain the optimal state estimate from the KF. The RMSE displayed in Figure [5](#fig_13) is averaged over 100 repeats, except for n = 64 where we only consider 50 realisations due to the computational cost. The results shown in Figure [5](#fig_13) are very close to the ones obtained in the linear case, with all three algorithms maintaining a similar level of performance despite the non-linearities.  Because of the similarities between the results with the linear model and the LR96 model, we only highlight where noticeable differences arise. Figure [6a](#fig_15) shows that, in the partially-observed case with m = 1 and n = 5, the SqrtEnKF has a RMSE that is now closer to the StEnKF than to the p-EnKF. Localisation in the p-EnKF still has a very mild effect, although the error increases around time step 70. This increase in error is however captured by the associated covariance matrix, so that the Mahalanobis distance remains constant throughout the scenario, as required. This behaviour suggests that the correlation increased around time step 70, forcing the p-EnKF with banded matrix to increase the amount of inflation to compensate for the loss of information caused by the imposed conditional independence.  

## Conclusion

We have introduced the possibilistic ensemble Kalman filter, or p-EnKF, a data assimilation technique treating the state of a state-space model as a fixed quantity about which limited information is available. By using possibility theory to model this form of epistemic uncertainty, we found that much of the intuition behind the standard versions of the EnKF remains valid, with the differences between the theories of possibility and probability leading to key features in the p-EnKF. Specifically, the properties of the expected value and variance in possibility theory appeared to be beneficial for inference problems of small to moderate dimensions, with the p-EnKF closely approximating the Kalman filter in the linear-Gaussian case. These properties also allowed for localisation to be seamlessly applied with no parameter tuning required to compensate for the loss of information incurred by the imposed conditional independence.

In the current version of the p-EnKF, the computation of covariance matrices relies on solving a constrained optimisation problem, which is time-consuming and requires an ensemble size greater than the dimension of the state. Although beyond the scope of this work, lifting these constraints appears to be feasible through the use of specialised optimisation techniques and suitable regularisation.

There are also several possible avenues for further investigation. We highlight two directions that we think are immediately interesting: although we have found the algorithms performance to be robust to the specification of the initial ensemble, there is a potential for developing systematic approaches to specifying it which might lead to further improved performance with small ensembles; and, an open question is how can the update step be reformulated to operate directly in terms of the precision matrix to avoid the need for matrix inversion, which would be required to facilitate the use of the p-EnKF in high-dimension.

![Computational time of sample variance]()

![Figure 1: Analysis of the proposed procedure for Gaussian fitting when the underlying possibility function / probability distribution is Gaussian.]()

![Initial step of the p-EnKF Input: The prior Gaussian possibility function N(µ 0 , Σ 0 ) Output: The initial ensemble {(w i , xi 0 )} N i=0 for i ∈ {1, . . . , N } do 2:xi 0 ∼ N(µ 0 , Σ 0 ) \\Draw a particle 3:]()

![Λk ← Λ * {(w i , xi k-1 )} i \\Compute the precision matrix Σk ← Λfor i ∈ {1, . . . , N } do 8:]()

![Average RMSE w.r.t. the posterior mean of the KF.]()

![Average RMSE w.r.t. the true state.]()

![Average RMSE w.r.t. the posterior variance of the KF.]()

![Figure 2: Performance assessment for the fully-observed linear model.]()

![Average RMSE w.r.t. the true state for n = 3 Average RMSE w.r.t. the true state for n = 5 with UKF ini (c) Average RMSE with the interquartile range for n = 3 with UKF ini (d) Average RMSE with the interquartile range for n = 5]()

![Figure 3: Average RMSE and the error range of different algorithms in the linear model. Left: state's dimension of 3; Right: state's dimension of 5, averaged over 1000 repeats.]()

![with H k = I m 0 m×(n-m) and the covariance matrix of the observation noise ε k is V k = 0.1I m .Mahalanobis distance between the estimate and the true state]()

![Figure 4: Performance assessment for the linear model with n = 5 when (left) fully observed, m = 5 and (right) partially observed, m = 1, averaged over 1000 repeats.]()

![Figure 5: Performance of p-EnKF in terms of RMSE w.r.t. the true state, compared with the SqrtEnKF and the UKF for the LR96 model]()

![Mahalanobis distance between the estimate and the true state]()

![Figure 6: Performance for a partially-observed LR96 model (m = 1) when n = 5, averaged over 1000 repeats.]()

Although p(• | θ) is not formally a conditional probability distribution, it is useful to slightly abuse notations and write it as such.

