# BRIDGING THE DATA PROVENANCE GAP ACROSS TEXT, SPEECH, AND VIDEO

## Abstract

## 

Progress in AI is driven largely by the scale and quality of training data. Despite this, there is a deficit of empirical analysis examining the attributes of well-established datasets beyond text. In this work we conduct the largest and first-of-its-kind longitudinal audit across modalities-popular text, speech, and video datasetsfrom their detailed sourcing trends and use restrictions to their geographical and linguistic representation. Our manual analysis covers nearly 4000 public datasets between 1990-2024, spanning 608 languages, 798 sources, 659 organizations, and 67 countries. We find that multimodal machine learning applications have overwhelmingly turned to web-crawled, synthetic, and social media platforms, such as YouTube, for their training sets, eclipsing all other sources since 2019. Secondly, tracing the chain of dataset derivations we find that while less than 33% of datasets are restrictively licensed, over 80% of the source content in widelyused text, speech, and video datasets, carry non-commercial restrictions. Finally, counter to the rising number of languages and geographies represented in public AI training datasets, our audit demonstrates measures of relative geographical and multilingual representation have failed to significantly improve their coverage since 2013. We believe the breadth of our audit enables us to empirically examine trends in data sourcing, restrictions, and Western-centricity at an ecosystem-level, and that visibility into these questions are essential to progress in responsible AI. As a contribution to ongoing improvements in dataset transparency and responsible use, we release our entire multimodal audit, allowing practitioners to trace data provenance across text, speech, and video.

## INTRODUCTION

The capabilities and flaws of multimodal foundation models are often directly attributable to their training data [[66]](#), [[74]](#), [[75]](#), [[90]](#), [[91]](#), [[117]](#), [[130]](#). While the importance of data measurement has been widely established by prior work [[118]](#), so has a prevailing absence of data documentation [[10]](#), [[39]](#), transparency [[73]](#), and detailed understanding [[34]](#), [[37]](#), [[47]](#)-especially for modalities other than text. A lack of thorough data analysis has led to significant challenges, including privacy issues [[107]](#), retracting datasets with harmful content [[35]](#), [[80]](#), adversarially bypassing safety filters [[66]](#), facial recognition bias with respect to gender and skin type [[11]](#), gender bias in hiring [[77]](#), benchmark contamination from overlapping train and test sets [[87]](#), and challenges in copyright [[84]](#). Understanding data provenance can aid mitigation attempts to reduce model bias and toxicity [[50]](#), [[102]](#) address representation in data [[51]](#), contamination [[81]](#), and quality [[59]](#), [[95]](#), as well as practical challenges with identifying copyright-free and permissively licensed sets [[96]](#). modalities [[70]](#). As such, we focus our analysis on datasets that represent one or a pair of these modalities.

## Annotation Features & Methodology

In particular, we analyze data trends for the state of data permissions (licenses and terms), sourcing (the web, human annotation, and synthetic generation), and representation (of tasks, organizations, languages, and countries). We adopt Longpre, Mahari, Chen, et al. [[123]](#)'s methodology, including the license annotation taxonomy and process, to manually audit these features precisely and rigorously. We go beyond prior work, which considers dataset licenses, by extending the taxonomy to consider the terms of use of the sources of the dataset, either from models used to generate synthetic data (e.g. OpenAI's non-compete clause 1 or Meta's acceptable use policy for Llama 3.1 2 ), or the source's policy on content restrictions, which can be conveyed in the form of a license, terms of use, or content policy on a website [[119]](#). For each dataset, the source terms are annotated as Unrestricted, Unspecified, Source Closed or Model Closed, as defined in Table [2](#). For Figure [2](#) we combine Source Closed and Model Closed into Restricted.

As with prior work [[123]](#), [[124]](#), we engage domain experts for these annotation tasks-AI researchers whose work pertains to the modality and topic. Because many datasets are iteratively re-packaged before they appear in their final form and often shared on popular dataset marketplaces like Hugging-Face, Papers with Code or Github, prior work has found that relevant licensing terms or sourcing information for AI training data is frequently omitted [[123]](#). To ensure we collect this information, we require a full trace of metadata back to their original sources (sometimes a chain of github repositories, websites, or academic papers). This search can be onerous, especially for terms and licenses, but ensures rigor in the results. Table [1](#) enumerates the full statistics of our audit. All annotations and analysis code will be made publicly available on release.

## Scope & Dataset Selection

For each modality, we define the scope of the audit (detailed separately below), then aggregate resources to distill a list of relevant datasets. The scope is focused on (a) publicly available datasets, (b) widely used tasks in the context of general-purpose model development, and (c) relevance to generative tasks. However, we do consider classification-based datasets in text, speech, and video that can and are frequently re-purposed for generative uses (e.g. instruction tuning). Within the defined audit scope, we use a mix of the HuggingFace Datasets platform, survey papers, survey repositories, workshop proceedings, and expert review to accumulate relevant datasets. More detail about the dataset selection and collection process is given for each modality below. Each modality requires its own independent process, by virtue of their community dataset ecosystems being unique (discussed in Section 4). Note that text has a wider heterogeneity of published publicly available datasets than speech or video. Typically those datasets have been aggregated into large, standardized text-to-text collections, and as such we trace both these Text (Collections) and their constituent Text (Datasets). All datasets are described, linked, and attributed in Appendix D.

## TEXT

Scope We focus on providing an extensive audit for post-training datasets, used in training language models. We include single and multi-turn formats, encompassing both datasets typically used for instruction finetuning (SFT) and preference alignment [[105]](#). This scope reflects the prominent role of general-purpose language models, which benefit from multi-task training on heterogeneous collections that span a variety of linguistic, reasoning, and knowledge intensive tasks like question answering, coding, tool use, translation, and classification [[49]](#), [[64]](#).

Dataset Selection We expand the study conducted by the Data Provenance Collection [[123]](#), from 44 dataset collections (of 1858 supervised text datasets) to a superset of 108 collections of 3717 datasets, prioritizing recent, popular publicly available HuggingFace Datasets introduced between 2022 and April 2024. Our collection sourced popular datasets from recent survey papers [[114]](#), [[121]](#) and tools [[122]](#). We additionally reviewed HuggingFace Datasets' most downloaded datasets every month, from April to July 2024, under the Natural Language Processing category, as well as the SFT/DPO datasets associated with popular open model releases. We also drew from major multilingual data repositories, including the SEACrowd Catalogue [[126]](#), the Masader Arabic Data Catalogue [[52]](#), AI4Bharat [[27]](#), and the Aya Collection [[134]](#). Lastly, our list of datasets was reviewed and supplemented by language model experts to fill in notable omissions. In total, we trace 1 OpenAI Terms of Use 2 Llama 3.1 Acceptable Use Policy the provenance and features of 3713 text datasets from 108 collections, covering 395 popular tasks, spanning from 1994 to 2024.

2.2 SPEECH Scope We audit speech datasets for which automatic speech recognition (ASR) was noted as a primary task. We focus on ASR datasets because: (1) ASR is fundamental to many speech technologies, including dictation tools, voice assistants, and chatbots [[32]](#), [[68]](#); (2) large-scale speech datasets are typically designed for ASR [[89]](#); (3) ASR data follows standardized formats, making comparisons easier (e.g., corpus of audio clips paired with text); and (4) ASR data can often be reused for other tasks like text to speech (TTS) [[7]](#) or language identification [[20]](#).

## Dataset Selection

To curate a representative sample of popular ASR datasets, we relied on a combination of survey repositories [3](#foot_0) , and HuggingFace Datasets using the "Automatic Speech Recognition" and "Text-to-Speech" task tags. We expanded coverage to well-documented datasets on the OpenSLR[foot_1](#foot_1) platform, even if they were newer or less widely used. We expect this might reflect datasets that could be adopted more widely in the future. Finally, we included datasets related to low-resource languages and other languages not well-covered by our initial searches. Speech recognition models are increasingly highly multilingual [[33]](#), [[104]](#), [[131]](#), and datasets serving different communities of builders and end-users around the world are a priority for making speech recognition technologies more inclusive. In total, we trace the provenance and features of 95 speech datasets, covering 18 popular ASR tasks, spanning from 1990 to 2024.

## VIDEO

Scope Early video understanding models primarily focused on video classification, detection and action recognition, where short clips were categorized into predefined classes [[30]](#), [[69]](#). More advanced tasks such as temporal action segmentation, video question answering, and video captioning were later introduced to build upon these foundational tasks [[63]](#), [[111]](#). Recently, following the success in the field of image generation, video generation from text has become a new task that has shown promising results [[72]](#), [[82]](#), [[115]](#), [[140]](#). Given the scarcity of datasets for text-to-video and the often undocumented sources of data used in recent video generation models [[127]](#), we take a broader approach to our collection of video datasets. We focus on annotating popular video tasks and limit our scope to datasets corresponding to video tasks that are either published, highly cited, or have 100+ downloads on HuggingFace. This approach is justified by three key factors: (1) the usefulness of video data to the research community stems from its collection and presentation in peer-reviewed work, (2) datasets can often be repurposed between different tasks, allowing for applicability to new tasks such as video generation from text, and (3) focusing on highly cited datasets ensures that datasets' quality and relevance has been validated by the research community.

Dataset Selection We include datasets tagged with "Video Classification", "Text-to-Video", and "Video-Text-to-Text" from HuggingFace Datasets. We augmented this with datasets tagged by "Video Understanding" or "Video Generation" in PapersWithCode, as well as datasets listed in a popular Github survey repository. We also consulted the proceedings of recent video workshops: the Large Scale Video Understanding and Egocentric Vision workshops. We separately consulted a committee of non-author video experts to supplement the list with relevant datasets published at CVPR, ICCV, ECCV, and IJCV. In total, we trace the provenance and features of 104 video datasets, covering 33 popular video tasks, spanning from 2009 to 2024.

## RESULTS

We discuss three key results related to (1) the rising use of web, social media and synthetic sources, (2) inconsistent and opaque restrictions on data use, and (3) a lack of improvement in geographical or linguistic representation. Each of these findings holds across modalities, at the ecosystem level.

## RISING USE OF WEB, SOCIAL MEDIA & SYNTHETIC DATA

The need for scale, and heterogeneity have driven rising use of data from web-crawled, social media, and synthetic data sources. Developers have sought out ever larger and conveniently Figure [1](#): The cumulative size of data (log-scale tokens for text, hours for speech/video) from each source category, across modalities. The source categories in the legend are ordered by descending quantity. Speech and video sources are increasingly dominated by internet videos and YouTube. Whereas text is predominantly web or encyclopedia-based (wiki) sources, synthetic text is rising in popularity.

accessible sources of training data [[24]](#), [[57]](#). While small, human-curated datasets are often sufficient and sometimes preferred due to higher quality, these sources often do not scale to present demands [[24]](#), [[26]](#). In Figure [1](#), we empirically measure the rising use of web crawling and social media (or "forum") websites that provide some of the most scalable and fresh content. While web-sourced data was always prominent, the balance of sources becomes much more skewed after 2018-note the use of the y-axis log scale. We find for Speech and Video that by far the most prominent source of data has become internet videos, and specifically YouTube. Nearly 1M hours each of Speech and Video data from this source far outstrips the next most common sources, which comprise less than 100K hours. For Speech, the primary data sources used to be Calling Platforms (pre-2017), content manually collected with Human Participation, and Audiobooks, but since 2018 internet videos have supplanted these other sources. For Video, since 2013, YouTube, synthetic, and general web data sources all constitute a significantly larger portion of data used in prominent video datasets, outstripping the use of Movies, Flickr, Getty, or human curated sources. Among text post-training datasets, we see a similar trend with general or news web-based sources, including encyclopedic sources (mainly Wikipedia), providing the majority of tokens over time. Encyclopedic sources alone now contribute over 1T tokens in total.

Synthetic data sources are rising the most rapidly. Within the video modality, the introduction of VidProm [[138]](#) in 2024, consisting of nearly 7M synthetically generated videos, offered a large shift in the video source distribution. Within the textual modality, from fig. [1](#), synthetic data represented <0.1% of the quantity of Web Encyclopedia data in 2020, but is now 10% its proportion in 2024, making up the 5th largest source of tokens. The top models used in generating datasets are mainly from OpenAI. The top 5 consist of ChatGPT, version unspecified (15.0% of synthetic datasets), GPT-4 (14.4%), BART (10.1%), GPT-3 (8.3%) and GPT-3.5-Turbo (4.9%). The average synthetic dataset also has notably longer turns (in tokens) than the average natural dataset: 1,756 tokens vs 1,065. The task distribution of textual synthetic datasets is shifted towards longer form, open-generation and creative tasks. For example, 88.1% of natural datasets contain classification tasks, compared to only 66.3% of synthetic datasets. Natural data is also more likely to cover translation than synthetic data (72.4% of datasets vs only 22.9% of synthetic datasets).

## INCONSISTENT USE RESTRICTIONS

In the United States, creators of a work automatically have a copyright interest that gives them exclusive rights to make copies and derivatives of the work (17 U.S.C. Â§ 106). Licenses are legal documents through which the owners of a work express how others may use their work. By contrast, Terms of Service express a contract between a platform and its users to spell out how a platform and its content may be used [[28]](#). For simplicity, we use "Licenses" to refer to dataset restrictions, and "Terms" to refer to restrictions on the sources of datasets. There remain open questions about whether certain data licenses are enforceable, but these licenses signal the intention of data creators and therefore warrant consideration as the data creators may be best positioned to understand the sensitivities of the data (privacy, copyright, representation, etc.), and the most impacted by its downstream use [[88]](#), [[93]](#), [[94]](#), [[97]](#). The extent to which a practitioner adheres to dataset licenses or source terms remains Figure [2](#): The distribution of restrictions from dataset licenses and their sources' terms. We break this down by the count of datasets (top), as well as total tokens or hours (bottom). Each license is categorized as Non-commercial/Academic (NC/Acad), Unspecified, or Commercially licensed. Each dataset may also have terms from the source: Restricted to non-commercial use, Unspecified restrictions, or Unrestricted. Two main findings across modalities emerge: (1) Commercially licensed datasets represent a larger set of tokens and hours, relative to number of datasets; however, (2) the vast majority of those commercially licensed tokens/hours bare restrictions from their sources. Tables [3](#tab_3) and [4](#) in the appendix provide detailed numbers.

an open question, and may depend on jurisdiction or the desired model's use cases [[88]](#). This work does not propose one standard for all developers. For these reasons we restrict our treatment and discussion here to tracing the lineage and distribution of licenses and terms for a given modality.

Data source terms are much more restrictive than the dataset's documented license restrictions. In Figure [2](#), we find only 25%, 33%, and 32% of text/speech/video datasets are licensed noncommercially. This value is even lower if we consider the proportion of tokens or hours, with 21%, 26%, and 33% of text/speech/video quantities carrying license restrictions. However, a staggering 99.8%, 78%, and 99% of those quantities carry some form of non-commercial restriction on one of their sources. For text, these restrictions are frequently from being generated by OpenAI or other models with a non-compete clause, while for speech and videos this is often since the datasets are derived from web or social media sources.

Inconsistencies between dataset licenses and their source's restrictions pose challenges to practitioners. A large amount of datasets have permissive or unspecified licenses, but some set of their sources carry non-commercial restrictions. This inconsistency is measurable-representing 79% of tokens in text datasets, 55% of speech hours, and 65% of video hours. Additionally, 19%, 14%, and 36% of text, speech, and video datasets have no license or intended use documentation (from our audit of the datasets' documentation on Hugging Face Datasets, GitHub, and Papers with Code). A lack of centralized documentation around these restrictions means it can be misleading to developers who are attempting to source data according to their own legal standards for copyright and privacy. Furthermore, lack of documentation can hamper developers following best practices around data preparation and transparency [[39]](#), [[73]](#).

Large quantities of commercially licensed text datasets are locked in collections without clear information to separate them from restrictive datasets. In Figure [2](#) (top and bottom), we see the number of datasets and number of tokens without restrictions is significantly higher for Text (Datasets) than Text (Collections). Specifically, 60% more Datasets (or 75% more tokens) are commercially licensed, than for Collections. This demonstrates that many collections contain significant amounts of commercially licensed data. While our audit traces licenses for all datasets within a collection, most collections do not aggregate or expose this documentation. As a result, practitioners may be left without easy access to filter for the subsets appropriate for their sourcing standards.

3.3 GEOGRAPHICAL & LINGUISTIC REPRESENTATION IS NOT IMPROVING AFRICA ASIA EUROPE N AMERICA OCEANIA S AMERICA By Count TEXT 0.3 13.4 24.0 61.5 0.7 0.2 SPEECH 3.6 35.7 30.4 30.4 0.0 0.0 VIDEO 0.0 25.2 24.4 48.0 0.8 1.6 By Tokens or Hours TEXT 0.0 6.1 55.4 38.4 0.1 0.0 SPEECH 0.1 38.8 18.8 42.4 0.0 0.0 VIDEO 0.0 23.1 22.0 38.2 16.7 0.1 Figure 3: The geographical distribution of countries (world maps) and continents (table) represented by dataset creators. Despite some differences in European, Russian, and Middle Eastern representation, creators are heavily concentrated in the US, China, and Western Europe, with little to no representation in South America or Africa, across modalities. The current Gini coefficient for (Text, Speech, Video) = (0.92, 0.86, 0.74), where higher values indicate more concentration.

The importance and progress of representation in AI training data. Diversity and representation in training datasets, and among their creators, are widely acknowledged as essential to building AI models that are less biased, more useful, and more equitable [[6]](#), [[18]](#), [[25]](#), [[31]](#), [[61]](#), [[101]](#), [[112]](#), [[113]](#), [[134]](#), [[137]](#). Prior work has measured the diversity of languages in data along with cultural, ideological, and geographical imbalances [[8]](#), [[14]](#), [[41]](#), [[55]](#), [[62]](#). These studies have exposed significant flaws, often in the form of bias and discrimination, stemming directly from poor representation in data [[12]](#), [[35]](#). As this problem has now been widely acknowledged for decades, recent efforts have foregrounded sourcing data multilingually and multi-culturally, from native speakers and creators (e.g. ROOTS [[60]](#), the Aya Dataset [[134]](#), the SEACrowd Catalogue [[126]](#), the Masader Catalogue [[52]](#), Common Voice [[13]](#), Causal Conversations V2 [[101]](#) or Moments in Time [[18]](#)).

Measuring geographical and linguistic representation. Naturally, we aim to use our audit to measure the progress of these efforts on geographical and linguistic representation in the AI ecosystem. We measure the progress of two forms of representation: (1) language diversity of text and speech data, and (2) geographical diversity of the creators, in all three modalities. For languages, we use the ISO 639-1 and 639-3 language codes and categories of language families from Glottolog 5.0. [5](#foot_2) In Figure [4](#fig_0)(a, c) we display the cumulative sum of unique languages and countries present across all audited datasets, at each time period since 2013. While these measurements illustrate the absolute rise in diversity, we also hope to measure the relative dispersion, or equality of languages and countries in the distribution. In Figure [4](#fig_0)(b, d), we use the Gini Index [[1]](#), [[2]](#), a traditional measure of statistical dispersion, frequently used to quantify inequality. This allows us to understand if the distributions of languages and creators are more representative of the international community over the last decade, or equally concentrated despite apparent efforts at the margins.

Inequality in geographical representation remains very high, with few organizations creating datasets from the Global South. For every dataset, our audit recorded the organizational affiliations of each creator of the dataset. [6](#foot_3) These organizations were then manually mapped to the country in which they are headquartered. Occasionally, organizations like BigScience, BigCode, or Masakhane have international or continental representation, and were counted as such. In Figure [3](#), we measure the current state of diversity among these creator organizations-where a Gini coefficient of 1 indicates highest concentration, and lower values more broad representation. Without taking up the normative question of what a truly "fair" score would be, these values provide useful comparisons across modalities and over time. We find that Text dataset developers are particularly homogeneous, with a Gini-coefficient of 0.92; followed by Speech, at 0.86 and Video at 0.74, which remain high, but are meaningfully less concentrated. Figure [3](#) also illustrates that even this limited diversity is still concentrated in North America, Europe, East Asia, and less so in the Global South.

In Figure [3](#), we also compare the distribution of datasets, and of tokens or hours by continent. Dataset creators affiliated with African or South American organizations account for fewer than 0.2% of all tokens or hours, in each modality. In contrast, Asian affiliated organizations represent large proportions of the data, particularly for speech (39% of hours, attributed predominantly to YODAS [[89]](#)). Much of this driven by Chinese, Indian, Russian, and Saudi Arabian creators. Most prominently, the combination of North American and European datasets comprises 93% of text tokens, 61% of speech hours, and 60% of video hours. Geographical representation has not significantly improved for over a decade. In Figure [4](#fig_0)(c), we measure the total unique number of countries represented across all dataset creator organizations.

While individual creators will have varying ethnic and national affiliation, we treat this as an estimate for the influence of each locale in dataset development. We find that while the number of represented countries has risen steadily each year, for each modality, this represents only an illusion of progress. Empirically, the Gini coefficient for each modality has not significantly changed since the start of the period we examine in 2013. Geographic diversity has increased only among Video datasets, and these increases are not significant at the ð‘ = 0.05 level. Text and Speech geographical representations appear to remain stable over the last decade of AI development.

Multilingual representation has not improved by most measures. Similar to geographical representation, we measure the cumulative number of ISO 639-1 languages and language families over time, as well as the per-modality Gini-coefficient. Figure [4](#fig_0)(a) shows significant increases in the number of languages available for speech and text, especially in 2019, and 2023, with the introduction of large sets like Flores [[56]](#), xP3x [[98]](#), Common Voice [[13]](#), and the Aya Collection [[134]](#). However, once again, when measuring the cumulative dispersion of these datasets in Figure [4](#fig_0)(b), only Text language families demonstrate any improvement from pre-2013 to the present. Improvements in the Gini coefficient appear to be largely driven by individual large-scale projects like xP3x and Common Voice, both introduced in 2019. Subsequently, newer datasets remain predominantly monolingual, causing measures of concentration in text languages, speech languages, and language families to remain consistently high. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Text Speech Video 47.3 6.0 7.1 15.2 10.3 8.2 6.0 16.4 16.9 15.7 15.4 15.1 10.6 9.3 71.1 5.5 8.5 10.4 Academic Research Group Industry Lab Corporation Startup Other Government Unspecified Creator Category Academia, research non-profits, and industry labs continue to drive public dataset development.

As well as understanding the geographic associations of the organizations creating popular datasets, we manually categorize them into: Academic Organization (e.g., universities), Research Groups (e.g., non-profits such as BigScience, EleutherAI or AI2), Industry Labs (e.g., Cohere For AI, Google DeepMind), Corporations (e.g. Google, Meta), Startups (e.g., OpenAI, Anthropic), Governments, Unspecified (datasets where owner affiliation is not shared), or Other. When a dataset is released in collaboration between organizations, we record each organization. In Figure [5](#fig_1), we find that universities and other academic organizations account for 16%, 47%, and 71% of all recorded dataset releases, across Text, Speech, and Video respectively. Research groups, industry labs and even corporations are also significant contributors, especially for Text datasets, where ecosystem contributors are far more distributed. The significant role of academic organizations in Video and Speech may suggest that the risk profile of releasing Text datasets differs somewhat from Video and Speech datasets, which may have more distinct privacy concerns.

## DISCUSSION

The rise of web-based, social media, and synthetic datasets may pose greater risks to privacy, copyright, and bias. Section 3.1 discusses the rise of web-based sources and particularly social media as primary sources for speech and video. Figure [1](#) shows these sources now exceed more traditional, curated sources such as movies, audiobooks, radio, TV, or content hand-crafted by human participants-by at least one order of magnitude. These websites made of mostly user-generated content are a natural choice, given that they scale in the quantity, freshness, and heterogeneity that is best suited to train general-purpose models [[70]](#), [[92]](#). However, prior work suggests that crowd-sourced, user-generated web content also introduces more challenges than curated content, particularly for privacy, copyright, bias, harm, and factuality.

Web-based and particularly user-generated content is disproportionately likely to include personally identifiable information (PII) [[40]](#), [[81]](#), [[107]](#), and copyrighted content [[16]](#), [[88]](#). These can be reproduced in the outputs of AI models [[53]](#), [[78]](#), creating privacy and copyright concerns [[110]](#). Open datasets being used to train GPAI often attempt to filter-but frequently miss-PII and copyrighted data [[107]](#), [[136]](#) (although not all do [[99]](#)). Social media, in particular, is also known to have bias, toxicity and factuality issues [[19]](#), which can manifest in trained models, even after alignment [[85]](#). Lastly, while synthetic data can help reduce the prevalence of PII, copyright, or bias in data, it comes with its own challenges [[86]](#), [[120]](#).

Social Media websites have become one of the most prominent data sources, but their Terms often restrict crawling or commercial use. We find that 71% of Video data and 69% of Speech data is from YouTube which has become a prominent source of data, given its scale, freshness, and multimodality (containing videos, speech, images, and text) [[4]](#), [[9]](#), [[22]](#), [[79]](#), [[89]](#), [[109]](#). However, YouTube is a social media platform owned by Google and its Terms of Service[foot_4](#foot_4) prohibit third parties from crawling YouTube. While content creators maintain their ownership rights in the material they upload to YouTube, the YouTube Terms of Service also grant Google a license to reproduce, modify, display, and use the content for purposes connected to YouTube's "business", which may include building machine learning models; even if the copyright holder has selected a permissive license, YouTube's Terms disallow external parties from crawling that data. Model developers such as Nvidia and OpenAI have been sued in the U.S. by content creators who allege that they unlawfully trained on YouTube videos [[116]](#), [[135]](#). Large social media platforms and forums have also adopted restrictive terms in recent years, including Reddit and StackOverflow. [8](#foot_5) As these data sources become critical to scaling AI systems, access has been made exclusive, which may hamper academic, non-profit, or open source model development-to the extent that social media platforms can enforce their terms against third party developers. [9](#foot_6)Ambiguous and poorly documented use restrictions may significantly inhibit model developers adhering to cautious legal and ethical data sourcing standards. In Section 3.2. we find that a significant amount of data carry non-commercial restrictions in their sources, rather than on the final dataset, which can contain no license or a permissive one. For text and video, these restrictions can equate to 99% of all tokens and hours. These inconsistencies are the result of datasets being iteratively re-packaged and re-licensed, without carrying on documentation [[123]](#). While not every developer will employ the same filtering standards, our work shows that the challenges to separate and identify appropriate datasets remain difficult across these modalities. Without continued audits and documentation, practitioners may be forced to forego large collections of partially viable data, hampering data scaling laws [[26]](#), or take on avoidable risk. We hope this released audit will provide greater tools for practitioners to apply their own standards, to make informed decisions on training data use.

The limitations of measures of geographical and linguistic representation. It is important to note that measures of geographical and linguistic representation are imperfect. We are limited by partial information about the developers' identities (including for privacy reasons), limited transparency into how frequently these datasets are used, and the extent to which proprietary datasets may fill in representation gaps behind closed doors. Nonetheless, we believe the breadth and rigour of the audit make this the best available empirical measure of representation in publicly documented datasets. Further, we propose the goal of measuring representation in AI data as essential to understanding progress, or its absence, towards AI systems that fairly serve the broader community of users. Figure [3](#) and Figure [4](#fig_0) demonstrate that despite the absolute rise of geographical and linguistic representation, the relative western-centric concentration persists, across thousands of surveyed datasets. We release all audit materials for transparency and replicability, and for further use by the research community.

Conducting representative analyses of an ecosystem comes with assumptions. First, an ecosystem for AI is by nature, not centralized or organized. Widely used datasets for Text are often hosted on Hugging Face, but this is frequently not the case for Speech or Video. Similarly, while Text data undergoes frequent dataset re-packaging for general-purpose post-training, this is not true to the same extent for other modalities. As such, the scope and dataset selection process need to be designed for each modality, rather than a single, simple protocol, which inevitably will not accurately represent one modality at its ecosystem-level. Similarly, we chose a subset of modalities of interest to foundation model development [[104]](#), [[115]](#), but note there are many other left for future work (e.g., images, 3D representations, tabular, time series, graphs, and geospatial data).

## LABEL DEFINITION MODEL CLOSED

A model used to generate part or all of the dataset prohibits using its outputs commercially, to develop a competing AI model, or in general. SOURCE CLOSED The source has a license or terms that prohibits use of the data, either commercially, from being crawled, to develop AI, or in general.

## UNSPECIFIED

No information can be found relevant to restrictions, or lack thereof, for this source.

## UNRESTRICTED

The source has a commercially permissive license, such as CC BY, or explicitly states the data is open for broad use.

Table 2: The taxonomy used to determine use restrictions on each dataset source. Each source in a dataset is examined and fit into one of these categories. The dataset Terms are then labelled according to the strictest terms across the sources, with Model Closed and Source Closed considered stricter than Unspecified which is in turn stricter than Unrestricted.

## A EXTENDED RELATED WORK

Progress in machine learning across modalities from speech [[104]](#) to vision [[38]](#) to text [[21]](#), [[49]](#) has benefited from advancements in large pre-training and fine-tuning corpora. The development of multimodal corpora has also been key to several recent advances, as with CLIP in the image/text domain [[45]](#), CLAP for audio/text settings [[54]](#), and a number of other models involving both text and images, audio or video [[65]](#), [[67]](#), [[104]](#), [[132]](#).

The datasets powering these advances are not, however, always well-documented, despite the existence of standards and frameworks for recording and annotating dataset metadata that range from 'data statements' [[10]](#) to 'datasheets for datasets' [[39]](#) and others [[17]](#). The key problem is not a deficiency of any particular framework, but rather inconsistent adoption and fragmentation [[125]](#). Much prior work has argued for the need to document and audit these datasets [[44]](#), [[46]](#), motivated by concerns from reproducibility [[58]](#) to interpretability [[92]](#) to bias and fairness problems that may stem from problematic content in training data [[35]](#).

There have been several attempts to carry out such audits, with prior work examining pretraining data [[124]](#), general web corpora [[23]](#), [[37]](#), instruction fine-tuning datasets [[123]](#), and the documentation fields of the HuggingFace Datasets platform in particular [[139]](#). For speech and vision, there has been less work, with many discussions of datasets in the aggregate occurring in survey papers [[3]](#), [[106]](#), research aimed directly at improving model performance [[83]](#) or close examinations of questions like bias in small groups of datasets [[12]](#), [[133]](#).

Prior work has also examined the identities, affiliations and national origin of paper authors [[128]](#) in AI, but an analogous look at the producers of datasets is lacking. We aim to carry out such analyses: replicating those for pretraining and text finetuning datasets in video and audio domains, and surveying provenance and legal status. Finally, there has also been significant recent attention to legal questions in the collection and use of AI training data [[29]](#), [[84]](#). The complex process involved in preparing these datasets [[88]](#), and the ambiguous licensing of inputs, can make understanding the legal status of the final output quite difficult.

## B DATASET LICENSES & TERMS

Detailed taxonomy We code the legal restrictions placed on use of datasets along two axes. First, we identify whether a dataset's license permits commercial use ("Commercial" in Table [3](#tab_3)), only non-commercial / academic use ("NC / Acad"), or does not clearly specify what is permitted ("Unspecified"). The latter category includes datasets for which we were unable to locate a license. Datasets which are in the public domain and not subject to a license are counted as commercially usable. Second, we annotate the contractual or terms-of-use restrictions placed on dataset use by the source of each dataset. There are four levels, defined in Table [3](#tab_3). Note that the Model Closed status can only apply to datasets that are AI-generated, at least in part. Some datasets can carry both Model Closed and Source Closed status, but we count the Model Closed first for simplicity.

Detailed breakdown Tables [3](#tab_3) and [4](#) present crosstabs of these two dimensions, according to respectively the total amount of content and the number of datasets. The most notable finding, as discussed in the main text, is the frequency of clashing restriction status between licenses and terms. By amount of content, fully 73.0% of text content, 55.0% of speech content, and 21.6% of video content is subject to a license permitting commercial use but also to terms restrictions forbidding it, or the reverse. The absolute level of restrictions is also high, with < 0.1% of text content, 5.4% of speech content, and 0.6% of video content usable for commercial purposes under both licenses and terms. Table  [4](#): A breakdown of the percentage of license and terms restrictions by dataset count. The much higher frequency of restrictions at the collection level is because we consider a collection's license or terms status to be the most restrictive of those for its datasets. Note that percentages may not add to exactly 100% because of rounding. over other, increasingly prominent tasks like generation and reasoning. Given our selection criteria, all datasets for speech are for ASR tasks, but other tasks like speaker identification and translation are also represented.

## LICENSE / TERMS RESTRICTED UNSPECIFIED UNRESTRICTED TOTAL

## D DATASETS

This section provides a detailed overview of the datasets we have collected and analyzed. Table [5](#tab_6) summarizes the text datasets, Table [6](#tab_7) the audio datasets, and Table [7](#tab_9) the video datasets. Each of these tables lists broad collections of data, sorted in chronological order, and provides information about their properties, sizes, sources and permissions. Each collection can include multiple datasets, and

0% 5% 10% 15% 20% 25% Classification Q&A Generation Reasoning Creativity Code Summarization Bias Detection Translation Resp. Ranking Text 0% 20% 40% 60% 80% 100% Recognition Speaker ID Translation Language ID Text-To-Speech Bias Detection Keyword Spotting Query by Ex Speech Synthesis Speech 0% 10% 20% 30% 40% 50% Classification Captioning Segmentation Summarization Localization Action Recognition Q&A Action Detection Pose Estimation Object Detection Video they generally reflect the ways dataset creators have grouped their datasets (such as in the same paper). Because of the large number of datasets, we provide detailed information about their licenses and original published papers, where applicable, in the supplementary Attribution Card in Appendix F.

Annotation Details: Text For post-training text datasets it is common to package many together as collections, such as Flan [[49]](#) or P3 [[48]](#). This practice is not common to the same extent for speech or video datasets. For much of the text analysis, where possible, we chose to analyze statistics at the collection-level, since practitioners are more likely to adopt a collection for general-purpose posttraining, than an individual dataset within the collection. Also, in dataset-level statistics, metadata for a single collection with many datasets can get repeated and overwhelm the statistics unfairly (e.g. the dataset aggregator/creator being repeated hundreds of times). Consequently, our collection-level analysis of the text modality is reflected in Figure [1](#), Figure [3](#), Figure [5](#fig_1), Figure [4](#fig_0), Figure [7](#fig_3), and Figure [6](#fig_2). However, for Figure [2](#) we draw the distinction between collection and dataset metrics, as practitioners may wish to unpack collections to extract only commercially licensed data. In that case a Collection inherits the most restrictive license and terms of its constituent datasets.

For annotating creator organizations, we follow prior work's instructions [[123]](#). For each dataset they record the affiliations listed on the academic paper or GitHub or HuggingFace object in which the dataset was released. This does not include the organizations who created or owned the sources from which the data was derived. For instance, the SQuAD dataset [[5]](#) would be associated with Stanford (the authors' affiliation), but not Wikipedia, which the data was partially derived from. For a dataset that has authors affiliated with multiple organizations, the dataset will be counted towards each organization.

Annotation Details: Speech In many cases, multiple versions of a dataset exist due to datasets being expanded or updated. In these scenarios, we used the release date from the initial version (since release dates for subsequent versions were not always clear), but used metadata from the most recently released version for which information was available to offer an overview of the current landscape of data. However, if the dataset versions could not be meaningfully aggregated (e.g. different licenses), or did not appear to be cumulatively designed (non-overlapping or otherwise semantically disjoint data), we maintained separate records. We kept only datasets for which ASR was noted as a primary task. For example, if a dataset was primarily intended for text-to-speech or speaker recognition, we did not keep it even if it could conceivably be repurposed for ASR. When computing hours, we excluded any hours without supervisory transcripts/scripts (unlabeled data), but kept hours with "weak supervision" (e.g. model-generated transcripts from speech audio). We recognize the difficulty in comprehensively covering all relevant datasets. Annotation Details: Video In video, a single dataset can be re-purposed and annotated to address different tasks [[18]](#), [[43]](#). We consider these as two different datasets even if they have the same video source since now they can be used for different computer vision tasks. Table [5](#tab_6): Alignment tuning (text) collections and properties. Collection properties include numbers of datasets, tasks, languages, and text domains. The SOURCE column indicates whether a collection contains human-generated web text ( ), language model outputs ( ) or both (

). The USE column indicates whether a collection includes data freely usable even for commercial purposes ( ), data usable only for noncommercial purposes or academic research ( ) and data whose license status is not specified precisely enough to allow us to determine commercial use permissions ( ). Note that each collection may have different datasets with one, two, or all three of these statuses. Finally, the OAI column indicates collections which include OpenAI model generations. Datasets are sorted chronologically to highlight trends over time. Table [6](#tab_7): Audio collections and properties. Collection properties include numbers of audio hours (HR), speakers (SPKR), languages (LANG), creator institutions (CREAT), tasks (TASKS), data sources (SRC), and topics (TOPICS). The number of datasets is not listed because all collections include only one dataset, except for M2ASR which has four. The US column indicates datasets from or partly from the United States, the AC column datasets created by academic institutions, and the IND column datasets created by industry. Note that a dataset can have all of these, none of them, or any combination of them. The USE column indicates whether a collection includes data freely usable even for commercial purposes ( ), data usable only for noncommercial purposes or academic research ( ) and data whose license status is not specified precisely enough to allow us to determine commercial use permissions ( ). Note that each collection may have different datasets with one, two, or all three of these statuses. Datasets are sorted chronologically to highlight trends over time. 

## COLLECTION

## COLLECTION

## F ATTRIBUTION CARD

Here we provide detailed information about the licenses of each data collection and its constituent datasets, and cite all of the papers (455 in all) which introduced datasets we consider. Text datasets are laid out in Table [8](#tab_12), audio datasets in Table [9](#tab_16), and video datasets in Table [10](#tab_19). Because of the large number of references, we include a second bibliography after the tables (named 'Attribution Card References'), with numbered citations in this section referring to that second bibliography. Various [[18]](#), [[34]](#), [[47]](#), [[50]](#), [[152]](#b299), [[174]](#b324), [[202]](#b355), [[210]](#b363), [[225]](#b380), [[250]](#b406), [[337]](#b503), [[368]](#b537), [[430]](#b602), [[431]](#b603), [[436]](#b608), [445] KIWI CC BY-SA 4.0 [452] LMSYS-Chat-1M LMSYS-Chat-1M Dataset License, Anthropic, Llama 2 [455] Llama2-MedTuned-Instr. CC BY-NC 4.0 [407] LongAlign-10k Anthropic, Apache License 2.0 [434] Magpie-Pro Meta Llama3 Community License [453] MathDial CC BY-SA 4.0, MIT License [398] MathInstr. MIT License [422] MedInstr. Unspecified [454] Medical Meadow

Various [[230]](#b385), [[256]](#b412), [[264]](#b420), [[391]](#b561) Continued on next page 

![Figure4: The cumulative totals (left) of languages and countries represented in the data over time, and the 95% confidence intervals of the gini-coefficients over time (right) to measure the representativeness of these variables. Gini-coefficients are a measure of statistical dispersion, frequently used to quantify inequality. A Gini coefficient of 1 indicates highest concentration, and lower values more broad representation. While the number of represented languages and geographies continue to rise (left), the equality of their distribution has in most cases, not significantly changed.]()

![Figure 5: The distribution of creator organizations by modality. Most public speech and video datasets are developed by academic organizations, whereas text datasets are developed by a wide mix of academia, non-profit or industry labs, as well as startups.]()

![Figure 6: The distribution of dataset sizes for each modality. Most text data collections are between 100M-1B tokens. Speech datasets average 100-1k hours, and video datasets are usually the smallest, commonly less than 100 hours.]()

![Figure 7: The task distribution of datasets, across modalities. Post-training text and video datasets are predominantly based on classification. For text, generation and reasoning are rising categories. All speech datasets are recognition-based, particularly for speaker, language, or in the process of translation.]()

![A breakdown of the percentage of license and terms restrictions across datasets, by total tokens or hours of content. The much higher frequency of restrictions at the collection level is because we consider a collection's license or terms status to be the most restrictive of those for its datasets. Note that percentages may not add to exactly 100% because of rounding.]()

![Alignment tuning (text) collections and properties.]()

![Audio collections and properties.]()

![Audio collections and properties.]()

![Video collections and properties. Collection properties include numbers of hours of video, datasets, creator institutions, countries of creator institutions, and data sources. The USE column indicates whether a collection includes data freely usable even for commercial purposes ( ), data usable only for noncommercial purposes or academic research ( ) and data whose license status is not specified precisely enough to allow us to determine commercial use permissions ( ). Note that each collection may have different datasets with one, two, or all three of these statuses. Finally, the AVAIL column indicates whether a dataset is available online ( ) or has been taken down, usually for legal reasons ( ). Datasets are sorted chronologically to highlight trends over time.]()

![Video collections and properties.]()

![Video collections and properties.we break down contributions to this work. Contributors are listed alphabetically, except for team leads who are placed first. â€¢ Text Datasets Shayne Longpre (lead), Jad Kabbara (lead), Ahmad Anis, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Kun Qian, Lester Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Nayan Saxena, Niklas Muennighoff, Naana Obeng-Marnu, Robert Mahari, Seonghyeon Ye, Seungone Kim, Shayne Longpre, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Vu Minh Chien, William Brannon, Xuhui Zhou, Yizhi Li, An Dinh, Caroline Chitongo, Christopher Klamm, Da Yin, Damien Sileo, Ariel Lee â€¢ Reviewing Text Dataset Metadata Jad Kabbara (lead), Shayne Longpre (lead), Robert Mahari, Damien Sileo, Niklas Muennighoff, William Brannon,]()

![References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for alignment-tuning (text) dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for audio dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for video dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

![References and licenses for video dataset collections presented in this paper. Collections containing material under more than three distinct licenses are marked as having "Various" licenses, and we refer readers to our raw data for the full details. Datasets are sorted alphabetically for ease of dataset lookup.]()

The Speech Datasets Collection

openslr.org: Open Speech and Language Resources. OpenSLR is a widely used platform in the speech community, dedicated to hosting resources for speech tasks.

We use top level Glottolog families.

A dataset creator, following[123], is defined as an organization associated with the release of the dataset as created for machine learning-not any of the upstream sources. More details in Appendix D.

YouTube Terms of Service.

Reddit User Agreement and StackOverflow Terms of Service.

We treat the enforceability of licenses and terms as an open legal question, beyond the scope of our work.

