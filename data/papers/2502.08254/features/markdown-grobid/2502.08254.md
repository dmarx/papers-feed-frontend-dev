# UniCoRN: Unified Commented Retrieval Network with LMMs

## Abstract

## 

Multimodal retrieval methods have limitations in handling complex, compositional queries that require reasoning about the visual content of both the query and the retrieved entities. On the other hand, Large Multimodal Models (LMMs) can answer with language to more complex visual questions, but without the inherent ability to retrieve relevant entities to support their answers. We aim to address these limitations with UniCoRN, a Unified Commented Retrieval Network that combines the strengths of composed multimodal retrieval methods and generative language approaches, going beyond Retrieval-Augmented Generation (RAG). We introduce an entity adapter module to inject the retrieved multimodal entities back into the LMM, so it can attend to them while generating answers and comments. By keeping the base LMM frozen, UniCoRN preserves its original capabilities while being able to perform both retrieval and text generation tasks under a single integrated framework. To assess these new abilities, we introduce the Commented Retrieval task (CoR) and a corresponding dataset, with the goal of retrieving an image that accurately answers a given question and generate an additional textual response that provides further clarification and details about the visual information. We demonstrate the effectiveness of UniCoRN on several datasets showing improvements of +4.5% recall over the state of the art for composed multimodal retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.

## Introduction

Multimodal retrieval, the task of retrieving relevant information across text, visual and other modalities, has gained significant attention in recent years [[8,](#b7)[14,](#b13)[21,](#b20)[25,](#b24)[26,](#b25)[28,](#b27)[40,](#b39)[47,](#b46)[53,](#b52)[68,](#b67)[69,](#b68)[82,](#b81)[89,](#b88)[92,](#b91)[95,](#b94)[96,](#b95)[98]](#b97) demonstrating strong performance on a variety of tasks in supervised, zero-shot, and out-of-domain applications. However, recent methods still struggle to capture complex, compositional requests that require specific reasoning about visual content. On the other hand, LMMs [[1,](#b0)[4,](#b3)[18,](#b17)[19,](#b18)[23,](#b22)[41,](#b40)[56,](#b55)[57,](#b56)[62,](#b61)[64,](#b63)[75,](#b74)[81,](#b80)[87,](#b86)[90,](#b89)[99]](#b98) can deal with complex visio-linguistic re- quests with language output, without the inherent ability to retrieve and output relevant images to support the answer. This raises an intriguing question for which we seek to make advancements in our work: How can we integrate the multimodal reasoning capabilities of LMMs with the retrieval capabilities of composed multimodal models?

Let us consider a user asking "what is the larval state of this butterfly?" as in Fig. [1](#fig_0). The model must not only recognize the species of butterfly, but also understand the query and find an image depicting the corresponding caterpillar stage, which has little visual similarity with the query. While prior work [[2,](#b1)[6,](#b5)[16,](#b15)[29,](#b28)[32,](#b31)[33,](#b32)[36,](#b35)[37,](#b36)[78]](#b77) has explored composed retrieval techniques for such structured requests, these approaches have limitations in providing comprehensive responses. On the other hand, Visual Question Answering (VQA) methods [[3, 5, 30, 34, 70-72, 86, 88]](#), even when leveraging LMMs, are restricted to answering solely with language. An ideal model should both retrieve a relevant image and generate explanatory text that complements and further explains the visual information. Referring back to Fig. [1](#fig_0), the model returns an image of the butterfly's larval stage and a comment, e.g. "The larval stage of the Old World swallowtail is a caterpillar, which can be seen feeding on a wild carrot plant in the image.". We refer to this novel multimodal task as Commented Retrieval (CoR).

In this work, we propose UniCoRN, a novel Unified Commented Retrieval Network that leverages the complementary strengths of discriminative cross-modal matching and generative language models to perform CoR. First, we present a retrieval module that aligns the hidden state of LMMs with the retrieval space of CLIP models. This allows us to integrate the deep visio-linguistic reasoning of LMMs with powerful multimodal and cross-modal retrieval capabilities. The retrieval model is trained to be aware of the complex answers and comments, rather than simple image captions like in previous work [[42,](#b41)[55,](#b54)[83]](#b82). Second, we introduce an entity adapter module to inject the retrieved entities back into the LMM, so they can condition the generated answers and comments. This technique enables interleaved, integrated multimodal output at inference time, which goes beyond post-processing of output tokens or prompt engineering of the LMM, e.g. with RAG [[51]](#b50). By training these new modules for CoR, we strengthen the connection between the initial question and the retrieved entities, enabling the generation of more coherent and relevant textual responses. Notably, our contributions are additions to a frozen LMM, so, unlike [[42,](#b41)[55]](#b54), we guarantee the preservation of all its original capabilities (such as captioning, VQA, grounding, and more) within our unified framework.

Existing multimodal tasks and datasets [[17,](#b16)[38,](#b37)[45,](#b44)[59,](#b58)[85]](#b84) have simplistic (1-3 words) answers that do not explain why and how the retrieved entity is relevant to the initial query, and thus are not well suited for training and evaluating models for CoR. In this work, we introduce two challenging human-curated CoR datasets based on the CIRR [[58]](#b57) and WikiWeb2M dataset [[11]](#b10), that blend aspects of composed retrieval and VQA. Given an input image and question, the goal is to retrieve from a large candidate pool the entity that answers the question, and produce an additional textual response that offers further clarification and details (Fig. [1](#fig_0)). This task echoes research showing that humans learn better when they can jointly examine textual responses alongside the relevant visuals [[61]](#b60).

Our evaluations on diverse datasets for composed retrieval and commented retrieval show significant improvements of UniCoRN over state-of-the-art models. In particular, we show an average improvement for composed retrieval of +4.5% on Fashion-IQ, CIRR, OVEN, InfoSeek, and the proposed Wiki-CoR dataset in terms of recall when compared to UniIR [[83]](#b82). Moreover, we show an improvement of +14.9% in terms of METEOR score over a RAG approach combining UniIR and InternVL2 on the CoR task.

## Related Work

Instructable Retrieval. Multimodal representation for retrieval [[8,](#b7)[14,](#b13)[25,](#b24)[40,](#b39)[47,](#b46)[53,](#b52)[68,](#b67)[69,](#b68)[82,](#b81)[89,](#b88)[92,](#b91)[95,](#b94)[96,](#b95)[98]](#b97) has been a prominent area of research demonstrating strong performance. To handle compositional requests that require reasoning about the visual content, recent work focused on composed retrieval [[6,](#b5)[7,](#b6)[15,](#b14)[29,](#b28)[31,](#b30)[44,](#b43)[60,](#b59)[73,](#b72)[77,](#b76)[79,](#b78)[80,](#b79)[84]](#b83). However, these methods focus on limited specific retrieval domains and can only follow predefined instruction templates. To address this limitation, recent models have been made instructable [[42,](#b41)[44,](#b43)[49,](#b48)[55,](#b54)[66,](#b65)[83,](#b82)[97]](#b96) to capture richer multimodal relationships. MagicLens [[97]](#b96) is a simple instructable model trained on a curated large dataset including instructions, while UniIR [[83]](#b82) proposed training CLIP/BLIP variants conditioned on prompted instructions. While these approaches have demonstrated significant progress, they lack expressiveness in the multimodal output space due to the limitations of CLIP models. In contrast, methods based on LMMs [[42,](#b41)[55]](#b54) that are fine-tuned for retrieval do not preserve the original capabilities of the LMMs. UniCoRN aims to overcome this issue and preserve capabilities by design by enabling a frozen LMM to retrieve relevant content and generate textual responses tailored to both the input question and the retrieved visual content. Retrieval via Generation. LMMs [[1, 4, 18, 19, 23, 41, 55-57, 62, 64, 75, 81, 87, 90, 99]](#) have demonstrated remarkable reasoning capabilities when processing language and visual elements (see survey in [[91]](#b90)). Recently, LMMs were combined with diffusion models to generate images along with textual answers [[27,](#b26)[46,](#b45)[52,](#b51)[74,](#b73)[76,](#b75)[87,](#b86)[93]](#b92). Despite providing an interesting user experience, the generated images are by nature not grounded to real entities, which is critical for retrieval applications, such as online shopping, news, or Wikipedia. These models are based on memoryintensive diffusion models and trained with a reconstruction objective that is not in line with retrieval. In contrast, Uni-CoRN uses a retrieval approach that preserves the factuality of the images that are shown to the user, while commenting them in a generative way. It is lightweight, as it does not use diffusion models, and the LMMs are frozen, thus preserving their reasoning capabilities of other non-retrieval tasks. Retrieval-Augmented Generation (RAG). The parametric way that language models are trained and store knowledge limits the ease to expand or revise their memory with more recent information. RAG [[24,](#b23)[35,](#b34)[43,](#b42)[48,](#b47)[50]](#b49) and Multimodal RAG [[12,](#b11)[13,](#b12)[39,](#b38)[66,](#b65)[67,](#b66)[88,](#b87)[94]](#b93) have been proposed as a non-parametric way to provide up-to-date knowledge to an LMM by adding relevant content to the prompt before processing by the LMM. Using this augmented prompt, the LMM can select, summarize and alter the entities to generate its output. UniCoRN differs from traditional RAG in that it processes user input via the LMM to build a better query and outputs the retrieved entity unaltered. Then, Uni-CoRN's entity adapter module is trained to feed optimized entity representations to the LMM, which guides the extraction of the information necessary to answer the request. Commented Retrieval (CoR). CoR is an underexplored task that requires not only retrieving relevant images, as in composed retrieval, but also generating plausible textual answers that refer to the image and contain additional information. While preliminary work [[42,](#b41)[44,](#b43)[66,](#b65)[83]](#b82) and datasets [[17,](#b16)[38,](#b37)[63]](#b62) focuse on some aspects of this task, they have two key limitations: 1) The complexity of the textual answers is constrained, typically limited to a few words or basic captions. 2) The textual answers are aligned with the image, either providing redundant information (e.g., image captions) and failing to add useful complementary information. To fully cover the CoR task, we claim that the answers should be multimodal in nature, with the image and text working in tandem to provide a richer, more comprehensive response. The modalities should have a clear connection, yet offer complementary information that enhances the overall response. To alleviate the limitations of existing CoR datasets, we created two challenging human-curated ones that we hope will foster future research in this area.

## UniCoRN

With UniCoRN, we propose to expand the capabilities of a frozen base LMMs with two novel interconnected modules: 1) Comment-aware retrieval, which is responsible for understanding the visio-linguistic representation of the query question and image, and projecting it to the target comment and image space for retrieval (Sec. 3.1); 2) Retrieval-aware generation, which learns to leverage the retrieved entity to generate an answer to the user question (Sec. 3.2).

## Comment-aware Retrieval

Given a query text and image q = (q ⟨t⟩ , q ⟨i⟩ ), we aim to train a retrieval model R θ , with parameters θ, that is capable of understanding q and retrieve a related target entity composed of text and image d = (d ⟨t⟩ , d ⟨i⟩ ), such that:

$d = argmax d ′ ∈D R θ (q, d ′ ) (1$$)$where D is a database of multimodal entities (a.k.a documents). The best definition for the retriever R θ depends on the target application. For cross-modal and multimodal retrieval, CLIP-like models [[25,](#b24)[26,](#b25)[40,](#b39)[68,](#b67)[83]](#b82) are the defacto choice based on their performance in diverse scenarios. However, they are limited in the visio-linguistic understanding capabilities, especially when moving to more complex applications like composed retrieval and commented retrieval. To address this limitation, we leverage the reasoning capabilities of LMMs in the training process of the retriever integrated with CLIP, as shown in Fig. [2](#fig_1).

The multimodal query q is fed into the base LMM to obtain its output hidden state h = H LMM (q). In parallel, the query is fed into a multimodal encoder ψ MM (q) = ψ ⟨t⟩ (q ⟨t⟩ ) + ψ ⟨i⟩ (q ⟨i⟩ ), where ψ ⟨t⟩ and ψ ⟨i⟩ are text and image CLIP-like encoders, respectively. Then, we define a Hidden State Adapter ψ LMM to map h into the same space as ψ MM , as follows: where FCs are fully connected layers and GeLU is the activation function. The final embedding for q combines the CLIP-based embeddings and the adapter output:

$ψ LMM (h) = FC(GeLU(FC(h)))(2)$$ψ(q) = β • ψ LMM (H LMM (q)) + (1 -β) • ψ MM (q) (3)$where β is a learnable weight part of θ. Note, this scorefusion approach to CLIP is inspired by UniIR [[83]](#b82), where the authors show this is one of the most effective ways to combine multimodal inputs for multimodal retrieval. Finally, we define R θ as the dot product of query and document embeddings: R θ (q, d) = ψ(q) T • ψ MM (d). As shown in Fig. [2](#fig_1), the multimodal target documents are not using the LMM and therefore only use the multimodal encoder ψ MM . This asymmetrical approach between query and document has several advantages: 1) it saves cost during the indexing phase by avoiding to use an expensive LMM on the potentially large pool of multimodal documents, and 2) it bridges the domain gap between the queries and the database via fine-tuning of the adapter. In contrast with other methods that use LMMs for indexing [[42,](#b41)[55]](#b54), our method keeps indexing lightweight.

We train our retriever using contrastive loss [[68]](#b67). Unlike previous work [[42,](#b41)[55,](#b54)[83]](#b82), we combine target captions and more complex target comments that cover aspects beyond the query and target images, as we will describe in Sec. [4](#b3). This makes our model aware of complex comments during training and thus will better fit with retrieval-aware generation as described in Sec. 3.2.

## Retrieval-aware Generation

We now detail how we enable an LMM to ingest the query question and image, encode retrieved entities, and generate an answer to the question. The process is shown in Fig. [3](#fig_2).

## Retrieved Entities as Input Modality

In essence, a Large Language Model (LLM) predicts the probability of the next token τ n ∈ {1, . . . , V }, where V is the vocabulary size, given an input sequence of n tokens: 

LMMs [[19,](#b18)[56]](#b55) extend this principle by adapting visual data into input image tokens τ ⟨i⟩ or embeddings ϕ ⟨i⟩ . Notably, Eq. 4 is unchanged for LMMs, except that the inputs are now a mix of text ϕ ⟨t⟩ and image ϕ ⟨i⟩ embeddings in R d .

As described in Sec. 3.1, the retriever finds the most relevant multimodal document d m in the database based on the multimodal query q and the projected hidden state of the LMM. As an important contribution of our method, we then inject d m into the LMM as a new input modality ⟨r⟩ in order to condition the rest of the generation process, and therefore ensure that the model attends to it while generating the textual comment. This is important because of the finite nature of any database: the retrieved documents can substantially differ from the vector used as query. Thus, using solely the user input to condition the generation may lead to comments that are irrelevant to the retrieved entity.

Similarly to other input modalities, features of the retrieved document are extracted and adapted into the input embedding space of the LMM:

$φ ⟨r⟩ (d m ) = [ϕ ⟨r⟩ m,0 , . . . , ϕ ⟨r⟩ m,lm ] = ϕ ⟨r⟩ m ∈ R d×lm ,(5)$where l m is the length of the representation of the document d m . In this way, the LMM can attend to the retrieved data in addition to all previous inputs as it continues generation:

$p(τ n | • • • , ϕ ⟨t⟩ k , • • • , ϕ ⟨i⟩ p , • • • , ϕ ⟨r⟩ m , • • • ).(6)$However, the frozen LMM was not trained to distinguish entities that are retrieved from a database and treat them ap-propriately to generate a convincing answer. Thus, we make the entity adapter a trainable module as described next.

## Entity Adapter Module

The definition of φ ⟨r⟩ (d m ) depends on the nature of the dataset and the entities it contains: text, images, audio, video, pre-computed features, or any combination thereof.

We propose to design an Entity Adapter module φ )). In our datasets, the textual components are natural text, so we provide them directly to the LMM, without fine-tuning the token embeddings, i.e., φ ⟨t⟩ ξ (d m ) = ϕ ⟨t⟩ . For the visual part, we use the same architecture as the base LMM [[19]](#b18), with the adapter:

$φ ⟨i⟩ ξ (d m ) = FC(GeLU(FC(LayerNorm(x m )))),(7)$where {x m } are the ViT image features of the variable number of tiles of the image of d ⟨i⟩ m .

## Training

To train our novel Entity Adapter, we leverage a dataset D of ground-truth multimodal triplets (q, d, c), where q is the multimodal query, d is the ground-truth target document, and c is the ground-truth answer that relates the query and the target. The training of ξ is performed for the task of next token prediction of c, using the cross-entropy loss:

$L CE (ξ) = (q,d,c)∈D τn∈c -δ n . log(p n (ξ)),(8)$where

$p n (ξ) = p(τ n |ϕ ⟨i⟩ q , ϕ ⟨t⟩ q , φ ⟨r⟩ ξ (d), ϕ ⟨t⟩ 0 , . . . , ϕ ⟨t⟩ n-1 )$, and δ n is the ground-truth distribution for τ n . As shown in Fig. [3](#fig_2), in practice, all other parts of the model are kept frozen during training and only the adapter in Eq. 7 is trained. For faster convergence, this adapter is initialized from the pretrained visual adapter of the original LMM.

With this approach, despite keeping the base LMM frozen, we are achieving two objectives: 1) we align the multimodal entities with the LMM input embeddings, thus helping the LMM extract the relevant information from the entities and closing any domain gap between the data used to pre-train the LMM and the retrieval dataset, and 2) we encode commenting instructions in the adapter, thus optimizing the inputs to the LMM to bias it to generate useful, high-quality comments for documents of this dataset. As a consequence, we train separate, specialized Entity Adapters for different tasks and datasets.

## Commented Retrieval (CoR) Task

As already discussed in related work, the related task of composed retrieval is well explored in the literature, and there exist a number of datasets for it. Some datasets for this task, e.g., CIRR [[58]](#b57) and Fashion-IQ [[33]](#b32), include multimodal queries, while the targets are only images. Other relevant datasets, e.g., OVEN [[38]](#b37) and InfoSeek [[17]](#b16), have both multimodal queries and targets, however the connection between inputs and outputs are simple high-level entities. Overall, existing datasets either lack target text altogether or feature overly simplistic textual responses that are directly related to the target image. These datasets are not well-suited to evaluate the full extent of the commented retrieval task, where the goal is not only to retrieve a relevant target image, but also to generate a detailed, complex textual response that refer to both the query and target image. Therefore, we propose a technique to automatically augment some existing datasets with textual answers and comments to the multimodal questions and image-only answers. In this work, we focus on 2 different domains, images of real-world object categories with CIRR [[58]](#b57) and Wikipedia concepts with WikiWeb2M [[10]](#b9), but the approach itself generalizes to other domains.

The proposed approach to obtain a tuple of a query image, a textual question, a target image and a textual answer for CoR is composed by three parts: 1) pair related images; 2) generate questions and answers related to those images; and 3) manual annotation to create a golden set for evaluation. We couple images that belong to similar concepts, which definition depends on the task. For CIRR, image pairs are already present in the original set, as images were grouped together based on visual similarity. For Wiki-Web2M, we pair images belonging to the same Wikipedia article, because that denotes a semantic link. We subsample a set of 50K samples for training and 2K for testing.

In the second step, we aim at generating questions and answers that can relate the two images, with the goal of simulating the behavior of a user that asks a question about an uploaded image, and obtain an answer with text and the target image supporting the text. We automatically generate these questions and answers by prompting using Anthropic Claude 3.5 Sonnet. For CIRR, the question is the textual modification instruction that is already included in the original set, so we only generate the textual answer. For WikiWeb2M, we generate a question-answer pair that is related to the query and target images, their captions and the Wikipedia article. This task is more challenging and results in noisier data. We name these datasets CIRR-CoR and Wiki-CoR, respectively, and show examples 1 in Fig. [5](#fig_6).

Finally, we create golden sets for evaluation purposes by 1 The Supplementary Material shows more examples of the datasets and the prompts used to generate them. asking annotators to validate the plausibility of the question and the quality of the generated text, manually adjusting them if necessary. We performed human auditing of the generated comments on the validation set of CIRR-CoR and found that ∼ 97% of the audited comments were marked as high quality. For the test set of Wiki-CoR, we conducted a manual review by filtering out image pairs that are not related to each other, questions that are irrelevant and comments that are not aligned with the question. In addition, annotators performed the necessary edits of the question and answer pairs to improve their quality. This results in a golden set of 695 out of 1768 (∼ 39%), showing the importance of having an annotation process for evaluation, that is often not considered in previous datasets.

## Experiments

We evaluate the multimodal retrieval and commenting capabilities of our methods with respect to the state of the art in Sec. 5.1 and Sec. 5.2, respectively.

## Retrieval Results

Implementation Details. We choose InternVL2-4B [[19]](#b18) as the base, frozen LMM and use it to extract hidden states H LMM (q) corresponding to queries. Specifically, we take the 3072-dimensional hidden state of the last transformer layer after processing of the last input token before generating the response. We train the encoder ψ MM and adapter ψ LMM using the UniIR framework [[83]](#b82) so as to align the queries and targets using contrastive loss [[40]](#b39) in two stages. First, we train the hidden state adapter exclusively to align them to UniIR embeddings, so we can obtain a good initialization. Second, as depicted in Fig. [2](#fig_1), we additionally unlock parameter β and the multimodal encoder, initialized from a pre-trained CLIP ViT-L-14 model [[68]](#b67). We append instructions in the query as proposed by [[83]](#b82) and train a joint model on all 5 datasets[foot_0](#foot_0) , i.e Fashion-IQ, CIRR, OVEN, InfoSeek and Wiki-CoR. For Wiki-CoR, we randomly sample comment or caption as text with 50%/50% chance. We train for 40 epochs using learning rate 1e-5, batch size 55, effectively 110 thanks to gradient accumulation of 2, on a single host with 8×48GB NVIDIA L40S GPUs.

Metrics. We use standard recall@k metrics to measure the retrieval performance: we compute the neighbourhoods of the query vectors and verify if they contain the target image specified in the ground-truth annotations of the datasets. We compare with pretrained CLIP ViT-L-14 [[68]](#b67) and the state-of-the-art model UniIR CLIP SF [[83]](#b82).

## Fashion-IQ [33]

CIRR [[58]](#b57) OVEN [[38]](#b37) InfoSeek [[17]](#b16) Wiki-CoR Average R@10 R@20 R@50 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Results. The main results are shown in Table [1](#tab_0). First, we observe a low zero-shot performance on all datasets. This is in line with observations from previous work on composed retrieval [[83,](#b82)[97]](#b96), which showed that CLIP models cannot easily compose multimodal features to match multimodal answers that are semantically different from the original query. Second, we see that UniCoRN is able to leverage the hidden states of the LMM, leading to improved performance, especially for the first ranks (recall@1 and 5), over all datasets but InfoSeek. We also show qualitative results of UniIR and UniCoRN in Fig. [4](#fig_4). In the first example, UniIR retrieves visually similar traditional clothing and headwear. Instead, UniCoRN correctly retrieves art forms from Bali and Java (mentioned in the captions), consistent with the query (Indonesia). In the challenging second example, the model needs to infer from the query image that it's a hair dryer and connect the textual "show in use". While UniIR fails, UniCoRN correctly retrieves the ground truth image on rank 3. Also the other examples on CIRR and OVEN show that UniCoRN is able to combine query and image text in an abstract way to retrieve visually dissimilar correct targets, while UniIR focuses on visual similarity.

Ablation Study. Table [2](#tab_1) shows an ablation study, where we train our model without Wiki-CoR and we remove the LMM adapter. For this analysis, we study recall@1 because this will be the retrieved image that the LMM will comment (Sec. 5.2). The inclusion of our adapter (row 1 vs 2) enhances performance across both datasets, leveraging LMM hidden states. This improvement comes at a negligible computational cost during inference since the LMM hidden state is computed already, highlighting our model's efficiency and effectiveness. When we do not train on Wiki-CoR data, we observe a big drop in performance, highlighting how distinctive Wiki-CoR is from other Wikipedia datasets such as OVEN and InfoSeek. Our model convincingly outperforms the zero-shot baseline (row 1 vs 4), showcasing its robust generalization capabilities and underscores the value of our training approach.

## Commenting Results

Implementation Details. To implement retrieval-aware commenting, we need to modify standard LMM procedures, because we perform retrieval during generation and output images interleaved with text. We do so by extending the generation function of LMMs with the following changes. First, we specify a retrieval token, the prediction of which will trigger the retriever R θ . We use the startof-sentence tag, as our commented retrieval datasets do not exhibit comments prior to retrieval. The retriever yields a reference to the target entity, which we process via feature extraction and the Entity Adapter to make the LMM understand the retrieved content. We convert all previously predicted output tokens to input embeddings and append the embedding of the retrieved entity, then feed this as input to the LMM for predicting the next tokens, until the end-ofsentence token is detected.

In addition, we train our Entity Adapters specifically so that InternVL2-4B processes retrieved content differently than content input by a user. We therefore fine-tune the Entity Adapters separately on CIRR-CoR and Wiki-CoR. For CIRR-CoR, we have 28,225 valid training conversations, where the retrieved entities are simply the target images. For Wiki-CoR, we have 50,126 valid training conversations, where the retrieved entities are Wikipedia images with caption, as well as metadata from the corresponding Wikipedia page (title, description). We train both adapters using the InternVL2 framework [[19]](#b18) for two epochs on a single host with 4×L40S GPUs, using a learning rate of 1.0e -4 , and evaluate on their respective evaluation sets as we described in Sec. 4.

## Metrics.

We evaluate the predicted comments with respect to the corresponding ground-truth (GT) comments in the golden test set using standard metrics for language understanding: i) METEOR [[20]](#b19), the harmonic mean of word precision and recall that takes into account stemming and synonymy; ii) BLEU [[65]](#b64), a metric with high correlation with human judgments of translation quality; iii) ROUGE-1 [[54]](#b53) (resp. ROUGE-2) measuring the overlap of words (resp, bi-grams) between the two comments. 

## CIRR-CoR

Wiki-CoR

$METEOR BLEU ROUGE-1 ROUGE-2 BEM SigLIP-R@1 METEOR BLEU ROUGE-1 ROUGE-2 BEM SigLIP-R@1$InternVL2 [19] 0.192 0.013 0.280 0.082 11.9% 41.4% 0.128 0.009 0.237 0.089 31.8% 49.1% EchoSight [88] + InternVL2 0.311 0.038 0.397 0.119 31.6% 41.0% 0.204 0.018 0.276 0.065 16.2% 32.1% UniIR CLIP SF [83] + InternVL2 0.313 0.039 0.399 0.121 32.0% 42.6% 0.212 0.025 0.293 0.078 21.4% 41.2% UniCoRN 0.444 0.115 0.516 0.280 50.4% 45.1% 0.379 0.128 0.446 0.196 39.3% 66.3% UniCoRN retriever + RAG 0.318 0.041 0.400 0.123 33.7% 43.3% 0.236 0.037 0.323 0.098 28.7% 55.2% UniCoRN w/o trained adapter 0.317 0.029 0.346 0.106 28.8% 33.7% 0.264 0.033 0.285 0.089 23.7% 52.4% UniCoRN w/ shared adapter 0.407 0.060 0.392 0.181 55.9% 42.7% 0.374 0.127 0.447 0.195 39.3% 67.6% UniCoRN w/ UniIR CLIP SF 0.439 0.113 0.510 0.276 49.8% 44.0% 0.347 0.106 0.411 0.168 34.7% 55.4% UniCoRN w/ GT target oracle 0.462 0.130 0.534 0.297 56.1% 55.9% 0.443 0.176 0.517 0.258 55.8% 92.1% Table 4. Modality ablation study for entity encoding. We compare results on Wiki-CoR using UniCoRN and RAG when removing images, captions and Wikipedia descriptions when providing the entities to the model.

iv) BEM [[9]](#b8), a BERT-based [[22]](#b21) measure to evaluate the equivalence of answers to a question. We also use a separate vision-language model, namely ViT-SO400M-14-SigLIP-384 [[96]](#b95), to measure if the predicted comment can be used to retrieve the GT comment with high accuracy, thus measuring if they are specific and distinct from other comments. For this approach, we use again recall@1.

Results. In the top section of Table [3](#tab_3), we report the results for UniCoRN as well as three state-of-the-art LMM approaches that can produce comments based on multimodal inputs: i) InternVL2 [[19]](#b18), a state-of-the-art LMM that is prompted to generate a comment for the unseen image based only on the multimodal user query; ii) EchoSight [[88]](#b87), a state-of-the-art multimodal Retrieval-Augmented Generation (RAG) and re-ranking approach, using InternVL2 as the base LMM for comment generation. iii) a system with the state-of-the-art compositional retriever UniIR [[83]](#b82) that we use to build a multi-image multi-modal RAG prompt for InternVL2; The results show that InternVL2 is able to generate comments that contain relevant information (METEOR 0.192/0.128 on CIRR-CoR/Wiki-CoR resp.). This entails that the LMM is able to interpret the user query, justifying that its hidden state has indeed the potential to contribute to the retrieval component. However, the comments are not very similar to the ground-truth: this is clearly due to the absence of retrieval capabilities in InternVL2, which thus has to produce This image shows a size comparison of Quilmesaurus with other carnotaurins. Quilmesaurus appears to be relatively small compared to the other dinosaurs depicted.

It is shown as a smaller figure in the group, with the other carnotaurins being larger in size. This suggests that Quilmesaurus was a smaller member of its family compared to some of its relatives.

The second image is a scale diagram of Rajasaurus, a carnivorous abelisaurid theropod dinosaur, compared to a human. The first image is a full-body illustration of the same dinosaur. The second image provides a scale reference, showing that Rajasaurus was approximately 6.6 meters long, with a single horn on its forehead.

Add 8 chairs and a car.

The image shows a larger room with more chairs and a car visible through the windows compared to the other image.

The second image differs from the first by having 8 additional chairs and a car. a plausible comment without having access to any information about the content present in the retrieval dataset. Using EchoSight [[88]](#b87) or UniIR [[83]](#b82) to retrieve an entity and providing it in the prompt for InternVL2 improves results significantly on all metrics (e.g., METEOR 0.313/212), with a slight edge to the compositional retriever (UniIR) over the reranking one (EchoSight). This proves that conditioning the generation on actual retrieved images is critical to ensure the relevance of the comments, and explains the broad success of RAG methods. Then, using UniCoRN, we further improve to 0.444/0.379 METEOR. Notably, the improvements are consistent over all metrics on both datasets. These results show the effectiveness of our approach: without losing any other ability of InternVL2, UniCoRN is able to optimize its processing of user queries and retrieval results so as to output comments that are significantly better than static prompts built via RAG.

Ablation Study. In the middle section of Table [3](#tab_3), we further analyze the main contributions to the performance of UniCoRN. First, when using UniCoRN's retrieval results in a RAG prompt for InternVL2, we observe only a slight improvement over UniIR+RAG. Similarly, using UniCoRN's commenting abilities on UniIR retrieval results performs only slightly worse than our full UniCoRN. Thus, the improvements in retrieval are only a part of the contribution to the commenting performance. In contrast, using the pretrained input visual encoder from InternVL2 to encode retrieved documents does indeed drop the performance to the level for RAG. This highlights that our contribution of training dataset-specific entity embeddings makes the biggest contribution in making the base LMM provide more relevant and well-formatted comments. Training a share entity encoder leads to mixed results, with a slight improvement on BEM and SigLIP at the expense of other metrics.

In Table [4](#), we further decompose the contribution of the various modalities available in the retrieved documents.

Since CIRR-CoR only has target images, we focus on Wiki-CoR, in which retrieved documents contain an image and its caption on Wikipedia, as well as the title and description of the corresponding page in Wikipedia as metadata. UniCoRN, like RAG, uses all information by default in our experiments (to build the Entity encoding, resp. the prompt). Using the UniCoRN retriever and the trained adapter, we show the drop of performance that occurs when removing, in turn, i) the Wikipedia title and description: -12.1% METEOR, relatively; ii) all textual inputs (image-only): -19.1%; iii) image and Wikipedia metadata (caption-only): -13.1%. Since the drops are consistent for all metrics and both comment generation models (RAG and UniCoRN) based on the same inputs [3](#foot_1) , we can therefore conclude that all three are needed to provide relevant answers.

Finally, we analyze the qualitative performance of UniIR with RAG and UniCoRN in Fig. [5](#fig_6) with success cases for UniCoRN. The first row shows an example where both models retrieve the correct image. However, the comments provided by RAG do not answer the question correctly, as indicated by the part of the comment highlighted in red. The second row presents an example where the UniIR model was unable to retrieve the correct image, and therefore it could not answer the question correctly. This is similar to the third example, where the UniIR model retrieved the wrong query image, yet provided a comment that follows the instructions but does not actually correspond to the retrieved image.

## Conclusions

This work introduces UniCoRN, a Unified Commented Retrieval Network that unifies discriminative retrieval and generative commenting. We combined the strengths of composed multimodal retrieval methods with gen-erative language capabilities, resulting in a system that surpasses traditional RAG techniques.

Our approach preserves the original capabilities of the base LMM while extending its functionality to perform both retrieval and text generation tasks within a single framework. We introduced the CoR task and a corresponding dataset to evaluate these new abilities of LMMs. Future work could focus on further improving the integration of multimodal information, exploring additional tasks, and investigating the scalability of our approach.

![Figure 1. Commented retrieval. Given an query image and question, UniCoRN can retrieve an image from a database and can produce a textual answer that offers further clarification and details.]()

![Figure 2. Comment-aware Retrieval. The query is inputed to both a CLIP-trained image-text encoder and an LMM. The LMM representation is projected to the space of the image-text encoder. Alignment of query and targets is done using contrastive loss.]()

![Figure 3. Retrieval-aware Generation. The query image and text are fed to the LMM, which asks the retriever for relevant entities.The best entity is provided to the user and adapted into the LMM, so it can attend to it for generating a useful comment.]()

![⟨r⟩ ξ (d m ), as shown in Fig. 3, with parameters ξ, which can be finetuned on training data. More specifically for our CoR tasks, the entities are images, or multimodal combinations of images and text data (e.g. titles, captions, ...), that are used to generate the answer. As a consequence, the Entity Adapter treats the various modalities differently, i.e., φ ⟨r⟩ ξ (d m ) = (φ ⟨i⟩ ξ (d ⟨i⟩ m ), φ ⟨t⟩ ξ (d ⟨t⟩ m]()

![Figure 4. Qualitative results retrieval. We show retrieved images for UniIR and UniCoRN on three datasets. Captions are not displayed because of space limits.]()

![This image shows St. Andrew's College's clock tower. The tower is a prominent stone structure with a clock face visible on the side. It appears to be a tall, rectangular building with arched openings and a flag flying at the top. This clock tower is a significant architectural feature of the college, standing tall and visible from a distance.The second image is a close-up of the clock tower at St. Andrew's College, while the first image shows the entire building, including the chapel and surrounding area. The second image focuses on the clock tower, highlighting its architectural details and the flag flying atop it.]()

![Figure 5. Qualitative results. We show retrieved images and comments for UniIR with RAG and UniCoRN on two different datasets. Comments highlighted in red indicate responses that either do not answer the original question or are not related to the retrieved image.]()

![Main results on retrieval. Retrieval results in terms of recall on different datasets while comparing with state of the art methods.]()

![Ablation study on retrieval. Retrieval results in terms of recall@1 on different dataset.]()

![Main results and ablation study on commenting. Commenting results by computing several language-based metrics between the predicted comments and the ground-truth (GT) ones on the CIRR-CoR and Wiki-CoR dataset. (Top) Comparison with the state of the art; (Middle) Ablation study for retrieval and generation in UniCoRN; (Bottom) Upper-bound using GT target documents as an oracle.]()

Fashion-IQ, CIRR, OVEN, and InfoSeek are a subset of M-BEIR[[83]](#b82) that has multimodal inputs.

Except the caption-only version of RAG for the METEOR score, which does better without the image.

