<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UniCoRN: Unified Commented Retrieval Network with LMMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-12">12 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Jaritz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sabine</forename><surname>Sternig</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
						</author>
						<title level="a" type="main">UniCoRN: Unified Commented Retrieval Network with LMMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-12">12 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">952DCFA7CA4D9976F98220BA840E8353</idno>
					<idno type="arXiv">arXiv:2502.08254v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal retrieval methods have limitations in handling complex, compositional queries that require reasoning about the visual content of both the query and the retrieved entities. On the other hand, Large Multimodal Models (LMMs) can answer with language to more complex visual questions, but without the inherent ability to retrieve relevant entities to support their answers. We aim to address these limitations with UniCoRN, a Unified Commented Retrieval Network that combines the strengths of composed multimodal retrieval methods and generative language approaches, going beyond Retrieval-Augmented Generation (RAG). We introduce an entity adapter module to inject the retrieved multimodal entities back into the LMM, so it can attend to them while generating answers and comments. By keeping the base LMM frozen, UniCoRN preserves its original capabilities while being able to perform both retrieval and text generation tasks under a single integrated framework. To assess these new abilities, we introduce the Commented Retrieval task (CoR) and a corresponding dataset, with the goal of retrieving an image that accurately answers a given question and generate an additional textual response that provides further clarification and details about the visual information. We demonstrate the effectiveness of UniCoRN on several datasets showing improvements of +4.5% recall over the state of the art for composed multimodal retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimodal retrieval, the task of retrieving relevant information across text, visual and other modalities, has gained significant attention in recent years <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b97">98]</ref> demonstrating strong performance on a variety of tasks in supervised, zero-shot, and out-of-domain applications. However, recent methods still struggle to capture complex, compositional requests that require specific reasoning about visual content. On the other hand, LMMs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b98">99]</ref> can deal with complex visio-linguistic re- quests with language output, without the inherent ability to retrieve and output relevant images to support the answer. This raises an intriguing question for which we seek to make advancements in our work: How can we integrate the multimodal reasoning capabilities of LMMs with the retrieval capabilities of composed multimodal models?</p><p>Let us consider a user asking "what is the larval state of this butterfly?" as in Fig. <ref type="figure" target="#fig_0">1</ref>. The model must not only recognize the species of butterfly, but also understand the query and find an image depicting the corresponding caterpillar stage, which has little visual similarity with the query. While prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b77">78]</ref> has explored composed retrieval techniques for such structured requests, these approaches have limitations in providing comprehensive responses. On the other hand, Visual Question Answering (VQA) methods <ref type="bibr">[3, 5, 30, 34, 70-72, 86, 88]</ref>, even when leveraging LMMs, are restricted to answering solely with language. An ideal model should both retrieve a relevant image and generate explanatory text that complements and further explains the visual information. Referring back to Fig. <ref type="figure" target="#fig_0">1</ref>, the model returns an image of the butterfly's larval stage and a comment, e.g. "The larval stage of the Old World swallowtail is a caterpillar, which can be seen feeding on a wild carrot plant in the image.". We refer to this novel multimodal task as Commented Retrieval (CoR).</p><p>In this work, we propose UniCoRN, a novel Unified Commented Retrieval Network that leverages the complementary strengths of discriminative cross-modal matching and generative language models to perform CoR. First, we present a retrieval module that aligns the hidden state of LMMs with the retrieval space of CLIP models. This allows us to integrate the deep visio-linguistic reasoning of LMMs with powerful multimodal and cross-modal retrieval capabilities. The retrieval model is trained to be aware of the complex answers and comments, rather than simple image captions like in previous work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b82">83]</ref>. Second, we introduce an entity adapter module to inject the retrieved entities back into the LMM, so they can condition the generated answers and comments. This technique enables interleaved, integrated multimodal output at inference time, which goes beyond post-processing of output tokens or prompt engineering of the LMM, e.g. with RAG <ref type="bibr" target="#b50">[51]</ref>. By training these new modules for CoR, we strengthen the connection between the initial question and the retrieved entities, enabling the generation of more coherent and relevant textual responses. Notably, our contributions are additions to a frozen LMM, so, unlike <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55]</ref>, we guarantee the preservation of all its original capabilities (such as captioning, VQA, grounding, and more) within our unified framework.</p><p>Existing multimodal tasks and datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b84">85]</ref> have simplistic (1-3 words) answers that do not explain why and how the retrieved entity is relevant to the initial query, and thus are not well suited for training and evaluating models for CoR. In this work, we introduce two challenging human-curated CoR datasets based on the CIRR <ref type="bibr" target="#b57">[58]</ref> and WikiWeb2M dataset <ref type="bibr" target="#b10">[11]</ref>, that blend aspects of composed retrieval and VQA. Given an input image and question, the goal is to retrieve from a large candidate pool the entity that answers the question, and produce an additional textual response that offers further clarification and details (Fig. <ref type="figure" target="#fig_0">1</ref>). This task echoes research showing that humans learn better when they can jointly examine textual responses alongside the relevant visuals <ref type="bibr" target="#b60">[61]</ref>.</p><p>Our evaluations on diverse datasets for composed retrieval and commented retrieval show significant improvements of UniCoRN over state-of-the-art models. In particular, we show an average improvement for composed retrieval of +4.5% on Fashion-IQ, CIRR, OVEN, InfoSeek, and the proposed Wiki-CoR dataset in terms of recall when compared to UniIR <ref type="bibr" target="#b82">[83]</ref>. Moreover, we show an improvement of +14.9% in terms of METEOR score over a RAG approach combining UniIR and InternVL2 on the CoR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instructable Retrieval. Multimodal representation for retrieval <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b97">98]</ref> has been a prominent area of research demonstrating strong performance. To handle compositional requests that require reasoning about the visual content, recent work focused on composed retrieval <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b83">84]</ref>. However, these methods focus on limited specific retrieval domains and can only follow predefined instruction templates. To address this limitation, recent models have been made instructable <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b96">97]</ref> to capture richer multimodal relationships. MagicLens <ref type="bibr" target="#b96">[97]</ref> is a simple instructable model trained on a curated large dataset including instructions, while UniIR <ref type="bibr" target="#b82">[83]</ref> proposed training CLIP/BLIP variants conditioned on prompted instructions. While these approaches have demonstrated significant progress, they lack expressiveness in the multimodal output space due to the limitations of CLIP models. In contrast, methods based on LMMs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55]</ref> that are fine-tuned for retrieval do not preserve the original capabilities of the LMMs. UniCoRN aims to overcome this issue and preserve capabilities by design by enabling a frozen LMM to retrieve relevant content and generate textual responses tailored to both the input question and the retrieved visual content. Retrieval via Generation. LMMs <ref type="bibr">[1, 4, 18, 19, 23, 41, 55-57, 62, 64, 75, 81, 87, 90, 99]</ref> have demonstrated remarkable reasoning capabilities when processing language and visual elements (see survey in <ref type="bibr" target="#b90">[91]</ref>). Recently, LMMs were combined with diffusion models to generate images along with textual answers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b92">93]</ref>. Despite providing an interesting user experience, the generated images are by nature not grounded to real entities, which is critical for retrieval applications, such as online shopping, news, or Wikipedia. These models are based on memoryintensive diffusion models and trained with a reconstruction objective that is not in line with retrieval. In contrast, Uni-CoRN uses a retrieval approach that preserves the factuality of the images that are shown to the user, while commenting them in a generative way. It is lightweight, as it does not use diffusion models, and the LMMs are frozen, thus preserving their reasoning capabilities of other non-retrieval tasks. Retrieval-Augmented Generation (RAG). The parametric way that language models are trained and store knowledge limits the ease to expand or revise their memory with more recent information. RAG <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50]</ref> and Multimodal RAG <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b93">94]</ref> have been proposed as a non-parametric way to provide up-to-date knowledge to an LMM by adding relevant content to the prompt before processing by the LMM. Using this augmented prompt, the LMM can select, summarize and alter the entities to generate its output. UniCoRN differs from traditional RAG in that it processes user input via the LMM to build a better query and outputs the retrieved entity unaltered. Then, Uni-CoRN's entity adapter module is trained to feed optimized entity representations to the LMM, which guides the extraction of the information necessary to answer the request. Commented Retrieval (CoR). CoR is an underexplored task that requires not only retrieving relevant images, as in composed retrieval, but also generating plausible textual answers that refer to the image and contain additional information. While preliminary work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b82">83]</ref> and datasets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b62">63]</ref> focuse on some aspects of this task, they have two key limitations: 1) The complexity of the textual answers is constrained, typically limited to a few words or basic captions. 2) The textual answers are aligned with the image, either providing redundant information (e.g., image captions) and failing to add useful complementary information. To fully cover the CoR task, we claim that the answers should be multimodal in nature, with the image and text working in tandem to provide a richer, more comprehensive response. The modalities should have a clear connection, yet offer complementary information that enhances the overall response. To alleviate the limitations of existing CoR datasets, we created two challenging human-curated ones that we hope will foster future research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">UniCoRN</head><p>With UniCoRN, we propose to expand the capabilities of a frozen base LMMs with two novel interconnected modules: 1) Comment-aware retrieval, which is responsible for understanding the visio-linguistic representation of the query question and image, and projecting it to the target comment and image space for retrieval (Sec. 3.1); 2) Retrieval-aware generation, which learns to leverage the retrieved entity to generate an answer to the user question (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Comment-aware Retrieval</head><p>Given a query text and image q = (q ⟨t⟩ , q ⟨i⟩ ), we aim to train a retrieval model R θ , with parameters θ, that is capable of understanding q and retrieve a related target entity composed of text and image d = (d ⟨t⟩ , d ⟨i⟩ ), such that:</p><formula xml:id="formula_0">d = argmax d ′ ∈D R θ (q, d ′ ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where D is a database of multimodal entities (a.k.a documents). The best definition for the retriever R θ depends on the target application. For cross-modal and multimodal retrieval, CLIP-like models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b82">83]</ref> are the defacto choice based on their performance in diverse scenarios. However, they are limited in the visio-linguistic understanding capabilities, especially when moving to more complex applications like composed retrieval and commented retrieval. To address this limitation, we leverage the reasoning capabilities of LMMs in the training process of the retriever integrated with CLIP, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>The multimodal query q is fed into the base LMM to obtain its output hidden state h = H LMM (q). In parallel, the query is fed into a multimodal encoder ψ MM (q) = ψ ⟨t⟩ (q ⟨t⟩ ) + ψ ⟨i⟩ (q ⟨i⟩ ), where ψ ⟨t⟩ and ψ ⟨i⟩ are text and image CLIP-like encoders, respectively. Then, we define a Hidden State Adapter ψ LMM to map h into the same space as ψ MM , as follows: where FCs are fully connected layers and GeLU is the activation function. The final embedding for q combines the CLIP-based embeddings and the adapter output:</p><formula xml:id="formula_2">ψ LMM (h) = FC(GeLU(FC(h)))<label>(2)</label></formula><formula xml:id="formula_3">ψ(q) = β • ψ LMM (H LMM (q)) + (1 -β) • ψ MM (q) (3)</formula><p>where β is a learnable weight part of θ. Note, this scorefusion approach to CLIP is inspired by UniIR <ref type="bibr" target="#b82">[83]</ref>, where the authors show this is one of the most effective ways to combine multimodal inputs for multimodal retrieval. Finally, we define R θ as the dot product of query and document embeddings: R θ (q, d) = ψ(q) T • ψ MM (d). As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the multimodal target documents are not using the LMM and therefore only use the multimodal encoder ψ MM . This asymmetrical approach between query and document has several advantages: 1) it saves cost during the indexing phase by avoiding to use an expensive LMM on the potentially large pool of multimodal documents, and 2) it bridges the domain gap between the queries and the database via fine-tuning of the adapter. In contrast with other methods that use LMMs for indexing <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55]</ref>, our method keeps indexing lightweight.</p><p>We train our retriever using contrastive loss <ref type="bibr" target="#b67">[68]</ref>. Unlike previous work <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b82">83]</ref>, we combine target captions and more complex target comments that cover aspects beyond the query and target images, as we will describe in Sec. <ref type="bibr" target="#b3">4</ref>. This makes our model aware of complex comments during training and thus will better fit with retrieval-aware generation as described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Retrieval-aware Generation</head><p>We now detail how we enable an LMM to ingest the query question and image, encode retrieved entities, and generate an answer to the question. The process is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Retrieved Entities as Input Modality</head><p>In essence, a Large Language Model (LLM) predicts the probability of the next token τ n ∈ {1, . . . , V }, where V is the vocabulary size, given an input sequence of n tokens: </p><p>LMMs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b55">56]</ref> extend this principle by adapting visual data into input image tokens τ ⟨i⟩ or embeddings ϕ ⟨i⟩ . Notably, Eq. 4 is unchanged for LMMs, except that the inputs are now a mix of text ϕ ⟨t⟩ and image ϕ ⟨i⟩ embeddings in R d .</p><p>As described in Sec. 3.1, the retriever finds the most relevant multimodal document d m in the database based on the multimodal query q and the projected hidden state of the LMM. As an important contribution of our method, we then inject d m into the LMM as a new input modality ⟨r⟩ in order to condition the rest of the generation process, and therefore ensure that the model attends to it while generating the textual comment. This is important because of the finite nature of any database: the retrieved documents can substantially differ from the vector used as query. Thus, using solely the user input to condition the generation may lead to comments that are irrelevant to the retrieved entity.</p><p>Similarly to other input modalities, features of the retrieved document are extracted and adapted into the input embedding space of the LMM:</p><formula xml:id="formula_5">φ ⟨r⟩ (d m ) = [ϕ ⟨r⟩ m,0 , . . . , ϕ ⟨r⟩ m,lm ] = ϕ ⟨r⟩ m ∈ R d×lm ,<label>(5)</label></formula><p>where l m is the length of the representation of the document d m . In this way, the LMM can attend to the retrieved data in addition to all previous inputs as it continues generation:</p><formula xml:id="formula_6">p(τ n | • • • , ϕ ⟨t⟩ k , • • • , ϕ ⟨i⟩ p , • • • , ϕ ⟨r⟩ m , • • • ).<label>(6)</label></formula><p>However, the frozen LMM was not trained to distinguish entities that are retrieved from a database and treat them ap-propriately to generate a convincing answer. Thus, we make the entity adapter a trainable module as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Entity Adapter Module</head><p>The definition of φ ⟨r⟩ (d m ) depends on the nature of the dataset and the entities it contains: text, images, audio, video, pre-computed features, or any combination thereof.</p><p>We propose to design an Entity Adapter module φ )). In our datasets, the textual components are natural text, so we provide them directly to the LMM, without fine-tuning the token embeddings, i.e., φ ⟨t⟩ ξ (d m ) = ϕ ⟨t⟩ . For the visual part, we use the same architecture as the base LMM <ref type="bibr" target="#b18">[19]</ref>, with the adapter:</p><formula xml:id="formula_7">φ ⟨i⟩ ξ (d m ) = FC(GeLU(FC(LayerNorm(x m )))),<label>(7)</label></formula><p>where {x m } are the ViT image features of the variable number of tiles of the image of d ⟨i⟩ m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training</head><p>To train our novel Entity Adapter, we leverage a dataset D of ground-truth multimodal triplets (q, d, c), where q is the multimodal query, d is the ground-truth target document, and c is the ground-truth answer that relates the query and the target. The training of ξ is performed for the task of next token prediction of c, using the cross-entropy loss:</p><formula xml:id="formula_8">L CE (ξ) = (q,d,c)∈D τn∈c -δ n . log(p n (ξ)),<label>(8)</label></formula><p>where</p><formula xml:id="formula_9">p n (ξ) = p(τ n |ϕ ⟨i⟩ q , ϕ ⟨t⟩ q , φ ⟨r⟩ ξ (d), ϕ ⟨t⟩ 0 , . . . , ϕ ⟨t⟩ n-1 )</formula><p>, and δ n is the ground-truth distribution for τ n . As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, in practice, all other parts of the model are kept frozen during training and only the adapter in Eq. 7 is trained. For faster convergence, this adapter is initialized from the pretrained visual adapter of the original LMM.</p><p>With this approach, despite keeping the base LMM frozen, we are achieving two objectives: 1) we align the multimodal entities with the LMM input embeddings, thus helping the LMM extract the relevant information from the entities and closing any domain gap between the data used to pre-train the LMM and the retrieval dataset, and 2) we encode commenting instructions in the adapter, thus optimizing the inputs to the LMM to bias it to generate useful, high-quality comments for documents of this dataset. As a consequence, we train separate, specialized Entity Adapters for different tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Commented Retrieval (CoR) Task</head><p>As already discussed in related work, the related task of composed retrieval is well explored in the literature, and there exist a number of datasets for it. Some datasets for this task, e.g., CIRR <ref type="bibr" target="#b57">[58]</ref> and Fashion-IQ <ref type="bibr" target="#b32">[33]</ref>, include multimodal queries, while the targets are only images. Other relevant datasets, e.g., OVEN <ref type="bibr" target="#b37">[38]</ref> and InfoSeek <ref type="bibr" target="#b16">[17]</ref>, have both multimodal queries and targets, however the connection between inputs and outputs are simple high-level entities. Overall, existing datasets either lack target text altogether or feature overly simplistic textual responses that are directly related to the target image. These datasets are not well-suited to evaluate the full extent of the commented retrieval task, where the goal is not only to retrieve a relevant target image, but also to generate a detailed, complex textual response that refer to both the query and target image. Therefore, we propose a technique to automatically augment some existing datasets with textual answers and comments to the multimodal questions and image-only answers. In this work, we focus on 2 different domains, images of real-world object categories with CIRR <ref type="bibr" target="#b57">[58]</ref> and Wikipedia concepts with WikiWeb2M <ref type="bibr" target="#b9">[10]</ref>, but the approach itself generalizes to other domains.</p><p>The proposed approach to obtain a tuple of a query image, a textual question, a target image and a textual answer for CoR is composed by three parts: 1) pair related images; 2) generate questions and answers related to those images; and 3) manual annotation to create a golden set for evaluation. We couple images that belong to similar concepts, which definition depends on the task. For CIRR, image pairs are already present in the original set, as images were grouped together based on visual similarity. For Wiki-Web2M, we pair images belonging to the same Wikipedia article, because that denotes a semantic link. We subsample a set of 50K samples for training and 2K for testing.</p><p>In the second step, we aim at generating questions and answers that can relate the two images, with the goal of simulating the behavior of a user that asks a question about an uploaded image, and obtain an answer with text and the target image supporting the text. We automatically generate these questions and answers by prompting using Anthropic Claude 3.5 Sonnet. For CIRR, the question is the textual modification instruction that is already included in the original set, so we only generate the textual answer. For WikiWeb2M, we generate a question-answer pair that is related to the query and target images, their captions and the Wikipedia article. This task is more challenging and results in noisier data. We name these datasets CIRR-CoR and Wiki-CoR, respectively, and show examples 1 in Fig. <ref type="figure" target="#fig_6">5</ref>.</p><p>Finally, we create golden sets for evaluation purposes by 1 The Supplementary Material shows more examples of the datasets and the prompts used to generate them. asking annotators to validate the plausibility of the question and the quality of the generated text, manually adjusting them if necessary. We performed human auditing of the generated comments on the validation set of CIRR-CoR and found that ∼ 97% of the audited comments were marked as high quality. For the test set of Wiki-CoR, we conducted a manual review by filtering out image pairs that are not related to each other, questions that are irrelevant and comments that are not aligned with the question. In addition, annotators performed the necessary edits of the question and answer pairs to improve their quality. This results in a golden set of 695 out of 1768 (∼ 39%), showing the importance of having an annotation process for evaluation, that is often not considered in previous datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the multimodal retrieval and commenting capabilities of our methods with respect to the state of the art in Sec. 5.1 and Sec. 5.2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Retrieval Results</head><p>Implementation Details. We choose InternVL2-4B <ref type="bibr" target="#b18">[19]</ref> as the base, frozen LMM and use it to extract hidden states H LMM (q) corresponding to queries. Specifically, we take the 3072-dimensional hidden state of the last transformer layer after processing of the last input token before generating the response. We train the encoder ψ MM and adapter ψ LMM using the UniIR framework <ref type="bibr" target="#b82">[83]</ref> so as to align the queries and targets using contrastive loss <ref type="bibr" target="#b39">[40]</ref> in two stages. First, we train the hidden state adapter exclusively to align them to UniIR embeddings, so we can obtain a good initialization. Second, as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, we additionally unlock parameter β and the multimodal encoder, initialized from a pre-trained CLIP ViT-L-14 model <ref type="bibr" target="#b67">[68]</ref>. We append instructions in the query as proposed by <ref type="bibr" target="#b82">[83]</ref> and train a joint model on all 5 datasets<ref type="foot" target="#foot_0">foot_0</ref> , i.e Fashion-IQ, CIRR, OVEN, InfoSeek and Wiki-CoR. For Wiki-CoR, we randomly sample comment or caption as text with 50%/50% chance. We train for 40 epochs using learning rate 1e-5, batch size 55, effectively 110 thanks to gradient accumulation of 2, on a single host with 8×48GB NVIDIA L40S GPUs.</p><p>Metrics. We use standard recall@k metrics to measure the retrieval performance: we compute the neighbourhoods of the query vectors and verify if they contain the target image specified in the ground-truth annotations of the datasets. We compare with pretrained CLIP ViT-L-14 <ref type="bibr" target="#b67">[68]</ref> and the state-of-the-art model UniIR CLIP SF <ref type="bibr" target="#b82">[83]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashion-IQ [33]</head><p>CIRR <ref type="bibr" target="#b57">[58]</ref> OVEN <ref type="bibr" target="#b37">[38]</ref> InfoSeek <ref type="bibr" target="#b16">[17]</ref> Wiki-CoR Average R@10 R@20 R@50 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 Results. The main results are shown in Table <ref type="table" target="#tab_0">1</ref>. First, we observe a low zero-shot performance on all datasets. This is in line with observations from previous work on composed retrieval <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b96">97]</ref>, which showed that CLIP models cannot easily compose multimodal features to match multimodal answers that are semantically different from the original query. Second, we see that UniCoRN is able to leverage the hidden states of the LMM, leading to improved performance, especially for the first ranks (recall@1 and 5), over all datasets but InfoSeek. We also show qualitative results of UniIR and UniCoRN in Fig. <ref type="figure" target="#fig_4">4</ref>. In the first example, UniIR retrieves visually similar traditional clothing and headwear. Instead, UniCoRN correctly retrieves art forms from Bali and Java (mentioned in the captions), consistent with the query (Indonesia). In the challenging second example, the model needs to infer from the query image that it's a hair dryer and connect the textual "show in use". While UniIR fails, UniCoRN correctly retrieves the ground truth image on rank 3. Also the other examples on CIRR and OVEN show that UniCoRN is able to combine query and image text in an abstract way to retrieve visually dissimilar correct targets, while UniIR focuses on visual similarity.</p><p>Ablation Study. Table <ref type="table" target="#tab_1">2</ref> shows an ablation study, where we train our model without Wiki-CoR and we remove the LMM adapter. For this analysis, we study recall@1 because this will be the retrieved image that the LMM will comment (Sec. 5.2). The inclusion of our adapter (row 1 vs 2) enhances performance across both datasets, leveraging LMM hidden states. This improvement comes at a negligible computational cost during inference since the LMM hidden state is computed already, highlighting our model's efficiency and effectiveness. When we do not train on Wiki-CoR data, we observe a big drop in performance, highlighting how distinctive Wiki-CoR is from other Wikipedia datasets such as OVEN and InfoSeek. Our model convincingly outperforms the zero-shot baseline (row 1 vs 4), showcasing its robust generalization capabilities and underscores the value of our training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Commenting Results</head><p>Implementation Details. To implement retrieval-aware commenting, we need to modify standard LMM procedures, because we perform retrieval during generation and output images interleaved with text. We do so by extending the generation function of LMMs with the following changes. First, we specify a retrieval token, the prediction of which will trigger the retriever R θ . We use the startof-sentence tag, as our commented retrieval datasets do not exhibit comments prior to retrieval. The retriever yields a reference to the target entity, which we process via feature extraction and the Entity Adapter to make the LMM understand the retrieved content. We convert all previously predicted output tokens to input embeddings and append the embedding of the retrieved entity, then feed this as input to the LMM for predicting the next tokens, until the end-ofsentence token is detected.</p><p>In addition, we train our Entity Adapters specifically so that InternVL2-4B processes retrieved content differently than content input by a user. We therefore fine-tune the Entity Adapters separately on CIRR-CoR and Wiki-CoR. For CIRR-CoR, we have 28,225 valid training conversations, where the retrieved entities are simply the target images. For Wiki-CoR, we have 50,126 valid training conversations, where the retrieved entities are Wikipedia images with caption, as well as metadata from the corresponding Wikipedia page (title, description). We train both adapters using the InternVL2 framework <ref type="bibr" target="#b18">[19]</ref> for two epochs on a single host with 4×L40S GPUs, using a learning rate of 1.0e -4 , and evaluate on their respective evaluation sets as we described in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics.</head><p>We evaluate the predicted comments with respect to the corresponding ground-truth (GT) comments in the golden test set using standard metrics for language understanding: i) METEOR <ref type="bibr" target="#b19">[20]</ref>, the harmonic mean of word precision and recall that takes into account stemming and synonymy; ii) BLEU <ref type="bibr" target="#b64">[65]</ref>, a metric with high correlation with human judgments of translation quality; iii) ROUGE-1 <ref type="bibr" target="#b53">[54]</ref> (resp. ROUGE-2) measuring the overlap of words (resp, bi-grams) between the two comments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIRR-CoR</head><p>Wiki-CoR</p><formula xml:id="formula_10">METEOR BLEU ROUGE-1 ROUGE-2 BEM SigLIP-R@1 METEOR BLEU ROUGE-1 ROUGE-2 BEM SigLIP-R@1</formula><p>InternVL2 [19] 0.192 0.013 0.280 0.082 11.9% 41.4% 0.128 0.009 0.237 0.089 31.8% 49.1% EchoSight [88] + InternVL2 0.311 0.038 0.397 0.119 31.6% 41.0% 0.204 0.018 0.276 0.065 16.2% 32.1% UniIR CLIP SF [83] + InternVL2 0.313 0.039 0.399 0.121 32.0% 42.6% 0.212 0.025 0.293 0.078 21.4% 41.2% UniCoRN 0.444 0.115 0.516 0.280 50.4% 45.1% 0.379 0.128 0.446 0.196 39.3% 66.3% UniCoRN retriever + RAG 0.318 0.041 0.400 0.123 33.7% 43.3% 0.236 0.037 0.323 0.098 28.7% 55.2% UniCoRN w/o trained adapter 0.317 0.029 0.346 0.106 28.8% 33.7% 0.264 0.033 0.285 0.089 23.7% 52.4% UniCoRN w/ shared adapter 0.407 0.060 0.392 0.181 55.9% 42.7% 0.374 0.127 0.447 0.195 39.3% 67.6% UniCoRN w/ UniIR CLIP SF 0.439 0.113 0.510 0.276 49.8% 44.0% 0.347 0.106 0.411 0.168 34.7% 55.4% UniCoRN w/ GT target oracle 0.462 0.130 0.534 0.297 56.1% 55.9% 0.443 0.176 0.517 0.258 55.8% 92.1% Table 4. Modality ablation study for entity encoding. We compare results on Wiki-CoR using UniCoRN and RAG when removing images, captions and Wikipedia descriptions when providing the entities to the model.</p><p>iv) BEM <ref type="bibr" target="#b8">[9]</ref>, a BERT-based <ref type="bibr" target="#b21">[22]</ref> measure to evaluate the equivalence of answers to a question. We also use a separate vision-language model, namely ViT-SO400M-14-SigLIP-384 <ref type="bibr" target="#b95">[96]</ref>, to measure if the predicted comment can be used to retrieve the GT comment with high accuracy, thus measuring if they are specific and distinct from other comments. For this approach, we use again recall@1.</p><p>Results. In the top section of Table <ref type="table" target="#tab_3">3</ref>, we report the results for UniCoRN as well as three state-of-the-art LMM approaches that can produce comments based on multimodal inputs: i) InternVL2 <ref type="bibr" target="#b18">[19]</ref>, a state-of-the-art LMM that is prompted to generate a comment for the unseen image based only on the multimodal user query; ii) EchoSight <ref type="bibr" target="#b87">[88]</ref>, a state-of-the-art multimodal Retrieval-Augmented Generation (RAG) and re-ranking approach, using InternVL2 as the base LMM for comment generation. iii) a system with the state-of-the-art compositional retriever UniIR <ref type="bibr" target="#b82">[83]</ref> that we use to build a multi-image multi-modal RAG prompt for InternVL2; The results show that InternVL2 is able to generate comments that contain relevant information (METEOR 0.192/0.128 on CIRR-CoR/Wiki-CoR resp.). This entails that the LMM is able to interpret the user query, justifying that its hidden state has indeed the potential to contribute to the retrieval component. However, the comments are not very similar to the ground-truth: this is clearly due to the absence of retrieval capabilities in InternVL2, which thus has to produce This image shows a size comparison of Quilmesaurus with other carnotaurins. Quilmesaurus appears to be relatively small compared to the other dinosaurs depicted.</p><p>It is shown as a smaller figure in the group, with the other carnotaurins being larger in size. This suggests that Quilmesaurus was a smaller member of its family compared to some of its relatives.</p><p>The second image is a scale diagram of Rajasaurus, a carnivorous abelisaurid theropod dinosaur, compared to a human. The first image is a full-body illustration of the same dinosaur. The second image provides a scale reference, showing that Rajasaurus was approximately 6.6 meters long, with a single horn on its forehead.</p><p>Add 8 chairs and a car.</p><p>The image shows a larger room with more chairs and a car visible through the windows compared to the other image.</p><p>The second image differs from the first by having 8 additional chairs and a car. a plausible comment without having access to any information about the content present in the retrieval dataset. Using EchoSight <ref type="bibr" target="#b87">[88]</ref> or UniIR <ref type="bibr" target="#b82">[83]</ref> to retrieve an entity and providing it in the prompt for InternVL2 improves results significantly on all metrics (e.g., METEOR 0.313/212), with a slight edge to the compositional retriever (UniIR) over the reranking one (EchoSight). This proves that conditioning the generation on actual retrieved images is critical to ensure the relevance of the comments, and explains the broad success of RAG methods. Then, using UniCoRN, we further improve to 0.444/0.379 METEOR. Notably, the improvements are consistent over all metrics on both datasets. These results show the effectiveness of our approach: without losing any other ability of InternVL2, UniCoRN is able to optimize its processing of user queries and retrieval results so as to output comments that are significantly better than static prompts built via RAG.</p><p>Ablation Study. In the middle section of Table <ref type="table" target="#tab_3">3</ref>, we further analyze the main contributions to the performance of UniCoRN. First, when using UniCoRN's retrieval results in a RAG prompt for InternVL2, we observe only a slight improvement over UniIR+RAG. Similarly, using UniCoRN's commenting abilities on UniIR retrieval results performs only slightly worse than our full UniCoRN. Thus, the improvements in retrieval are only a part of the contribution to the commenting performance. In contrast, using the pretrained input visual encoder from InternVL2 to encode retrieved documents does indeed drop the performance to the level for RAG. This highlights that our contribution of training dataset-specific entity embeddings makes the biggest contribution in making the base LMM provide more relevant and well-formatted comments. Training a share entity encoder leads to mixed results, with a slight improvement on BEM and SigLIP at the expense of other metrics.</p><p>In Table <ref type="table">4</ref>, we further decompose the contribution of the various modalities available in the retrieved documents.</p><p>Since CIRR-CoR only has target images, we focus on Wiki-CoR, in which retrieved documents contain an image and its caption on Wikipedia, as well as the title and description of the corresponding page in Wikipedia as metadata. UniCoRN, like RAG, uses all information by default in our experiments (to build the Entity encoding, resp. the prompt). Using the UniCoRN retriever and the trained adapter, we show the drop of performance that occurs when removing, in turn, i) the Wikipedia title and description: -12.1% METEOR, relatively; ii) all textual inputs (image-only): -19.1%; iii) image and Wikipedia metadata (caption-only): -13.1%. Since the drops are consistent for all metrics and both comment generation models (RAG and UniCoRN) based on the same inputs <ref type="foot" target="#foot_1">3</ref> , we can therefore conclude that all three are needed to provide relevant answers.</p><p>Finally, we analyze the qualitative performance of UniIR with RAG and UniCoRN in Fig. <ref type="figure" target="#fig_6">5</ref> with success cases for UniCoRN. The first row shows an example where both models retrieve the correct image. However, the comments provided by RAG do not answer the question correctly, as indicated by the part of the comment highlighted in red. The second row presents an example where the UniIR model was unable to retrieve the correct image, and therefore it could not answer the question correctly. This is similar to the third example, where the UniIR model retrieved the wrong query image, yet provided a comment that follows the instructions but does not actually correspond to the retrieved image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This work introduces UniCoRN, a Unified Commented Retrieval Network that unifies discriminative retrieval and generative commenting. We combined the strengths of composed multimodal retrieval methods with gen-erative language capabilities, resulting in a system that surpasses traditional RAG techniques.</p><p>Our approach preserves the original capabilities of the base LMM while extending its functionality to perform both retrieval and text generation tasks within a single framework. We introduced the CoR task and a corresponding dataset to evaluate these new abilities of LMMs. Future work could focus on further improving the integration of multimodal information, exploring additional tasks, and investigating the scalability of our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Commented retrieval. Given an query image and question, UniCoRN can retrieve an image from a database and can produce a textual answer that offers further clarification and details.</figDesc><graphic coords="1,317.25,199.82,236.24,103.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comment-aware Retrieval. The query is inputed to both a CLIP-trained image-text encoder and an LMM. The LMM representation is projected to the space of the image-text encoder. Alignment of query and targets is done using contrastive loss.</figDesc><graphic coords="3,339.63,72.00,189.00,117.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Retrieval-aware Generation. The query image and text are fed to the LMM, which asks the retriever for relevant entities.The best entity is provided to the user and adapted into the LMM, so it can attend to it for generating a useful comment.</figDesc><graphic coords="4,58.50,72.00,236.24,132.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>⟨r⟩ ξ (d m ), as shown in Fig. 3, with parameters ξ, which can be finetuned on training data. More specifically for our CoR tasks, the entities are images, or multimodal combinations of images and text data (e.g. titles, captions, ...), that are used to generate the answer. As a consequence, the Entity Adapter treats the various modalities differently, i.e., φ ⟨r⟩ ξ (d m ) = (φ ⟨i⟩ ξ (d ⟨i⟩ m ), φ ⟨t⟩ ξ (d ⟨t⟩ m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Qualitative results retrieval. We show retrieved images for UniIR and UniCoRN on three datasets. Captions are not displayed because of space limits.</figDesc><graphic coords="7,83.25,72.00,445.50,172.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>This image shows St. Andrew's College's clock tower. The tower is a prominent stone structure with a clock face visible on the side. It appears to be a tall, rectangular building with arched openings and a flag flying at the top. This clock tower is a significant architectural feature of the college, standing tall and visible from a distance.The second image is a close-up of the clock tower at St. Andrew's College, while the first image shows the entire building, including the chapel and surrounding area. The second image focuses on the clock tower, highlighting its architectural details and the flag flying atop it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative results. We show retrieved images and comments for UniIR with RAG and UniCoRN on two different datasets. Comments highlighted in red indicate responses that either do not answer the original question or are not related to the retrieved image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main results on retrieval. Retrieval results in terms of recall on different datasets while comparing with state of the art methods.</figDesc><table><row><cell>CLIP (zero-shot) [68]</cell><cell>5.50</cell><cell>8.26</cell><cell>13.84</cell><cell>0.86</cell><cell>12.13</cell><cell>18.30</cell><cell>10.79 24.12</cell><cell>30.47</cell><cell>8.73</cell><cell>21.79</cell><cell>29.62</cell><cell>13.24 25.04</cell><cell>30.50</cell><cell>16.88</cell></row><row><cell>UniIR CLIPSF [83]</cell><cell>24.59</cell><cell cols="2">32.22 43.66</cell><cell>9.50</cell><cell>45.01</cell><cell>58.61</cell><cell>49.50 68.86</cell><cell>74.56</cell><cell cols="3">27.08 49.04 58.24</cell><cell>21.73 40.72</cell><cell>46.76</cell><cell>43.34</cell></row><row><cell>UniCoRN (ours)</cell><cell cols="3">25.20 32.40 43.33</cell><cell cols="3">16.45 47.24 59.76</cell><cell cols="2">51.08 70.22 75.54</cell><cell cols="2">26.41 48.01</cell><cell>57.24</cell><cell cols="2">40.14 57.70 67.05</cell><cell>47.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on retrieval. Retrieval results in terms of recall@1 on different dataset.</figDesc><table><row><cell></cell><cell>CIRR [58]</cell><cell>Wiki-CoR</cell></row><row><cell>UniCoRN</cell><cell>16.45</cell><cell>40.14</cell></row><row><cell>w/o adapter</cell><cell>16.35</cell><cell>39.14</cell></row><row><cell>w/o Wiki-CoR</cell><cell>17.29</cell><cell>23.02</cell></row><row><cell>w/o training (zero-shot)</cell><cell>0.86</cell><cell>13.24</cell></row><row><cell>UniIR CLIP SF [83]</cell><cell>9.50</cell><cell>21.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Main results and ablation study on commenting. Commenting results by computing several language-based metrics between the predicted comments and the ground-truth (GT) ones on the CIRR-CoR and Wiki-CoR dataset. (Top) Comparison with the state of the art; (Middle) Ablation study for retrieval and generation in UniCoRN; (Bottom) Upper-bound using GT target documents as an oracle.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Wiki-CoR</cell><cell></cell></row><row><cell></cell><cell>METEOR</cell><cell>BLEU</cell><cell>ROUGE-1</cell><cell>ROUGE-2</cell><cell>SigLIP-R@1</cell></row><row><cell>UniCoRN</cell><cell cols="3">0.379 0.128 0.446</cell><cell>0.196</cell><cell>66.3%</cell></row><row><cell>w/o Wikipedia desc.</cell><cell>0.302</cell><cell>0.069</cell><cell>0.386</cell><cell>0.143</cell><cell>55.1%</cell></row><row><cell>image-only</cell><cell>0.295</cell><cell>0.065</cell><cell>0.379</cell><cell>0.137</cell><cell>50.6%</cell></row><row><cell>caption-only</cell><cell>0.244</cell><cell>0.034</cell><cell>0.299</cell><cell>0.090</cell><cell>47.3%</cell></row><row><cell>UniCoRN retr. + RAG</cell><cell>0.236</cell><cell>0.037</cell><cell>0.323</cell><cell>0.098</cell><cell>55.2%</cell></row><row><cell>w/o Wikipedia desc.</cell><cell>0.227</cell><cell>0.032</cell><cell>0.321</cell><cell>0.098</cell><cell>51.5%</cell></row><row><cell>image-only</cell><cell>0.198</cell><cell>0.028</cell><cell>0.314</cell><cell>0.094</cell><cell>49.6%</cell></row><row><cell>caption-only</cell><cell>0.258</cell><cell>0.024</cell><cell>0.277</cell><cell>0.079</cell><cell>46.0%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Fashion-IQ, CIRR, OVEN, and InfoSeek are a subset of M-BEIR<ref type="bibr" target="#b82">[83]</ref> that has multimodal inputs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Except the caption-only version of RAG for the METEOR score, which does better without the image.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Pravesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Antoniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">Bou</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Chudnovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophile</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07073</idno>
		<title level="m">Amélie Héliou, Paul Jacob, et al. Pixtral 12b</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning attribute representations with localization for flexible fashion search</title>
		<author>
			<persName><surname>Kenan E Ak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joo Hwee</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><forename type="middle">Yew</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><surname>Tham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><surname>Ai Anthropic</surname></persName>
		</author>
		<title level="m">The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective conditioned and composed image retrieval combining clip-based features</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Baldrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiberio</forename><surname>Uricchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-shot composed image retrieval with textual inversion</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Baldrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Agnolucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15338" to="15347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vlmo: Unified vision-language pre-training with mixture-of-modality-experts</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation</title>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Friedrich</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><forename type="middle">Paweł</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Boerschinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A suite of generative tasks for multilevel multimodal webpage understanding</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03668</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A suite of generative tasks for multi-level multimodal webpage understanding</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<idno>2023. 2</idno>
	</analytic>
	<monogr>
		<title level="m">The 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Caffagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Moratelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sarto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Murag: Multimodal retrieval-augmented generator for open question answering over images and text</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02928</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pali-3 vision language models: Smaller, faster, stronger</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09199</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image search with text feedback by visiolinguistic attention learning</title>
		<author>
			<persName><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image search with text feedback by visiolinguistic attention learning</title>
		<author>
			<persName><forename type="first">Yanbei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3001" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Can pre-trained vision and language models answer visual information-seeking questions</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11713</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglong</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kongzhi</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hewei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinlong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangchao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiannan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2024. 1, 2, 4, 5, 6, 7</date>
			<biblScope unit="page" from="24185" to="24198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperbolic image-text representations</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Rajpurohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanmukha</forename><forename type="middle">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<idno>PMLR, 2023. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="7694" to="7731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">N</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Entities as experts: Sparse memory access with entity supervision</title>
		<author>
			<persName><surname>Thibault Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07202</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pyramidclip: Hierarchical feature alignment for vision-language model pretraining</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Softclip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17561</idno>
		<title level="m">Softer crossmodal alignment makes clip stronger</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Making llama see and draw with seed tokenizer</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01218</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cyclip: Cyclic contrastive language-image pretraining</title>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwa</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6704" to="6719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fashionvlp: Vision language transformer for fashion retrieval with feedback</title>
		<author>
			<persName><forename type="first">Sonam</forename><surname>Goenka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Chada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language-only training of zero-shot composed image retrieval</title>
		<author>
			<persName><forename type="first">Geonmo</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoohoon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="13225" to="13234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dialog-based interactive image retrieval</title>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The fashion iq dataset: Retrieving images by combining side information and relative natural language feedback</title>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12794</idno>
		<imprint>
			<date type="published" when="2006">2019. 1, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pretraining</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingwei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic spatially-aware fashion concept discovery</title>
		<author>
			<persName><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Phoenix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning attribute-driven disentangled representations for interactive fashion retrieval</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International conference on computer vision</title>
		<meeting>the IEEE/CVF International conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12147" to="12157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities</title>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Fathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23369" to="23379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Openclip</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2021. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Mantis: Interleaved multi-image instruction tuning</title>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaye</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vlm2vec: Training vision-language models for massive multimodal embedding tasks</title>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.05160</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Vision-by-language for trainingfree compositional image retrieval</title>
		<author>
			<persName><forename type="first">Shyamgopal</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.09291</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grounding language models to images for multimodal inputs and outputs</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17283" to="17300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Uniclip: Unified framework for contrastive language-image pretraining</title>
		<author>
			<persName><forename type="first">Janghyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongsuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyounguk</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bumsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung Hwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.13430</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00300</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Chatting makes perfect: Chat-based image retrieval</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Darshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sebastian Riedel, and Douwe Kiela</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Textbind: Multi-turn interleaved multimodal instructionfollowing in the wild</title>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Mm-embed: Universal multimodal retrieval with multimodal llms</title>
		<author>
			<persName><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chankyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.02571</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image retrieval on real-life images with pre-trained vision-and-language models</title>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Image retrieval on real-life images with pretrained vision-and-language models</title>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2125" to="2134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bi-directional training for composed image retrieval via text prompt learning</title>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="5753" to="5762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Three facets of visual and verbal learners: Cognitive ability, cognitive style, and learning preference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">J</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational psychology</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">833</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Mm1: Methods, analysis &amp; insights from multimodal llm pre-training</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Mckinzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Fauconnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruti</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Futang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floris</forename><surname>Weers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.09611</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arushi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3113" to="3124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Rora-vlm: Robust retrieval-augmented vision language models</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rulin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Alleviating hallucination in large vision-language models with active retrieval augmentation</title>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00555</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/2103.00020</idno>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A-okvqa: A benchmark for visual question answering using world knowledge</title>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Knowledge-aware visual question answering</title>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naganand</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><surname>Kvqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8876" to="8884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Knowledge-enhanced dual-stream zero-shot composed image retrieval</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26951" to="26962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Chameleon: Mixed-modal early-fusion foundation models</title>
		<author>
			<persName><forename type="first">Chameleon</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.09818</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">Gemini</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer</title>
		<author>
			<persName><forename type="first">Changyao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.10208</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Genecis: A benchmark for general conditional image similarity</title>
		<author>
			<persName><forename type="first">Sagar</forename><surname>Vaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6862" to="6872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval -an empirical odyssey</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval-an empirical odyssey</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6439" to="6448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Cross-modal feature alignment and fusion for composed image retrieval</title>
		<author>
			<persName><forename type="first">Yongquan</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guobing</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bofeng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8384" to="8388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Qwen2-vl: Enhancing vision-language model&apos;s perception of the world at any resolution</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuejing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.12191</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Image as a foreign language: Beit pretraining for vision and visionlanguage tasks</title>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Som</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Uniir: Training and benchmarking universal multimodal information retrievers</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.17136</idno>
		<imprint>
			<date type="published" when="2008">2023. 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Fashion iq: A new dataset towards retrieving images by natural language feedback</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11307" to="11317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Fashion iq: A new dataset towards retrieving images by natural language feedback</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11307" to="11317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Visual question answering: A survey of methods and datasets</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="40" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<author>
			<persName><forename type="first">Shengqiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leigang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05519</idno>
		<title level="m">Next-gpt: Any-to-any multimodal llm</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">EchoSight: Advancing visuallanguage models with Wiki knowledge</title>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2024</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guansong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07783</idno>
		<title level="m">Filip: fine-grained interactive language-image pre-training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">mplug-owl3: Towards long image-sequence understanding in multi-modal large language models</title>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.04840</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Shukang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13549</idno>
		<title level="m">A survey on multimodal large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Coca: Contrastive captioners are image-text foundation models</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Legg</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Seyedhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01917</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Language model beats diffusion-tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05737</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Visrag: Vision-based retrieval-augmented generation on multi-modality documents</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyue</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.10594</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">A large-scale study of representation learning with the visual task adaptation benchmark</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04867</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Sigmoid loss for language image pre-training</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2023. 1, 2, 7</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19651</idno>
		<title level="m">Magiclens: Self-supervised image retrieval with open-ended instructions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09304</idno>
		<title level="m">Non-contrastive learning meets language-image pretraining</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">MiniGPT-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
