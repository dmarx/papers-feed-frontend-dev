- **Vocabulary Transfer (VT) Overview**: A method to adapt language models (LMs) to in-domain tokenizers, reducing model size and inference time while maintaining performance.
  
- **Key Equations**:
  - **Embedding Initialization**:
    - For tokens in both vocabularies:
      \[
      E_{in}(t_i) = E_{gen}(t_i) \quad \text{if } t_i \in V_{in} \cap V_{gen}
      \]
    - For new tokens:
      \[
      E_{in}(t_i) = \frac{1}{|T_{gen}(t_i)|} \sum_{t_j \in T_{gen}(t_i)} E_{gen}(t_j) \quad \text{if } t_i \in V_{in} \setminus V_{gen}
      \]

- **Fast Vocabulary Transfer (FVT)**: A simplified implementation of Vocabulary Initialization with Partial Inheritance (VIPI) that uses a straightforward assignment mechanism for embedding initialization.

- **Model Compression Techniques**: VT can be combined with other methods like Knowledge Distillation (KD), quantization, and pruning to further reduce model size and improve inference speed.

- **Knowledge Distillation (KD) Process**:
  - Two-step distillation: 
    1. Distill the student model on general domain data.
    2. Distill the student model on in-domain data.
  - Loss functions used: distillation loss, masked language modeling (MLM) loss, and cosine embedding loss.

- **Performance Metrics**: Evaluate FVT based on:
  - Quality (F1 score)
  - Model size reduction
  - Inference speedup (achieved speedup between 1.07x and 1.40x).

- **Experimental Setup**:
  - Pre-trained model: BERT base with 28,996 wordpieces.
  - Vocabulary sizes tested: 100%, 75%, 50%, and 25%.
  - Fine-tuning for 10 epochs with early stopping.

- **Datasets Used**:
  - **ADE**: Adverse Drug Events corpus for medical domain.
  - **LEDGAR**: Legal document classification dataset.
  - **CoNLL03**: Named Entity Recognition benchmark from news stories.

- **Results Summary**:
  - In-domain tokenization leads to a significant reduction in the average number of tokens per sequence (up to 32% in specialized domains).
  - The learned tokenizers shift sequence length distribution, improving efficiency.

- **Conclusion**: VT is an effective method for compressing language models, particularly when combined with other techniques like KD, leading to substantial reductions in model size and inference time with minimal performance loss.