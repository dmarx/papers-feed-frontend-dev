- Decision to focus on vocabulary transfer as a method for model compression
- Choice of using a pre-trained language model (BERT) as the baseline
- Selection of tokenization strategy (subword-level tokenization)
- Decision to implement Fast Vocabulary Transfer (FVT) over other methods
- Choice of combining Vocabulary Transfer with Knowledge Distillation (KD)
- Decision to evaluate the method across multiple vertical domains (medical, legal, news)
- Selection of performance metrics (F1 score, model size, inference speed)
- Decision on the vocabulary sizes for retraining tokenizers (100%, 75%, 50%, 25%)
- Choice of training parameters (learning rate, batch size, sequence length)
- Decision to use early stopping during model fine-tuning
- Choice of datasets for experiments (ADE, LEDGAR, CoNLL03)
- Decision to apply MLM as a pre-training step before fine-tuning
- Choice of embedding initialization strategy (VIPI vs. FVT)
- Decision to conduct experiments with multiple random initializations
- Choice of loss functions for the distillation process (distillation loss, MLM loss, cosine embedding loss)
- Decision to apply VT after the initial KD process
- Choice of evaluation framework for measuring the effectiveness of VT