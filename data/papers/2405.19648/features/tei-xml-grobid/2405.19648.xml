<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ernesto</forename><surname>Quevedo</surname></persName>
							<email>quevedo1@baylor.edu</email>
							<idno type="ORCID">0000-0002-8938-2230</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jorge</forename><surname>Yero</surname></persName>
							<email>jorgeyero1@baylor.edu</email>
							<idno type="ORCID">0000-0002-5033-4805</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salazar</forename><surname>Id</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rachel</forename><surname>Koerner</surname></persName>
							<email>koerner1@baylor.edu</email>
							<idno type="ORCID">0009-0009-5294-4349</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior, IEEE</roleName><forename type="first">Pablo</forename><surname>Rivas</surname></persName>
							<email>pablorivas@baylor.edu</email>
							<idno type="ORCID">0000-0002-8690-0987</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomas</forename><surname>Cerny</surname></persName>
							<email>cerny@baylor.edu</email>
							<idno type="ORCID">0000-0002-5882-5502</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep3">Department of Computer Science School of Eng. &amp; Computer Science</orgName>
								<orgName type="department" key="dep4">Department of Computer Science School of Engineering &amp; Computer Science</orgName>
								<orgName type="department" key="dep5">Department of Systems &amp; Industrial Engineering College of Engineering</orgName>
								<orgName type="institution" key="instit1">Baylor University</orgName>
								<orgName type="institution" key="instit2">Baylor University</orgName>
								<orgName type="institution" key="instit3">Baylor University</orgName>
								<orgName type="institution" key="instit4">Baylor University</orgName>
								<orgName type="institution" key="instit5">The University of Arizona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">00E8B9C21595F4A50C28DE211DBD8CDA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Large Language Models, Hallucinations</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Concerns regarding the propensity of Large Language Models (LLMs) to produce inaccurate outputs, also known as hallucinations, have escalated. Detecting them is vital for ensuring the reliability of applications relying on LLM-generated content. Current methods often demand substantial resources and rely on extensive LLMs or employ supervised learning with multidimensional features or intricate linguistic and semantic analyses difficult to reproduce and largely depend on using the same LLM that hallucinated. This paper introduces a supervised learning approach employing two simple classifiers utilizing only four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, which are not necessarily the same. The method yields promising results, surpassing state-of-the-art outcomes in multiple tasks across three different benchmarks. Additionally, we provide a comprehensive examination of the strengths and weaknesses of our approach, highlighting the significance of the features utilized and the LLM employed as an evaluator. We have released our code publicly at <ref type="url" target="https://github.com/Baylor-AI/HalluDetect">https://github.com/Baylor-AI/HalluDetect</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Large Language Models (LLMs) have become the core of many state-of-the-art Natural Language Processing (NLP) algorithms and have revolutionized various domains in NLP and computer vision and even more specialized applications in healthcare, finance, and the creative arts. Because of their impressive Natural Language Generation (NLG) capabilities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, they have attracted great interest from the public with great modern tools like ChatGPT <ref type="bibr" target="#b2">[3]</ref>, Github-Copilot <ref type="bibr" target="#b3">[4]</ref>, Dalle <ref type="bibr" target="#b4">[5]</ref>, and others <ref type="bibr" target="#b0">[1]</ref>. These models, with millions to billions of parameters, are often praised for their impressive ability to generate human-like text and tackle intricate tasks with limited to no fine-tuning with techniques like In-Context-Learning <ref type="bibr" target="#b5">[6]</ref>.</p><p>Since many of the most popular applications and stateof-the-art algorithms in NLP rely on LLMs, any error they produce affects the results. Particularly in the cases of a Chatbot like ChatGPT, the generated responses are expected to maintain factual consistency with the source text <ref type="bibr" target="#b6">[7]</ref>. Currently, a pressing concern with LLMs is their propensity to "hallucinate," which intuitively means to produce outputs that, while seemingly coherent, might be misleading, fictitious, or not genuinely reflective of their training data or actual facts <ref type="bibr" target="#b7">[8]</ref>.</p><p>Furthermore, the consequences of hallucinatory-generated text when used by the public are a significant ethical concern. This fictitious content can lead to misinformation and have severe implications in delicate medical, legal, educational, and financial fields. Besides the ethical consequences, these errors can lead to limitations in the use of the LLMs to automate programming tasks completely and tedious hand-work, limiting their contribution to NLP tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>While there have been efforts to detect and mitigate hallucinations, many of the prevalent methods rely on supervised learning with many multidimensional features. In contrast, others used in-context-learning techniques based on intricate linguistic and semantic analyses <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>. These methods affect the latency for use in real time. Additionally, current research has shown that even state-of-the-art approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7]</ref> struggle to detect hallucinations.</p><p>However, recent research has hinted at the potential of numerical features mathematically <ref type="bibr" target="#b12">[13]</ref> and empirically <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, as indicators of hallucinations on LLM outputs. These features could provide a resource-efficient method to detect and mitigate hallucinations. In this paper, we introduce a supervised learning approach employing two classifiers that use four numerical features derived from tokens and vocabulary probabilities obtained from other LLM evaluators, usually different ones.</p><p>Our research not only highlights the effectiveness of this method in comparison with current approaches in some scenarios but also paves the way for potential uses that validate the credibility of LLM outputs. Our main contributions are:</p><p>• Propose a supervised learning approach using four features to detect hallucinations in conditional text generated by arXiv:2405.19648v1 [cs.CL] 30 May 2024</p><p>LLMs, achieving success with two classifiers. • Evaluate the performance of this approach across three datasets, comparing it with state-of-the-art methods and highlighting its strengths and weaknesses. • Explore the impact of using the same LLM-Generator vs. different LLMs as evaluators, finding that alternative LLMs provide better indicators to identify hallucinations. • Compare the difference in performance when using smaller LLM evaluators concerning bigger LLM evaluators like LLaMa-Chat-7b <ref type="bibr" target="#b15">[16]</ref>.</p><p>• Feature importance study with ablation and coefficients analysis. This paper is structured as follows. First, we present the related works. Second, we describe our methodology. Next, we offer the experiments performed and the results in three datasets. After that, we present an ablation analysis, followed by the conclusions. Finally, we conclude the paper with the limitations section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The occurrence of hallucinations in LLMs raises concerns, compromising performance in practical implementations like chatbots producing incorrect information. Various research directions have been explored to detect and mitigate hallucinations in different Natural Language Generation tasks <ref type="bibr" target="#b7">[8]</ref>. A text summarization verification system has been proposed to detect and mitigate inaccuracies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8]</ref>. In dialogue generation, hallucinations have been studied with retrieval augmentation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref>. Also, researchers aim to understand why hallucinations occur in different tasks and how these reasons might be connected <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Recent approaches to detect and mitigate hallucinations include self-evaluation <ref type="bibr" target="#b20">[21]</ref> and self-consistency decoding for intricate reasoning tasks <ref type="bibr" target="#b10">[11]</ref>. Knowledge graphs are proposed for gathering evidence <ref type="bibr" target="#b21">[22]</ref>. Token probabilities as an indicator of model certainty have been used, addressing uncertainty in sequential generation tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Scores from conditional language models are used to assess text characteristics <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Additionally, Azaria et al. <ref type="bibr" target="#b13">[14]</ref> trained a classifier that outputs the probability that a statement is truthful based on the hidden layer activations of the LLM as it reads or generates the statement. Recently, the work SelfCheckGPT suggests that LLM's probabilities correlate with factuality <ref type="bibr" target="#b9">[10]</ref>. Furthermore, Su et al. <ref type="bibr" target="#b14">[15]</ref> introduced Modeling of Internal states for hallucination Detection (MIND), an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Finally, a mathematical investigation by Lee et al. <ref type="bibr" target="#b12">[13]</ref> suggests that token probabilities are crucial in generating hallucinations in GPT models under certain assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We implement two classifiers, a Logistic Regression (LR) and a Simple Neural Network (SNN), using four numerical features obtained from the token and vocabulary probabilities from a forward pass to an LLM with the conditional generation approach <ref type="bibr" target="#b26">[27]</ref>. In this section, we described our entire methodology to detect hallucinations on a generated text by an LLM conditioned on a piece of text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Statement</head><p>Given a pair of texts (condition-text, generated-text) that represent the text used to condition the LLM to its generation. We want to detect if a given generated-text is a hallucination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General Pipeline</head><p>Given a set of pairs of texts of the type (condition-text, generated-text) from an LLM (we will call it the LLM-Generator (LLM G )), we extract four numerical features based on the generated tokens and vocabulary tokens probabilities from another LLM that we call the LLM-Evaluator (LLM E ). <ref type="foot" target="#foot_0">1</ref>Then, using these four features, we trained two different classifiers: a Logistic Regression (LR) and a Simple Neural Network (SNN). Finally, we evaluate these classifiers on a test set they did not see before. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Features Description</head><p>This section provides more details on each feature extracted. Every feature is computed using token probabilities and the vocabulary probability distribution corresponding to each token on the generated-text. These can be formally defined as follows: (i) The token probability of each token of the Vocabulary of the LLM E corresponding to t i as P LLM E i (v k ) = (v k |t i-1 , ..., t 1 , c m , ..., c 1 ; θ) for every k. (ii) The token with the highest probability at position i in the generated-text according to LLM E as the v * = arg max k P LLM E i (v k ). (iii) The token with the lowest probability at position i according to LLM E as the v -= arg min k P LLM E i (v k ).</p><p>Next is a natural language description of the four features and, the mathematical definition:</p><p>• Minimum Token Probability (mtp): Minimum of the probabilities that the LLM E gives to the tokens on the generated-text. • Average Token Probability (avgtp): Average of the probabilities that the LLM E gives to the tokens on the generated-text. • Maximum LLM E Probability Deviation (Mpd): Maximum from all the differences between the token with the highest probability according to LLM E at position i and the assigned probability from LLM E to t i which is the token generated by LLM G . • Minimum LLM E Probability Spread (mps): Maximum from all the differences between the token with the highest probability according to LLM E at position i (v * ) and the token with the lowest probability according to LLM E at position i (v -). Formally, these features can be defined as:</p><formula xml:id="formula_0">mtp = min i P LLM E (t i ) avgtp = n i=1 P LLM E (t i ) n M pd = max 1≤i≤n (P LLM E (v * ) -P LLM E (t i )) mps = min 1≤i≤n (P LLM E i (v * ) -P LLM E i (v -))</formula><p>These four numerical features are inspired by the mathematical investigation of the GPT model in <ref type="bibr" target="#b12">[13]</ref>, and recent results in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>, suggesting there is a correlation between the minimum token probability on the generation, the average of the token probabilities, and the average and maximum entropy.</p><p>Lee et al. <ref type="bibr" target="#b12">[13]</ref> proposes that a reliable indicator of hallucination during GPT model generation is the low probability of a token being generated. This is based on the assumption that forcing the model to generate such a low-probability token occurs when the difference between the token with the highest probability and all other tokens is less than a small constant δ. Here, mps is an estimator to avoid the cost of calculating differences across a large vocabulary and generated text.</p><p>Additionally, Azaria et al. <ref type="bibr" target="#b13">[14]</ref> trained a simple classifier called Statement Accuracy Prediction, based on Language Model Activations (SAPLMA) that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. The authors showed that SAPLMA, which leverages the LLM's internal states, performs better than prompting the LLM to state explicitly whether a statement is true or false. Different from <ref type="bibr" target="#b13">[14]</ref>, Su et al. <ref type="bibr" target="#b14">[15]</ref> introduced MIND, an unsupervised training framework that leverages the internal states of LLMs for hallucination detection requiring no manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Distinctive Methodological Approach</head><p>Diverging from these three previous papers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>, the approach here differs in several aspects. This is an empirical paper, not a theoretical one like <ref type="bibr" target="#b12">[13]</ref>. We do not use Self-Consistency as <ref type="bibr" target="#b9">[10]</ref> does, and also, our approach is not zeroshot or few-shot learning. Our approach follows supervised learning like <ref type="bibr" target="#b13">[14]</ref>. However, instead of using the contextual embeddings and hidden layers, we only use four features that result from aggregations of the token and vocabulary probabilities. We also test a simpler Logistic Regression classifier besides a Simple Dense Neural network.</p><p>Moreover, instead of using only the LLM generating the text (LLM G ), the argument is made that depending on the task and model type, different LLM-Evaluators (LLM E ) can provide consistent but quantitatively different results than using probabilities from LLM G . The belief is that probabilities from a different model, varying in architecture, size, parameters, context length, and training data, can also serve as reliable indicators of hallucinations in the text generated by LLM G .</p><p>Since LLM E and LLM G are not always the same, an additional numerical feature, M pd (Maximum LLM E Probability Deviation), is introduced. This feature indicates the difference between the maximum probability token in the vocabulary of LLM E and the token generated by LLM G .</p><p>Using different LLMs as evaluators takes advantage of the diversity of training data among different language models (LLMs) and captures various linguistic patterns and styles. Detecting hallucinations in LLM outputs, such as in LLM G , might be possible through analyzing probability distributions. Yet, specific patterns may remain undetectable, potentially addressed by other models specialized in particular topics. Evaluations from multiple models enhance robustness by mitigating biases inherent in individual models' training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Feature Extraction</head><p>In the previous section, we described the numerical features selected, but the process of extracting these features is still ongoing. To extract the features, we used LLM E models that can be used for the Conditional Generation Task. Notably, in our case, it is a force decoding since the tokens of generatedtext were potentially generated by a different LLM (LLM G ). Instead of letting the model generate the answer token-by-token from the condition-text alone, we provide it with the token predicted by LLM G at each step. This way, LLM E is forced to follow the path to generate the generated-text and, from there, extract the token probabilities from LLM E if it would generate that sequence itself. Then, using these token probabilities, we compute the four numerical features previously described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Models Specification</head><p>The classifiers used are a Logistic Regression <ref type="bibr" target="#b27">[28]</ref> (LR) and a Simple Neural Network (SNN). Both for a data point of the type (condition-text, generated-text) only use the four numerical features extracted. We selected the LR for its simplicity, fast training, and effectiveness in binary classification tasks. However, we implemented a SNN to explore complex nonlinear relationships in the data. The SNN architecture consists of an input layer with four neurons representing each features, followed by two hidden layers, each comprising 512 neurons with the ReLU activation function. Followed by an output layer, containing a single neuron activated by a sigmoid function, suitable for binary classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP AND RESULTS</head><p>In this section, we describe the details of our experimental setup, the dataset we use, and the results we obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Here, we list the three datasets we are using for experimentation and comparison.</p><p>• HaluEval: Hallucination Evaluation for Large Language Models (HaluEval) benchmark is a collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucinations. HaluEval includes 5,000 general user queries with Chat-GPT responses and 30,000 task-specific examples (10,000 per task) from three tasks: question answering, knowledgegrounded dialogue, and text summarization <ref type="bibr" target="#b11">[12]</ref>. • HELM: Hallucination detection Evaluation for multiple LLMs (HELM) benchmark is a list of 3582 sentences from randomly sampling 50,000 articles from WikiText-103 <ref type="bibr" target="#b28">[29]</ref> where the selected LLMs were tasked with prompt-based continuation writing. The resulting sentences were annotated as hallucination or not <ref type="bibr" target="#b14">[15]</ref>. • True-False: Comprises 6,084 sentences divided into the topics of "Cities," "Inventions," "Chemical Elements," "Animals," "Companies," and "Scientific Facts." All sentences in each category are conformed of true statements and false statements <ref type="bibr" target="#b13">[14]</ref>. Unlike HaluEval, this dataset only has generated-text and does not include any condition-text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LLM Evaluators Used</head><p>The LLMs selected as evaluators to study the impact of factors such as architecture, training method, and training data include GPT-2, its large version (gpt2-large) <ref type="bibr" target="#b29">[30]</ref>; Bidirectional and Auto-Regressive Transformers (BART), its CNN-Large version (bart-large-cnn) <ref type="bibr" target="#b30">[31]</ref>; Longformer Encoder-Decoder (LED) <ref type="bibr" target="#b31">[32]</ref>, with the version fine-tuned on the arXiv dataset (led-large-16384-arxiv). Also, we used four bigger LLMs like OPT-6.7B (OPT) <ref type="bibr" target="#b32">[33]</ref>, GPT-J-6.7B (GPT-J) <ref type="bibr" target="#b33">[34]</ref>, LLaMA-2-Chat-7B (LLC-7b) <ref type="bibr" target="#b15">[16]</ref> and Gemma-7b (Gemma) <ref type="bibr" target="#b34">[35]</ref>. We used the known transformers library. 2 In most cases, we utilized the Conditional Generation 2 <ref type="url" target="https://huggingface.co/">https://huggingface.co/</ref> setup. For GPT-2, we employed the GPT2LMHeadModel setup. Additionally, when forwarding inputs to these models with a pair of (condition-text, generated-text), we encountered the challenge of context limitation, which varied depending on the LLM. To address this issue, we did not truncate the generated-text if possible. Instead, if truncation was necessary (with a truncation length of truncate_len), we removed the excess from the condition-text. If additional knowledge was included, we evenly split the truncation between the knowledge and the condition-text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Process of the Classifiers</head><p>To train LR, we used the sklearn library <ref type="foot" target="#foot_1">3</ref> with the Limited-memory Broyden-Fletcher-Goldfarb-Shanno Algorithm solver <ref type="bibr" target="#b36">[37]</ref> and default parameters. The SNN was trained during 10 4 epochs. We used the Adam <ref type="bibr" target="#b37">[38]</ref> optimizer with learning rate of 10 -3 . All experiments, including feature extraction, training, and evaluation of the classifiers, were conducted on an NVIDIA L40S GPU core with 48GB of memory. Given a dataset and a single LLM E , training takes anywhere from 30 minutes (HELM) to 4.5 hours (HaluEval), depending on the size of the training data and the length of the condition-text and generated-text.</p><p>1) HaluEval: We train both classifiers for each of the tasks in the HaluEval benchmark. Each data point is split into two data points: (condition-text, right-answer) and (condition-text, hallucinated-answer). Therefore, the datasets would be of 20,000 examples for each of the Question Answering (QA), Knowledge-Grounded Dialogue (KGD), and Summarization tasks where in each case half of the dataset is comprised of data points of the type (condition-text, right-answer) and the other half are of the type (condition-text, hallucinated-answer).</p><p>In the case of the General-User Queries, the dataset is already in that format, with each data point classified as a hallucination. Therefore, the dataset size is the same, which is 5,000.</p><p>Then, with this adaptation of the HaluEval benchmark dataset when we were approaching a given task, we will sample 10% of the data points (half with the right-answer and the other with a hallucinated-answer) randomly. These 10% data points are used to train both classifiers, and we test the model capabilities on the remaining 90% of the dataset for a given task.</p><p>2) HELM: In the HELM dataset <ref type="bibr" target="#b14">[15]</ref> the sentences were separated into categories depending on which LLM generated them. Therefore, when we wanted to evaluate the sentences generated by a given LLM like LlaMa-Chat-7b (LLC-7b), we would train on all other sentences produced by other LLMs.</p><p>3) True-False: We followed the same process as Azaria et al. <ref type="bibr" target="#b13">[14]</ref>. We pick a category like "animals" for testing and train in all other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) HaluEval:</head><p>We evaluate each classifier trained on the 10% data of the given task on the other 90%. We selected the following metrics: Accuracy, F 1 , and Precision-Recall</p><p>TABLE I AVERAGE RESULTS IN THE TEST SET FOR EACH TASK IN THE HALUEVAL BENCHMARK GIVEN THE SELECTED LLM E . NC ACC STANDS FOR ACCURACY WITHOUT condition-text AND K ACC FOR ACCURACY INCLUDING EXTRA KNOWLEDGE. Summarization Question Answering KGD Models Acc F1 PR AUC NC Acc Acc F1 PR AUC NC Acc K Acc Acc F1 PR AUC NC Acc K Acc GPT-2 0.82 0.78 0.89 0.90 0.82 0.82 0.86 0.78 0.88 0.62 0.60 0.70 0.64 0.62 BART 0.77 0.78 0.83 0.99 0.95 0.95 0.97 0.96 0.94 0.66 0.63 0.74 0.65 0.60 LED 0.97 0.76 0.81 0.97 0.88 0.88 0.91 0.86 0.87 0.62 0.61 0.70 0.62 0.60 OPT 0.98 0.98 0.99 0.99 0.79 0.78 0.85 0.76 0.79 0.66 0.67 0.74 0.67 0.61 GPT-J 0.98 0.98 0.99 0.99 0.77 0.78 0.84 0.73 0.83 0.66 0.67 0.74 0.67 0.66 LLC-7b 0.67 0.68 0.69 0.77 0.73 0.69 0.81 0.75 0.77 0.60 0.54 0.64 0.63 0.61 Gemma 0.51 0.51 0.52 0.57 0.76 0.73 0.82 0.79 0.71 0.58 0.58 0.66 0.62 0.55 TABLE II RESULTS TAKEN FROM [36] MEASURED IN ACCURACY (%) ON THE HALU-EVAL DATASET. Models QA KGD Summ. GUQ ChatGPT 62.59 72.40 58.53 79.44 Claude 2 69.78 64.73 57.75 75.00 Claude 67.60 64.83 53.76 73.88 Davinci-003 49.65 68.37 48.07 80.40 Davinci-002 60.05 60.81 47.77 80.42 GPT-3 49.21 50.02 51.23 72.72 Llama-2-Ch 49.60 43.99 49.55 20.46 ChatGLM 6B 47.93 44.41 48.57 30.92 Falcon 7B 39.66 29.08 42.71 18.98 Vicuna 7B 60.34 46.35 45.62 19.48 Alpaca 7B 6.68 17.55 20.63 9.54 TABLE III OUR RESULTS FOR EACH LLM E AND TASK USING THE LR CLASSIFIER AND MEASURE IN ACCURACY ON THE TEST SET. Model Summ. QA KGD GUQ GPT-2 0.66 0.77 0.62 0.77 BART 0.65 0.94 0.49 0.54 LED 0.55 0.87 0.62 0.52 OPT 0.73 0.75 0.61 0.80 GPT-J 0.90 0.75 0.61 0.81 LLC-7b 0.70 0.74 0.55 0.81 Gemma 0.52 0.73 0.53 0.80</p><p>Area Under Curve (PR AUC ). Table <ref type="table">II</ref> shows current stateof-the-art results in HaluEval. All the results were extracted from <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref>, which is the paper that introduces the HaluEval benchmark and an empirical study from the same authors on factuality hallucination in LLMs. Next, Table <ref type="table">III</ref> shows the accuracy results on the test set for each task using every LLM E selected and the Logistic Regression as the classifier. As it can be appreciated, the Logistic Regression obtains great results compared to what previous approaches would have gotten on the 90% of the dataset. Finally, Table <ref type="table">I</ref> shows our average 4 results per model of our approach in each metric evaluated on the test set for each of the tasks of Summarization, QA, and KGD respectively. The methods of Table II are based on In-Context-Learning approaches and evaluated in 100% of the dataset. In contrast, our approach utilizes supervised learning, but we believe it's 4 Average results with data sampled randomly in three runs. fair to compare it to existing methods. We train our models using only 10% of the data, reserving the remaining 90% for testing. We argue that the performance of current approaches on the 90% test set will not deviate significantly from their performance on the full dataset, especially when there's a significant accuracy difference. Consequently, we've chosen not to present our results alongside the baseline results in Table <ref type="table">II</ref>. For an exact comparison, we would need to apply their methods to the same test set we've used but we do not have the resources for such heavy computation. The key findings in the results showed in Tables <ref type="table">III</ref> and <ref type="table">Table</ref> I are summarized as follows:</p><p>First, our classifiers trained on only 10% of the data demonstrate effectiveness on the test set (the other 90%) for the Summarization and Question Answering tasks using as LLM E the GPT-J and BART respectively. The best results in both tasks have an accuracy and F 1 over 98% with the SNN classifier and over 90% with the LR classifier in Summarization and F 1 over 95% in QA. These results outperform the results previous approaches would have gotten on the same test set.</p><p>Additionally, while not surpassing the state-of-the-art, we achieved competitive results in the dialogue task. Finally, on the GUQ task, a table was not included for the SNN classifier since we obtained similar results for each LLM E . When employing various LLM E models with the SNN classifier, results suggest overfitting to the negative class, yielding an accuracy of 81%, F 1 of 1%, and PR AUC of 10%. This can be attributed to data imbalance, where only about 20% are not hallucinations. An alternative attempt with a training set of 500 positive and 500 negative examples tested on the remaining 4,000 revealed little success, with a best accuracy at 69% and F 1 at 0.23%. Also, in the tasks of QA and KGD, our method, including the knowledge in the condition-text, improved the accuracy, while in a few, it did not. Adding knowledge can help LLM evaluators provide meaningful token probabilities for any task with our approach.</p><p>An unexpected finding we encountered was that when the LLM E only provided probabilities for the generated-text without the condition-text, it yielded remarkably high results in Summarization and QA tasks with specific LLM E models like GPT-J, achieving up to 99% accuracy or BART with 96%. This anomaly prompted us to verify that we had not inadvertently used testing data for training or made similar errors. However, this was not the case and supported by the fact that it did not happen in KGD and GUQ, using the same code, nor did it occur with all LLM E models in Summarization and QA. We hypothesize that there may be a probabilistic pattern in the Summarization and QA tasks within the HaluEval dataset generation process that our approach can learn with some LLM E . Note that this observation does not render the benchmark useless; instead, it suggests that a supervised approach may not be the most suitable fit, and instead, unsupervised approaches might be more appropriate for evaluation in this benchmark.</p><p>Despite the significant improvement achieved by selecting specific LLM E models in certain tasks, using other LLM E models still yielded competitive results, with some instances even surpassing state-of-the-art benchmarks. For example, OPT coupled with the SNN classifier achieved an F 1 score of 79% in the QA task, while GPT-2 attained an 82% F 1 score.</p><p>2) HELM: Table <ref type="table">IV</ref> shows the results of our approach in the HELM benchmark <ref type="bibr" target="#b14">[15]</ref>. The overall results showed that our approach did not surpass MIND <ref type="bibr" target="#b14">[15]</ref> except with LLC-13B sentences. However, we only use four features, and still, our approach surpasses the results of other methods like SAPLMA <ref type="bibr" target="#b13">[14]</ref> and, in some cases, SelfCheckGPT with Natural Language Inference (SCG-NLI) <ref type="bibr" target="#b9">[10]</ref> and others reported in <ref type="bibr" target="#b14">[15]</ref>. Also, we showed in the Appendix section how removing the condition-text affects our approach, which, different from the Halu-Eval, in this dataset, decreases performance as expected.</p><p>Still, unlike MIND, which gets its training data unsupervised without annotation, we are training with the annotated data they provided in their HELM benchmark.</p><p>3) True-False: The results obtained in this dataset using any of our selected LLMs were below the baseline provided by <ref type="bibr" target="#b13">[14]</ref> and significantly lower than their approach. This highlights a major weakness in our methodology and points out the importance of utilizing hidden layers as features. We recommend that any future method demonstrate its performance on this challenging dataset. Detailed results are provided in</p><p>TABLE IV RESULTS OF OUR APPROACH AND PREVIOUS METHODS IN THE HELM BENCHMARK MEASURED IN PR AUC . Baselines Falcon GPT-J LLB -7B LLC -13B LLC -7B OPT PE-max 0.648 0.750 0.685 0.444 0.493 0.726 SCG-NLI 0.685 0.868 0.764 0.583 0.657 0.810 SAPLMA 0.513 0.699 0.578 0.305 0.407 0.621 MIND 0.790 0.877 0.788 0.604 0.676 0.884 Ours GPT-2 0.683 0.847 0.759 0.618 0.616 0.850 BART 0.710 0.828 0.695 0.569 0.568 0.825 LED 0.683 0.809 0.722 0.527 0.548 0.829 OPT 0.719 0.839 0.773 0.634 0.637 0.864 GPT-J 0.702 0.808 0.751 0.642 0.588 0.834 LLC-7b 0.727 0.855 0.785 0.563 0.644 0.842 Gemma 0.738 0.850 0.786 0.601 0.651 0.843</p><p>the Appendix section. 4) Overall Conclusions from Results: In general, our results demonstrate that our supervised learning approach, utilizing only four features, exhibits competitive performance compared to current methods. Our approach surpasses the current state-ofthe-art methods across various scenarios in tasks and datasets.</p><p>In the HaluEval benchmark, where generations originate from ChatGPT powered by GPT-3.5, we found that some of the top-performing LLMs were two GPT-based models (GPT-2 and GPT-J), which are the nearest to the LLM-Generator that we were able to test. Interestingly, even models not based on the LLM-Generator, such as OPT, BART, LED, achieved comparable results and surpassed them in some tasks. Notably, smaller models like BART outperformed all LLM evaluators in the QA task, suggesting that varying LLMs, regardless of size, can yield superior results due to training data and architecture differences.</p><p>In the HELM benchmark, where each test set comprised sentences generated by accessible LLMs, we explored the impact of using the actual LLM-Generator as the LLM-Evaluator (like the case of GPT-J or LLC-7B) compared to using a different one. Results revealed that different LLMs as evaluators often yielded similar or better results than the corresponding LLM-Generator, showing the advantages of employing diverse LLMs for evaluation purposes. Furthermore, our experiments demonstrated that the performance disparity between larger LLMs like GPT-J and LLC-7B versus smaller ones such as GPT-2 and BART is not significant in many scenarios.</p><p>Finally, in the True-False dataset, it became evident that our method exhibits weaknesses when applied to this type of data, and the features lack the necessary significance for detecting hallucinations present in that dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Feature Importance Analysis -Ablation</head><p>We performed experiments using single numerical feature to determine which features were significant or not to the results. Table <ref type="table">V</ref> showed for three tasks in HaluEval and three LLM E how the results in accuracy were affected by which features were used or not. In most cases, the use of all features provides the best results. Then, when each feature was used alone, we discovered that the most meaningful features in our approach were mtp and avgtp, especially in Summarization and QA. However, in the KGD task, it was more important for bigger models like GPT-J and LLC-7b, the feature introduced by this paper: M pd.</p><p>Additionally, in the Appendix, we conduct a feature analysis using the coefficients obtained from the LR classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORKS</head><p>This work introduced a supervised learning approach for identifying hallucinations in conditional text generated by LLMs. Leveraging just four features derived from conditional token probabilities, our approach demonstrated competitive performance compared to existing methodologies across various tasks and datasets. Future work includes exploring hybrid</p><p>TABLE V FEATURE IMPORTANCE BASED ON ACCURACY FOR THREE TASKS IN THE HALUEVAL BENCHMARK GIVEN THREE LLM E . Features Summarization Question Answering KGD mtp avgtp Mpd mps GPT-J BART LLC-7b GPT-J BART LLC-7b GPT-J BART LLC-7b ✓ ✓ ✓ ✓ 0.98 0.77 0.69 0.76 0.95 0.74 0.66 0.65 0.60 ✓ 0.50 0.79 0.50 0.64 0.92 0.50 0.56 0.60 0.50 ✓ 0.98 0.64 0.57 0.69 0.95 0.51 0.62 0.66 0.50 ✓ 0.50 0.61 0.54 0.72 0.90 0.73 0.62 0.58 0.61 ✓ 0.51 0.62 0.60 0.62 0.64 0.57 0.53 0.53 0.52 methods that combine In-Context-Learning approaches with probabilistic-based methods, including supervised classifiers.</p><p>Through extensive evaluation across three datasets, we uncovered insights into the effectiveness of our approach. Our exploration of using different LLMs as evaluators further emphasized the advantages of employing diverse models for evaluation purposes. By comparing results obtained from using the actual LLM-Generator as the evaluator against those from different LLMs, we showcased the potential for alternative models to yield comparable or even superior evaluation outcomes. In future work, it is possible to investigate advanced ensemble learning techniques to further enhance the performance of hallucination detection systems by effectively combining predictions from multiple LLM evaluators. Furthermore, we identified weaknesses in our approach when applied to datasets like the True-False dataset. Future research could explore whether augmenting our four features with hidden layer features could improve the state-of-the-art performance on that dataset.</p><p>This research extends to every domain relying on LLMs. By enhancing the reliability of LLM outputs, the proposed method contributes to the ethical use of these models in sensitive applications, such as medical, legal, educational, and financial domains. This work is a step toward creating a reliable method to detect hallucinations in LLMs based on token and vocabulary probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIMITATIONS</head><p>The first limitation is the numerical features and models selected as LLM E . While our current approach has demonstrated effectiveness in specific tasks, it may only capture the richness and complexity of some textual content types. The derived features must be more meaningful for tasks like Knowledge-Grounded Dialogue (KGD), which involve intricate context and real-time exchanges.</p><p>Our method outperformed state-of-the-art tasks like Summarization and Question Answering in HaluEval. However, in KGD and General User Queries, it achieved competitive but not leading results. This could hint at potential over-specialization or the need for task-specific feature engineering. Another reason could be the inherent limitations of the LLMs selected as LLM E .</p><p>Additionally, because of the context length limitation of some of the LLMs, we needed to truncate the condition-text or extra knowledge, which might cause us to lose the necessary context to get the correct token probabilities to classify correctly. However, some of the LLMs needed more context length to use everything without losing information.</p><p>One of the main limitations is that our approach's results and effectiveness may be tied to the characteristics of the datasets used. The model's performance could be skewed if the dataset has inherent biases or lacks diversity in certain aspects. For instance, it might be in the specific patterns obtained on the HaluEval benchmark that these four numerical features are good indicators for detecting this type of hallucination. However, it does not change the fact that current complex state-of-the-art approaches have yet to show this level of performance under the same circumstances.</p><p>Although we achieved competitive results in the HELM benchmark, it is worth noting that, apart from SALPMA, most other approaches relied on unsupervised methods. As our method is supervised, it inherently depends on curated and annotated data, which poses a limitation. Additionally, the performance in the True-False dataset highlights a weakness in our approach, suggesting that it may need to be augmented with additional features to be effective in a broader range of scenarios.</p><p>Finally, this method is grounded in binary classification. In real-world scenarios, hallucination might be more nuanced, with varying degrees of severity, which our current approach might not account for. Furthermore, there needs to be more interpretability; even when we can get intuition from the numerical features, we cannot obtain the exact explanation of what specific wrong fact or fictitious information is being added. We intend to explore other ideas on datasets that make this separation to increase the interpretability.</p><p>TABLE VI RESULTS OF OUR APPROACH AND PREVIOUS METHODS IN THE TRUE-FALSE DATASET MEASURED IN ACCURACY. THE SALPMA RESULTS SHOWN ARE USING THE 16TH HIDDEN LAYER WITH LLC-7B. Model Cities Invent. Elem. Anim. Comp Facts BERT-5-shot 0.5416 0.4799 0.5676 0.5643 0.5540 0.5148 SAPLMA 0.9223 0.8938 0.6939 0.7774 0.8658 0.8254 Ours GPT-2 0.4312 0.5353 0.4924 0.4920 0.5041 0.5049 BART 0.3846 0.5365 0.5172 0.4920 0.4550 0.4607 LED 0.4985 0.4954 0.5182 0.5357 0.5191 0.4787 OPT 0.4950 0.5479 0.5118 0.4573 0.5050 0.5392 GPT-J 0.5023 0.5308 0.5268 0.4871 0.5283 0.5408 LLC-7b 0.5182 0.5216 0.5267 0.5287 0.5208 0.5669 Gemma 0.5091 0.5205 0.4870 0.4692 0.4983 0.4705 TABLE VII RESULTS OF OUR APPROACH AND PREVIOUS METHODS IN THE HELM BENCHMARK MEASURED IN PR AUC WITHOUT condition-text. Baselines Falcon GPT-J LLB-7B LLC-13B LLC-7B OPT PE-max 0.6479 0.7497 0.6851 0.4439 0.4931 0.7263 SCG-NLI 0.6846 0.8680 0.7644 0.5834 0.6565 0.8103 SAPLMA 0.5128 0.6987 0.5777 0.3047 0.4066 0.6212 MIND 0.7895 0.8774 0.7876 0.6043 0.6755 0.8835 Ours GPT-2 0.7110 0.8097 0.7384 0.5194 0.6085 0.7994 BART 0.6853 0.8129 0.7139 0.5624 0.5686 0.8258 LED 0.7017 0.8424 0.6931 0.5194 0.5494 0.8204 OPT 0.7163 0.7748 0.6695 0.5608 0.6751 0.7773 GPT-J 0.7051 0.7873 0.6984 0.6121 0.5856 0.7989 LLC-7b 0.6968 0.8403 0.7423 0.5464 0.6370 0.8395 Gemma 0.6872 0.8256 0.7319 0.5788 0.6428 0.8265</p><p>TABLE VIII SCALED LOGISTIC REGRESSION COEFFICIENTS ASSIGNED TO EACH INPUT FEATURE BASED ON THE SELECTED LLM E . Summ. QA KGD Models mtp avgt Mpd mps mtp avgt Mpd mps mtp avgt Mpd mps GPT-2 1.0 127.8 0.69 17.69 1.01 1.0 0.01 957.36 1.0 1.1 0.03 15.88 BART 0.14 1.04 51.28 0.017 0.005 0.0017 1.49 0.48 1.5 0.003 4.4 2.03 LED 0.05 0.81 0.61 1.34 1.0 1.26 0.007 31.0 24.86 0.004 0.1 2.9 OPT 1.0 1791.86 0.93 70.01 1.0 1.0 0.006 232.66 1.0 3.02 0.02 3.41 GPT-J 1.0 1956.69 0.93 3.45 1.1 1.0 0.006 223.04 1.0 3.38 0.03 4.6 LLC-7b 1.0 3.03 1.1 0.03 1.0 11.3 0.002 22.97 1.0 0.85 0.03 3.94 Gemma 1.0 2.36 1.0 5.75 1.0 4.7 0.0004 21.5 1.0 0.86 0.1 3.29</p><p>highlights the significance of the condition-text and once again suggests that the anomaly observed in the HaluEval dataset in the Summarization and QA tasks may be attributed to a probabilistic pattern in the generated-text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Importance Analysis -Logistic Regression Coefficients</head><p>Additionally, we extracted the Logistic Regression coefficients for each feature in each task. Due to the utilization of the logit function, the coefficients in logistic regression signify the logarithm of the odds that an observation belongs to the target class ("1") based on the values of its input variables. Therefore, to interpret these coefficients appropriately, they must be transformed into regular odds. We achieved this by exponentiating the logarithmic odds coefficients, a task easily accomplished using the np.exp() function.</p><p>The interpretation in this case can be read as: for each incremental unit rise in the given input variable (for example, the feature avgtp), the likelihood of the observation belonging to the positive class increases by a factor of the value of the coefficient, while maintaining all other variables constant, compared to the odds of it being in the negative class.</p><p>Table <ref type="table">VIII</ref> indicates that for the LR classifier, the most significant features during Summarization were predominantly avgtp, but also occasionally included Mpd and mps. However, in the QA task, the most relevant feature consistently remained mps. Similarly, in the KGD task, mps was predominantly critical, but occasionally mtp and Mpd also played a role. However, it's important to note that LR does not achieve results as strong as the SNN. Therefore, the feature importance of SNN, as shown in Table <ref type="table">V</ref>, may carry more weight.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. General Pipeline of the Proposed Methodology.</figDesc><graphic coords="3,48.96,50.54,514.08,125.04" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Which could be the same as LLM G .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://pypi.org/project/sklearn/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. True-False Dataset results</head><p>In this appendix section, Table VI provides the exact numerical results of our approach in the True-False dataset, which, as discussed in the paper, yielded significantly weak results. Once again, we want to highlight that while this paper demonstrates competitive and achievable performance on many tasks of HaluEval and HELM using only four features, the performance in this dataset was notably poor. Therefore, we strongly recommend that future hallucination evaluations with supervised approaches be conducted on a dataset like this or on a dataset where a rapid method like ours has been tested to ensure no clear probabilistic pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HELM results without condition-text</head><p>In this section, Table <ref type="table">VII</ref> presents the results of our approach in the HELM benchmark using the token probabilities obtained solely from the generated-text. We observe how the performance was impacted when using most of the LLM E models in comparison with the results in Table <ref type="table">IV</ref>. This</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of large language models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Challenges and applications of large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mozes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mchardy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An exploratory survey about using chatgpt in education, healthcare, and research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Liebovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2023" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mini-dalle3: Interactive text to image by prompting large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zeqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xizhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jifeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wenhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07653</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Are emergent abilities in large language models just in-context learning?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bigoulaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Madabushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01809</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Chain of natural language inference for reducing large language model ungrounded hallucinations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03951</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Siren&apos;s song in the ai ocean: A survey on hallucination in large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.01219</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liusie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12">Dec. 2023</date>
			<biblScope unit="page" from="9004" to="9017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=1PL1NIMMrw" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HaluEval: A large-scale hallucination evaluation benchmark for large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12">Dec. 2023</date>
			<biblScope unit="page" from="6449" to="6464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A mathematical investigation of hallucination and creativity in gpt models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">2320</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The internal state of an LLM knows when it&apos;s lying</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.68" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12">Dec. 2023</date>
			<biblScope unit="page" from="967" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised real-time hallucination detection based on the internal states of large language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.06448</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reducing quantity hallucinations in abstractive summarization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.203" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="2237" to="2249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieval augmentation reduces hallucination in conversation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>-T</surname></persName>
		</editor>
		<editor>
			<persName><surname>Yih</surname></persName>
		</editor>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">Nov. 2021</date>
			<biblScope unit="page" from="3784" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Why does chatgpt fall short in answering questions faithfully?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10513</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diving deep into modes of fact hallucinations in dialogue systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.findings-emnlp.48" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">Dec. 2022</date>
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do large language models know what they don&apos;t know?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.findings-acl.551" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07">Jul. 2023</date>
			<biblScope unit="page" from="8653" to="8665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">StructGPT: A general framework for large language model to reason over structured data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12">Dec. 2023</date>
			<biblScope unit="page" from="9237" to="9251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On hallucination and predictive uncertainty in conditional language generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Merlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Tiedemann</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tsarfaty</surname></persName>
		</editor>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">Apr. 2021</date>
			<biblScope unit="page" from="2734" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Uncertainty estimation in autoregressive structured prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07650</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bartscore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="263" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04166</idno>
		<title level="m">Gptscore: Evaluate as you desire</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A survey of controllable text generation using transformerbased pre-trained language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="DOI">10.1145/3617680</idno>
		<ptr target="https://doi.org/10.1145/3617680" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023-10">oct 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reading and Understanding Multivariate Statistics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="217" to="244" />
		</imprint>
	</monogr>
	<note>Logistic regression</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">Jul. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gpt-j-6b: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dadashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Love</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Gemma: Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The dawn after the dark: An empirical study on factuality hallucination in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.03205</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Limited memory broyden-fletcher-goldfarb-shanno (l-bfgs) method for the parameter estimation on geographically weighted ordinal logistic regression model (gwolr)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R S</forename><surname>Saputro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Widyaningsih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIP Conference Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
