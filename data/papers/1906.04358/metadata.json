{
  "arxivId": "1906.04358",
  "title": "Weight Agnostic Neural Networks",
  "authors": "Adam Gaier, David Ha",
  "abstract": "Not all neural network architectures are created equal, some perform much\nbetter than others for certain tasks. But how important are the weight\nparameters of a neural network compared to its architecture? In this work, we\nquestion to what extent neural network architectures alone, without learning\nany weight parameters, can encode solutions for a given task. We propose a\nsearch method for neural network architectures that can already perform a task\nwithout any explicit weight training. To evaluate these networks, we populate\nthe connections with a single shared weight parameter sampled from a uniform\nrandom distribution, and measure the expected performance. We demonstrate that\nour method can find minimal neural network architectures that can perform\nseveral reinforcement learning tasks without weight training. On a supervised\nlearning domain, we find network architectures that achieve much higher than\nchance accuracy on MNIST using random weights. Interactive version of this\npaper at https://weightagnostic.github.io/",
  "url": "https://arxiv.org/abs/1906.04358",
  "issue_number": 235,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/235",
  "created_at": "2025-01-04T15:03:24.864204",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 20,
  "last_read": "2025-01-04T15:03:24.866258",
  "last_visited": "2024-12-24T03:27:56.957Z",
  "main_tex_file": null,
  "published_date": "2019-06-11T02:40:11Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.NE",
    "stat.ML"
  ]
}