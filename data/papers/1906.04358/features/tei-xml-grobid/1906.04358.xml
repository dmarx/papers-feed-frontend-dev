<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weight Agnostic Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-09-05">5 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Adam</forename><surname>Gaier</surname></persName>
							<email>adam.gaier@h-brs.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems (NeurIPS 2019</orgName>
								<orgName type="institution" key="instit1">Bonn-Rhein-Sieg University of Applied Sciences Inria / CNRS</orgName>
								<orgName type="institution" key="instit2">Université de Lorraine</orgName>
								<orgName type="institution" key="instit3">Google Brain Tokyo</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country>Japan Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Ha</surname></persName>
							<email>hadavid@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems (NeurIPS 2019</orgName>
								<orgName type="institution" key="instit1">Bonn-Rhein-Sieg University of Applied Sciences Inria / CNRS</orgName>
								<orgName type="institution" key="instit2">Université de Lorraine</orgName>
								<orgName type="institution" key="instit3">Google Brain Tokyo</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<country>Japan Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weight Agnostic Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-05">5 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">B27751CBF805A8BFD096EFE20248EE0A</idno>
					<idno type="arXiv">arXiv:1906.04358v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at <ref type="url" target="https://weightagnostic.github.io/">https://weightagnostic.github.io/</ref> † Work done while at Google Brain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In biology, precocial species are those whose young already possess certain abilities from the moment of birth. There is evidence to show that lizard <ref type="bibr" target="#b79">[79]</ref> and snake <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b82">82]</ref> hatchlings already possess behaviors to escape from predators. Shortly after hatching, ducks are able to swim and eat on their own <ref type="bibr" target="#b111">[111]</ref>, and turkeys can visually recognize predators <ref type="bibr" target="#b28">[29]</ref>. In contrast, when we train artificial agents to perform a task, we typically choose a neural network architecture we believe to be suitable for encoding a policy for the task, and find the weight parameters of this policy using a learning algorithm. Inspired by precocial behaviors evolved in nature, in this work, we develop neural networks with architectures that are naturally capable of performing a given task even when their weight parameters are randomly sampled. By using such neural network architectures, our agents can already perform well in their environment without the need to learn weight parameters. Decades of neural network research have provided building blocks with strong inductive biases for various task domains. Convolutional networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">61]</ref> are especially suited for image processing <ref type="bibr" target="#b16">[17]</ref>.</p><p>Recent work <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b117">117]</ref> demonstrated that even randomly-initialized CNNs can be used effectively for image processing tasks such as superresolution, inpainting and style transfer. Schmidhuber et al. <ref type="bibr" target="#b103">[103]</ref> have shown that a randomly-initialized LSTM <ref type="bibr" target="#b49">[49]</ref> with a learned linear output layer can predict time series where traditional reservoir-based RNNs <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b99">99]</ref> fail. More recent developments in self-attention <ref type="bibr" target="#b121">[121]</ref> and capsule <ref type="bibr" target="#b100">[100]</ref> networks expand the toolkit of building blocks for creating architectures with strong inductive biases for various tasks. Fascinated by the intrinsic capabilities of randomly-initialized CNNs and LSTMs, we aim to search for weight agnostic neural networks, architectures with strong inductive biases that can already perform various tasks with random weights.</p><p>In order to find neural network architectures with strong inductive biases, we propose to search for architectures by deemphasizing the importance of weights. This is accomplished by (1) assigning a single shared weight parameter to every network connection and (2) evaluating the network on a wide range of this single weight parameter. In place of optimizing weights of a fixed network, we optimize instead for architectures that perform well over a wide range of weights. We demonstrate our approach can produce networks that can be expected to perform various continuous control tasks with a random weight parameter. As a proof of concept, we also apply our search method on a supervised learning domain, and find it can discover networks that, even without explicit weight training, can achieve a much higher than chance test accuracy of ∼ 92% on MNIST. We hope our demonstration of such weight agnostic neural networks will encourage further research exploring novel neural network building blocks that not only possess useful inductive biases, but can also learn using algorithms that are not necessarily limited to gradient-based methods. <ref type="foot" target="#foot_0">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work has connections to existing work not only in deep learning, but also to various other fields:</p><p>Architecture Search Search algorithms for neural network topologies originated from the field of evolutionary computing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b76">76,</ref><ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b87">87,</ref><ref type="bibr" target="#b92">92,</ref><ref type="bibr" target="#b116">116,</ref><ref type="bibr" target="#b124">124,</ref><ref type="bibr" target="#b126">126]</ref>. Our method is based on NEAT <ref type="bibr" target="#b110">[110]</ref>, an established topology search algorithm notable for its ability to optimize the weights and structure of networks simultaneously. In order to achieve state-of-the-art results, recent methods narrow the search space to architectures composed of basic building blocks with strong domain priors such as CNNs <ref type="bibr" target="#b70">[70,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b95">95,</ref><ref type="bibr" target="#b128">128]</ref>, recurrent cells <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b128">128]</ref> and self-attention <ref type="bibr" target="#b107">[107]</ref>.</p><p>It has been shown that random search can already achieve SOTA results if such priors are used <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b94">94,</ref><ref type="bibr" target="#b104">104]</ref>. The inner loop for training the weights of each candidate architecture before evaluation makes the search costly, although efforts have been made to improve efficiency <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b91">91]</ref>. In our approach, we evaluate architectures without weight training, bypassing the costly inner loop, similar to the random trial approach in <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b106">106]</ref> that evolved architectures to be more weight tolerant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Neural Networks</head><p>The weight parameters of a BNN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b74">74,</ref><ref type="bibr" target="#b84">84]</ref> are not fixed values, but sampled from a distribution. While the parameters of this distribution can be learned <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b59">59]</ref>, the number of parameters is often greater than the number of weights. Recently, Neklyudov et al. <ref type="bibr" target="#b85">[85]</ref> proposed variance networks, which sample each weight from a distribution with a zero mean and a learned variance parameter, and show that ensemble evaluations can improve performance on image recognition tasks. We employ a similar approach, sampling weights from a fixed uniform distribution with zero mean, as well as evaluating performance on network ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithmic Information Theory</head><p>In AIT <ref type="bibr" target="#b108">[108]</ref>, the Kolmogorov complexity <ref type="bibr" target="#b56">[56]</ref> of a computable object is the minimum length of the program that can compute it. The Minimal Description Length (MDL) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b98">98]</ref> is a formalization of Occam's razor, in which a good model is one that is best at compressing its data, including the cost of describing of the model itself. Ideas related to MDL for making neural networks "simple" was proposed in the 1990s, such as simplifying networks by soft-weight sharing <ref type="bibr" target="#b86">[86]</ref>, reducing the amount of information in weights by making them noisy <ref type="bibr" target="#b47">[47]</ref>, and simplifying the search space of its weights <ref type="bibr" target="#b102">[102]</ref>. Recent works offer a modern treatment <ref type="bibr" target="#b6">[7]</ref> and application <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b115">115]</ref> of these principles in the context of larger, deep neural network architectures.</p><p>While the aforementioned works focus on the information capacity required to represent the weights of a predefined network architecture, in this work we focus on finding minimal architectures that can represent solutions to various tasks. As our networks still require weights, we borrow ideas from AIT and BNN, and take them a bit further. Motivated by MDL, in our approach, we apply weight-sharing to the entire network and treat the weight as a random variable sampled from a fixed distribution.</p><p>Network Pruning By removing connections with small weight values from a trained neural network, pruning approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b72">72,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b81">81]</ref> can produce sparse networks that keep only a small fraction of the connections, while maintaining similar performance on image classification tasks compared to the full network. By retaining the original weight initialization values, these sparse networks can even be trained from scratch to achieve a higher test accuracy <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b65">65]</ref> than the original network. Similar to our work, a concurrent work <ref type="bibr" target="#b127">[127]</ref> found pruned networks that can achieve image classification accuracies that are much better than chance even with randomly initialized weights.</p><p>Network pruning is a complementary approach to ours; it starts with a full, trained network, and takes away connections, while in our approach, we start with no connections, and add complexity as needed. Compared to our approach, pruning requires prior training of the full network to obtain useful information about each weight in advance. In addition, the architectures produced by pruning are limited by the full network, while in our method there is no upper bound on the network's complexity.</p><p>Neuroscience A connectome <ref type="bibr" target="#b105">[105]</ref> is the "wiring diagram" or mapping of all neural connections of the brain. While it is a challenge to map out the human connectome <ref type="bibr" target="#b109">[109]</ref>, with our 90 billion neurons and 150 trillion synapses, the connectome of simple organisms such as roundworms <ref type="bibr" target="#b120">[120,</ref><ref type="bibr" target="#b122">122]</ref> has been constructed, and recent works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b112">112]</ref> mapped out the entire brain of a small fruit fly.</p><p>A motivation for examining the connectome, even of an insect, is that it will help guide future research on how the brain learns and represents memories in its connections. For humans it is evident, especially during early childhood <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b114">114]</ref>, that we learn skills and form memories by forming new synaptic connections, and our brain rewires itself based on our new experiences <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b55">55]</ref>.</p><p>The connectome can be viewed as a graph <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b118">118]</ref>, and analyzed using rich tools from graph theory, network science and computer simulation. Our work also aims to learn network graphs that can encode skills and knowledge for an artificial agent in a simulation environment. By deemphasizing learning of weight parameters, we encourage the agent instead to develop ever-growing networks that can encode acquired skills based on its interactions with the environment. Like the connectome of simple organisms, the networks discovered by our approach are small enough to be analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weight Agnostic Neural Network Search</head><p>Creating network architectures which encode solutions is a fundamentally different problem than that addressed by neural architecture search (NAS). The goal of NAS techniques is to produce architectures which, once trained, outperform those designed by humans. It is never claimed that the solution is innate to the structure of the network. Networks created by NAS are exceedingly 'trainable' -but no one supposes these networks will solve the task without training the weights. The weights are the solution; the found architectures merely a better substrate for the weights to inhabit.</p><p>To produce architectures that themselves encode solutions, the importance of weights must be minimized. Rather than judging networks by their performance with optimal weight values, we can instead measure their performance when their weight values are drawn from a random distribution.</p><p>Replacing weight training with weight sampling ensures that performance is a product of the network topology alone. Unfortunately, due to the high dimensionality, reliable sampling of the weight space is infeasible for all but the simplest of networks.</p><p>Though the curse of dimensionality prevents us from efficiently sampling high dimensional weight spaces, by enforcing weight-sharing on all weights, the number of weight values is reduced to one. Systematically sampling a single weight value is straight-forward and efficient, enabling us to approximate network performance in only a handful of trials. This approximation can then be used to drive the search for ever better architectures.</p><p>The search for these weight agnostic neural networks (WANNs) can be summarized as follows (See Figure <ref type="figure" target="#fig_1">2</ref> for an overview): (1) An initial population of minimal neural network topologies is created, (2) each network is evaluated over multiple rollouts, with a different shared weight value assigned at each rollout, (3) networks are ranked according to their performance and complexity, and (4) a new population is created by varying the highest ranked network topologies, chosen probabilistically through tournament selection <ref type="bibr" target="#b80">[80]</ref>. The algorithm then repeats from (2), yielding weight agnostic topologies of gradually increasing complexity that perform better over successive generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.) Evaluate</head><p>Test with range of shared weight values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.) Initialize</head><p>Create population of minimal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.) Rank</head><p>Rank by performance and complexity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.) Vary</head><p>Create new population by varying best networks. Topology Search The operators used to search for neural network topologies are inspired by the wellestablished neuroevolution algorithm NEAT <ref type="bibr" target="#b110">[110]</ref>. While in NEAT the topology and weight values are optimized simultaneously, we ignore the weights and apply only topological search operators.</p><p>The initial population is composed of sparsely connected networks, networks with no hidden nodes and only a fraction of the possible connections between input and output. New networks are created by modifying existing networks using one of three operators: insert node, add connection, or change activation (Figure <ref type="figure" target="#fig_2">3</ref>). To insert a node, we split an existing connection into two connections that pass through this new hidden node. The activation function of this new node is randomly assigned. New connections are added between previously unconnected nodes, respecting the feed-forward property of the network. When activation functions of hidden nodes are changed, they are assigned at random. Activation functions include both the common (e.g. linear, sigmoid, ReLU) and more exotic (Gaussian, sinusoid, step), encoding a variety of relationships between inputs and outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Activations Minimal Network</head><p>Insert Node Add Connection Change Activation Performance and Complexity Network topologies are evaluated using several shared weight values. At each rollout a new weight value is assigned to all connections, and the network is tested on the task. In these experiments we used a fixed series of weight values ([-2, -1, -0.5, +0.5, +1, +2]) to decrease the variance between evaluations. <ref type="foot" target="#foot_1">3</ref> We calculate the mean performance of a network topology by averaging its cumulative reward over all rollouts using these different weight values.</p><p>Motivated by algorithmic information theory <ref type="bibr" target="#b108">[108]</ref>, we are not interested in searching merely for any weight agnostic neural networks, but networks that can be described with a minimal description length <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b98">98]</ref>. Given two different networks with similar performance we prefer the simpler network. By formulating the search as a multi-objective optimization problem <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b83">83]</ref> we take into account the size of the network as well as its performance when ranking it in the population.</p><p>We apply the connection cost technique from <ref type="bibr" target="#b15">[16]</ref> shown to produce networks that are more simple, modular, and evolvable. Networks topologies are judged based on three criteria: mean performance over all weight values, max performance of the single best weight value, and the number of connections in the network. Rather than attempting to balance these criteria with a hand-crafted reward function for each new task, we rank the solutions based on dominance relations <ref type="bibr" target="#b19">[20]</ref>.</p><p>Ranking networks in this way requires that any increase in complexity is accompanied by an increase in performance. While encouraging minimal and modular networks, this constraint can make larger structural changes -which may require several additions before paying off -difficult to achieve. To relax this constraint we rank by complexity only probabilistically: in 80% of cases networks are ranked according to mean performance and the number of connections, in the other 20% ranking is done by mean performance and max performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>Continuous Control Weight agnostic neural networks (WANNs) are evaluated on three continuous control tasks. The first, CartPoleSwingUp, is a classic control problem where, given a cart-pole system, a pole must be swung from a resting to upright position and then balanced, without the cart going beyond the bounds of the track. The swingup task is more challenging than the simpler CartPole <ref type="bibr" target="#b10">[11]</ref>, where the pole starts upright. Unlike the simpler task, it cannot be solved with a linear controller <ref type="bibr" target="#b93">[93,</ref><ref type="bibr" target="#b113">113]</ref>. The reward at every timestep is based on the distance of the cart from track edge and the angle of the pole. Our environment is closely based on the one described in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b129">129]</ref>.</p><p>The second task, BipedalWalker-v2 <ref type="bibr" target="#b10">[11]</ref>, is to guide a two-legged agent across randomly generated terrain. Rewards are awarded for distance traveled, with a cost for motor torque to encourage efficient movement. Each leg is controlled by a hip and knee joint in reaction to 24 inputs, including LIDAR sensors which detect the terrain and proprioceptive information such as the agent's joint speeds. Compared to the low dimensional CartPoleSwingUp, BipedalWalker-v2 has a non-trivial number of possible connections, requiring WANNs to be selective about the wiring of inputs to outputs.</p><p>The third, CarRacing-v0 <ref type="bibr" target="#b10">[11]</ref>, is a top-down car racing from pixels environment. A car, controlled with three continuous commands (gas, steer, brake) is tasked with visiting as many tiles as possible of a randomly generated track within a time limit. Following the approach described in <ref type="bibr" target="#b38">[39]</ref>, we delegate the pixel interpretation element of the task to a pre-trained variational autoencoder <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b96">96]</ref> (VAE) which compresses the pixel representation to 16 latent dimensions. These dimensions are given as input to the network. The use of learned features tests the ability of WANNs to learn abstract associations rather than encoding explicit geometric relationships between inputs.</p><p>Hand-designed networks found in the literature <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> are compared to the best weight agnostic networks found for each task. We compare the mean performance over 100 trials under 4 conditions:</p><p>1. Random weights: individual weights drawn from U(-2, 2); 2. Random shared weight: a single shared weight drawn from U(-2, 2); 3. Tuned shared weight: the highest performing shared weight value in range (-2, 2); 4. Tuned weights: individual weights tuned using population-based REINFORCE <ref type="bibr" target="#b123">[123]</ref>.</p><p>Table 1: Performance of Randomly Sampled and Trained Weights for Continuous Control Tasks</p><p>We compare the cumulative reward (average of 100 random trials) of the best weight agnostic network architectures found with standard feed forward network policies commonly used in previous work (i.e. <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>). The intrinsic bias of a network topology can be observed by measuring its performance using a shared weight sampled from a uniform distribution. By tuning this shared weight parameter we can measure its maximum performance. To facilitate comparison to baseline architectures we also conduct experiments where networks are allowed unique weight parameters and tuned.</p><p>Swing Up Random Weights Random Shared Weight Tuned Shared Weight Tuned Weights WANN 57 ± 121 515 ± 58 723 ± 16 932 ± 6 Fixed Topology 21 ± 43 7 ± 2 8 ± 1 918 ± 7 Biped Random Weights Random Shared Weight Tuned Shared Weight Tuned Weights WANN -46 ± 54 51 ± 108 261 ± 58 332 ± 1 Fixed Topology -129 ± 28 -107 ± 12 -35 ± 23 347 ± 1 [38] CarRacing Random Weights Random Shared Weight Tuned Shared Weight Tuned Weights WANN -69 ± 31 375 ± 177 608 ± 161 893 ± 74 Fixed Topology -82 ± 13 -85 ± 27 -37 ± 36 906 ± 21 [39]</p><p>The results are summarized in Table <ref type="table">1</ref>. <ref type="foot" target="#foot_2">4</ref> In contrast to the conventional fixed topology networks used as baselines, which only produce useful behaviors after extensive tuning, WANNs perform even with random shared weights. Though their architectures encode a strong bias toward solutions, WANNs are not completely independent of the weight values -they do fail when individual weight values are assigned randomly. WANNs function by encoding relationships between inputs and outputs, and so while the importance of the magnitude of the weights is not critical, their consistency, especially consistency of sign, is. An added benefit of a single shared weight is that it becomes trivial to tune this single parameter, without requiring the use of gradient-based methods.</p><p>The best performing shared weight value produces satisfactory if not optimal behaviors: a balanced pole after a few swings, effective if inefficient gaits, wild driving behaviour that cuts corners. These basic behaviors are encoded entirely within the architecture of the network. And while WANNs are able to perform without training, this predisposition does not prevent them from reaching similar state-of-the-art performance when the weights are trained. As the networks discovered are small enough to interpret, we can derive insights into how they function by looking at network diagrams (See Figure <ref type="figure">4</ref>). Examining the development of a WANN which solves CartPoleSwingUp is also illustrative of how relationships are encoded within an architecture. In the earliest generations the space of networks is explored in an essentially random fashion. By generation 32, preliminary structures arise which allow for consistent performance: the three inverters applied to the x position keep the cart from leaving the track. The center of the track is at 0, left is negative, right is positive. By applying positive force when the cart is in a negative position and vice versa a strong attractor towards the center of the track is encoded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s t e p a b s c o s s i g s i g s i n s t e p G a u s G a u s s t e p i n v i n v i n v i n v i n v i n v l i n i n v i n v i n v</head><p>The interaction between the regulation of position and the Gaussian activation on dθ is responsible for the swing-up behavior, also developed by generation 32. At the start of the trial the pole is stationary: the Gaussian activation of dθ is 1 and force is applied. As the pole moves toward the edge the nodes connected to the x input, which keep the cart in the center, begin sending an opposing force signal. The cart's progress toward the edge is slowed and the change in acceleration causes the pole to swing, increasing dθ and so decreasing the signal that is pushing the cart toward the edge. This slow down causes further acceleration of the pole, setting in motion a feedback loop that results in the rapid dissipation of signal from dθ. The resulting snap back of the cart towards the center causes the pole to swing up. As the pole falls and settles the same swing up behavior is repeated, and the controller is rewarded whenever the pole is upright.</p><p>As the search process continues, some of these controllers linger in the upright position longer than others, and by generation 128, the lingering duration is long enough for the pole to be kept balanced. Though this more complicated balancing mechanism is less reliable under variable weights than the swing-up and centering behaviors, the more reliable behaviors ensure that the system recovers and tries again until a balanced state is found. Notably, as these networks encode relationships and rely on tension between systems set against each other, their behavior is still consistent even with a wide range of shared weight values. For video demonstrations of the policies learned at various developmental phases of the weight agnostic topologies, please refer to the supplementary website.</p><p>WANN controllers for BipedalWalker-v2 and CarRacing-v0 (Figure <ref type="figure" target="#fig_0">1</ref>, page 1) are likewise remarkable in their simplicity and modularity. The biped controller uses only 17 of the 25 possible inputs, ignoring many LIDAR sensors and knee speeds. The WANN architecture not only solves the task without training the individual weights, but uses only 210 connections, an order of magnitude fewer than commonly used topologies (2804 connections used in the SOTA baseline <ref type="bibr" target="#b37">[38]</ref>).</p><p>The architecture which encodes stable driving behavior in the car racer is also striking in its simplicity (Figure <ref type="figure" target="#fig_0">1</ref>, right). Only a sparsely connected two layer network and a single weight value is required to encode competent driving behavior. While the SOTA baseline <ref type="bibr" target="#b38">[39]</ref> also gave the hidden states of a pre-trained RNN world model, in addition to the VAE's representation to its controller, our controller operates on the VAE's latent space alone. Nonetheless, it was able to develop a feed-forward controller that achieves a comparable score. Future work will explore removing the feed-forward constraint from the search to allow WANNs to develop recurrent connections with memory states.</p><p>The networks shows in Figure <ref type="figure" target="#fig_0">1</ref> (Page 1) were selected for both performance and readability. In many cases a great deal of complexity is added for only minimal gains in performance, in these cases we preferred to showcase more elegant networks. The final champion networks are shown in Figure <ref type="figure" target="#fig_4">5</ref>. Classification Promising results on reinforcement learning tasks lead us to consider how widely a WANN approach can be applied. WANNs which encode relationships between inputs are well suited to RL tasks: low-dimensional inputs coupled with internal states and environmental interaction allow discovery of reactive and adaptive controllers. Classification, however, is a far less fuzzy and forgiving problem. A problem where, unlike RL, design of architectures has long been a focus. As a proof of concept, we investigate how WANNs perform on the MNIST dataset <ref type="bibr" target="#b60">[60]</ref>, an image classification task which has been a focus of human-led architecture search for decades <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b100">100]</ref>.</p><p>Even in this high-dimensional classification task WANNs perform remarkably well (Figure <ref type="figure" target="#fig_6">6</ref> 5 Weight Value 10% 30% 50% 70% 90% WANN Random Weight Ensemble Weights Tuned Weight Trained Weights ANN Linear Regression Two-Layer CNN Test Accuracy 82.0% ± 18.7% 91.6% 91.9% 94.2% Test Accuracy 91.6% [62] 99.3% [15] It is straight forward to sweep over the range of weights to find the value which performs best on the training set, but the structure of WANNs offers another intriguing possibility. At each weight value the prediction of a WANN is different. On MNIST this can be seen in the varied accuracy on each digit (Figure <ref type="figure" target="#fig_6">6</ref>, Right). Each weight value of the network can be thought of as a distinct classifier, creating the possibility of using one WANN with multiple weight values as a self-contained ensemble.</p><p>In the simplest ensemble approach, a collection of networks are created by instantiating a WANN with a range of weight values. Each of these networks is given a single vote, and the ensemble classifies samples according to the category which received the most votes. This approach yields predictions far more accurate than randomly selected weight values, and only slightly worse than the best possible weight. That the result of this naive ensemble is successful is encouraging for experimenting with more sophisticated ensemble techniques when making predictions or searching for architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>In this work we introduced a method to search for simple neural network architectures with strong inductive biases. Since networks are optimized to perform well using a shared weight over a range of values, this single parameter can easily be tuned to increase performance. Individual weights can be further tuned from a best shared weight. The ability to quickly fine-tune weights is useful in few-shot learning <ref type="bibr" target="#b21">[22]</ref> and may find uses in continual learning <ref type="bibr" target="#b89">[89]</ref> where agents continually acquire, fine-tune, and transfer skills throughout their lifespan, as in animals <ref type="bibr" target="#b125">[125]</ref>. Inspired by the Baldwin effect <ref type="bibr" target="#b2">[3]</ref>, weight tolerant networks have long linked theories of evolution and learning in AI <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b106">106]</ref>.</p><p>To develop a single WANN capable of encoding many different useful tasks in its environment, one might consider developing a WANN with a strong intrinsic bias for intrinsic motivation <ref type="bibr" target="#b88">[88,</ref><ref type="bibr" target="#b90">90,</ref><ref type="bibr" target="#b101">101]</ref>, and continuously optimize its architecture to perform well at pursuing novelty in an open-ended environment <ref type="bibr" target="#b66">[66]</ref>. Such a WANN might encode, through a curiosity reward signal, a multitude of skills that can easily be fine-tuned for a particular downstream task in its environment later on.</p><p>While our approach learns network architectures of increasing complexity by adding connections, network pruning approaches find new architectures by their removal. It is also possible to learn a pruned network capable of performing additional tasks without learning weights <ref type="bibr" target="#b75">[75]</ref>. A concurrent work <ref type="bibr" target="#b127">[127]</ref> to ours learns a supermask where the sub-network pruned using this mask performs well at image recognition even with randomly initialized weights -it is interesting that their approach achieves a similar range of performance on MNIST compared to ours. While our search method is based on evolution, future work may extend the approach by incorporating recent ideas that formulate architecture search in a differentiable manner <ref type="bibr" target="#b71">[71]</ref> to make the search more efficient.</p><p>The success of deep learning is attributed to our ability to train the weights of large neural networks that consist of well-designed building blocks on large datasets, using gradient descent. While much progress has been made, there are also limitations, as we are confined to the space of architectures that gradient descent is able to train. For instance, effectively training models that rely on discrete components <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b52">52]</ref> or utilize adaptive computation mechanisms <ref type="bibr" target="#b30">[31]</ref> with gradient-based methods remain a challenging research area. We hope this work will encourage further research that facilitates the discovery of new architectures that not only possess inductive biases for practical domains, but can also be trained with algorithms that may not require gradient computation.</p><p>That the networks found in this work do not match the performance of convolutional neural networks is not surprising. It would be an almost embarrassing achievement if they did. For decades CNN architectures have been refined by human scientists and engineers -but it was not the reshuffling of existing structures which originally unlocked the capabilities of CNNs. Convolutional layers were themselves once novel building blocks, building blocks with strong biases toward vision tasks, whose discovery and application have been instrumental in the incredible progress made in deep learning. The computational resources available to the research community have grown significantly since the time convolutional neural networks were discovered. If we are devoting such resources to automated discovery and hope to achieve more than incremental improvements in network architectures, we believe it is also worth trying to discover new building blocks, not just their arrangements.</p><p>Finally, we see similar ideas circulating in the neuroscience community. A recent neuroscience commentary, "What artificial neural networks can learn from animal brains" <ref type="bibr" target="#b125">[125]</ref> provides a critique of how learning (and also meta-learning) is currently implemented in artificial neural networks. Zador <ref type="bibr" target="#b125">[125]</ref> highlights the stark contrast with how biological learning happens in animals:</p><p>"The first lesson from neuroscience is that much of animal behavior is innate, and does not arise from learning. Animal brains are not the blank slates, equipped with a general purpose learning algorithm ready to learn anything, as envisioned by some AI researchers; there is strong selection pressure for animals to restrict their learning to just what is needed for their survival." <ref type="bibr" target="#b125">[125]</ref> This paper is strongly motivated towards these goals of blending innate behavior and learning, and we believe it is a step towards addressing the challenge posed by Zador. We hope this work will help bring neuroscience and machine learning communities closer together to tackle these challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 MNIST</head><p>The MNIST version used in this paper is a downsampled version, reducing the digits from [28x28] to [16x16], and deskewed using the OpenCV library <ref type="bibr" target="#b7">[8]</ref>. The best MNIST network weight was chosen as the network with the highest accuracy on the training set.</p><p>To fit into our existing approach MNIST classification is reframed as a reinforcement learning problem. Each sample in MNIST is downsampled to a 16x16 image, deskewed, and pixel intensity normalized between 0 and 1. WANNs are created with input for each of the 256 pixels and one output for each of the 10 digits. At each evaluation networks are fed 1000 samples randomly selected from the training set, and given reward based on the softmax cross entropy. Networks are tested with a variety of shared weight values, maximizing performance over all weights while minimizing the number of connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Hyperparameters and Setup</head><p>All experiments but those on Car Racing were performed used 96 core machines on the Google Cloud Platform. As evaluation of the population is embarrassingly parallel, populations were sized as multiples of 96 to make efficient use of all processors. Car Racing was performed on a 64 core machine and the population size used reflects this. The code and setup of the VAE for the Car Racing task is taken from <ref type="bibr" target="#b38">[39]</ref>, were a VAE with a latent size of 16 was trained following the same procedure as <ref type="bibr" target="#b38">[39]</ref>. Tournament sizes were scaled in line with the population size. The number of generations were determined after initial experiments to ensure that a majority of runs would converge.</p><p>SwingUp Biped CarRace MNIST Population Size 192 480 64 960 Generations 1024 2048 1024 4096 Change Activation Probability (%) 50 50 50 50 Add Node Probability (%) 25 25 25 25 Add Connection Probability (%) 25 25 25 25 Initial Active Connections (%) 50 25 50 5 Tournament Size 8 16 8 32 A.5 Results over multiple independent search runs</p><p>For each task a WANN search was run 9 times. At regular intervals the network in the population with the best mean performance was compared to that with the previously best found network. If the newer network had a higher mean, the network was evaluated 96 or 64 times (depending on the number of processors on the machine), and if the mean of those evaluations was better than the previous best network, it was kept as the new 'best' network. These best networks were kept only for record keeping and did not otherwise interact with the population.</p><p>These best networks at the end of each run were reevaluated thirty times on each weight in the series [-2, -1.5, -1, -0.5, 0.5, 1, 1.5, 2] and the network with the best mean chosen as the champion for more intensive analysis and examination. Shown below are the results of these initial tests, both as individual runs and as distributions of performance over weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Optimizing for individual weight parameters</head><p>In our experiments, we also fine-tuned individual weight parameters for the champion networks found to measure the performance impact of further training. For this, we used population-based REINFORCE, as in Section 6 of <ref type="bibr" target="#b123">[123]</ref>. Our specific approach is based on the open source estool <ref type="bibr" target="#b36">[37]</ref> implementation of population-based REINFORCE. We use a population size of 384, and each agent performs the task 16 times with different initial random seeds for Swing Up Cartpole and Bipedal Walker. The agent's reward signal used by the policy gradient method is the average reward of the 16 rollouts. For Car Racing, due to the extra computation time required, we instead use a population size of 64 and the average cumulative reward of 4 rollouts to calculate the reward signal. All models trained for 3000 generations. All other parameters are set to the default settings of estool <ref type="bibr" target="#b36">[37]</ref>.</p><p>For MNIST, we use the negative of the cross entropy loss as the reward signal, and optimize directly on the training set with population-based REINFORCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Fixed Topology Baselines</head><p>For Bipedal Walker, we used the model and architecture available from estool <ref type="bibr" target="#b36">[37]</ref> as our baseline.</p><p>To our knowledge, this baseline currently, at the time of writing, achieves the state-of-the-art average cumulative score (over 100 random rollouts) on Bipedal Walker as reported in <ref type="bibr" target="#b37">[38]</ref>.</p><p>In the Swing Up Cartpole task, we trained a baseline controller with 1 hidden layer of 10 units (71 weight parameters), using the same training methodology as the one used to produce SOTA results for the Bipedal Walker task mentioned earlier. We experimented with a larger number of nodes in the hidden layer, and an extra hidden layer, but did not see meaningful differences in performance.</p><p>For the Car Racing baseline, we used the code and model provided in <ref type="bibr" target="#b38">[39]</ref> and treated the 867 parameters of the controller as free weight parameters, while keeping the parameters of the pretrained VAE and RNN fixed. As of writing, the average cumulative score (over 100 random rollouts) produced by <ref type="bibr" target="#b38">[39]</ref> for Car Racing is currently the state-of-the-art. As mentioned in the main text, for simplicity, the WANN controller has access only to the pre-trained VAE, and not to the RNN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of Weight Agnostic Neural Networks: Bipedal Walker (left), Car Racing (right) We search for architectures by deemphasizing weights. In place of training, networks are assigned a single shared weight value at each rollout. Architectures that are optimized for expected performance over a wide range of weight values are still able to perform various tasks without weight training.</figDesc><graphic coords="1,412.71,553.34,91.08,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Weight Agnostic Neural Network Search Weight Agnostic Neural Network Search avoids weight training while exploring the space of neural network topologies by sampling a single shared weight at each rollout. Networks are evaluated over several rollouts. At each rollout a value for the single shared weight is assigned and the cumulative reward over the trial is recorded. The population of networks is then ranked according to their performance and complexity. The highest ranking networks are then chosen probabilistically and varied randomly to form a new population, and the process repeats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Operators for Searching the Space of Network Topologies Left: A minimal network topology, with input and outputs only partially connected. Middle: Networks are altered in one of three ways. Insert Node: a new node is inserted by splitting an existing connection. Add Connection: a new connection is added by connecting two previously unconnected nodes. Change Activation: the activation function of a hidden node is reassigned. Right: Possible activation functions (linear, step, sin, cosine, Gaussian, tanh, sigmoid, absolute value, invert (i.e. negative linear), ReLU) shown over the range [2, 2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : 8 :</head><label>48</label><figDesc>Figure 4: Development of Weight Agnostic Neural Network Topologies Over Time Generation 8: An early network which performs poorly with nearly all weights. Generation 32: Relationships between the position of the cart and velocity of the pole are established. The tension between these relationships produces both centering and swing-up behavior. Generation 128: Complexity is added to refine the balancing behavior of the elevated pole.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>ForceFigure 5 :</head><label>5</label><figDesc>Figure 5: Champion Networks for Continuous Control Tasks Left to Right (Number of Connections): Swing up (52), Biped (210), Car Racing (245) Shown in Figure 1 (Page 1) are high performing, but simpler networks, chosen for clarity. The three network architectures in this figure describe the champion networks whose results are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>, Left).Restricted to a single weight value, WANNs are able to classify MNIST digits as well as a single layer neural network with thousands of weights trained by gradient descent. The architectures created still maintain the flexibility to allow weight training, allowing further improvements in accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Classification Accuracy on MNIST. Left: WANNs instantiated with multiple weight values acting as an ensemble perform far better than when weights are sampled at random, and as well as a linear classifier with thousands of weights. Right: No single weight value has better accuracy on all digits. That WANNs can be instantiated as several different networks has intriguing possibilities for the creation of ensembles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: MNIST classifier network (1849 connections) Not all neurons and connections are used to predict each digit. Starting from the output connection for a particular digit, we can trace the sub-network and also identify which part of the input image is used for classifying each digit. Please refer to the supplementary website for more detailed visualizations.</figDesc><graphic coords="8,249.10,537.87,199.15,128.49" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We released a software toolkit not only to facilitate reproduction, but also to further research in this direction. Refer to the Supplementary Materials for more information about the code repository.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Variations on these particular values had little effect, though weight values in the range [-2, 2] showed the most variance in performance. Networks whose weight values were set to greater than 3 tended to perform similarly -presumably saturating many of the activation functions. Weight values near 0 were also omitted to reduce computation, as regardless of the topology little to no information is passed to the output.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We conduct several independent search runs to measure variability of results in Supplementary Materials.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank our three reviewers for their helpful comments, and also express gratitude to <rs type="person">Douglas Eck</rs>, <rs type="person">Geoffrey Hinton</rs>, <rs type="person">Anja Austermann</rs>, <rs type="person">Jeff Dean</rs>, <rs type="person">Luke Metz</rs>, <rs type="person">Ben Poole</rs>, <rs type="person">Jean-Baptiste Mouret</rs>, <rs type="person">Michiel Adriaan Unico Bacchiani</rs>, <rs type="person">Heiga Zen</rs>, and <rs type="person">Alex Lamb</rs> for their thoughtful feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Materials for Weight Agnostic Neural Networks A.1 Code Release</head><p>We release a general purpose tool, not only to facilitate reproduction, but also for further research in this direction. Our NumPy <ref type="bibr" target="#b119">[119]</ref> implementation of NEAT <ref type="bibr" target="#b110">[110]</ref> supports MPI <ref type="bibr" target="#b32">[33]</ref> and OpenAI Gym <ref type="bibr" target="#b10">[11]</ref> environments. All code used to run these experiments, in addition to the best networks found in each run, is referenced in the interactive article: <ref type="url" target="https://weightagnostic.github.io/">https://weightagnostic.github.io/</ref> A.2 "Have you also thought about trying ... ?"</p><p>In this section, we highlight things that we have attempted, but did not explore in sufficient depth.</p><p>A.2.1 Searching for network architecture using a single weight rather than range of weights.</p><p>We experimented with setting all weights to a single fixed value, e.g. 0.7, and saw that the search is faster and the end result better. However, if we then nudge that value by a small amount, to say 0.6, the network fails completely at the task. By training on a wide range of weight parameters, akin to training on uniform samples weight values, networks were able to perform outside of the training values. In fact, the best performing values were outside of this training set.</p><p>A.2.2 Searching for network architecture using random weights for each connection.</p><p>This was the first thing we tried, and did not have much luck. We tried quite a few things to get this to work-at one point it seemed like we finally had it, poles were balanced and walkers walking, but it turned out to be a bug in the code! Instead of setting all of the weights to different random values we had set all of the weights to the same random value. It was in the course of trying to understand this result that we began to view and approach the problem through the lens of MDL and AIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Adding noise to the single weight values.</head><p>We experimented adding Gaussian noise to the weight values so that each weight would be different, but vary around a set mean at each rollout. We only did limited experiments on swing-up and found no large difference, except with very high levels of noise where it performed poorly. Our intuition is that adding noise would make the final topologies even more robust to changes in weight value, but at the cost of making the evaluation of topologies more noisy (or more rollouts to mitigate the variance between trials). With no clear benefit we chose to keep the approach as conceptually simple as possible-but see this as a logical next step towards making the networks more weight tolerant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Using backpropagation to fine-tune weights of a WANN.</head><p>We explored the use of autograd packages such as JAX <ref type="bibr" target="#b23">[24]</ref> to fine-tune individual weights of WANNs for the MNIST experiment. Performance improved, but ultimately we find that black-box optimization methods such as CMA-ES and population-based REINFORCE can find better solutions for the WANN architectures evolved for MNIST, suggesting that the various activations proposed by the WANN search algorithm may have produced optimization landscapes that are more difficult for gradient-based methods to traverse compared to standard ReLU-based deep network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.5 Why did you choose to use many different activation functions in the same network?</head><p>Why not just ReLU? Wouldn't too many activations break biological plausibility?</p><p>Without concrete weight values to lean on, we instead relied on encoding relationships between inputs into the network. This could have been done with ReLUs or sigmoids, but including relationships such as symmetry and repetition allow for more compact networks.</p><p>We didn't do much experimentation, but our intuition is that the variety of activations is key. That is not to say that all of them are necessary, but we're not confident this could have been accomplished with only linear activations. As for biological corollaries, we're not going to claim that a cosine activation is an accurate model of a how neurons fire-but don't think a feed forward network of instantaneously communicating sigmoidal neurons would be any more biologically plausible.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactions between learning and evolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<ptr target="http://www2.hawaii.edu/~nreed/ics606/papers/Ackley91learningEvolution.pdf" />
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="487" to="509" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An evolutionary algorithm that constructs recurrent neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Angeline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
		<ptr target="http://tiny.cc/3hgv7y" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new factor in evolution</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The american naturalist</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">354</biblScope>
			<biblScope unit="page" from="441" to="451" />
			<date type="published" when="1896">1896</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensemble learning in bayesian neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<ptr target="http://tiny.cc/8jgv7y" />
	</analytic>
	<monogr>
		<title level="m">NATO ASI series. Series F: computer and system sciences</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="215" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning. springer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<ptr target="https://aka.ms/prml" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning causes synaptogenesis, whereas motor activity causes angiogenesis, in cerebellar cortex of adult rats</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alcantara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Greenough</surname></persName>
		</author>
		<ptr target="https://www.pnas.org/content/pnas/87/14/5568.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="5568" to="5572" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The description length of deep learning models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Blier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.07044" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2216" to="2226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning OpenCV: Computer vision with the OpenCV library</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<ptr target="https://opencv.org/" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evolving feedforward neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weisbrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ANNGA93, International Conference on Artificial Neural Networks and Genetic Algorithms</title>
		<meeting>ANNGA93, International Conference on Artificial Neural Networks and Genetic Algorithms<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smash: One-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rydeCEhs-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<ptr target="https://arxiv.org/abs/1606.01540" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural connections: Some you use, some you lose</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Bruer</surname></persName>
		</author>
		<ptr target="http://www.oecd.org/education/ceri/31709587.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Phi Delta Kappan</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="264" to="277" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Complex brain networks: graph theoretical analysis of structural and functional systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<ptr target="http://tiny.cc/xyiv7y" />
	</analytic>
	<monogr>
		<title level="j">Nature reviews neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Antipredator behaviour of hatchling snakes: effects of incubation temperature and simulated predators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Behaviour</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="553" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The evolutionary origins of modularity</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="DOI">10.1098/rspb.2012.2863</idno>
		<ptr target="https://royalsocietypublishing.org/doi/pdf/10.1098/rspb.2012.2863" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society b: Biological sciences</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<date type="published" when="1755">1755. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive bias of deep convolutional networks through pooling geometry</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BkVsEMYel" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Designing application-specific neural networks using the structured genetic algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Mcgregor</surname></persName>
		</author>
		<ptr target="http://tiny.cc/2ggv7y" />
	</analytic>
	<monogr>
		<title level="m">[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neuroplasticity subserving motor skill learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://tiny.cc/1ziv7y" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="443" to="454" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
		<ptr target="http://www.dmi.unict.it/mpavone/nc-cs/materiale/NSGA-II.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The complete connectome of a learning and memory centre in an insect brain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Litwin-Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Schneider-Mizell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saumweber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eschbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gerber</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5806122/" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">548</biblScope>
			<biblScope unit="issue">7666</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.03400" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJl-b3RcF7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Compiling machine learning programs via high-level tracing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leary</surname></persName>
		</author>
		<ptr target="https://github.com/google/jax" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyake</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/afs/cs/user/bhiksha/WWW/courses/deeplearning/Fall.2016/pdfs/Fukushima_Miyake.pdf" />
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="455" to="469" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using marker-based genetic encoding of neural networks to evolve finite-state behaviour</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fullmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<ptr target="http://tiny.cc/lggv7y" />
	</analytic>
	<monogr>
		<title level="m">Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<ptr target="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving pilco with bayesian neural network dynamics models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<ptr target="http://mlg.eng.cam.ac.uk/yarin/website/PDFs/DeepPILCO.pdf" />
	</analytic>
	<monogr>
		<title level="m">Data-Efficient Machine Learning workshop, ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Innate predator-recognition in australian brush-turkey (alectura lathami, megapodiidae) hatchlings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behaviour</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">117</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<ptr target="https://arxiv.org/abs/1603.08983" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<ptr target="https://arxiv.org/abs/1410.5401" />
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Using MPI: portable parallel programming with the message-passing interface</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skjellum</surname></persName>
		</author>
		<ptr target="https://www.mpi-forum.org/" />
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparison between cellular encoding and direct encoding for genetic neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gruau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pyeatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st annual conference on genetic programming</title>
		<meeting>the 1st annual conference on genetic programming</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="81" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The minimum description length principle</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Grünwald</surname></persName>
		</author>
		<ptr target="https://mitpress.mit.edu/books/minimum-description-length-principle" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic network surgery for efficient dnns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1379" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<ptr target="http://blog.otoro.net/2017/11/12/evolving-stable-strategies/" />
		<title level="m">Evolving stable strategies</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03779</idno>
		<ptr target="https://designrl.github.io" />
		<title level="m">Reinforcement learning for improving agent design</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="https://worldmodels.github.io" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2451" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meiosis networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/227-meiosis-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="533" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A stochastic version of the delta rule</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing application-specific neural networks using the genetic algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Samad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guha</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">-designing-application-specific-neural-networks-using-the-genetic-algorithm</title>
		<imprint/>
	</monogr>
	<note>pdf</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A powerful generative model using random weights for the deep image representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.04801" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="631" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph theoretical modeling of brain connectivity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Evans</surname></persName>
		</author>
		<ptr target="http://tiny.cc/82iv7y" />
	</analytic>
	<monogr>
		<title level="j">Current opinion in neurology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="350" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Keeping neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~hinton/absps/colt93.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Ann. ACM Conf. on Computational Learning Theory</title>
		<meeting>of the 6th Ann. ACM Conf. on Computational Learning Theory</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How learning can guide evolution</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<ptr target="http://www.cogsci.ucsd.edu/~rik/courses/cogs184_w10/readings/HintonNowlan97.pdf" />
	</analytic>
	<monogr>
		<title level="m">Adaptive individuals in evolving populations: models and algorithms</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://people.idsia.ch/~juergen/rnn.html" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Morphometric study of human cerebral cortex development</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="517" to="527" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haas</surname></persName>
		</author>
		<ptr target="http://tiny.cc/t3wd8y" />
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">5667</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>id= rkE3y85ee</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/jozefowicz15.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=33X9fd2-9FyZd" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Motor learning-dependent synaptogenesis is localized to functionally reorganized motor cortex</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kleim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Reidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Remple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Nudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurobiology of learning and memory</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Three approaches to the quantitative definition of information</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Kolmogorov</surname></persName>
		</author>
		<ptr target="http://tiny.cc/xmgv7y" />
	</analytic>
	<monogr>
		<title level="j">Problems of information transmission</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-objective optimization using genetic algorithms: A tutorial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Konak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Coit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://tiny.cc/bglv7y" />
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering and System Safety</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="992" to="1007" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Delta-gann: A new approach to training neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Ciesielski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of Queensland. Citeseer</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04759</idno>
		<ptr target="https://arxiv.org/abs/1710.04759" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Bayesian hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf" />
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
	<note>The handbook of brain theory and neural networks</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Evolutionary ordered neural network with a linked-list encoding scheme</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Evolutionary Computation</title>
		<meeting>IEEE International Conference on Evolutionary Computation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="665" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Snip: Single-shot network pruning based on connection sensitivity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1VZqjAcYX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Exploiting open-endedness to solve problems through the search for novelty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<ptr target="https://www.cs.ucf.edu/eplex/noveltysearch/userspage/" />
	</analytic>
	<monogr>
		<title level="m">ALIFE</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Measuring the intrinsic dimension of objective landscapes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farkhoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ryup8-WCW" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJqFGTslg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<ptr target="https://arxiv.org/abs/1902.07638" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJQRKzbA-" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1eYHoC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJlnB3C5Ym" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.06342" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<ptr target="https://authors.library.caltech.edu/13792/1/MACnc92a.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1801.06519" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Representation and evolution of neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mandischer</surname></persName>
		</author>
		<ptr target="http://tiny.cc/ofgv7y" />
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Nets and Genetic Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="643" to="649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Genetic evolution of the topology and weight distribution of neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Maniezzo</surname></persName>
		</author>
		<ptr target="http://tiny.cc/mhgv7y" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="53" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Evolving deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shahrzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.00548" />
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence in the Age of Neural Networks and Brain Computing</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="293" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Morphological correlates of locomotor performance in hatchling amblyrhynchus cristatus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Snell</surname></persName>
		</author>
		<ptr target="http://tiny.cc/t5iv7y" />
	</analytic>
	<monogr>
		<title level="j">Oecologia</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="264" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Genetic algorithms, tournament selection, and the effects of noise</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://tiny.cc/4ckv7y" />
	</analytic>
	<monogr>
		<title level="j">Complex systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="212" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJGCiw5gl" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Does prey matter? geographic variation in antipredator responses of hatchlings of a japanese natricine snake (rhabdophis tigrinus)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Burghardt</surname></persName>
		</author>
		<ptr target="http://tiny.cc/r8iv7y" />
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Psychology</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">408</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Novelty-based multiobjectivization</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<ptr target="http://tiny.cc/uelv7y" />
	</analytic>
	<monogr>
		<title level="m">New horizons in evolutionary robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<ptr target="http://tiny.cc/vkgv7y" />
	</analytic>
	<monogr>
		<title level="j">Springer Science and Business Media</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Variance networks: When expectation does not meet your expectations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Neklyudov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashukha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1GAUs0cKQ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~hinton/absps/sunspots.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Connectionist theory refinement: Genetically searching the space of network topologies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
		<ptr target="http://tiny.cc/oigv7y" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="177" to="209" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Intrinsic motivation systems for autonomous mental development</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Hafner</surname></persName>
		</author>
		<ptr target="http://www.pyoudeyer.com/ims.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Part</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07569</idno>
		<ptr target="https://arxiv.org/abs/1802.07569" />
		<title level="m">Continual lifelong learning with neural networks: A review</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Curiosity-driven exploration by self-supervised prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://pathak22.github.io/noreward-rl/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="16" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.03268" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Evolving the topology and the weights of neural networks using a dual representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C F</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poli</surname></persName>
		</author>
		<ptr target="http://tiny.cc/uigv7y" />
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="84" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Variational bayesian learning of nonlinear hidden state-space models for model predictive control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tornio</surname></persName>
		</author>
		<ptr target="https://users.ics.aalto.fi/praiko/papers/raikotornio2009.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.01548" />
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Largescale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.01041" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1401.4082" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<ptr target="http://tiny.cc/dngv7y" />
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Information and complexity in statistical modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science and Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Roli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Melandri</surname></persName>
		</author>
		<ptr target="https://amslaurea.unibo.it/8268/1/melandri_luca_tesi.pdf" />
		<title level="m">Introduction to reservoir computing methods</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1710.09829" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Curious model-building control systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="http://people.idsia.ch/~juergen/interest.html" />
	</analytic>
	<monogr>
		<title level="m">[Proceedings] 1991 IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1458" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Discovering neural nets with low kolmogorov complexity and high generalization capability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<ptr target="ftp://ftp.idsia.ch/pub/juergen/loconet.pdf" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="857" to="873" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Training recurrent networks by evolino</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gagliolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<ptr target="http://people.idsia.ch/~juergen/evolino.html" />
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="757" to="779" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<ptr target="https://arxiv.org/abs/1902.08142" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Connectome: How the brain&apos;s wiring makes us who we are</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Connectome_(book)" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>HMH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">When learning guides evolution</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/329761a0.pdf" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="issue">6142</biblScope>
			<biblScope unit="page">761</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1901.11117" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A formal theory of inductive inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Solomonoff</surname></persName>
		</author>
		<ptr target="http://tiny.cc/7cgv7y" />
	</analytic>
	<monogr>
		<title level="j">part i. Information and control</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The human connectome: a structural description of the human brain</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kötter</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0010042</idno>
		<ptr target="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.0010042" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<ptr target="http://www.cs.ucf.edu/~kstanley/neat.html" />
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Patterns of development: the altricial-precocial spectrum</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Ricklefs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Oxford Ornithology Series</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">A connectome of a learning and memory center in the adult drosophila brain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>S.-Y. Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Parag</surname></persName>
		</author>
		<ptr target="https://elifesciences.org/articles/26975" />
		<imprint>
			<date type="published" when="2017">26975. 2017</date>
			<publisher>Elife</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Underactuated robotics: Learning, planning, and control for efficient and agile machines: Course notes for mit 6</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<ptr target="http://tiny.cc/v8lv7y" />
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">832</biblScope>
		</imprint>
	</monogr>
	<note>Working draft edition</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Brain development and the role of experience in the early years</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Tierney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3722610/" />
	</analytic>
	<monogr>
		<title level="j">Zero to three</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Neural arithmetic logic units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1808.00508" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8035" to="8044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
		<ptr target="https://weightagnostic.github.io/papers/turing1948.pdf" />
		<title level="m">Intelligent machinery. NPL. Mathematics Division</title>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<ptr target="https://dmitryulyanov.github.io/deep_image_prior" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9446" to="9454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Rich-club organization of the human connectome</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Heuvel</surname></persName>
		</author>
		<author>
			<persName><surname>Sporns</surname></persName>
		</author>
		<ptr target="http://www.jneurosci.org/content/jneuro/31/44/15775.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="15775" to="15786" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">The numpy array: a structure for efficient numerical computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<ptr target="https://www.numpy.org/" />
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Structural properties of the caenorhabditis elegans neuronal network</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Paniagua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
		<idno type="DOI">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066</idno>
		<ptr target="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066" />
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">1001066. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.03762" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">The structure of the nervous system of the nematode caenorhabditis elegans</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Southgate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brenner</surname></persName>
		</author>
		<ptr target="http://tiny.cc/kiiv7y" />
	</analytic>
	<monogr>
		<title level="j">Philos Trans R Soc Lond B Biol Sci</title>
		<imprint>
			<biblScope unit="volume">314</biblScope>
			<biblScope unit="page" from="1" to="340" />
			<date type="published" when="1165">1165. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf" />
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Towards designing artificial neural networks by evolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://tiny.cc/gjgv7y" />
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A critique of pure learning and what artificial neural networks can learn from animal brains</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Zador</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/s41467-019-11786-6" />
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Evolving optimal neural networks using genetic algorithms with occam&apos;s razor. Complex systems</title>
		<author>
			<persName><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muhlenbein</surname></persName>
		</author>
		<ptr target="http://muehlenbein.org/gpevolv93.pdf" />
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="199" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01067</idno>
		<ptr target="https://arxiv.org/abs/1905.01067" />
		<title level="m">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ue8Hcxg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Pytorch implementation of improving pilco with bayesian neural network dynamics models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<ptr target="https://github.com/zuoxingdong/DeepPILCO" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
