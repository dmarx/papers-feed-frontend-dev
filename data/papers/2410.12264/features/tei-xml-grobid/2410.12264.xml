<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Game Theory Meets Statistical Mechanics in Deep Learning Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-16">16 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Djamel</forename><surname>Bouchaffra</surname></persName>
							<email>djamel.bouchaffra@gmail.com</email>
						</author>
						<author>
							<persName><surname>Fayc ¸al Ykhlef</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bilal</forename><surname>Faye</surname></persName>
							<email>faye@lipn.univ-paris13.fr</email>
						</author>
						<author>
							<persName><forename type="first">Hanane</forename><surname>Azzag</surname></persName>
							<email>azzag@univ-paris13.fr</email>
						</author>
						<author>
							<persName><forename type="first">Mustapha</forename><surname>Lebbah</surname></persName>
							<email>mustapha.lebbah@uvsq.fr</email>
						</author>
						<title level="a" type="main">Game Theory Meets Statistical Mechanics in Deep Learning Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-16">16 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">AA0AD0E8154814D9572CBBD198BCB44C</idno>
					<idno type="arXiv">arXiv:2410.12264v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-26T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel deep graphical representation that seamlessly merges principles of game theory with laws of statistical mechanics. It performs feature extraction, dimensionality reduction, and pattern classification within a single learning framework. Our approach draws an analogy between neurons in a network and players in a game theory model. Furthermore, each neuron viewed as a classical particle (subject to statistical physics' laws) is mapped to a set of actions representing specific activation value, and neural network layers are conceptualized as games in a sequential cooperative game theory setting. The feed-forward process in deep learning is interpreted as a sequential game, where each game comprises a set of players. During training, neurons are iteratively evaluated and filtered based on their contributions to a payoff function, which is quantified using the Shapley value driven by an energy function. Each set of neurons that significantly contributes to the payoff function forms a strong coalition. These neurons are the only ones permitted to propagate the information forward to the next layers. We applied this methodology to the task of facial age estimation and gender classification. Experimental results demonstrate that our approach outperforms both multi-layer perceptron and convolutional neural network models in terms of efficiency and accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Deep learning (DL) based on graphical representations has proven effective, especially when domain-specific knowledge for feature extraction is limited <ref type="bibr" target="#b0">[1]</ref>. For instance, DL models have demonstrated high performance in complex tasks such as medical image classification <ref type="bibr" target="#b1">[2]</ref> [3], natural language processing, and speech recognition <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref>  <ref type="bibr" target="#b5">[6]</ref>. However, these deep learning models often function as black boxes, delivering impressive results in data classification without providing insights into understanding the model's internal workings as well as unraveling the causal mechanisms underlying their predictions <ref type="bibr">[7] [8]</ref>. This lack of interpretability and explainability, essentially the ability to comprehend and trace cause and effect within a system, limits their applicability in domains they were not specifically trained for. In terms of their structural design, DL models exhibit several critical limitations: (i) they process information sequentially from one layer to the next without formally evaluating the individual contribution of each neuron, (ii) they have difficulty determining activation levels associated with groups of neurons within a layer, (iii) they struggle to identify the most informative neurons within a layer, often relying on random dropout techniques to mitigate noise and reduce overfitting, and (iv) most of the DL models lack probabilistic measures to express information uncertainty.</p><p>To address these limitations, several approaches exploiting game theory (GT) were proposed in the deep machine learning literature <ref type="bibr" target="#b8">[9]</ref> [10] <ref type="bibr" target="#b10">[11]</ref> [12] <ref type="bibr" target="#b12">[13]</ref>. Likewise, <ref type="bibr" target="#b13">[14]</ref> relied on game theory to improve prediction in ensemble learning. They defined the pruned ensemble as the minimal winning coalition made of the members that together exhibit moderate diversity. Moreover, <ref type="bibr" target="#b14">[15]</ref> explored unlearnable example attacks using a game-theoretic approach, where the attack is modeled as a nonzero-sum Stackelberg game. <ref type="bibr" target="#b15">[16]</ref> introduced an efficient approach that leverages a combination of deep learning techniques and game theory to enhance the performance and scalability of solving extensive-form games. These games, characterized by their complex decision-making processes with latent information, pose significant challenges in strategic planning. The research by <ref type="bibr" target="#b16">[17]</ref> addresses the challenge of robots finding optimal paths while avoiding collisions with humans and other robots. Traditional Deep Reinforcement Learning (DRL) struggles with slow convergence in such complex scenarios. To improve performance, the study introduces a hybrid approach that integrates DRL with game theory. Furthermore, <ref type="bibr" target="#b10">[11]</ref> have demonstrated that a deep neural network can be modeled as a non-atomic congestion game, irrespective of whether it is fully connected or only locally connected. Additionally, they have proved that optimizing the weight and bias vectors for a given training set is equivalent to finding the optimal solution for the associated non-atomic congestion game. Other applications of game theory to deep neural networks can be found in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>.</p><p>A different front emanates from the field of statistical mechanics (SM) has been investigated in order to gain insight into the understanding and optimization of deep learning models <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b21">[22]</ref> applied mean-field theories to analyze the information propagation in neural networks, which helps identify the 'edge of chaos' and dynamic isometry conditions for optimal learning and generalization. These theories provide a framework for initializing neural networks in a way that maximizes mutual information, enhancing their performance from the start. In the context of continual learning, statistical mechanics offers insights through the development of variational principles and mean-field potentials.</p><p>Our main contribution in this manuscript is fourfold:</p><p>• Fusion of GT and SM: A seamless combination of game theory and statistical mechanics in deep learning design is applied. In this setting that we propose under the name of 'NEUROGAME', the collaboration between neurons within layers in a neural network is grounded in game theory driven by statistical mechanics laws.</p><p>• Probabilistic Signal Transmission: The flow of information, with a Gibbs distribution value, is propagated across layers in the network. • Cortical Activation: A neuronal region of activation within the network is described as a coalition of players-connected neurons cooperating to optimize the payoff function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Information Filtering and Model Regularization:</head><p>The coalition with the maximum payoff is deemed the winning coalition, and the contribution of each neuron within this coalition is quantified using the Shapley value. Neurons with high contributions form a strong coalition, and only these neurons transmit information forward to the next layer. In this very phase, some neurons are dropped out to a achieve a dynamic model regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SOME BASICS OF GAME THEORY</head><p>The following definitions are essential to grasp some knowledge about game theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simple Games</head><p>To gain insight into the proposed methodology, we introduce some principles related to game theory, focusing on the concepts of simple games and cooperative sequential games <ref type="bibr" target="#b22">[23]</ref> [24] <ref type="bibr" target="#b24">[25]</ref>. A simple game involves a set of n players; a set of strategies s i ∈ S i (possible actions) associated with each player i ∈ N = {1, 2, . . . , n}, where s = (s 1 , s 2 , . . . , s n ) ∈ S = (S 1 × S 2 × . . . × S n ) is a set of pure strategy profiles; a set of payoffs (real values) v i (s 1 , s 2 , . . . , s n ) ∈ R (v i : S -→ R) assigned to each player i for every possible list of strategy choices-where strategies translate into outcomes and each player has preferences over these outcomes represented by their payoffs-and a level of information or belief, which encompasses what players know and believe about the situation and one another, and what actions they observe before making decisions. The game is finite if S is finite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Notion of Simple Coalition</head><p>We define the concept of coalition and the contribution of each player within this coalition. A simple coalition is a group of players C ⊂ N that cooperate to achieve a common goal. The set N is often referred to as the grand coalition. Every coalition C has a set of actions. If the payoffs v(C) associated with a coalition C are freely redistributed among its members, this condition is known as the Transferable Utility Assumption (TUA). A coalitional game with transferable utility is a pair (N , v), in which N is a finite set of players, and v : 2 N -→ R maps each coalition C to a real-valued payoff function v(C) that the coalition members can distribute among themselves. We assume that v(∅) = 0. Given a coalitional game (N , v), the Shapley value associated with player i ∈ N is given by:</p><formula xml:id="formula_0">ϕ i (N , v) = 1 n! C⊆N \{i} |C|!(n -|C| -1)![v(C ∪ {i}) -v(C)].</formula><p>(1) The Shapley value expresses the average marginal contribution of player i, averaging over all different coalitions with respect to which the grand coalition can be built starting from the empty one <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NEUROGAME: GAME THEORY MEETS STATISTICAL MECHANICS</head><p>We present in this section the analogy between conventional DL and NEUROGAME, as well as the description of all the components needed to fully comprehend how this proposed deep learning model operates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison between Conventional Deep Learning and NEUROGAME</head><p>we make the following correspondence between cooperative game theory and deep learning representation:</p><p>1) A layer of a deep neural network represents a game.</p><p>2) A neuron in a layer of a deep neural network represents a player of the game. Neurons are viewed as particles interacting via statistical mechanics laws. 3) Each neuron is mapped to a set of actions representing its current state (a specific interval of neuron activation values). This set of actions acts as its strategy. 4) An input to the deep neural network structure corresponds to the information (or observation within the environment) that is available at any time of the game. In our setting, an input is a 2D image. 5) A neuronal region depicts a group of connected neurons (s 1 , . . . , s n ) that are located within a certain neighborhood in the cerebral cortex; it constitutes a simple coalition of players. 6) A payoff v i (s 1 , . . . , s n ) assigned to this coalition expresses the worth of the actions exhibited by all players forming this coalition. This function is conveyed through the energy function assigned to a tuple of activations of neurons. This tuple is called a configuration state of the coalition. The coalitions with high payoffs are sought: They represent the winning coalitions. The payoff function reflects the quality level of the information available. 7) The contribution of a neuron within each winning coalition is expressed by its Shapley value expressed through equation ( <ref type="formula">1</ref>). Neurons with high Shapley values are members of strong coalitions. Only strong coalitions, extracted from the winning coalitions, are permitted to forward the flow of information from one layer to the next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computation of a Coalition Payoff</head><p>This section describes the relationship between the energy function, the Gibbs distribution and the payoff (also known as utility) function assigned to a coalition. The computation of all three functions requires a definition of a neighborhood system between neurons that compose a coalition.</p><p>1) Neurons Neighborhood System:: For the sake of illustration and without loss of generality, we focus on neighbors of a neuron within a (3,3) neuronal grid. A (3, 3) neighborhood system H of a neuron located at coordinates (i, j) is the set {(i -1, j), (i -1, j + 1), (i, j + 1), (i + 1, j + 1), (i + 1, j), (i + 1, j -1), (i, j -1), (i -1, j -1)}.</p><p>This neighborhood system is needed during the clique structure used by the energy function.</p><p>Definition 1: A set of random variables is a Gibbs random field (GRF) on a set Ω with respect to a neighborhood system H if and only if its configuration obeys a Gibbs distribution. We now introduce the notion of configuration state that is needed in the evaluation of the energy and payoff functions.</p><p>Definition 2: A configuration state assigned to a simple coalition is a sequence of activation values of neurons that form this coalition. This configuration state is denoted: ω i = (a i s1 , . . . , a i sn ) where: a i sj is the neuron activation value at location s j in the coalition i and n is the size of the coalition.</p><p>2) Gibbs Distribution of a Configuration State:: During a regression or classification task, we aim for the activation of neurons to progressively increase from the first layer to the last layer in a deep neural network. This behavior is compatible with the energy minimization principle. The Gibbs (or Boltzmann) distribution relies on the energy function assigned to the i-th configuration state.</p><p>Definition 3: The Gibbs distribution function is defined as:</p><formula xml:id="formula_1">P (ω i , T ) = 1 Q e -E(ω i ) k B ×T = e -E(ω i ) k B ×T j=M j=1 e -E(ω j ) k B ×T ,<label>(2)</label></formula><p>• P (ω i , T ) is the probability of the i-th configuration state at temperature T , • E(ω i ) is the energy of the i-th configuration state,</p><formula xml:id="formula_2">• k B represents the Boltzmann constant (k B ≈ 1.38 × 10 -23</formula><p>), • T is the temperature of the system, • M denotes the number of all configuration states associated to all simple coalitions within a layer, • Q is the canonical partition function (normalizing factor). This distribution shows that configuration states with lower energy will always be assigned a higher probability of being occupied than those with higher energy. However, the energy assigned to a configuration state is defined via a potential function expressed through the Ising model using bonding strengths (synaptic links) between neurons in a lattice structure. This energy, which is a Hamiltonian function, is therefore expressed as:</p><formula xml:id="formula_3">E(ω) = &lt;p,q&gt; b pq × 1 a p × a q + p f p × 1 a p ,<label>(3)</label></formula><p>• b pq is the bonding strength between two neighbor neurons p and q,</p><p>• f q is the external magnetic field interacting with the lattice, • a p , and a q are non-zero activation values assigned to neuron p and q, respectively, • &lt; p, q &gt; is a pair of neighbor neurons. If we set f p = α and b pq = β, therefore the Ising model expressed via equation 3 can be rewritten as follows:</p><formula xml:id="formula_4">E(ω) = α p 1 a p + β q∈H(p) 1 a p × a q , ∀p<label>(4)</label></formula><p>where H(p) is a (m, n) neighborhood system. The second summation is over pairs of neighboring neurons. The energy decreases when the activation values in a configuration state are high. In other words, a smaller energy means a higher neuronal activation. However, we consider the temperature T as dependent on the iteration number i during the training of NEUROGAME. It is expressed as follows:</p><formula xml:id="formula_5">T (i) = c × 10 23 ln (1 + i) ,<label>(5)</label></formula><p>where the numerator is a large constant value that ensures a high temperature at initialization. Therefore, using equation 2, one can compute the Gibbs distribution P (ω i , T ) assigned to each configuration state.</p><p>3) Generation of Configurations States:: In order to compute the Gibbs distribution, one has to estimate the normalizing term that requires M configuration states. The set of configuration states contained in one layer is built using a grouping (set of neurons acting together) containing neurons that are close to each other. An element of this grouping can be a 4×4 (or 5×5) grid of neurons. A simple coalition in a layer is composed of neurons that are nearby with respect to a distance measure. We generate through this partitioning process M configuration states with different levels of neuron activations. Moreover, each configuration state is assigned a Gibbs distribution value.</p><p>4) Layer Neighborhood System:: We show in this step how a neighborhood system (a lattice structure) can be constructed in order to compute the energy associated to the Ising model. A neighborhood system is based on a metric (or distance) between neurons of a layer. This set of neighbors associated to neuron (i, j) is composed of the sites {(i, j -1), (i -1, j), (i, j + 1), (i + 1, j)}.</p><p>5) The Payoff Function:: Since we are in the context of a collaborative game theory, therefore, the contribution of a group of players should induce a higher payoff than the one incurred by a single player within a simple coalition. Furthermore, a maximum payoff value should be assigned a minimum energy value. Using Boltzmann's distribution, this minimum energy value corresponds to a maximum Gibbs distribution value. We now define the payoff as being proportional to the Gibbs distribution:</p><p>Definition 4: The payoff function assigned to a simple coalition is expressed as follows:</p><formula xml:id="formula_6">Payoff(ω i , T ) = ln k 1 1 -P (ω i , T ) ,<label>(6)</label></formula><p>where k 1 is a positive control parameter and the natural logarithm is applied to smooth this function.</p><p>One can notice that a high payoff corresponds to a low neuronal energy value. Neurons are supposed to behave as microscopic physical particles interacting seamlessly. The configuration state ω * with the maximum Payoff value is assigned the winning coalition among all simple coalition associated to the M possible configuration states. Definition 5: A configuration state ω * with a maximum Payoff value is associated to a winning coalition among all possible simple coalitions. The Payoff value represents the worth of the winning coalition. It tallies the total expected sum of payoffs the members of this coalition can gain by cooperating. However, instead of considering only one winning coalition, a set of p winning coalitions derived from p top choices of payoff values are considered.</p><p>6) The Concept of Strong Coalition:: The payoff value assigned to a configuration state of is needed during the Shapley value computation. This payoff corresponds to the utility function v used in the Shapley function expressed by equation 1. This payoff function requires the computation of: v(C ∪ {i}) -v(C), which is the leading term in the Shapley value computation, associated to player i, denoted ϕ i (N , v = Payoff). This leading term corresponds to:</p><formula xml:id="formula_7">Payoff(C ∪ {i}) -Payoff(C), ∀C⊆ (N \ {i}).<label>(7)</label></formula><p>The summation used in equation 1 consists in extracting all subsets C from the simple coalition N (set of players) that do not contain player i. The number of these subsets is 2 N -1 . However, among all subsets, only those subsets whose cardinalities are greater or equal than 2 are considered, since the singletons do not form coalitions.</p><p>Once the winning coalition is identified, members of this coalition who contributed most to the payoff are maintained; the other members with low contributions are dropped out. This regularization technique that is not based on randomness represents one key feature of novelty exhibited by NEUROGAME. Definition 6: A strong coalition is composed of all neurons whose Shapley values are greater than a threshold value ρ. Neurons with a high payoff (or high Gibbs distribution) are those that form the strong coalition. The threshold ρ is dynamic; it involves the contribution of each neuron forming a coalition within a network layer and the iteration number i: it is expressed specifically via the following function:</p><formula xml:id="formula_8">ρ(S i cj , i) = Q 1 (S i cj ) × ln(1 + i).<label>(8)</label></formula><p>The function 'ln' represents the natural logarithm, while Q 1 (S i cj ) denotes the first quartile of the set S of Shapley values (sorted in ascending order) assigned to the set of neurons forming a winning coalition c j for j = 1, . . . , p, with each coalition being of size n. If n is the number of these values, therefore, this first quartile is equal to (n+1)/4; it indicates that 25% of the data are below this point.</p><p>Theorem 1 (Shapley Threshold Behavior): For large values of n (coalition size) and i (iteration number during training), the function ρ(S i cj , i) will tend to increase, with the growth rate influenced by ln(1 + i).</p><p>Proof: If i increases during training, the natural logarithm function ln(i) grows without bound, but it does so very slowly compared to linear functions. Therefore, ln(1+i) will continue to increase, but at a gradually slowing, logarithmic rate. However, Q 1 (S i cj ) depends on the Shapley values distribution. As we increase the coalition size n, the value of Q 1 (S i cj ) does not necessarily increase. It reflects the position within the ordered data rather than growing unbounded. Finally, the combined effect on ρ(S i cj , i) will be affected by the product of these two functions: Q 1 (S i cj ) and ln(1 + i). In conclusion, the primary driver of the behavior of the threshold ρ(S i cj , i) for large i and n will be ln(1 + i).</p><p>It is worth noting that as NEUROGAME learns, the selected coalitions grow progressively stronger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Different Phases in NEUROGAME</head><p>The following sequence of operations describes NEUROGAME:</p><p>1) The input is a colored (or grey-level) image with its three colors components red, green, blue (n c = 3), with a dimension equal to (n×n) for each color:</p><formula xml:id="formula_9">(n × n × n c ).</formula><p>2) A convolution operation with three filters F 1 , F 2 and F 3 , each one with a dimension equal to f × f is subsequently applied, to each color (f × f × n c ). An arithmetic mean value is computed for each element of the three colored matrices after convolution.</p><p>3) The results of the convolution between filters and the image is represented by three feature map matrices with dimension (n -f + 1) × (n -f + 1) × n c . 4) An activation function is applied to the product of the feature map matrices and the first weight matrix W 1 , and the result is stored as the three activation map matrices with a predefined dimension (l × l).  maintained and neurons with Shapley values that fall under this threshold value are removed. 9) This entire process continues during the first training iteration until reaching the last layer k. The activation values of the strong coalitions corresponding to the three energy maps are concatenated to form a feature vector assigned to the input image. 10) This feature vector is subsequently fed to a fully connected neural network with k hidden layers. 11) The Softmax operation is applied for the evaluation of the loss function during training. 12) All weights are updated using the opposite direction of the gradient of the loss function.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates the NEUROGAME training procedure when the observation input is an image and the number of labels for a classification task is four (C 1 , C 2 , C 3 , C 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. NEUROGAME Layers and Information Propagation</head><p>In this section, we show how a layer of NEUROGAME is built. We also describe how the nformative signals are communicated to the next layer during training of the entire deep neural network.</p><p>1) NEUROGAME Layer:: A layer in this proposed deep neural network is composed of five components: (i) activation maps, (ii) a set of M coalitions, (iii) a set of energy maps, (iv) a set of winning coalitions, and (v) a set of strong coalitions (refer to Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>2) Transmission of the Information:: The most informative signals generated from neurons pertaining to the strong coalitions (those that passed the ρ test) of layer l are forwarded to neurons of layer (l + 1). In fact, these signals represent the image by the activation function of the product of two quantities: (i) The activation value of a neuron within a strong coalition in layer l, and (ii) the weight (synaptic link) that connects this neuron to a specific neuron of layer (l + 1). These two quantities are the ones involved during a forward transmission of information during NEUROGAME training-based on backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head><p>To demonstrate the effectiveness of the proposed methodology, we have performed two different classification tasks: 1) gender classification, and 2) simultaneous age and gender classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Architecture</head><p>To assess NEUROGAME's performance, we used two benchmarked datasets designed for distinct classification tasks: CelebA (CelebFaces Attributes) <ref type="bibr" target="#b25">[26]</ref> dataset for gender classification and UTKFace dataset <ref type="bibr" target="#b26">[27]</ref> for age and gender classification concurrently. We now present the architectures of the two baseline models for gender classification and simultaneous age and gender classification, alongside comparisons with our proposed NEUROGAME method.</p><p>Gender Classification: Multi-Layer Perceptron (MLP):</p><p>• Input: Images of size (64, 64, 3).</p><p>• Layers: Flattened input followed by dense layers (256 units ReLU, Batch-Normalization, Dropout; 128 units ReLU, Batch-Normalization, Dropout; 64 units ReLU, Batch-Normalization, Dropout). • Output: Single unit with sigmoid activation for binary classification. For comparison with NEUROGAME, a single NEUROGAME layer is added with a coalition size of (2,2) and a top-p value of 0.85.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simultaneous</head><p>Gender and Age Classification: Convolutional Neural Network (CNN):</p><p>• Input: Images of size (128, 128, 1).</p><p>• Layers: Four Conv2D layers (32, 64, 128, 256 filters with (3, 3) kernels and ReLU activation), followed by max-pooling (2, 2). • Flatten and two dense layers (256 units each, ReLU activation), each followed by a Dropout layer. • Outputs: Gender (sigmoid activation), Age (ReLU activation). For comparison, Conv2D layers are replaced with NEUROGAME layers (top-p=0.85, coalition size=(2,2)) to evaluate NEUROGAME's performance in classification tasks. In all NEUROGAME models, we applied a Convolutional layer with three filters to generate feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameter Tuning for NEUROGAME</head><p>To determine the optimal hyperparameter values for NEUROGAME, extensive experimentation was carried out. The results showed that the most effective configuration was achieved by setting α to 0 and β to 1 (refer to Equation <ref type="formula" target="#formula_4">4</ref>). With α = 0, the model's energy is determined exclusively through the interactions between neighboring neurons, thereby simplifying the system by excluding the contributions of individual neurons. Setting β = 1 preserves the original form of neighbor interactions without any additional weighting. For temperature estimation, a value of c = 1 was found to be optimal (refer to Equation <ref type="formula" target="#formula_5">5</ref>), while k 1 = 1 was determined to be the best setting for the payoff calculation (refer to Equation <ref type="formula" target="#formula_6">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gender Classification</head><p>For gender classification, we applied data augmentation through random cropping and horizontal flipping to increase training diversity and model robustness. The images were normalized by scaling pixel values to [0, 1] and dividing by 255. Models were trained with a batch size of 64 using Adam optimizer <ref type="bibr" target="#b27">[28]</ref> and binary cross-entropy loss. Figure <ref type="figure">2</ref> shows that NEUROGAME achieved more effective reduction in loss and better generalization, with a test loss of 0.2645 compared to MLP's 0.4335, as highlighted in Table <ref type="table">I</ref>, demonstrating NEUROGAME's superior performance. Model Test Loss Test Accuracy (%) MLP 0.4335 80.19 NEUROGAME 0.2645 88.26</p><p>TABLE I: Test performance comparison between MLP and NEUROGAME models on CelebA dataset. Furthermore, the test accuracy of NEUROGAME is 88.26%, substantially higher than MLP model's test accuracy of 80.19%. This improvement in accuracy demonstrates NEUROGAME's superior capability in generalizing from the training data to unseen data, confirming that the incorporation of NEUROGAME's specialized layer results in enhanced model performance and reliability. To further assess the efficacy of NEUROGAME, we expanded our investigation to include in the next section a more intricate task: simultaneous classification of age and gender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Simultaneous Age and Gender Classification</head><p>In age and gender classification, we use two classifiers: the first focuses on gender with binary cross-entropy loss and accuracy as the metric, while the second predicts age as a continuous value using mean absolute error as the loss function. During inference, a correct age class prediction is considered a success. Both models are optimized using the Adam optimizer with a batch size of 32 and trained for 100 epochs without data augmentation. This setup thoroughly assesses the performance and robustness of CNN and NEUROGAME in age and gender classification.   Comparison of training and validation performance between CNN and NEUROGAME for age classification. gender and age classification. NEUROGAME consistently outperforms CNN, demonstrating superior generalization with lower validation loss and better metrics. Precision was computed for each model across gender, age, and combined classifications. NEUROGAME, with fewer parameters, shows higher average precision than CNN. As shown in Table II, NEUROGAME achieves higher precision across all age groups in gender classification and generally leads in age classification. This was validated on UTKFace test set, where NEUROGAME maintained higher precision, especially in younger and middle age categories, highlighting its robustness in multitask learning. Due to limited data in this age group, CNN model predicts an age of 93 years, while NEUROGAME predicts 101 years, closer to the ground truth, showing superior generalization. This indicates NEUROGAME's better handling of sparse data compared to CNN. The image was randomly selected, underscoring NEUROGAME's robustness and reliability. The two experiments highlight the effectiveness of NEUROGAME, especially in classification tasks, when compared to well-established ML models. Indeed, NEUROGAME has outperformed both MLP and CNN models in gender classification as well as in the simultaneous Class Gender Age Gender and Age CNN NEUROGAME CNN NEUROGAME CNN NEUROGAME</p><p>TABLE II: precisions (%) of CNN and NEUROGAME on UTKFace test set across age and gender categories.</p><p>classification of gender and age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND PERSPECTIVES</head><p>We have developed a novel DL architecture, NEUROGAME, which integrates game theory and statistical physics principles. This allows neurons in the same layer to collaborate using the Shapley value function to assign contribution scores and perform controlled dropout, reducing overfitting. This Shapley-based regularization enhances network robustness and provides transparency within the architecture, functioning as a glass-box framework.</p><p>Comparative studies show NEUROGAME outperforms MLP and CNN in gender and joint gender-age classification, showing better generalization and accuracy. This research signals a paradigm shift in deep learning, paving the way for more interpretable, efficient, and effective neural networks. As a perspective, we will explore the Banzhaf power index to assess the influence of neuronal states in prediction tasks, potentially improving model generalization further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 )</head><label>5</label><figDesc>Generation of M simple coalitions within each of the the three activation map matrices of the first layer. The value of M is equal to the dimension of a layer divided by n. Therefore, each simple coalition has a size of n neurons and is assigned a configuration state (activation values). 6) Computation of the energy value for each simple coalition (configuration state) within an activation map matrix. The set of energy values within an activation map of the first layer forms an energy map. Therefore, we obtained three energy maps. 7) Selection of p-top choices simple coalitions given their payoff values. They are the p winning coalitions of each energy map. 8) Extraction of the strong coalitions amongst the winning ones. Neurons of the winning coalitions whose Shapley values exceed the threshold value ρ(S l cj , i) are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The training procedure of NEUROGAME showing the passage from M simple coalitions to p winning coalitions and then to p strong coalitions generated via the Shapley filtering process. The computation of the strong coalitions (integrated into a fully connected neural network) is repeated across all k layers until NEUROGAME converges. The feature vector extracted at this convergence point is composed of activation values of the last optimal strong coalitions.</figDesc><graphic coords="5,91.80,54.00,428.41,311.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figures 3 and 4 Fig. 2 :</head><label>42</label><figDesc>Fig. 2: Comparison of training and validation losses and accuracies between MLP and NEUROGAME models. NEUROGAME shows better generalization performance, as evidenced by the lower validation loss and improved validation metrics.</figDesc><graphic coords="6,313.20,462.36,251.99,208.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Comparison of training and validation performance between CNN and NEUROGAME models for gender classification.</figDesc><graphic coords="7,54.00,53.99,252.00,206.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of training and validation performance between CNN</figDesc><graphic coords="7,54.00,301.97,251.98,107.23" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning models for digital image processing: a review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Archana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jeevaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Medical images classification using deep learning: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumbharkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Medical image classification for alzheimer&apos;s using a deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vishvakarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Engineering and Applied Science</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">54</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning-based chatbot by natural language processing for supportive risk management in river dredging projects</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">107744</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic speech recognition using advanced deep learning approaches: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kheddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hemis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Himeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="page">102422</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning based speech recognition for hyperkinetic dysarthria disorder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hashan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dmitrievich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Valerievich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Vasilyevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Alexandrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Andreevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="12" to="015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Maschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Game</forename><surname>Theory</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ore image segmentation application based on deep learning and game theory</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World science: problems and innovations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A game-theoretic analysis of deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Aspects in Information and Management: 15th International Conference, AAIM 2021, Virtual Event</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">December 20-22, 2021. 2021</date>
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning and game theory for computation offloading in dynamic edge computing markets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="121456" to="121466" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A game-theoretic approach for federated learning: A trade-off among privacy, accuracy and energy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Communications and Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="403" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient ensemble pruning approach based on simple coalitional games</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ykhlef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchaffra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28" to="42" />
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Game-theoretic unlearnable example generator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="21349" to="21358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient deep reinforcement learning algorithm for solving imperfect information extensive-form games</title>
		<author>
			<persName><forename type="first">L</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="5823" to="5831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning and game theory for ai-enabled human-robot collaboration system design in industry 4.0</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Applications of game theory in deep learning: a survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anjaria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="8963" to="8994" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Applications of game theory in deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anjaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Game Theory in Deep Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tuckerman</surname></persName>
		</author>
		<title level="m">Statistical Mechanics: Theory and Molecular Simulation</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning in computational mechanics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kollmannsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>D'angella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jokeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Computational Mechanics</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="55" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical mechanics of continual learning: Variational principle and mean-field potential</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">14309</biblScope>
			<date type="published" when="2023-07">Jul 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Stengel</surname></persName>
		</author>
		<title level="m">Game Theory Basics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Who Gets What-and Why</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HarperCollins</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introducing Game Theory and its Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zwillinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Utkface</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://susanqq.github.io/UTKFace/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
