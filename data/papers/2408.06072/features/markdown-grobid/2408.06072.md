# CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer

## Abstract

## 

Figure 1: CogVideoX can generate long-duration, high-resolution videos with coherent actions and rich semantics.

## Introduction

The rapid development of text-to-video models has been phenomenal, driven by both the Transformer architecture [(Vaswani et al., 2017)](#b30) and diffusion model [(Ho et al., 2020)](#b12). Early attempts to pretrain and scale Transformers to generate videos from text have shown great promise, such as CogVideo [(Hong et al., 2022)](#b14) and Phenaki [(Villegas et al., 2022)](#b31). Meanwhile, diffusion models have recently made exciting advancements in video generation [(Singer et al., 2022;](#b27)[Ho et al., 2022)](#b13). By using Transformers as the backbone of diffusion models, i.e., Diffusion Transformers (DiT) [(Peebles & Xie, 2023](#b21)), text-to-video generation has reached a new milestone, as evidenced by the impressive Sora showcases [(OpenAI, 2024)](#b20).

Despite these rapid advancements in DiTs, it remains technically unclear how to achieve long-term consistent video generation with dynamic plots. For example, previous models had difficulty generating a video based on a prompt like "a bolt of lightning splits a rock, and a person jumps out from inside the rock."

In this work, we train and introduce CogVideoX, a set of large-scale diffusion transformer models designed for generating long-term, temporally consistent videos with rich motion semantics. We address the challenges mentioned above by developing a 3D Variational Autoencoder, an expert Transformer, a progressive training pipeline, and a video data filtering and captioning pipeline, respectively.

First, to efficiently consume high-dimension video data, we design and train a 3D causal VAE that compresses the video along both spatial and temporal dimensions. Compared to previous method [(Blattmann et al., 2023)](#b5) of fine-tuning 2D VAE, this strategy helps significantly reduce the sequence length and associated training compute and also helps prevent flicker in the generated videos, that is, ensuring continuity among frames. Second, to improve the alignment between videos and texts, we propose an expert Transformer with expert adaptive Layer-Norm to facilitate the fusion between the two modalities. To ensure the temporal consistency in video generation and capture largescale motions, we propose to use 3D full attention to comprehensively model the video along both temporal and spatial dimensions.

Third, as most video data available online lacks accurate textual descriptions, we develop a video captioning pipeline capable of accurately describing video content. This pipeline is used to generate new textual descriptions for all video training data, which significantly enhances CogVideoX's ability to grasp precise semantic understanding.

In addition, we adopt and design progressive training techniques, including multi-resolution frame pack and resolution progressive training, to further enhance the generation performance and stability of CogVideoX. Furthermore, we propose Explicit Uniform Sampling, which stablizes the training loss curve and accelerates convergence by setting different timestep sampling intervals on each data parallel rank.

To date, we have completed the CogVideoX training with two parameter sizes: 5 billion and 2 billion, respectively. Both machine and human evaluations suggest that CogVideoX-5B outperforms well-known public models and CogVideoX-2B is very competitive across most dimensions.

Figure [2](#fig_0) shows the performance of CogVideoX-5B and CogVideoX-2B in different aspects. It shows that CogVideoX has the property of being scalable. As the size of model parameters, data volume, and training volume increase, the performance will get better in the future.

Our contributions can be summarized as follows:

• We propose CogVideoX, a simple and scalable structure with a 3D causal VAE and an expert transformer, designed for generating coherent, long-duration, highaction videos. It can generate long videos with multiple aspect ratios, up to 768×1360 resolution, 10 seconds in length, at 16fps, without super-resolution or frame-interpolation.

• We evaluate CogVideoX through automated metric evaluation and human assessment, compared with openly-accessible top-performing text-to-video models. CogVideoX achieves state-of-the-art performance.

• We publicly release our 5B and 2B models, including text-to-video and image-tovideo versions, the first commercial-grade open-source video generation models. We hope it can advance the filed of video generation.

## The CogVideoX Architecture

In the section, we present the CogVideoX model. Figure [3](#) illustrates the overall architecture. Given a pair of video and text input, we design a 3D causal VAE to compress the video into the latent space, and the latents are then patchified and unfolded into a long sequence denoted as z vision . Simultaneously, we encode the textual input into text embeddings z text using T5 [(Raffel et al., 2020)](#b23). Subsequently, z text and z vision are concatenated along the sequence dimension. The concatenated embeddings are then fed into a stack of expert transformer blocks. Finally, the model output are unpatchified to restore the original latent shape, which is then decoded using a 3D causal VAE decoder to reconstruct the video. We illustrate the technical design of the 3D causal VAE and expert transfomer in detail.

Figure [3](#): The overall architecture of CogVideoX. Videos contain both spatial and temporal information, typically resulting in much larger data volumes than images. To tackle the computational challenge of modeling video data, we propose to implement a video compression module based on 3D Variational Autoencoders [(Yu et al., 2023)](#b11). The idea is to incorporate three-dimentional convolutions to compress videos both spatially and temporally. This can help achieve a higher compression ratio with largely improved quality and continuity of video reconstruction.  We adopt the temporally causal convolution [(Yu et al., 2023)](#b11), which places all the paddings at the beginning of the convolution space, as shown in Figure [4](#fig_1)[(b)](#). This ensures that future information does not influence the present or past predictions.

We also conducted ablation studies comparing different compression ratios and latent channels in table 1. After using 3D structures, the reconstructed video shows almost no more jitter, and as the latent channels increase, the restoration quality improves. However, when spatialtemporal compression is too aggressive (16×16×8), even if the channel dimensions are correspondingly increased, the convergence of the model also becomes extremely difficult.

Exploring VAE with larger compression ratios is our future work.

Given that processing videos with a large number of frames introduces excessive GPU memory usage, we apply context parallel at the temporal dimension for 3D convolution to distribute computation among multiple devices. As illustrated by Figure [4](#fig_1) (b), due to the causal nature of the convolution, each rank simply sends a segment of length k -1 to the next rank, where k indicates the temporal kernel size. This results in relatively low communication overhead. During training, we first train a 3D VAE at 256 × 256 resolution and 17 frames to save computation. Randomly select 8 or 16 fps to enhance the model's robustness. We observe that the model can encode larger resolution videos well without additional training as it has no attention modules, but this isn't effective when encoding videos with more frames. Therefore, we conduct a two-stage training process by first training on a video of 17 frames and finetuning by context parallel on videos of 161 frames. Both stages of training utilize a weighted combination of the L1 reconstruction loss, LPIPS (Zhang et al., 2018) perceptual loss, and KL loss. After a few thousand training steps, we introduce the GAN loss from a 3D discriminator as an additional loss term.

## Expert Transformer

We introduce the design choices in Transformer for CogVideoX, including the patching, positional embedding, and attention strategies for handling the text-video data effectively and efficiently.

Patchify. The 3D causal VAE encodes a video latent of shape T × H × W × C, where T represents the number of frames, H and W represent the height and width of each frame, C represents the channel number, respectively. The video latents are then patchified, generating sequence z vision of length T q • H p • W p . When q > 1, we repeat the first frame of videos and images at the beginning of the sequence to enable joint training of images and videos. 3D-RoPE. Rotary Position Embedding (RoPE) [(Su et al., 2024](#b28)) is a relative positional encoding that has been demonstrated to capture inter-token relationships effectively in LLMs, particularly excelling in modeling long sequences. To adapt to video data, we extend the original RoPE to 3D-RoPE. Each latent in the video tensor can be represented by a 3D coordinate (x, y, t). We independently apply 1D-RoPE to each dimension of the coordinates, each occupying 3/8, 3/8, and 2/8 of the hidden states' channel. The resulting encoding is then concatenated along the channel dimension to obtain the final 3D-RoPE encoding.

Figure [5](#): The separated spatial and temporal attention makes it challenging to handle the large motion between adjacent frames. In the figure, the head of the person in frame i + 1 cannot directly attend to the head in frame i. Instead, visual information can only be implicitly transmitted through other background patches. This can lead to inconsistency issues in the generated videos.

Expert Adaptive Layernorm. We concatenate the embeddings of both text and video at the input stage to better align visual and semantic information. However, the feature spaces of these two modalities differ significantly, and their embeddings may even have different numerical scales. To better process them within the same sequence, we employ the Expert Adaptive Layernorm to handle each modality independently. As shown in Figure [3](#), following DiT [(Peebles & Xie, 2023)](#b21), we use the timestep t of the diffusion process as the input to the modulation module. Then, the Vision Expert Adaptive Layernorm (Vison Expert AdaLN) and Text Expert Adaptive Layernorm (Text Expert AdaLN) apply this modulation mechanism to the vision hidden states and text hidden states, respectively. This strategy promotes the alignment of feature spaces across two modalities while minimizing additional parameters.

3D Full Attention. Previous works [(Singer et al., 2022;](#b27)[Guo et al., 2023)](#b11) often employ separated spatial and temporal attention to reduce computational complexity and facilitate fine-tuning from text-to-image models. However, as illustrated in Figure [5](#), this separated attention approach requires extensive implicit transmission of visual information, significantly increasing the learning complexity and making it challenging to maintain the consistency of large-movement objects. Considering the great success of long-context training in LLMs (AI@Meta, 2024) and the efficiency of FlashAttention [(Dao et al., 2022)](#b8), we propose a 3D text-video hybrid attention mechanism. This mechanism not only achieves better results but can also be easily adapted to various parallel acceleration methods. 

## Training CogVideoX

We mix images and videos during training, treating each image as a single-frame video. Additionally, we employ progressive training from the resolution perspective. For the diffusion setting, we adopt v-prediction [(Salimans & Ho, 2022](#b25)) and zero SNR [(Lin et al., 2024)](#b18), following the noise schedule used in LDM [(Rombach et al., 2022)](#b24).

## Multi-Resolution Frame Pack

Previous video training methods often involve joint training of images and videos with a fixed number of frames [(Singer et al., 2022;](#b27)[Blattmann et al., 2023)](#b5). However, this approach usually leads to two issues: First, there is a significant gap between the two input types using bidirectional attention, with images having one frame while videos having dozens of frames. We observe that models trained this way tend to diverge into two generative modes based on the token count and not to have good generalizations. Second, to train with a fixed duration, we have to discard short videos and truncate long videos, which prevents full utilization of the videos of varying number of frames.

To address these issues, we choose mixed-duration training, which means training videos of different lengths together. However, inconsistent data shapes within the batch make training difficult. Inspired by Patch'n Pack [(Dehghani et al., 2024)](#b9), we place videos of different duration (also in different resolutions) into the same batch to ensure consistent shapes within each batch, a method we refer to as Multi-Resolution Frame Pack. The process is illustrated in Figure [6](#fig_3).

We use 3D RoPE to model the position relationship of various video shape. There are two ways to adapt RoPE to different resolutions and durations. One approach is to expand the position encoding table and, for each video, select the front portion of the table according to the resolution (extrapolation). The other is to scale a fixed-length position encoding table to match the resolution of the video (interpolation). Considering that RoPE is a relative position encoding, we chose the first approach to keep the clarity of model details.

## Progressive Training

Videos from the Internet usually include a significant amount of low-resolution ones. And directly training on high-resolution videos is extremely expensive. To fully utilize data and save costs, the model is first trained on 256px videos to learn semantic and low-frequency knowledge. Then it is trained on gradually increased resolutions, from 256px to 512px, 768px, to learn high-frequency knowledge. To maintain the ability of generating videos with different aspect ratios, we keep the aspect ratio unchanged and resize the short side to above resolutions. Finally, we select a subset of high-quality videos to fine-tune the model, since the filtered pre-training data still contains a certain proportion of dirty data, such as subtitles, watermarks, and low-bitrate videos. We find this step can effectively remove generated subtitles and watermarks and improve the visual quality. Moreover, we trained an image-to-video model based on above model. See Appendix D for details.

## Explicit Uniform Sampling

## Ho et al. (2020) defines the training objective of diffusion as

$L simple (θ) := E t,x0,ϵ ϵ -ϵ θ ( √ ᾱt x 0 + √ 1 -ᾱt ϵ, t) 2 , (1$$)$where t is uniformly distributed between 1 and T. The common practice is for each rank in the data parallel group to uniformly sample a value between 1 and T , which is in theory equivalent to Equation 1. However, in practice, the results obtained from such random sampling are often not sufficiently uniform, and since the magnitude of the diffusion loss is related to the timesteps, this can lead to significant fluctuations in the loss. Thus, we propose to use Explicit Uniform Sampling to divide the range from 1 to T into n intervals, where n is the number of ranks. Each rank then uniformly samples within its respective interval. This method ensures a more uniform distribution of timesteps. As shown in Figure [10 (d)](#), the loss curve from training with Explicit Uniform Sampling is noticeably more stable.

In addition, we compare the loss at each diffusion timestep alone between two choices for a more precise comparison. We find after using explicit uniform sampling, the loss at all timesteps decreased faster, indicating that this method can also accelerate loss convergence.

## Data

We construct a collection of relatively high-quality video clips with text descriptions with video filters and recaption models. After filtering, approximately 35M single-shot clips remain, with each clip averaging about 6 seconds. We additionally use 2B images filtered with aesthetics score from LAION-5B [(Schuhmann et al., 2022)](#b26) and COYO-700M [(Byeon et al., 2022)](#b6) datasets to assist training.

Video Filtering. Video generation models should capture the dynamic nature of the world. However, raw video data often contains significant noise for two intrinsic reasons: First, the artificial editing during video creation can distort the true dynamic information; Second, video quality may suffer due to filming issues such as camera shakes or using subpar equipment. In addition to the intrinsic quality of the videos, we also consider how well the video data supports model training. Videos with minimal dynamic information or lacking connectivity in dynamic aspects are considered detrimental. Consequently, we have developed a set of negative labels, which include:

• Editing: Videos that have undergone noticeable artificial processing, such as reediting and special effects, which compromise the visual integrity.

• Lack of Motion Connectivity: Video segments with transitions that lack coherent motion, often found in artificially spliced videos or those edited from static images.

• Low Quality: Poorly shot videos with unclear visuals or excessive camera shake.

• Lecture Type: Videos focusing primarily on a person continuously talking with minimal effective motion, such as educational content, lectures, and live-streamed discussions.

• Text Dominated: Videos containing a large amount of visible text or primarily focusing on textual content.

• Noisy Screenshots: Videos captured directly from phone or computer screens, often characterized by poor quality.

We first sample 20,000 videos and label each video as positive or negative by their quality.

Using these annotations, we train 6 filters based on Video-LLaMA [(Zhang et al., 2023)](#b35) to screen out low-quality video data. Examples of negative labels and the classifier's performance on the test set can be found in appendix J. In addition, we calculate the optical flow scores and image aesthetic scores of all training videos, and dynamically adjust their threshold during training to ensure the dynamic and aesthetic quality of generated videos.

Video Captioning. Video-text pairs are essential for the training of text-to-video generation models. However, most video data does not come with corresponding descriptive text. Therefore, it is necessary to label the video data with comprehensive textual descriptions. Currently, there are some video caption datasets available, such as Panda70M (Chen et al., 2024), COCO Caption [(Lin et al., 2014)](#b19), and WebVid [Bain et al. (2021b)](#). However, the captions in these datasets are usually very short and fail to describe the video's content comprehensively.

Figure [7](#): The pipeline for dense video caption data generation. In this pipeline, we generate short video captions with the Panda70M model, extract frames to create dense image captions, and use GPT-4 to summarize these into final video captions. To accelerate this process, we fine-tuned a Llama 2 model with the GPT-4 summaries.

To generate high-quality video caption data, we establish a Dense Video Caption Data Generation pipeline, as detailed in Figure [7](#). The main idea is to generate video captions with the help of image captions.

First, we use the video caption model from Chen et al. ( [2024](#)) to generate short captions for the videos. Then, we employ the image recaptioning model CogVLM [(Wang et al., 2023)](#b32) used in CogView3 [(Zheng et al., 2024)](#b37) to create dense image captions for each frame. Subsequently, we use GPT-4 to summarize all the image captions to produce the final video caption. To accelerate the generation from image captions to video captions, we fine-tune a LLaMA2 [(Touvron et al., 2023)](#b29) using the summary data generated by GPT-4 [(Achiam et al., 2023)](#b0), enabling large-scale video caption data generation. Additional details regarding the video caption data generation process can be found in Appendix F.

To further accelerate video recaptioning, we also fine-tune an end-to-end video understanding model CogVLM2-Caption 1 , based on the CogVLM2-Video [(Hong et al., 2024)](#b15) and Llama3 (AI@Meta, 2024), by using the dense caption data generated from the aforementioned pipeline. Examples of video captions generated by this end-to-end CogVLM2-Caption model are shown in fig. [15](#) and Appendix G. CogVLM2-Caption can provide detailed descriptions of video content and object changes. Interestingly, we find that we can perform video-to-video generation by connecting CogVideoX and CogVLM2-Caption, as detailed in appendix H.

## Experiments

## Ablation Study

We conducted ablation studies on some of the designs mentioned in Section 2 to verify their effectiveness.  Position Embedding. We compared 3D RoPE with sinusoidal absolute position embedding. As shown in Figure [10a](#) indicates the loss curve of RoPE converges significantly faster than absolute one. This is consistent with the common choice in LLMs.

Expert Adaptive Layernorm. We compare three architectures in Figure [8a](#fig_5) and Figure 10c: MMDiT Esser et al. (2024), Expert AdaLN, without Expert AdaLN. Cross-attention DiT has been shown to be inferior to MMDiT in (), so we don't repeat. According to FVD and loss, expert AdaLN significantly outperforms the model without expert AdaLN and MMDiT with the same number of parameters. Moreover, the design of Expert AdaLN is more simplified than MMDiT and is closer to current LLMs, making it easier to scale up further.

3D Full Attention. In Figure [8b](#fig_5) and Figure [10b](#) , when we replace 3D full attention with 2D + 1D attention, we observe that the model is unstable and prone to collapse.

Explicit Uniform Sampling. From Figure [8c](#fig_5) and Figure [10d](#), we find that using Explicit Uniform Sampling can make a more stable decrease in loss and get a better performance.

4.2 Evaluation 4.2.1 Automated Metric Evaluation VAE Reconstruction Effect We compared our 3DVAE with other open-source 3DVAE on 256 × 256 resolution 17-frame videos, using the validation set of the WebVid(Bain et al., 1 The CogVLM2-Caption model weight is openly available at [https://huggingface.co/THUDM/cogvlm2-llama3-caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption). Other metrics, such as color, tend to give higher scores to simple, static videos, so we do not use them.

For longer-generated videos, some models might produce videos with minimal changes between frames to get higher scores, but these videos lack rich content. Therefore, metrics for evaluating the dynamism become important. To address this, we use two video evaluation tools: Dynamic Quality [(Liao et al., 2024)](#b17) and GPT4o-MTScore [(Yuan et al., 2024)](#b34).

Dynamic Quality is defined by the integration of various quality metrics with dynamic scores, mitigating biases arising from negative correlations between video dynamics and video quality. GPT4o-MTScore is a metric designed to measure the metamorphic amplitude of time-lapse videos using GPT-4o, such as those depicting physical, biological, and meteorological changes.

Results. Table [3](#tab_4) provides the performance comparison of CogVideoX and other models.

CogVideoX-5B achieves the best performance in five out of the seven metrics and shows competitive results in the remaining two metrics. These results demonstrate that the model not only excels in video generation quality but also outperforms previous models in handling various complex dynamic scenes. In addition, Figure [2](#fig_0) presents a radar chart that visually illustrates the performance advantages of CogVideoX.

## Human Evaluation

In addition to automated scoring mechanisms, we also establish a comprehensive human evaluation framework to assess the general capabilities of video generation models. Evaluators will score the generated videos on four aspects: Sensory Quality, Instruction Following, Physics Simulation, and Cover Quality, using three levels: 0, 0.5, or 1. Each level is defined by detailed guidelines. The specific details are provided in the Appendix I.

We compare Kling (2024.7), one of the best closed-source models, with CogVideoX-5B under this framework. The results shown in

Table 4 indicate that CogVideoX-5B wins the human preference over Kling across all aspects.     

## E Caption Upsampler

To ensure that text input distribution during inference is as close as possible to the distribution during training, similar to [(Betker et al., 2023)](#b4), we use a large language model to upsample the user's input during inference, making it more detailed and precise. Finetuned LLM can generate better prompts than zero/few-shot.

For image-to-video, we use the vision language model to upsample the prompt, such as GPT4V, CogVLM [(Wang et al., 2023)](#b32).

## Zero-shot prompt for Text Upsampler

You are part of a team of bots that create videos. You work with an assistant bot that will draw anything you say in square brackets. For example, outputting \" a beautiful morning in the woods with the sun peaking through the trees \" will trigger your partner bot to output a video of a forest morning, as described. You will be prompted by people looking to create detailed, amazing videos. The way to accomplish this is to take their short prompts and make them extremely detailed and descriptive.

There are a few rules to follow : You will only ever output a single video description per user request. When modifications are requested, you should not simply make the description longer. You should refactor the entire description to integrate the suggestions.

## F Dense Video Caption Data Generation

In the pipeline for generating video captions, we extract one frame every two seconds for image captioning. Ultimately, we collected 50,000 data points to fine-tune the summary model. Below is the prompt we used for summarization with GPT-4:

Prompt for GPT-4 Summary

We extracted several frames from this video and described each frame using an image understanding model, stored in the dictionary variable 'image_captions: Dict[str: str]'.

In 'image_captions', the key is the second at which the image appears in the video, and the value is a detailed description of the image at that moment. Please describe the content of this video in as much detail as possible, based on the information provided by 'image_captions', including the objects, scenery, animals, characters, and camera movements within the video. \n image_captions={new_captions}\n You should output your summary directly, and not mention variables like 'image_captions' in your response. Do not include '\\n' and the word 'video' in your response. Do not use introductory phrases such as: \"The video presents\", \"The video depicts\", \"This video showcases\", \"The video captures\" and so on.\n Please start the description with the video content directly, such as \"A man first sits in a chair, then stands up and walks to the kitchen....\"\n Do not use phrases like: \"as the video progressed\" and \"Throughout the video\".\n Please describe the content of the video and the changes that occur, in chronological order.\n Please keep the description of this video within 100 English words. 

## G Video Caption Example

## Caption Generated by Panda-70M

A close up of a white dragon with blue eyes.

## Caption Generated by CogVLM2-Caption

A meticulously crafted white dragon with a serene expression and piercing blue eyes is showcased in various close-ups, its scales detailed with golden highlights. Initially, the dragon's horns are curved back, but as time passes, they change to spiraling gold, adding elegance. The creature's demeanor shifts from tranquil majesty to a more formidable presence, with its mouth agape revealing sharp teeth at times. Its expressions alternate between calmness and fierce determination, maintaining an air of mythical serenity throughout. The consistent neutral background focuses attention on the dragon's textured features and expressive eyes.

## Caption Generated by Panda-70M

A crab is walking on the beach with a light bulb on its back.

## Caption Generated by CogVLM2-Caption

A hermit crab with a dark, glossy shell and reddish-brown legs is seen carrying an illuminated light bulb on its back across the sandy terrain of a beach at night. The scene transitions from a soft glow to a more pronounced illumination as the crab moves, highlighting its delicate limbs against the smooth sand and tranquil sea backdrop. This surreal tableau blends natural beauty with human ingenuity, creating a serene yet whimsical atmosphere that emphasizes the crab's unique adaptation and the contrast between nature and technology in this quiet nocturnal setting.

## Caption Generated by Panda-70M

A young black man is sitting on a cloud and reading a book with a blue sky in the background.

## Caption Generated by CogVLM2-Caption

A young Black man with an afro hairstyle and a neatly trimmed beard is seen sitting cross-legged on fluffy white clouds, deeply engrossed in reading a book with a red cover. He wears a plain white T-shirt and dark pants against a vivid blue sky dotted with cumulus clouds. Throughout the scenes, his expression remains one of deep concentration and peaceful contemplation, highlighting a moment of intellectual pursuit amidst nature's grandeur. The imagery suggests a serene atmosphere that emphasizes solitude and introspection, with no other people or objects around him.

## I Human Evaluation Details

One hundred meticulously crafted prompts are used for human evaluators, characterized by their broad distribution, clear articulation, and well-defined conceptual scope.

A panel of evaluators is instructed to assign scores for each detail on a scale from zero to one, with the overall total score rated on a scale from 0 to 5, where higher scores reflect better video quality.

To better complement automated evaluation, human evaluation emphasizes the instructionfollowing capability: the total score cannot exceed 2 if the generated video fails to follow the instructions.

Sensory Quality: This part focuses mainly on the perceptual quality of videos, including subject consistency, frame continuity, and stability. Instruction Following: This part focuses on whether the generated video aligns with the prompt, including the accuracy of the subject, quantity, elements, and details. Physics Simulation: This part focuses on whether the model can adhere to the objective law of the physical world, such as the lighting effect, interactions between different objects, and the realism of fluid dynamics.

![Figure 2: The performance of openly-accessible text-to-video models in different aspects.]()

![Figure 4: (a) The structure of the 3D VAE in CogVideoX. It comprises an encoder, a decoder and a latent space regularizer, achieving a 8×8×4 compression from pixels to the latents. (b) The context parallel implementation on the temporally causal convolution.]()

![Figure 4 (a) shows the structure of the proposed 3D VAE. It comprises an encoder, a decoder and a latent space regularizer, Kullback-Leibler (KL) regularizer. The encoder and decoder consist of symmetrically arranged stages, respectively performing 2× downsampling and upsampling by the interleaving of ResNet block stacked stages. Some blocks perform 3D downsampling (upsampling), while others only perform 2D downsampling (upsampling), depending on the setting.]()

![Figure 6: The diagram of mixed-duration training and Frame Pack. To fully utilize the data and enhance the model's generalization capability, we train on videos of different duration within the same batch.]()

![Figure 8: Ablation studies on WebVid test dataset with 500 videos. MMDiT1 has the same number of parameters with the expert AdaLN. MMDiT2 has the same number of layers but twice number of parameters.]()

![Figure 11: Text to video showcases. The displayed prompt will be upsampled before being fed into the model. The generated videos contain large motion and various styles.]()

![Figure 12: Text to video showcases.]()

![Figure 13: Image to video showcases. The displayed prompt will be upsampled before being fed into the model.]()

![Figure 14: Image to video showcases.]()

![Figure 15: An example from CogVLM2-Caption provides a detailed description of all specific objects and movements.]()

![]()

![]()

![]()

![]()

![]()

![]()

![Ablation with different variants of 3D VAE. The baseline is SDXL(Podell et al., 2023) 2D VAE. Flickering calculates the L1 difference between each pair of adjacent frames to evaluate the degree of flickering in the video. We use variant B for pretraining.]()

![Evaluation results of CogVideoX-5B and CogVideoX-2B.On table 2, our VAE achieved the best PSNR and exhibited the least jitter. Notably, other VAE methods use fewer latent channels than ours.]()

![Comparison with the performance of other spatiotemporal compression VAEs.]()

![Human evaluation between CogVideoX and Kling.In this paper, we present CogVideoX, a state-of-the-art text-to-video diffusion model. It leverages a 3D VAE and an Expert Transformer architecture to generate coherent long duration videos with significant motion. We are also exploring the scaling laws of video generation models and aim to train larger and more powerful models to generate longer and higher-quality videos, pushing the boundaries of what is achievable in text-to-video generation.]()

![Sensory Quality Evaluation Criteria. High sensory quality: 1. The appearance and morphological features of objects in the video are completely consistent 2. High picture stability, maintaining high resolution consistently 3. Overall composition/color/boundaries match reality 4. The picture is visually appealing 0.5 Average sensory quality: 1. The appearance and morphological features of objects in the video are at least 80% consistent 2. Moderate picture stability, with only 50% of the frames maintaining high resolution 3. Overall composition/color/boundaries match reality by at least 70% 4. The picture has some visual appeal 0 Poor sensory quality: large inconsistencies in appearance and morphology, low video resolution, and composition/layout not matching reality]()

![Instruction Following Evaluation Criteria.100% follow the text instruction requirements, but the implementation has minor flaws such as distorted main subjects or inaccurate features.0Does not 100% follow the text instruction requirements, with any of the following issues: 1. Generated elements are inaccurate 2. Quantity is incorrect 3. Elements are incomplete 4. Features are inaccurate]()

