{
  "arxivId": "2409.19606",
  "title": "Hyper-Connections",
  "authors": "Defa Zhu, Hongzhi Huang, Zihao Huang, Yutao Zeng, Yunyao Mao, Banggu Wu, Qiyang Min, Xun Zhou",
  "abstract": "We present hyper-connections, a simple yet effective method that can serve as\nan alternative to residual connections. This approach specifically addresses\ncommon drawbacks observed in residual connection variants, such as the seesaw\neffect between gradient vanishing and representation collapse. Theoretically,\nhyper-connections allow the network to adjust the strength of connections\nbetween features at different depths and dynamically rearrange layers. We\nconduct experiments focusing on the pre-training of large language models,\nincluding dense and sparse models, where hyper-connections show significant\nperformance improvements over residual connections. Additional experiments\nconducted on vision tasks also demonstrate similar improvements. We anticipate\nthat this method will be broadly applicable and beneficial across a wide range\nof AI problems.",
  "url": "https://arxiv.org/abs/2409.19606",
  "issue_number": 718,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/718",
  "created_at": "2025-01-04T06:52:21.616669",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2025-01-02T08:05:54.690Z",
  "main_tex_file": null,
  "published_date": "2024-09-29T07:57:07Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.CL",
    "cs.CV",
    "cs.NE"
  ]
}