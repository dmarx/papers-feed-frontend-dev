- Decision to propose hyper-connections as an alternative to residual connections
- Choice of using learnable depth-connections and width-connections
- Decision to implement both static and dynamic hyper-connections
- Selection of Pre-Norm as a baseline for comparison
- Decision to conduct experiments on both language and vision tasks
- Choice of using a specific initialization strategy for hyper-connections
- Decision to analyze the seesaw effect between gradient vanishing and representation collapse
- Choice of metrics for evaluating model performance (e.g., convergence speed, accuracy)
- Decision to visualize the impact of hyper-connections on feature similarity
- Choice of expansion rate (n) for hyper-connections
- Decision to explore sequential-parallel duality in layer arrangements
- Choice of normalization techniques before and after linear transformations in dynamic hyper-connections
- Decision to use PyTorch for implementation of hyper-connections
- Choice of datasets for training and validation (e.g., C4-en, HellaSwag, ARC-Challenge)
- Decision to analyze the computational cost and parameter increase of hyper-connections
- Choice of activation functions for dynamic parameter computation (e.g., tanh)
- Decision to document the theoretical foundations of hyper-connections in relation to existing methods