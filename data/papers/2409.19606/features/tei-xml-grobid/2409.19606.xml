<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HYPER-CONNECTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-28">28 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Defa</forename><surname>Zhu</surname></persName>
							<email>zhudefa@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Huang</surname></persName>
							<email>huanghongzhi.51@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutao</forename><surname>Zeng</surname></persName>
							<email>yutao.zeng@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunyao</forename><surname>Mao</surname></persName>
							<email>maoyunyao.myy@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Banggu</forename><surname>Wu</surname></persName>
							<email>wubanggu@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiyang</forename><surname>Min</surname></persName>
							<email>minqiyang@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Zhou</surname></persName>
							<email>zhouxun@bytedance.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Seed-Foundation-Model Team</orgName>
								<address>
									<country>ByteDance</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HYPER-CONNECTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-28">28 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">12D842837F5304FCED40B1CED012C00C</idno>
					<idno type="arXiv">arXiv:2409.19606v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyperconnections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(1) and (2) show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens. (3) and (4) show the accuracy curves on HellaSwag and ARC-Challenge, demonstrating the superior performance of the OLMoE-1B-7B-DHC×4 model.</p><p>Deep learning has achieved tremendous success across various domains, where residual connections <ref type="bibr" target="#b13">(He et al., 2016)</ref> have been instrumental in contemporary neural network architectures, including transformers and CNNs. Residual connections help mitigate the problem of gradient vanishing, enabling the effective training of very deep networks. However, it is important to acknowledge that residual connections are not infallible solutions and still present limitations that remain unresolved.</p><p>The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing <ref type="bibr" target="#b2">(Bengio et al., 1994;</ref><ref type="bibr" target="#b11">Glorot &amp; Bengio, 2010)</ref>. However, it can also lead to the issue of collapse in deep representations <ref type="bibr" target="#b18">(Liu et al., 2020)</ref>, where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers. This approach can alleviate the issue of representation collapse but  (b) Hyper-connections: β 1 , β 2 , α 0,0 , α 0,1 , α 1,0 , α 1,1 , α 2,1 , and α 2,2 are learnable scalars or scalars predicted by the network , depending on the specific HC version. These connections enable lateral information exchange and vertical integration of features across depths. The Transformer with HC is shown in Fig. <ref type="figure" target="#fig_6">17</ref>. They can be decoupled into depth-connections and width-connections. (c) Depth-connections perform a weighted sum between the layer output and the hidden vector h 1 . (d) Width-connections allow information exchange between the hidden vectors h 1 and h 2 . also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer. Layer Index i Figure <ref type="figure">3</ref>: Cosine similarity between the input of the current layer and the previous layers for the OLMo-1B Model <ref type="bibr" target="#b12">(Groeneveld et al., 2024)</ref>. The curve represents the median of similarity, while the shaded area indicates the range between the 5th and 95th percentiles. The red curve shows the model with Pre-Norm, and the blue curve shows that with hyper-connections.</p><p>Driven by the limitations of residual connections, an important question arises: Can neural networks autonomously learn the optimal strength of connections to improve performance? To address this, we propose hyper-connections (HC), which lead to significantly improved performance with a negligible increase in computation and parameters. We will show that both Post-Norm and Pre-Norm variants can be expressed as specific non-trainable forms of hyper-connections, as discussed in § 3.1.</p><p>The core idea of hyper-connections (HC) is to propose learnable depth-connections and width-connections, as depicted in Fig. <ref type="figure" target="#fig_2">2 (b</ref>). These connections flexibly integrate features vertically across depths, compared to the residual connections shown in Fig. <ref type="figure" target="#fig_2">2 (a)</ref>. Depth-connections can be considered as a generalized residual connections, assigning weights to the connections between the inputs and outputs of each layer. To enable the network to model different depth-connections simultaneously, we expand the network's input into n copies, each having its own depth connection, as shown in Fig. <ref type="figure" target="#fig_2">2 (b</ref>). This design allows multiple hidden vectors to reserve multiple patterns connecting preceding layers, as shown in § 4.5. Moreover, we establish width connections between the n hidden vectors, allowing information exchange between hidden vectors within the same layer, as shown in Fig. <ref type="figure" target="#fig_2">2 (b</ref>). We argue that n (&gt; 1) hidden states are necessary. As analyzed in Appendix F, the seesaw effect persists when n = 1, and experiments show that it does not improve performance, as shown in Fig. <ref type="figure">5</ref>. In contrast, when n &gt; 1, hyper-connections can not only learn to adjust the strength of residuals but also rearrange layers, either sequentially or in parallel, as discussed in § 3.2. To further enhance flexibility, we introduce dynamic hyper-connections (DHC), enabling the network to adjust connection weights according to the input. Notably, although HC seem to increase the network's width by n times, the additional parameters and computational cost are almost negligible, as analyzed in Appendix B. The Transformer with HC is shown in Fig. <ref type="figure" target="#fig_6">17</ref>.</p><p>Our research, primarily centered on large language models (LLMs) pre-training, also extends to visual generation and classification tasks. Using Pre-Norm as a baseline, we demonstrate the significant Preprint benefits of hyper-connections, including 1B and 7B dense models as well as 7B MoE models, as detailed in § 4. The benefits are particularly prominent for OLMoE <ref type="bibr" target="#b22">(Muennighoff et al., 2024)</ref> as presented in Fig. <ref type="figure" target="#fig_0">1</ref>. The model utilizing DHC converges 1.8 times faster and shows an improvement of 6 points on ARC-Challenge compared to the baseline trained with 500 B tokens. According to our visualization analysis, as shown in Fig. <ref type="figure">3</ref>, the baseline model tends toward representation collapse, characterized by high similarity between features of adjacent layers. In contrast, models with HC exhibit significantly lower similarity between features across adjacent layers and a wider range of similarities. This suggests that HC enhance the impact of each layer. Further discussion is provided in §4.5 and in Appendix F. These compelling pieces of evidence demonstrate the generality of the hyper-connections principle, and we anticipate their applicability in numerous other AI challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHOD</head><p>2.1 STATIC HYPER-CONNECTIONS Consider the hidden vector h k-1 ∈ R d (or h k-1 ∈ R d×1 ) as the input to the k-th layer, with the initial input h 0 to the network. Initially, h 0 ∈ R d is replicated n times to form the initial hyper hidden matrix H 0 = h 0 h 0 . . . h 0 ⊺ ∈ R n×d . Here, n is the expansion rate. For the k-th layer, the input consists of the hyper hidden matrix from the previous layer</p><formula xml:id="formula_0">H k-1 = h k-1 1 h k-1 2 . . . h k-1 n ⊺ ∈ R n×d .</formula><p>Finally, we sum the last hyper hidden matrix row-wise to obtain the required hidden vector, which is then passed through a final projector to produce the final output of the network (i.e., a normalization layer and an unembedding layer in transformers). To simplify the notation in subsequent analysis, we omit the layer index and simply denote the hyper-hidden matrix as</p><formula xml:id="formula_1">H = (h 1 h 2 . . . h n ) ⊺ .</formula><p>The hyper-connections (HC) can be represented by a matrix HC, where each element defines the connection weight. The matrix is structured as follows: n+1) .</p><formula xml:id="formula_2">HC = 0 1×1 B A m A r =       0 β 1 β 2 • • • β n α 1,0 α 1,1 α 1,2 • • • α 1,n α 2,0 α 2,1 α 2,2 • • • α 2,n . . . . . . . . . . . . . . . α n,0 α n,1 α n,2 • • • α n,n       ∈ R (n+1)×(</formula><p>(1)</p><p>Consider a network layer T , it integrates self-attention layers and feed-forward networks within transformers. The output of the HC, denoted by Ĥ, can be simply formulated as follows:</p><formula xml:id="formula_3">Ĥ = HC(T , H) = B ⊺ T (H ⊺ A m ) ⊺ + A r ⊺ H.<label>(2)</label></formula><p>We use A m as weights to perform a weighted sum on the input H = (h 1 h 2 . . . h n ) ⊺ to obtain the input h 0 of the current layer T , which is given by:</p><formula xml:id="formula_4">h ⊺ 0 = A m ⊺ H,<label>(3)</label></formula><p>While A r is used to connect H and map it to a hyper hidden matrix H ′ , as shown below:</p><formula xml:id="formula_5">H ′ = A r ⊺ H. (4)</formula><p>Subsequently, the output is given by:</p><formula xml:id="formula_6">Ĥ = B ⊺ (T h 0 ) ⊺ + H ′ .<label>(5)</label></formula><p>The depth-connections can be decoupled as the following matrix, which is shown at Fig <ref type="figure" target="#fig_2">2 (a)</ref>:</p><formula xml:id="formula_7">DC = B diag(A r ) = β 1 β 2 • • • β n α 1,1 α 2,2 • • • α n,n ∈ R 2×n ,<label>(6)</label></formula><p>where the first row B represents the weights of the output of the current layer T , and the last row diag(A r ) represents the weights of the input. We use diag(A r ) to represent the flatten vector of the diagonal entries of A r .</p><p>The width-connections matrix can be defined as follows, which is shown at Fig</p><formula xml:id="formula_8">2 (b): WC = (A m A r ) ∈ R n×(n+1) .<label>(7)</label></formula><p>The algorithm that employs hyper-connections is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DYNAMIC HYPER-CONNECTIONS</head><p>The entries of HC can dynamically depend on the input H, which the matrix representation of dynamic hyper-connections (DHC) is defined as follows:</p><formula xml:id="formula_9">HC(H) = 0 1×1 B(H) A m (H) A r (H)<label>(8)</label></formula><p>Similarly, given a layer T and input H, we obtain the output of the DHC as follows:</p><formula xml:id="formula_10">Ĥ = HC(H)(T , H).<label>(9)</label></formula><p>In practice, we combine the dynamic and static matrices to achieve DHC. The dynamic parameters are obtained through a linear transformation. To stabilize the training process, we introduce normalization before the linear transformation and apply the tanh activation function after it, scaling it by a small initial learnable factor. The following equations detail how these dynamic parameters are computed:</p><formula xml:id="formula_11">H = norm(H)<label>(10)</label></formula><formula xml:id="formula_12">B(H) = s β • tanh(HW β ) ⊺ + B ∈ R 1×n (11) A m (H) = s α • tanh(HW m ) + A m ∈ R n×1 (12) A r (H) = s α • tanh(HW r ) + A r ∈ R n×n (13)</formula><p>Our experiments in § 4 demonstrate that dynamic hyper-connections outperform static hyperconnections in language modeling tasks. The PyTorch implementations for both the static and dynamic variants of hyper-connections are detailed in Algorithm 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">INITIALIZATION</head><p>In order to make the initialization of the hyper-connections equivalent to the Pre-Norm residual connections, we adopt the following initialization strategy. The dynamic parameters W β , W m , and W r in Eqs. 11, 12, and 13 are initialized to 0, while the static matrices are initialized as follows:</p><formula xml:id="formula_13">0 1×1 B k A m k A r k = 0 1×1 1 1×n e k mod n e n×n , (<label>14</label></formula><formula xml:id="formula_14">)</formula><p>where k is the index of the layer. mod denotes the modulo operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WHY HYPER-CONNECTIONS</head><p>In this section, we elucidate the rationale behind hyper-connections. We explore how variants of residual connections, namely Pre-Norm and Post-Norm, can be viewed as non-trainable hyperconnections, and introduce the concept of sequential-parallel duality, demonstrating how hyperconnections can dynamically optimize layer arrangements to enhance network performance. A visulize analysis of hyper-connections through an unfolded view is discussed in § 4.5. Preprint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RESIDUAL CONNECTIONS AS NON-TRAINABLE HYPER-CONNECTIONS</head><p>The Pre-Norm and Post-Norm residual connections can be represented as the following hyperconnections matrices with an expansion rate n = 1:</p><formula xml:id="formula_15">HC P reN orm = 0 1 1 1 , (<label>15</label></formula><formula xml:id="formula_16">) HC P ostN orm =   0 1 √ σ 2 i +σ 2 o +2σio 1 1 √ σ 2 i +σ 2 o +2σio   , (<label>16</label></formula><formula xml:id="formula_17">)</formula><p>where σ i and σ o denote the standard deviations of the input and output of the neural network layer, respectively, and σ io is the covariance between them.</p><p>For Pre-Norm, its hyper-connection matrix is a 2 × 2 matrix where the bottom right triangular part is filled with 1 and the rest is a placeholder 0. For Post-Norm, the weights depend on the variances and covariance of the input and output, forming a 2 × 2 matrix. Therefore, their hyper-connection matrices are non-trainable. In this work, we propose hyper-connections that can be (n + 1) × (n + 1) matrices, with weights that are trainable or even predicted based on the input. The complete derivation is provided in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SEQUENTIAL-PARALLEL DUALITY</head><p>Given a series of neural network modules, we have the option to arrange them either sequentially or in parallel. However, hyper-connections offer an approach that learns to rearrange these layers in a configuration blending both sequential and parallel arrangements. Without loss of generality, we set the expansion rate to n = 2. If the hyper-connections are learned as the following matrix, the neural network will be arranged sequentially:</p><formula xml:id="formula_18">HC = 0 1 1 1 1 0 0 0 1 . (<label>17</label></formula><formula xml:id="formula_19">)</formula><p>In this case, the depth connection degenerates into a residual connection, as shown in Fig. <ref type="figure" target="#fig_4">4 (a)</ref>.</p><p>When the hyper-connections for odd and even layers (with layer numbering starting from 1) are defined by the following matrices, the neural network will be arranged in parallel every two consecutive layers, similar to the arrangement of parallel transformer blocks in transformers <ref type="bibr" target="#b32">(Wang, 2021)</ref>, as shown in <ref type="bibr">Fig. 4 (b)</ref>. The general and complete derivation is provided in Appendix H.</p><p>Preprint</p><formula xml:id="formula_20">HC odd = 0 1 0 1 1 1 1 1 1 , (18) HC even = 0 0 1 0 1 0 1 0 1 . (<label>19</label></formula><formula xml:id="formula_21">)</formula><p>Thus, learning the hyper-connection matrix in various forms can create layer arrangements that surpass traditional sequential and parallel configurations, resulting in a soft-mixture or even dynamic arrangement. For static hyper-connections, the layer arrangement within the network remains fixed after training. In contrast, dynamic hyper-connections allow the arrangement to adapt dynamically for each token. Experiment Settings. We employ the experimental setup outlined by OLMo <ref type="bibr" target="#b12">(Groeneveld et al., 2024)</ref> for dense models and by OLMoE <ref type="bibr" target="#b22">(Muennighoff et al., 2024)</ref> for MoE models. For dense models, we use dolmap-v1.5-sample <ref type="bibr" target="#b29">(Soldaini et al., 2024)</ref> as our training dataset. We conduct ablation studies on 1B models and assess the effectiveness of our method at the 7B model scale. For MoE models, we train the OLMoE-1B-7B model, both with and without hyper-connections, on the OLMOE-MIX dataset. These models activate 1.3B out of a total of 7B parameters. All experiments are trained on 500B tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Implementation. We maintain the training configuration of the baseline model, replacing the residual connections with hyper-connections. The static component in <ref type="bibr">Eqs. 1,</ref><ref type="bibr">11,</ref><ref type="bibr">12,</ref><ref type="bibr">13</ref> does not utilize weight decay, whereas the dynamic component does. Since the hyper hidden vectors of the final transformer block are ultimately summed, we ensure that the standard deviation (std) of the output (before the final layernorm and unembedding layers) remains consistent with the original. At initialization, we scale the std of the weights of the output module at all layers, including those of the second linear layer of the feedforward network and the output projector of the attention module, by a factor of √ n, where n represents the expansion rate. The parameters and computational overhead introduced by hyper-connections is negligible, see Table <ref type="table" target="#tab_6">. 7</ref> and <ref type="table" target="#tab_7">8.</ref> Metrics. In accordance with the methodology of OLMo <ref type="bibr" target="#b12">(Groeneveld et al., 2024)</ref>, we report the average perplexities (PPL) and losses on both the V2 and V3 validation sets, along with the average metrics for zero-shot evaluation on downstream benchmarks (refer to Table <ref type="table" target="#tab_1">12</ref>). We observe significant volatility in the zero-shot performance indicators for the datasets (highlighted in grey in Table <ref type="table" target="#tab_1">12</ref>), with fluctuations exceeding 20% across neighboring checkpoints. For more reliable and consistent results, we excludes these volatile datasets from our analysis. For the MoE models, in line with OLMoE, we also present losses on V3 validation sets, and accuracies on downstream benchmarks (refer to Table <ref type="table" target="#tab_15">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ABLATION STUDY</head><p>We use the dynamic hyperconnections with an expansion rate of n = 4 and include the tanh function as the default method, marked with the suffix -DHC, while -SHC denotes static hyper-connections.</p><p>The evaluation results are presented in Table <ref type="table" target="#tab_0">1</ref>, and the training loss curves are depicted in Fig. <ref type="figure">5</ref>. We observe that with an expansion rate of n = 1, the performance of DHC is inferior to the baseline. However, for n &gt; 1, DHC significantly outperforms the baseline, achieving superior results at n = 4, with the increase to n = 8 providing minimal additional benefits. Notably, OLMo-1B-DHC×8 W/O tanh excels on both V2 and V3 validation sets, with a reduction in V2 Eval Loss by 0.034 and V3 Eval Loss by 0.029 compared to the baseline. Furthermore, the decline rate of training losses for DHC (n ≥ 2) is steeper than that of the baseline, and DHC demonstrates greater stability, with no spikes observed in any DHC experiments.</p><p>Static and dynamic hyper-connections. Table <ref type="table" target="#tab_1">2</ref> presents an ablation study comparing SHC and DHC. All hyper-connection (HC) variants significantly outperform the baseline. At an expansion rate of 2, the improvements of DHC and SHC are similar. However, at an expansion rate of 4, DHC performs notably better than SHC. The importance of B and WC. As shown in Table <ref type="table">3</ref>, not training WC leads to significant performance declines, with the V2 loss increasing by 0.021 and the V3 loss by 0.017, as seen when comparing the 4th and 6th lines of Table <ref type="table">3</ref>. In contrast, the impact is less pronounced when B is not trained. Therefore, ensuring the trainability of both WC and B is crucial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISON WITH RELATED WORKS</head><p>We implemented the Altup <ref type="bibr" target="#b1">(Baykal et al., 2024)</ref> and ResiDual <ref type="bibr" target="#b34">(Xie et al., 2023)</ref> methods in OLMo.</p><p>Altup is motivated to widen the hidden dimension while maintaining low computation cost by passing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>Table <ref type="table">3</ref>: Ablation study on OLMo-1B-DHC×4. In the B or WC column, the symbol "✗" denotes parameters that are not trainable from initialization. </p><formula xml:id="formula_22">WC B Tanh V2 Eval Loss ↓ V2 Eval PPL ↓ V3 Eval Loss ↓ V3 Eval PPL ↓ Down Stream Avg, Acc. ↑ ✗ ✓ ✗ 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SciQ Acc.</head><p>OLMo-7B OLMo-7B-DHCx4 We evaluate the effectiveness of hyper-connections on the 7B model, training a model with DHCs with an expansion rate of 4, denoted as OLMo-7B-DHC×4. According to Table <ref type="table" target="#tab_3">5</ref>, OLMo-7B-DHC×4 significantly outperforms the baseline OLMo-7B model in all average metrics. In the V2 evaluation, OLMo-7B-DHC×4 shows improvements of 0.022 for loss and 0.293 for PPL. Furthermore, the average score of downstream benchmarks 0.710 surpasses the baseline 0.701, with the results of specific tasks shown in Fig. <ref type="figure" target="#fig_9">10</ref>.</p><p>Based on Fig 6, the OLMo-7B-DHC×4 model consistently shows better metrics compared to baseline, including training and validation loss and accuracy in downstream benchmarks. Notably, after 400 B tokens, the model maintains its improvement without the gains diminishing. This indicates that the OLMo-7B-DHC×4 model continues to provide consistent benefits in reducing loss, even at higher token counts. Furthermore, according to Fig. <ref type="figure" target="#fig_5">6</ref>, the baseline model exhibits frequent spikes, while Preprint </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">MOE MODELS</head><p>We evaluate the effectiveness of hyper-connections on the Mixture-of-Experts (MoE) model. We retrain the original OLMoE-1B-7B model as the baseline and train a model that applies Dynamic Hyper-Connections (DHC) with n = 4, replacing the residual connections. The full results are shown in Fig. <ref type="figure" target="#fig_8">9</ref>, which illustrates that hyper-connections outperform residual connections in almost all metrics. In many metrics, our method requires only half of the training tokens to achieve the same performance as the baseline. Fig. <ref type="figure" target="#fig_0">1</ref> and Table <ref type="table">6</ref> highlight some of the results, such as a reduction in training loss of approximately 0.027, a reduction in loss on the C4-en validation set of 0.028, an improvement of 6 points on the ARC-Challengeand an improvement of 1.2 points on MMLU Var.</p><p>Table 6: Downstream evaluations for MoE models training with 500B tokens under the OLMoE evaluation setting. ARC-C stands for ARC-Challenge, and ARC-E for ARC-Easy. MMLU Var is a modified version of MMLU that includes varying few-shot examples, providing stable feedback during early training, as outlined in the OLMoE setting (Muennighoff et al., 2024). Methods MMLU Var Hella-Swag ARC-C ARC-E PIQA Wino-Grande BoolQ OLMoE-1B-7B 38.5 69.5 41.8 72.8 77.6 64.4 65.4 OLMoE-1B-7B-DHC×4 39.7 70.2 47.8 76.7 78.2 64.6 68.5 4.5 VISUALIZATION ANALYSIS 0 4 8 12 16 20 24 28 32 Hyper-Connection 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Post-Norm 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Pre-Norm 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Pre-Norm PTB 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Two-hop Residual In this section, we investigate the learned hyper-connection weights and show how the output of the former layer contributes to the latter ones. To this end, we convert hyper-connections to dense connections cross layers. Consider the input hidden vectors h k 0 in k-th layer, it can be unfolded as a weighted summation over previous layer outputs:</p><formula xml:id="formula_23">h k 0 = k-1 j=0 c (0) kj T j (h j 0 ),<label>(20)</label></formula><p>where c</p><p>(0) kj describes how much layer-j (T j ) contributes to layer-k's input h k 0 . Then, C (0) denotes a dense connection weight matrix. In particular, let layer-0 be the word embedding and T 0 be an Preprint identity mapping, layer-L+1 be the hidden state before the unembedding layer, which is a summation over the last hidden vectors, i.e., h L+1 0 = j h L j . OLMo-1B-DHC×4 model is adopted for visualization. We take the checkpoint at 500B tokens and forward random validation text to obtain dynamic hyper-connection weights. In addition, we show connection patterns for some related baseline methods. Finally, the visualization is illustrated in Fig. <ref type="figure" target="#fig_13">13</ref>. We present the following findings, with more detailed discussions provided in Appendix F.</p><p>Connection patterns for baseline methods. For Pre-Norm baseline, the connection matrix is simply a lower triangular matrix with diagonal elements erased, because each transformer layer joins the residual equally. In the Pre-Norm parallel transformer block (PTB) baseline, the connection matrix appears jagged because the input to the FFN layer does not depend on the output of the previous attention layer. For Post-Norm baseline, the connection only holds for adjacent layers, as the weight for bottom layers decays every time the residual passes a post-norm layer. For the two-hop residual baseline <ref type="bibr" target="#b20">(Ma et al., 2024)</ref>, the outputs of attention layers are not added to residual and only contributes to the next one FFN layer, resulting in a vertical strip pattern in the connection matrix.</p><p>Λ-shaped connection pattern. In the connection matrix for hyper-connections, a long-term decay pattern can be observed, where layers are generally preferred to rely on a few adjacent layer outputs. Moreover, the bottom layers (e.g. layer 0,2) are observed frequently used in most of subsequent layers. Therefore, the two patterns together form a Λ-shaped connection pattern. Note that the long-term decay pattern is a Post-Norm style pattern, while the frequently accessed pattern is Pre-Norm style, indicating that the hyper-connection introduces a free mixture of Pre-and Post-Norm architecture.</p><p>Input word embedding is eliminated from model output. As per the first column in the connection matrix for layer inputs, the input word embedding contributes to most of the layers except for the final one. This last layer, which products the model's output, is used for next token prediction. In most cases, keeping a component of input embedding in model output is harmful to next token prediction, especially when using a tied word embedding such as that employed by OLMo-1B. Similar results are found in previous works <ref type="bibr" target="#b19">(Ma et al., 2023)</ref>.</p><p>Parallel transformer blocks are observed. As discussed in § 3.2, parallel transformer block, which performs attention and FFN in parallel, is a special case for hyper-connection. In practice, PTB-like patterns, which can be identified by the local jagged pattern, are surprisingly observed to be learned by hyper-connections. For instance, layer 11 has a minimal contribution to the input of layer 12 (refer to row 12 in the hyper-connection connection matrix). This suggests that layers 11 and 12 can operate in parallel, thereby forming a PTB module.</p><p>Attention layers tend to have fewer long-term connections. It is observed that attention layers at the bottom barely have long-term contribution, a trend that persists until layer 17. Upon examining the connection matrix for hyper hiddens (refer to Fig. <ref type="figure" target="#fig_13">13</ref> in the appendix), it's evident that the outputs of the FFN layers have significantly greater magnitudes than those of the attention layers. This pattern resembles a two-hop residual connection design, wherein the attention output contributes to the input of the following FFN layer, but doesn't join the main residual path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> have revolutionized various fields, particularly natural language processing and computer vision. They rely heavily on residual connections to facilitate the training of deep models. Our hyper-connections approach can replace residual connections, providing stable training and consistent improvements in both natural language processing and computer vision.</p><p>The issues of gradient vanishing and representation collapse <ref type="bibr" target="#b2">(Bengio et al., 1994;</ref><ref type="bibr" target="#b11">Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b18">Liu et al., 2020)</ref> have been extensively studied. The combinations of normalization techniques <ref type="bibr" target="#b15">(Ioffe &amp; Szegedy, 2015;</ref><ref type="bibr" target="#b0">Ba et al., 2016)</ref> and residual connections <ref type="bibr" target="#b13">(He et al., 2016)</ref>, like Pre-Norm and Post-Norm, actually reflects different emphases in solving these two issues. However, despite these advancements, the fundamental trade-off between gradient vanishing and representation collapse in deep networks remains a critical challenge. Building on these findings, our work introduces a novel approach that enables neural networks to autonomously learn the optimal strength of connections, potentially improving both gradient stability and representation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In conclusion, we have introduced hyper-connections as an effective alternative to residual connections in transformers. Our analysis reveals that hyper-connections not only overcome the limitations of residuals but also enable dynamic adjustments in network architecture. Experimental results confirm their promising benefits across various tasks, including pre-training of large language model, image generation, and image classification. </p><formula xml:id="formula_24">FFN + + + + 𝛼 ",! " 𝛽 " " h " ! 𝛼 $,! " 𝛼 "," " 𝛼 ",$ " 𝛼 ",$ " 𝛼 $,$ " 𝛽 $ " h $ ! 𝛼 ",! $ 𝛽 " $ h " " 𝛼 $,! $ 𝛼 "," $ 𝛼 ",$ $ 𝛼 ",$ $ 𝛼 $,$ $ 𝛽 $ $ h $ " 𝛼 ",! % 𝛽 " % h " $ 𝛼 $,! % 𝛼 "," % 𝛼 ",$ % 𝛼 ",$ % 𝛼 $,$ % 𝛽 $ % h $ $ h $ % h " ' h $ ( ℎ ' ℎ ! ℎ '</formula><p>Figure <ref type="figure">8</ref>: Comparison between transformers with hyper-connections and that with residual connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint B PARAMETERS, COMPUTATION AND MEMORY FOOTPRINT ANALYSIS</head><p>Static Hyper-Connections. All learnable parameters are included in the hyper-connection matrix HC in Eq. 1. The number of parameters in one HC is given by:</p><formula xml:id="formula_25">|θ SHC | = |θ B | + |θ A | = n + n • (n + 1) = n • (n + 2), (<label>21</label></formula><formula xml:id="formula_26">)</formula><p>where n is the expansion rate, |θ B | is the number of parameters in B in SHC, and |θ A | is the number of parameters in A. Each layer contains two hyper-connection modules (one for the self attention and one for the feedforward network). Thus, the number of extra parameters is:</p><formula xml:id="formula_27">P extra = |θ SHC | × 2 × L, (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>where L is the number of layers. For example, in OLMo-1B-SHC×4, P extra = 4×(4+2)×2×16 = 768.</p><p>Dynamic Hyper-Connections. The parameters of DHC are defined in Eqs. 10, 11, 12, and 13, and the number of parameters is given by:</p><formula xml:id="formula_29">|θ DHC | = |θ norm | + |s β | + |θ W β | + |θ B | + |s α | + |θ Wm | + |θ Am | + |θ Wr | + |θ Ar | (23) = |θ norm | + 1 + d model + n + 1 + d model + n + d model × n + n × n (24) = |θ norm | + d model × (n + 2) + n × (n + 2) + 2, (<label>25</label></formula><formula xml:id="formula_30">)</formula><p>where d model is the dimension of the hidden states in the transformer, and |θ norm | depends on the type of normalization module. In OLMo models, there are no parameters for normalization, so |θ norm | = 0. In OLMoE, |θ norm | = d model . Similar to the static hyper-connections, the number of extra parameters is:</p><formula xml:id="formula_31">P extra = |θ DHC | × 2 × L,<label>(26)</label></formula><p>For example, for OLMo-1B-DHC×4, P extra = (0 + 2048</p><formula xml:id="formula_32">× (4 + 2) + 4 × (4 + 2) + 2) × 2 × 16 = 394, 048.</formula><p>The number of parameters for DHC and SHC used in the experiments is detailed in Table <ref type="table" target="#tab_6">7</ref>, while their corresponding FLOPs comparisons are provided in Table <ref type="table" target="#tab_7">8</ref>. Regardless of whether SHC or DHC is used, the additional parameters and computational overhead introduced are minimal and can be considered negligible.  2.5 2.6 2.7 c4 en val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.6 2.7 2.8 2.9 dolma books val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) 2.5 2.6 2.7 dolma cc val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.1 2.2 2.3 dolma pes2o val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.8 2.9 3.0 dolma reddit val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) 0.95 1.00 1.05 dolma stack val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.3 2.4 2.5 dolma wiki val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.5 2.6 2.7 2.8 ice val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint E VISION EXPERIMENTS</head><p>Datasets. We use the ILSVRC-2012 ImageNet dataset <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> with 1k classes and 1.3M images (see ImageNet in the following) for image generation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 IMAGE GENERATION</head><p>To investigate the generalizability of hyper-connections in image generation, our experiments are conducted using the DiT framework <ref type="bibr" target="#b23">(Peebles &amp; Xie, 2022)</ref> training the models for 1400 epochs.</p><p>In order to save experimental costs, we use FP16 precision, introduce flash-attention to speed up training, and introduce QK-Norm <ref type="bibr" target="#b33">(Wortsman et al., 2023)</ref> to stabilize training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Loss</head><p>ViT/16-Lagre ViT/16-Lagre-DHCx2</p><p>Figure <ref type="figure" target="#fig_0">11</ref>: Training loss curves of ViT/16-Large and ViT/16-Large-DHC×2, smoothed using an Exponential Moving Average (EMA) with a decay rate of 0.999. The gain from Hyper-Connections decreases as training progresses, likely due to pass over the same dataset across many epochs, resulting in diminishing returns from the additional capacity provided by Hyper-Connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 VISULIZATION OF DHC</head><p>We randomly select three categories from the ImageNet dataset and sample the corresponding examples from the validation set. These samples are fed into the ViT-Base/16-DHC×2 model to compute the dynamic connection weights of the DHC in the final layer. As shown in Fig. <ref type="figure" target="#fig_11">12</ref>, we visualize the distribution of these weights. We observe that the intra-class distribution of beta is highly concentrated, indicating that samples within the same category tend to have similar beta values.</p><p>In contrast, the distribution of alpha is less concentrated, but the differences between the distributions of different categories are more pronounced, as exemplified by α 2,0 .  2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2, 0 0 1 2 3 4 5 6 Frequency 33:loggerhead turtle 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2, 0 0 5 10 15 Frequency 998:capitulum 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F MORE VISUALIZATION AND ANALYSIS</head><p>Unfolding hyper-connections. We first introduce how to determine the connection matrix C (0) for hyper-connections. To simplify writing, the layer output T k (h k 0 ) is denoted by T k for short. The</p><p>0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 1.0 0.5 0.0 0.5 1.0</p><formula xml:id="formula_33">C (0) C (1) C (2) C (3) C (4)</formula><p>(a) Connection matrix for DHC model.  </p><formula xml:id="formula_34">C (0) (Connections for {h j 0 } L+1 j=0 ), C (i) (Connections for {h ′ j i } L+1 j=0</formula><p>) for i ∈ {1, 2, 3, 4}. The attention layers, which have odd ids, are marked with green tick marks. recurrent form of hyper connection in Eq. 2 is expanded as follows:</p><formula xml:id="formula_35">h 0 k =H k ⊺ A m k = (T k-1 B k-1 + H k-1 ⊺ A r k-1 )A m k = k-1 j=0 T j B j (A r j+1 A r j+2 ...A r k-1 )A m k = k-1 j=0 T j B j ( k-1 t=j+1 A r t )A m k . (<label>27</label></formula><formula xml:id="formula_36">)</formula><p>Therefore, we obtain connection matrix c</p><formula xml:id="formula_37">(0) kj = B j ( k-1 t=j+1 A r t )A m k .</formula><p>Similarly, the connection matrix C (i) for the i-th hyper hidden from k-th layer can be computed by substituting the last A m k with A r k in Eq. 27, i.e.,</p><formula xml:id="formula_38">H ′ k = A r k ⊺ H k = k-1 j=0 ( k t=j+1 A r t ) ⊺ B j ⊺ T j ⊺ (28) c (i) kj =   ( k t=j+1 A r t ) ⊺ B j ⊺   i . (<label>29</label></formula><formula xml:id="formula_39">)</formula><p>Visualization for hyper hidden. We visualize connection matrices for hyper hiddens in Fig. <ref type="figure" target="#fig_13">13</ref> to reveal how hyper-connection maintains intermediate layer outputs. First of all, the four hyper hiddens are dissimilar and show completely different connection patterns. Then, we can see outputs from FFN layers are preserved long-termly in hyper hiddens, while attention layers are reserved less. It is also observed that the long-term connections are usually stored in pairs of hyper hiddens, where the connection is positive in one hyper hidden but negative in the other, for example, column 0 and 2 in C (1) , C (3) . With such strategy, these connections can be easily eliminated in the sum-pooling operation before the unembedding layer.</p><p>SHC shares similar connection pattern with DHC. We show the connection matrices for OLMo-1B-SHC×4 model in Fig. <ref type="figure" target="#fig_13">13b</ref>. Comparing to DHC, as shown in Fig. <ref type="figure" target="#fig_13">13a</ref>, SHC shares exactly the same connection patterns. Moreover, we observe many more PTB-like blocks in SHC, e.g., layers from 13 to 18. Note that the connection relation for SHC is token independent, and such PTB-like blocks can be physically reorganized to be parallelly computed. How HC×1 fails. The OLMo-1B×1 model is observed to perform worse than baseline in our experiments. Its connection matrix is visualized in Fig. <ref type="figure" target="#fig_14">14</ref> to show how it fails. Above all, we observe that layer 17 is wasted, who has no connection to subsequent layers at all. Secondly, compared to HC×2 and HC×4 models, the Λ shaped pattern does not appear. Note that HC×1 does not support the pattern of Λ in its mathematical formulation, where the connections to previous layers must be weakened or strengthened simultaneously. Thus, the lack of connection from the early layers to the final layers may suffer from gradient vanishing, like post-norm style transformers, which leads to performance degeneration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G DERIVATION OF NON-TRAINABLE HYPER-CONNECTION MATRIX FOR RESIDUAL CONNECTIONS</head><p>G.1 PRE-NORM RESIDUAL CONNECTION</p><p>In the Pre-Norm residual connection, the input to a layer is first normalized before being passed through the layer. The output of the layer is then added to the original input. This can be represented as:</p><formula xml:id="formula_40">ĥ = T (Norm(h)) + h.<label>(30)</label></formula><p>By incorporating the normalization operator into the layer, T := T • Norm, we can express the entire process as:</p><formula xml:id="formula_41">ĥ = T (h) + h.<label>(31)</label></formula><p>To express this using hyper-connections, the matrix for Pre-Norm can be structured as follows:</p><formula xml:id="formula_42">HC P reN orm = 0 1 1 1<label>(32)</label></formula><p>Given hyper hidden matrix H = h ⊺ , we prove that the output of HC PreNorm Ĥ = ĥ⊺ .</p><p>Proof. Ĥ = HC(T , H)</p><formula xml:id="formula_43">= B ⊺ T (H ⊺ A m ) ⊺ + A r ⊺ H = T (h) ⊺ + h ⊺ = ĥ⊺ .<label>(33)</label></formula><p>G.2 POST-NORM RESIDUAL CONNECTION</p><p>In the Post-Norm residual connection, the input to a layer is passed through the layer first, and then the output is normalized after being added to the original input. In matrix form, this can be represented as:</p><formula xml:id="formula_44">h ′ = T (h)<label>(34)</label></formula><p>The summation of the input and the normalized output of the layer is:</p><formula xml:id="formula_45">ĥ = Norm(h + h ′ )<label>(35)</label></formula><p>We consider Norm to be LayerNorm <ref type="bibr" target="#b36">(Zhang &amp; Sennrich, 2019)</ref>. The analysis process for RMSNorm is almost identical. In fact, the affine transformation can be incorporated into the subsequent layer, while the mean subtraction operation can be integrated into the current layer.</p><formula xml:id="formula_46">T = C • T • A, (<label>36</label></formula><p>) where A is the affine transformation, and C is the re-centering operator. Thus, the mean of the output of T is 0.</p><p>To express this using hyper-connections with an expansion rate n = 1, we need a hyper-connection matrix HC that encapsulates this operation:</p><formula xml:id="formula_47">HC P ostN orm =   0 1 √ σ 2 h +σ 2 h ′ +2σ hh ′ 1 1 √ σ 2 h +σ 2 h ′ +2σ hh ′   = 0 B A m A r . (<label>37</label></formula><formula xml:id="formula_48">)</formula><p>Preprint Similar to the previous proof, we prove that the output of HC PostNorm is equivalent to the transpose of the output of the Post-Norm residual connection:</p><formula xml:id="formula_49">Ĥ = ĥ⊺ .<label>(38)</label></formula><p>Proof. Note that</p><formula xml:id="formula_50">σ h+h ′ = σ 2 h + σ 2 h ′ + 2σ hh ′ . (<label>39</label></formula><formula xml:id="formula_51">)</formula><p>Given this fact, we can derive the Post-Norm:</p><formula xml:id="formula_52">ĥ = Norm(h ′ + h) = h ′ + h -µ h ′ +h σ h+h ′ = 1 σ h ′ +h (h ′ + h) = 1 σ 2 h + σ 2 h ′ + 2σ hh ′ (h ′ + h)<label>(40)</label></formula><p>For hyper-connections side, we have: In this section, we demonstrate that the following hyper-connection matrix will produce n identical networks arranged sequentially with residual connections between them:</p><formula xml:id="formula_53">Ĥ = B ⊺ h ′⊺ + H ′ = B ⊺ h ′⊺ + A r H = B ⊺ h ′⊺ + A r h ⊺ = 1 σ 2 h + σ 2 h ′ + 2σ hh ′ h ′⊺ + 1 σ 2 h + σ 2 h ′ + 2σ hh ′ h ⊺ = ĥ⊺ .<label>(41)</label></formula><formula xml:id="formula_54">HC = 0 1×1 1 1×n e 1 e n×n ,<label>(42)</label></formula><p>where e n×n denotes an n × n identity matrix, e i ∈ R n×1 represents the i-th column of e n×n , and 1 1×n signifies a 1 × n matrix of ones.</p><p>We will use mathematical induction to prove that</p><formula xml:id="formula_55">h k i = h k j and h k+1 i = T k (h k i ) + h k i , ∀i, j ∈ {0, 1, . . . , n}, ∀k ∈ {0, 1, . . . , L},</formula><p>where L is the number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. BASE CASE</head><p>For k = 0, we have the initial condition h 0 i = h 0 j , ∀i, j ∈ {0, 1, . . . , n}, as we define</p><formula xml:id="formula_56">H 0 = h 0 h 0 . . . h 0 ⊺ ∈ R n×d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDUCTION HYPOTHESIS</head><p>Assume that for some k ∈ {1, . . . , L -1}, we have</p><formula xml:id="formula_57">h k i = h k j and h k i = T k (h k-1 i ) + h k-1 i , ∀i, j ∈ {0, 1, . . . , n}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDUCTION STEP</head><p>We have</p><formula xml:id="formula_58">H k+1 = HC(T k , H k ) (43) = B ⊺ (h ′k 0 ) ⊺ + H ′ k (44) = B ⊺ A m ⊺ H k + A r ⊺ H k (45) = 1 n×1 T k (e ⊺ 1 H k ) + e n×n H k (46) = T k (h k 1 ) T k (h k 1 ) . . . T k (h k 1 ) ⊺ + h k 1 h k 2 . . . h k n ⊺ (47) = T k (h k 1 ) + h k 1 T k (h k 1 ) + h k 2 . . . T k (h k 1 ) + h k n ⊺ (48) = h k+1 1 h k+1 2 . . . h k+1 n ⊺ (49) Since h k i = h k j , ∀i, j ∈ {0, 1, . . . , n}, it follows that T k (h k 1 ) + h k i = T k (h k 1 ) + h k j .</formula><p>Thus, we have</p><formula xml:id="formula_59">h k+1 i = h k+1 j (50) Since h k i = h k j , ∀i, j ∈ {0, 1, . . . , n}, it follows that h k 1 = h k i , ∀i ∈ {0,<label>1</label></formula><p>, . . . , n}. Thus, we have</p><formula xml:id="formula_60">h k+1 i = T k (h k 1 ) + h k i (51) = T k (h k i ) + h k i (52) Preprint H.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HYPER-CONNECTION MATRIX OF PARALLEL ARRANGEMENT</head><p>In this section, we demonstrate that the following hyper-connection matrix will produce a network where every n adjacent layers are arranged in parallel, with each layer incorporating residual connections. We define a parallel-arranged network such that n adjacent layers form a group, with layers within a group being parallel and groups arranged sequentially. The output of k-th group is given by:</p><formula xml:id="formula_61">h k+1 = n i=1 (T k×n+i (h k ) + h k ).<label>(53)</label></formula><p>It can be proved that this arrangement can be described by the following hyper-connection matrices.</p><p>First, for k where k -1 ≡ 0 (mod n):</p><formula xml:id="formula_62">HC {k|k-1≡0 (mod n)} = 0 1×1 e ⊺ 1 1 n×1 1 n×n ,<label>(54)</label></formula><p>where the HC matrix can be decomposed into two operations: 1) sum up all the outputs of the previous group and use it as the input of the current layer and as the residual of the subsequent layers; 2) sum up the output and input saving to the first hidden vector slot.</p><p>Next, for k where k -1 ≡ i (mod n) and i ̸ = 0:</p><formula xml:id="formula_63">HC {k|k-1≡i (mod n),i̸ =0} = 0 1×1 e ⊺ i e i e n×n , .<label>(55)</label></formula><p>where the HC matrix selects the i-th hidden vector as the input of the current layer, and sums up the output and input, saving to the i-th hidden vector slot.</p><p>This means:</p><formula xml:id="formula_64">h k+1 =HC (k+1)×n (T (k+1)×n ,<label>(56)</label></formula><p>HC (k+1)×n-1 (T (k+1)×n-1 , (57)</p><formula xml:id="formula_65">• • • (58) HC k×n+1 (T k×n+1 , h k )))<label>(59)</label></formula><p>This can also be proved by mathematical induction; however, the conclusion is quite obvious through drawing, and the proof process is very tedious. Therefore, we don't repeat the similar proof here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint I PSEUDOCODE OF HYPER-CONNECTIONS</head><p>Algorithm 1 Network with Hyper-Connections Require: Initial hidden vector h 0 ∈ R d Require: Expansion rate n Ensure: Final output y 1: Initialize: Table 17: Losses of V3 validation sets for 1B model. </p><formula xml:id="formula_66">2: H 0 ← h 0 h 0 . . . h 0 ⊺ ∈ R n×d 3: for k = 1 to L do ▷ For each layer 4: H ← H k-1 5: (h 0 H ′ ) ← WC k⊺ H ▷ Width Connections 6: h ′ 0 ← T k (h 0 ) ▷ Layer Computation 7: Ĥ ← B k⊺ h ′ 0 + H ′ ▷ Depth</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The performance of the baseline model OLMoE-1B-7B and the model with hyperconnections, OLMoE-1B-7B-DHC×4. (1) and (2) show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens. (3) and (4) show the accuracy curves on HellaSwag and ARC-Challenge, demonstrating the superior performance of the OLMoE-1B-7B-DHC×4 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hyper-connections (HC) with an expansion rate of n = 2. (a) Residual connections.(b) Hyper-connections: β 1 , β 2 , α 0,0 , α 0,1 , α 1,0 , α 1,1 , α 2,1 , and α 2,2 are learnable scalars or scalars predicted by the network , depending on the specific HC version. These connections enable lateral information exchange and vertical integration of features across depths. The Transformer with HC is shown in Fig.17. They can be decoupled into depth-connections and width-connections. (c) Depth-connections perform a weighted sum between the layer output and the hidden vector h 1 . (d) Width-connections allow information exchange between the hidden vectors h 1 and h 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sequential and parallel arrangements of hyper-connections with n = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (1) and (2) Training loss (0.99 EMA smoothed) and C4-en validation loss for OLMo-7B and OLMo-7B-DHC×4 models. (3) and (4) Accuracy curves on hellaswag and sciq, demonstrating the superior performance of the OLMo-7B-DHC×4 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of connection matrices for hyper-connections and various related baseline methods. The attention layers, which have odd ids, are marked with green tick marks.</figDesc><graphic coords="9,119.25,494.26,60.93,62.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Loss curves in V3 validation sets and accuracy curves on downstream tasks for OLMoE-1B7B and OLMoE-1B7B-DHC×4 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Loss curves in V3 validation set and accuracy curves on downstream tasks for OLMo-7B and OLMo-7B-DHC×4 models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Distribution of weights of last DHC in ViT-Base/16-DHC×2 model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>Connection matrix for SHC model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Visualization of unfolded connection matrix. Matrices from left to right areC (0) (Connections for {h j 0 } L+1 j=0 ), C (i) (Connections for {h ′ j i } L+1 j=0) for i ∈ {1, 2, 3, 4}. The attention layers, which have odd ids, are marked with green tick marks.</figDesc><graphic coords="22,118.25,179.54,60.93,62.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparison of unfolded connection matrices for OLMo-1B-DHC×1, OLMo-1B-DHC×2 and OLMo-1B-DHC×4 model.</figDesc><graphic coords="23,258.21,91.92,80.05,82.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on expansion rates n with training on 500 B tokens.</figDesc><table><row><cell>2.45 2.50 2.55 2.60 Training Loss</cell><cell cols="2">Training Loss vs Tokens OLMo-1B-baseline OLMo-1B-DHCx1 OLMo-1B-DHCx2 OLMo-1B-DHCx4 OLMo-1B-DHCx8</cell><cell cols="2">2.45 2.50 2.55 2.60 Training Loss</cell><cell cols="2">Training Loss vs Tokens OLMo-1B-baseline OLMo-1B-DHCx1 W/O tanh OLMo-1B-DHCx2 W/O tanh OLMo-1B-DHCx4 W/O tanh OLMo-1B-DHCx8 W/O tanh</cell></row><row><cell>2.40</cell><cell></cell><cell></cell><cell></cell><cell>2.40</cell><cell></cell></row><row><cell cols="3">100 150 200 250 300 350 400 450 500 Tokens (Billions)</cell><cell></cell><cell cols="3">100 150 200 250 300 350 400 450 500 Tokens (Billions)</cell></row><row><cell cols="7">Figure 5: Comparison of training loss curves for different expansion rate. The left subfigure includes</cell></row><row><cell cols="7">models with dynamic hyper-connections (DHC) at various expansion rates, while the right subfigure</cell></row><row><cell cols="7">shows the effect of omitting the tanh function. Both subfigures illustrate how increasing the expansion</cell></row><row><cell cols="7">rate leads to improved training loss performance over 500B tokens. Results are smoothed using an</cell></row><row><cell cols="4">exponential moving average with a coefficient of 0.99.</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell>V2 Eval Loss ↓</cell><cell>V2 Eval PPL ↓</cell><cell>V3 Eval Loss ↓</cell><cell>V3 Eval PPL ↓</cell><cell>Down Stream Avg, Acc. ↑</cell></row><row><cell cols="2">OLMo-1B</cell><cell>2.811</cell><cell>18.023</cell><cell>2.544</cell><cell>14.229</cell><cell>62.5</cell></row><row><cell cols="2">OLMo-1B-DHC×1 W/O tanh</cell><cell>2.822</cell><cell>18.270</cell><cell>2.556</cell><cell>14.428</cell><cell>62.3</cell></row><row><cell cols="2">OLMo-1B-DHC×2 W/O tanh</cell><cell>2.792</cell><cell>17.663</cell><cell>2.537</cell><cell>14.033</cell><cell>63.8</cell></row><row><cell cols="2">OLMo-1B-DHC×4 W/O tanh</cell><cell>2.779</cell><cell>17.451</cell><cell>2.516</cell><cell>13.844</cell><cell>64.4</cell></row><row><cell cols="2">OLMo-1B-DHC×8 W/O tanh</cell><cell>2.777</cell><cell>17.425</cell><cell>2.514</cell><cell>13.819</cell><cell>63.8</cell></row><row><cell cols="2">OLMo-1B-DHC×1</cell><cell>2.819</cell><cell>18.125</cell><cell>2.556</cell><cell>14.418</cell><cell>62.3</cell></row><row><cell cols="2">OLMo-1B-DHC×2</cell><cell>2.802</cell><cell>17.950</cell><cell>2.534</cell><cell>14.114</cell><cell>63.0</cell></row><row><cell cols="2">OLMo-1B-DHC×4</cell><cell>2.781</cell><cell>17.509</cell><cell>2.514</cell><cell>13.826</cell><cell>63.8</cell></row><row><cell cols="2">OLMo-1B-DHC×8</cell><cell>2.778</cell><cell>17.445</cell><cell>2.516</cell><cell>13.843</cell><cell>62.8</cell></row><row><cell cols="7">We primarily conduct experiments on pre-training of large language model, including dense and</cell></row><row><cell cols="7">Mixture-of-Experts (MoE) (Shazeer et al., 2017) models, and extend to visual generation and</cell></row><row><cell cols="7">classification tasks. Due to space constraints, we include the vision experiments in the Appendix E.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on static and dynamic hyper-connections with training on 500 B tokens.</figDesc><table><row><cell>Methods</cell><cell>V2 Eval Loss ↓</cell><cell>V2 Eval PPL ↓</cell><cell>V3 Eval Loss ↓</cell><cell>V3 Eval PPL ↓</cell><cell>Down Stream Avg, Acc. ↑</cell></row><row><cell>OLMo-1B</cell><cell>2.811</cell><cell>18.023</cell><cell>2.544</cell><cell>14.229</cell><cell>62.5</cell></row><row><cell>OLMo-1B-SHC×2</cell><cell>2.799</cell><cell>17.778</cell><cell>2.538</cell><cell>14.152</cell><cell>63.4</cell></row><row><cell>OLMo-1B-DHC×2</cell><cell>2.802</cell><cell>17.950</cell><cell>2.534</cell><cell>14.114</cell><cell>63.0</cell></row><row><cell>OLMo-1B-DHC×2 W/O tanh</cell><cell>2.792</cell><cell>17.663</cell><cell>2.529</cell><cell>14.033</cell><cell>63.8</cell></row><row><cell>OLMo-1B-SHC×4</cell><cell>2.791</cell><cell>17.671</cell><cell>2.528</cell><cell>14.025</cell><cell>63.6</cell></row><row><cell>OLMo-1B-DHC×4</cell><cell>2.781</cell><cell>17.509</cell><cell>2.515</cell><cell>13.826</cell><cell>63.8</cell></row><row><cell>OLMo-1B-DHC×4 W/O tanh</cell><cell>2.779</cell><cell>17.451</cell><cell>2.516</cell><cell>13.844</cell><cell>64.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Performance of related methods on OLMo-1B models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>804</cell><cell cols="2">17.912</cell><cell cols="2">2.537</cell><cell>14.145</cell><cell>62.5</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>2.781</cell><cell cols="2">17.493</cell><cell cols="2">2.518</cell><cell>13.874</cell><cell>63.6</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell>2.779</cell><cell cols="2">17.773</cell><cell cols="2">2.516</cell><cell>13.823</cell><cell>64.4</cell></row><row><cell>✗</cell><cell>✓</cell><cell>✓</cell><cell>2.802</cell><cell cols="2">17.914</cell><cell cols="2">2.532</cell><cell>14.072</cell><cell>63.4</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell>2.783</cell><cell cols="2">17.504</cell><cell cols="2">2.520</cell><cell>13.906</cell><cell>63.4</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>2.781</cell><cell cols="2">17.835</cell><cell cols="2">2.515</cell><cell>13.807</cell><cell>63.8</cell></row><row><cell>Methods</cell><cell></cell><cell></cell><cell cols="2">V2 Eval Loss ↓</cell><cell cols="2">V2 Eval PPL ↓</cell><cell cols="2">V3 Eval Loss ↓</cell><cell>V3 Eval PPL ↓</cell><cell>Down Stream Avg, Acc. ↑</cell></row><row><cell>OLMo-1B</cell><cell></cell><cell></cell><cell></cell><cell>2.811</cell><cell cols="2">18.023</cell><cell></cell><cell>2.544</cell><cell>14.229</cell><cell>62.5</cell></row><row><cell cols="3">OLMo-1B-ResiDual</cell><cell></cell><cell>2.825</cell><cell cols="2">18.375</cell><cell></cell><cell>2.551</cell><cell>14.346</cell><cell>62.0</cell></row><row><cell cols="3">OLMo-1B-Altup×2</cell><cell></cell><cell>2.827</cell><cell cols="2">18.268</cell><cell></cell><cell>2.558</cell><cell>14.454</cell><cell>62.4</cell></row><row><cell cols="2">OLMo-1B-DHC×2</cell><cell></cell><cell></cell><cell>2.802</cell><cell cols="2">17.950</cell><cell></cell><cell>2.534</cell><cell>14.114</cell><cell>63.0</cell></row><row><cell cols="4">OLMo-1B-DHC×2 W/O tanh</cell><cell>2.792</cell><cell cols="2">17.663</cell><cell></cell><cell>2.529</cell><cell>14.033</cell><cell>63.8</cell></row><row><cell cols="2">4.3 7B MODELS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">100 200 300 400 500 Tokens (Billions) 2.2 2.3 2.4 Loss Training Loss OLMo-7B OLMo-7B-DHCx4</cell><cell></cell><cell cols="3">100 200 300 400 500 Tokens (Billions) 2.5 2.6 2.7 Loss C4-en Loss OLMo-7B OLMo-7B-DHCx4</cell><cell cols="3">70 100 200 300 400 500 HellaSwag Acc. Tokens (Billions) 55 60 65 Accuracy (%) OLMo-7B OLMo-7B-DHCx4</cell><cell>92 100 200 300 400 500 Tokens (Billions) 82 84 86 88 90 Accuracy (%)</cell></row></table><note><p>only a part of hidden state to transformer blocks. By contrast, ResiDual is proposed to combine both Pre-and Post-Norm in a two-stream style. Both methods expand the hidden size by n times with negligible computational overhead, with ResiDual expanding it exactly 2 times. For a fair comparison, we set n = 2 in our experiments. Unfortunately, although these methods show gains in the early stages of training, they are gradually surpassed by the baseline, as demonstrated by the results in Table4</p><p>and the training loss curves in Fig.15</p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Performance of 7B models. FLOPs refers to the computation per token in the forward pass.</figDesc><table><row><cell>Methods</cell><cell>Params (B)</cell><cell>FLOPs (G)</cell><cell>V2 Loss ↓</cell><cell>V2 PPL ↓</cell><cell>V3 Loss ↓</cell><cell>V3 PPL ↓</cell><cell>Tasks Avg. Acc. ↑</cell></row><row><cell>OLMo-7B</cell><cell>6.9</cell><cell>13.36</cell><cell cols="4">2.581 14.316 2.322 11.324</cell><cell>70.1</cell></row><row><cell>OLMo-7B-DHC×4</cell><cell>6.9</cell><cell>13.38</cell><cell cols="4">2.559 14.023 2.304 11.120</cell><cell>71.0</cell></row><row><cell cols="8">our model with DHCs shows no spikes throughout the training. This shows that our approach not</cell></row><row><cell cols="5">only achieves better loss but also ensures more stable training.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison of number of parameters.</figDesc><table><row><cell>Method</cell><cell cols="3">HC Params(B) Total Params(B) Total Params ∆ rate (%)</cell></row><row><cell>OLMo-1B</cell><cell>-</cell><cell>1.17676442</cell><cell>-</cell></row><row><cell>OLMo-1B-SHC×2</cell><cell>0.0000026</cell><cell>1.17676467</cell><cell>+0.00002%</cell></row><row><cell>OLMo-1B-SHC×4</cell><cell>0.0000077</cell><cell>1.17676518</cell><cell>+0.00007%</cell></row><row><cell>OLMo-1B-DHC×2</cell><cell>0.0002625</cell><cell>1.17702688</cell><cell>+0.02230%</cell></row><row><cell>OLMo-1B-DHC×4</cell><cell>0.0003940</cell><cell>1.17715846</cell><cell>+0.03349%</cell></row><row><cell>OLMo-7B</cell><cell>-</cell><cell>6.88809574</cell><cell>-</cell></row><row><cell>OLMo-7B-DHC×4</cell><cell>0.0013124</cell><cell>6.88967027</cell><cell>+0.02286%</cell></row><row><cell>OLMoE-1B-7B</cell><cell>-</cell><cell>6.91909427</cell><cell>-</cell></row><row><cell>OLMoE-1B-7B-DHC×4</cell><cell>0.0003940</cell><cell>6.91948832</cell><cell>+0.00570%</cell></row><row><cell cols="4">Computation Analysis. The main computational cost of SHC and DHC lies in line 5 of Algorithm 1,</cell></row><row><cell cols="4">where the complexity is O(d model × n × (n + 1)). The computational cost of the FFN is O(2 ×</cell></row><row><cell>d</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>model × d ffn ), and that of the projection part of attention is</p><p>O(4 × d model × d model ). Since O(d model × n × (n + 1)) ≪ O(4 × d model × d model ) &lt; O(2 × d model × d ffn )</p><p>, the computational cost of HC is negligible compared to the cost of both FFN and the attention projection part. Here, d ffn is the inner dimension of the FFN. The detailed computation cost statistics are presented in Table8</p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>FLOPs per token in forward pass.The introduction of HC results in a minor increase in activation memory usage during training. For a transformer model with L layers, a model dimension of d model , batch size b, sequence length s, and number of attention heads a, the activation memory is calculated as sbd model L(34 + 5as/d model ), as outlined in<ref type="bibr" target="#b17">Korthikanti et al. (2022)</ref>. Incorporating HC with an expansion rate of n adds an extra memory overhead of 2nsbd model L. For n = 2, this contributes less than 15% to the total memory usage of a standard transformer. Notably, the memory consumption is mostly driven by the weight parameters, which experience only a slight increase with HC. Additionally, given HC's low computational cost, the hidden states generated by HC can be discarded post forward pass and recomputed during backpropagation to further optimize memory usage. With this approach, the additional memory requirement is reduced to nsbd model . During inference, the memory usage for activations is largely determined by the Key-Value cache, which is not impacted by the extra activations brought by HC. Moreover, the hidden states from earlier layers can be released as soon as the next layer's computations start, significantly lowering memory requirements.</figDesc><table><row><cell>Method</cell><cell cols="3">HC FLOPs (G) Total FLOPs (G) Total FLOPs ∆ rate (%)</cell></row><row><cell>OLMo-1B</cell><cell>-</cell><cell>2.3536</cell><cell>-</cell></row><row><cell>OLMo-1B-SHC×2</cell><cell>0.0010</cell><cell>2.3545</cell><cell>+0.038%</cell></row><row><cell>OLMo-1B-SHC×4</cell><cell>0.0031</cell><cell>2.3566</cell><cell>+0.127%</cell></row><row><cell>OLMo-1B-DHC×2</cell><cell>0.0020</cell><cell>2.3554</cell><cell>+0.076%</cell></row><row><cell>OLMo-1B-DHC×4</cell><cell>0.0049</cell><cell>2.3583</cell><cell>+0.200%</cell></row><row><cell>OLMo-7B</cell><cell>-</cell><cell>13.3647</cell><cell>-</cell></row><row><cell>OLMo-7B-DHC×4</cell><cell>0.0197</cell><cell>13.3844</cell><cell>+0.147%</cell></row><row><cell>OLMoE-1B-7B</cell><cell>-</cell><cell>2.3580</cell><cell>-</cell></row><row><cell>OLMoE-1B-7B-DHC×4</cell><cell>0.0049</cell><cell>2.3629</cell><cell>+0.208%</cell></row><row><cell>Memory Footprint.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Benchmarking class-conditional image generation on ImageNet 256×256, with cfg=1.50. NP, P, and R are short for Numerical Precision, Precision, and Recall, respectively.For the Large model (307M parameters), ViT/16 achieves 77.25% accuracy. The SHC and DHC configurations further enhance accuracy to 78.38% and 79.94%, respectively. This corresponds to relative improvements of 1.13% and 2.69%, with DHC showing the highest performance. These results demonstrate that hyper-connections (SHC and DHC) significantly improve accuracy, especially in the Large model scale.</figDesc><table><row><cell>Method</cell><cell>NP</cell><cell cols="4">QK-Norm Size (M) FID↓ sFID↓</cell><cell>IS↑</cell><cell>P↑</cell><cell>R↑</cell></row><row><cell>DiT-XL/2</cell><cell>FP32</cell><cell>✗</cell><cell>675</cell><cell>2.27</cell><cell>4.60</cell><cell cols="2">278.24 0.83 0.57</cell></row><row><cell>DiT-XL/2</cell><cell>FP16</cell><cell>✓</cell><cell>675</cell><cell>2.36</cell><cell>4.54</cell><cell cols="2">269.46 0.83 0.58</cell></row><row><cell>DiT-1B/2</cell><cell>FP16</cell><cell>✓</cell><cell>983</cell><cell>2.13</cell><cell>4.50</cell><cell cols="2">288.69 0.82 0.59</cell></row><row><cell cols="2">DiT-XL/2-SHC×2 FP16</cell><cell>✓</cell><cell>675</cell><cell>2.18</cell><cell>4.52</cell><cell cols="2">287.24 0.82 0.60</cell></row></table><note><p>Our experimental results demonstrate that DiT models incorporating hyper-connections exhibit comparable performance metrics to DiT models with 50% more parameters. This finding underscores the efficiency and efficacy of hyper-connections in enhancing model performance without increasing model size.</p><p>E.2 IMAGE CLASSIFICATION</p><p>For the image classification experiments, we train ViT/16-Base and ViT/16-Large models with images at a resolution of 224 × 224 for 300 epochs, following the experimental setup used by<ref type="bibr" target="#b10">(Dosovitskiy et al., 2020)</ref></p><p>.To speed up the training process, we use bfloat16 numerical precision. The training configuration is detailed in Table11</p><p>. Within this configuration, we replace the residual connections with static and dynamic hyper-connections, referred to as SHC and DHC, respectively, using an expansion rate of n = 2. The top-1 accuracy results are presented in Table10</p><p>, and the training loss curves for ViT/16-Large and ViT/16-Large with DHC×2 are shown in Fig.11</p><p>.</p><p>For the Base model (85M), our re-implemented ViT/16 achieves 76.38% accuracy on 224 × 224 images. The SHC and DHC enhance performance to 77.60% and 77.26%, respectively. representing relative increases of 1.22% and 0.88%.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Accuracy on ImageNet. ViT*/16 refers to the results reported by<ref type="bibr" target="#b10">(Dosovitskiy et al., 2020)</ref>, whereas ViT/16 denotes our re-implemented baseline. SHC and DHC indicate that residual connections are replaced with static and dynamic hyper-connections, respectively.</figDesc><table><row><cell cols="2">Model Scales Params (M)</cell><cell>ViT*/16</cell><cell cols="3">ViT/16 ViT/16-SHC×2 ViT/16-DHC×2</cell></row><row><cell></cell><cell></cell><cell>384 × 384</cell><cell></cell><cell>224 × 224</cell><cell></cell></row><row><cell>Base</cell><cell>85</cell><cell>77.91</cell><cell>76.38</cell><cell>77.60</cell><cell>77.26</cell></row><row><cell>Large</cell><cell>307</cell><cell>76.53</cell><cell>77.25</cell><cell>78.38</cell><cell>79.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Training hyperparameters for ViT.</figDesc><table><row><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>Learning Rate (lr)</cell><cell>0.003</cell></row><row><cell>Batch Size</cell><cell>4096</cell></row><row><cell>Scheduler</cell><cell>Cosine Annealing with Linear Warmup (10k steps)</cell></row><row><cell cols="2">Data Augmentation Mixup (α = 0.2)</cell></row><row><cell>Epochs</cell><cell>300</cell></row><row><cell>Optimizer</cell><cell>AdamW (β 1 = 0.9, β 2 = 0.999, ϵ = 1e -8)</cell></row><row><cell>Gradient Clipping</cell><cell>1.0</cell></row><row><cell>Weight Decay</cell><cell>0.3</cell></row><row><cell>Dropout</cell><cell>0.1</cell></row><row><cell>Precision</cell><cell>bf16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Final Output:11: h L ← sum rows of H L 12: h L ← Normalization Layer(h L ) 13: y ← Output Layer(h L ) 14: return y Downstream Benchmarks for OLMoE.</figDesc><table><row><cell>Connections</cell></row></table><note><p>Figure 17: Training loss curves of DHC without tanh over 500 billion tokens, smoothed using Exponential Moving Average (EMA) with a decay rate of 0.99. Preprint</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Results on downstream benchmarks for 1B models.</figDesc><table><row><cell>Method</cell><cell cols="7">arc_easy copa hellaswag openbook_qa piqa sciq winogrande avg.</cell></row><row><cell>OLMo-1B</cell><cell>56.8</cell><cell>76.0</cell><cell>56.1</cell><cell>33.8</cell><cell>74.4 85.1</cell><cell>55.6</cell><cell>62.5</cell></row><row><cell></cell><cell></cell><cell cols="3">Scaling n in DHC W/O tanh</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OLMo-1B-DHCx1 W/O tanh</cell><cell>56.8</cell><cell>75.0</cell><cell>55.3</cell><cell>33.4</cell><cell>72.9 85.4</cell><cell>57.1</cell><cell>62.3</cell></row><row><cell>OLMo-1B-DHCx2 W/O tanh</cell><cell>63.0</cell><cell>74.0</cell><cell>57.1</cell><cell>34.6</cell><cell>73.5 86.0</cell><cell>58.2</cell><cell>63.8</cell></row><row><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>61.2</cell><cell>80.0</cell><cell>57.5</cell><cell>33.6</cell><cell>75.5 85.8</cell><cell>56.9</cell><cell>64.4</cell></row><row><cell>OLMo-1B-DHCx8 W/O tanh</cell><cell>61.1</cell><cell>75.0</cell><cell>57.6</cell><cell>35.4</cell><cell>73.8 85.2</cell><cell>58.5</cell><cell>63.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scaling n in DHC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OLMo-1B-DHCx1</cell><cell>59.7</cell><cell>74.0</cell><cell>55.5</cell><cell>33.6</cell><cell>73.5 85.4</cell><cell>54.5</cell><cell>62.3</cell></row><row><cell>OLMo-1B-DHCx2</cell><cell>59.7</cell><cell>73.0</cell><cell>56.7</cell><cell>34.0</cell><cell>74.7 85.2</cell><cell>57.9</cell><cell>63.0</cell></row><row><cell>OLMo-1B-DHCx4</cell><cell>59.8</cell><cell>79.0</cell><cell>58.1</cell><cell>32.4</cell><cell>74.3 86.1</cell><cell>57.1</cell><cell>63.8</cell></row><row><cell>OLMo-1B-DHCx8</cell><cell>56.8</cell><cell>75.0</cell><cell>58.0</cell><cell>34.4</cell><cell>73.8 84.2</cell><cell>57.3</cell><cell>62.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scaling n in SHC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OLMo-1B-SHCx2</cell><cell>59.1</cell><cell>77.0</cell><cell>56.6</cell><cell>35.4</cell><cell>74.2 85.3</cell><cell>56.4</cell><cell>63.4</cell></row><row><cell>OLMo-1B-SHCx4</cell><cell>59.3</cell><cell>77.0</cell><cell>56.7</cell><cell>34.0</cell><cell>74.3 86.6</cell><cell>57.1</cell><cell>63.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Non-trainable WC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OLMo-1B-DHCx4</cell><cell>60.5</cell><cell>78.0</cell><cell>56.2</cell><cell>34.0</cell><cell>73.5 86.0</cell><cell>55.8</cell><cell>63.4</cell></row><row><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>59.1</cell><cell>72.0</cell><cell>56.8</cell><cell>35.0</cell><cell>73.3 86.0</cell><cell>55.5</cell><cell>62.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Non-trainable B</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OLMo-1B-DHCx4</cell><cell>59.5</cell><cell>77.0</cell><cell>57.9</cell><cell>33.8</cell><cell>73.3 85.6</cell><cell>56.6</cell><cell>63.4</cell></row><row><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>60.4</cell><cell>74.0</cell><cell>57.6</cell><cell>34.0</cell><cell>74.9 86.7</cell><cell>57.5</cell><cell>63.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>Losses of V2 validation sets for 1B Model.</figDesc><table><row><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx4</cell><cell></cell><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx4</cell><cell></cell><cell>OLMo-1B-SHCx4</cell><cell>OLMo-1B-SHCx2</cell><cell></cell><cell>OLMo-1B-DHCx8</cell><cell>OLMo-1B-DHCx4</cell><cell>OLMo-1B-DHCx2</cell><cell>OLMo-1B-DHCx1</cell><cell></cell><cell>OLMo-1B-DHCx8 W/O tanh</cell><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx2 W/O tanh</cell><cell>OLMo-1B-DHCx1 W/O tanh</cell><cell></cell><cell>OLMo-1B</cell><cell>Method</cell></row><row><cell>2.295</cell><cell>2.296</cell><cell></cell><cell>2.308</cell><cell>2.312</cell><cell></cell><cell>2.300</cell><cell>2.307</cell><cell></cell><cell>2.295</cell><cell>2.290</cell><cell>2.309</cell><cell>2.323</cell><cell></cell><cell>2.292</cell><cell>2.295</cell><cell>2.311</cell><cell>2.320</cell><cell></cell><cell>2.319</cell><cell>4chan</cell></row><row><cell>2.592</cell><cell>2.594</cell><cell></cell><cell>2.609</cell><cell>2.608</cell><cell></cell><cell>2.603</cell><cell>2.610</cell><cell></cell><cell>2.591</cell><cell>2.591</cell><cell>2.608</cell><cell>2.625</cell><cell></cell><cell>2.589</cell><cell>2.591</cell><cell>2.600</cell><cell>2.626</cell><cell></cell><cell>2.615</cell><cell>c4_100_domains</cell></row><row><cell>2.739</cell><cell>2.742</cell><cell></cell><cell>2.755</cell><cell>2.752</cell><cell></cell><cell>2.751</cell><cell>2.757</cell><cell></cell><cell>2.739</cell><cell>2.738</cell><cell>2.754</cell><cell>2.775</cell><cell></cell><cell>2.734</cell><cell>2.735</cell><cell>2.749</cell><cell>2.773</cell><cell></cell><cell>2.762</cell><cell>c4_en</cell></row><row><cell>3.347</cell><cell>3.348</cell><cell></cell><cell>3.357</cell><cell>3.357</cell><cell></cell><cell>3.357</cell><cell>3.360</cell><cell></cell><cell>3.353</cell><cell>3.354</cell><cell>3.367</cell><cell>3.376</cell><cell></cell><cell>3.350</cell><cell>3.344</cell><cell>3.362</cell><cell>3.379</cell><cell></cell><cell>3.364</cell><cell>gab</cell></row><row><cell>2.689</cell><cell>2.684</cell><cell></cell><cell>2.710</cell><cell>2.700</cell><cell></cell><cell>2.692</cell><cell>2.703</cell><cell></cell><cell>2.684</cell><cell>2.683</cell><cell>2.703</cell><cell>2.728</cell><cell></cell><cell>2.685</cell><cell>2.686</cell><cell>2.700</cell><cell>2.725</cell><cell></cell><cell>2.719</cell><cell>ice</cell></row><row><cell>3.066 2.567</cell><cell>3.051 2.569</cell><cell>Non-trainable Beta</cell><cell>3.100 2.585</cell><cell>3.077 2.583</cell><cell>Non-trainable WC</cell><cell>3.062 2.580</cell><cell>3.063 2.587</cell><cell>Scaling n in SHC</cell><cell>3.054 2.567</cell><cell>3.064 2.564</cell><cell>3.061 2.587</cell><cell>3.090 2.606</cell><cell>Scaling n in DHC</cell><cell>3.060 2.562</cell><cell>3.056 2.562</cell><cell>3.069 2.583</cell><cell>3.102 2.609</cell><cell>Scaling n in DHC W/O tanh</cell><cell>3.085 2.594</cell><cell>m2d2_s2orc m2d2_wiki</cell></row><row><cell>3.005</cell><cell>3.008</cell><cell></cell><cell>3.025</cell><cell>3.024</cell><cell></cell><cell>3.018</cell><cell>3.023</cell><cell></cell><cell>3.008</cell><cell>3.005</cell><cell>3.022</cell><cell>3.037</cell><cell></cell><cell>3.006</cell><cell>3.005</cell><cell>3.015</cell><cell>3.036</cell><cell></cell><cell>3.028</cell><cell>manosphere</cell></row><row><cell>2.496</cell><cell>2.497</cell><cell></cell><cell>2.510</cell><cell>2.508</cell><cell></cell><cell>2.504</cell><cell>2.511</cell><cell></cell><cell>2.493</cell><cell>2.492</cell><cell>2.509</cell><cell>2.533</cell><cell></cell><cell>2.492</cell><cell>2.492</cell><cell>2.503</cell><cell>2.531</cell><cell></cell><cell>2.522</cell><cell>mc4_en</cell></row><row><cell>2.222</cell><cell>2.221</cell><cell></cell><cell>2.240</cell><cell>2.238</cell><cell></cell><cell>2.232</cell><cell>2.238</cell><cell></cell><cell>2.219</cell><cell>2.218</cell><cell>2.237</cell><cell>2.262</cell><cell></cell><cell>2.218</cell><cell>2.221</cell><cell>2.231</cell><cell>2.264</cell><cell></cell><cell>2.250</cell><cell>pile</cell></row><row><cell>2.887</cell><cell>2.917</cell><cell></cell><cell>2.945</cell><cell>2.959</cell><cell></cell><cell>2.899</cell><cell>2.933</cell><cell></cell><cell>2.876</cell><cell>2.890</cell><cell>2.930</cell><cell>2.961</cell><cell></cell><cell>2.878</cell><cell>2.898</cell><cell>2.908</cell><cell>2.948</cell><cell></cell><cell>2.953</cell><cell>ptb</cell></row><row><cell>3.638</cell><cell>3.627</cell><cell></cell><cell>3.663</cell><cell>3.678</cell><cell></cell><cell>3.653</cell><cell>3.643</cell><cell></cell><cell>3.631</cell><cell>3.641</cell><cell>3.704</cell><cell>3.652</cell><cell></cell><cell>3.628</cell><cell>3.632</cell><cell>3.635</cell><cell>3.703</cell><cell></cell><cell>3.672</cell><cell>twitterAAE</cell></row><row><cell>2.606</cell><cell>2.622</cell><cell></cell><cell>2.644</cell><cell>2.636</cell><cell></cell><cell>2.627</cell><cell>2.643</cell><cell></cell><cell>2.608</cell><cell>2.611</cell><cell>2.636</cell><cell>2.678</cell><cell></cell><cell>2.609</cell><cell>2.610</cell><cell>2.625</cell><cell>2.672</cell><cell></cell><cell>2.657</cell><cell>wikitext_103</cell></row><row><cell>2.781</cell><cell>2.783</cell><cell></cell><cell>2.804</cell><cell>2.802</cell><cell></cell><cell>2.791</cell><cell>2.799</cell><cell></cell><cell>2.778</cell><cell>2.781</cell><cell>2.802</cell><cell>2.819</cell><cell></cell><cell>2.777</cell><cell>2.779</cell><cell>2.792</cell><cell>2.822</cell><cell></cell><cell>2.811</cell><cell>avg</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Perplexities of V2 validation sets for 1B models.</figDesc><table><row><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx4</cell><cell></cell><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx4</cell><cell></cell><cell>OLMo-1B-SHCx4</cell><cell>OLMo-1B-SHCx2</cell><cell></cell><cell>OLMo-1B-DHCx8</cell><cell>OLMo-1B-DHCx4</cell><cell>OLMo-1B-DHCx2</cell><cell>OLMo-1B-DHCx1</cell><cell></cell><cell>OLMo-1B-DHCx8 W/O tanh</cell><cell>OLMo-1B-DHCx4 W/O tanh</cell><cell>OLMo-1B-DHCx2 W/O tanh</cell><cell>OLMo-1B-DHCx1 W/O tanh</cell><cell></cell><cell>OLMo-1B</cell><cell>Method</cell></row><row><cell>9.932</cell><cell>9.927</cell><cell></cell><cell>10.092</cell><cell>10.054</cell><cell></cell><cell>9.977</cell><cell>10.046</cell><cell></cell><cell>9.922</cell><cell>9.877</cell><cell>10.061</cell><cell>10.210</cell><cell></cell><cell>9.897</cell><cell>10.082</cell><cell>9.920</cell><cell>10.174</cell><cell></cell><cell>10.167</cell><cell>4chan</cell></row><row><cell>13.386</cell><cell>13.354</cell><cell></cell><cell>13.566</cell><cell>13.587</cell><cell></cell><cell>13.507</cell><cell>13.601</cell><cell></cell><cell>13.346</cell><cell>13.344</cell><cell>13.568</cell><cell>13.810</cell><cell></cell><cell>13.313</cell><cell>13.470</cell><cell>13.340</cell><cell>13.815</cell><cell></cell><cell>13.666</cell><cell>c4_100_domains</cell></row><row><cell>15.510</cell><cell>15.475</cell><cell></cell><cell>15.666</cell><cell>15.721</cell><cell></cell><cell>15.655</cell><cell>15.753</cell><cell></cell><cell>15.467</cell><cell>15.430</cell><cell>15.710</cell><cell>16.031</cell><cell></cell><cell>15.387</cell><cell>15.625</cell><cell>15.412</cell><cell>16.004</cell><cell></cell><cell>15.829</cell><cell>c4_en</cell></row><row><cell>28.436</cell><cell>28.417</cell><cell></cell><cell>28.704</cell><cell>28.689</cell><cell></cell><cell>28.691</cell><cell>28.782</cell><cell></cell><cell>28.591</cell><cell>28.624</cell><cell>29.002</cell><cell>29.265</cell><cell></cell><cell>28.488</cell><cell>28.848</cell><cell>28.340</cell><cell>29.328</cell><cell></cell><cell>28.901</cell><cell>gab</cell></row><row><cell>14.641</cell><cell>14.722</cell><cell></cell><cell>14.873</cell><cell>15.023</cell><cell></cell><cell>14.766</cell><cell>14.931</cell><cell></cell><cell>14.640</cell><cell>14.633</cell><cell>14.925</cell><cell>15.302</cell><cell></cell><cell>14.658</cell><cell>14.882</cell><cell>14.676</cell><cell>15.259</cell><cell></cell><cell>15.166</cell><cell>ice</cell></row><row><cell>21.130 13.051</cell><cell>21.454 13.021</cell><cell>Non-trainable Beta</cell><cell>21.696 13.242</cell><cell>22.186 13.263</cell><cell>Non-trainable WC</cell><cell>21.372 13.194</cell><cell>21.391 13.294</cell><cell>Scaling n in SHC</cell><cell>21.198 13.025</cell><cell>21.410 13.006</cell><cell>21.349 13.284</cell><cell>21.986 13.539</cell><cell>Scaling n in DHC</cell><cell>21.337 12.960</cell><cell>21.521 13.234</cell><cell>21.243 12.965</cell><cell>22.231 13.587</cell><cell>Scaling n in DHC W/O tanh</cell><cell>21.860 13.377</cell><cell>m2d2_s2orc m2d2_wiki</cell></row><row><cell>20.253</cell><cell>20.185</cell><cell></cell><cell>20.579</cell><cell>20.594</cell><cell></cell><cell>20.457</cell><cell>20.562</cell><cell></cell><cell>20.240</cell><cell>20.186</cell><cell>20.524</cell><cell>20.847</cell><cell></cell><cell>20.200</cell><cell>20.392</cell><cell>20.181</cell><cell>20.823</cell><cell></cell><cell>20.651</cell><cell>manosphere</cell></row><row><cell>12.142</cell><cell>12.135</cell><cell></cell><cell>12.276</cell><cell>12.310</cell><cell></cell><cell>12.234</cell><cell>12.319</cell><cell></cell><cell>12.097</cell><cell>12.080</cell><cell>12.294</cell><cell>12.584</cell><cell></cell><cell>12.084</cell><cell>12.217</cell><cell>12.079</cell><cell>12.562</cell><cell></cell><cell>12.453</cell><cell>mc4_en</cell></row><row><cell>9.220</cell><cell>9.228</cell><cell></cell><cell>9.377</cell><cell>9.390</cell><cell></cell><cell>9.315</cell><cell>9.374</cell><cell></cell><cell>9.196</cell><cell>9.189</cell><cell>9.362</cell><cell>9.606</cell><cell></cell><cell>9.185</cell><cell>9.312</cell><cell>9.219</cell><cell>9.620</cell><cell></cell><cell>9.488</cell><cell>pile</cell></row><row><cell>18.478</cell><cell>17.932</cell><cell></cell><cell>19.272</cell><cell>19.016</cell><cell></cell><cell>18.149</cell><cell>18.791</cell><cell></cell><cell>17.749</cell><cell>18.102</cell><cell>18.727</cell><cell>19.326</cell><cell></cell><cell>17.782</cell><cell>18.321</cell><cell>18.129</cell><cell>19.071</cell><cell></cell><cell>19.161</cell><cell>ptb</cell></row><row><cell>37.610</cell><cell>38.005</cell><cell></cell><cell>39.570</cell><cell>38.959</cell><cell></cell><cell>38.569</cell><cell>38.212</cell><cell></cell><cell>37.743</cell><cell>38.136</cell><cell>40.592</cell><cell>38.564</cell><cell></cell><cell>37.650</cell><cell>37.905</cell><cell>37.768</cell><cell>40.580</cell><cell></cell><cell>39.328</cell><cell>twitterAAE</cell></row><row><cell>13.766</cell><cell>13.553</cell><cell></cell><cell>13.963</cell><cell>14.070</cell><cell></cell><cell>13.836</cell><cell>14.060</cell><cell></cell><cell>13.570</cell><cell>13.606</cell><cell>13.957</cell><cell>14.555</cell><cell></cell><cell>13.592</cell><cell>13.806</cell><cell>13.594</cell><cell>14.462</cell><cell></cell><cell>14.251</cell><cell>wikitext_103</cell></row><row><cell>17.504</cell><cell>17.493</cell><cell></cell><cell>17.914</cell><cell>17.912</cell><cell></cell><cell>17.671</cell><cell>17.778</cell><cell></cell><cell>17.445</cell><cell>17.509</cell><cell>17.950</cell><cell>18.125</cell><cell></cell><cell>17.425</cell><cell>17.663</cell><cell>17.451</cell><cell>18.270</cell><cell></cell><cell>18.023</cell><cell>avg</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>&amp;𝛽 " &amp;</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>&amp; 𝛽 $ &amp;</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This research was conducted at <rs type="institution">ByteDance Inc.</rs> We are grateful for the suggestions and assistance provided by <rs type="person">Yaowei Zheng</rs>, <rs type="person">Yuyu Zhang</rs>, <rs type="person">Yunshui Li</rs>, <rs type="person">Xiang Li</rs>, <rs type="person">Bairen Yi</rs>, <rs type="person">Zhenyi Lu</rs> and <rs type="person">Xintian Han</rs>.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K VALIDATION SETS AND DOWNSTREAM TASKS</head><p>Table 12: OLMo's default configuration was evaluated using multiple metrics. Perplexity (PPL) and loss were used for the V2 and V3 Validation Sets, while zero-shot testing was applied to the Downstream Benchmarks. However, the grey benchmarks were excluded from our analysis due to the instability of their performance indicators.</p><p>V2 Validation Sets v2-small-4chan-validation v2-small-c4_100_domains-validation v2-small-c4_en-validation v2-small-gab-validation v2-small-ice-validation v2-small-m2d2_s2orc-validation v2-small-m2d2_wiki-validation v2-small-manosphere-validation v2-small-mc4_en-validation v2-small-pile-validation v2-small-ptb-validation v2-small-twitterAEE-validation v2-small-wikitext_103-validation V3 Validation Sets v3-small-c4_en-validation v3-small-dolma_books-validation v3-small-dolma_common-crawl-validation v3-small-dolma_pes2o-validation v3-small-dolma_reddit-validation v3-small-dolma_stack-validation v3-small-dolma_wiki-validation v3-small-ice-validation v3-small-m2d2_s2orc-validation v3-small-pile-validation v3-small-wikitext_103-validation Downstream Benchmarks piqa <ref type="bibr" target="#b3">(Bisk et al., 2020)</ref> hellaswag <ref type="bibr" target="#b35">(Zellers et al., 2019)</ref> winogrande (Sakaguchi et al., 2021)   openbook_qa <ref type="bibr" target="#b21">(Mihaylov et al., 2018)</ref> sciq (Johannes Welbl, 2017) arc_easy <ref type="bibr" target="#b5">(Clark et al., 2018)</ref> copa <ref type="bibr" target="#b24">(Roemmele et al., 2011</ref>)  commitment_bank (De Marneffe et al., 2019)   mrpc <ref type="bibr" target="#b9">(Dolan &amp; Brockett, 2005)</ref> rte <ref type="bibr" target="#b6">(Dagan et al., 2005)</ref> sst2 <ref type="bibr" target="#b28">(Socher et al., 2013)</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint J PYTORCH IMPLEMENTATION OF HYPER-CONNECTIONS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alternating updates for efficient transformers</title>
		<author>
			<persName><forename type="first">Cenk</forename><surname>Baykal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishanth</forename><surname>Dikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<title level="m">Exploring the surprising difficulty of natural yes/no questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457v1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Ieee</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third international workshop on paraphrasing (IWP2005)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reducing activation recomputation in large transformer models</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkug</forename><surname>Lym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Andersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.05198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08249</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denseformer: A dense transformer framework for person re-identification</title>
		<author>
			<persName><forename type="first">Haoyan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Computer Vision</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Megalodon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.08801</idno>
		<title level="m">Efficient llm pretraining and inference with unlimited context length</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Olmoe: Open mixture-of-experts language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.02060" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09748</idno>
		<title level="m">Scalable diffusion models with transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI spring symposium series</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Socialiqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<title level="m">Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Outrageously large neural networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<title level="m">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<title level="m">Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX</title>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.14322</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Shufang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Hany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arul</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14802</idno>
		<title level="m">Residual: Transformer with dual residual connections</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">528 Non-trainable WC OLMo-1B-DHCx</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m">532 Non-trainable Beta OLMo-1B-DHCx</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
