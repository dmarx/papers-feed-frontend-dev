# HYPER-CONNECTIONS

## Abstract

## 

We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyperconnections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems.

## 

(1) and (2) show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens. (3) and (4) show the accuracy curves on HellaSwag and ARC-Challenge, demonstrating the superior performance of the OLMoE-1B-7B-DHC×4 model.

Deep learning has achieved tremendous success across various domains, where residual connections [(He et al., 2016)](#b13) have been instrumental in contemporary neural network architectures, including transformers and CNNs. Residual connections help mitigate the problem of gradient vanishing, enabling the effective training of very deep networks. However, it is important to acknowledge that residual connections are not infallible solutions and still present limitations that remain unresolved.

The two main variants of residual connections, Pre-Norm and Post-Norm, each make distinct trade-offs between gradient vanishing and representation collapse. Pre-Norm applies normalization operations to the input before each residual block, effectively addressing the problem of gradient vanishing [(Bengio et al., 1994;](#b2)[Glorot & Bengio, 2010)](#b11). However, it can also lead to the issue of collapse in deep representations [(Liu et al., 2020)](#b18), where hidden features in deeper layers become highly similar, diminishing the contribution of additional layers as their number increases. In contrast, Post-Norm applies normalization after the output of each residual block, reducing the influence of a hidden state on subsequent layers. This approach can alleviate the issue of representation collapse but  (b) Hyper-connections: β 1 , β 2 , α 0,0 , α 0,1 , α 1,0 , α 1,1 , α 2,1 , and α 2,2 are learnable scalars or scalars predicted by the network , depending on the specific HC version. These connections enable lateral information exchange and vertical integration of features across depths. The Transformer with HC is shown in Fig. [17](#fig_6). They can be decoupled into depth-connections and width-connections. (c) Depth-connections perform a weighted sum between the layer output and the hidden vector h 1 . (d) Width-connections allow information exchange between the hidden vectors h 1 and h 2 . also reintroduces the problem of vanishing gradients. The vanishing gradient and the representation collapse are like two ends of a seesaw, with these two variants making respective trade-offs between these issues. The key issue is that residual connections, including both Pre-Norm and Post-Norm variants, predefine the strength of connections between the output and input within a layer. Layer Index i Figure [3](#): Cosine similarity between the input of the current layer and the previous layers for the OLMo-1B Model [(Groeneveld et al., 2024)](#b12). The curve represents the median of similarity, while the shaded area indicates the range between the 5th and 95th percentiles. The red curve shows the model with Pre-Norm, and the blue curve shows that with hyper-connections.

Driven by the limitations of residual connections, an important question arises: Can neural networks autonomously learn the optimal strength of connections to improve performance? To address this, we propose hyper-connections (HC), which lead to significantly improved performance with a negligible increase in computation and parameters. We will show that both Post-Norm and Pre-Norm variants can be expressed as specific non-trainable forms of hyper-connections, as discussed in § 3.1.

The core idea of hyper-connections (HC) is to propose learnable depth-connections and width-connections, as depicted in Fig. [2 (b](#fig_2)). These connections flexibly integrate features vertically across depths, compared to the residual connections shown in Fig. [2 (a)](#fig_2). Depth-connections can be considered as a generalized residual connections, assigning weights to the connections between the inputs and outputs of each layer. To enable the network to model different depth-connections simultaneously, we expand the network's input into n copies, each having its own depth connection, as shown in Fig. [2 (b](#fig_2)). This design allows multiple hidden vectors to reserve multiple patterns connecting preceding layers, as shown in § 4.5. Moreover, we establish width connections between the n hidden vectors, allowing information exchange between hidden vectors within the same layer, as shown in Fig. [2 (b](#fig_2)). We argue that n (> 1) hidden states are necessary. As analyzed in Appendix F, the seesaw effect persists when n = 1, and experiments show that it does not improve performance, as shown in Fig. [5](#). In contrast, when n > 1, hyper-connections can not only learn to adjust the strength of residuals but also rearrange layers, either sequentially or in parallel, as discussed in § 3.2. To further enhance flexibility, we introduce dynamic hyper-connections (DHC), enabling the network to adjust connection weights according to the input. Notably, although HC seem to increase the network's width by n times, the additional parameters and computational cost are almost negligible, as analyzed in Appendix B. The Transformer with HC is shown in Fig. [17](#fig_6).

Our research, primarily centered on large language models (LLMs) pre-training, also extends to visual generation and classification tasks. Using Pre-Norm as a baseline, we demonstrate the significant Preprint benefits of hyper-connections, including 1B and 7B dense models as well as 7B MoE models, as detailed in § 4. The benefits are particularly prominent for OLMoE [(Muennighoff et al., 2024)](#b22) as presented in Fig. [1](#fig_0). The model utilizing DHC converges 1.8 times faster and shows an improvement of 6 points on ARC-Challenge compared to the baseline trained with 500 B tokens. According to our visualization analysis, as shown in Fig. [3](#), the baseline model tends toward representation collapse, characterized by high similarity between features of adjacent layers. In contrast, models with HC exhibit significantly lower similarity between features across adjacent layers and a wider range of similarities. This suggests that HC enhance the impact of each layer. Further discussion is provided in §4.5 and in Appendix F. These compelling pieces of evidence demonstrate the generality of the hyper-connections principle, and we anticipate their applicability in numerous other AI challenges.

## METHOD

2.1 STATIC HYPER-CONNECTIONS Consider the hidden vector h k-1 ∈ R d (or h k-1 ∈ R d×1 ) as the input to the k-th layer, with the initial input h 0 to the network. Initially, h 0 ∈ R d is replicated n times to form the initial hyper hidden matrix H 0 = h 0 h 0 . . . h 0 ⊺ ∈ R n×d . Here, n is the expansion rate. For the k-th layer, the input consists of the hyper hidden matrix from the previous layer

$H k-1 = h k-1 1 h k-1 2 . . . h k-1 n ⊺ ∈ R n×d .$Finally, we sum the last hyper hidden matrix row-wise to obtain the required hidden vector, which is then passed through a final projector to produce the final output of the network (i.e., a normalization layer and an unembedding layer in transformers). To simplify the notation in subsequent analysis, we omit the layer index and simply denote the hyper-hidden matrix as

$H = (h 1 h 2 . . . h n ) ⊺ .$The hyper-connections (HC) can be represented by a matrix HC, where each element defines the connection weight. The matrix is structured as follows: n+1) .

$HC = 0 1×1 B A m A r =       0 β 1 β 2 • • • β n α 1,0 α 1,1 α 1,2 • • • α 1,n α 2,0 α 2,1 α 2,2 • • • α 2,n . . . . . . . . . . . . . . . α n,0 α n,1 α n,2 • • • α n,n       ∈ R (n+1)×($(1)

Consider a network layer T , it integrates self-attention layers and feed-forward networks within transformers. The output of the HC, denoted by Ĥ, can be simply formulated as follows:

$Ĥ = HC(T , H) = B ⊺ T (H ⊺ A m ) ⊺ + A r ⊺ H.(2)$We use A m as weights to perform a weighted sum on the input H = (h 1 h 2 . . . h n ) ⊺ to obtain the input h 0 of the current layer T , which is given by:

$h ⊺ 0 = A m ⊺ H,(3)$While A r is used to connect H and map it to a hyper hidden matrix H ′ , as shown below:

$H ′ = A r ⊺ H. (4)$Subsequently, the output is given by:

$Ĥ = B ⊺ (T h 0 ) ⊺ + H ′ .(5)$The depth-connections can be decoupled as the following matrix, which is shown at Fig [2 (a)](#fig_2):

$DC = B diag(A r ) = β 1 β 2 • • • β n α 1,1 α 2,2 • • • α n,n ∈ R 2×n ,(6)$where the first row B represents the weights of the output of the current layer T , and the last row diag(A r ) represents the weights of the input. We use diag(A r ) to represent the flatten vector of the diagonal entries of A r .

The width-connections matrix can be defined as follows, which is shown at Fig

$2 (b): WC = (A m A r ) ∈ R n×(n+1) .(7)$The algorithm that employs hyper-connections is presented in Algorithm 1.

## DYNAMIC HYPER-CONNECTIONS

The entries of HC can dynamically depend on the input H, which the matrix representation of dynamic hyper-connections (DHC) is defined as follows:

$HC(H) = 0 1×1 B(H) A m (H) A r (H)(8)$Similarly, given a layer T and input H, we obtain the output of the DHC as follows:

$Ĥ = HC(H)(T , H).(9)$In practice, we combine the dynamic and static matrices to achieve DHC. The dynamic parameters are obtained through a linear transformation. To stabilize the training process, we introduce normalization before the linear transformation and apply the tanh activation function after it, scaling it by a small initial learnable factor. The following equations detail how these dynamic parameters are computed:

$H = norm(H)(10)$$B(H) = s β • tanh(HW β ) ⊺ + B ∈ R 1×n (11) A m (H) = s α • tanh(HW m ) + A m ∈ R n×1 (12) A r (H) = s α • tanh(HW r ) + A r ∈ R n×n (13)$Our experiments in § 4 demonstrate that dynamic hyper-connections outperform static hyperconnections in language modeling tasks. The PyTorch implementations for both the static and dynamic variants of hyper-connections are detailed in Algorithm 2 and 3.

## INITIALIZATION

In order to make the initialization of the hyper-connections equivalent to the Pre-Norm residual connections, we adopt the following initialization strategy. The dynamic parameters W β , W m , and W r in Eqs. 11, 12, and 13 are initialized to 0, while the static matrices are initialized as follows:

$0 1×1 B k A m k A r k = 0 1×1 1 1×n e k mod n e n×n , (14$$)$where k is the index of the layer. mod denotes the modulo operation.

## WHY HYPER-CONNECTIONS

In this section, we elucidate the rationale behind hyper-connections. We explore how variants of residual connections, namely Pre-Norm and Post-Norm, can be viewed as non-trainable hyperconnections, and introduce the concept of sequential-parallel duality, demonstrating how hyperconnections can dynamically optimize layer arrangements to enhance network performance. A visulize analysis of hyper-connections through an unfolded view is discussed in § 4.5. Preprint

## RESIDUAL CONNECTIONS AS NON-TRAINABLE HYPER-CONNECTIONS

The Pre-Norm and Post-Norm residual connections can be represented as the following hyperconnections matrices with an expansion rate n = 1:

$HC P reN orm = 0 1 1 1 , (15$$) HC P ostN orm =   0 1 √ σ 2 i +σ 2 o +2σio 1 1 √ σ 2 i +σ 2 o +2σio   , (16$$)$where σ i and σ o denote the standard deviations of the input and output of the neural network layer, respectively, and σ io is the covariance between them.

For Pre-Norm, its hyper-connection matrix is a 2 × 2 matrix where the bottom right triangular part is filled with 1 and the rest is a placeholder 0. For Post-Norm, the weights depend on the variances and covariance of the input and output, forming a 2 × 2 matrix. Therefore, their hyper-connection matrices are non-trainable. In this work, we propose hyper-connections that can be (n + 1) × (n + 1) matrices, with weights that are trainable or even predicted based on the input. The complete derivation is provided in Appendix G.

## SEQUENTIAL-PARALLEL DUALITY

Given a series of neural network modules, we have the option to arrange them either sequentially or in parallel. However, hyper-connections offer an approach that learns to rearrange these layers in a configuration blending both sequential and parallel arrangements. Without loss of generality, we set the expansion rate to n = 2. If the hyper-connections are learned as the following matrix, the neural network will be arranged sequentially:

$HC = 0 1 1 1 1 0 0 0 1 . (17$$)$In this case, the depth connection degenerates into a residual connection, as shown in Fig. [4 (a)](#fig_4).

When the hyper-connections for odd and even layers (with layer numbering starting from 1) are defined by the following matrices, the neural network will be arranged in parallel every two consecutive layers, similar to the arrangement of parallel transformer blocks in transformers [(Wang, 2021)](#b32), as shown in [Fig. 4 (b)](#). The general and complete derivation is provided in Appendix H.

Preprint

$HC odd = 0 1 0 1 1 1 1 1 1 , (18) HC even = 0 0 1 0 1 0 1 0 1 . (19$$)$Thus, learning the hyper-connection matrix in various forms can create layer arrangements that surpass traditional sequential and parallel configurations, resulting in a soft-mixture or even dynamic arrangement. For static hyper-connections, the layer arrangement within the network remains fixed after training. In contrast, dynamic hyper-connections allow the arrangement to adapt dynamically for each token. Experiment Settings. We employ the experimental setup outlined by OLMo [(Groeneveld et al., 2024)](#b12) for dense models and by OLMoE [(Muennighoff et al., 2024)](#b22) for MoE models. For dense models, we use dolmap-v1.5-sample [(Soldaini et al., 2024)](#b29) as our training dataset. We conduct ablation studies on 1B models and assess the effectiveness of our method at the 7B model scale. For MoE models, we train the OLMoE-1B-7B model, both with and without hyper-connections, on the OLMOE-MIX dataset. These models activate 1.3B out of a total of 7B parameters. All experiments are trained on 500B tokens.

## RESULTS

Implementation. We maintain the training configuration of the baseline model, replacing the residual connections with hyper-connections. The static component in [Eqs. 1,](#)[11,](#)[12,](#)[13](#) does not utilize weight decay, whereas the dynamic component does. Since the hyper hidden vectors of the final transformer block are ultimately summed, we ensure that the standard deviation (std) of the output (before the final layernorm and unembedding layers) remains consistent with the original. At initialization, we scale the std of the weights of the output module at all layers, including those of the second linear layer of the feedforward network and the output projector of the attention module, by a factor of √ n, where n represents the expansion rate. The parameters and computational overhead introduced by hyper-connections is negligible, see Table [. 7](#tab_6) and [8.](#tab_7) Metrics. In accordance with the methodology of OLMo [(Groeneveld et al., 2024)](#b12), we report the average perplexities (PPL) and losses on both the V2 and V3 validation sets, along with the average metrics for zero-shot evaluation on downstream benchmarks (refer to Table [12](#tab_1)). We observe significant volatility in the zero-shot performance indicators for the datasets (highlighted in grey in Table [12](#tab_1)), with fluctuations exceeding 20% across neighboring checkpoints. For more reliable and consistent results, we excludes these volatile datasets from our analysis. For the MoE models, in line with OLMoE, we also present losses on V3 validation sets, and accuracies on downstream benchmarks (refer to Table [13](#tab_15)).

## ABLATION STUDY

We use the dynamic hyperconnections with an expansion rate of n = 4 and include the tanh function as the default method, marked with the suffix -DHC, while -SHC denotes static hyper-connections.

The evaluation results are presented in Table [1](#tab_0), and the training loss curves are depicted in Fig. [5](#). We observe that with an expansion rate of n = 1, the performance of DHC is inferior to the baseline. However, for n > 1, DHC significantly outperforms the baseline, achieving superior results at n = 4, with the increase to n = 8 providing minimal additional benefits. Notably, OLMo-1B-DHC×8 W/O tanh excels on both V2 and V3 validation sets, with a reduction in V2 Eval Loss by 0.034 and V3 Eval Loss by 0.029 compared to the baseline. Furthermore, the decline rate of training losses for DHC (n ≥ 2) is steeper than that of the baseline, and DHC demonstrates greater stability, with no spikes observed in any DHC experiments.

Static and dynamic hyper-connections. Table [2](#tab_1) presents an ablation study comparing SHC and DHC. All hyper-connection (HC) variants significantly outperform the baseline. At an expansion rate of 2, the improvements of DHC and SHC are similar. However, at an expansion rate of 4, DHC performs notably better than SHC. The importance of B and WC. As shown in Table [3](#), not training WC leads to significant performance declines, with the V2 loss increasing by 0.021 and the V3 loss by 0.017, as seen when comparing the 4th and 6th lines of Table [3](#). In contrast, the impact is less pronounced when B is not trained. Therefore, ensuring the trainability of both WC and B is crucial.

## COMPARISON WITH RELATED WORKS

We implemented the Altup [(Baykal et al., 2024)](#b1) and ResiDual [(Xie et al., 2023)](#b34) methods in OLMo.

Altup is motivated to widen the hidden dimension while maintaining low computation cost by passing

## Preprint

Table [3](#): Ablation study on OLMo-1B-DHC×4. In the B or WC column, the symbol "✗" denotes parameters that are not trainable from initialization. 

$WC B Tanh V2 Eval Loss ↓ V2 Eval PPL ↓ V3 Eval Loss ↓ V3 Eval PPL ↓ Down Stream Avg, Acc. ↑ ✗ ✓ ✗ 2.$
## SciQ Acc.

OLMo-7B OLMo-7B-DHCx4 We evaluate the effectiveness of hyper-connections on the 7B model, training a model with DHCs with an expansion rate of 4, denoted as OLMo-7B-DHC×4. According to Table [5](#tab_3), OLMo-7B-DHC×4 significantly outperforms the baseline OLMo-7B model in all average metrics. In the V2 evaluation, OLMo-7B-DHC×4 shows improvements of 0.022 for loss and 0.293 for PPL. Furthermore, the average score of downstream benchmarks 0.710 surpasses the baseline 0.701, with the results of specific tasks shown in Fig. [10](#fig_9).

Based on Fig 6, the OLMo-7B-DHC×4 model consistently shows better metrics compared to baseline, including training and validation loss and accuracy in downstream benchmarks. Notably, after 400 B tokens, the model maintains its improvement without the gains diminishing. This indicates that the OLMo-7B-DHC×4 model continues to provide consistent benefits in reducing loss, even at higher token counts. Furthermore, according to Fig. [6](#fig_5), the baseline model exhibits frequent spikes, while Preprint 

## MOE MODELS

We evaluate the effectiveness of hyper-connections on the Mixture-of-Experts (MoE) model. We retrain the original OLMoE-1B-7B model as the baseline and train a model that applies Dynamic Hyper-Connections (DHC) with n = 4, replacing the residual connections. The full results are shown in Fig. [9](#fig_8), which illustrates that hyper-connections outperform residual connections in almost all metrics. In many metrics, our method requires only half of the training tokens to achieve the same performance as the baseline. Fig. [1](#fig_0) and Table [6](#) highlight some of the results, such as a reduction in training loss of approximately 0.027, a reduction in loss on the C4-en validation set of 0.028, an improvement of 6 points on the ARC-Challengeand an improvement of 1.2 points on MMLU Var.

Table 6: Downstream evaluations for MoE models training with 500B tokens under the OLMoE evaluation setting. ARC-C stands for ARC-Challenge, and ARC-E for ARC-Easy. MMLU Var is a modified version of MMLU that includes varying few-shot examples, providing stable feedback during early training, as outlined in the OLMoE setting (Muennighoff et al., 2024). Methods MMLU Var Hella-Swag ARC-C ARC-E PIQA Wino-Grande BoolQ OLMoE-1B-7B 38.5 69.5 41.8 72.8 77.6 64.4 65.4 OLMoE-1B-7B-DHC×4 39.7 70.2 47.8 76.7 78.2 64.6 68.5 4.5 VISUALIZATION ANALYSIS 0 4 8 12 16 20 24 28 32 Hyper-Connection 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Post-Norm 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Pre-Norm 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Pre-Norm PTB 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 Two-hop Residual In this section, we investigate the learned hyper-connection weights and show how the output of the former layer contributes to the latter ones. To this end, we convert hyper-connections to dense connections cross layers. Consider the input hidden vectors h k 0 in k-th layer, it can be unfolded as a weighted summation over previous layer outputs:

$h k 0 = k-1 j=0 c (0) kj T j (h j 0 ),(20)$where c

(0) kj describes how much layer-j (T j ) contributes to layer-k's input h k 0 . Then, C (0) denotes a dense connection weight matrix. In particular, let layer-0 be the word embedding and T 0 be an Preprint identity mapping, layer-L+1 be the hidden state before the unembedding layer, which is a summation over the last hidden vectors, i.e., h L+1 0 = j h L j . OLMo-1B-DHC×4 model is adopted for visualization. We take the checkpoint at 500B tokens and forward random validation text to obtain dynamic hyper-connection weights. In addition, we show connection patterns for some related baseline methods. Finally, the visualization is illustrated in Fig. [13](#fig_13). We present the following findings, with more detailed discussions provided in Appendix F.

Connection patterns for baseline methods. For Pre-Norm baseline, the connection matrix is simply a lower triangular matrix with diagonal elements erased, because each transformer layer joins the residual equally. In the Pre-Norm parallel transformer block (PTB) baseline, the connection matrix appears jagged because the input to the FFN layer does not depend on the output of the previous attention layer. For Post-Norm baseline, the connection only holds for adjacent layers, as the weight for bottom layers decays every time the residual passes a post-norm layer. For the two-hop residual baseline [(Ma et al., 2024)](#b20), the outputs of attention layers are not added to residual and only contributes to the next one FFN layer, resulting in a vertical strip pattern in the connection matrix.

Λ-shaped connection pattern. In the connection matrix for hyper-connections, a long-term decay pattern can be observed, where layers are generally preferred to rely on a few adjacent layer outputs. Moreover, the bottom layers (e.g. layer 0,2) are observed frequently used in most of subsequent layers. Therefore, the two patterns together form a Λ-shaped connection pattern. Note that the long-term decay pattern is a Post-Norm style pattern, while the frequently accessed pattern is Pre-Norm style, indicating that the hyper-connection introduces a free mixture of Pre-and Post-Norm architecture.

Input word embedding is eliminated from model output. As per the first column in the connection matrix for layer inputs, the input word embedding contributes to most of the layers except for the final one. This last layer, which products the model's output, is used for next token prediction. In most cases, keeping a component of input embedding in model output is harmful to next token prediction, especially when using a tied word embedding such as that employed by OLMo-1B. Similar results are found in previous works [(Ma et al., 2023)](#b19).

Parallel transformer blocks are observed. As discussed in § 3.2, parallel transformer block, which performs attention and FFN in parallel, is a special case for hyper-connection. In practice, PTB-like patterns, which can be identified by the local jagged pattern, are surprisingly observed to be learned by hyper-connections. For instance, layer 11 has a minimal contribution to the input of layer 12 (refer to row 12 in the hyper-connection connection matrix). This suggests that layers 11 and 12 can operate in parallel, thereby forming a PTB module.

Attention layers tend to have fewer long-term connections. It is observed that attention layers at the bottom barely have long-term contribution, a trend that persists until layer 17. Upon examining the connection matrix for hyper hiddens (refer to Fig. [13](#fig_13) in the appendix), it's evident that the outputs of the FFN layers have significantly greater magnitudes than those of the attention layers. This pattern resembles a two-hop residual connection design, wherein the attention output contributes to the input of the following FFN layer, but doesn't join the main residual path.

## RELATED WORK

Transformers [(Vaswani et al., 2017)](#b31) have revolutionized various fields, particularly natural language processing and computer vision. They rely heavily on residual connections to facilitate the training of deep models. Our hyper-connections approach can replace residual connections, providing stable training and consistent improvements in both natural language processing and computer vision.

The issues of gradient vanishing and representation collapse [(Bengio et al., 1994;](#b2)[Glorot & Bengio, 2010;](#b11)[Liu et al., 2020)](#b18) have been extensively studied. The combinations of normalization techniques [(Ioffe & Szegedy, 2015;](#b15)[Ba et al., 2016)](#b0) and residual connections [(He et al., 2016)](#b13), like Pre-Norm and Post-Norm, actually reflects different emphases in solving these two issues. However, despite these advancements, the fundamental trade-off between gradient vanishing and representation collapse in deep networks remains a critical challenge. Building on these findings, our work introduces a novel approach that enables neural networks to autonomously learn the optimal strength of connections, potentially improving both gradient stability and representation quality.

## Preprint

## CONCLUSION

In conclusion, we have introduced hyper-connections as an effective alternative to residual connections in transformers. Our analysis reveals that hyper-connections not only overcome the limitations of residuals but also enable dynamic adjustments in network architecture. Experimental results confirm their promising benefits across various tasks, including pre-training of large language model, image generation, and image classification. 

$FFN + + + + 𝛼 ",! " 𝛽 " " h " ! 𝛼 $,! " 𝛼 "," " 𝛼 ",$ " 𝛼 ",$ " 𝛼 $,$ " 𝛽 $ " h $ ! 𝛼 ",! $ 𝛽 " $ h " " 𝛼 $,! $ 𝛼 "," $ 𝛼 ",$ $ 𝛼 ",$ $ 𝛼 $,$ $ 𝛽 $ $ h $ " 𝛼 ",! % 𝛽 " % h " $ 𝛼 $,! % 𝛼 "," % 𝛼 ",$ % 𝛼 ",$ % 𝛼 $,$ % 𝛽 $ % h $ $ h $ % h " ' h $ ( ℎ ' ℎ ! ℎ '$Figure [8](#): Comparison between transformers with hyper-connections and that with residual connections.

## Preprint B PARAMETERS, COMPUTATION AND MEMORY FOOTPRINT ANALYSIS

Static Hyper-Connections. All learnable parameters are included in the hyper-connection matrix HC in Eq. 1. The number of parameters in one HC is given by:

$|θ SHC | = |θ B | + |θ A | = n + n • (n + 1) = n • (n + 2), (21$$)$where n is the expansion rate, |θ B | is the number of parameters in B in SHC, and |θ A | is the number of parameters in A. Each layer contains two hyper-connection modules (one for the self attention and one for the feedforward network). Thus, the number of extra parameters is:

$P extra = |θ SHC | × 2 × L, (22$$)$where L is the number of layers. For example, in OLMo-1B-SHC×4, P extra = 4×(4+2)×2×16 = 768.

Dynamic Hyper-Connections. The parameters of DHC are defined in Eqs. 10, 11, 12, and 13, and the number of parameters is given by:

$|θ DHC | = |θ norm | + |s β | + |θ W β | + |θ B | + |s α | + |θ Wm | + |θ Am | + |θ Wr | + |θ Ar | (23) = |θ norm | + 1 + d model + n + 1 + d model + n + d model × n + n × n (24) = |θ norm | + d model × (n + 2) + n × (n + 2) + 2, (25$$)$where d model is the dimension of the hidden states in the transformer, and |θ norm | depends on the type of normalization module. In OLMo models, there are no parameters for normalization, so |θ norm | = 0. In OLMoE, |θ norm | = d model . Similar to the static hyper-connections, the number of extra parameters is:

$P extra = |θ DHC | × 2 × L,(26)$For example, for OLMo-1B-DHC×4, P extra = (0 + 2048

$× (4 + 2) + 4 × (4 + 2) + 2) × 2 × 16 = 394, 048.$The number of parameters for DHC and SHC used in the experiments is detailed in Table [7](#tab_6), while their corresponding FLOPs comparisons are provided in Table [8](#tab_7). Regardless of whether SHC or DHC is used, the additional parameters and computational overhead introduced are minimal and can be considered negligible.  2.5 2.6 2.7 c4 en val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.6 2.7 2.8 2.9 dolma books val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) 2.5 2.6 2.7 dolma cc val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.1 2.2 2.3 dolma pes2o val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.8 2.9 3.0 dolma reddit val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) 0.95 1.00 1.05 dolma stack val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.3 2.4 2.5 dolma wiki val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 500 Tokens (B) 2.5 2.6 2.7 2.8 ice val. loss OLMo-7B-OLMo-7B-DHCx4 100 200 300 400 Tokens (B) 

## Preprint E VISION EXPERIMENTS

Datasets. We use the ILSVRC-2012 ImageNet dataset [(Deng et al., 2009)](#b8) with 1k classes and 1.3M images (see ImageNet in the following) for image generation and classification.

## E.1 IMAGE GENERATION

To investigate the generalizability of hyper-connections in image generation, our experiments are conducted using the DiT framework [(Peebles & Xie, 2022)](#b23) training the models for 1400 epochs.

In order to save experimental costs, we use FP16 precision, introduce flash-attention to speed up training, and introduce QK-Norm [(Wortsman et al., 2023)](#b33) to stabilize training. 

## Loss

## Training Loss

ViT/16-Lagre ViT/16-Lagre-DHCx2

Figure [11](#fig_0): Training loss curves of ViT/16-Large and ViT/16-Large-DHC×2, smoothed using an Exponential Moving Average (EMA) with a decay rate of 0.999. The gain from Hyper-Connections decreases as training progresses, likely due to pass over the same dataset across many epochs, resulting in diminishing returns from the additional capacity provided by Hyper-Connections.

## E.3 VISULIZATION OF DHC

We randomly select three categories from the ImageNet dataset and sample the corresponding examples from the validation set. These samples are fed into the ViT-Base/16-DHC×2 model to compute the dynamic connection weights of the DHC in the final layer. As shown in Fig. [12](#fig_11), we visualize the distribution of these weights. We observe that the intra-class distribution of beta is highly concentrated, indicating that samples within the same category tend to have similar beta values.

In contrast, the distribution of alpha is less concentrated, but the differences between the distributions of different categories are more pronounced, as exemplified by α 2,0 .  2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2, 0 0 1 2 3 4 5 6 Frequency 33:loggerhead turtle 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 2, 0 0 5 10 15 Frequency 998:capitulum 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 2.40 

## F MORE VISUALIZATION AND ANALYSIS

Unfolding hyper-connections. We first introduce how to determine the connection matrix C (0) for hyper-connections. To simplify writing, the layer output T k (h k 0 ) is denoted by T k for short. The

0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 0 4 8 12 16 20 24 28 32 1.0 0.5 0.0 0.5 1.0

$C (0) C (1) C (2) C (3) C (4)$(a) Connection matrix for DHC model.  

$C (0) (Connections for {h j 0 } L+1 j=0 ), C (i) (Connections for {h ′ j i } L+1 j=0$) for i ∈ {1, 2, 3, 4}. The attention layers, which have odd ids, are marked with green tick marks. recurrent form of hyper connection in Eq. 2 is expanded as follows:

$h 0 k =H k ⊺ A m k = (T k-1 B k-1 + H k-1 ⊺ A r k-1 )A m k = k-1 j=0 T j B j (A r j+1 A r j+2 ...A r k-1 )A m k = k-1 j=0 T j B j ( k-1 t=j+1 A r t )A m k . (27$$)$Therefore, we obtain connection matrix c

$(0) kj = B j ( k-1 t=j+1 A r t )A m k .$Similarly, the connection matrix C (i) for the i-th hyper hidden from k-th layer can be computed by substituting the last A m k with A r k in Eq. 27, i.e.,

$H ′ k = A r k ⊺ H k = k-1 j=0 ( k t=j+1 A r t ) ⊺ B j ⊺ T j ⊺ (28) c (i) kj =   ( k t=j+1 A r t ) ⊺ B j ⊺   i . (29$$)$Visualization for hyper hidden. We visualize connection matrices for hyper hiddens in Fig. [13](#fig_13) to reveal how hyper-connection maintains intermediate layer outputs. First of all, the four hyper hiddens are dissimilar and show completely different connection patterns. Then, we can see outputs from FFN layers are preserved long-termly in hyper hiddens, while attention layers are reserved less. It is also observed that the long-term connections are usually stored in pairs of hyper hiddens, where the connection is positive in one hyper hidden but negative in the other, for example, column 0 and 2 in C (1) , C (3) . With such strategy, these connections can be easily eliminated in the sum-pooling operation before the unembedding layer.

SHC shares similar connection pattern with DHC. We show the connection matrices for OLMo-1B-SHC×4 model in Fig. [13b](#fig_13). Comparing to DHC, as shown in Fig. [13a](#fig_13), SHC shares exactly the same connection patterns. Moreover, we observe many more PTB-like blocks in SHC, e.g., layers from 13 to 18. Note that the connection relation for SHC is token independent, and such PTB-like blocks can be physically reorganized to be parallelly computed. How HC×1 fails. The OLMo-1B×1 model is observed to perform worse than baseline in our experiments. Its connection matrix is visualized in Fig. [14](#fig_14) to show how it fails. Above all, we observe that layer 17 is wasted, who has no connection to subsequent layers at all. Secondly, compared to HC×2 and HC×4 models, the Λ shaped pattern does not appear. Note that HC×1 does not support the pattern of Λ in its mathematical formulation, where the connections to previous layers must be weakened or strengthened simultaneously. Thus, the lack of connection from the early layers to the final layers may suffer from gradient vanishing, like post-norm style transformers, which leads to performance degeneration.

## G DERIVATION OF NON-TRAINABLE HYPER-CONNECTION MATRIX FOR RESIDUAL CONNECTIONS

G.1 PRE-NORM RESIDUAL CONNECTION

In the Pre-Norm residual connection, the input to a layer is first normalized before being passed through the layer. The output of the layer is then added to the original input. This can be represented as:

$ĥ = T (Norm(h)) + h.(30)$By incorporating the normalization operator into the layer, T := T • Norm, we can express the entire process as:

$ĥ = T (h) + h.(31)$To express this using hyper-connections, the matrix for Pre-Norm can be structured as follows:

$HC P reN orm = 0 1 1 1(32)$Given hyper hidden matrix H = h ⊺ , we prove that the output of HC PreNorm Ĥ = ĥ⊺ .

Proof. Ĥ = HC(T , H)

$= B ⊺ T (H ⊺ A m ) ⊺ + A r ⊺ H = T (h) ⊺ + h ⊺ = ĥ⊺ .(33)$G.2 POST-NORM RESIDUAL CONNECTION

In the Post-Norm residual connection, the input to a layer is passed through the layer first, and then the output is normalized after being added to the original input. In matrix form, this can be represented as:

$h ′ = T (h)(34)$The summation of the input and the normalized output of the layer is:

$ĥ = Norm(h + h ′ )(35)$We consider Norm to be LayerNorm [(Zhang & Sennrich, 2019)](#b36). The analysis process for RMSNorm is almost identical. In fact, the affine transformation can be incorporated into the subsequent layer, while the mean subtraction operation can be integrated into the current layer.

$T = C • T • A, (36$) where A is the affine transformation, and C is the re-centering operator. Thus, the mean of the output of T is 0.

To express this using hyper-connections with an expansion rate n = 1, we need a hyper-connection matrix HC that encapsulates this operation:

$HC P ostN orm =   0 1 √ σ 2 h +σ 2 h ′ +2σ hh ′ 1 1 √ σ 2 h +σ 2 h ′ +2σ hh ′   = 0 B A m A r . (37$$)$Preprint Similar to the previous proof, we prove that the output of HC PostNorm is equivalent to the transpose of the output of the Post-Norm residual connection:

$Ĥ = ĥ⊺ .(38)$Proof. Note that

$σ h+h ′ = σ 2 h + σ 2 h ′ + 2σ hh ′ . (39$$)$Given this fact, we can derive the Post-Norm:

$ĥ = Norm(h ′ + h) = h ′ + h -µ h ′ +h σ h+h ′ = 1 σ h ′ +h (h ′ + h) = 1 σ 2 h + σ 2 h ′ + 2σ hh ′ (h ′ + h)(40)$For hyper-connections side, we have: In this section, we demonstrate that the following hyper-connection matrix will produce n identical networks arranged sequentially with residual connections between them:

$Ĥ = B ⊺ h ′⊺ + H ′ = B ⊺ h ′⊺ + A r H = B ⊺ h ′⊺ + A r h ⊺ = 1 σ 2 h + σ 2 h ′ + 2σ hh ′ h ′⊺ + 1 σ 2 h + σ 2 h ′ + 2σ hh ′ h ⊺ = ĥ⊺ .(41)$$HC = 0 1×1 1 1×n e 1 e n×n ,(42)$where e n×n denotes an n × n identity matrix, e i ∈ R n×1 represents the i-th column of e n×n , and 1 1×n signifies a 1 × n matrix of ones.

We will use mathematical induction to prove that

$h k i = h k j and h k+1 i = T k (h k i ) + h k i , ∀i, j ∈ {0, 1, . . . , n}, ∀k ∈ {0, 1, . . . , L},$where L is the number of layers.

## Proof. BASE CASE

For k = 0, we have the initial condition h 0 i = h 0 j , ∀i, j ∈ {0, 1, . . . , n}, as we define

$H 0 = h 0 h 0 . . . h 0 ⊺ ∈ R n×d .$
## INDUCTION HYPOTHESIS

Assume that for some k ∈ {1, . . . , L -1}, we have

$h k i = h k j and h k i = T k (h k-1 i ) + h k-1 i , ∀i, j ∈ {0, 1, . . . , n}.$
## INDUCTION STEP

We have

$H k+1 = HC(T k , H k ) (43) = B ⊺ (h ′k 0 ) ⊺ + H ′ k (44) = B ⊺ A m ⊺ H k + A r ⊺ H k (45) = 1 n×1 T k (e ⊺ 1 H k ) + e n×n H k (46) = T k (h k 1 ) T k (h k 1 ) . . . T k (h k 1 ) ⊺ + h k 1 h k 2 . . . h k n ⊺ (47) = T k (h k 1 ) + h k 1 T k (h k 1 ) + h k 2 . . . T k (h k 1 ) + h k n ⊺ (48) = h k+1 1 h k+1 2 . . . h k+1 n ⊺ (49) Since h k i = h k j , ∀i, j ∈ {0, 1, . . . , n}, it follows that T k (h k 1 ) + h k i = T k (h k 1 ) + h k j .$Thus, we have

$h k+1 i = h k+1 j (50) Since h k i = h k j , ∀i, j ∈ {0, 1, . . . , n}, it follows that h k 1 = h k i , ∀i ∈ {0,1$, . . . , n}. Thus, we have

$h k+1 i = T k (h k 1 ) + h k i (51) = T k (h k i ) + h k i (52) Preprint H.$
## HYPER-CONNECTION MATRIX OF PARALLEL ARRANGEMENT

In this section, we demonstrate that the following hyper-connection matrix will produce a network where every n adjacent layers are arranged in parallel, with each layer incorporating residual connections. We define a parallel-arranged network such that n adjacent layers form a group, with layers within a group being parallel and groups arranged sequentially. The output of k-th group is given by:

$h k+1 = n i=1 (T k×n+i (h k ) + h k ).(53)$It can be proved that this arrangement can be described by the following hyper-connection matrices.

First, for k where k -1 ≡ 0 (mod n):

$HC {k|k-1≡0 (mod n)} = 0 1×1 e ⊺ 1 1 n×1 1 n×n ,(54)$where the HC matrix can be decomposed into two operations: 1) sum up all the outputs of the previous group and use it as the input of the current layer and as the residual of the subsequent layers; 2) sum up the output and input saving to the first hidden vector slot.

Next, for k where k -1 ≡ i (mod n) and i ̸ = 0:

$HC {k|k-1≡i (mod n),i̸ =0} = 0 1×1 e ⊺ i e i e n×n , .(55)$where the HC matrix selects the i-th hidden vector as the input of the current layer, and sums up the output and input, saving to the i-th hidden vector slot.

This means:

$h k+1 =HC (k+1)×n (T (k+1)×n ,(56)$HC (k+1)×n-1 (T (k+1)×n-1 , (57)

$• • • (58) HC k×n+1 (T k×n+1 , h k )))(59)$This can also be proved by mathematical induction; however, the conclusion is quite obvious through drawing, and the proof process is very tedious. Therefore, we don't repeat the similar proof here.

## Preprint I PSEUDOCODE OF HYPER-CONNECTIONS

Algorithm 1 Network with Hyper-Connections Require: Initial hidden vector h 0 ∈ R d Require: Expansion rate n Ensure: Final output y 1: Initialize: Table 17: Losses of V3 validation sets for 1B model. 

$2: H 0 ← h 0 h 0 . . . h 0 ⊺ ∈ R n×d 3: for k = 1 to L do ▷ For each layer 4: H ← H k-1 5: (h 0 H ′ ) ← WC k⊺ H ▷ Width Connections 6: h ′ 0 ← T k (h 0 ) ▷ Layer Computation 7: Ĥ ← B k⊺ h ′ 0 + H ′ ▷ Depth$![Figure 1: The performance of the baseline model OLMoE-1B-7B and the model with hyperconnections, OLMoE-1B-7B-DHC×4. (1) and (2) show the training loss (0.99 EMA smoothed) and the C4-en validation loss, respectively. Our method converges 1.8 times faster compared to the baseline and maintains a significant advantage at the 500B tokens. (3) and (4) show the accuracy curves on HellaSwag and ARC-Challenge, demonstrating the superior performance of the OLMoE-1B-7B-DHC×4 model.]()

![Figure 2: Hyper-connections (HC) with an expansion rate of n = 2. (a) Residual connections.(b) Hyper-connections: β 1 , β 2 , α 0,0 , α 0,1 , α 1,0 , α 1,1 , α 2,1 , and α 2,2 are learnable scalars or scalars predicted by the network , depending on the specific HC version. These connections enable lateral information exchange and vertical integration of features across depths. The Transformer with HC is shown in Fig.17. They can be decoupled into depth-connections and width-connections. (c) Depth-connections perform a weighted sum between the layer output and the hidden vector h 1 . (d) Width-connections allow information exchange between the hidden vectors h 1 and h 2 .]()

![Figure 4: Sequential and parallel arrangements of hyper-connections with n = 2.]()

![Figure 6: (1) and (2) Training loss (0.99 EMA smoothed) and C4-en validation loss for OLMo-7B and OLMo-7B-DHC×4 models. (3) and (4) Accuracy curves on hellaswag and sciq, demonstrating the superior performance of the OLMo-7B-DHC×4 model.]()

![Figure 7: Visualization of connection matrices for hyper-connections and various related baseline methods. The attention layers, which have odd ids, are marked with green tick marks.]()

![Figure 9: Loss curves in V3 validation sets and accuracy curves on downstream tasks for OLMoE-1B7B and OLMoE-1B7B-DHC×4 models.]()

![Figure 10: Loss curves in V3 validation set and accuracy curves on downstream tasks for OLMo-7B and OLMo-7B-DHC×4 models.]()

![Figure 12: Distribution of weights of last DHC in ViT-Base/16-DHC×2 model.]()

![Connection matrix for SHC model.]()

![Figure 13: Visualization of unfolded connection matrix. Matrices from left to right areC (0) (Connections for {h j 0 } L+1 j=0 ), C (i) (Connections for {h ′ j i } L+1 j=0) for i ∈ {1, 2, 3, 4}. The attention layers, which have odd ids, are marked with green tick marks.]()

![Figure 14: Comparison of unfolded connection matrices for OLMo-1B-DHC×1, OLMo-1B-DHC×2 and OLMo-1B-DHC×4 model.]()

![Ablation study on expansion rates n with training on 500 B tokens.]()

![Ablation study on static and dynamic hyper-connections with training on 500 B tokens.]()

![Performance of related methods on OLMo-1B models.]()

![Performance of 7B models. FLOPs refers to the computation per token in the forward pass.]()

![Comparison of number of parameters.]()

![FLOPs per token in forward pass.The introduction of HC results in a minor increase in activation memory usage during training. For a transformer model with L layers, a model dimension of d model , batch size b, sequence length s, and number of attention heads a, the activation memory is calculated as sbd model L(34 + 5as/d model ), as outlined inKorthikanti et al. (2022). Incorporating HC with an expansion rate of n adds an extra memory overhead of 2nsbd model L. For n = 2, this contributes less than 15% to the total memory usage of a standard transformer. Notably, the memory consumption is mostly driven by the weight parameters, which experience only a slight increase with HC. Additionally, given HC's low computational cost, the hidden states generated by HC can be discarded post forward pass and recomputed during backpropagation to further optimize memory usage. With this approach, the additional memory requirement is reduced to nsbd model . During inference, the memory usage for activations is largely determined by the Key-Value cache, which is not impacted by the extra activations brought by HC. Moreover, the hidden states from earlier layers can be released as soon as the next layer's computations start, significantly lowering memory requirements.]()

![Benchmarking class-conditional image generation on ImageNet 256×256, with cfg=1.50. NP, P, and R are short for Numerical Precision, Precision, and Recall, respectively.For the Large model (307M parameters), ViT/16 achieves 77.25% accuracy. The SHC and DHC configurations further enhance accuracy to 78.38% and 79.94%, respectively. This corresponds to relative improvements of 1.13% and 2.69%, with DHC showing the highest performance. These results demonstrate that hyper-connections (SHC and DHC) significantly improve accuracy, especially in the Large model scale.]()

![Accuracy on ImageNet. ViT*/16 refers to the results reported by(Dosovitskiy et al., 2020), whereas ViT/16 denotes our re-implemented baseline. SHC and DHC indicate that residual connections are replaced with static and dynamic hyper-connections, respectively.]()

![Training hyperparameters for ViT.]()

![Final Output:11: h L ← sum rows of H L 12: h L ← Normalization Layer(h L ) 13: y ← Output Layer(h L ) 14: return y Downstream Benchmarks for OLMoE.]()

![Results on downstream benchmarks for 1B models.]()

![Losses of V2 validation sets for 1B Model.]()

![Perplexities of V2 validation sets for 1B models.]()

&𝛽 " &

& 𝛽 $ &

