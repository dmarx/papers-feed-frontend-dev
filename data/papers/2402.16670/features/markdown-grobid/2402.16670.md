# Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance

## Abstract

## 

Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attentioncapturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.

## An unsustainable attention market

Since the advent of mass market in the 50's, media and advertisement providers have relentlessly tried to figure out effective methods to capture our attention and turn it into revenue. During the last two decades, supported by advances in artificial intelligence (AI), major online social media and Web platforms have brought this process of capturing attention to an unprecedented scale. Based almost exclusively on advertising revenues, their business model consists in providing free services that, in return, collect behavioral traces. This data is then used to maximize the impact of advertisements on users by 1 ensuring their mental availability at the time of being shown the advertisement, and 2 ensuring that the message meets their interests, beliefs and moods (i.e. targeted advertising). Based on research in psychology, sociology and neuroscience, several actors including online social media, games and Web platforms have engineered techniques capable of very effectively plundering our "available brain time" [[37,](#b36)[27]](#b26). We can distinguish two broad categories of such techniques. Firstly, some techniques are explicitly designed to leverage cognitive biases as a means to capture attention. For instance, the likes collected after posting content activate the brain's dopaminergic pathways (involved in the reward system) and tap into our need for social approval, giving "bright dings of pseudopleasure" [[37]](#b36); notifications of smartphone applications feed our appetite for novelty and surprise such that it is difficult to resist the urge to check them; the pull-to-refresh mechanism [[37]](#b36), alike slot machines, exploits the variable reward pattern whereby each time we pull down the screen we may get an update or nothing at all; infinite scrolling (of news, posts or videos...) traps us because of our fear of missing out important information (FOMO) to the point that we can hardly break the flow; automatic video chaining replaces a deliberate action to continue watching with a required action to stop watching, and entails a frustrating feeling of incompleteness when stopped; etc. Similarly, some techniques harness dark patterns [1](#foot_0)[[26]](#b25) to manipulate users into taking actions or decisions they wouldn't take otherwise. This is typically the case when one accepts all notifications of an application without really noticing it, while deactivating notifications would require an additional, less intuitive, series of actions.

Secondly, recent advances in machine learning allow the training of content recommendation algorithms on massive online behavioral data. These algorithms learn to recommend content that not only captures attention but also increases user engagement [2](#foot_1) . They discover the content's key features that help predict whether such content will effectively attract users' attention, and typically end up selecting content related to conflictuality, fear or sexuality [[10]](#b9). They also learn to exploit humans' negativity bias [[61,](#b61)[59]](#b59) and, as a consequence, content conveying high-arousal negative emotions (such as anger, resentment, indignation and disgust) are more likely to be read and eventually shared online than those conveying other emotions [[50,](#b50)[35]](#b34). Concerningly enough, false information (a broad term including misinformation and other forms of disin-formation) typically relies on such negative emotions as a trick to foster sharing. Finally, recommendation systems may do all this without it being explicit in the features they select, nor in the succinct feedback that some of them happen to provide [3](#foot_2) .

Since the amount of attention available is both limited and precious, it would be unsustainable for a civilization to waste it with impunity for questionable or futile purposes [[10]](#b9). Today, we might precisely be at that moment: while mental time has become a new oil, we have created an attention economy and subsequent attention markets [[28,](#b27)[29]](#b28) that, although sustainable from an economic point of view, may be unsustainable from a civilization point of view. From these first references, let us define what the term "attention market" refers to in this article.

Definition 1.1 (Attention market) Economic environment where businesses compete to capture and retain the resource represented by people's focused mental engagement that we call attention.

The attention market treats attention as a tradable commodity and involves multiple actors: from producers (the end users whose attention is the resource), to content creators whose work is used to capture the attention, brokers who trade and monetize the attention, and consumers who use it for their purposes such as exposing users to advertisements.

## ‚ùà

In this article, we propose a discussion aimed at spurring introspection and debate within the computer science community. In line with the Web Science Manifesto [[6]](#b5) calling for interdisciplinary approaches to prepare the future of the Web, we bring together and synthesize the conclusions of more than 70 papers and books from a wide range of disciplines to analyze the practices and drifts of these systems designed to capture attention on a worldwide scale. We make the point that, with the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of negative emotions tends to foster radicalization and polarization, amplify the dissemination of false information, spur the emergence of populism, and eventually put a threat on democracies and human societies in general.

Promoting awareness about these issues, this paper is directly related to UN's Sustainable Development Goal[foot_3](#foot_3) (SDG) 16 "Peace, Justice, and Strong Institutions", since it suggests actions to combat the instrumentalization of negative emotions, the associated false information that mechanically increase the level of anger and resentment among populations, and it promotes "societies that respect the right (...) to freedom of expression, and access to information" [5](#foot_4) . By pointing to the rise of populism worldwide, it addresses the connected question of how to strengthen institutions. The paper is also relevant with respect to SDG 3 "Good Health and Well-being" considering the aggravating effects of online social media on mental health, and the public health issues caused by false information (e.g. during the Covid-19 pandemics).

So far, the public and private research in computer science has invested large efforts in dealing with some aspects of the problem like radicalization, violent speech and false information. These works rely on post hoc measures such as content detection, deletion or downgrading. Nevertheless, we argue that additional measures must be considered to actively prevent the issues that stem from attention capturing rather than only mitigating their impact after they have occurred. Presumably, such measures would be political as well as technical, meaning that this socio-technical problematic situation calls for socio-technical solutions. And although the measures may not be associated with immediate research opportunities for the computer science community, we believe that the potential impacts are crucial enough for the community to be fully aware of, and actively involved in, this reflection.

In the rest of this paper we will first review the general principles of recommendation systems and the consequences of the recommendation loop that they implement (section 2). Then, we will explain how having recommendation systems harness emotions can lead to detrimental situations including what we shall name an algorithmic emotional governance (section 3). We will touch upon the threat to creative jobs (section 4) and then review some known post hoc measures (section 5), before discussing preventive measures to reclaim our attention (section 6).

## Users in the loop... of recommendation systems

Content recommendation algorithms are a key component of a wide range of applications, including social media, search engines and major Web platforms in general. Through many applications they have changed our lives, helping us to be more efficient, assisting us in daily tasks, or improving our education and information. In a number of other applications however, the truth in not so bright. In the case of social media for instance, they are presented to us as if designed to provide us with content that matches our needs and desires, while what they really seek is to maximize the attention we pay to their hosting platform and advertisements thereof. Through the training process, recommendation algorithms automatically learn to extract from massive behavioral traces the content's features that most effectively capture our attention and maximize our continuous engagement with the platform. For instance, they can learn that some categories of topics, such as conflictuality, fear or sexuality, irresistibly attract our attention [[10]](#b9), and thus lean toward recommending these particular categories. They can also learn to select content tailored for a certain user by taking into account the content's features (topics, source, emotions conveyed...) and its adequacy with the user's profile (interests, inclinations, past behavior...). This adequacy likely involves many other features that are not transparent since the platforms rarely inform users about how and for which purpose their personalized feed was composed. This is underlined in a study by DeVito [[18]](#b17) who analyzed Facebook's patents, press releases, and Securities and Exchange Commission filings, to identify "the set of algorithmic values that drive the News Feed". Some of the features he identified are objective, i.e. they can be observed or measured: friend relationships, explicitly expressed user interests, prior engagement, post age and page relationships. By contrast, other features are up to interpretation and thus raise multiple questions: implicitly expressed user preferences (what are the signals of such implicit expression?), platform priorities (what are they and who decides them?), content quality (what are the quality criterion?).

Finally, it may seem that recommendation algorithms learn to leverage psychological traits and cognitive biases. Yet, it is important to stress that the algorithm does not discover such things as a psychological trait or a cognitive bias itself. Rather, it discovers the features that enable it to exploit what psychologists would describe as a trait or bias. Such criteria are not explicitly formulated, they may not even be explicable nor verifiable. They remain implicit in the models unless a study be carried out a posteriori, that would surface the biases that emerge from the recommendations. This is yet another example where AI techniques without explanations nor feedback are problematic.

Another specificity of recommendation algorithms is that they tend to implement a self-reinforcing loop that we define as follows: Definition 2.1 (Self-reinforcing recommendation loop) The continuous cycle of recommendation systems providing personalized suggestions to a user based on data collected from their preferences and behaviors and integrating these to further recommendations.

A classical self-reinforcing recommendation loop is illustrated in figure [1:](#) 1 The algorithms recommend content to the user. [2](#b1) The behavior of the user is captured, possibly partially due to the focus of the platform and the limited choices that the interface offers, and possibly biased due to the fact that these choices may be oriented, again by the interest of the platform and the chosen interface. 3 The algorithms integrate these reactions in future recommendations. As a result, the reactions of the user will reinforce the recommendation and propagation of the attached content.

## Connect

Select [(1)](#b0) Perceive & React Trace (2) A.I. processing [(3)](#b2) Figure [1](#): The self-reinforcing recommendation loop of platforms: the ellipses are activities on the user side, the boxes are activities on the platform side. Select and Trace are grey boxes because only partially observable. The A.I. processing is, more than often, a black box for the end-user.

Of course there are externalities to that loop, that can increase its impact. Smartphones, for example, provide additional means to profile users by tracking their every moves, making recommendation even more efficient and targeted to the point that it competes and sometimes takes over more traditional ways of advertising [[65]](#b65). Another (detrimental) externality of this loop is that it opens the door to spoofing tech-niques and other malevolent actors intentionally biasing usage traces to "hijack" recommendation systems. Indeed, as soon as a process is known and documented, it runs the risk of being diverted from its original purpose and manipulated beyond its original objectives. For instance, fake reviews and reactions alter recommendations; black hat techniques of SEO (Search Engine Optimization) such as hidden texts, link farms, cloaking 6 or text spinning are disapproved by search engines as they impact the recommendations they make by unduly increasing the ranking of targeted pages or avoiding their downgrading.

## ‚ùà

As a result, the fact that a few recommendation systems influence a significant fraction of the human population may have a number of detrimental side effects on their users and our societies at large. A first side effect is that recommendation algorithms tend to lock users in an informational space in accordance with their tastes and beliefs, a "filter bubble" [[45]](#b45) that confines them to a "cognitive comfort zone" and activates their confirmation bias as they are faced with information which seems to go towards the same directions or conclusions [[55,](#b55)[34]](#b33) Eventually, users are no longer confronted with contradiction, debate nor disturbing facts or ideas, and this algorithmic amplification tends to be a powerful driver of the radicalization and polarization of opinions, leading to extremist ideas in some cases [[74]](#b74).

Furthermore, at a time where we need to change our behaviors (e.g. over-consumption of goods and energy) and redirect our attention to important matters (e.g. climate change), we should question whether recommendation algorithms make the right recommendations, and for whom. Considering the billions of users caught in recommendation loops everyday 7 , it is important to continuously monitor how and for what purpose these systems capture our attention. Because when our attention is spent on a content chosen by these platforms, it is lost for anything else.

## Algorithmic Emotional Governance

Considering the platform's recommendation loop introduced in section 2, we now want to stress that, directly or indirectly, emotions are a key feature of the selected recommendations. In fact, the whole attention market could be seen as driven by a complex equation involving, at least, emotions, cognitive biases and content recommendation algorithms. This could lead to what we will call here an algorithmic emotional governance merging two concepts: emotional governance [[48]](#b48) which is the informed management of the emotional dynamics of the governed population, and algorithmic governance [[54]](#b54) which is a governance of our societies based on the algorithmic processing of massive data. 6 Cloaking denotes a technique in which the content presented to a search engine crawler is different from that presented to an actual user. It aims at deceiving search engines so they display the page that they would otherwise downgrade or dismiss. Adapted from [https://en.wikipedia.org/wiki/Cloaking](https://en.wikipedia.org/wiki/Cloaking). 7 In 2018, Google revealed that 70% of the time spent watching videos on Youtube is about videos recommended by Youtube's algorithms. [https://qz.com/1178125/youtubes-recommendations-drive-70-of-what-we-watch](https://qz.com/1178125/youtubes-recommendations-drive-70-of-what-we-watch)  Emotions are a powerful attractor of our attention, especially emotions with a high negative valence [[61]](#b61). As a result, information that arouses anger, fear, indignation, resentment, frustration or disgust is among those that most effectively catch our attention [[50,](#b50)[35](#b34)]. An explanation is that witnessing others' negative emotions activates our comparison bias and subjects us to some sort of injunction to take sides, to show our emotional response, and hence publicly demonstrate our "irreproachable morality" [[17,](#b16)[10]](#b9). Note that catching attention and increasing user engagement are different things, and although high-arousal negative emotions catch attention more efficiently than other emotions, it remains unclear whether they induce a higher user engagement on social media. In some cases a higher sharing rate of information conveying positive emotions was observed [[33,](#b32)[36]](#b35). Nevertheless, in several contributions, researchers showed the overwhelming impact of emotions in argumentation and debates and the means to detect them [[5,](#b4)[68,](#b68)[4]](#b3), and it has also been shown that anger spreads faster on social media than any other emotion [[21]](#b20). Note that this attraction for negative content can be observed in completely different domains, e.g. in literature where the anti-utopian and dystopian fiction genres became more prominent within the utopian genre [[39]](#b38).

Combined together, the construction of filter bubbles by recommendation systems and the ability of these systems to learn the content's features that trigger a particular emotional response in a particular individual, can lead to some form of polarisation and end up trapping users in radicalization pathways. Consider the supporter of a sports club: it is because the system chooses the right topic (e.g. the right sport), the right content (e.g. an article about an opponent club) and the right tone and emotion (e.g. mocking criticism) that an emotion is provoked, followed by a registered reaction (like, comment, repost) and, over time, a potential polarisation is developed such as hatred for the opponent's supporters.

Recommendation after recommendation, the filter bubble becomes an opinion bubble where users are isolated from discrepant opinions, and eventually an emotion bubble where they are maintained in certain emotional states as the result of optimized recommendations. In the end, the complex interaction of negative emotions, cognitive biases (e.g. negativity bias and impulsive tendency to show indignation) and recommendation algorithms leads to an emotional escalation. Often, this escalation is further worsened by the affordances offered by the platforms, that tend to make exchanges ever briefer and more simplistic: How to express a nuanced reflection in a 280-character tweet? How to underline a doubt when the only available choices are essentially limited to / (and sometimes )? How to agree with one part of a post and disagree with another one when this post is treated as a monolithic block by the interface that only offers the options / /Ôá† ? This extreme discretization of choices adds to the mechanisms at work and reinforces the polarization of opinions and communities. Some dark patterns are even intentionally employed to make some actions easy and some more difficult: for instance, in Facebook the button to like a post is always visible whereas the option to report a post is at the bottom of a submenu, a pattern falling in the category known as "longer than necessary" [[7]](#b6).

Eventually, nuance, doubt or agnosticism are mechanically made invisible because the low emotional response that they induce simply downgrades their ranking. It is imperative to have an opinion, preferably definite and cleaving. Amplified by digital disinhibition 8 [[63]](#b63), this emotional escalation can lead to outpourings of violence and hatred whose outcome is sometimes tragic as attested by the suicide attempts of teenagers being cyberbullied [[56]](#b56). Moreover, the full consequences of triggering or regulating emotions on our cognitive functions in general and on memory in particular remain to be studied extensively [[49]](#b49).

## ‚ùà

We just described the combined effect of emotions, cognitive biases and recommendation algorithms, which is at work whatever the type of content a platform serves. But things get even worse when it comes specifically to false information. False information are frequently meant to arouse strong negative emotions [[75]](#b75), and the combination with cognitive biases and recommendation algorithms provides them with a particularly fertile ground and a formidable cognitive efficiency [[2,](#b1)[38]](#b37). Some studies reported that negatively biased fake news enhance users' willingness to share them [[14]](#b13), and reveal a positive correlation between the virality of fake news and the anger they carry [[12]](#b11). Another study contended that falsehood spreads "significantly farther, faster, deeper, and more broadly than the truth" on social media [[69]](#b69), which underlines that recommendation of content arousing negative emotions does not only induce local individual reaction: it creates a chain reaction leveraging the network effect of social media to spread that "content-emotion" couple through the acquaintance links. Other studies reported that recommendation algorithms mechanically tend to favor false information conveying divisive ideas, shocking events and negative emotions [[22,](#b21)[23]](#b22). This type of content entails a felt injunction to take sides and compulsively spread shocking information rather than appealing to critical thinking, questioning its veracity and verifying its source. And since this information is often relayed by acquaintances, the social proof bias [[13]](#b12) entices users to deem it credible and trustworthy.

Concerningly, the contents we are exposed to leave a trace in our implicit memory: although we cannot recall seeing it, it may impact our choices for several months [[15]](#b14). Even more concerning is the fact that, due to the negativity bias, negative information leaves a longer memory trace than positive information. Therefore, even when a false information is denied or rectified, there remains a negative feeling that stems from the strong emotional response it triggered in the first place. Repeated again and again, associated with representations of the world that summon conspiracy theories, reinforced under the pressure of filter and emotion bubbles, propelled by the network effect, such information gradually and insidiously un-dermines our trust in the elites (scholars, experts, journalists, politicians, etc.), entails risks for public health [[72,](#b72)[51]](#b51), and spurs the emergence of extreme ideas and populism that eventually undermine democracies [[74,](#b74)[1,](#b0)[30,](#b29)[23]](#b22), among other pitfalls.

Finally, let us stress that if "previous studies have shown how personality, values, emotions and vulnerability of users affect their likelihood to propagate misinformation" [[22]](#b21), in this section we only considered an average user without any particular health condition. But we should envisage more complex situations when it comes to users with disabilities or mental disorders e.g. depression, anxiety, compulsive shopping disorder, paranoia, FOMO, FOBO [9](#foot_5) ... Let us just mention one specific condition: the attention deficit (AD) disorder. There is evidence that AD symptoms could be worsened by the use of digital media and their attention-grabbing applications, and more importantly that these applications could provoke AD among people without previous record of such a disorder [[47]](#b47). To the very least, more research is needed in this respect.

## Attention, attention, all thinkers

We firstly intended this section for all the scientists reading this paper, concurring with the article of David R. Smith: "Attention, attention: your most valuable scientific assets are under attack" [[60]](#b60). In this article, Smith calls for attention to what media platforms are doing to research and the academic domain. Indeed, even the most informed scientists and engineers are not immune to these problems [[37]](#b36) such that digital contraptions (as Smith calls them) are contributing to academic attention deficit disorder [[60]](#b60). In fact, concentration but also boredom, mind-wandering and daydreaming times are vital to creative thinking. Many of us experienced the sudden burst of an idea in the middle of a relaxing moment. Attentioncapturing systems steal these moments from all of us and hamper the creativity process of wondering minds [[77]](#b77).

Of course these remarks can be generalized to many other activities requiring concentration, creativity and imagination, and one could wonder what digital contraptions are doing to politics, healthcare or education, for instance. To mention just one example, countless information media now report the cases of Youtubers experiencing a burnout [[46]](#b46), or musicians complaining that they spend more time making Tiktok videos to promote their music than actually creating music [[57,](#b57)[73]](#b73). This reveals that, to hook and keep the attention of content consumers, platforms also exercise some sort of visibility tyranny over content creators.

In other words: attention, attention, thinkers, we need to redesign the systems for our own needs, rather than the other way around, especially in creative jobs since the true currency of these jobs are ideas [[60]](#b60).

## Known Post Hoc Counter-measures

Among the various issues raised in the previous sections, the questions of false information, radicalization, hateful speech and bullying are among the most concerning, and therefore have been extensively addressed by the research community [[58]](#b58). In [[22]](#b21) authors identified three different points where recommendation systems can be adapted to tackle these issues: 1 pre recommendation, 2 within the recommendation model, and 3 post recommendation. Most of the current counter-measures to deal with false information lie in this third category. Below we touch upon some of them.

Firstly, to dyke the spread of false information as well as inappropriate content such as bullying, hateful or violent speech, social media and content hosting platforms have obligations that vary depending on the legislation and its jurisdiction [[24]](#b23). Measures range from content deletion and suspension of users spreading inappropriate content, to re-ranking of recommended items before presenting them to the user [[22]](#b21), flagging to indicate potentially deceptive content, etc. Yet, despite these various approaches, progress is still necessary. For instance, subtle violent content may be hard to detect as soon as it does not contain explicit hateful or violent terms, or when it uses sarcasm [[44]](#b43). Conversely, content may be erroneously assessed as abusive or illicit although it is in fact using irony to convey perfectly acceptable ideas. An in-depth analysis of implicit and subtlety in linguistic content remains an open question [[44]](#b43).

In addition, any action must carefully consider the dangers of transferring regulation and enforcement to private companies. [[62]](#b62) argue that over-filtering content is just as dangerous as letting bad content spread. Indeed, deletion and filtering may deviate from initial purpose to over-censorship of content if it becomes safer for the platforms to do so than take a risk of being sued. Furthermore, assessing the trustworthiness of information raises multiple ethical and political concerns: Who decides what is true or false? According to which criteria? Under whose control? Secondly, to mitigate the effect of false information, multiple post hoc measures rely on the impact of additional corrective content. For [[70]](#b70), pointing to a coherent alternative explanation, with references to expert and highly credible factual sources, remains a solid starting point. The authors describe the strategy of "observational correction" leveraging the fact that users who witness the correction of a misinformation item, but have not directly engaged with that item, are less affected by cognitive dissonance and are therefore more amenable to correction. This is consistent with the findings of [[8]](#b7) who suggest that exposing users to related stories that correct a post that contained misinformation will significantly reduce misperceptions. The impact of the correction can be further reinforced by explicitly pointing to the demographic similarity between the user and the authors of opposing content [[25]](#b24), which taps into the homophily effect [10](#foot_6) . In other words, we are more likely to accept the correction when it comes from someone who is socially close to us, e.g. having the same professional activity or background. [[70]](#b70) also suggest to multiply correction actions for each targeted content to reinforce the effect.

## Reclaiming our attention

The methods presented in the previous section all have one thing in common: they deal with the problems in a post hoc manner, that is, after these problems have occurred, with all the limitations that come with this "coming after". To go further however, we need to figure out measures, may they be legal, political or technical, capable of preventing the attention from being looted in the first place. More importantly, we need to consider this reflection not only from the perspective of regulating the attention consumers (the platforms and multiple intermediaries), but also from the perspective of the producers (the end-users) who want to reclaim their attention, especially in times when our attention is needed on a number of urgent matters. This involves actively preventing recommendation systems from finding ways to exploit our inner limitations and manipulate us through sometime ancient and deeply embedded structures of our brain (e.g. our striatum) [[9]](#b8).

Below we formulate a set of propositions stemming from the observations and findings reported in the previous sections. We organize them around the challenges that they address, together with suggestions made by other authors from multiple disciplines. Finally we extract from them a set of empirical principles that could be used do drive further works on good practices.

## The carrot and the stick

Taking the example of false information, [[70]](#b70) insist on the fact that a posteriori corrections are not sufficient and must happen as early as possible, that is, before misperceptions are entrenched. Besides, avoiding the algorithmic amplification effect of such information by recommendation systems requires to mitigate the popularity effect before its happens [[22]](#b21). But if online social media are required by law to combat false information, they have conflicting incentive to do it, not to say no incentive at all. Indeed, as we described in section 3, false information largely relies on negative emotions to capture users' attention. As such, they are very effective in fostering user engagement which is what online social media strive to obtain. Consequently, from an economic point of view, it is counterproductive for online social media to prevent the spread of false information. More generally, it is counter-productive for platforms to mitigate the popularity effect, mitigate the impact of negative emotions, or reduce filter bubbles and the subsequent polarization of opinions.

The authors of [[42]](#b41) suggest to rethink existing trade regulation laws such as antitrust and fair competition laws under the new realm of attention markets. They propose to enforce taxes on attention consumption to "disincentivize attention intermediaries from vacuuming up as much attention as possible", for instance by restraining the amount of advertisements that can be shown to a user, or reducing the deductibility of advertising expenditures from the companies' revenues to alleviate their taxes. They also propose to regulate the attention costs that can be charged, with the idea that if attention becomes less lucrative then financial resources will be redirected towards more lucrative markets, thus reducing the amount of attention being captured and traded.

In other words, things would not change without strong in-centives on one side, and disincentives on the other. We can summarize this in the following general principle:

## principle of the right incentive

At a Web governance level, we must leverage legal and economic means to drive platforms' practices towards desirable behaviours, while penalizing undesired behaviours.

## Usage regulation

Some of the measures meant to regulate the attention market lie in the way the services are consumed. Some legal measures could be taken to enforce a regulation of the daily use of Web platforms. As has already been done in some countries, laws could be voted to limit the daily time spent by users on certain services, especially among the youngest [[32,](#b31)[11]](#b10).

Another simple measure applies to video streaming platforms, that consists in imposing few-second pauses between videos. This apparently naive technique may actually shatter the infinite feed trap by giving users the short amount of time they need to realize that they have been in an attention tunnel for a while, and that they want to "reclaim" their attention. This can be generalized by formulating the following principle:

## principle of supported due diligence

All means should be provided to foster and update the due diligence of users. In particular they should always be made aware of their options to escape the systems' loops, processes and goals.

Policy makers could also tackle the problem of attention fragmentation entailed by the multiple, often invasive, notifications that smartphone applications raise. Whenever a notification occurs, users are tempted to interrupt their current activity, check the reason of the notification, possibly react to it, before eventually returning to their activity. It has been shown that switching our attention between tasks or contexts has a cost: it is time-consuming and creates a more error prone context [[31,](#b30)[40,](#b39)[52]](#b52). Furthermore it has even been shown that the mere presence of such devices, although turned off, impairs our cognitive capacity [[71]](#b71). In a way similar to the European General Data Protection Regulation (GDPR) [11](#foot_7) which imposes the consent of users for the use of cookies, law could impose that smartphone applications obtain users' explicit and informed consent for the notifications that they raise, and deactivate them by default ("opt-in only"). Hence the following principle:

## principle of opt-in by default

Recommendation and notification services should be turned off by default and only turned on on demand and after informed consent and preference setting This could be complemented by more punitive measures, as proposed in [[41]](#b40), for instance by demonetizing and forbidding collaboration with platforms that do not follow the rules.

## Content recommendation monitoring

The echo-chamber effect of recommendation algorithms is at the root of multiple examples of polarization and radicalization. It could be mitigated by imposing a certain share of non-recommended content, content that is outside of the user's interests, or content that originates from users they are not acquainted with. In this respect, some approaches lie in the second category proposed by [[22]](#b21), i.e. modifications "within the recommendation" system. For instance, the same authors suggest using clustering approaches to assemble the contacts of the user according to different levels of similarity with the user, and leverage these groups to increase the diversity of recommendations while maintaining a certain coherence and similarity. [[20]](#b19) propose a method to come up with relevant recommendations while reducing the likelihood of enticing the user towards radicalization pathways. Also, to counter the misuse and abuse of anger, indignation or fear, which are often associated with false information, platforms could be required to carry out sentiment analysis on every content in order to keep the amount of recommendations associated to negative emotions below a given threshold.

## principle of balanced recommendations

Recommendation-based platforms should prevent the over specialization of recommendations w.r.t. all features and should support monitoring and preventing the formation of bubbles of any type (opinion, source, emotion, etc.).

Moreover, there exists an asymmetry of visibility between the viral spreading of an information that was proven to be false or misleading, and the denial or rectification of that information. The denial of a false information usually puts forward a pondered, nuanced position that appeals to reasoning and facts (logos) over emotion (pathos). Hence, it does not trigger an emotional response compared to the one generated by the false information in the first place, and it is therefore silently downgraded by recommendation algorithms. This is commonly summarized by the so-called Bradolini's law which states that "the amount of energy needed to refute false information is an order of magnitude bigger than that needed to produce it." As a result, users who propagate false information often never get to know about their mistake. [[67]](#b67) insist on the fact that it is critical to jointly address content-checking and digital virality. Thus, to counterbalance this visibility asymmetry, social media could be required to impose on the denial/rectification of a false information a visibility equivalent to that of the initial information, for instance, by ensuring that the population who was exposed to the false information be exposed to the denial too. A warning could also be presented to users who propagated this false information in order to increase their awareness. Of course, this type of measure could be coupled with other post recommendation measures such as strategies involving the observational correction or demographic similarity presented in section 5. More generally, one has to figure out how we can use recommendation systems to recommend counter measures, i.e. we could train a recommendation system to learn the most relevant content and the most impactful entries in the acquaintance network to inject a counter measure.

## principle of balanced visibility

Recommendation-based platforms should ensure that preventive and corrective measures have a visibility at least equal to the visibility of the problems being prevented or corrected.

## Affordances and interaction design

As discussed in section 3, the affordances of platforms are optimized towards extremely brief and basic exchanges, leaving no room for nuance, pondering, doubt nor substantiated reasoning. Interfaces could be redesigned to facilitate non-binary reactions, starting with a range of nuanced emotions. Rather than implementing deceptive dark patterns, they could rely on nudges to gently drive users towards critical thinking, and by valuing/rewarding this kind of behavior. [[1]](#b0) recommend engaging users in the validation of content before sharing it, both manually and with automated analysis methods on content and context. For instance, X (formerly Twitter) asks confirmation before retweeting the link to an article that the user did not click. Similarly, interfaces could encourage users to comment on content instead of merely clicking , or , and they could question a user about whether they really want to share or support a content associated to strong negative emotions or for which a counter-measure was triggered.

## principle of benevolent interaction design

Affordances and interactions should be designed and evaluated with the well-being of end-users in mind first.

## Societal impact and educational mission

We, as a society, could decide that large online social media, because of their influence on the society, public opinion, public health and economy, can no longer be considered as sheer private companies regulated by markets law only. Instead, they could be seen as digital commons and be assigned a specific status that would endow them with a societal mission including an educational purpose, for instance. As an example, they could instruct users in detecting false or misleading information, they could promote content meant to increase awareness w.r.t. attention mechanisms and cognitive biases, foster critical thinking and "distill" the scientific method, etc. On the same page, authors of [[1]](#b0) insist on the need for civic education, and [[43]](#b42) recommend integrating democratic values into the algorithms that impact our lives, especially the ones participating in an algorithmic governance (e.g. platform used for debates, for information, for legal actions, educational orientation) which, in our case, means going beyond the optimization of user engagement and attention catching, and including ethical criteria.

## principle of digital commons preservation

When a digital service, platform or resource reaches the potential of having a world-wide impact on human societies, it must be assigned the status of digital common and must be subjected to preservation rules and policies.

## Feedback and transparency enforcement

Since one of the pitfalls we identified is the fact that users are being caught by the recommendation loops, approaches such as quantified self and lifelog could be specialized to the case of recommendation-based platforms, in order to foster awareness and introspection. Self-tracking tools could provide users with usage metrics and feedback with respect to the total time spent on the platform, the total exposure to negative news, etc. This could be a way to counter the fact that online sharing of fake news increases with social media fatigue [[64]](#b64). Indicators could inform users about the diversity of the recommendations they are shown, and make them aware of low-diversity risks. For instance the fact that "90% of the content one sees come from 10% of one's contacts or are on the same topic" may indicate that one is experiencing a filter bubble. We summarize this in the principle below where user's reflexivity is the ability of users to be self-aware of their usage and engagement with the system.

## principle of continuous reflexivity

Users must be provided a continuously updated feedback on their usage of the system and on themselves to support their reflexivity and maintain an up-to-date informed consent.

Among other measures, the European Digital Services Act [[66]](#b66), that took effect in August 2023, requires that platforms set up mechanisms to explain the reasons that led to recommending a certain content, and to offer users an alternative recommendation not based on profiling. Such measures are especially crucial when coupling AI and the Web since we need to set transparency and explanation as a prerequisite to any approach, to ensure the awareness and informed consent of, potentially, billions of users [[6]](#b5).

## principle of full user awareness

Users must be made aware of all the features and motivations leading to a recommendation, before and when it is provided.

## Build on existing practices

Finally, and although it may seem obvious, one rule is worth remembering: to review and take inspiration from existing best practices in other domains. In most jurisdictions there exist advertising laws to protect consumers, ensure they remain able to make informed decisions, and more generally to maintain a level playing field[foot_8](#foot_8) between all players. Most countries regulate advertising through legislations that target different forms of false, misleading or deceptive advertising contents and claims, and forbid a whole range of practices (unsubstantiated comparison, forged testimonial, puffery, misleading packaging/label, unsolicited commercial messages, alleged contests and sweepstakes, etc.). The work and literature on regulating advertising should be reviewed and built-upon in regulating the attention market at large. This topic is also close to that of clickbaits that are recommended links designed to attract attention and to entice users to follow them while being typically deceptive, sensationalized, or otherwise misleading. Clickbaits are not just teasers but headlines with an element of dishonesty, "using enticements that do not accurately reflect the content being delivered" [13](#foot_9) . As far as we know, there is no regulation of clickbait practices on the Web, although some of these techniques bear similarities with the misleading or deceptive advertisement practices that we just mentioned and that, on the contrary, are regulated.

## principle of best practice transfer

Methods and tools used to regulate similar situations in relevant domains should be surveyed, benchmarked and systematically considered as input to a Web governance.

To give another example coming from a completely different angle, we know that parenting practices in terms of TV viewing have an impact on the behaviour of young watchers [[3]](#b2). Again, approaches and good practices in this domain, and more generally in educating and parenting in the digital media age [[16]](#b15), must be considered in the case of "Web viewing" in general and when addressing the problem of attention capturing in particular.

More generally speaking, we need to put in place a governance bodies, starting with the Web and AI, that are prepared to tackle new problematic practices and regulate them, as is done in other areas of activity. And we also need to keep a constant watch on these other areas, if only to draw inspiration from the initiatives and feedback they have on similar issues. Taking the example of the video game industry, there is evidence of a relationship between "loot box" [14](#foot_10) spending and gambling addiction [[76]](#b76), and that a loot box is psychologically akin to gambling [[19]](#b18) and can result in addictive behaviors and endangered players. The way to study and address that unwanted exploitation of our behaviors is inspirational for other problematic practices on the Web such as those we surveyed.

7 Thank you for... your attention AI is domain-independent. It is being applied in all our areas of interest: information, business, money, politics, employment, sports, games, sex etc. And the worldwide deployment of these techniques, partly due to its coupling with the Web, could have detrimental consequences in all these areas alike, unless properly regulated. This is a commonplace observation but it is the reason why, to prevent such detrimental effects, an ethical AI approach to AI governance must be multidisciplinary and interdisciplinary.

With this mindset, this paper brought together conclusions from more than 70 articles and books from different disciplines (psychology, sociology, neuroscience, politics, legal domain, computer science, education, etc.) to analyze and call for actions against the current practices competing to capture our attention in several "Web Wild West corners". The problem is both critical and complex, and authors of [[34]](#b33) defend the need for a "nuanced multidimensional view of how social media use may shape information consumption" and they urge us to consider "the complex variety in social media platforms [and the] considerable variation in observed impacts among them". In [[22]](#b21) authors add that "This research requires to navigate the careful tension between privacy, security, economic interests, censorship and cultural differences, and requires to be addressed from multiple disciplines that can assess not only the technological aspect, but also the individual and the social one (...) There is ample room for investigation (...), opening a novel, exciting and interdisciplinary line of research."

At the same time, the problem is getting worse with every technological innovation. The pervasiveness of smartphones in our lives has further reinforced the effectiveness of these techniques that can now grab our attention at every moment of the day, and in particular these moments that were previously those of boredom, waiting, daydreaming or intellectual strolling. As we pointed out in section 4, these moments are known to be necessary to spur imagination and creativity. In the continuation of smartphones, smart objects and the resulting internet of things and Web of things will only make things worse.

Recommendation systems that learn to predict us effectively learn to manipulate us, and to be predictable is to lose freedom. Everyday we fuel the predictors in exchange for immediate satisfaction and instant pleasure, this amounts to continually mortgaging our freedom. Besides, these systems that compete for our attention end up pressuring us to consume and to react more and more quickly to their recommendations. And, as we know, acceleration is a form of alienation [[53]](#b53).

In another context and to address our own human limitations, [[10]](#b9) recommended to find ways to increase our overall level of consciousness and reclaim the power of long-term reflection. Our leaders and role models 15 struggle to embody the values of patience, conscience and moderation [[10]](#b9), but our computer systems rarely drive us in that direction either. On the contrary, current AI applications are pushing us not to use our conscience, but to play their automation game. Yet there is no reason for these systems to live in our mind rent free and it is urgent to redesign them so they regularly push us to take a step back, to be more conscious of what we are doing, viewing, saying, spreading, etc. The challenge is to (re)take and (re)give time for awareness, attention and reflection: we need to (re)take that source of freedom. And for this, we proposed a non-exhaustive first set of principles to (re)design Web applications and inscribe in them a set of agreed-upon values.

![Algorithmic emotional governance) The governance of societies based on algorithms processing massive data to harness the emotional dynamics of the governed population.]()

The legal definition in California is "A user interface is a dark pattern if the interface has the effect of substantially subverting or impairing user autonomy, decision-making, or choice. A business's intent in designing the interface is not determinative in whether the user interface is a dark pattern, but a factor to be considered." CPRA ¬ß 7004 (c)

There are multiple definitions of user engagement. In the context of social media, this typically refers to the fact that a user would interact with a content: e.g. like, comment or repost it. Engagement is usually public in that it leaves public traces on the platform, unlike sheer content consumption that remains private[[50]](#b50).

For instance, a recommendation system may tell us "you liked this movie, you may also like this one". But we don't know what features were selected to recommend this one: Do they have an actor in common? Did my contacts like both of them? etc.

https://www.un.org/sustainabledevelopment/

https://www.un.org/sustainabledevelopment/peace-justice/

Fear of Better Options: the inability to choose when faced with a multitude of options.

Homophily: the tendency to associate with similar others.

GeneralData Protection Regulation (GDPR) https://gdpr-info.eu/

Metaphor denoting the fact that, in business, all players compete fairly, i.e. they all play by the same set of rules. https://en.wikipedia.org/wiki/Level playing field

Definition adapted from https://en.wikipedia.org/wiki/Clickbait

"loot boxes" are video game items with randomized contents that can be paid for with real-world money.

