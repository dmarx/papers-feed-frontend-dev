- **Sparse Autoencoders (SAEs)**: Single hidden layer networks that reconstruct activations with a sparsity penalty; consist of an encoder and decoder.
- **Transcoders**: Approximate input-output functions of network components using a sparse bottleneck; enable fine-grained circuit analysis.
- **Skip Transcoders**: Introduce an affine skip connection to transcoders, improving reconstruction error without compromising interpretability.
- **Reconstruction vs. Interpretability Tradeoff**: Higher sparsity generally increases reconstruction error; transcoders and skip transcoders achieve better interpretability for the same reconstruction loss compared to SAEs.
- **Training Methodology**: Trained using mean squared error on MLP layers of various models (Pythia, Llama, Gemma) with TopK activation function for desired sparsity.
- **Automated Interpretability Pipeline**: Utilizes Llama 3.1 70b to generate explanations and scores for sparse coder latents; evaluates performance on SAEBench.
- **Feature Absorption Phenomenon**: General features can absorb specific features, affecting interpretability; evaluated using SAEBench.
- **Key Findings**: Skip transcoders Pareto-dominate SAEs in terms of reconstruction and interpretability; competitive performance in sparse probing.
- **Future Directions**: Explore the role of skip connections in circuit detection and feature learning; potential for improving circuit reconstruction with skip transcoders.
- **Formula for Skip Transcoder**: 
  \[
  f(x) = W_2 \text{TopK}(W_1 x + b_1) + W_{\text{skip}} x + b_2
  \]
  - \(W_2\) and \(W_{\text{skip}}\) are zero-initialized; \(b_2\) initialized to empirical mean of MLP outputs.