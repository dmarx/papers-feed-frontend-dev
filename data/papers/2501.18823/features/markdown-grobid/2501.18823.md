# Transcoders Beat Sparse Autoencoders for Interpretability

## Abstract

## 

Sparse autoencoders (SAEs) extract humaninterpretable features from deep neural networks by transforming their activations into a sparse, higher dimensional latent space, and then reconstructing the activations from these latents. Transcoders are similar to SAEs, but they are trained to reconstruct the output of a component of a deep network given its input. In this work, we compare the features found by transcoders and SAEs trained on the same model and data, finding that transcoder features are significantly more interpretable. We also propose skip transcoders, which add an affine skip connection to the transcoder architecture, and show that these achieve lower reconstruction loss with no effect on interpretability.

## Introduction

Recently, large language models have achieved human-level reasoning performance in many tasks [(Guo et al., 2025)](#b21). Interpretability aims to improve the safety and reliability of these systems by understanding their internal mechanisms and representations. While early research attempted to produce natural language explanations of individual neurons [(Olah et al., 2020;](#b32)[Gurnee et al., 2023;](#b22)[2024)](#b31), it is now widely recognized that most neurons are "polysemantic", activating in semantically diverse contexts [(Arora et al., 2018;](#b0)[Elhage et al., 2022)](#b16).

Sparse autoencoders (SAEs) have emerged as a promising tool for partially overcoming polysemanticity, by decomposing activations into interpretable features [(Bricken et al., 2023a;](#)[Templeton et al., 2024b;](#)[Gao et al., 2024)](#b19). SAEs are single hidden layer neural networks trained with the objective of reconstructing activations with a sparsity penalty [(Bricken et al., 2023a;](#)[Rajamanoharan et al., 2024)](#b35), sparsity constraint [(Gao et al., 2024;](#b19)[Bussmann et al., 2024)](#b10), or an information bottleneck [(Ayonrinde et al., 2024)](#b1). They consist of two parts: an encoder that projects activations into a sparse, high-dimensional latent space, and a decoder that reconstructs the original activations from the latents. [Bricken et al. (2023a)](#) introduced a technique of evaluating the interpretability of SAEs by simulating them with an LLM-based scorer, similar to what had been done on neurons [(Bills et al., 2023)](#). This approach is commonly called automated interpretability, or autointerp. SAE features perform much better on this benchmark compared to neurons, even when neurons are "sparsified" by selecting only the top-k most active neurons in a layer for analysis [(Paulo et al., 2024)](#b33). One problem with SAEs is that they focus on compressing intermediate activations rather than modeling the functional behavior of network components (e.g., feedforward modules).

Transcoders are an alternative to sparse autoencoders, initially proposed in [Li et al. (2023)](#b28) and [Templeton et al. (2024a)](#), and first rigorously evaluated by [Dunefsky et al. (2024)](#b15). Unlike SAEs, transcoders approximate the inputoutput function of a target component, such as an an MLP, using a sparse bottleneck. [Dunefsky et al. (2024)](#b15) demonstrate that transcoders enable fine-grained circuit analysis by learning input-invariant descriptions of component behavior, complementing automated circuit discovery tools like [Conmy et al. (2023)](#b13).

Transcoder design faces inherent challenges. While ReLU MLPs carve up the input space into polytopes,[foot_0](#foot_0) with each polytope corresponding to a relatively high-rank linear function [(Black et al., 2022)](#b6), transcoders' sparse activations mean that each activation pattern corresponds to a lowrank linear transformation. Furthermore, since [Marks et al. (2024)](#b29) and [Bricken et al. (2023b)](#), new benchmarks for sparse feature evaluation have emerged [(Gao et al., 2024;](#b19)[Karvonen et al., 2024;](#b26)[Juang et al., 2024)](#b25), motivating a broader evaluation of transcoders across models and tasks. We investigate the tradeoff between reconstruction error and interpretability by comparing SAEs and transcoders and address challenges mentioned above by proposing an architectural improvement-the skip transcoder-which mitigates rank limitations via an affine skip connection. In this work, we:

1. Introduce skip transcoders, which reduce reconstruction error without compromising interpretability.

2. Compare transcoders, skip transcoders, and SAEs across diverse models (up to 2B parameters), showing skip transcoders Pareto-dominate SAEs on reconstruction vs. interpretability tradeoffs.

3. Evaluate transcoders on SAEBench [(Karvonen et al., 2024)](#b26) demonstrating improved quality in both latentlevel phenomena like absorption and performance on various tasks through sparse probing.

We conclude that interpretability researchers should shift their focus away from sparse autoencoders trained on the outputs of MLPs and toward (skip) transcoders.

## Methods

Skip transcoders add a linear "skip connection" to the transcoder, which we find improves its ability to approximate the original MLP at no cost to interpretability scores. Specifically, the transcoder takes the functional form

$f (x) = W 2 TopK(W 1 x + b 1 ) + W skip x + b 2 (1)$Both W 2 and W skip are zero-initialized, and b 2 is initialized to the empirical mean of the MLP outputs, so that the transcoder is a constant function at the beginning of training. We leave a deeper analysis of the skip connection, perhaps interpreting it using SVD [(Millidge & Black, 2022)](#), for future work.

## Training

We train a collection of sparse coders: sparse autoencoders (SAE), sparse transcoders (ST), and sparse skip transcoders (SST), on the MLP layers of Pythia 160M [(Biderman et al., 2023)](#b4). We also train SAEs and SSTs on Llama 3.2 1B and Gemma 2 2B. We train with mean squared error between the output of the sparse coder and the MLP output, with no auxiliary loss terms. Unlike prior work on transcoders, we adopt the state-of-the-art TopK activation function proposed by [Gao et al. (2024)](#b19), which directly enforces a desired sparsity level on the latent activations without the need to tune an L1 sparsity penalty. We sweep across k values of 32, 64, and 128 in our experiments.

For sparse coders trained on Pythia, we train over the first 8B tokens of Pythia's training corpus, the Pile [(Gao et al., 2020)](#b18).

For the other models, we use 8B tokens of the RedPajama v2 corpus [(Computer, 2023)](#b12). All sparse coders are trained using the Adam optimizer [(Kingma & Ba, 2015)](#b27), a sequence length of 2049, and a batch size of 64 sequences.

## Evaluation

We use the automated interpretability pipeline released by [Paulo et al. (2024)](#b33) to generate explanations and scores for sparse coder latents. Activations of latents were collected over 10M tokens, sampled from the Pile for the Pythia models and from FineWeb [(Penedo et al., 2024)](#b34) for Llama and Gemma. The explanations were generated by showing an explainer model, Llama 3.1 70b, 40 activating examples, four from each of ten different quantiles. Each example had 32 tokens, and the active tokens were highlighted. Detection and fuzzing scores were computed over 50 activating We also use the SAEBench repository [(Karvonen et al., 2024)](#b26) to evaluate the sparse coders. We use it to compute the variance explained and the cross-entropy loss increase over 500K tokens of the OpenWebText corpus [(Gokaslan et al., 2019)](#b20). SAEBench also provides the ability to train and evaluate sparse probes that measure the ability of the SAE's encoder to select information relevant to classification tasks such as sentiment and language detection.

Recently, [Chanin et al. (2024)](#b11) drew attention to the phenomenon of feature absorption. In some cases, a more general feature like starts with the letter L appears alongside a specific feature like the token "lion", which may prevent the general feature from being active in contexts where intuitively, both the general and the specific feature apply. They argue that this is undesirable. We use SAEBench to compute the frequency of absorption of general letter features into specific features, in SAEs, STs, and STSs.

## Skip Transcoders Pareto Dominate SAEs

The utility of any sparse coding method for interpretability lies in its ability to accurately reconstruct activations while also generating human-interpretable latent features. This is a fundamental tradeoff: while sparser latents are generally more interpretable, higher sparsity also tends to increase the reconstruction error. The reconstruction error of a sparse coder can be viewed as "dark matter" containing features not captured by the latents [(Engels et al., 2024)](#b17).

Following earlier work on SAEs, we can represent this tradeoff using a reconstruction vs. interpretability curve (Figure [1](#fig_0)). Here we compare the reconstruction loss of different models, varying the number number of latents and sparsity, with their interpretability scores, measured as the average between detection and fuzzing score over a set of features. We find transcoders and skip transcoders with the same number of latents generally have higher interpretability scores for the same reconstruction loss than SAEs.

Not only are the average interpretability scores of transcoders and skip transcoders higher than those of SAEs but their distribution is narrower (Figure [2](#fig_1), left panel). Latents of (skip) transcoders also seem to represent more monosemantic features, as the explanations found hold for larger portion of the activation distribution. This can be seen by comparing the accuracy of explanations in examples sampled from different quantiles of the activation distribution (Figure [2](#fig_1) We replicated these results in models of the same architecture but different sizes, Pythia 160m and Pythia 410m, and on larger models with different architectures, Llama 3.2 1B [(Dubey et al., 2024)](#b14) and Gemma 2 2B [(Team et al., 2024)](#b36). On all cases studied, SSTs had higher automated interpretability scores and lower CE loss increase when patched in, see Table [1](#tab_0).

We found that performance on sparse probing was similar for SSTs and SAEs (Appendix A), with SSTs winning out for later layers by a small margin. Sparse probing measures the ability of SAEs to preserve information in the original latent, but for transcoders the latents should relate more to concepts necessary for processing the input. It is thus surprising that they are competitive with SAEs on compressing the residual stream without being trained with that objective. We also find that SAEs and transcoders have similar feature absorption behavior, but that those results are noisy. We don't expect this to be a problem, since there are other methods orthogonal to ours which seem to improve feature absorption; see discussion in Section 5.

## Conclusion

Our experiments suggest that interpretability researchers should shift their focus from sparse autoencoders trained in the outputs of MLPs to (skip) transcoders. In our view, the only downside of transcoders compared to sparse autoencoders is that SAEs can be trained directly on the residual stream, while transcoders need to be trained on particular components of the model (usually a feedforward layer). However, one can easily convert a skip transcoder trained on an FFN into a "residual stream transcoder" by adding the identity matrix to its skip connection. In this way, skip transcoders can be viewed as bridging the gap between these two types of sparse coding. Additionally, it is known that SAEs trained on nearby layers in the residual stream learn very similar features, effectively wasting training compute, while SAEs trained on nearby FFNs learn disjoint sets of features [(Balagansky et al., 2024)](#b2). For this reason, we suggest that practitioners who are planning to train more than one sparse coder on a model should consider training transcoders on FFNs in lieu of SAEs on the residual stream.

## Future work

As we have shown, transcoders preserve more of a model's behavior and produce more interpretable latents. We believe skip connections let the transcoder avoid the redundant work of translating the linear map, letting it focus on learning important features. Future work may illuminate the role of the skip connection by comparing it to a learned or analytically derived affine approximation of the MLP component. [Dunefsky et al. (2024)](#b15) highlights the usefulness of transcoders for circuit detection.While we have not run experiments on circuit analysis like in that paper, we expect that skip transcoders to be better for reconstructing circuits thanks to their lower reconstruction error. It is unlikely that the skip connection impedes gradient-based circuit discovery: work like [(Marks et al., 2024)](#b29) shows ways of incorporating linear skip connections into circuit discovery faithfully.

Transcoder and skip transcoder features may be used for steering, but we could not translate the unlearning and concept erasure benchmarks from [Karvonen et al. (2024)](#b26), which require the latent to contain all information in any given residual stream position.

Transcoders also do not help improve the feature learning in SAEs the way new architectures like [Gao et al. (2024)](#b19) do. They merely change the objective of the SAE, which is something that cannot lessen inefficiencies in training. We see the effects of this in the evaluation results on feature absorption: transcoders and skip transcoders can exhibit it just as much as SAEs. Further, feature density plots do not exhibit significant differences Appendix B, showing that SAEs and SSTs are similar on a mechanistic level. Work like Matryoshka SAEs [(Bussman et al., 2024;](#b9)[Nabeshima, 2024)](#b31) may help tackle these issues for both SAEs and SSTs.

## Contributions

Nora Belrose had the idea that transcoders might be a superior architecture to SAEs, and came up with the idea of skip transcoders. Gonc ¸alo

Paulo trained most of the SAEs and transcoders, and performed the experiments and data analysis. Gonc ¸alo and Stepan wrote the first draft. Nora Belrose, Stepan, and Gonc ¸alo wrote the final version. Nora Belrose provided guidance and suggested experiments. Gonc ¸alo, Nora, and Stepan are funded by a grant from Open Philanthropy. We thank Coreweave for computing resources. 7. Code availability. Code for training transcoders and skip transcoders is available in the sparsify GitHub repo. The skip transcoder checkpoints for Llama 3.2 1B are available on the HuggingFace Hub here, and others will be uploaded to the Hub soon.

## Impact Statement

This paper presents work whose goal is to advance the field of Mechanistic Interpretability. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## A. SAEBench results

This section contains results on SAEBench [(Karvonen et al., 2024)](#b26). We run three of the evaluations: core (reconstruction quality), sparse probing and absorption. We describe all three in the main body and point out that it is not expected for transcoders to outperform SAEs on absorption.

Variance Table 4. Results for llama-1B.∆ NLL represents the increase in cross-entropy loss.

![Figure 1. Skip transcoders are a Pareto improvement on interpretability vs performance degradation. We compare the increase in cross-entropy loss of 3 different sizes of SAEs and transcoders, 32768 (top right), 65536 (bottom left) and 131072 (bottom right), when patched into the model. For all sizes, skip transcoders are better than transcoders and sparse autoencoders, having both lower increase in model loss and a higher average auto interpretability score. On each quadrant we show 3 models that were trained with a different number of active latents, 32, 64 and 128, except for the 65536 latent model, which only has 32 and 64. The auto interp score is defined as the average fuzzing and detection score of c.a. 500 latents.]()

![Figure 2. Interpretability of latents and generalization of explanations. The interpretability scores of both detection and fuzzing are higher for skip transcoders and transcoders when compared to SAEs, with the distribution being wider for SAEs. Dots in the left plot indicate the average score. The accuracy of the explanations on examples sampled from different quantiles of the activation distribution we can observe that The accuracy of explanations remains higher even for lower quantiles, where the activations are smaller, showing that transcoder and skip-transcoder latents are probably representing more monosemantic concepts along the full distribution.]()

![Performance of sparse coders on different models We compute different interpretability scores, fuzzing, detection and simulation, for SAEs and SSTs trained on different models, as well as the increase of cross-entropy loss when patched into the model. 500 latents are used for fuzzing and detection, but only 50 latents are used for simulation due to it being more computationally expensive. 0.5M tokens are used to compute the cross-entropy loss increase.]()

![Results for gemma-2-2B. ∆ NLL represents the increase in cross-entropy loss.73.7 36.4 ± 28.9 30.5 ± 29.9 33.9 ± 25.1]()

![Results for pythia-160m. ∆ NLL represents the increase in cross-entropy loss.]()

The polytope interpretation can also be applied, in a somewhat modified form, to MLPs with other activation functions(Balestriero & Baraniuk,  

2019).

