<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transcoders Beat Sparse Autoencoders for Interpretability</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-12">12 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">¸alo</forename><surname>Gonc</surname></persName>
							<email>&lt;goncalo@eleuther.ai&gt;.</email>
						</author>
						<author>
							<persName><surname>Paulo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stepan</forename><surname>Shabalin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nora</forename><surname>Belrose</surname></persName>
						</author>
						<title level="a" type="main">Transcoders Beat Sparse Autoencoders for Interpretability</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-12">12 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">99B530D16462E93C1D6CF0B7592337D0</idno>
					<idno type="arXiv">arXiv:2501.18823v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparse autoencoders (SAEs) extract humaninterpretable features from deep neural networks by transforming their activations into a sparse, higher dimensional latent space, and then reconstructing the activations from these latents. Transcoders are similar to SAEs, but they are trained to reconstruct the output of a component of a deep network given its input. In this work, we compare the features found by transcoders and SAEs trained on the same model and data, finding that transcoder features are significantly more interpretable. We also propose skip transcoders, which add an affine skip connection to the transcoder architecture, and show that these achieve lower reconstruction loss with no effect on interpretability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, large language models have achieved human-level reasoning performance in many tasks <ref type="bibr" target="#b21">(Guo et al., 2025)</ref>. Interpretability aims to improve the safety and reliability of these systems by understanding their internal mechanisms and representations. While early research attempted to produce natural language explanations of individual neurons <ref type="bibr" target="#b32">(Olah et al., 2020;</ref><ref type="bibr" target="#b22">Gurnee et al., 2023;</ref><ref type="bibr" target="#b31">2024)</ref>, it is now widely recognized that most neurons are "polysemantic", activating in semantically diverse contexts <ref type="bibr" target="#b0">(Arora et al., 2018;</ref><ref type="bibr" target="#b16">Elhage et al., 2022)</ref>.</p><p>Sparse autoencoders (SAEs) have emerged as a promising tool for partially overcoming polysemanticity, by decomposing activations into interpretable features <ref type="bibr">(Bricken et al., 2023a;</ref><ref type="bibr">Templeton et al., 2024b;</ref><ref type="bibr" target="#b19">Gao et al., 2024)</ref>. SAEs are single hidden layer neural networks trained with the objective of reconstructing activations with a sparsity penalty <ref type="bibr">(Bricken et al., 2023a;</ref><ref type="bibr" target="#b35">Rajamanoharan et al., 2024)</ref>, sparsity constraint <ref type="bibr" target="#b19">(Gao et al., 2024;</ref><ref type="bibr" target="#b10">Bussmann et al., 2024)</ref>, or an information bottleneck <ref type="bibr" target="#b1">(Ayonrinde et al., 2024)</ref>. They consist of two parts: an encoder that projects activations into a sparse, high-dimensional latent space, and a decoder that reconstructs the original activations from the latents. <ref type="bibr">Bricken et al. (2023a)</ref> introduced a technique of evaluating the interpretability of SAEs by simulating them with an LLM-based scorer, similar to what had been done on neurons <ref type="bibr">(Bills et al., 2023)</ref>. This approach is commonly called automated interpretability, or autointerp. SAE features perform much better on this benchmark compared to neurons, even when neurons are "sparsified" by selecting only the top-k most active neurons in a layer for analysis <ref type="bibr" target="#b33">(Paulo et al., 2024)</ref>. One problem with SAEs is that they focus on compressing intermediate activations rather than modeling the functional behavior of network components (e.g., feedforward modules).</p><p>Transcoders are an alternative to sparse autoencoders, initially proposed in <ref type="bibr" target="#b28">Li et al. (2023)</ref> and <ref type="bibr">Templeton et al. (2024a)</ref>, and first rigorously evaluated by <ref type="bibr" target="#b15">Dunefsky et al. (2024)</ref>. Unlike SAEs, transcoders approximate the inputoutput function of a target component, such as an an MLP, using a sparse bottleneck. <ref type="bibr" target="#b15">Dunefsky et al. (2024)</ref> demonstrate that transcoders enable fine-grained circuit analysis by learning input-invariant descriptions of component behavior, complementing automated circuit discovery tools like <ref type="bibr" target="#b13">Conmy et al. (2023)</ref>.</p><p>Transcoder design faces inherent challenges. While ReLU MLPs carve up the input space into polytopes,<ref type="foot" target="#foot_0">foot_0</ref> with each polytope corresponding to a relatively high-rank linear function <ref type="bibr" target="#b6">(Black et al., 2022)</ref>, transcoders' sparse activations mean that each activation pattern corresponds to a lowrank linear transformation. Furthermore, since <ref type="bibr" target="#b29">Marks et al. (2024)</ref> and <ref type="bibr">Bricken et al. (2023b)</ref>, new benchmarks for sparse feature evaluation have emerged <ref type="bibr" target="#b19">(Gao et al., 2024;</ref><ref type="bibr" target="#b26">Karvonen et al., 2024;</ref><ref type="bibr" target="#b25">Juang et al., 2024)</ref>, motivating a broader evaluation of transcoders across models and tasks. We investigate the tradeoff between reconstruction error and interpretability by comparing SAEs and transcoders and address challenges mentioned above by proposing an architectural improvement-the skip transcoder-which mitigates rank limitations via an affine skip connection. In this work, we:</p><p>1. Introduce skip transcoders, which reduce reconstruction error without compromising interpretability.</p><p>2. Compare transcoders, skip transcoders, and SAEs across diverse models (up to 2B parameters), showing skip transcoders Pareto-dominate SAEs on reconstruction vs. interpretability tradeoffs.</p><p>3. Evaluate transcoders on SAEBench <ref type="bibr" target="#b26">(Karvonen et al., 2024)</ref> demonstrating improved quality in both latentlevel phenomena like absorption and performance on various tasks through sparse probing.</p><p>We conclude that interpretability researchers should shift their focus away from sparse autoencoders trained on the outputs of MLPs and toward (skip) transcoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>Skip transcoders add a linear "skip connection" to the transcoder, which we find improves its ability to approximate the original MLP at no cost to interpretability scores. Specifically, the transcoder takes the functional form</p><formula xml:id="formula_0">f (x) = W 2 TopK(W 1 x + b 1 ) + W skip x + b 2 (1)</formula><p>Both W 2 and W skip are zero-initialized, and b 2 is initialized to the empirical mean of the MLP outputs, so that the transcoder is a constant function at the beginning of training. We leave a deeper analysis of the skip connection, perhaps interpreting it using SVD <ref type="bibr">(Millidge &amp; Black, 2022)</ref>, for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Training</head><p>We train a collection of sparse coders: sparse autoencoders (SAE), sparse transcoders (ST), and sparse skip transcoders (SST), on the MLP layers of Pythia 160M <ref type="bibr" target="#b4">(Biderman et al., 2023)</ref>. We also train SAEs and SSTs on Llama 3.2 1B and Gemma 2 2B. We train with mean squared error between the output of the sparse coder and the MLP output, with no auxiliary loss terms. Unlike prior work on transcoders, we adopt the state-of-the-art TopK activation function proposed by <ref type="bibr" target="#b19">Gao et al. (2024)</ref>, which directly enforces a desired sparsity level on the latent activations without the need to tune an L1 sparsity penalty. We sweep across k values of 32, 64, and 128 in our experiments.</p><p>For sparse coders trained on Pythia, we train over the first 8B tokens of Pythia's training corpus, the Pile <ref type="bibr" target="#b18">(Gao et al., 2020)</ref>.</p><p>For the other models, we use 8B tokens of the RedPajama v2 corpus <ref type="bibr" target="#b12">(Computer, 2023)</ref>. All sparse coders are trained using the Adam optimizer <ref type="bibr" target="#b27">(Kingma &amp; Ba, 2015)</ref>, a sequence length of 2049, and a batch size of 64 sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation</head><p>We use the automated interpretability pipeline released by <ref type="bibr" target="#b33">Paulo et al. (2024)</ref> to generate explanations and scores for sparse coder latents. Activations of latents were collected over 10M tokens, sampled from the Pile for the Pythia models and from FineWeb <ref type="bibr" target="#b34">(Penedo et al., 2024)</ref> for Llama and Gemma. The explanations were generated by showing an explainer model, Llama 3.1 70b, 40 activating examples, four from each of ten different quantiles. Each example had 32 tokens, and the active tokens were highlighted. Detection and fuzzing scores were computed over 50 activating We also use the SAEBench repository <ref type="bibr" target="#b26">(Karvonen et al., 2024)</ref> to evaluate the sparse coders. We use it to compute the variance explained and the cross-entropy loss increase over 500K tokens of the OpenWebText corpus <ref type="bibr" target="#b20">(Gokaslan et al., 2019)</ref>. SAEBench also provides the ability to train and evaluate sparse probes that measure the ability of the SAE's encoder to select information relevant to classification tasks such as sentiment and language detection.</p><p>Recently, <ref type="bibr" target="#b11">Chanin et al. (2024)</ref> drew attention to the phenomenon of feature absorption. In some cases, a more general feature like starts with the letter L appears alongside a specific feature like the token "lion", which may prevent the general feature from being active in contexts where intuitively, both the general and the specific feature apply. They argue that this is undesirable. We use SAEBench to compute the frequency of absorption of general letter features into specific features, in SAEs, STs, and STSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Skip Transcoders Pareto Dominate SAEs</head><p>The utility of any sparse coding method for interpretability lies in its ability to accurately reconstruct activations while also generating human-interpretable latent features. This is a fundamental tradeoff: while sparser latents are generally more interpretable, higher sparsity also tends to increase the reconstruction error. The reconstruction error of a sparse coder can be viewed as "dark matter" containing features not captured by the latents <ref type="bibr" target="#b17">(Engels et al., 2024)</ref>.</p><p>Following earlier work on SAEs, we can represent this tradeoff using a reconstruction vs. interpretability curve (Figure <ref type="figure" target="#fig_0">1</ref>). Here we compare the reconstruction loss of different models, varying the number number of latents and sparsity, with their interpretability scores, measured as the average between detection and fuzzing score over a set of features. We find transcoders and skip transcoders with the same number of latents generally have higher interpretability scores for the same reconstruction loss than SAEs.</p><p>Not only are the average interpretability scores of transcoders and skip transcoders higher than those of SAEs but their distribution is narrower (Figure <ref type="figure" target="#fig_1">2</ref>, left panel). Latents of (skip) transcoders also seem to represent more monosemantic features, as the explanations found hold for larger portion of the activation distribution. This can be seen by comparing the accuracy of explanations in examples sampled from different quantiles of the activation distribution (Figure <ref type="figure" target="#fig_1">2</ref> We replicated these results in models of the same architecture but different sizes, Pythia 160m and Pythia 410m, and on larger models with different architectures, Llama 3.2 1B <ref type="bibr" target="#b14">(Dubey et al., 2024)</ref> and Gemma 2 2B <ref type="bibr" target="#b36">(Team et al., 2024)</ref>. On all cases studied, SSTs had higher automated interpretability scores and lower CE loss increase when patched in, see Table <ref type="table" target="#tab_0">1</ref>.</p><p>We found that performance on sparse probing was similar for SSTs and SAEs (Appendix A), with SSTs winning out for later layers by a small margin. Sparse probing measures the ability of SAEs to preserve information in the original latent, but for transcoders the latents should relate more to concepts necessary for processing the input. It is thus surprising that they are competitive with SAEs on compressing the residual stream without being trained with that objective. We also find that SAEs and transcoders have similar feature absorption behavior, but that those results are noisy. We don't expect this to be a problem, since there are other methods orthogonal to ours which seem to improve feature absorption; see discussion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>Our experiments suggest that interpretability researchers should shift their focus from sparse autoencoders trained in the outputs of MLPs to (skip) transcoders. In our view, the only downside of transcoders compared to sparse autoencoders is that SAEs can be trained directly on the residual stream, while transcoders need to be trained on particular components of the model (usually a feedforward layer). However, one can easily convert a skip transcoder trained on an FFN into a "residual stream transcoder" by adding the identity matrix to its skip connection. In this way, skip transcoders can be viewed as bridging the gap between these two types of sparse coding. Additionally, it is known that SAEs trained on nearby layers in the residual stream learn very similar features, effectively wasting training compute, while SAEs trained on nearby FFNs learn disjoint sets of features <ref type="bibr" target="#b2">(Balagansky et al., 2024)</ref>. For this reason, we suggest that practitioners who are planning to train more than one sparse coder on a model should consider training transcoders on FFNs in lieu of SAEs on the residual stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future work</head><p>As we have shown, transcoders preserve more of a model's behavior and produce more interpretable latents. We believe skip connections let the transcoder avoid the redundant work of translating the linear map, letting it focus on learning important features. Future work may illuminate the role of the skip connection by comparing it to a learned or analytically derived affine approximation of the MLP component. <ref type="bibr" target="#b15">Dunefsky et al. (2024)</ref> highlights the usefulness of transcoders for circuit detection.While we have not run experiments on circuit analysis like in that paper, we expect that skip transcoders to be better for reconstructing circuits thanks to their lower reconstruction error. It is unlikely that the skip connection impedes gradient-based circuit discovery: work like <ref type="bibr" target="#b29">(Marks et al., 2024)</ref> shows ways of incorporating linear skip connections into circuit discovery faithfully.</p><p>Transcoder and skip transcoder features may be used for steering, but we could not translate the unlearning and concept erasure benchmarks from <ref type="bibr" target="#b26">Karvonen et al. (2024)</ref>, which require the latent to contain all information in any given residual stream position.</p><p>Transcoders also do not help improve the feature learning in SAEs the way new architectures like <ref type="bibr" target="#b19">Gao et al. (2024)</ref> do. They merely change the objective of the SAE, which is something that cannot lessen inefficiencies in training. We see the effects of this in the evaluation results on feature absorption: transcoders and skip transcoders can exhibit it just as much as SAEs. Further, feature density plots do not exhibit significant differences Appendix B, showing that SAEs and SSTs are similar on a mechanistic level. Work like Matryoshka SAEs <ref type="bibr" target="#b9">(Bussman et al., 2024;</ref><ref type="bibr" target="#b31">Nabeshima, 2024)</ref> may help tackle these issues for both SAEs and SSTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Contributions</head><p>Nora Belrose had the idea that transcoders might be a superior architecture to SAEs, and came up with the idea of skip transcoders. Gonc ¸alo</p><p>Paulo trained most of the SAEs and transcoders, and performed the experiments and data analysis. Gonc ¸alo and Stepan wrote the first draft. Nora Belrose, Stepan, and Gonc ¸alo wrote the final version. Nora Belrose provided guidance and suggested experiments. Gonc ¸alo, Nora, and Stepan are funded by a grant from Open Philanthropy. We thank Coreweave for computing resources. 7. Code availability. Code for training transcoders and skip transcoders is available in the sparsify GitHub repo. The skip transcoder checkpoints for Llama 3.2 1B are available on the HuggingFace Hub here, and others will be uploaded to the Hub soon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Mechanistic Interpretability. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SAEBench results</head><p>This section contains results on SAEBench <ref type="bibr" target="#b26">(Karvonen et al., 2024)</ref>. We run three of the evaluations: core (reconstruction quality), sparse probing and absorption. We describe all three in the main body and point out that it is not expected for transcoders to outperform SAEs on absorption.</p><p>Variance Table 4. Results for llama-1B.∆ NLL represents the increase in cross-entropy loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Skip transcoders are a Pareto improvement on interpretability vs performance degradation. We compare the increase in cross-entropy loss of 3 different sizes of SAEs and transcoders, 32768 (top right), 65536 (bottom left) and 131072 (bottom right), when patched into the model. For all sizes, skip transcoders are better than transcoders and sparse autoencoders, having both lower increase in model loss and a higher average auto interpretability score. On each quadrant we show 3 models that were trained with a different number of active latents, 32, 64 and 128, except for the 65536 latent model, which only has 32 and 64. The auto interp score is defined as the average fuzzing and detection score of c.a. 500 latents.</figDesc><graphic coords="2,55.44,67.06,486.01,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Interpretability of latents and generalization of explanations. The interpretability scores of both detection and fuzzing are higher for skip transcoders and transcoders when compared to SAEs, with the distribution being wider for SAEs. Dots in the left plot indicate the average score. The accuracy of the explanations on examples sampled from different quantiles of the activation distribution we can observe that The accuracy of explanations remains higher even for lower quantiles, where the activations are smaller, showing that transcoder and skip-transcoder latents are probably representing more monosemantic concepts along the full distribution.</figDesc><graphic coords="3,55.44,67.06,486.00,243.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of sparse coders on different models We compute different interpretability scores, fuzzing, detection and simulation, for SAEs and SSTs trained on different models, as well as the increase of cross-entropy loss when patched into the model. 500 latents are used for fuzzing and detection, but only 50 latents are used for simulation due to it being more computationally expensive. 0.5M tokens are used to compute the cross-entropy loss increase.</figDesc><table><row><cell></cell><cell cols="3">Fuzzing (%, ↑)</cell><cell cols="3">Detection (%, ↑)</cell><cell cols="3">Simulation ( ↑)</cell><cell>CE Loss Increase (%, ↓)</cell></row><row><cell>Model</cell><cell cols="2">Size K SAE ST</cell><cell cols="3">SST SAE ST</cell><cell cols="3">SST SAE ST</cell><cell cols="2">SST SAE ST</cell><cell>SST</cell></row><row><cell cols="11">pythia-160m 2 15 32 74.6 85.4 86.4 70.2 78.7 80.9 0.28 0.46 0.47 1.10 1.23</cell><cell>0.73</cell></row><row><cell cols="8">pythia-160m 2 15 64 71.8 83.7 86.9 67.2 77.5 81.1 0.30</cell><cell>-</cell><cell cols="2">0.42 0.79 1.46</cell><cell>0.59</cell></row><row><cell cols="2">pythia-410m 2 16 32 78.4</cell><cell>-</cell><cell cols="2">89.5 72.7</cell><cell>-</cell><cell cols="2">83.8 0.35</cell><cell>-</cell><cell cols="2">0.51 0.49</cell><cell>-</cell><cell>0.43</cell></row><row><cell>llama-1b</cell><cell>2 17 32 77.2</cell><cell>-</cell><cell cols="2">85.7 71.7</cell><cell>-</cell><cell cols="2">79.4 0.34</cell><cell>-</cell><cell cols="2">0.44 1.50</cell><cell>-</cell><cell>1.00</cell></row><row><cell cols="2">gemma-2-2b 2 17 32 80.5</cell><cell>-</cell><cell cols="2">84.6 75.8</cell><cell>-</cell><cell cols="2">79.6 0.35</cell><cell>-</cell><cell cols="2">0.44 1.60</cell><cell>-</cell><cell>0.53</cell></row></table><note><p>, left). The accuracy of explanations decreases more slowly for STs and SSTs than SAEs. The explanations are also more sensitive, as the false positive rate is lower.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results for gemma-2-2B. ∆ NLL represents the increase in cross-entropy loss.73.7 36.4 ± 28.9 30.5 ± 29.9 33.9 ± 25.1</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">Explained (%) ∆ NLL (↓, %) Sparse probing (↑)</cell><cell>Absorption score (↓)</cell></row><row><cell></cell><cell cols="2">Layer SAE</cell><cell>SST</cell><cell></cell><cell>SAE</cell><cell>SST</cell><cell>SAE</cell><cell>SST</cell><cell>SAE</cell><cell>SST</cell></row><row><cell></cell><cell>L10</cell><cell>16.5</cell><cell>67.1</cell><cell></cell><cell>1.1</cell><cell>0.5</cell><cell>71.5</cell><cell>70.6</cell><cell>36.3 ± 20.2 28.6 ± 13.3</cell></row><row><cell></cell><cell>L14</cell><cell>17.0</cell><cell>72.4</cell><cell></cell><cell>1.1</cell><cell>0.5</cell><cell>80.0</cell><cell>76.0</cell><cell>33.1 ± 19.4 25.0 ± 18.4</cell></row><row><cell></cell><cell>L18</cell><cell>20.9</cell><cell>81.7</cell><cell></cell><cell>2.1</cell><cell>0.5</cell><cell>78.9</cell><cell>75.2</cell><cell>26.3 ± 23.7 53.3 ± 19.6</cell></row><row><cell></cell><cell>L22</cell><cell>21.6</cell><cell>73.2</cell><cell></cell><cell>1.6</cell><cell>0.5</cell><cell>76.1</cell><cell>24.4 ± 22.0 18.8 ± 29.5</cell></row><row><cell></cell><cell cols="3">Variance Explained (%)</cell><cell cols="3">∆ NLL (↓, %)</cell><cell cols="2">Sparse probing (↑)</cell><cell>Absorption score (↓)</cell></row><row><cell cols="3">Layer SAE ST</cell><cell>SST</cell><cell cols="4">SAE ST SST SAE ST</cell><cell>SST</cell><cell>SAE</cell><cell>ST</cell><cell>SST</cell></row><row><cell>L0</cell><cell cols="2">99.4 99.4</cell><cell>99.9</cell><cell>0.2</cell><cell cols="2">0.2 0.0</cell><cell cols="2">71.9 66.1 69.3</cell><cell>95.5 ± 5.9 54.1 ± 10.5 60.3 ± 16.3</cell></row><row><cell>L2</cell><cell cols="2">82.3 80.7</cell><cell>85.3</cell><cell>1.1</cell><cell cols="2">1.4 1.0</cell><cell cols="2">59.8 67.0 66.0 14.5 ± 14.3 47.4 ± 19.2 33.5 ± 14.8</cell></row><row><cell>L4</cell><cell cols="2">75.9 74.3</cell><cell>87.4</cell><cell>1.1</cell><cell cols="2">1.2 0.7</cell><cell cols="2">67.7 67.3 69.4 19.3 ± 18.7 62.2 ± 26.2 40.5 ± 17.5</cell></row><row><cell>L6</cell><cell cols="2">81.0 77.8</cell><cell>86.5</cell><cell>1.1</cell><cell cols="2">1.2 0.7</cell><cell cols="2">65.9 69.5 69.9</cell><cell>8.2 ± 15.8 15.6 ± 18.9 31.3 ± 25.7</cell></row><row><cell>L8</cell><cell cols="2">87.8 85.2</cell><cell>90.3</cell><cell>1.1</cell><cell cols="2">1.3 0.9</cell><cell cols="2">68.3 67.4 71.6 18.1 ± 16.8 82.9 ± 23.6 45.1 ± 21.7</cell></row><row><cell>L10</cell><cell cols="2">86.5 84.4</cell><cell>88.8</cell><cell>1.4</cell><cell cols="2">1.7 1.3</cell><cell>69.0 71.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results for pythia-160m. ∆ NLL represents the increase in cross-entropy loss.</figDesc><table><row><cell></cell><cell cols="6">Variance Explained (%) ∆ NLL (↓, %) Sparse probing (↑)</cell><cell cols="2">Absorption score (↓)</cell></row><row><cell cols="2">Layer SAE</cell><cell>SST</cell><cell>SAE</cell><cell>SST</cell><cell>SAE</cell><cell>SST</cell><cell>SAE</cell><cell>SST</cell></row><row><cell>L0</cell><cell>93.0</cell><cell>93.8</cell><cell>1.5</cell><cell>1.0</cell><cell>69.9</cell><cell>69.5</cell><cell>80.7 ± 12.7</cell><cell>4.7 ± 2.8</cell></row><row><cell>L2</cell><cell>73.8</cell><cell>80.5</cell><cell>1.5</cell><cell>1.5</cell><cell>69.5</cell><cell>72.4</cell><cell cols="2">30.2 ± 22.1 42.3 ± 5.1</cell></row><row><cell>L4</cell><cell>63.3</cell><cell>81.2</cell><cell>1.5</cell><cell>1.0</cell><cell>70.9</cell><cell>74.5</cell><cell cols="2">46.7 ± 19.4 60.7 ± 27.5</cell></row><row><cell>L6</cell><cell>57.8</cell><cell>78.9</cell><cell>1.5</cell><cell>1.0</cell><cell>68.7</cell><cell>69.9</cell><cell cols="2">82.6 ± 8.7 36.0 ± 14.6</cell></row><row><cell>L8</cell><cell>63.3</cell><cell>82.8</cell><cell>1.5</cell><cell>1.0</cell><cell>72.2</cell><cell>69.9</cell><cell cols="2">66.1 ± 12.7 75.1 ± 10.3</cell></row><row><cell>L10</cell><cell>69.9</cell><cell>84.4</cell><cell>2.0</cell><cell>1.5</cell><cell>74.9</cell><cell>73.6</cell><cell cols="2">44.7 ± 21.0 52.3 ± 16.2</cell></row><row><cell>L12</cell><cell>71.1</cell><cell>77.0</cell><cell>2.0</cell><cell>2.0</cell><cell></cell><cell>74.6</cell><cell cols="2">9.4 ± 11.0 45.4 ± 18.9</cell></row><row><cell>L14</cell><cell>71.1</cell><cell>75.8</cell><cell>2.0</cell><cell>2.0</cell><cell>71.1</cell><cell>75.3</cell><cell>0.1 ± 0.4</cell><cell>52.0 ± 19.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The polytope interpretation can also be applied, in a somewhat modified form, to MLPs with other activation functions(Balestriero &amp; Baraniuk,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2019).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr">(Bricken et al., 2023b)</ref> <p>and the consistent activation heuristic (sum of activations over all tokens divided by the number of tokens). These plots show that STs and SSTs are similar in terms of feature density and have less high-density features and more low-density features. This is not a problem because there exist methods for getting rid of low-density features <ref type="bibr">(Bricken et al., 2023b;</ref><ref type="bibr">Jermyn &amp; Templeton, 2024;</ref><ref type="bibr" target="#b19">Gao et al., 2024)</ref>, but not for regularizing high-density features.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Interpretability as compression: Reconsidering sae explanations of neural activations with mdl-saes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ayonrinde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sharkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.11179</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Balagansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maksimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.07656</idno>
		<title level="m">Mechanistic permutability: Match features across layers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From hard to soft: Understanding deep network nonlinearities via vector quantization and statistical inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syxt2jC5FX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models can explain neurons in language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mossing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<ptr target="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html" />
		<imprint/>
	</monogr>
	<note>Date accessed: 14.05. 2023), 2, 2023</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sharkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grinsztajn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Winsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Guevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12312</idno>
		<ptr target="https://arxiv.org/abs/2211.12312" />
		<title level="m">Interpreting neural networks through the polytope lens</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards monosemanticity: Decomposing language models with dictionary learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2023/monosemantic-features/index.html" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards monosemanticity: Decomposing language models with dictionary learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Denison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2023/monosemantic-features" />
	</analytic>
	<monogr>
		<title level="m">Transformer Circuits Thread, 2023b</title>
		<imprint>
			<date type="published" when="2023-10-04">October 4, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning multi-level features with matryoshka saes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<ptr target="https://www.alignmentforum.org/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-sae" />
	</analytic>
	<monogr>
		<title level="j">AI Alignment Forum</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Bussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Leask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno>2412.06410</idno>
		<ptr target="https://arxiv.org/abs/2412.06410" />
		<title level="m">Batchtopk sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Chanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wilken-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dulka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bloom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.14507</idno>
		<title level="m">A is for absorption: Studying feature splitting and absorption in sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Redpajama: an open dataset for training large language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Computer</surname></persName>
		</author>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Mavor-Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heimersheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14997</idno>
		<ptr target="https://arxiv.org/abs/2304.14997" />
		<title level="m">Towards automated circuit discovery for mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dunefsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chlenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11944</idno>
		<ptr target="https://arxiv.org/abs/2406.11944" />
		<title level="m">Transcoders find interpretable llm feature circuits</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10652</idno>
		<title level="m">Toy models of superposition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Decomposing the dark matter of sparse autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.14670</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>La Tour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Troll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04093</idno>
		<title level="m">Scaling and evaluating sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<title level="m">Openwebtext corpus</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Finding neurons in a haystack: Case studies with sparse probing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Troitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01610</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Kheirkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hathaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12181</idno>
		<title level="m">Universal neurons in gpt2 language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ghost grads: An improvement on resampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2024/jan-update/index.html#dict-learning-resampling" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Open source automated interpretability for sparse autoencoder features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Belrose</surname></persName>
		</author>
		<ptr target="https://blog.eleuther.ai/autointerp/" />
		<imprint>
			<date type="published" when="2024-07">July 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Karvonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tigges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mc-Dougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayonrinde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><surname>Saebench</surname></persName>
		</author>
		<ptr target="https://www.neuronpedia.org/sae-bench/info" />
		<title level="m">A comprehensive benchmark for sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A. dictionary learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><surname>Mueller</surname></persName>
		</author>
		<ptr target="https://github.com/saprmarks/dictionary_learning?tab=readme-ov-file#extra-functionality-supported-by-this-repo" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19647</idno>
		<title level="m">Sparse feature circuits: Discovering and editing interpretable causal graphs in language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The singular value decompositions of transformer weight matrices are highly interpretable</title>
		<author>
			<persName><forename type="first">B</forename><surname>Millidge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI Alignment Forum</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<ptr target="https://www.alignmentforum.org/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders" />
		<title level="m">Matryoshka sparse autoencoders. AI Alignment Forum</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zoom in: An introduction to circuits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00024.001</idno>
		<ptr target="http://dx.doi.org/10.23915/distill.00024.001" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<idno type="ISSN">2476-0757</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020-03">March 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatically interpreting millions of features in large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Belrose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13928</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<author>
			<persName><forename type="first">G</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=n6SCkn2QaG" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rajamanoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sonnerat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.14435</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Gemma 2: Improving open language models at a practical size</title>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Predicting future activations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2024/jan-update/index.html#predict-future" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread</title>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ameisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macdiarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sumers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
