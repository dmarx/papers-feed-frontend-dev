<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Embeddings of Graphs in Hyperbolic Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">Paul</forename><surname>Chamberlain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marc</forename><forename type="middle">Peter</forename><surname>Deisenroth</surname></persName>
							<email>m.deisenroth@imperial.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Centre for Complexity Science</orgName>
								<orgName type="department" key="dep2">Department of Computing Imperial</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Embeddings of Graphs in Hyperbolic Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A4BFB84A0F26677896CF0CB2A85AC65A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>graph embeddings</term>
					<term>complex networks</term>
					<term>geometry</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted signi cant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in highdimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the at Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry signi cantly improves performance on downstream tasks for several real-world public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Embedding (or vector space) methods nd a lower-dimensional continuous space in which to represent high-dimensional complex data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. The distance between objects in the lower-dimensional space gives a measure of their similarity. This is usually achieved by rst postulating a low-dimensional vector space and then optimising an objective function of the vectors in that space. Vector space representations provide three principle bene ts over sparse schemes: (1) They encapsulate similarity, (2) they are compact, (3) they perform better as inputs to machine learning models <ref type="bibr" target="#b26">[27]</ref>. This is true of graph structured data where the native data format is the adjacency matrix, a typically large, sparse matrix of connection weights.</p><p>Neural embedding models are a avour of embedding scheme where the vector space corresponds to a subset of the network weights, which are learned through backpropagation. Neural embedding models have been shown to improve performance in a large number of downstream tasks across multiple domains. These include word analogies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, machine translation <ref type="bibr" target="#b27">[28]</ref>, document comparison <ref type="bibr" target="#b15">[16]</ref>, missing edge prediction <ref type="bibr" target="#b11">[12]</ref>, vertex attribution <ref type="bibr" target="#b23">[24]</ref>, product recommendations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>, customer value prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> and item categorisation <ref type="bibr" target="#b2">[3]</ref>. In all cases the embeddings are learned without labels (unsupervised) from a sequence of entities.</p><p>To the best of our knowledge, all previous work on neural embedding models either explicitly or implicitly (by using the Euclidean dot product) assumes that the vector space is Euclidean. Recent work from the eld of complex networks has found that many interesting networks, such as the Internet <ref type="bibr" target="#b4">[5]</ref> or academic citations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> can be well described by a framework with an underlying non-Euclidean hyperbolic geometry. Hyperbolic geometry provides a continuous analogue of tree-like graphs, and even in nite trees have nearly isometric embeddings in hyperbolic space <ref type="bibr" target="#b10">[11]</ref>. Additionally, the de ning features of complex networks, such as power-law degree distributions, strong clustering and hierarchical community structure, emerge naturally when random graphs are embedded in hyperbolic space <ref type="bibr" target="#b14">[15]</ref>.</p><p>The starting point for our model is the celebrated word2vec Skipgram architecture, which is shown in Figure <ref type="figure" target="#fig_3">3</ref>  <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Skipgram is a shallow neural network with three layers: (1) An input projection layer that maps from a one-hot-encoded to a distributed representation, (2) a hidden layer, and (3) an output softmax layer. The network is necessarily simple for tractability as there are a very large number of output states (every word in a language). Skipgram is trained on a sequence of words that is decomposed into (input word, context word)-pairs. The model employs two separate vector representations, one for the input words and another for the context words, with the input representation comprising the learned embedding. The word pairs are generated by taking a sequence of words and running a sliding window (the context) over them. As an example the word sequence "chance favours the prepared mind" with a context window of size three would generate the following training data: (chance, favours), (chance, the), (favours, chance), ... }. Words are initially randomly allocated to vectors within the two vector spaces. Then, for each training pair, the vector representations of the observed input and context words are pushed towards each other and away from all other words (see Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>The concept can be extended from words to network structured data using random walks to create sequences of vertices. The vertices are then treated exactly analogously to words in the NLP formulation. This was originally proposed as DeepWalk <ref type="bibr" target="#b23">[24]</ref>. Extensions varying the nature of the random walks have been explored in LINE <ref type="bibr" target="#b28">[29]</ref> and Node2vec <ref type="bibr" target="#b11">[12]</ref>.</p><p>Contribution. In this paper, we introduce the new concept of neural embeddings in hyperbolic space. We formulate backpropagation in hyperbolic space and show that using the natural geometry of complex networks improves performance in vertex classi cation tasks across multiple networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HYPERBOLIC GEOMETRY</head><p>Hyperbolic geometry emerges from relaxing Euclid's fth postulate (the parallel postulate) of geometry. In hyperbolic space there is not arXiv:1705.10359v1 [stat.ML] 29 May 2017  just one, but an in nite number of parallel lines that pass through a single point. This is illustrated in Figure <ref type="figure" target="#fig_1">1b</ref> where every line is parallel to the bold, blue line and all pass through the same point. Hyperbolic space is one of only three types of isotropic spaces that can be de ned entirely by their curvature. The most familiar is Euclidean, which is at, having zero curvature. Space with uniform positive curvature has an elliptic geometry (e.g. the surface of a sphere), and space with uniform negative curvature is called hyperbolic, which is analogous to a saddle-like surface. As, unlike Euclidean space, in hyperbolic space even in nite trees have nearly isometric embeddings, it has been successfully used to model complex networks with hierarchical structure, power-law degree distributions and high clustering <ref type="bibr" target="#b14">[15]</ref>.</p><p>One of the de ning characteristics of hyperbolic space is that it is in some sense larger than the more familiar Euclidean space; the area of a circle or volume of a sphere grows exponentially with its radius, rather than polynomially. This suggests that low-dimensional hyperbolic spaces may provide e ective representations of data in ways that low-dimensional Euclidean spaces cannot. However this makes hyperbolic space hard to visualise as even the 2D hyperbolic plane can not be isometrically embedded into Euclidean space of any dimension,(unlike elliptic geometry where a 2-sphere can be embedded into 3D Euclidean space). For this reason there are many di erent ways of representing hyperbolic space, with each representation conserving some geometric properties, but distorting others. In the remainder of the paper we use the Poincaré disk model of hyperbolic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Poincaré Disk Model</head><p>The Poincaré disk models two-dimensional hyperbolic space where the in nite plane is represented as a unit disk. We work with the two-dimensional disk, but it is easily generalised to the ddimensional Poincaré ball, where hyperbolic space is represented as a unit d-ball.</p><p>In this model hyperbolic distances grow exponentially towards the edge of the disk. The circle's boundary represents in nitely distant points as the in nite hyperbolic plane is squashed inside the nite disk. This property is illustrated in Figure <ref type="figure" target="#fig_1">1a</ref> where each tile is of constant area in hyperbolic space, but the tiles rapidly shrink to zero area in Euclidean space. Although volumes and distances are warped, the Poincaré disk model is conformal, meaning that Euclidean and hyperbolic angles between lines are equal. Straight lines in hyperbolic space intersect the boundary of the disk orthogonally and appear either as diameters of the disk, or arcs of a circle. Figure <ref type="figure" target="#fig_1">1b</ref> shows a collection of straight hyperbolic lines in the Poincaré disk. Just as in spherical geometry, the shortest path from one place to another is a straight line, but appears as a curve on a at map. Similarly, these straight lines show the shortest path (in terms of distance in the underlying hyperbolic space) from one point on the disk to another, but they appear curved. This is because it is quicker to move close to the centre of the disk, where distances are shorter, than nearer the edge. In our proposed approach, we will exploit both the conformal property and the circular symmetry of the Poincaré disk.</p><p>Overall, the geometric intuition motivating our approach is that vertices embedded near the middle of the disk can have more close neighbours than they could in Euclidean space, whilst vertices nearer the edge of the disk can still be very far from each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inner Product, Angles, and Distances</head><p>The mathematics is considerably simpli ed if we exploit the symmetries of the model and describe points in the Poincaré disk using polar coordinates, x = (r e , θ ), with r e ∈ [0, 1) and θ ∈ [0, 2π ). To de ne similarities and distances, we require an inner product. In the Poincaré disk, the inner product of two vectors x = (r x , θ x ) and = (r , θ ) is given by</p><formula xml:id="formula_0">x, = x cos(θ x -θ ) (1) = 4 arctanh r x arctanh r cos(θ x -θ )<label>(2)</label></formula><p>The distance of x = (r e , θ ) from the origin of the hyperbolic coordinate system is given by r h = 2 arctanh r e and the circumference of a circle of hyperbolic radius R is C = 2π sinh R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NEURAL EMBEDDING IN HYPERBOLIC SPACE</head><p>We adopt the original notation of <ref type="bibr" target="#b17">[18]</ref> whereby the input vertex is w I and the output is w O . Their corresponding vector representations are w I and w O , which are elements of the two vector spaces shown in Figure <ref type="figure" target="#fig_3">3</ref>, W and W respectively. Skipgram has a geometric interpretation, which we visualise in Figure <ref type="figure" target="#fig_2">2</ref> for vectors in W . Updates to w j are performed by simply adding (if w j is the observed output vertex) or subtracting (otherwise) an errorweighted portion of the input vector. Similar, though slightly more complicated, update rules apply to the vectors in W. Given this interpretation, it is natural to look for alternative geometries in which to perform these updates.</p><p>To embed a graph in hyperbolic space we replace Skipgram's two Euclidean vector spaces (W and W in Figure <ref type="figure" target="#fig_3">3</ref>) with two Poincaré disks. We learn embeddings by optimising an objective function that predicts output/context vertices from an input vertex, but we replace the Euclidean dot products used in Skipgram with hyperbolic inner products. A softmax function is used for the conditional predictive distribution</p><formula xml:id="formula_1">v (old) v (new) wO v (new)</formula><formula xml:id="formula_2">p(w O |w I ) = exp( w O , w I ) V i=1 exp( w i , w I ) ,<label>(3)</label></formula><p>where w i is the vector representation of the i t h vertex, primed indicates members of the output vector space (See Figure <ref type="figure" target="#fig_3">3</ref>) and •, • is the hyperbolic inner product. Directly optimising (3) is computationally demanding as the sum in the denominator extends over every vertex in the graph. Two commonly used techniques to make word2vec more e cient are (a) replacing the softmax with a hierarchical softmax <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> and (b) negative sampling <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. We use negative sampling as it is faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Negative Sampling</head><p>Negative sampling is a form of Noise Contrastive Estimation (NCE) <ref type="bibr" target="#b12">[13]</ref>. NCE is an estimation technique that is based on the assumption that a good model should be able to separate signal from noise using only logistic regression.</p><p>As we only care about generating good embeddings, the objective function does not need to produce a well-speci ed probability distribution. The negative log likelihood using negative sampling is</p><formula xml:id="formula_3">E = -log σ ( w O , w I ) - w j ∈W ne log σ (-w j , w I ) (4) = -log σ (u O ) - K j=1 E w j ∼P n [log σ (-u j )]<label>(5)</label></formula><p>where w I , w O are the vector representation of the input and output vertices, u j = w j , w I , W neg is a set of samples drawn from the noise distribution, K is the number of samples and σ is the sigmoid function. The rst term represents the observed data and the second term the negative samples. To draw W neg , we specify the noise distribution P n to be unigrams raised to 3  4 as in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Learning</head><p>We learn the model using backpropagation. To perform backpropagation it is easiest to work in natural hyperbolic co-ordinates on the disk and map back to Euclidean co-ordinates only at the end. In natural co-ordinates r ∈ (0, ∞), θ ∈ (0, 2π ] and u j = r j r I cos(θ I -θ j ).</p><p>The major drawback of this co-ordinate system is that it introduces a singularity at the origin. To address the complexities that result from radii that are less than or equal to zero, we initialise all vectors to be in a patch of space that is small relative to its distance from the origin. The gradient of the negative log-likelihood in (5) w.r.t. u j is given by</p><formula xml:id="formula_4">∂E ∂u j =          σ (u j ) -1, if w j = w O σ (u j ), if w j = W ne 0, otherwise<label>(6)</label></formula><p>Taking the derivatives w.r.t. the components of vectors in W (in natural polar hyperbolic co-ordinates) yields</p><formula xml:id="formula_5">∂E ∂(r j ) k = ∂E ∂u j ∂u j ∂(r j ) k = ∂E ∂u j r I cos(θ I -θ j ) (7) ∂E ∂(θ j ) k = ∂E ∂u j r j r I sin(θ I -θ j ) . (<label>8</label></formula><formula xml:id="formula_6">)</formula><p>The Jacobian is then</p><formula xml:id="formula_7">∇ r E = ∂E ∂r r + 1 sinh r ∂E ∂θ θ ,<label>(9)</label></formula><p>which leads to</p><formula xml:id="formula_8">r new j = r old j -ηϵ j r I cos(θ I -θ j ), if w j ∈ w O ∪ W ne r old j , otherwise<label>(10)</label></formula><formula xml:id="formula_9">θ new j = θ old j -ηϵ j r I r j sinh r j sin(θ I -θ j ), if w j ∈ w O ∪ W ne θ old j , otherwise (<label>11</label></formula><formula xml:id="formula_10">)</formula><p>where η is the learning rate and ϵ j is the prediction error de ned in Equation ( <ref type="formula" target="#formula_4">6</ref>). Calculating the derivatives w.r.t. the input embedding follows the same pattern, and we obtain</p><formula xml:id="formula_11">∂E ∂r I = j:w j ∈w O ∪W ne ∂E ∂u j ∂u j ∂r I (12) = j:w j ∈w O ∪W ne ∂E ∂u j r j cos(θ I -θ j ) ,<label>(13)</label></formula><formula xml:id="formula_12">∂E ∂θ I = j:w j ∈w O ∪W ne ∂E ∂u j ∂u j ∂θ I (14) = j:w j ∈w O ∪W ne - ∂E ∂u j r I r j sin(θ I -θ j ) .<label>(15)</label></formula><p>N -dim hidden layer  The corresponding update equations are</p><formula xml:id="formula_13">V -dim input layer W V ×N W N ×V C × V -dim output layer x k h i [y 1j , ..., y Cj ] y 1j y Cj y 2j</formula><formula xml:id="formula_14">r new I = r old I -η j:w j ∈w O ∪W ne ϵ j r j cos(θ I -θ j ) ,<label>(16)</label></formula><formula xml:id="formula_15">θ new I = θ old I -η j:w j ∈w O ∪W ne ϵ j r I r j sinh r I sin(θ I -θ j ) ,<label>(17)</label></formula><p>where t j is an indicator variable s.t. t j = 1 if and only if w j = w O , and t j = 0 otherwise. On completion of backpropagation, the vectors are mapped back to Euclidean co-ordinates on the Poincaré disk through θ h → θ e and r h → tanh r h 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL EVALUATION</head><p>In this section, we assess the quality of hyperbolic embeddings and compare them to embeddings in Euclidean spaces on a number of public benchmark networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We report results on ve publicly available network datasets for the problem of vertex attribution.</p><p>(1) Karate: Zachary's karate club contains 34 vertices divided into two factions <ref type="bibr" target="#b29">[30]</ref>.</p><p>(2) Polbooks: A network of books about US politics published around the time of the 2004 presidential election and sold by the online bookseller Amazon.com. Edges between books represent frequent co-purchasing of books by the same buyers. (3) Football: A network of American football games between Division IA colleges during regular season Fall 2000 <ref type="bibr" target="#b8">[9]</ref>. (4) Adjnoun: Adjacency network of common adjectives and nouns in the novel David Copper eld by Charles Dickens <ref type="bibr" target="#b22">[23]</ref>. (5) Polblogs: A network of hyperlinks between weblogs on US politics, recorded in 2005 <ref type="bibr" target="#b0">[1]</ref>.</p><p>Statistics for these datasets are recorded in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualising Embeddings</head><p>To illustrate the utility of hyperbolic embeddings we compare embeddings in the Poincaré disk to the two-dimensional deepwalk embeddings for the 34-vertex karate network with two factions. The results are shown in Figure <ref type="figure">4</ref>. Both embeddings were generated by running for ve epochs on an intermediate dataset of 34, ten step random walks, one originating at each vertex. The gure clearly shows that the hyperbolic embedding is able to capture the community structure of the underlying network. When embedded in hyperbolic space, the two factions (black and white discs) of the underlying graph are linearly separable, while the Deepwalk embedding does not exhibit such an obvious structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Vertex Attribute Prediction</head><p>We evaluate the success of neural embeddings in hyperbolic space by using the learned embeddings to predict held-out labels of vertices in networks. In our experiments, we compare our embedding to deepwalk <ref type="bibr" target="#b23">[24]</ref> embeddings of dimensions 2, 4, 8, 16, 32, 64 and 128. To generate embeddings we rst create an intermediate dataset by taking a series of random walks over the networks. For each network we use a ten-step random walk originating at each vertex.</p><p>The embedding models are all trained using the same parameters and intermediate random walk dataset. For deepwalk, we use the gensim <ref type="bibr" target="#b24">[25]</ref> python package, while our hyperbolic embeddings are Figure <ref type="figure">4</ref>: The factions of the Zachary karate network are easily linearly separable when embedded in 2D hyperbolic space. This is not true when embedding in Euclidean space. Both embeddings were run for 5 epochs on the same random walks written in custom TensorFlow. In both cases, we use ve training epochs, a window size of ve and do not prune any vertices.</p><p>The results of our experiments are shown in Figure <ref type="figure">5</ref>. The graphs show macro F1 scores against the percentage of labelled data used to train a logistic regression classi er. Here we follow the method for generating F1 scores when each test case can have multiple labels that is described in <ref type="bibr" target="#b16">[17]</ref>. The error bars show one standard error from the mean over ten repetitions. The blue lines show hyperbolic embeddings while the red lines depict deepwalk embeddings at various dimensions. It is apparent that in all datasets hyperbolic embeddings signi cantly outperform deepwalk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have introduced the concept of neural embeddings in hyperbolic space. To the best of our knowledge, all previous embeddings models have assumed a at Euclidean geometry. However, a at geometry is not the natural geometry of all data structures. A hyperbolic space has the property that power-law degree distributions, strong clustering and hierarchical community structure emerge naturally when random graphs are embedded in hyperbolic space. It is therefore logical to exploit the structure of the hyperbolic space for useful embeddings of complex networks. We have demonstrated that when applied to the task of classifying vertices of complex networks, hyperbolic space embeddings signi cantly outperform embeddings in Euclidean space.   Figure <ref type="figure">5</ref>: Macro F1 score ( -axis) against percentage of labelled vertices used for training (x-axis). In all cases hyperbolic embeddings (blue) signi cantly outperform Euclidean deepwalk embeddings (red). Error bars show standard error from the mean over ten repetitions. The legend used in sub gure (a) applies to all sub gures. A consistent trend across the datasets is that an embedding into a 2D hyperbolic space outperforms deepwalk architectures with embeddings ranging from 2D to 128D.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>(a) "Circle Limit 1" by M.C. Escher illustrates the Poincaré disc model of hyperbolic space. Each tile is of constant area in hyperbolic space, but vanishes in Euclidean space at the boundary. (b) A set of straight lines in the Poincare disk that all pass through a given point and are all parallel to the blue (thicker) line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of properties of hyperbolic space. a Tiles of constant area b Parallel lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Geometric interpretation of the update equations in the Skipgram model. The vector representation of the output vertex (new) w O is moved closer (blue) to the vector representation of the input vertex I , while all other vectors (new) w j move further away (red). The magnitude of the change is proportional to the prediction error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The skipgram model predicts the context vertices from a single input vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>(a) Zachary's karate network. The network is split into two factions. Two-dimensional hyperbolic embedding of the karate network in the Poincaré disk. Two dimensional Deepwalk embedding of the karate network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Description of experimental datasets. 'Largest class' gives the fraction of the dataset composed by the largest class and thereby provides the benchmark for random prediction accuracy.</figDesc><table><row><cell>name</cell><cell>|V|</cell><cell>|E|</cell><cell cols="2">|y| largest class</cell><cell>Labels</cell></row><row><cell>karate</cell><cell>34</cell><cell>77</cell><cell>2</cell><cell>0.53</cell><cell>Factions</cell></row><row><cell cols="2">polbooks 105</cell><cell>441</cell><cell>3</cell><cell>0.46</cell><cell>A liation</cell></row><row><cell cols="2">football 115</cell><cell>613</cell><cell cols="2">12 0.11</cell><cell>League</cell></row><row><cell cols="2">adjnoun 112</cell><cell>425</cell><cell>2</cell><cell>0.52</cell><cell>Part of Speech</cell></row><row><cell cols="4">polblogs 1,224 16,781 2</cell><cell>0.52</cell><cell>A liation</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The political blogosphere and the 2004 U.S. election</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd international workshop on Link discovery -LinkKDD &apos;05</title>
		<meeting>the 3rd international workshop on Link discovery -LinkKDD &apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Saez-Trumper</surname></persName>
		</author>
		<title level="m">Wisdom of the Crowd or Wisdom of a Few? Proceedings of the 26th ACM Conference on Hypertext &amp; Social Media -HT &apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Item2Vec : Neural Item Embedding for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sustaining the Internet with Hyperbolic Mapping</title>
		<author>
			<persName><forename type="first">Marian</forename><surname>Boguna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">62</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Customer Life Time Value Prediction Using Embeddings</title>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">P</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName><surname>Deisenroth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What is the dimension of citation space?</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">S</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">448</biblScope>
			<biblScope unit="page" from="235" to="247" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transitive reduction of citation networks</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Gollings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamar</forename><forename type="middle">V</forename><surname>Loach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">S</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Community structure in social and biological networks</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Girvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="7821" to="7826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">E-commerce in Your Inbox: Product Recommendations at Scale Categories and Subject Descriptors</title>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Bhamidipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaikit</forename><surname>Savla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bhagwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1809" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Metric Structures for Riemannian and Non-riemannian Spaces</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Gromov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science and Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec : Scalable Feature Learning for Networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics</title>
		<author>
			<persName><surname>Michael U Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">iPhone&apos;s Digital Marketplace: Characterizing the Big Spenders</title>
		<author>
			<persName><forename type="first">Farshad</forename><surname>Kooti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihajlo</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName><surname>Lerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperbolic Geometry of Complex Networks</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From Word Embeddings To Document Distances</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-label learning by constrained non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Arti cial Intelligence</title>
		<meeting>the AAAI Conference on Arti cial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="421" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">E cient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word embeddings e ciently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geo</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Fast and Simple Algorithm for Training Neural Probabilistic Language Models</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding community structure in networks using the eigenvectors of matrices</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E -Statistical, Nonlinear, and Soft Matter Physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DeepWalk : Online Learning of Social Representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName><forename type="first">Radim</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear Dimensionality Reduction by Locally Linear Embedding</title>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science, New Series</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A vector space model for automatic indexing</title>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Shu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="613" to="620" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An information ow model for con ict and ssion in small groups</title>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="452" to="473" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
