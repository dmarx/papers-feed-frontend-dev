The decisions made by researchers in the context of neural embeddings of graphs in hyperbolic space are grounded in a combination of theoretical insights and empirical observations. Below is a detailed technical explanation and rationale for each of the key components mentioned:

### Neural Embeddings Overview
Neural embeddings serve as compact representations of data, capturing the underlying structure and relationships within the data. In the context of graphs, embeddings can encapsulate vertex similarity, which is crucial for tasks like edge prediction and vertex labeling. The compactness of embeddings allows for efficient storage and computation, making them suitable for machine learning applications where high-dimensional data is common.

### Hyperbolic Space
The choice of hyperbolic space for embedding complex networks is motivated by the unique properties of hyperbolic geometry, which is particularly well-suited for representing hierarchical structures and networks with power-law degree distributions. Many real-world networks exhibit characteristics such as strong clustering and a tendency to form communities, which can be more naturally represented in hyperbolic space than in Euclidean space. The exponential growth of distances in hyperbolic space allows for a more efficient representation of relationships among nodes, especially in sparse networks.

### Poincaré Disk Model
The Poincaré disk model is a specific representation of hyperbolic space that allows for the visualization and manipulation of hyperbolic geometry in a bounded format. By mapping hyperbolic space to a unit disk, researchers can leverage the conformal property, which preserves angles, making it easier to apply traditional geometric and analytical techniques. The exponential growth of distances towards the edge of the disk enables the representation of a large number of vertices in a compact manner, which is essential for embedding complex networks.

### Skipgram Architecture
The Skipgram architecture, originally designed for word embeddings, is adapted for graph data by treating vertices as words and using random walks to generate sequences of vertices. This approach allows the model to learn embeddings that capture the local structure of the graph, similar to how word embeddings capture semantic relationships in language. The flexibility of the Skipgram model makes it a natural choice for extending to graph-structured data.

### Objective Function
The objective function is designed to predict the context vertices given an input vertex, utilizing hyperbolic inner products instead of Euclidean dot products. This change is crucial because it aligns the embedding process with the underlying geometry of the data, allowing for more accurate representations of relationships in hyperbolic space. The softmax function used in the objective function facilitates the learning of embeddings by providing a probabilistic framework for vertex relationships.

### Negative Sampling
Negative sampling is employed to efficiently approximate the objective function, which would otherwise be computationally expensive due to the need to consider all vertices in the graph. By sampling negative examples, the model can focus on learning the most relevant relationships, significantly speeding up the training process while still producing high-quality embeddings. This technique is particularly useful in large graphs where the number of vertices can be substantial.

### Backpropagation in Hyperbolic Space
Backpropagation in hyperbolic space requires careful consideration of the geometry involved. The use of natural hyperbolic coordinates allows for effective gradient updates while addressing the singularities that can occur at the origin. This adaptation is essential for ensuring that the learning process remains stable and converges effectively.

### Gradient Updates
The gradient updates are formulated to account for the unique properties of hyperbolic space, particularly the polar coordinates used to represent points. By deriving the updates in this manner, the model can effectively adjust the embeddings based on the prediction errors, ensuring that the learned representations accurately reflect the underlying graph structure.

### Experimental Evaluation
The experimental evaluation of hyperbolic embeddings against Euclidean embeddings on benchmark datasets provides empirical evidence for the effectiveness of the proposed approach. By demonstrating improved performance on tasks such as vertex classification and edge prediction, the researchers validate their hypothesis that hyperbolic space is a more appropriate representation for complex networks.

### Datasets
The choice of datasets for evaluation, such as Karate, Polbooks, and Football, is strategic, as these networks exhibit characteristics that are representative of real-world complex networks. By using diverse datasets, the researchers can assess the generalizability of their approach and its applicability to various types of graph-structured data.

In summary, the researchers' decisions are informed by a combination of theoretical foundations in geometry, practical considerations in machine learning, and empirical validation through experimental evaluation. The integration of hyperbolic geometry into neural embeddings represents a significant advancement in the field, enabling more effective representations of complex networks.