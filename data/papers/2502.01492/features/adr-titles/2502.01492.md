- Decision to use Factorio as the primary sandbox for training AI agents
- Choice of evaluation metrics for AI agents in system engineering tasks
- Adoption of a dynamic and open-ended evaluation environment
- Selection of core trade-offs to assess (efficiency, scalability, adaptability)
- Decision to focus on multi-agent coordination challenges
- Strategy for integrating real-world complexities into training scenarios
- Approach to feedback collection and adaptation mechanisms
- Framework for measuring long-term planning capabilities of AI agents
- Decision on the level of human oversight in AI agent operations
- Choice of AI model architecture for agent development
- Strategy for handling uncertainty and trade-offs in system design
- Decision to prioritize certain system engineering domains (e.g., energy, logistics)
- Approach to ensuring alignment of AI agents with human values
- Decision on the iterative design process for AI agent capabilities
- Choice of tools and interfaces for AI agents to interact with the environment
- Strategy for scaling AI agent capabilities over time
- Decision on collaboration frameworks for AI agents and human engineers
- Approach to documenting and sharing findings from experiments
- Strategy for addressing potential ethical concerns in AI agent deployment
- Decision on the timeline and milestones for project development
- Choice of data sources for training AI agents on system engineering tasks