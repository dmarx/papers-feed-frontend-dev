<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Send a Real Number Using a Single Bit (and Some Shared Randomness)</title>
				<funder ref="#_KrBXfgh #_XQgYPd3 #_qgcWXCq">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-05-20">20 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ben</forename><surname>Ran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">VMware Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Basat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">VMware Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">VMware Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shay</forename><surname>Vargaftik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University College London</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">VMware Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Send a Real Number Using a Single Bit (and Some Shared Randomness)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-20">20 May 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">A5A295D8F2DD343E9205C397567B804D</idno>
					<idno type="arXiv">arXiv:2010.02331v4[cs.DS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2012 ACM Subject Classification Theory of computation → Rounding techniques; Theory of computation → Stochastic approximation phrases Randomized Algorithms</term>
					<term>Approximation Algorithms</term>
					<term>Shared Randomness</term>
					<term>Distributed Protocols</term>
					<term>Estimation</term>
					<term>Subtractive Dithering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the fundamental problem of communicating an estimate of a real number x ∈ [0, 1] using a single bit. A sender that knows x chooses a value X ∈ {0, 1} to transmit. In turn, a receiver estimates x based on the value of X. The goal is to minimize the cost, defined as the worst-case (over the choice of x) expected squared error.</p><p>We first overview common biased and unbiased estimation approaches and prove their optimality when no shared randomness is allowed. We then show how a small amount of shared randomness, which can be as low as a single bit, reduces the cost in both cases. Specifically, we derive lower bounds on the cost attainable by any algorithm with unrestricted use of shared randomness and propose optimal and near-optimal solutions that use a small number of shared random bits. Finally, we discuss open problems and future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the fundamental problem of communicating an estimate of a real number x ∈ [0, 1] using a single bit. A sender, that we call Buffy, knows x, and chooses a value X ∈ {0, 1} to transmit. In turn, a receiver, that we call Angel, estimates x based on the value of X.</p><p>This problem naturally appears in distributed computations where multiple machines perform parallel tasks and transmit their results/state to an aggregator. If the bandwidth to the aggregator is limited, the machines must compress the data before sending it. Bandwidth optimization is fundamental in many domains, including network measurements <ref type="bibr">[3,</ref><ref type="bibr">9]</ref> and telemetry <ref type="bibr">[4]</ref>, load balancing <ref type="bibr">[15,</ref><ref type="bibr" target="#b19">22]</ref>, and satellite communication <ref type="bibr" target="#b22">[25]</ref>. We are especially motivated by recent work addressing the communication bottleneck in distributed and federated machine learning <ref type="bibr">[12]</ref>. There, clients compute a local gradient and send it to a parameter server that computes the global gradient and updates the model <ref type="bibr">[14]</ref>. For the typical large-scale federated learning problems over edge devices (e.g., mobile phones), the devices may only be able to communicate a small number of bits per gradient coordinate. In fact, solutions such as 1-Bit SGD <ref type="bibr" target="#b16">[19]</ref> and signSGD <ref type="bibr">[5]</ref>, have recently been studied as appealing low-communication solutions that use a single bit per coordinate. Another common Table 1 A summary of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We start with some notation. We use <ref type="bibr">[n]</ref> to denote {0, 1, . . . , n -1}, and ∆(S) to denote all possible probability distributions over the set S. (An element of ∆(S) will be expressed as a density function when S is uncountable, e.g., if S = [0, 1].) We also use, for a binary predicate B, 1 B as an indicator such that 1 B = 1 if B is true and 0 otherwise. Lastly, ϕ = (1 + √ 5)/2 denotes the Golden Ratio, which naturally comes up in some of our results.</p><p>Problem statement: Given a real number x ∈ [0, 1], Buffy compresses it to a single bit value X ∈ {0, 1} that is sent to Angel, who derives an estimate x. We also consider the special case where x is known to be in {0, 1/2, 1}. Our objective is to minimize the cost that is defined as the worst-case expected squared error, i.e., max x∈[0,1] E[( x -x) 2 ]. Note that the worst-case is taken over the value of x and the expectation is over the randomness of the algorithm. In the unbiased setting, we additionally require E[ x] = x, in which case the cost becomes Var[ x], i.e., the estimation variance. In some cases, we allow the parties to use ℓ bits of shared randomness. That is, we assume that they have access to a random value h ∈ [2 ℓ ], known to both Buffy and Angel. When applicable, we use r ∈ [0, 1] to denote the private randomness of Buffy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms without Shared Randomness</head><p>We recap the performance of two standard algorithms -randomized and deterministic rounding. Interestingly, we show that when no shared randomness is allowed, randomized rounding is an optimal unbiased algorithm, and deterministic rounding is an optimal biased algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Randomized Rounding</head><p>In randomized rounding, Buffy uses private randomness to generate X ∼ Bernoulli(x) which is sent using a single bit. In turn, Angel estimates x = X. Clearly, we have that E[ x] = E[X] = x, and thus the algorithm is unbiased. The variance of the algorithm is Var[ x] = Var[X] = x(1 -x), and thus the worst-case is reached at x = 1/2, which gives a cost of 1/4. The following theorem, whose proof is deferred to Appendix A, shows that randomized rounding is optimal, in the sense that no unbiased algorithm without shared randomness can have a worst-case variance lower than 1/4. Intuitively, requiring the estimate to be unbiased forces the algorithm to send 1 with a probability that is linear in x, maximizing its cost for x = 1/2. The proof also establishes the intuitive idea that it is not possible to benefit from randomness used solely by Angel.</p><p>▶ Theorem 1. Any unbiased algorithm without shared randomness must have a worst-case variance of at least 1/4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deterministic Rounding</head><p>With deterministic rounding, Buffy sends X = 1 when x ≥ 1/2. Angel then estimates x = X/2 + 1/4. Deterministic rounding has an (absolute) error of at most 1/4, which is achieved for x ∈ {0, 1/2, 1}. Therefore, its cost is 1/16. The next theorem, whose proof appears in Appendix B, shows that deterministic rounding is optimal, as no algorithm that does not use shared randomness can have a lower cost (even with unrestricted private randomness). We show that any such algorithm must have an expected squared error of at least 1/16 on at least one of {0, 1/2, 1}.</p><p>▶ Theorem 2. Any algorithm without shared randomness must have a worst-case expected squared error of at least 1/16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Lower Bounds</head><p>We next explore lower bounds for algorithms with shared randomness. We use Yao's minimax principle <ref type="bibr" target="#b21">[24]</ref> to prove a lower bound on the cost of any biased shared randomness protocol. Then, we show a stronger lower bound for unbiased algorithms using a different approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lower Bound for Biased Algorithms</head><p>We place Yao's general formulation in the context of our specific problem.</p><p>▶ Theorem 3. <ref type="bibr">([24]</ref>) Consider our estimation problem over the inputs x ∈ [0, 1], and let A be the set of all possible deterministic algorithms. For a (deterministic) algorithm a ∈ A and input x ∈ [0, 1], let the function c(a, x) = (a(x) -x) 2 be its squared error. Then for any randomized algorithm A and input distribution q ∈ ∆([0, 1]) such that X ∼ q:</p><formula xml:id="formula_0">max x∈[0,1] E [c(A, x)] ≥ min a∈A E [c(a, X)] .</formula><p>That is, the expected squared error (over the choice of x from distribution q) of the best deterministic algorithm (for q) lower bounds the expected squared error of any randomized (potentially biased) algorithm A for the worst-case x (i.e., its cost). Further, the inequality holds as an equality for the optimal distribution q and algorithm A, i.e.,</p><formula xml:id="formula_1">min A max x∈[0,1] E [c(A, x)] = max q min a∈A E [c(a, X)] .</formula><p>We proceed by selecting distributions q to lower bound min a∈A E [c(a, X)]. Notice that a deterministic algorithm can be defined using two values v</p><formula xml:id="formula_2">0 , v 1 ∈ [0, 1], such that if |x-v 0 | ≤ |x-v 1 | then Buffy sends 0 and Angel estimates x as v 0 . Similarly, if |x-v 0 | &gt; |x-v 1 |</formula><p>then Buffy sends 1 and Angel estimates x as v 1 .<ref type="foot" target="#foot_0">foot_0</ref> In general, the above framework asserts that the cost, for the worst-case input, of any randomized algorithm is</p><formula xml:id="formula_3">max q∈∆([0,1]) min v0,v1∈[0,1] 1 0 min (x -v 0 ) 2 , (x -v 1 ) 2 q(x)dx .</formula><p>(1)</p><p>Our framework lower bounds the cost for any (biased or unbiased) algorithm that may use any amount of (shared or private) randomness. We now consider distributions q to lower bound the cost and later discuss the limitations of this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">The {0, 1/2, 1} case</head><p>First, consider a discrete probability distribution q over {0, 1/2, 1}, and assume without loss of generality that q(0) ≤ q(1). Any deterministic algorithm cannot estimate all values exactly, and it must map at least two of the points to a single value, thus allowing us to lower bound its cost. In Appendix C, we prove the following.</p><p>▶ Lemma 4. Any deterministic algorithm must incur a cost of at least q(0)•q(1/2) 4(q(0)+q(1/2)) .</p><p>For q(0) = q(1) = (2 -√ 2)/2 and q(1/2) = √ 2 -1, this lemma yields a lower bound of 3/4 -1/ √ 2 ≈ 0.04289. In Section 6.1, we show that this is an optimal lower bound when x is known to be in {0, 1/2, 1}, by giving an algorithm with a matching cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">The [0, 1] case</head><p>In the general case, where x can take on any value in [0, 1], we can get a tighter bound by looking at mixed distributions. Specifically, for parameters a, w ∈ [0, 1/2], we consider the distribution where:</p><formula xml:id="formula_4">q(x) =        0 with probability w 1 with probability w uniform on [a, 1 -a] otherwise .</formula><p>Directly analyzing the optimal deterministic algorithm for this distribution proves complex. Instead, we first hypothesize that there exists an optimal deterministic algorithm for which either (1) v 1 = 1 -v 0 or (2) v 1 = 1. We emphasize that the lower bound holds even if the hypothesis is false. We then analyze what values of a, w maximize the cost of the best deterministic algorithm with the above form. Finally, we verify that the lower bound for the resulting distribution (with the specific a, w values) holds for all deterministic algorithms.</p><p>For case (1), we can express the cost as 2</p><formula xml:id="formula_5">• wv 2 0 + 1/2-w 1/2-a 1/2 a (x -v 0 ) 2 dx . Similarly, for case (2), we get a cost of wv 2 0 + 1-2w 1-2a • min{1-a,(v0+1)/2} a (x -v 0 ) 2 dx + 1-a (v0+1)/2 (x -1) 2 dx .</formula><p>Therefore, the cost of the optimal algorithm from the above family is given as:</p><formula xml:id="formula_6">min 2 • wv 2 0 + 1/2 -w 1/2 -a 1/2 a (x -v0) 2 dx , wv 2 0 + 1 -2w 1 -2a • min{1-a,(v 0 +1)/2} a (x -v0) 2 dx + 1-a (v 0 +1)/2 (x -1) 2 dx .</formula><p>This cost is maximized for a</p><formula xml:id="formula_7">= -2w 2 +w-2 √ w(1-w)+1 4w 2 -6w+2</formula><p>, where the value of w satisfies</p><formula xml:id="formula_8">32w 3 -56w 2 + w(1 -w) • (8w 4 -24w 3 + 38w 2 -8w -7) + 24w = 0.</formula><p>The resulting bound is slightly larger than 0.0459. Next, we verify that for these a, w values, no deterministic algorithm can achieve a lower cost. Specifically, instead of using q(x) as described above, we generate a finite discrete distribution. For a parameter n ∈ N, we define:</p><formula xml:id="formula_9">q n (x) =        ⌊n•w⌋ n if x ∈ {0, 1} 1 n if x ∈ a + 1-2a 2(n-2⌊n•w⌋)) + i • 1-2a n-2⌊n•w⌋ | i ∈ [n -2 ⌊n • w⌋] 0 otherwise .</formula><p>Note that lim n→∞ q n (x) = q(x). We then find the optimal deterministic solution for this distribution by using a deterministic k-means clustering algorithm (for k = 2), that is guaranteed to converge, e.g., using <ref type="bibr">[8]</ref>. The code that we used to obtain this result is available at <ref type="bibr">[2]</ref>. The optimal deterministic algorithm for q n (x) tends to have either v 1 = 1 -v 0 or v 1 = 1 as hypothesized. Finally, for n = 10 6 we get a cost higher than 0.0459 which we use as a lower bound.</p><p>We do not believe that this bound is tight. Nonetheless, as we show in Section 6.2.4, our bound is within 0.2% of the optimum.</p><p>given in Appendix D, proceeds with a case analysis based on the value of p 0,1 , the probability that the sender would send the same bit for 0, 1. We show that there exists an optimal algorithm in which p 0,1/2 + p 1/2,1 + p 0,1 = 1, p 0,1/2 = p 1/2,1 , and G 1/2,1 = 1 -G 0,1/2 . This reduces the number of variables to three, allowing us to optimize the expression and show a lower bound of 1/16 on any unbiased algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Algorithms with Unbounded Private Randomness</p><p>Here, we consider the case where the shared randomness is limited to ℓ bits, i.e., h ∈ [2 ℓ ], but Buffy may use unbounded private randomness r ∼ U [0, 1] (that is independent of h).</p><p>We present the following algorithm: Buffy sends X to Angel, where</p><formula xml:id="formula_10">X ≜ 1 if x ≥ (r + h)2 -ℓ 0 otherwise . Angel then estimates x = X + (h -0.5(2 ℓ -1)) • 2 -ℓ .</formula><p>We first show that our protocol is unbiased. It holds that</p><formula xml:id="formula_11">E[h] = 0.5(2 ℓ -1) and (r + h) ∼ U [0, 2 ℓ ] (i.e., (r + h)2 -ℓ ∼ U [0, 1]), and thus E[ x] = E[X] = x.</formula><p>We now state theorem, whose proof appears in Appendix E, that bounds the variance:</p><formula xml:id="formula_12">▶ Theorem 5. Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).</formula><p>In Appendix F, we describe a simple generalization of this algorithm, together with a lower bound, for sending k &gt; 1 bits. We now explain the connection to subtractive dithering and explore the applicability of the algorithm for the x ∈ {0, 1/2, 1} special case.</p><p>Connection to subtractive dithering: First invented for improving the visibility of quantized pictures <ref type="bibr">[17]</ref>, subtractive dithering aims to alleviate potential distortions that originate from quantization. Subtractive dithering was later extended for other domains such as speech <ref type="bibr">[6]</ref>, distributed deep learning <ref type="bibr">[1]</ref>, and federated learning <ref type="bibr" target="#b17">[20]</ref>.</p><p>In our setting, subtractive dithering corresponds to using shared randomness to add noise ς to x before applying a deterministic quantization and subtracting ς from the estimation. Specifically, let Q : [0, 1] → {0, 1} be a two-level deterministic quantizer such that Q(g) = 1 if g ≥ 1/2 and 0 otherwise. Then, in subtractive dithering Buffy sends X = Q(x + ς) and Angel estimates x = X -ς.</p><p>There are several noise classes that ς can be drawn from, as classified in <ref type="bibr" target="#b15">[18]</ref>, that yield</p><formula xml:id="formula_13">x ∼ U [x -1/2, x + 1/2]. For example, ς can be distributed uniformly on [-1/2, 1/2].</formula><p>Consider our algorithm of this section without restricting the number of random bits (i.e., ℓ → ∞, and rescale so h ∈ U [0, 1]). This would yield the following algorithm:</p><formula xml:id="formula_14">X ≜ 1 if x ≥ h 0 otherwise and x = X + h -0.5. Similarly to subtractive dithering, we get that x ∼ U [x -1/2, x + 1/2],</formula><p>as we prove in Appendix G for completeness. To see that the two algorithms are equivalent</p><formula xml:id="formula_15">(for ς ∼ U [-1/2, 1/2]), denote h ′ = 1/2 -h (i.e., h ′ ∼ U [-1/2, 1/2]). Then X = 1 if x + h ′ ≥ 1/2 and x = X -h ′ .</formula><p>Therefore, we conclude that our algorithm provides a spectrum between randomized rounding (ℓ = 0) and a form of subtractive dithering (ℓ → ∞). In practice, this means that a small number of shared random bits yields a variance that is close to that of subtractive dithering (Var[ x] = 1/12). For example, with a single shared random byte (i.e., ℓ = 8), our algorithm has a worst-case variance that is within 0.02% of 1/12. The x ∈ {0, 1/2, 1} case:</p><p>Notice that if x is known to be in {0, 1/2, 1}, then our (ℓ = 1) algorithm gives Var[ x] = 1/16, as evident from Theorem 5. Further, in this case, we do not require the private randomness as we can rewrite Buffy's algorithm as:</p><formula xml:id="formula_16">X ≜        0 if x = 0 1 -h if x = 1/2 1 if x = 1</formula><p>, while Angel estimates x = X + (h -0.5)/2. This algorithm considerably improves over randomized rounding (which is optimal when no shared randomness is allowed, as shown in Appendix A), that has a variance of 1/4 for x = 1/2; i.e., a single shared random bit reduces the worst-case variance by a factor of 4. Further, it also improves over subtractive dithering, reducing the variance by a 4/3 factor. Finally, this result is optimal according to the Section 4.2 lower bound, even if unbounded shared randomness is allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Algorithms without Private Randomness</head><p>In some cases, generating random bits may be expensive, e.g., when running on powerconstrained devices. This is particularly acute when the device operates in an energy harvesting mode <ref type="bibr" target="#b23">[26]</ref>. Past works have even considered how to "recycle" random bits (e.g., <ref type="bibr">[11]</ref>). Therefore, it is important to study how to design algorithms that use just a few random bits. To address this need, we consider scenarios where Buffy and Angel have access to a shared ℓ-bit random value h, but no private randomness. One thing to notice is that Angel can produce at most 2 ℓ+1 different values since Angel is deterministic after obtaining the ℓ + 1 bits of h and X. In particular, this means there is no unbiased protocol for general x ∈ [0, 1]. Therefore, we focus on biased algorithms and study how shared randomness allows improving over deterministic rounding (which is optimal without shared randomness, as we show in Section 3.2).</p><p>We start by proposing an optimal algorithm for the case where x is known to be in {0, 1/2, 1}. Then, we present adaptations of the subtractive dithering estimation method for the biased x ∈ [0, 1] setting. These improve over both (unbiased) subtractive dithering and deterministic rounding. To the best of our knowledge, these adaptations are novel. Next, we show how Buffy can further reduce the cost while, among other changes, using a small number of shared random bits. We conclude by giving design principles for numerically approximating the optimal algorithm and give realizations for small number of shared random bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The x ∈ {0, 1/2, 1} Case</head><p>We now consider the scenario where x is guaranteed to be in {0, 1/2, 1} using a single shared randomness bit h ∈ {0, 1}. For some α ∈ [0, 1], Buffy sends</p><formula xml:id="formula_17">X = 1 if x = 1 ∨ (x = 1/2 ∧ h = 0) 0 otherwise while Angel estimates x = α • h + (1 -α) • X.</formula><p>For example, this means that if x = 0, the squared error is 0 if h = 0 and α 2 otherwise. That is, the expected squared error is α 2 /2. We optimize over the α value to minimize the cost min</p><formula xml:id="formula_18">α∈[0,1] max α 2 /2, (1 -(1 -α)) 2 /2, E (1/2 -(α • h + (1 -α) • (1 -h))) 2 . This is optimized for α = 1 -1/ √ 2, yielding a cost of 3/4 -1/ √ 2 ≈ 0.</formula><p>04289, which is optimal according to our Section 4 lower bound, even if unbounded shared randomness is allowed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The x ∈ [0, 1] Case</head><p>An important observation regarding optimal biased algorithms is that they, without loss of generality, can be expressed as a pair of monotone increasing functions T, Z 0 : [0, 1] → [0, 1] as follows. Here T is a threshold function that determines whether 0 or 1 is sent, Z 0 is the estimator when 0 is received, and</p><formula xml:id="formula_19">Z 1 : [0, 1] → [0, 1], given by Z 1 (h) = 1 -Z 0 (1 -h), is the estimator when 1 is received. That is, Buffy sends X = 1 if x ≥ T (h) 0 otherwise .</formula><p>In turn, Angel estimates x = Z X (h). We further explain this representation in Appendix H.</p><p>Based on this observation, we next lay out a sequence of algorithmic improvements over deterministic rounding that leverage the shared randomness to reduce the cost. We visualize the algorithms resulting from each improvement in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Subtractive dithering adaptations</head><p>As subtractive dithering provides the lowest cost (albeit using unbounded shared randomness) of the previously mentioned unbiased algorithms, one may wonder if it is possible to adapt it to the biased scenario. Accordingly, we first briefly overview two natural adjustments that use unbounded shared randomness and improve over the 1/16 cost of deterministic rounding.</p><p>We then propose improved protocols that reduce the cost further despite using only a small number (e.g., ℓ = 3) of random bits. Intuitively, subtractive dithering may produce estimates that are outside the [0, 1] range. Therefore, by truncating the estimates to [0, 1] one may only reduce the expected squared error for any x ̸ = 1/2. However, it does not reduce the expected squared error for x = 1/2, and thus the cost would remain 1/12.</p><p>To reduce the cost, one may further truncate the estimates to [z, 1-z] for some z ∈ [0, 1/2]. Indeed, we show in Appendix I that this truncation reduces the cost to ≈ 0.0602, for z satisfying 1/24 + z 2 /2 + (2z 3 )/3 = 0 (z ≈ 0.17349).</p><p>A better adaptation strategy is obtained by changing the estimation to a linear combination of X and h. Specifically, consider the protocol where Buffy sends (for a shared h ∼ U [0, 1])</p><formula xml:id="formula_20">X = 1 if x ≥ h 0 otherwise</formula><p>and Angel estimates, for some α ∈ [0, 1],</p><formula xml:id="formula_21">x = α • h + (1 -α) • X. 1 3 2 3 T (h) Buffy: X =    1 if x ≥ T (h) 0 Otherwise 0 1 4 1 2 3 4 1 h 1 3 Z (h) Angel: x =    Z 0 (h) if X = 0 Z 1 (h) Otherwise Deterministic Rounding (Section 3.2)</formula><p>Subtractive Adaptation (Section 6.2.1) Deterministic Edge Rounding (Section 6.2.2)</p><p>Linear Sigmoid (Section 6.2.3)</p><p>Optimal Sigmoid 8-bit Approximation (Section 6.2.4)</p><formula xml:id="formula_22">0 1 4 1 2 3 4 1 h 0 1 3 2 3 ∂T (h) ∂h Figure 1</formula><p>Illustration of the different biased algorithms. While deterministic rounding does not use the shared randomness and is thus constant, the other algorithms have both the threshold and estimation be monotone functions of h.</p><p>Optimizing the parameters, we show in Appendix J that this algorithm achieves a cost of 5/3 -ϕ ≈ 0.04863, which is obtained for α = 2 -ϕ ≈ 0.382. Interestingly, this cost is achieved for all x ∈ [0, 1].</p><p>In Figure <ref type="figure">1</ref>, we illustrate this algorithm. As shown, the subtractive dithering adaption has T (h) = h and Z 0 (h) = α • h. This means that Buffy sends X = 1 if x ≥ h and Angel estimates</p><formula xml:id="formula_23">x = α • h if X = 0 and x = (1 -α • (1 -h)) = (1 -α) + α • h otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Deterministically rounding extreme values</head><p>We now show how to leverage a finite number of shared random bits ℓ to design improved algorithms. As we show, it is possible to benefit from deterministically rounding values that are "close" to 0 or 1 and use the shared randomness otherwise.</p><p>Similarly to the subtractive dithering adaptation above, Angel estimates x using a linear combination of h (with weight α) and X (with a weight of 1 -α), where α ∈ [0, 1] is chosen later. For all i ∈ [2 ℓ -1], define the interval</p><formula xml:id="formula_24">I i = (1 -α)/2 + i • α 2 ℓ -1 , (1 -α)/2 + (i + 1) • α 2 ℓ -1 . (<label>2</label></formula><formula xml:id="formula_25">)</formula><p>In our algorithm, Buffy sends</p><formula xml:id="formula_26">X =        0 if x &lt; (1 -α)/2 1 h≤i if x ∈ I i , i ∈ [2 ℓ -1] 1 if x ≥ (1 + α)/2</formula><p>, and Angel estimates:</p><formula xml:id="formula_27">x = α • h/(2 ℓ -1) + (1 -α) • X.</formula><p>Note that we deterministically partition the range [(1 -α)/2, (1 + α)/2] into 2 ℓ -1 equally spaced intervals. Intuitively, these intervals are chosen in a way that makes the expected squared error a continuous function of x, as our analysis, given in Appendix K, indicates.</p><p>As we show, minimizing cost = min α max x E[( x -x)<ref type="foot" target="#foot_1">foot_1</ref> ] yields cumbersome expressions. For example, we get that with one shared random bit (ℓ = 1), our algorithm has a cost of 1/18 ≈ 0.05556 (obtained for 2 α = 1/3), lower than that of deterministic rounding (i.e., 1/16). For ℓ = 2, we obtain a cost of 259-140</p><formula xml:id="formula_28">√ 3 338 ≈ 0.04885 (reached for α = 15-6 √ 3 13</formula><p>), and ℓ = 3 bits further reduces the cost to 35/722 ≈ 0.04848 (when α = 7/19). Additionally, with ℓ = 3 bits, this improves over the subtractive dithering adaptions (that use unbounded shared randomness) for all x ∈ [0, 1]. Notice that these costs are ≈21%, ≈6.4%, and ≈5.6% from the ≈ 0.0459 lower bound (see Section 4.1.2), and thus from the optimal algorithm. For completeness, we give the limiting algorithm (as ℓ → ∞) in Appendix L. For intuition, we illustrate the limiting algorithm (h ∈ [0, 1]) in Figure <ref type="figure">1</ref>. As shown, we have</p><formula xml:id="formula_29">T (h) = 1-α 2 +α •h (where α = 2 -ϕ ≈ 0.38) and Z 0 (h) = α • h.</formula><p>Observe that Angel uses the same estimation function as in Section 6.2.1, but Buffy's threshold function is different. Intuitively, the new threshold function ensures that each x is mapped to the closest estimate value. For example, if x = 0.1 and h = 0, the subtractive adaptation would have X = 1 and thus x = 1 -α ≈ 0.62 while here we get X = 0 and x = 0.</p><p>Interestingly, the cost slightly and monotonically increases when increasing the number of bits ℓ beyond 3. This phenomenon suggests that we need more complex algorithms to leverage additional available random bits. We explore several approaches; in Appendix M, we show that by probabilistically selecting between the above algorithm (for ℓ → ∞) and the {0, 1/2, 1} algorithm from Section 6.1, we can reduce the error to 6</p><formula xml:id="formula_30">√ 10+11 √ 5-18 √ 2-17 24 ≈ 0.04644.</formula><p>Intuitively, Buffy and Angel can implicitly agree on the chosen algorithm using the shared randomness. Here, we proceed by analyzing the potential benefits of non-uniform partitioning of the h values, which reduces the error further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Non-uniform partitioning</head><p>Intuitively, the above algorithms have a threshold function that is linear in h; i.e., it takes the form T (h) = a • h + b. We now show that this can be improved by looking at sigmoidlike functions. For ease of exposition, in this section, we consider h ∈ [0, 1] to represent unbounded shared randomness, although the algorithm can be discretized given sufficient random bits. Recall from Section 6.2 that an algorithm can be expressed as a pair of functions T, Z 0 : [0, 1] → [0, 1] such that Buffy sends 1 if x ≥ T (h) while Angel estimates Z 0 (h) when receiving X = 0 and Z 1 (h) = 1 -Z 0 (1 -h) otherwise. Here, we consider a linear sigmoid function (also illustrated in Figure <ref type="figure">1</ref>), which, for some h 0 ∈ [0, 1/2], is defined as</p><formula xml:id="formula_31">T (h) =        α if h &lt; h 0 α + (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ] 1 -α otherwise . Z 0 (h) =        0 if h &lt; h 0 (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ] 1 -2α otherwise .</formula><p>Notice that in this algorithm we have Z 0 (h) = T (h) -α.</p><p>Our analysis, given in Appendix N, shows that the cost is minimized for h 0 = 1/4, α = 1/3, where the error is:</p><formula xml:id="formula_32">E[( x -x) 2 ] = E[( x) 2 ] -2xE[ x] + x 2 =        5/108 -x/3 + x 2 if x &lt; 1/3 5/108 if x ∈ [1/3, 2/3] 77/108 -5x/3 + x 2 otherwise .</formula><p>Therefore, the cost is 5/108 ≈ 0.0463, which is less than 0.9% higher than the 0.0459 lower bound (Section 4.1.2). The algorithm has two interesting properties. First, its expected squared error is constant for all x ∈ {0, 1} ∪ [1/3, 2/3] and, second, its expectation is not continuous as a function of x, as shown in Figure <ref type="figure">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E[ x]</head><p>Deterministic Rounding (Section 3.2)</p><p>Subtractive Adaptation (Section 6.2.1) Deterministic Edge Rounding (Section 6.2.2)</p><p>Linear Sigmoid (Section 6.2.3)</p><p>Optimal Sigmoid 8-bit Approximation (Section 6.2.4)</p><p>Figure <ref type="figure">2</ref> Illustration of the expectation of the different biased algorithms. Deterministic rounding does not use randomization and is therefore a step function, while others increase gradually in x. Notice that the expectations of the linear sigmoid and optimal approximation are not continuous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.4">Towards the optimal algorithm</head><p>We now consider more general algorithms that have arbitrary estimate function Z 0 . To that end, we use a numerical solver that approximates the optimal solution. Clearly, to define the input problem, we need to limit the number of variables and constraints. We achieve this using several observations:</p><p>We consider bounded shared randomness h ∈ [2 ℓ ] for ℓ ∈ N bits. In fact, bounded shared randomness is precisely what allows us to develop this numerical approach. We use the observation that an optimal algorithm's T and Z 0 functions are not independent and satisfy ∀h ∈ [0, 1] :</p><formula xml:id="formula_33">T (h) = Z0(h)+Z1(h) 2 = Z0(h)+(1-Z0(1-h))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>; this is because, that way, every x ∈ [0, 1] is estimated using the value closer to it between Z 0 (h) and Z 1 (h) = 1 -Z 0 (1 -h). In fact, the algorithms in sections 6.2.2-6.2.3 follow this rule, while the subtractive adaptation (Section 6.2.1) does not. As a result, we can define the variables z h |h ∈ [2 ℓ ] and derive the thresholds from the solver's output by Z 0 (h) = z h . For computing the maximal error for any x ∈ [0, 1], it is enough to look at a discrete set of points. This is because the number of possible estimates is 2 ℓ+1 . Therefore, given two estimates z h , 1 -z 2 ℓ -1-h that correspond to the values Angel uses given h and X = 0 or X = 1, the worst expected squared error (for this h) is obtained for</p><formula xml:id="formula_34">y h ≜ z h +1-z 2 ℓ -1-h 2</formula><p>. Therefore, by checking all x ∈ y h | h ∈ [2 ℓ ] , we can compute the cost. Deterministic Rounding (Section 3.2) Subtractive Adaptation (Section 6.2.2) Deterministic Edge Rounding `=3 (Section 6.2.3) Linear Sigmoid (Section 6.2.4) Optimal Sigmoid 8-bit Approximation (Section 6.2.5) 0 1=2 1 x Deterministic Rounding (Section 3.2) Subtractive Adaptation (Section 6.2.2) Biased © 0;1=2;1 ª (Section 6.1) 0:848 0:852 0:08333 0:08334</p><p>Figure <ref type="figure">3</ref> An illustration of the variance and expected squared error of the different algorithms. As shown, our unbiased algorithm is competitive with subtractive dithering despite using a single shared random byte, while our single-bit algorithm improves over subtractive on {0, 1/2, 1}. For the biased case, in addition to improving the {0, 1/2, 1} case, our optimal sigmoid approximation algorithm achieves the lowest cost (less than 0.2% of the optimum!) while using a single shared random byte.</p><p>Using these observations, we formulate the input as: minimize</p><formula xml:id="formula_35">{z h |h∈[2 ℓ ]} C subject to C ≥ h j=0 (y h -(1 -z 2 ℓ -1-h )) 2 + 2 ℓ -1 j=h+1 (y h -z h ) 2 , h = 0, . . . , 2 ℓ -1 y h = z h +1-z 2 ℓ -1-h 2 , z h ∈ [0, 1] h = 0, . . . , 2 ℓ -1</formula><p>In the above, we express the expected squared error at y h by considering the h values for which x ≥ T (h) (j ∈ [h]) and those that x &lt; T (h). The output for the above problem does not seem to follow a compact representation. However, it is still possible to implement using a simple lookup table. For example, if ℓ = 8, we can store all z h when implementing Buffy and Angel. This algorithm's cost is lower than that of the linear sigmoid (that uses unbounded randomness) when using ℓ ≥ 4 bits. Specifically, using 4 shared random bits, the cost is ≈ 0.04611, while using 8 bits, it further reduces to ≈ 0.04599. Notice that these are less than 0.5% and 0.2% higher than the lower bound of Section 4.1.2. We note that this approach yields improvement even for a small number of shared random bits; for example, using ℓ = 1 bit (h ∈ {0, 1}), we get a cost of 1/20 for z 0 = 0.1, z 1 = 0.3 which is equivalent to the following algorithm:</p><formula xml:id="formula_36">X = 1 if x ≥ 0.4 + 0.2h 0 otherwise , x = 0.1 + 0.2h + 0.6X .</formula><p>We visualize the resulting algorithm, for ℓ = 8, in figures 1 and 2. Notice that while the algorithm looks almost similar to our linear sigmoid, looking that the derivative ∂T (h) ∂h (Figure <ref type="figure">1</ref>) shows that this optimal solution is not piece-wise linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Comparison of the Algorithm Costs</head><p>We illustrate the various algorithms in Figure <ref type="figure">3</ref>. In the unbiased case, notice how a single (ℓ = 1) shared random bit significantly improves over randomized rounding (which is optimal when Buffy and Angel are restricted to private randomness). This further improves for larger ℓ values, where for ℓ = 8 we have a cost that is only 0.02% higher than that of subtractive dithering, which uses unbounded shared randomness (the difference shown in zoom). When</p><p>x is known to be in {0, 1/2, 1} (right-hand side of the figure), it is evident how our unbiased ℓ = 1 algorithm improves over both randomized rounding and subtractive dithering.</p><p>In the biased case, our adaptation to the subtractive dithering estimation (termed Subtractive Adaptation) improves over the cost of deterministic rounding. This is further improved by the algorithm of Section 6.2.2, termed Deterministic Edge Rounding, which is depicted using ℓ = 3 bits as it minimizes its cost. Next, the Linear sigmoid (Section 6.2.3) shows how to lower the cost (using unbounded shared randomness) by non-uniform partitioning of the h values. Additionally, we show the optimal 8-bit algorithm (Section 6.2.4) that gets within 0.2% from the lower bound while using a single shared random byte. Finally, if x is known to be in {0, 1/2, 1}, our (optimal) biased {0, 1/2, 1} algorithm improves over all other solutions while using only a single shared random bit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>In this paper, we studied upper and lower bounds for the problem of sending a real number using a single bit. The goal is to minimize the cost, which is the worst-case variance for unbiased algorithms, or the worst-case expected squared error for biased ones. For all cases, we demonstrated how shared randomness helps to reduce the cost. Motivated by real-world applications, we derived algorithms with a bounded number of random bits that can be as low as a single shared bit. For example, in the unbiased case, using just one shared random bit reduces the variance two-fold compared to randomized rounding (which is optimal when no shared randomness is available). Further, using a single byte of shared randomness, our algorithm's variance is within 0.02% from the state of the art, which uses unbounded shared randomness.</p><p>Our results are also near-optimal in the biased case, with a gap lower than 0.2% between the upper and lower bounds with a single shared random byte. Our upper bound is presented as a sequence of algorithms, each generalizing the previous while reducing the cost further.</p><p>For the special case where x is known to be in {0, 1/2, 1}, we give optimal unbiased and biased algorithms, together with matching lower bounds. Our algorithms use a single shared random bit, and the lower bounds show that the cost cannot be improved even when unbounded shared randomness is allowed.</p><p>We conclude by identifying directions for future research, beyond settling the correct bounds. First, our lower bounds apply for algorithms that use unbounded shared randomness, and new techniques for developing sharper bounds for other cases are of interest. Another direction is looking into optimizing the cost when sending k bits, for some k &gt; 1. We make a first small step in Appendix F, where we provide simple generalizations of our unbiased algorithm and lower bound to sending k bits. Also, in a recent followup work <ref type="bibr" target="#b18">[21]</ref>, we showed that for sending d-sized real vector using d(1 + o(1)) bits it is better to encode all coordinates together rather than sending them separately. Intuitively, we can reduce the error by generating an encoded vector, that mixes the original vector entries, before sending them. It is interesting to formalize the bounds for sending vectors similarly to the single number case. One possible direction is to send the encoded coordinates using the tools developed in this paper. Finally, we are unclear on whether private randomness can help improve biased algorithms (see Table <ref type="table">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Optimality of Randomized Rounding</head><p>We show that without shared randomness, randomized rounding is optimal in the sense that it minimizes the worst-case estimation variance. Consider an arbitrary protocol. We model it as follows: we have two (deterministic) parameters: Y : [0, 1] → [0, 1] and Γ : {0, 1} → ∆([0, 1]).</p><p>Buffy computes p = Y (x) and sends X ∼ Bernoulli(p). In turn, Angel receives X, and estimates x by drawing from the distribution Γ(X). We also denote by Z 0 ∼ Γ(0) and Z 1 ∼ Γ(1) random variables such that the final estimate is</p><formula xml:id="formula_37">x = 1 X=0 • Z 0 + 1 X=1 • Z 1 .</formula><p>Notice that this formulation captures any protocol. For example, randomized rounding is defined as</p><formula xml:id="formula_38">Y (x) = x and Γ(X)(y) = 1 if y = X 0 otherwise .</formula><p>(this is a slight abuse of notation as the above definition assumes that Γ(X) is a density function). We demand that the protocol will produce unbiased estimates for any x. That is, it must satisfy:</p><formula xml:id="formula_39">E[ x] = Y (x) • E [Z 1 ] + (1 -Y (x)) • E [Z 0 ] = x.</formula><p>(3)</p><p>In particular, for x = 0, we have:</p><formula xml:id="formula_40">Y (0) • E [Z 1 ] + (1 -Y (0)) • E [Z 0 ] = 0.</formula><p>and equivalently:</p><formula xml:id="formula_41">E [Z 0 ] = - Y (0) 1 -Y (0) • E [Z 1 ] . (4)</formula><p>Similarly, plugging x = 1 into (3) gives:</p><formula xml:id="formula_42">Y (1) • E [Z 1 ] + (1 -Y (1)) • E [Z 0 ] = 1.</formula><p>Using (4), we proceed with several simplifications:</p><formula xml:id="formula_43">Y (1) • E [Z 1 ] + (1 -Y (1)) • - Y (0) 1 -Y (0) • E [Z 1 ] = 1. E [Z 1 ] • Y (1) -(1 -Y (1)) Y (0) 1 -Y (0) = 1. E [Z 1 ] • Y (1) • (1 -Y (0)) -(1 -Y (1))Y (0) 1 -Y (0) = 1. E [Z 1 ] • Y (1) -Y (0) 1 -Y (0) = 1. E [Z 1 ] = 1 -Y (0) Y (1) -Y (0) . (<label>5</label></formula><formula xml:id="formula_44">)</formula><p>Plugging ( <ref type="formula" target="#formula_43">5</ref>) into (4), we also get:</p><formula xml:id="formula_45">E [Z 0 ] = - Y (0) 1 -Y (0) • E [Z 1 ] = - Y (0) Y (1) -Y (0) . (<label>6</label></formula><formula xml:id="formula_46">)</formula><p>Substituting ( <ref type="formula" target="#formula_43">5</ref>) and ( <ref type="formula" target="#formula_45">6</ref>) in (3), we simplify the expression further:</p><formula xml:id="formula_47">Y (x) • E [Z 1 ] + (1 -Y (x)) • E [Z 0 ] = x. Y (x) • 1 -Y (0) Y (1) -Y (0) + (1 -Y (x)) • - Y (0) Y (1) -Y (0) = x. Y (x) • (1 -Y (0)) -(1 -Y (x)) • Y (0) Y (1) -Y (0) = x. Y (x) -Y (0) Y (1) -Y (0) = x. Y (x) = x • (Y (1) -Y (0)) + Y (0). Y (x) = x • Y (1) + (1 -x) • Y (0). (<label>7</label></formula><formula xml:id="formula_48">)</formula><p>That is, we have that the probability to send X = 1 must be linear in x. We analyze the variance that results for x = 0.5.</p><formula xml:id="formula_49">Var[ x|x = 0.5] = E[( x -0.5) 2 |x = 0.5] = E[( x) 2 |x = 0.5] -E[( x) |x = 0.5] + 0.25.</formula><p>Since x is unbiased, E[( x) |x = 0.5] = 0.5 and we get</p><formula xml:id="formula_50">Var[ x|x = 0.5] = E[( x) 2 |x = 0.5] -0.25. (8) Next, we analyze E[( x) 2 |x = 0.5]: E[( x) = E (Z 0 ) 2 • 1 X=0 |x = 0.5 + E (Z 1 ) 2 • 1 X=1 |x = 0.5 .</formula><p>We have that Z 0 , Z 1 are independent of 1 X=0 , 1 X=1 and of x, and thus</p><formula xml:id="formula_51">E[( x) 2 |x = 0.5] = E (Z 0 ) 2 |x = 0.5 • (1 -Y (0.5)) + E (Z 1 ) 2 |x = 0.5 • Y (0.5) = E (Z 0 ) 2 • (1 -Y (0.5)) + E (Z 1 ) 2 • Y (0.5) ≥ (E [Z 0 ]) 2 • (1 -Y (0.5)) + (E [Z 1 ]) 2 • Y (0.5). (<label>9</label></formula><formula xml:id="formula_52">)</formula><p>Using ( <ref type="formula" target="#formula_43">5</ref>) and ( <ref type="formula" target="#formula_45">6</ref>), we have:</p><formula xml:id="formula_53">E[( x) 2 |x = 0.5] ≥ Y (0) Y (1) -Y (0) 2 • (1 -Y (0.5)) + 1 -Y (0) Y (1) -Y (0) 2 • Y (0.5) = (Y (0)) 2 • (1 -Y (0.5)) + (1 -Y (0)) 2 • Y (0.5) (Y (1) -Y (0)) 2 = (Y (0)) 2 + Y (0.5) -2Y (0)Y (0.5) (Y (1) -Y (0)) 2 .</formula><p>We now use ( <ref type="formula" target="#formula_47">7</ref>) for x = 0.5 and get Y (0.5) = 0.5 • (Y (0) + Y (1)), which means:</p><formula xml:id="formula_54">E[( x) 2 |x = 0.5] ≥ (Y (0)) 2 + Y (0.5) -2Y (0)Y (0.5) (Y (1) -Y (0)) 2 = (Y (0)) 2 + 0.5 • (Y (0) + Y (1)) -2Y (0) • 0.5 • (Y (0) + Y (1)) (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) (Y (1) -Y (0)) 2 .</formula><p>Combined with (8), this gives:</p><formula xml:id="formula_55">Var[ x|x = 0.5] = E[( x) 2 |x = 0.5] -0.25 (10) ≥ 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) (Y (1) -Y (0)) 2 -0.25 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) -0.25 (Y (1) -Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) -0.25 (Y (1)) 2 + 0.5Y (0)Y (1) -0.25 (Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -0.5Y (0) • Y (1) -0.25 (Y (1)) 2 -0.25 (Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -0.5 • (Y (0) + Y (1)) 2 (Y (1) -Y (0)) 2 . (<label>11</label></formula><formula xml:id="formula_56">)</formula><p>Over the domain Y (0), Y (1) ∈ [0, 1], (11) has two minima: Y (0) = 0, Y (1) = 1 and Y (0) = 1, Y (1) = 0. Indeed, the first corresponds to randomized rounding, while the second is using a simple transform that negates the randomized rounding's bit.</p><p>To conclude, we established that randomized rounding has a minimal worst-case variance. As a side note, by deterministically estimating x = X, Inequality (9) holds as an equality and the variance is exactly 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Optimality of Deterministic Rounding</head><p>We show that without shared randomness, deterministic rounding is an optimal biased solution. Notice that, in such a case, any protocol is defined by the probability of sending 1, denoted Y (x), and the reconstruction distributions</p><formula xml:id="formula_57">V 0 , V 1 ∈ ∆([0, 1]). Let us examine E[V 0 ] and E[V 1 ]. We assume, without lost of generality, that E[V 0 ] ≤ E[V 1 ].</formula><p>We have that:</p><formula xml:id="formula_58">E[ x] = Y (x)E[V 1 ] + (1 -Y (x))E[V 0 ].</formula><p>That is, we have that for any x ∈ [0, 1]:</p><formula xml:id="formula_59">E[V 0 ] ≤ E[ x] ≤ E[V 1 ]. Next, we have that the cost, E[( x -x) 2 ], is bounded as E[( x -x) 2 ] ≥ (E[( x -x)]) 2 .</formula><p>In particular, for x = 0, we get that</p><formula xml:id="formula_60">E[( x -x) 2 |x = 0] ≥ (E[ x|x = 0]) 2 ≥ (E[V 0 ]) 2 .</formula><p>Similarly, for x = 1, we have</p><formula xml:id="formula_61">E[( x -x) 2 |x = 1] ≥ (E[( x)|x = 1] -1) 2 ≥ (1 -E[V 1 ]) 2 . Notice that if E[V 0 ] ≥ 0.25 then E[( x -x) 2 |x = 0] ≥ 1/16, and similarly, if E[V 1 ] ≤ 0.75 then E[( x -x) 2 |x = 1] ≥ 1/16</formula><p>. Assume to the contrary that there exists an algorithm with a with a worst-case expected squared error lower than 1/16, then we have</p><formula xml:id="formula_62">E[V 0 ] ≤ 0.25 and E[V 1 ] ≥ 0.</formula><p>75. However, we have that x = 0.5 gives: In the first inequality, we used that fact that for any random variable V :</p><formula xml:id="formula_63">E[( x -x) 2 |x = 0.5] = E[ x 2 |x = 0.5] -2xE[ x|x = 0.5] + 0.25 = Y (0.5)E[V 2 1 ] + (1 -Y (0.5))E[V 2 0 ] -(Y (0.5)E[V 1 ] + (1 -Y (0.5))E[V 0 ]) + 0.25 ≥ Y (0.5)(E[V 1 ]) 2 + (1 -Y (0.5))(E[V 0 ]) 2 -(Y (0.5)E[V 1 ] + (1 -Y (0.5))E[V 0 ]) + 0.25 = Y (0.5) • E[V 1 ] • (E[V 1 ] -1) + (1 -Y (0.5)) • E[V 0 ] • (E[V 0 ] -1)</formula><formula xml:id="formula_64">E[V 2 ] ≥ (E[V ]) 2 ,</formula><p>and in the second we used E[V 0 ] ≤ 0.25 and E[V 1 ] ≥ 0.75. This concludes the proof and establishes the optimality of deterministic rounding when no shared randomness is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>Proof of the Biased {0, 1/2, 1} Lower Bound</p><p>We recall Lemma 4:</p><p>▶ Lemma 4. Any deterministic algorithm must incur a cost of at least q(0)•q(1/2) 4(q(0)+q(1/2)) . Proof. We denote by X 0 the set of values in {0, 1/2, 1} that are closer to v 0 than to v 1 . We assume without loss of generality that v 0 ≤ v 1 and q(0) ≤ q(1) and prove that an optimal algorithm would set v 0 = q(1/2) 2(q(0)+q(1/2)) , v 1 = 1, which incurs a cost of q(0)•q(1/2) 4(q(0)+q(1/2)) . Indeed, for this choice of v 0 , v 1 we have that X 0 = {0, 1/2}, and we get a cost of</p><formula xml:id="formula_65">q(0) q(1/2) 2(q(0) + q(1/2)) 2 + q(1/2) 1 2 - q(1/2) 2(q(0) + q(1/2)) 2 = q(0) q(1/2) 2(q(0) + q(1/2)) 2 + q(1/2) q(0) 2(q(0) + q(1/2)) 2 = q(0)q(1/2) 2 + q(1/2)q(0) 2 4 (q(0) + q(1/2)) 2 = q(0) • q(1/2) 4 (q(0) + q(1/2))</formula><p>.</p><p>We now bound the performance of the optimal algorithm. We first notice that an optimal algorithm should have 0 ∈ X 0 and 1 ̸ ∈ X 0 . Next, notice that v 0 should be at most 1/2 and v 1 should be at least 1/2. Otherwise, one can improve the error for x = 0 or x = 1, respectively, without increasing the error at 1/2. Further, observe that an optimal algorithm must have v 0 = 0 or v 1 = 1. That is because if 1/2 ∈ X 0 , we can reduce the error for x = 1 by setting v 1 = 1. Similarly, when 1/2 ̸ ∈ X 0 , choosing v 0 = 0 decreases the error for x = 0. Now, we claim that there exists an optimal algorithm for which v 1 = 1. Consider some solution, and set v ′ 0 = 1 -v 1 and v ′ 1 = 1. This does not affect the error of x = 1/2, and does not increase the cost as q(0) ≤ q(1). We are left with choosing v 0 ; let us denote by c(v 0 ) = q(0)v 2 0 + q(1/2)(1/2 -v 0 ) 2 the resulting cost. This function has a minimum at</p><formula xml:id="formula_66">v 0 = q(1/2) 2(q(0)+q(1/2)) , which gives a cost of q(0)•q(1/2) 4(q(0)+q(1/2)) . ◀ This cost is maximized for q(1/2) = √ 2 -1 and q(0) = q(1) = 2- √ 2</formula><p>2 , giving a lower bound of 3/4 -1/ √ 2 ≈ 0.04289. In fact, one can verify that this is the best attainable lower bound for any discrete distribution on three points. Further, in Section 6.1, we show that this is an optimal lower bound when x is known to be in {0, 1/2, 1}, by giving an algorithm with a matching cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D</head><p>An Optimal Lower Bound for the Unbiased {0, 1/2, 1} Case</p><formula xml:id="formula_67">Assume that we have h ∈ [0, 1]. Buffy sends X(x, h) to Angel which estimates x(X(x, h), h). For x ′ , x ′′ ∈ {0, 1/2, 1}, let p x ′ ,x ′′ = Pr[X(x ′ , h) = X(x ′′ , h)]</formula><p>denote the probability (with respect to h) that the same bit is sent for x ′ , x ′′ . Since we send a single bit, we have that p 0,1/2 + p 1/2,1 + p 0,1 ≥ 1. Further, any algorithm for which p 0,1/2 + p 1/2,1 + p 0,1 &gt; 1 can be transformed to having p 0,1/2 + p 1/2,1 + p 0,1 = 1. For example, assume without loss of generality that for some h ∈ [0, 1], X(0, h) = X(1/2, h) = X(1, h). In this case, making the following modification still yields an algorithm with identical estimates: X(1, h) = 1 -X(0, h) and x(X(1, h), h) = x(X(0, h), h). Therefore, we can assume that:</p><formula xml:id="formula_68">p 0,1/2 + p 1/2,1 + p 0,1 = 1.</formula><p>For all x ′ , x ′′ ∈ {0, 1/2, 1}, we define by</p><formula xml:id="formula_69">H x ′ ,x ′′ = {h ∈ [0, 1] : X(x ′ , h) = X(x ′′ , h</formula><p>)} the set of shared-randomness values that would lead Buffy to send the same bit for both x and x ′ . Next, denote by</p><formula xml:id="formula_70">G x ′ ,x ′′ = E[ x|h ∈ H x ′ ,x ′′ , x ∈ {x ′ , x ′′ }]</formula><p>the expected estimate value, conditioned on the shared randomness being in H x ′ ,x ′′ . We have that:</p><formula xml:id="formula_71">Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + p 0,1 • (G 0,1 -0) 2 + p 1/2,1 • (E[ x|h ∈ H 1/2,1 , x = 0] -0) 2 (12) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + p 0,1 • (E[ x|h ∈ H 0,1 , x = 1/2] -1/2) 2 + p 1/2,1 • (G 1/2,1 -1/2) 2 (13) Var[ x|x = 1] ≥ p 0,1/2 • (E[ x|h ∈ H 0,1/2 , x = 1] -1) 2 + p 0,1 • (G 0,1 -1) 2 + p 1/2,1 • (G 1/2,1 -1) 2 . (<label>14</label></formula><formula xml:id="formula_72">)</formula><p>Due to unbiasedness, we must have</p><formula xml:id="formula_73">G 0,1/2 p 0,1/2 + G 0,1 p 0,1 + E[ x|h ∈ H 1/2,1 , x = 0]p 1/2,1 = 0 (15) G 0,1/2 p 0,1/2 + E[ x|h ∈ H 0,1 , x = 1/2]p 0,1 + G 1/2,1 p 1/2,1 = 1/2 (16) E[ x|h ∈ H 0,1/2 , x = 1]p 0,1/2 + G 0,1 p 0,1 + G 1/2,1 p 1/2,1 = 1. (<label>17</label></formula><formula xml:id="formula_74">)</formula><p>We proceed with a case analysis based on the p 0,1 , the probability that the sender would send the same bit for 0, 1. <ref type="table">p 0,</ref><ref type="table">1 = 0</ref> We start with the simpler case where the sender never sends the same bit for 0, 1 (and thus p 1/2,1 = 1 -p 0,1/2 ). Then ( <ref type="formula">12</ref>)-( <ref type="formula" target="#formula_71">14</ref>) yield:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Case</head><formula xml:id="formula_75">Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -p 0,1/2 ) • (E[ x|h ∈ H 1/2,1 , x = 0] -0) 2 Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1/2) 2 Var[ x|x = 1] ≥ p 0,1/2 • (E[ x|h ∈ H 0,1/2 , x = 1] -1) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1) 2 .</formula><p>Similarly, using ( <ref type="formula">15</ref>)-( <ref type="formula" target="#formula_73">17</ref>) we get:</p><formula xml:id="formula_76">G 0,1/2 p 0,1/2 + E[ x|h ∈ H 1/2,1 , x = 0](1 -p 0,1/2 ) = 0 G 0,1/2 p 0,1/2 + G 1/2,1 (1 -p 0,1/2 ) = 1/2 (18) E[ x|h ∈ H 0,1/2 , x = 1]p 0,1/2 + G 1/2,1 (1 -p 0,1/2 ) = 1.</formula><p>This gives</p><formula xml:id="formula_77">E[ x|h ∈ H 1/2,1 , x = 0] = 0 -G 0,1/2 p 0,1/2 1 -p 0,1/2 E[ x|h ∈ H 0,1/2 , x = 1] = 1 -G 1/2,1 (1 -p 0,1/2 ) p 0,1/2 ,</formula><p>and thus:</p><formula xml:id="formula_78">Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -p 0,1/2 ) • 0 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -0 2 (19) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1/2) 2 Var[ x|x = 1] ≥ p 0,1/2 • 1 -G 1/2,1 (1 -p 0,1/2 ) p 0,1/2 -1 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1) 2 .</formula><p>Equation <ref type="bibr" target="#b15">(18)</ref> gives</p><formula xml:id="formula_79">G 1/2,1 = 1/2-G 0,1/2 p 0,1/2 1-p 0,1/2</formula><p>and therefore:</p><formula xml:id="formula_80">Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -1/2 2 (20) Var[ x|x = 1] ≥ p 0,1/2 • 1 -(1/2 -G 0,1/2 p 0,1/2 ) p 0,1/2 -1 2 + (1 -p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -1 2 . (<label>21</label></formula><formula xml:id="formula_81">)</formula><p>Minimizing max {( <ref type="formula">19</ref>),( <ref type="formula">20</ref>),( <ref type="formula" target="#formula_80">21</ref>)}, we get a bound of 1/16, obtained for p 0,1/2 = 1/2 and G 0,1/2 = 1/4. ▶ Lemma 6. There exists an optimal unbiased solution for which p 0,1/2 = p 1/2,1 and</p><formula xml:id="formula_82">G 1/2,1 = 1 -G 0,1/2 .</formula><p>The lemma implies that p 0,1 = 1 -p 0,1/2 -p 1/2,1 = 1 -2p 0,1/2 . Therefore, we get</p><formula xml:id="formula_83">Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -2p 0,1/2 ) • (G 0,1 -0) 2 + p 0,1/2 • 0 -G 0,1/2 p 0,1/2 -G 0,1 (1 -2p 0,1/2 ) p 0,1/2 -0 2 (22) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -2p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 -(1 -G 0,1/2 )p 0,1/2 (1 -2p 0,1/2 ) -1/2 2 + p 0,1/2 • (1/2 -G 0,1/2 ) 2<label>(23)</label></formula><p>Var</p><formula xml:id="formula_84">[ x|x = 1] ≥ p 0,1/2 • 1 -G 0,1 (1 -2p 0,1/2 ) -(1 -G 0,1/2 )p 0,1/2 p 0,1/2 -1 2 + (1 -2p 0,1/2 ) • (G 0,1 -1) 2 + p 0,1/2 • (-G 0,1/2 ) 2 . (<label>24</label></formula><formula xml:id="formula_85">)</formula><p>The infimum of max {( <ref type="formula">22</ref>),( <ref type="formula" target="#formula_83">23</ref>),( <ref type="formula" target="#formula_84">24</ref>)}, over all possible p 0,1/2 , G 0,1/2 , G 0,1 values, we get a lower bound of 1/16, which is obtained for p 0,1/2 → 1/2, G 0,1/2 = 1/4 (i.e., p 0,1 → 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Proof of Lemma 6</head><p>Assume an optimal unbiased algorithm, defined using X * (x, h), x * (X, h), the probabilities p * 0,1/2 , p * 0,1 , p * 1/2,1 , and G * 0,1/2 , G * 0,1 , G * 1/2,1 . We consider the following algorithm; let h alg be a shared random bit that is independent of h. If h alg = 0 Buffy sends X = X * (x, h) and otherwise (if h alg = 1) X = X * (1 -x, h). In turn, Angel estimates x = x * (X, h) if h alg = 0 or x = 1 -x * (X, h) otherwise.</p><p>Notice that our algorithm is unbiased:</p><formula xml:id="formula_86">E[ x] = 1/2(E[ x|h alg = 0] + E[ x|h alg = 1]) = 1/2(x + E[1 -x * (X * (1 -x, h), h)|h alg = 1]) = 1/2(x + 1 -(1 -x)) = x.</formula><p>Next, observe that the algorithm, and thus the variance, remains unchanged for x = 1/2. For x = 0:</p><formula xml:id="formula_87">Var[Z 0 ] = 1/2(Var[ x|h alg = 0, x = 0] + Var[ x|h alg = 1, x = 0]) = 1/2(Var[ x * |x = 0] + Var[ x * |x = 1]).</formula><p>Here, we used the fact that for any random variable Var</p><formula xml:id="formula_88">[ x * (X * (1, h), h)] = 1-Var[ x * (X * (1, h), h)].</formula><p>Therefore, we get that Var[ x|x = 0] ≤ max {Var[ x * |x = 0], Var[ x * |x = 1]} and we have not increased the cost. A symmetric analysis applies to x = 1.</p><p>Finally, we get that</p><formula xml:id="formula_89">p 0,1/2 = Pr[X(0, h) = X(1/2, h)] = 1/2(Pr[X(0, h) = X(1/2, h)|h alg = 0] + Pr[X(1, h) = X(1/2, h)|h alg = 1]) = 1/2(p * 0,1/2 + p * 1/2,1 ).</formula><p>By symmetry, we also get p 1/2,1 = 1/2(p * 0,1/2 + p * 1/2,1 ) and thus p 0,1/2 = p 1/2,1 . Similarly,</p><formula xml:id="formula_90">G 0,1/2 = 1/2 G * 0,1/2 + (1 -G * 1/2,1 ) G 1/2,1 = 1/2 G * 1/2,1 + (1 -G * 0,1/2 ) .</formula><p>Notice that G 0,1/2 + G 1/2,1 = 1, which concludes the proof. ◀</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Analysis of the Unbiased Algorithm</head><p>In this appendix, we prove Theorem 5 which we restate here:</p><formula xml:id="formula_91">▶ Theorem 5. Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).</formula><p>First, we state a technical lemma, whose proof appears below in Appendix E.1, that shows the periodicity of the variance in our algorithm.</p><formula xml:id="formula_92">▶ Lemma 7. For any y ∈ [0, 1 -2 -ℓ ], Var[ x|x = y] = Var[ x|x = y + 2 -ℓ ].</formula><p>As a result of this periodicity, we can continue the analysis, without loss of generality, under the assumption that x ∈ [0, 2 -ℓ ]. We first calculate several useful quantities:</p><formula xml:id="formula_93">E [X] = E X 2 = x E[h] = (2 ℓ -1)/2 E[h 2 ] = (2 ℓ -1)(2 ℓ+1 -1)/6 E[X • h|x ≤ 2 -ℓ ] = 0</formula><p>(as either X = 0 or h = 0, since x ≤ 2 -ℓ ).</p><p>We now proceed with calculating the variance.</p><formula xml:id="formula_94">Var[ x|x ≤ 2 -ℓ ] = E X + (h -0.5(2 ℓ -1)) • 2 -ℓ 2 -x 2 = E X 2 + 2 -2ℓ • E h 2 -E [h] • (2 ℓ -1) + 0.25(2 ℓ -1) 2 + 2 -ℓ+1 • E [X • h] -2 -ℓ • (2 ℓ -1) • E [X] -x 2 = x + 2 -2ℓ • E h 2 -E [h] • (2 ℓ -1) + 0.25(2 ℓ -1) 2 -2 -ℓ • (2 ℓ -1) • x -x 2 = x + 2 -2ℓ • (2 ℓ -1)(2 ℓ+1 -1)/6 -(2 ℓ -1) 2 /2 + 0.25(2 ℓ -1) 2 -2 -ℓ • (2 ℓ -1) • x -x 2 = 1/12 • (1 -4 -ℓ ) + 2 -ℓ x -x 2 .</formula><p>Finally, according to Lemma 7, we get that:</p><formula xml:id="formula_95">Var[ x] = 1/12 • (1 -4 -ℓ ) + 2 -ℓ (x mod 2 -ℓ ) -(x mod 2 -ℓ ) 2 . (<label>25</label></formula><formula xml:id="formula_96">)</formula><p>This gives a worst-case bound, achieved for x ∈ 2 -(ℓ+1</p><formula xml:id="formula_97">) + i • 2 -ℓ | i ∈ [2 ℓ-1 ] , of Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Proof of Lemma 7</head><p>Let y ∈ [0, 1 -2 -ℓ ] and denote z = y + 2 -ℓ , m = y mod 2 -ℓ , and ζ = y • 2 ℓ . Notice that if h &lt; ζ, then Buffy will send X = 1 for both y and z. Similarly, if h ≥ ζ + 1, Buffy will send X = 0 for both y and z. Notice that, for any y and ℓ:</p><formula xml:id="formula_98">2 ℓ y -y • 2 ℓ = 2 ℓ (y mod 2 -ℓ ).</formula><p>and thus</p><formula xml:id="formula_99">y mod 2 -ℓ = y -y • 2 ℓ • 2 -ℓ .</formula><p>Therefore, we can write:</p><formula xml:id="formula_100">( x|x = y) = (h -0.5(2 ℓ -1)) • 2 -ℓ + 1 h&lt;ζ + 1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) ( x|x = z) = (h -0.5(2 ℓ -1)) • 2 -ℓ + 1 h&lt;ζ+1 + 1 (h=ζ+1)∧(r&lt;2 ℓ •(z mod 2 -ℓ )) Denote (h -0.5(2 ℓ -1)) • 2 -ℓ by ψ. Thus, since y mod 2 -ℓ = z mod 2 -ℓ : Var[ x|x = z] -Var[ x|x = y] = E ψ + 1 h&lt;ζ+1 + 1 (h=ζ+1)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) 2 - E (ψ + 1 h&lt;ζ + 1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) 2 -z 2 + y 2 = E 1 h&lt;ζ+1 + 1 (h=ζ+1)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h&lt;ζ+1 + 1 (h=ζ+1)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) -1 h&lt;ζ + 1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h&lt;ζ + 1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) -z 2 + y 2 = E 1 h=ζ + 1 (h=ζ+1)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) -1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h=ζ + 1 (h=ζ+1)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) -1 (h=ζ)∧(r&lt;2 ℓ •(y mod 2 -ℓ )) -z 2 + y 2 = E 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2ψ 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) • 1 h=ζ + 1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2ψ1 r&lt;2 ℓ •(y mod 2 -ℓ ) • 1 h=ζ+1 -z 2 + y 2 = E 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2ψ 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) |h = ζ • Pr[h = ζ]+ E 1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2ψ • 1 r&lt;2 ℓ •(y mod 2 -ℓ ) |h = ζ + 1 • Pr[h = ζ + 1] -z 2 + y 2 = 2 -ℓ • E 2ψ 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) |h = ζ + E 2ψ • 1 r&lt;2 ℓ •(y mod 2 -ℓ ) |h = ζ + 1 + 2 -ℓ -z 2 + y 2 = (as h is independent of r) 2 -ℓ • E 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ 1 -1 r&lt;2 ℓ •(y mod 2 -ℓ ) + E 2 (ζ + 1 -0.5(2 ℓ -1)) • 2 -ℓ • 1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ + 2 1-ℓ • E 1 r&lt;2 ℓ •(y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ + 2 1-ℓ • (2 ℓ • (y mod 2 -ℓ )) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2ζ -(2 ℓ -1) • 2 -ℓ + 2 • (y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • (2ζ + 1) • 2 -ℓ + 2 • (y mod 2 -ℓ ) -z 2 + y 2 = 2 -ℓ • 2 y • 2 ℓ + 1 • 2 -ℓ + 2 • y -y • 2 ℓ • 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 -ℓ + 2y -(z -y)(z + y) = 0. ◀ F Generalization to k Bits F.1 General Quantized Algorithm We use a hash function h such that h ∈ {0, 1} ℓ is uniformly distributed. Let A ∼ U [0, 1] be independent of h. C = 2 k -1 • x p = 2 k -1 • x -2 k -1 • x R = 2 k -1</formula><p>We then set</p><formula xml:id="formula_101">X ≜ C + 1 if p ≥ (A + h)2 -ℓ C otherwise</formula><p>We send X to Angel which estimates</p><formula xml:id="formula_102">x = X + (h -0.5(2 ℓ -1)) • 2 -ℓ R .</formula><p>To show that our protocol is unbiased, notice that:</p><formula xml:id="formula_103">E[X] = R•x and that E[h] = 0.5(2 ℓ -1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Lower Bounds</head><p>Similarly to the 1-bit case, we consider the discrete distribution over</p><formula xml:id="formula_104">i • 1 3 • 2 k-1 -1 | i ∈ 0, 1, . . . , 3 • 2 k-1 -1 . We set a 1/2 = √ 2-1 2 k-1 and a 0 = a 1 = 1-a 1/2 2 k and ∀i : q i • 1 3 • 2 k-1 -1 = a (i mod 3)/2 .</formula><p>When each consecutive set of three points has the same probability, one can derive an optimal algorithm with precisely two values between each such triplet. The optimal choice of locations of the values in each triplet is similar to our single-bit analysis of the previous subsection, i.e., one should have a values at</p><formula xml:id="formula_105">√ 2 -1 + i 2 k-1 i ∈ 0, 1, . . . , 2 k-1 -1 i 2 k-1 i ∈ 1, . . . , 2 k-1 .</formula><p>We turn into calculating the cost. Notice that every triplet has a width of 2 3•2 k-1 -1 . Therefore, the cost now reduces, compared to the 1-bit analysis, by a factor of</p><formula xml:id="formula_106">2 3•2 k-1 -1 2 . That is, we get a lower bound of 3-2 √ 2 (3•2 k-1 -1) 2 = 3-2 √ 2 2.25(2 k -2/3) 2 .</formula><p>We note that, for large k and ℓ values, our variance is within 10% of the lower bound, as</p><formula xml:id="formula_107">lim k,ℓ→∞ 1 12(2 k -1) 2 3-2 √ 2 2.25(2 k -2/3) 2 = 9 + 6 √ 2 16 ≈ 1.093.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Limiting Algorithm Uniformness Proof</head><p>Recall that the algorithm uses h ∼ U [0, 1] where Buffy sends</p><formula xml:id="formula_108">X ≜ 1 if x ≥ h 0 otherwise</formula><p>and Angel estimates x = X + h -0.5.</p><p>▶ Lemma 8. For a fixed value of x, it holds that x ∼ U x -1 2 , x + 1 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. Let</head><formula xml:id="formula_109">Z = 1 h≤x + h. We have that Z ∼ U [x, 1 + x], i.e., f Z (z) = 1 if z ∈ [x, 1 + x] 0 Otherwise . This is because Pr[Z ≤ z] =        1 if z ≥ 1 + x z -p if z ∈ (x, 1 + x) 0 if z ≤ x .</formula><p>Therefore,</p><formula xml:id="formula_110">X + h -1/2 = Z -1/2 ∼ U [x -1/2, (1 + x) -1/2] .</formula><p>This concludes the proof. ◀ ▶ Corollary 9. Our estimator is unbiased, i.e., E[ x] = x.</p><p>▶ Corollary 10. Our variance is constant for all x ∈ [0, 1] and satisfies Var[ x] = 1 12 .</p><p>Therefore, the potential extrema are x ∈ {1/2 -z, 1/2} and where E[cost] = 0, which gives x = 3+2z- √ 9-20z 2 6</p><p>. This yields</p><formula xml:id="formula_111">cost =        1/12 -2z 2 + (16z 3 )/3 if x = 1/2 -z 4/3z 3 -2z 2 + 3/4z + 1/12 if x = 1/2 1/12 + z -2z 2 + 20/27 • z 3 + √ 9 -20z 2 • (-1/12 + 5/27 • z 2 ) if x = 3+2z- √ 9-20z 2 6 It follows that 1/12 + z -2z 2 + 20/27 • z 3 + 9 -20z 2 • (-1/12 + 5/27 • z 2 ) ≤ 4/3z 3 -2z 2 + 3/4z + 1/12</formula><p>for all z ∈ [0, 0.5], and therefore we focus on x ∈ {0, 1/2 -z, 1/2}. Notice that min z∈[0,1/2] 1/12-2z 2 + (16z 3 )/3 = 1/24, which is achieved for z = 1/24.</p><p>Finally, by choosing the z value which minimizes max 2z(0.5</p><formula xml:id="formula_112">-z) 2 + 1-z z (0.5 -t) 2 dt, (0.5 + z) • z 2 + 0.5 z t 2 dt ,</formula><p>which is obtained for the z value that satisfies 1/24 + z 2 /2 + (2z 3 )/3 = 0 (z ≈ 0.17349), we get an expected worst-case squared error of ≈ 0.0602.</p><p>As a side note, one can obtain a slightly stronger bound by further truncating the estimations to e ± 1/2, where e = X + h -1/2. For example, if e = -0.4 then the algorithm should not estimate x ≈ 0.173 as x is guaranteed to be at most 0.1. Instead, the algorithm would estimate:</p><p>x = max(min(e, max(1 -z, e -1/2)), min(z, e + 1/2)).</p><p>In such a case, we can choose z ≈ 0.182 and get a cost of ≈ 0.05824. For simplicity, we omit the technical details. ◀</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Convex-combination Biased Adaptation for Subtractive Dithering</head><p>Here, we analyze the algorithm in which Buffy sends (for a shared h ∼ U [0, 1])</p><formula xml:id="formula_113">X = 1 if x ≥ h 0 otherwise ,</formula><p>and Angel estimates, for some α ∈ [0, 1],</p><formula xml:id="formula_114">x = α • h + (1 -α) • X.</formula><p>Notice that We now explore how to reduce the cost using randomized thresholding, achieved through probabilistic multiplexing of the above algorithms. That is, Buffy and Angel will randomly select the executed protocol using the shared randomness, thus achieving implicit coordination.</p><formula xml:id="formula_115">E[ x] = α/2 + (1 -α) • x. E[h 2 ] = 1/3. E[h • X] = x 0 tdt = x 2 /2.</formula><p>To simplify the notation, we use A [0,1] to denote our general (i.e., x ∈ [0, 1]) with ℓ → ∞ (also given explicitly in Appendix L) algorithm, and A {0,1/2,1} to denote our algorithm for when x is guaranteed to be in {0, 1/2, 1}. Our observation is that A [0,1] , behaves differently than A {0,1/2,1} . Specifically, for the first, the expected squared error is maximized at {0, 1/2, 1}, while for the latter, the expected squared error is lower at these points. This suggests that by randomly choosing which of these algorithms to execute one can lower the cost.</p><p>In particular, we propose to multiplex between A [0,1] and A {0,1/2,1} as follows. With probability p, to be determined later, both Buffy and Angel use A [0,1] and otherwise A {0,1/2,1} , using the shared randomness to implicitly decide on the protocol. This means that the expected squared error becomes:</p><formula xml:id="formula_116">E[( x -x) 2 ] = E[( x -x) 2 |running A [0,1] ] • p + E[( x -x) 2 |running A {0,1/2,1} ] • (1 -p).</formula><p>We get that the cost, optimized for p = ϕ -1, is 6</p><formula xml:id="formula_117">√ 10+11 √ 5-18 √ 2-17 24 ≈ 0.04644 (obtained for x ∈ {0, 1} ∪ 1 2 √ 2 , 1 -1 2 √ 2</formula><p>). Notice that the cost of the algorithm is within 3.01% from the lower bound in Section 4.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> illustrates how the non-hybrid (Section 6.2.2) ℓ = 3 algorithm has a lower cost than that of ℓ → ∞. However, as its worst-case expected squared error is not at x = 1/2, it does not multiplex as well with A {0,1/2,1} . Specifically, a hybrid algorithm that uses ℓ = 3 bits instead of the limit (ℓ → ∞) algorithm results in a higher cost of 102/361 -1/(3 √ 2) ≈ 0.04685 (which is obtained for p = 2/3). The above hybrid algorithms use unbounded shared randomness. In cases where we wish to use a small number of shared bits, we can approximate the better algorithm (that uses ℓ → ∞ for A [0,1] ); below we give a couple of examples.</p><p>Example I: Consider using ℓ = 4 bits. In this case, we use p = 3/4 and use the 2-bit algorithm from Section 6.2.2 as A [0,1] . The cost is then 1049-169 ), which improves over the 3-bit algorithm.</p><p>Example II: Consider using one random byte (ℓ = 8). In that case, we use p = 11/16 together with the 4-bit algorithm. The cost then becomes</p><p>1830635-1232945 √ 2 1858592 ≈ 0.04680 (obtained for x ∈ 109+6 √ 2 241 , 132-6 √ 2 241</p><p>). This further improves over the cost of Example I, over the best hybrid solution with ℓ = 3 (that uses unbounded randomness to represent the p = 2/3 value), and is within 1% of the unbounded shared randomness algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>Analysis of the Linear Sigmoid (Section 6.2.3)</p><p>First, we have (see Section 6.2) that the estimate function for X = 1 is:</p><formula xml:id="formula_118">Z 1 (h) = 1 -Z 0 (1 -h) =        2α if h &lt; h 0 2α + (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ]</formula><p>1 otherwise .</p><p>For x ∈ [α, 1 -α], denote by T -1 (x) = (1-2h0)(x-α)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-2α</head><p>+ h 0 the value such that T (T -1 (x)) = x. We proceed with computing the expectation:</p><formula xml:id="formula_119">E[ x|x &lt; α]( =⇒ X = 0) = h 0 • (1 -2α) + 1-h0 h0 (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 -α E[ x|x &gt; 1 -α]( =⇒ X = 1) = h 0 • (1 + 2α) + 1-h0 h0 2α + (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 + α E[ x|x ∈ [α, 1 -α]] = h 0 • (2α) + h 0 (1 -2α) + T -1 (x) h0 2α + (1 -2α) • h -h 0 1 -2h 0 dh + 1-h0 T -1 (x) (1 -2α) • h -h 0 1 -2h 0 dh = h 0 + 2α(T -1 (x) -h 0 ) + 1-h0 h0 (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 + (-1 + 2h 0 )α + 2α(T -1 (x) -h 0 ) = 1/2 -α + 2αT -1 (x)</formula><p>Therefore, we have:</p><formula xml:id="formula_120">E[ x] =        1/2 -α if x &lt; α 1/2 -α + 2α (1-2h0)(x-α) 1-2α + h 0 if x ∈ [α, 1 -α]</formula><p>1/2 + α otherwise .</p><p>Next, we calculate the second moment of the estimate:</p><formula xml:id="formula_121">E[( x) 2 |x &lt; α]( =⇒ X = 0) = h 0 • (1 -2α) 2 + 1-h0<label>h0</label></formula><p>(1 -2α) • h -h 0 1 -2h 0 2 dh = 1/3 + h 0 /3 -4α/3 -4h 0 α/3 + 4α 2 /3 + 4h 0 α 2 /3</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>+ 0. 25 ≥</head><label>25</label><figDesc>Y (0.5) • 0.75 • (-0.25) + (1 -Y (0.5)) • 0.25 • (-0.75) + 0.25 = 0.25 -3/16 = 1/16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4</head><label>4</label><figDesc>Figure4An illustration of the expected squared errors that motivate our choice of creating a hybrid of the optimal {0, 1/2, 1} and the biased ℓ → ∞ algorithms.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Other deterministic algorithms, e.g., that send 0 despite having |x -v0| &gt; |x -v1|, can trivially be improved by an algorithm with the above form.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Notice that it is different than the value used for the x ∈ {0, 1/2, 1} case.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>|x = 0.5] = E (1 X=0 • Z 0 + 1 X=1 • Z 1 ) 2 |x = 0.5</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>If for some h, A0(h) &gt; A1(h), there exists an equivalent algorithm that replaces the role of X = 0 and X = 1 for this specific h.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers, <rs type="person">Moshe Gabel</rs>, and <rs type="person">Gal Mendelson</rs> for their helpful feedback and comments.</p></div>
			</div>
			<div type="funding">
<div><p>Funding MM was supported in part by <rs type="funder">NSF</rs> grants <rs type="grantNumber">CCF-1563710</rs>, <rs type="grantNumber">CCF-1535795</rs> and <rs type="grantNumber">DMS-2023528</rs>. MM and RBB were supported in part by a gift to the <rs type="institution">Center for Research on Computation and Society at Harvard University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KrBXfgh">
					<idno type="grant-number">CCF-1563710</idno>
				</org>
				<org type="funding" xml:id="_XQgYPd3">
					<idno type="grant-number">CCF-1535795</idno>
				</org>
				<org type="funding" xml:id="_qgcWXCq">
					<idno type="grant-number">DMS-2023528</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>D.2 Case p 0,1/2 , p 1/2,1 , p 0,1 &gt; 0</p><p>We proceed with the case where x = 0 and x = 1 may result in the same bit being sent, for some h values. For convenience, we restate equations ( <ref type="formula">12</ref>)-( <ref type="formula">14</ref>):</p><p>and ( <ref type="formula">15</ref>)-( <ref type="formula">17</ref>):</p><p>This gives:</p><p>Plugging these into ( <ref type="formula">12</ref>)-( <ref type="formula">14</ref>) we have:</p><p>We use the following lemma, whose proof appear in D.3 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Reducing Algorithms to Monotone T, Z 0 Functions</head><p>We now show how an algorithm can be represented as described in Section 6.2. Fixing the shared randomness value h, Angel estimates x solely based on the sent bit X; denote these values by A X (h). Without loss of generality, assume that ∀h : A 1 (h) ≥ A 0 (h). 3 This means that, in an optimal algorithm, Buffy should send</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, as otherwise the error would be suboptimal for any x not satisfying the condition. In particular, this means that we can express Buffy's algorithm using a threshold function T .</p><p>Next, we claim that the threshold function can be considered monotone, without increasing the cost. To that end, we first consider a finite shared randomness h ∈ [2 ℓ ]. In such a case, if there exists some h 1 &gt; h 2 ∈ [2 ℓ ] such that T (h 1 ) &lt; T (h 2 ), we can modify the algorithm as follows: For all h / ∈ {h 1 , h 2 }, no modification is made. If h = h 1 , then the modified algorithm works as if h = h 2 , and vice versa. Following this process, we can sort T until it becomes monotone. A similar argument can be made for the continuous (h ∈ [0, 1]) case (possibly with an additional ϵ discretization cost).</p><p>We proceed with showing that there exists an optimal algorithm in which Z 1 (h) = 1 -Z 0 (1 -h). This is achieved using a symmetry observation. Specifically, if an algorithm does not satisfy the above, consider its "dual algorithm": instead of sending x using T (h), we send x ′ = 1 -x using T ′ (h) = 1 -T (1 -h); similarly, Angel estimates x ′ = 1 -x. Then, if both Buffy and Angel use the shared randomness to implicitly agree on whether to run the original or dual algorithms, each with probability half, the cost can only decrease. The proof follows similarly to that of Lemma 6 (Appendix D.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Truncated Dithering</head><p>We now analyze the cost attainable by truncating the subtractive dithering algorithm to some interval [z, 1 -z]. Let h ∼ U [0, 1] be a shared uniform random variable. Consider sending</p><p>similarly to our algorithm for ℓ → ∞ (see Section 5). However, unlike our algorithm, for a parameter z ∈ [0, 1/2], Angel estimates x as</p><p>That is, we truncate the estimation to the interval [z, 1-z], for some parameter z ∈ [0, 0.5] that we determine later.</p><p>Proof. Assume, without loss of generality, that x ∈ [0, 1/2]. According to Lemma 8, we have that</p><p>Therefore, Angel will estimate x as follows: With probability 1/2 + z -x, x = z; with probability max {0,</p><p>We proceed with a case analysis. First, let us consider the (x &lt; 1/2 -z) case. This yields</p><p>Therefore, the cost would be</p><p>We have that the derivative with respect to x is:</p><p>Therefore, the potential extrema are x ∈ {0, 1/2 -z} and when the derivative vanishes, which gives x = z (the other extreme point is not in [0, 0.5]). We then get</p><p>Next, we consider the x ≥ 1/2 -z case. In such a case, we get that</p><p>Then, our cost is:</p><p>We then get</p><p>We compute the expected squared error:</p><p>We then get</p><p>Therefore, the possible extrema are {0, 1/2, 1}. Minimizing <ref type="bibr" target="#b23">(26)</ref>, we get that the optimal choice is α = 2 -ϕ ≈ 0.382, which gives E[( x -x) 2 ] ≤ 5/3 -ϕ ≈ 0.04863.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Analysis of the Algorithm of Section 6.2.2</head><p>We first derive several quantities that will be useful for calculating the cost.</p><p>Next, we calculate the second moment of the estimate:</p><p>and</p><p>Finally, we are ready to express the expected squared error:</p><p>which is obtained for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Limiting Algorithm of Section 6.2.2</head><p>In the limiting algorithm, Buffy sends:</p><p>In turn, Angel estimates:</p><p>Let us calculate several useful quantities:</p><p>We proceed with analyzing the expected squared error:</p><p>Choosing α = 2 -ϕ as before, which minimizes the worst-case expected squared error, we get that</p><p>Our analysis indicates that the cost becomes 5/3 -ϕ ≈ 0.04863, which is reached for x ∈ {0, 1/2, 1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M Improving the Cost Further using a Hybrid Algorithm</head><p>As mentioned, for the case x ∈ [0, 1], our algorithms above do not improve when given access to more than ℓ = 3 bits. This suggests that an optimal algorithm, unlike the solution from Section 6.2.2, may need to use a non-uniform or probabilistic partitioning of the [0, 1] interval using the α parameter.</p><p>Putting it together, we get: Therefore, the cost is 5/108 ≈ 0.0463, which is less than 0.9% higher than the 0.0459 lower bound (Section 4.1.2).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Indirect Stochastic Gradient Quantization and Its Application in Distributed Deep Learning</title>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faramarz</forename><surname>Fekri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ben Basat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><surname>Vargaftik</surname></persName>
		</author>
		<ptr target="https://gist.github.com/ranbenbasat/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memento: Making Sliding Windows Efficient for Heavy Hitters</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ben-Basat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><surname>Einziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Keslassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Orda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><surname>Vargaftik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Waisbard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CoNEXT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">PINT: Probabilistic In-band Network Telemetry</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ben Basat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaramakrishnan</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication</title>
		<meeting>the Annual conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="662" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">signSGD: Compressed Optimisation for Non-Convex Problems</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="560" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The complexity of the generalized Lloyd-Max problem (Corresp.)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Witsenhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Neuhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quantization. IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982">1982. 1998</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2325" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast Exact K-Means, K-Medians and Bregman Divergence Clustering in 1D</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Grønlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kasper Green Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Mathiasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sindahl Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhou</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network-Wide Heavy Hitter Detection with Commodity Switches</title>
		<author>
			<persName><forename type="first">Rob</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Rexford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on SDN Research</title>
		<meeting>the Symposium on SDN Research</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Is Subtractive Dithering the Optimal Algorithm for Sending a Real Number Using One Bit</title>
		<author>
			<persName><forename type="first">Zarathustra</forename><surname>Elessar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brady</forename></persName>
		</author>
		<ptr target="https://cstheory.stackexchange.com/questions/48281" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How to Recycle Random Bits</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="248" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Error Feedback Fixes SignSGD and other Gradient Compression Schemes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Peter Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><forename type="middle">El</forename><surname>D'oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rouayheb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badih</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tancrède</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lepoint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayfer</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
	</analytic>
	<monogr>
		<title level="m">Advances and Open Problems in Federated Learning</title>
		<editor>
			<persName><forename type="first">Praneeth</forename><surname>Sai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quentin</forename><surname>Karimireddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Rebjock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Martin</forename><surname>Stich</surname></persName>
		</editor>
		<editor>
			<persName><surname>Jaggi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3252" to="3261" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03575</idno>
		<title level="m">Federated Optimization: Distributed Optimization Beyond the Datacenter</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15463</idno>
		<title level="m">Queues with Small Advice</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Private vs. Common Random bits in Communication Complexity</title>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information processing letters</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="71" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Picture Coding Using Pseudo-Random Noise</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dither Signals and Their Effect on Quantization Noise</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Schuchman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communication Technology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="162" to="165" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">1-Bit Stochastic Gradient Descent and its Application to Data-Parallel Distributed Training of Speech DNNs</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Nir</forename><surname>Shlezinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonina</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03262</idno>
		<title level="m">UVeQFed: Universal Vector Quantization for Federated Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">DRIVE: One-bit Distributed Mean Estimation</title>
		<author>
			<persName><forename type="first">Shay</forename><surname>Vargaftik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ben Basat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Portnoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Mendelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Ben-Itzhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08339</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LSQ: Load Balancing In Large-Scale Heterogeneous Systems With Multiple Dispatchers</title>
		<author>
			<persName><forename type="first">Shay</forename><surname>Vargaftik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Keslassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Orda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1186" to="1198" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Terngrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1509" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic Computations: Toward a Unified Measure of Complexity</title>
		<author>
			<persName><forename type="first">Chi-Chin</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE FOCS</title>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image Compression Systems on Board Satellites</title>
		<author>
			<persName><forename type="first">Guoxia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Vladimirova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">N</forename><surname>Sweeting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Astronautica</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="988" to="1005" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Enabling Bit-by-Bit Backscatter Communication in Severe Energy Harvesting Environments</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ganesan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th {USENIX} Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="345" to="357" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
