# How to Send a Real Number Using a Single Bit (and Some Shared Randomness)

## Abstract

## 

We consider the fundamental problem of communicating an estimate of a real number x ∈ [0, 1] using a single bit. A sender that knows x chooses a value X ∈ {0, 1} to transmit. In turn, a receiver estimates x based on the value of X. The goal is to minimize the cost, defined as the worst-case (over the choice of x) expected squared error.

We first overview common biased and unbiased estimation approaches and prove their optimality when no shared randomness is allowed. We then show how a small amount of shared randomness, which can be as low as a single bit, reduces the cost in both cases. Specifically, we derive lower bounds on the cost attainable by any algorithm with unrestricted use of shared randomness and propose optimal and near-optimal solutions that use a small number of shared random bits. Finally, we discuss open problems and future directions.

## Introduction

We consider the fundamental problem of communicating an estimate of a real number x ∈ [0, 1] using a single bit. A sender, that we call Buffy, knows x, and chooses a value X ∈ {0, 1} to transmit. In turn, a receiver, that we call Angel, estimates x based on the value of X.

This problem naturally appears in distributed computations where multiple machines perform parallel tasks and transmit their results/state to an aggregator. If the bandwidth to the aggregator is limited, the machines must compress the data before sending it. Bandwidth optimization is fundamental in many domains, including network measurements [[3,](#)[9]](#) and telemetry [[4]](#), load balancing [[15,](#)[22]](#b19), and satellite communication [[25]](#b22). We are especially motivated by recent work addressing the communication bottleneck in distributed and federated machine learning [[12]](#). There, clients compute a local gradient and send it to a parameter server that computes the global gradient and updates the model [[14]](#). For the typical large-scale federated learning problems over edge devices (e.g., mobile phones), the devices may only be able to communicate a small number of bits per gradient coordinate. In fact, solutions such as 1-Bit SGD [[19]](#b16) and signSGD [[5]](#), have recently been studied as appealing low-communication solutions that use a single bit per coordinate. Another common Table 1 A summary of our results.

## Preliminaries

We start with some notation. We use [[n]](#) to denote {0, 1, . . . , n -1}, and ∆(S) to denote all possible probability distributions over the set S. (An element of ∆(S) will be expressed as a density function when S is uncountable, e.g., if S = [0, 1].) We also use, for a binary predicate B, 1 B as an indicator such that 1 B = 1 if B is true and 0 otherwise. Lastly, ϕ = (1 + √ 5)/2 denotes the Golden Ratio, which naturally comes up in some of our results.

Problem statement: Given a real number x ∈ [0, 1], Buffy compresses it to a single bit value X ∈ {0, 1} that is sent to Angel, who derives an estimate x. We also consider the special case where x is known to be in {0, 1/2, 1}. Our objective is to minimize the cost that is defined as the worst-case expected squared error, i.e., max x∈[0,1] E[( x -x) 2 ]. Note that the worst-case is taken over the value of x and the expectation is over the randomness of the algorithm. In the unbiased setting, we additionally require E[ x] = x, in which case the cost becomes Var[ x], i.e., the estimation variance. In some cases, we allow the parties to use ℓ bits of shared randomness. That is, we assume that they have access to a random value h ∈ [2 ℓ ], known to both Buffy and Angel. When applicable, we use r ∈ [0, 1] to denote the private randomness of Buffy.

## Algorithms without Shared Randomness

We recap the performance of two standard algorithms -randomized and deterministic rounding. Interestingly, we show that when no shared randomness is allowed, randomized rounding is an optimal unbiased algorithm, and deterministic rounding is an optimal biased algorithm.

## Randomized Rounding

In randomized rounding, Buffy uses private randomness to generate X ∼ Bernoulli(x) which is sent using a single bit. In turn, Angel estimates x = X. Clearly, we have that E[ x] = E[X] = x, and thus the algorithm is unbiased. The variance of the algorithm is Var[ x] = Var[X] = x(1 -x), and thus the worst-case is reached at x = 1/2, which gives a cost of 1/4. The following theorem, whose proof is deferred to Appendix A, shows that randomized rounding is optimal, in the sense that no unbiased algorithm without shared randomness can have a worst-case variance lower than 1/4. Intuitively, requiring the estimate to be unbiased forces the algorithm to send 1 with a probability that is linear in x, maximizing its cost for x = 1/2. The proof also establishes the intuitive idea that it is not possible to benefit from randomness used solely by Angel.

▶ Theorem 1. Any unbiased algorithm without shared randomness must have a worst-case variance of at least 1/4.

## Deterministic Rounding

With deterministic rounding, Buffy sends X = 1 when x ≥ 1/2. Angel then estimates x = X/2 + 1/4. Deterministic rounding has an (absolute) error of at most 1/4, which is achieved for x ∈ {0, 1/2, 1}. Therefore, its cost is 1/16. The next theorem, whose proof appears in Appendix B, shows that deterministic rounding is optimal, as no algorithm that does not use shared randomness can have a lower cost (even with unrestricted private randomness). We show that any such algorithm must have an expected squared error of at least 1/16 on at least one of {0, 1/2, 1}.

▶ Theorem 2. Any algorithm without shared randomness must have a worst-case expected squared error of at least 1/16.

## Lower Bounds

We next explore lower bounds for algorithms with shared randomness. We use Yao's minimax principle [[24]](#b21) to prove a lower bound on the cost of any biased shared randomness protocol. Then, we show a stronger lower bound for unbiased algorithms using a different approach.

## Lower Bound for Biased Algorithms

We place Yao's general formulation in the context of our specific problem.

▶ Theorem 3. [([24]](#)) Consider our estimation problem over the inputs x ∈ [0, 1], and let A be the set of all possible deterministic algorithms. For a (deterministic) algorithm a ∈ A and input x ∈ [0, 1], let the function c(a, x) = (a(x) -x) 2 be its squared error. Then for any randomized algorithm A and input distribution q ∈ ∆([0, 1]) such that X ∼ q:

$max x∈[0,1] E [c(A, x)] ≥ min a∈A E [c(a, X)] .$That is, the expected squared error (over the choice of x from distribution q) of the best deterministic algorithm (for q) lower bounds the expected squared error of any randomized (potentially biased) algorithm A for the worst-case x (i.e., its cost). Further, the inequality holds as an equality for the optimal distribution q and algorithm A, i.e.,

$min A max x∈[0,1] E [c(A, x)] = max q min a∈A E [c(a, X)] .$We proceed by selecting distributions q to lower bound min a∈A E [c(a, X)]. Notice that a deterministic algorithm can be defined using two values v

$0 , v 1 ∈ [0, 1], such that if |x-v 0 | ≤ |x-v 1 | then Buffy sends 0 and Angel estimates x as v 0 . Similarly, if |x-v 0 | > |x-v 1 |$then Buffy sends 1 and Angel estimates x as v 1 .[foot_0](#foot_0) In general, the above framework asserts that the cost, for the worst-case input, of any randomized algorithm is

$max q∈∆([0,1]) min v0,v1∈[0,1] 1 0 min (x -v 0 ) 2 , (x -v 1 ) 2 q(x)dx .$(1)

Our framework lower bounds the cost for any (biased or unbiased) algorithm that may use any amount of (shared or private) randomness. We now consider distributions q to lower bound the cost and later discuss the limitations of this approach.

## The {0, 1/2, 1} case

First, consider a discrete probability distribution q over {0, 1/2, 1}, and assume without loss of generality that q(0) ≤ q(1). Any deterministic algorithm cannot estimate all values exactly, and it must map at least two of the points to a single value, thus allowing us to lower bound its cost. In Appendix C, we prove the following.

▶ Lemma 4. Any deterministic algorithm must incur a cost of at least q(0)•q(1/2) 4(q(0)+q(1/2)) .

For q(0) = q(1) = (2 -√ 2)/2 and q(1/2) = √ 2 -1, this lemma yields a lower bound of 3/4 -1/ √ 2 ≈ 0.04289. In Section 6.1, we show that this is an optimal lower bound when x is known to be in {0, 1/2, 1}, by giving an algorithm with a matching cost.

## The [0, 1] case

In the general case, where x can take on any value in [0, 1], we can get a tighter bound by looking at mixed distributions. Specifically, for parameters a, w ∈ [0, 1/2], we consider the distribution where:

$q(x) =        0 with probability w 1 with probability w uniform on [a, 1 -a] otherwise .$Directly analyzing the optimal deterministic algorithm for this distribution proves complex. Instead, we first hypothesize that there exists an optimal deterministic algorithm for which either (1) v 1 = 1 -v 0 or (2) v 1 = 1. We emphasize that the lower bound holds even if the hypothesis is false. We then analyze what values of a, w maximize the cost of the best deterministic algorithm with the above form. Finally, we verify that the lower bound for the resulting distribution (with the specific a, w values) holds for all deterministic algorithms.

For case (1), we can express the cost as 2

$• wv 2 0 + 1/2-w 1/2-a 1/2 a (x -v 0 ) 2 dx . Similarly, for case (2), we get a cost of wv 2 0 + 1-2w 1-2a • min{1-a,(v0+1)/2} a (x -v 0 ) 2 dx + 1-a (v0+1)/2 (x -1) 2 dx .$Therefore, the cost of the optimal algorithm from the above family is given as:

$min 2 • wv 2 0 + 1/2 -w 1/2 -a 1/2 a (x -v0) 2 dx , wv 2 0 + 1 -2w 1 -2a • min{1-a,(v 0 +1)/2} a (x -v0) 2 dx + 1-a (v 0 +1)/2 (x -1) 2 dx .$This cost is maximized for a

$= -2w 2 +w-2 √ w(1-w)+1 4w 2 -6w+2$, where the value of w satisfies

$32w 3 -56w 2 + w(1 -w) • (8w 4 -24w 3 + 38w 2 -8w -7) + 24w = 0.$The resulting bound is slightly larger than 0.0459. Next, we verify that for these a, w values, no deterministic algorithm can achieve a lower cost. Specifically, instead of using q(x) as described above, we generate a finite discrete distribution. For a parameter n ∈ N, we define:

$q n (x) =        ⌊n•w⌋ n if x ∈ {0, 1} 1 n if x ∈ a + 1-2a 2(n-2⌊n•w⌋)) + i • 1-2a n-2⌊n•w⌋ | i ∈ [n -2 ⌊n • w⌋] 0 otherwise .$Note that lim n→∞ q n (x) = q(x). We then find the optimal deterministic solution for this distribution by using a deterministic k-means clustering algorithm (for k = 2), that is guaranteed to converge, e.g., using [[8]](#). The code that we used to obtain this result is available at [[2]](#). The optimal deterministic algorithm for q n (x) tends to have either v 1 = 1 -v 0 or v 1 = 1 as hypothesized. Finally, for n = 10 6 we get a cost higher than 0.0459 which we use as a lower bound.

We do not believe that this bound is tight. Nonetheless, as we show in Section 6.2.4, our bound is within 0.2% of the optimum.

given in Appendix D, proceeds with a case analysis based on the value of p 0,1 , the probability that the sender would send the same bit for 0, 1. We show that there exists an optimal algorithm in which p 0,1/2 + p 1/2,1 + p 0,1 = 1, p 0,1/2 = p 1/2,1 , and G 1/2,1 = 1 -G 0,1/2 . This reduces the number of variables to three, allowing us to optimize the expression and show a lower bound of 1/16 on any unbiased algorithm.

## 5

Algorithms with Unbounded Private Randomness

Here, we consider the case where the shared randomness is limited to ℓ bits, i.e., h ∈ [2 ℓ ], but Buffy may use unbounded private randomness r ∼ U [0, 1] (that is independent of h).

We present the following algorithm: Buffy sends X to Angel, where

$X ≜ 1 if x ≥ (r + h)2 -ℓ 0 otherwise . Angel then estimates x = X + (h -0.5(2 ℓ -1)) • 2 -ℓ .$We first show that our protocol is unbiased. It holds that

$E[h] = 0.5(2 ℓ -1) and (r + h) ∼ U [0, 2 ℓ ] (i.e., (r + h)2 -ℓ ∼ U [0, 1]), and thus E[ x] = E[X] = x.$We now state theorem, whose proof appears in Appendix E, that bounds the variance:

$▶ Theorem 5. Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).$In Appendix F, we describe a simple generalization of this algorithm, together with a lower bound, for sending k > 1 bits. We now explain the connection to subtractive dithering and explore the applicability of the algorithm for the x ∈ {0, 1/2, 1} special case.

Connection to subtractive dithering: First invented for improving the visibility of quantized pictures [[17]](#), subtractive dithering aims to alleviate potential distortions that originate from quantization. Subtractive dithering was later extended for other domains such as speech [[6]](#), distributed deep learning [[1]](#), and federated learning [[20]](#b17).

In our setting, subtractive dithering corresponds to using shared randomness to add noise ς to x before applying a deterministic quantization and subtracting ς from the estimation. Specifically, let Q : [0, 1] → {0, 1} be a two-level deterministic quantizer such that Q(g) = 1 if g ≥ 1/2 and 0 otherwise. Then, in subtractive dithering Buffy sends X = Q(x + ς) and Angel estimates x = X -ς.

There are several noise classes that ς can be drawn from, as classified in [[18]](#b15), that yield

$x ∼ U [x -1/2, x + 1/2]. For example, ς can be distributed uniformly on [-1/2, 1/2].$Consider our algorithm of this section without restricting the number of random bits (i.e., ℓ → ∞, and rescale so h ∈ U [0, 1]). This would yield the following algorithm:

$X ≜ 1 if x ≥ h 0 otherwise and x = X + h -0.5. Similarly to subtractive dithering, we get that x ∼ U [x -1/2, x + 1/2],$as we prove in Appendix G for completeness. To see that the two algorithms are equivalent

$(for ς ∼ U [-1/2, 1/2]), denote h ′ = 1/2 -h (i.e., h ′ ∼ U [-1/2, 1/2]). Then X = 1 if x + h ′ ≥ 1/2 and x = X -h ′ .$Therefore, we conclude that our algorithm provides a spectrum between randomized rounding (ℓ = 0) and a form of subtractive dithering (ℓ → ∞). In practice, this means that a small number of shared random bits yields a variance that is close to that of subtractive dithering (Var[ x] = 1/12). For example, with a single shared random byte (i.e., ℓ = 8), our algorithm has a worst-case variance that is within 0.02% of 1/12. The x ∈ {0, 1/2, 1} case:

Notice that if x is known to be in {0, 1/2, 1}, then our (ℓ = 1) algorithm gives Var[ x] = 1/16, as evident from Theorem 5. Further, in this case, we do not require the private randomness as we can rewrite Buffy's algorithm as:

$X ≜        0 if x = 0 1 -h if x = 1/2 1 if x = 1$, while Angel estimates x = X + (h -0.5)/2. This algorithm considerably improves over randomized rounding (which is optimal when no shared randomness is allowed, as shown in Appendix A), that has a variance of 1/4 for x = 1/2; i.e., a single shared random bit reduces the worst-case variance by a factor of 4. Further, it also improves over subtractive dithering, reducing the variance by a 4/3 factor. Finally, this result is optimal according to the Section 4.2 lower bound, even if unbounded shared randomness is allowed.

## Algorithms without Private Randomness

In some cases, generating random bits may be expensive, e.g., when running on powerconstrained devices. This is particularly acute when the device operates in an energy harvesting mode [[26]](#b23). Past works have even considered how to "recycle" random bits (e.g., [[11]](#)). Therefore, it is important to study how to design algorithms that use just a few random bits. To address this need, we consider scenarios where Buffy and Angel have access to a shared ℓ-bit random value h, but no private randomness. One thing to notice is that Angel can produce at most 2 ℓ+1 different values since Angel is deterministic after obtaining the ℓ + 1 bits of h and X. In particular, this means there is no unbiased protocol for general x ∈ [0, 1]. Therefore, we focus on biased algorithms and study how shared randomness allows improving over deterministic rounding (which is optimal without shared randomness, as we show in Section 3.2).

We start by proposing an optimal algorithm for the case where x is known to be in {0, 1/2, 1}. Then, we present adaptations of the subtractive dithering estimation method for the biased x ∈ [0, 1] setting. These improve over both (unbiased) subtractive dithering and deterministic rounding. To the best of our knowledge, these adaptations are novel. Next, we show how Buffy can further reduce the cost while, among other changes, using a small number of shared random bits. We conclude by giving design principles for numerically approximating the optimal algorithm and give realizations for small number of shared random bits.

## The x ∈ {0, 1/2, 1} Case

We now consider the scenario where x is guaranteed to be in {0, 1/2, 1} using a single shared randomness bit h ∈ {0, 1}. For some α ∈ [0, 1], Buffy sends

$X = 1 if x = 1 ∨ (x = 1/2 ∧ h = 0) 0 otherwise while Angel estimates x = α • h + (1 -α) • X.$For example, this means that if x = 0, the squared error is 0 if h = 0 and α 2 otherwise. That is, the expected squared error is α 2 /2. We optimize over the α value to minimize the cost min

$α∈[0,1] max α 2 /2, (1 -(1 -α)) 2 /2, E (1/2 -(α • h + (1 -α) • (1 -h))) 2 . This is optimized for α = 1 -1/ √ 2, yielding a cost of 3/4 -1/ √ 2 ≈ 0.$04289, which is optimal according to our Section 4 lower bound, even if unbounded shared randomness is allowed.

## The x ∈ [0, 1] Case

An important observation regarding optimal biased algorithms is that they, without loss of generality, can be expressed as a pair of monotone increasing functions T, Z 0 : [0, 1] → [0, 1] as follows. Here T is a threshold function that determines whether 0 or 1 is sent, Z 0 is the estimator when 0 is received, and

$Z 1 : [0, 1] → [0, 1], given by Z 1 (h) = 1 -Z 0 (1 -h), is the estimator when 1 is received. That is, Buffy sends X = 1 if x ≥ T (h) 0 otherwise .$In turn, Angel estimates x = Z X (h). We further explain this representation in Appendix H.

Based on this observation, we next lay out a sequence of algorithmic improvements over deterministic rounding that leverage the shared randomness to reduce the cost. We visualize the algorithms resulting from each improvement in Figure [1](#).

## Subtractive dithering adaptations

As subtractive dithering provides the lowest cost (albeit using unbounded shared randomness) of the previously mentioned unbiased algorithms, one may wonder if it is possible to adapt it to the biased scenario. Accordingly, we first briefly overview two natural adjustments that use unbounded shared randomness and improve over the 1/16 cost of deterministic rounding.

We then propose improved protocols that reduce the cost further despite using only a small number (e.g., ℓ = 3) of random bits. Intuitively, subtractive dithering may produce estimates that are outside the [0, 1] range. Therefore, by truncating the estimates to [0, 1] one may only reduce the expected squared error for any x ̸ = 1/2. However, it does not reduce the expected squared error for x = 1/2, and thus the cost would remain 1/12.

To reduce the cost, one may further truncate the estimates to [z, 1-z] for some z ∈ [0, 1/2]. Indeed, we show in Appendix I that this truncation reduces the cost to ≈ 0.0602, for z satisfying 1/24 + z 2 /2 + (2z 3 )/3 = 0 (z ≈ 0.17349).

A better adaptation strategy is obtained by changing the estimation to a linear combination of X and h. Specifically, consider the protocol where Buffy sends (for a shared h ∼ U [0, 1])

$X = 1 if x ≥ h 0 otherwise$and Angel estimates, for some α ∈ [0, 1],

$x = α • h + (1 -α) • X. 1 3 2 3 T (h) Buffy: X =    1 if x ≥ T (h) 0 Otherwise 0 1 4 1 2 3 4 1 h 1 3 Z (h) Angel: x =    Z 0 (h) if X = 0 Z 1 (h) Otherwise Deterministic Rounding (Section 3.2)$Subtractive Adaptation (Section 6.2.1) Deterministic Edge Rounding (Section 6.2.2)

Linear Sigmoid (Section 6.2.3)

Optimal Sigmoid 8-bit Approximation (Section 6.2.4)

$0 1 4 1 2 3 4 1 h 0 1 3 2 3 ∂T (h) ∂h Figure 1$Illustration of the different biased algorithms. While deterministic rounding does not use the shared randomness and is thus constant, the other algorithms have both the threshold and estimation be monotone functions of h.

Optimizing the parameters, we show in Appendix J that this algorithm achieves a cost of 5/3 -ϕ ≈ 0.04863, which is obtained for α = 2 -ϕ ≈ 0.382. Interestingly, this cost is achieved for all x ∈ [0, 1].

In Figure [1](#), we illustrate this algorithm. As shown, the subtractive dithering adaption has T (h) = h and Z 0 (h) = α • h. This means that Buffy sends X = 1 if x ≥ h and Angel estimates

$x = α • h if X = 0 and x = (1 -α • (1 -h)) = (1 -α) + α • h otherwise.$
## Deterministically rounding extreme values

We now show how to leverage a finite number of shared random bits ℓ to design improved algorithms. As we show, it is possible to benefit from deterministically rounding values that are "close" to 0 or 1 and use the shared randomness otherwise.

Similarly to the subtractive dithering adaptation above, Angel estimates x using a linear combination of h (with weight α) and X (with a weight of 1 -α), where α ∈ [0, 1] is chosen later. For all i ∈ [2 ℓ -1], define the interval

$I i = (1 -α)/2 + i • α 2 ℓ -1 , (1 -α)/2 + (i + 1) • α 2 ℓ -1 . (2$$)$In our algorithm, Buffy sends

$X =        0 if x < (1 -α)/2 1 h≤i if x ∈ I i , i ∈ [2 ℓ -1] 1 if x ≥ (1 + α)/2$, and Angel estimates:

$x = α • h/(2 ℓ -1) + (1 -α) • X.$Note that we deterministically partition the range [(1 -α)/2, (1 + α)/2] into 2 ℓ -1 equally spaced intervals. Intuitively, these intervals are chosen in a way that makes the expected squared error a continuous function of x, as our analysis, given in Appendix K, indicates.

As we show, minimizing cost = min α max x E[( x -x)[foot_1](#foot_1) ] yields cumbersome expressions. For example, we get that with one shared random bit (ℓ = 1), our algorithm has a cost of 1/18 ≈ 0.05556 (obtained for 2 α = 1/3), lower than that of deterministic rounding (i.e., 1/16). For ℓ = 2, we obtain a cost of 259-140

$√ 3 338 ≈ 0.04885 (reached for α = 15-6 √ 3 13$), and ℓ = 3 bits further reduces the cost to 35/722 ≈ 0.04848 (when α = 7/19). Additionally, with ℓ = 3 bits, this improves over the subtractive dithering adaptions (that use unbounded shared randomness) for all x ∈ [0, 1]. Notice that these costs are ≈21%, ≈6.4%, and ≈5.6% from the ≈ 0.0459 lower bound (see Section 4.1.2), and thus from the optimal algorithm. For completeness, we give the limiting algorithm (as ℓ → ∞) in Appendix L. For intuition, we illustrate the limiting algorithm (h ∈ [0, 1]) in Figure [1](#). As shown, we have

$T (h) = 1-α 2 +α •h (where α = 2 -ϕ ≈ 0.38) and Z 0 (h) = α • h.$Observe that Angel uses the same estimation function as in Section 6.2.1, but Buffy's threshold function is different. Intuitively, the new threshold function ensures that each x is mapped to the closest estimate value. For example, if x = 0.1 and h = 0, the subtractive adaptation would have X = 1 and thus x = 1 -α ≈ 0.62 while here we get X = 0 and x = 0.

Interestingly, the cost slightly and monotonically increases when increasing the number of bits ℓ beyond 3. This phenomenon suggests that we need more complex algorithms to leverage additional available random bits. We explore several approaches; in Appendix M, we show that by probabilistically selecting between the above algorithm (for ℓ → ∞) and the {0, 1/2, 1} algorithm from Section 6.1, we can reduce the error to 6

$√ 10+11 √ 5-18 √ 2-17 24 ≈ 0.04644.$Intuitively, Buffy and Angel can implicitly agree on the chosen algorithm using the shared randomness. Here, we proceed by analyzing the potential benefits of non-uniform partitioning of the h values, which reduces the error further.

## Non-uniform partitioning

Intuitively, the above algorithms have a threshold function that is linear in h; i.e., it takes the form T (h) = a • h + b. We now show that this can be improved by looking at sigmoidlike functions. For ease of exposition, in this section, we consider h ∈ [0, 1] to represent unbounded shared randomness, although the algorithm can be discretized given sufficient random bits. Recall from Section 6.2 that an algorithm can be expressed as a pair of functions T, Z 0 : [0, 1] → [0, 1] such that Buffy sends 1 if x ≥ T (h) while Angel estimates Z 0 (h) when receiving X = 0 and Z 1 (h) = 1 -Z 0 (1 -h) otherwise. Here, we consider a linear sigmoid function (also illustrated in Figure [1](#)), which, for some h 0 ∈ [0, 1/2], is defined as

$T (h) =        α if h < h 0 α + (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ] 1 -α otherwise . Z 0 (h) =        0 if h < h 0 (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ] 1 -2α otherwise .$Notice that in this algorithm we have Z 0 (h) = T (h) -α.

Our analysis, given in Appendix N, shows that the cost is minimized for h 0 = 1/4, α = 1/3, where the error is:

$E[( x -x) 2 ] = E[( x) 2 ] -2xE[ x] + x 2 =        5/108 -x/3 + x 2 if x < 1/3 5/108 if x ∈ [1/3, 2/3] 77/108 -5x/3 + x 2 otherwise .$Therefore, the cost is 5/108 ≈ 0.0463, which is less than 0.9% higher than the 0.0459 lower bound (Section 4.1.2). The algorithm has two interesting properties. First, its expected squared error is constant for all x ∈ {0, 1} ∪ [1/3, 2/3] and, second, its expectation is not continuous as a function of x, as shown in Figure [2](#). 

## E[ x]

Deterministic Rounding (Section 3.2)

Subtractive Adaptation (Section 6.2.1) Deterministic Edge Rounding (Section 6.2.2)

Linear Sigmoid (Section 6.2.3)

Optimal Sigmoid 8-bit Approximation (Section 6.2.4)

Figure [2](#) Illustration of the expectation of the different biased algorithms. Deterministic rounding does not use randomization and is therefore a step function, while others increase gradually in x. Notice that the expectations of the linear sigmoid and optimal approximation are not continuous.

## Towards the optimal algorithm

We now consider more general algorithms that have arbitrary estimate function Z 0 . To that end, we use a numerical solver that approximates the optimal solution. Clearly, to define the input problem, we need to limit the number of variables and constraints. We achieve this using several observations:

We consider bounded shared randomness h ∈ [2 ℓ ] for ℓ ∈ N bits. In fact, bounded shared randomness is precisely what allows us to develop this numerical approach. We use the observation that an optimal algorithm's T and Z 0 functions are not independent and satisfy ∀h ∈ [0, 1] :

$T (h) = Z0(h)+Z1(h) 2 = Z0(h)+(1-Z0(1-h))$
## 2

; this is because, that way, every x ∈ [0, 1] is estimated using the value closer to it between Z 0 (h) and Z 1 (h) = 1 -Z 0 (1 -h). In fact, the algorithms in sections 6.2.2-6.2.3 follow this rule, while the subtractive adaptation (Section 6.2.1) does not. As a result, we can define the variables z h |h ∈ [2 ℓ ] and derive the thresholds from the solver's output by Z 0 (h) = z h . For computing the maximal error for any x ∈ [0, 1], it is enough to look at a discrete set of points. This is because the number of possible estimates is 2 ℓ+1 . Therefore, given two estimates z h , 1 -z 2 ℓ -1-h that correspond to the values Angel uses given h and X = 0 or X = 1, the worst expected squared error (for this h) is obtained for

$y h ≜ z h +1-z 2 ℓ -1-h 2$. Therefore, by checking all x ∈ y h | h ∈ [2 ℓ ] , we can compute the cost. Deterministic Rounding (Section 3.2) Subtractive Adaptation (Section 6.2.2) Deterministic Edge Rounding `=3 (Section 6.2.3) Linear Sigmoid (Section 6.2.4) Optimal Sigmoid 8-bit Approximation (Section 6.2.5) 0 1=2 1 x Deterministic Rounding (Section 3.2) Subtractive Adaptation (Section 6.2.2) Biased © 0;1=2;1 ª (Section 6.1) 0:848 0:852 0:08333 0:08334

Figure [3](#) An illustration of the variance and expected squared error of the different algorithms. As shown, our unbiased algorithm is competitive with subtractive dithering despite using a single shared random byte, while our single-bit algorithm improves over subtractive on {0, 1/2, 1}. For the biased case, in addition to improving the {0, 1/2, 1} case, our optimal sigmoid approximation algorithm achieves the lowest cost (less than 0.2% of the optimum!) while using a single shared random byte.

Using these observations, we formulate the input as: minimize

${z h |h∈[2 ℓ ]} C subject to C ≥ h j=0 (y h -(1 -z 2 ℓ -1-h )) 2 + 2 ℓ -1 j=h+1 (y h -z h ) 2 , h = 0, . . . , 2 ℓ -1 y h = z h +1-z 2 ℓ -1-h 2 , z h ∈ [0, 1] h = 0, . . . , 2 ℓ -1$In the above, we express the expected squared error at y h by considering the h values for which x ≥ T (h) (j ∈ [h]) and those that x < T (h). The output for the above problem does not seem to follow a compact representation. However, it is still possible to implement using a simple lookup table. For example, if ℓ = 8, we can store all z h when implementing Buffy and Angel. This algorithm's cost is lower than that of the linear sigmoid (that uses unbounded randomness) when using ℓ ≥ 4 bits. Specifically, using 4 shared random bits, the cost is ≈ 0.04611, while using 8 bits, it further reduces to ≈ 0.04599. Notice that these are less than 0.5% and 0.2% higher than the lower bound of Section 4.1.2. We note that this approach yields improvement even for a small number of shared random bits; for example, using ℓ = 1 bit (h ∈ {0, 1}), we get a cost of 1/20 for z 0 = 0.1, z 1 = 0.3 which is equivalent to the following algorithm:

$X = 1 if x ≥ 0.4 + 0.2h 0 otherwise , x = 0.1 + 0.2h + 0.6X .$We visualize the resulting algorithm, for ℓ = 8, in figures 1 and 2. Notice that while the algorithm looks almost similar to our linear sigmoid, looking that the derivative ∂T (h) ∂h (Figure [1](#)) shows that this optimal solution is not piece-wise linear.

## Visual Comparison of the Algorithm Costs

We illustrate the various algorithms in Figure [3](#). In the unbiased case, notice how a single (ℓ = 1) shared random bit significantly improves over randomized rounding (which is optimal when Buffy and Angel are restricted to private randomness). This further improves for larger ℓ values, where for ℓ = 8 we have a cost that is only 0.02% higher than that of subtractive dithering, which uses unbounded shared randomness (the difference shown in zoom). When

x is known to be in {0, 1/2, 1} (right-hand side of the figure), it is evident how our unbiased ℓ = 1 algorithm improves over both randomized rounding and subtractive dithering.

In the biased case, our adaptation to the subtractive dithering estimation (termed Subtractive Adaptation) improves over the cost of deterministic rounding. This is further improved by the algorithm of Section 6.2.2, termed Deterministic Edge Rounding, which is depicted using ℓ = 3 bits as it minimizes its cost. Next, the Linear sigmoid (Section 6.2.3) shows how to lower the cost (using unbounded shared randomness) by non-uniform partitioning of the h values. Additionally, we show the optimal 8-bit algorithm (Section 6.2.4) that gets within 0.2% from the lower bound while using a single shared random byte. Finally, if x is known to be in {0, 1/2, 1}, our (optimal) biased {0, 1/2, 1} algorithm improves over all other solutions while using only a single shared random bit.

## Discussion

In this paper, we studied upper and lower bounds for the problem of sending a real number using a single bit. The goal is to minimize the cost, which is the worst-case variance for unbiased algorithms, or the worst-case expected squared error for biased ones. For all cases, we demonstrated how shared randomness helps to reduce the cost. Motivated by real-world applications, we derived algorithms with a bounded number of random bits that can be as low as a single shared bit. For example, in the unbiased case, using just one shared random bit reduces the variance two-fold compared to randomized rounding (which is optimal when no shared randomness is available). Further, using a single byte of shared randomness, our algorithm's variance is within 0.02% from the state of the art, which uses unbounded shared randomness.

Our results are also near-optimal in the biased case, with a gap lower than 0.2% between the upper and lower bounds with a single shared random byte. Our upper bound is presented as a sequence of algorithms, each generalizing the previous while reducing the cost further.

For the special case where x is known to be in {0, 1/2, 1}, we give optimal unbiased and biased algorithms, together with matching lower bounds. Our algorithms use a single shared random bit, and the lower bounds show that the cost cannot be improved even when unbounded shared randomness is allowed.

We conclude by identifying directions for future research, beyond settling the correct bounds. First, our lower bounds apply for algorithms that use unbounded shared randomness, and new techniques for developing sharper bounds for other cases are of interest. Another direction is looking into optimizing the cost when sending k bits, for some k > 1. We make a first small step in Appendix F, where we provide simple generalizations of our unbiased algorithm and lower bound to sending k bits. Also, in a recent followup work [[21]](#b18), we showed that for sending d-sized real vector using d(1 + o(1)) bits it is better to encode all coordinates together rather than sending them separately. Intuitively, we can reduce the error by generating an encoded vector, that mixes the original vector entries, before sending them. It is interesting to formalize the bounds for sending vectors similarly to the single number case. One possible direction is to send the encoded coordinates using the tools developed in this paper. Finally, we are unclear on whether private randomness can help improve biased algorithms (see Table [1](#)).

## A Optimality of Randomized Rounding

We show that without shared randomness, randomized rounding is optimal in the sense that it minimizes the worst-case estimation variance. Consider an arbitrary protocol. We model it as follows: we have two (deterministic) parameters: Y : [0, 1] → [0, 1] and Γ : {0, 1} → ∆([0, 1]).

Buffy computes p = Y (x) and sends X ∼ Bernoulli(p). In turn, Angel receives X, and estimates x by drawing from the distribution Γ(X). We also denote by Z 0 ∼ Γ(0) and Z 1 ∼ Γ(1) random variables such that the final estimate is

$x = 1 X=0 • Z 0 + 1 X=1 • Z 1 .$Notice that this formulation captures any protocol. For example, randomized rounding is defined as

$Y (x) = x and Γ(X)(y) = 1 if y = X 0 otherwise .$(this is a slight abuse of notation as the above definition assumes that Γ(X) is a density function). We demand that the protocol will produce unbiased estimates for any x. That is, it must satisfy:

$E[ x] = Y (x) • E [Z 1 ] + (1 -Y (x)) • E [Z 0 ] = x.$(3)

In particular, for x = 0, we have:

$Y (0) • E [Z 1 ] + (1 -Y (0)) • E [Z 0 ] = 0.$and equivalently:

$E [Z 0 ] = - Y (0) 1 -Y (0) • E [Z 1 ] . (4)$Similarly, plugging x = 1 into (3) gives:

$Y (1) • E [Z 1 ] + (1 -Y (1)) • E [Z 0 ] = 1.$Using (4), we proceed with several simplifications:

$Y (1) • E [Z 1 ] + (1 -Y (1)) • - Y (0) 1 -Y (0) • E [Z 1 ] = 1. E [Z 1 ] • Y (1) -(1 -Y (1)) Y (0) 1 -Y (0) = 1. E [Z 1 ] • Y (1) • (1 -Y (0)) -(1 -Y (1))Y (0) 1 -Y (0) = 1. E [Z 1 ] • Y (1) -Y (0) 1 -Y (0) = 1. E [Z 1 ] = 1 -Y (0) Y (1) -Y (0) . (5$$)$Plugging ( [5](#formula_43)) into (4), we also get:

$E [Z 0 ] = - Y (0) 1 -Y (0) • E [Z 1 ] = - Y (0) Y (1) -Y (0) . (6$$)$Substituting ( [5](#formula_43)) and ( [6](#formula_45)) in (3), we simplify the expression further:

$Y (x) • E [Z 1 ] + (1 -Y (x)) • E [Z 0 ] = x. Y (x) • 1 -Y (0) Y (1) -Y (0) + (1 -Y (x)) • - Y (0) Y (1) -Y (0) = x. Y (x) • (1 -Y (0)) -(1 -Y (x)) • Y (0) Y (1) -Y (0) = x. Y (x) -Y (0) Y (1) -Y (0) = x. Y (x) = x • (Y (1) -Y (0)) + Y (0). Y (x) = x • Y (1) + (1 -x) • Y (0). (7$$)$That is, we have that the probability to send X = 1 must be linear in x. We analyze the variance that results for x = 0.5.

$Var[ x|x = 0.5] = E[( x -0.5) 2 |x = 0.5] = E[( x) 2 |x = 0.5] -E[( x) |x = 0.5] + 0.25.$Since x is unbiased, E[( x) |x = 0.5] = 0.5 and we get

$Var[ x|x = 0.5] = E[( x) 2 |x = 0.5] -0.25. (8) Next, we analyze E[( x) 2 |x = 0.5]: E[( x) = E (Z 0 ) 2 • 1 X=0 |x = 0.5 + E (Z 1 ) 2 • 1 X=1 |x = 0.5 .$We have that Z 0 , Z 1 are independent of 1 X=0 , 1 X=1 and of x, and thus

$E[( x) 2 |x = 0.5] = E (Z 0 ) 2 |x = 0.5 • (1 -Y (0.5)) + E (Z 1 ) 2 |x = 0.5 • Y (0.5) = E (Z 0 ) 2 • (1 -Y (0.5)) + E (Z 1 ) 2 • Y (0.5) ≥ (E [Z 0 ]) 2 • (1 -Y (0.5)) + (E [Z 1 ]) 2 • Y (0.5). (9$$)$Using ( [5](#formula_43)) and ( [6](#formula_45)), we have:

$E[( x) 2 |x = 0.5] ≥ Y (0) Y (1) -Y (0) 2 • (1 -Y (0.5)) + 1 -Y (0) Y (1) -Y (0) 2 • Y (0.5) = (Y (0)) 2 • (1 -Y (0.5)) + (1 -Y (0)) 2 • Y (0.5) (Y (1) -Y (0)) 2 = (Y (0)) 2 + Y (0.5) -2Y (0)Y (0.5) (Y (1) -Y (0)) 2 .$We now use ( [7](#formula_47)) for x = 0.5 and get Y (0.5) = 0.5 • (Y (0) + Y (1)), which means:

$E[( x) 2 |x = 0.5] ≥ (Y (0)) 2 + Y (0.5) -2Y (0)Y (0.5) (Y (1) -Y (0)) 2 = (Y (0)) 2 + 0.5 • (Y (0) + Y (1)) -2Y (0) • 0.5 • (Y (0) + Y (1)) (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) (Y (1) -Y (0)) 2 .$Combined with (8), this gives:

$Var[ x|x = 0.5] = E[( x) 2 |x = 0.5] -0.25 (10) ≥ 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) (Y (1) -Y (0)) 2 -0.25 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) -0.25 (Y (1) -Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -Y (0) • Y (1) -0.25 (Y (1)) 2 + 0.5Y (0)Y (1) -0.25 (Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -0.5Y (0) • Y (1) -0.25 (Y (1)) 2 -0.25 (Y (0)) 2 (Y (1) -Y (0)) 2 = 0.5 • (Y (0) + Y (1)) -0.5 • (Y (0) + Y (1)) 2 (Y (1) -Y (0)) 2 . (11$$)$Over the domain Y (0), Y (1) ∈ [0, 1], (11) has two minima: Y (0) = 0, Y (1) = 1 and Y (0) = 1, Y (1) = 0. Indeed, the first corresponds to randomized rounding, while the second is using a simple transform that negates the randomized rounding's bit.

To conclude, we established that randomized rounding has a minimal worst-case variance. As a side note, by deterministically estimating x = X, Inequality (9) holds as an equality and the variance is exactly 0.25.

## B Optimality of Deterministic Rounding

We show that without shared randomness, deterministic rounding is an optimal biased solution. Notice that, in such a case, any protocol is defined by the probability of sending 1, denoted Y (x), and the reconstruction distributions

$V 0 , V 1 ∈ ∆([0, 1]). Let us examine E[V 0 ] and E[V 1 ]. We assume, without lost of generality, that E[V 0 ] ≤ E[V 1 ].$We have that:

$E[ x] = Y (x)E[V 1 ] + (1 -Y (x))E[V 0 ].$That is, we have that for any x ∈ [0, 1]:

$E[V 0 ] ≤ E[ x] ≤ E[V 1 ]. Next, we have that the cost, E[( x -x) 2 ], is bounded as E[( x -x) 2 ] ≥ (E[( x -x)]) 2 .$In particular, for x = 0, we get that

$E[( x -x) 2 |x = 0] ≥ (E[ x|x = 0]) 2 ≥ (E[V 0 ]) 2 .$Similarly, for x = 1, we have

$E[( x -x) 2 |x = 1] ≥ (E[( x)|x = 1] -1) 2 ≥ (1 -E[V 1 ]) 2 . Notice that if E[V 0 ] ≥ 0.25 then E[( x -x) 2 |x = 0] ≥ 1/16, and similarly, if E[V 1 ] ≤ 0.75 then E[( x -x) 2 |x = 1] ≥ 1/16$. Assume to the contrary that there exists an algorithm with a with a worst-case expected squared error lower than 1/16, then we have

$E[V 0 ] ≤ 0.25 and E[V 1 ] ≥ 0.$75. However, we have that x = 0.5 gives: In the first inequality, we used that fact that for any random variable V :

$E[( x -x) 2 |x = 0.5] = E[ x 2 |x = 0.5] -2xE[ x|x = 0.5] + 0.25 = Y (0.5)E[V 2 1 ] + (1 -Y (0.5))E[V 2 0 ] -(Y (0.5)E[V 1 ] + (1 -Y (0.5))E[V 0 ]) + 0.25 ≥ Y (0.5)(E[V 1 ]) 2 + (1 -Y (0.5))(E[V 0 ]) 2 -(Y (0.5)E[V 1 ] + (1 -Y (0.5))E[V 0 ]) + 0.25 = Y (0.5) • E[V 1 ] • (E[V 1 ] -1) + (1 -Y (0.5)) • E[V 0 ] • (E[V 0 ] -1)$$E[V 2 ] ≥ (E[V ]) 2 ,$and in the second we used E[V 0 ] ≤ 0.25 and E[V 1 ] ≥ 0.75. This concludes the proof and establishes the optimality of deterministic rounding when no shared randomness is used.

## C

Proof of the Biased {0, 1/2, 1} Lower Bound

We recall Lemma 4:

▶ Lemma 4. Any deterministic algorithm must incur a cost of at least q(0)•q(1/2) 4(q(0)+q(1/2)) . Proof. We denote by X 0 the set of values in {0, 1/2, 1} that are closer to v 0 than to v 1 . We assume without loss of generality that v 0 ≤ v 1 and q(0) ≤ q(1) and prove that an optimal algorithm would set v 0 = q(1/2) 2(q(0)+q(1/2)) , v 1 = 1, which incurs a cost of q(0)•q(1/2) 4(q(0)+q(1/2)) . Indeed, for this choice of v 0 , v 1 we have that X 0 = {0, 1/2}, and we get a cost of

$q(0) q(1/2) 2(q(0) + q(1/2)) 2 + q(1/2) 1 2 - q(1/2) 2(q(0) + q(1/2)) 2 = q(0) q(1/2) 2(q(0) + q(1/2)) 2 + q(1/2) q(0) 2(q(0) + q(1/2)) 2 = q(0)q(1/2) 2 + q(1/2)q(0) 2 4 (q(0) + q(1/2)) 2 = q(0) • q(1/2) 4 (q(0) + q(1/2))$.

We now bound the performance of the optimal algorithm. We first notice that an optimal algorithm should have 0 ∈ X 0 and 1 ̸ ∈ X 0 . Next, notice that v 0 should be at most 1/2 and v 1 should be at least 1/2. Otherwise, one can improve the error for x = 0 or x = 1, respectively, without increasing the error at 1/2. Further, observe that an optimal algorithm must have v 0 = 0 or v 1 = 1. That is because if 1/2 ∈ X 0 , we can reduce the error for x = 1 by setting v 1 = 1. Similarly, when 1/2 ̸ ∈ X 0 , choosing v 0 = 0 decreases the error for x = 0. Now, we claim that there exists an optimal algorithm for which v 1 = 1. Consider some solution, and set v ′ 0 = 1 -v 1 and v ′ 1 = 1. This does not affect the error of x = 1/2, and does not increase the cost as q(0) ≤ q(1). We are left with choosing v 0 ; let us denote by c(v 0 ) = q(0)v 2 0 + q(1/2)(1/2 -v 0 ) 2 the resulting cost. This function has a minimum at

$v 0 = q(1/2) 2(q(0)+q(1/2)) , which gives a cost of q(0)•q(1/2) 4(q(0)+q(1/2)) . ◀ This cost is maximized for q(1/2) = √ 2 -1 and q(0) = q(1) = 2- √ 2$2 , giving a lower bound of 3/4 -1/ √ 2 ≈ 0.04289. In fact, one can verify that this is the best attainable lower bound for any discrete distribution on three points. Further, in Section 6.1, we show that this is an optimal lower bound when x is known to be in {0, 1/2, 1}, by giving an algorithm with a matching cost.

## D

An Optimal Lower Bound for the Unbiased {0, 1/2, 1} Case

$Assume that we have h ∈ [0, 1]. Buffy sends X(x, h) to Angel which estimates x(X(x, h), h). For x ′ , x ′′ ∈ {0, 1/2, 1}, let p x ′ ,x ′′ = Pr[X(x ′ , h) = X(x ′′ , h)]$denote the probability (with respect to h) that the same bit is sent for x ′ , x ′′ . Since we send a single bit, we have that p 0,1/2 + p 1/2,1 + p 0,1 ≥ 1. Further, any algorithm for which p 0,1/2 + p 1/2,1 + p 0,1 > 1 can be transformed to having p 0,1/2 + p 1/2,1 + p 0,1 = 1. For example, assume without loss of generality that for some h ∈ [0, 1], X(0, h) = X(1/2, h) = X(1, h). In this case, making the following modification still yields an algorithm with identical estimates: X(1, h) = 1 -X(0, h) and x(X(1, h), h) = x(X(0, h), h). Therefore, we can assume that:

$p 0,1/2 + p 1/2,1 + p 0,1 = 1.$For all x ′ , x ′′ ∈ {0, 1/2, 1}, we define by

$H x ′ ,x ′′ = {h ∈ [0, 1] : X(x ′ , h) = X(x ′′ , h$)} the set of shared-randomness values that would lead Buffy to send the same bit for both x and x ′ . Next, denote by

$G x ′ ,x ′′ = E[ x|h ∈ H x ′ ,x ′′ , x ∈ {x ′ , x ′′ }]$the expected estimate value, conditioned on the shared randomness being in H x ′ ,x ′′ . We have that:

$Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + p 0,1 • (G 0,1 -0) 2 + p 1/2,1 • (E[ x|h ∈ H 1/2,1 , x = 0] -0) 2 (12) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + p 0,1 • (E[ x|h ∈ H 0,1 , x = 1/2] -1/2) 2 + p 1/2,1 • (G 1/2,1 -1/2) 2 (13) Var[ x|x = 1] ≥ p 0,1/2 • (E[ x|h ∈ H 0,1/2 , x = 1] -1) 2 + p 0,1 • (G 0,1 -1) 2 + p 1/2,1 • (G 1/2,1 -1) 2 . (14$$)$Due to unbiasedness, we must have

$G 0,1/2 p 0,1/2 + G 0,1 p 0,1 + E[ x|h ∈ H 1/2,1 , x = 0]p 1/2,1 = 0 (15) G 0,1/2 p 0,1/2 + E[ x|h ∈ H 0,1 , x = 1/2]p 0,1 + G 1/2,1 p 1/2,1 = 1/2 (16) E[ x|h ∈ H 0,1/2 , x = 1]p 0,1/2 + G 0,1 p 0,1 + G 1/2,1 p 1/2,1 = 1. (17$$)$We proceed with a case analysis based on the p 0,1 , the probability that the sender would send the same bit for 0, 1. [p 0,](#)[1 = 0](#) We start with the simpler case where the sender never sends the same bit for 0, 1 (and thus p 1/2,1 = 1 -p 0,1/2 ). Then ( [12](#))-( [14](#formula_71)) yield:

## D.1 Case

$Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -p 0,1/2 ) • (E[ x|h ∈ H 1/2,1 , x = 0] -0) 2 Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1/2) 2 Var[ x|x = 1] ≥ p 0,1/2 • (E[ x|h ∈ H 0,1/2 , x = 1] -1) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1) 2 .$Similarly, using ( [15](#))-( [17](#formula_73)) we get:

$G 0,1/2 p 0,1/2 + E[ x|h ∈ H 1/2,1 , x = 0](1 -p 0,1/2 ) = 0 G 0,1/2 p 0,1/2 + G 1/2,1 (1 -p 0,1/2 ) = 1/2 (18) E[ x|h ∈ H 0,1/2 , x = 1]p 0,1/2 + G 1/2,1 (1 -p 0,1/2 ) = 1.$This gives

$E[ x|h ∈ H 1/2,1 , x = 0] = 0 -G 0,1/2 p 0,1/2 1 -p 0,1/2 E[ x|h ∈ H 0,1/2 , x = 1] = 1 -G 1/2,1 (1 -p 0,1/2 ) p 0,1/2 ,$and thus:

$Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -p 0,1/2 ) • 0 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -0 2 (19) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1/2) 2 Var[ x|x = 1] ≥ p 0,1/2 • 1 -G 1/2,1 (1 -p 0,1/2 ) p 0,1/2 -1 2 + (1 -p 0,1/2 ) • (G 1/2,1 -1) 2 .$Equation [(18)](#b15) gives

$G 1/2,1 = 1/2-G 0,1/2 p 0,1/2 1-p 0,1/2$and therefore:

$Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -1/2 2 (20) Var[ x|x = 1] ≥ p 0,1/2 • 1 -(1/2 -G 0,1/2 p 0,1/2 ) p 0,1/2 -1 2 + (1 -p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 1 -p 0,1/2 -1 2 . (21$$)$Minimizing max {( [19](#)),( [20](#)),( [21](#formula_80))}, we get a bound of 1/16, obtained for p 0,1/2 = 1/2 and G 0,1/2 = 1/4. ▶ Lemma 6. There exists an optimal unbiased solution for which p 0,1/2 = p 1/2,1 and

$G 1/2,1 = 1 -G 0,1/2 .$The lemma implies that p 0,1 = 1 -p 0,1/2 -p 1/2,1 = 1 -2p 0,1/2 . Therefore, we get

$Var[ x|x = 0] ≥ p 0,1/2 • (G 0,1/2 -0) 2 + (1 -2p 0,1/2 ) • (G 0,1 -0) 2 + p 0,1/2 • 0 -G 0,1/2 p 0,1/2 -G 0,1 (1 -2p 0,1/2 ) p 0,1/2 -0 2 (22) Var[ x|x = 1/2] ≥ p 0,1/2 • (G 0,1/2 -1/2) 2 + (1 -2p 0,1/2 ) • 1/2 -G 0,1/2 p 0,1/2 -(1 -G 0,1/2 )p 0,1/2 (1 -2p 0,1/2 ) -1/2 2 + p 0,1/2 • (1/2 -G 0,1/2 ) 2(23)$Var

$[ x|x = 1] ≥ p 0,1/2 • 1 -G 0,1 (1 -2p 0,1/2 ) -(1 -G 0,1/2 )p 0,1/2 p 0,1/2 -1 2 + (1 -2p 0,1/2 ) • (G 0,1 -1) 2 + p 0,1/2 • (-G 0,1/2 ) 2 . (24$$)$The infimum of max {( [22](#)),( [23](#formula_83)),( [24](#formula_84))}, over all possible p 0,1/2 , G 0,1/2 , G 0,1 values, we get a lower bound of 1/16, which is obtained for p 0,1/2 → 1/2, G 0,1/2 = 1/4 (i.e., p 0,1 → 0).

## D.3 Proof of Lemma 6

Assume an optimal unbiased algorithm, defined using X * (x, h), x * (X, h), the probabilities p * 0,1/2 , p * 0,1 , p * 1/2,1 , and G * 0,1/2 , G * 0,1 , G * 1/2,1 . We consider the following algorithm; let h alg be a shared random bit that is independent of h. If h alg = 0 Buffy sends X = X * (x, h) and otherwise (if h alg = 1) X = X * (1 -x, h). In turn, Angel estimates x = x * (X, h) if h alg = 0 or x = 1 -x * (X, h) otherwise.

Notice that our algorithm is unbiased:

$E[ x] = 1/2(E[ x|h alg = 0] + E[ x|h alg = 1]) = 1/2(x + E[1 -x * (X * (1 -x, h), h)|h alg = 1]) = 1/2(x + 1 -(1 -x)) = x.$Next, observe that the algorithm, and thus the variance, remains unchanged for x = 1/2. For x = 0:

$Var[Z 0 ] = 1/2(Var[ x|h alg = 0, x = 0] + Var[ x|h alg = 1, x = 0]) = 1/2(Var[ x * |x = 0] + Var[ x * |x = 1]).$Here, we used the fact that for any random variable Var

$[ x * (X * (1, h), h)] = 1-Var[ x * (X * (1, h), h)].$Therefore, we get that Var[ x|x = 0] ≤ max {Var[ x * |x = 0], Var[ x * |x = 1]} and we have not increased the cost. A symmetric analysis applies to x = 1.

Finally, we get that

$p 0,1/2 = Pr[X(0, h) = X(1/2, h)] = 1/2(Pr[X(0, h) = X(1/2, h)|h alg = 0] + Pr[X(1, h) = X(1/2, h)|h alg = 1]) = 1/2(p * 0,1/2 + p * 1/2,1 ).$By symmetry, we also get p 1/2,1 = 1/2(p * 0,1/2 + p * 1/2,1 ) and thus p 0,1/2 = p 1/2,1 . Similarly,

$G 0,1/2 = 1/2 G * 0,1/2 + (1 -G * 1/2,1 ) G 1/2,1 = 1/2 G * 1/2,1 + (1 -G * 0,1/2 ) .$Notice that G 0,1/2 + G 1/2,1 = 1, which concludes the proof. ◀

## E Analysis of the Unbiased Algorithm

In this appendix, we prove Theorem 5 which we restate here:

$▶ Theorem 5. Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).$First, we state a technical lemma, whose proof appears below in Appendix E.1, that shows the periodicity of the variance in our algorithm.

$▶ Lemma 7. For any y ∈ [0, 1 -2 -ℓ ], Var[ x|x = y] = Var[ x|x = y + 2 -ℓ ].$As a result of this periodicity, we can continue the analysis, without loss of generality, under the assumption that x ∈ [0, 2 -ℓ ]. We first calculate several useful quantities:

$E [X] = E X 2 = x E[h] = (2 ℓ -1)/2 E[h 2 ] = (2 ℓ -1)(2 ℓ+1 -1)/6 E[X • h|x ≤ 2 -ℓ ] = 0$(as either X = 0 or h = 0, since x ≤ 2 -ℓ ).

We now proceed with calculating the variance.

$Var[ x|x ≤ 2 -ℓ ] = E X + (h -0.5(2 ℓ -1)) • 2 -ℓ 2 -x 2 = E X 2 + 2 -2ℓ • E h 2 -E [h] • (2 ℓ -1) + 0.25(2 ℓ -1) 2 + 2 -ℓ+1 • E [X • h] -2 -ℓ • (2 ℓ -1) • E [X] -x 2 = x + 2 -2ℓ • E h 2 -E [h] • (2 ℓ -1) + 0.25(2 ℓ -1) 2 -2 -ℓ • (2 ℓ -1) • x -x 2 = x + 2 -2ℓ • (2 ℓ -1)(2 ℓ+1 -1)/6 -(2 ℓ -1) 2 /2 + 0.25(2 ℓ -1) 2 -2 -ℓ • (2 ℓ -1) • x -x 2 = 1/12 • (1 -4 -ℓ ) + 2 -ℓ x -x 2 .$Finally, according to Lemma 7, we get that:

$Var[ x] = 1/12 • (1 -4 -ℓ ) + 2 -ℓ (x mod 2 -ℓ ) -(x mod 2 -ℓ ) 2 . (25$$)$This gives a worst-case bound, achieved for x ∈ 2 -(ℓ+1

$) + i • 2 -ℓ | i ∈ [2 ℓ-1 ] , of Var[ x] ≤ 1/12 • (1 -4 -ℓ ) + 1/4 • 4 -ℓ = 1/6 • (1/2 + 4 -ℓ ).$
## E.1 Proof of Lemma 7

Let y ∈ [0, 1 -2 -ℓ ] and denote z = y + 2 -ℓ , m = y mod 2 -ℓ , and ζ = y • 2 ℓ . Notice that if h < ζ, then Buffy will send X = 1 for both y and z. Similarly, if h ≥ ζ + 1, Buffy will send X = 0 for both y and z. Notice that, for any y and ℓ:

$2 ℓ y -y • 2 ℓ = 2 ℓ (y mod 2 -ℓ ).$and thus

$y mod 2 -ℓ = y -y • 2 ℓ • 2 -ℓ .$Therefore, we can write:

$( x|x = y) = (h -0.5(2 ℓ -1)) • 2 -ℓ + 1 h<ζ + 1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) ( x|x = z) = (h -0.5(2 ℓ -1)) • 2 -ℓ + 1 h<ζ+1 + 1 (h=ζ+1)∧(r<2 ℓ •(z mod 2 -ℓ )) Denote (h -0.5(2 ℓ -1)) • 2 -ℓ by ψ. Thus, since y mod 2 -ℓ = z mod 2 -ℓ : Var[ x|x = z] -Var[ x|x = y] = E ψ + 1 h<ζ+1 + 1 (h=ζ+1)∧(r<2 ℓ •(y mod 2 -ℓ )) 2 - E (ψ + 1 h<ζ + 1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) 2 -z 2 + y 2 = E 1 h<ζ+1 + 1 (h=ζ+1)∧(r<2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h<ζ+1 + 1 (h=ζ+1)∧(r<2 ℓ •(y mod 2 -ℓ )) -1 h<ζ + 1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h<ζ + 1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) -z 2 + y 2 = E 1 h=ζ + 1 (h=ζ+1)∧(r<2 ℓ •(y mod 2 -ℓ )) -1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) + 2ψ 1 h=ζ + 1 (h=ζ+1)∧(r<2 ℓ •(y mod 2 -ℓ )) -1 (h=ζ)∧(r<2 ℓ •(y mod 2 -ℓ )) -z 2 + y 2 = E 1 -1 r<2 ℓ •(y mod 2 -ℓ ) + 2ψ 1 -1 r<2 ℓ •(y mod 2 -ℓ ) • 1 h=ζ + 1 r<2 ℓ •(y mod 2 -ℓ ) + 2ψ1 r<2 ℓ •(y mod 2 -ℓ ) • 1 h=ζ+1 -z 2 + y 2 = E 1 -1 r<2 ℓ •(y mod 2 -ℓ ) + 2ψ 1 -1 r<2 ℓ •(y mod 2 -ℓ ) |h = ζ • Pr[h = ζ]+ E 1 r<2 ℓ •(y mod 2 -ℓ ) + 2ψ • 1 r<2 ℓ •(y mod 2 -ℓ ) |h = ζ + 1 • Pr[h = ζ + 1] -z 2 + y 2 = 2 -ℓ • E 2ψ 1 -1 r<2 ℓ •(y mod 2 -ℓ ) |h = ζ + E 2ψ • 1 r<2 ℓ •(y mod 2 -ℓ ) |h = ζ + 1 + 2 -ℓ -z 2 + y 2 = (as h is independent of r) 2 -ℓ • E 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ 1 -1 r<2 ℓ •(y mod 2 -ℓ ) + E 2 (ζ + 1 -0.5(2 ℓ -1)) • 2 -ℓ • 1 r<2 ℓ •(y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ + 2 1-ℓ • E 1 r<2 ℓ •(y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 (ζ -0.5(2 ℓ -1)) • 2 -ℓ + 2 1-ℓ • (2 ℓ • (y mod 2 -ℓ )) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2ζ -(2 ℓ -1) • 2 -ℓ + 2 • (y mod 2 -ℓ ) + 2 -ℓ -z 2 + y 2 = 2 -ℓ • (2ζ + 1) • 2 -ℓ + 2 • (y mod 2 -ℓ ) -z 2 + y 2 = 2 -ℓ • 2 y • 2 ℓ + 1 • 2 -ℓ + 2 • y -y • 2 ℓ • 2 -ℓ -z 2 + y 2 = 2 -ℓ • 2 -ℓ + 2y -(z -y)(z + y) = 0. ◀ F Generalization to k Bits F.1 General Quantized Algorithm We use a hash function h such that h ∈ {0, 1} ℓ is uniformly distributed. Let A ∼ U [0, 1] be independent of h. C = 2 k -1 • x p = 2 k -1 • x -2 k -1 • x R = 2 k -1$We then set

$X ≜ C + 1 if p ≥ (A + h)2 -ℓ C otherwise$We send X to Angel which estimates

$x = X + (h -0.5(2 ℓ -1)) • 2 -ℓ R .$To show that our protocol is unbiased, notice that:

$E[X] = R•x and that E[h] = 0.5(2 ℓ -1).$
## F.2 Lower Bounds

Similarly to the 1-bit case, we consider the discrete distribution over

$i • 1 3 • 2 k-1 -1 | i ∈ 0, 1, . . . , 3 • 2 k-1 -1 . We set a 1/2 = √ 2-1 2 k-1 and a 0 = a 1 = 1-a 1/2 2 k and ∀i : q i • 1 3 • 2 k-1 -1 = a (i mod 3)/2 .$When each consecutive set of three points has the same probability, one can derive an optimal algorithm with precisely two values between each such triplet. The optimal choice of locations of the values in each triplet is similar to our single-bit analysis of the previous subsection, i.e., one should have a values at

$√ 2 -1 + i 2 k-1 i ∈ 0, 1, . . . , 2 k-1 -1 i 2 k-1 i ∈ 1, . . . , 2 k-1 .$We turn into calculating the cost. Notice that every triplet has a width of 2 3•2 k-1 -1 . Therefore, the cost now reduces, compared to the 1-bit analysis, by a factor of

$2 3•2 k-1 -1 2 . That is, we get a lower bound of 3-2 √ 2 (3•2 k-1 -1) 2 = 3-2 √ 2 2.25(2 k -2/3) 2 .$We note that, for large k and ℓ values, our variance is within 10% of the lower bound, as

$lim k,ℓ→∞ 1 12(2 k -1) 2 3-2 √ 2 2.25(2 k -2/3) 2 = 9 + 6 √ 2 16 ≈ 1.093.$
## G Limiting Algorithm Uniformness Proof

Recall that the algorithm uses h ∼ U [0, 1] where Buffy sends

$X ≜ 1 if x ≥ h 0 otherwise$and Angel estimates x = X + h -0.5.

▶ Lemma 8. For a fixed value of x, it holds that x ∼ U x -1 2 , x + 1 2 .

## Proof. Let

$Z = 1 h≤x + h. We have that Z ∼ U [x, 1 + x], i.e., f Z (z) = 1 if z ∈ [x, 1 + x] 0 Otherwise . This is because Pr[Z ≤ z] =        1 if z ≥ 1 + x z -p if z ∈ (x, 1 + x) 0 if z ≤ x .$Therefore,

$X + h -1/2 = Z -1/2 ∼ U [x -1/2, (1 + x) -1/2] .$This concludes the proof. ◀ ▶ Corollary 9. Our estimator is unbiased, i.e., E[ x] = x.

▶ Corollary 10. Our variance is constant for all x ∈ [0, 1] and satisfies Var[ x] = 1 12 .

Therefore, the potential extrema are x ∈ {1/2 -z, 1/2} and where E[cost] = 0, which gives x = 3+2z- √ 9-20z 2 6

. This yields

$cost =        1/12 -2z 2 + (16z 3 )/3 if x = 1/2 -z 4/3z 3 -2z 2 + 3/4z + 1/12 if x = 1/2 1/12 + z -2z 2 + 20/27 • z 3 + √ 9 -20z 2 • (-1/12 + 5/27 • z 2 ) if x = 3+2z- √ 9-20z 2 6 It follows that 1/12 + z -2z 2 + 20/27 • z 3 + 9 -20z 2 • (-1/12 + 5/27 • z 2 ) ≤ 4/3z 3 -2z 2 + 3/4z + 1/12$for all z ∈ [0, 0.5], and therefore we focus on x ∈ {0, 1/2 -z, 1/2}. Notice that min z∈[0,1/2] 1/12-2z 2 + (16z 3 )/3 = 1/24, which is achieved for z = 1/24.

Finally, by choosing the z value which minimizes max 2z(0.5

$-z) 2 + 1-z z (0.5 -t) 2 dt, (0.5 + z) • z 2 + 0.5 z t 2 dt ,$which is obtained for the z value that satisfies 1/24 + z 2 /2 + (2z 3 )/3 = 0 (z ≈ 0.17349), we get an expected worst-case squared error of ≈ 0.0602.

As a side note, one can obtain a slightly stronger bound by further truncating the estimations to e ± 1/2, where e = X + h -1/2. For example, if e = -0.4 then the algorithm should not estimate x ≈ 0.173 as x is guaranteed to be at most 0.1. Instead, the algorithm would estimate:

x = max(min(e, max(1 -z, e -1/2)), min(z, e + 1/2)).

In such a case, we can choose z ≈ 0.182 and get a cost of ≈ 0.05824. For simplicity, we omit the technical details. ◀

## J Convex-combination Biased Adaptation for Subtractive Dithering

Here, we analyze the algorithm in which Buffy sends (for a shared h ∼ U [0, 1])

$X = 1 if x ≥ h 0 otherwise ,$and Angel estimates, for some α ∈ [0, 1],

$x = α • h + (1 -α) • X.$Notice that We now explore how to reduce the cost using randomized thresholding, achieved through probabilistic multiplexing of the above algorithms. That is, Buffy and Angel will randomly select the executed protocol using the shared randomness, thus achieving implicit coordination.

$E[ x] = α/2 + (1 -α) • x. E[h 2 ] = 1/3. E[h • X] = x 0 tdt = x 2 /2.$To simplify the notation, we use A [0,1] to denote our general (i.e., x ∈ [0, 1]) with ℓ → ∞ (also given explicitly in Appendix L) algorithm, and A {0,1/2,1} to denote our algorithm for when x is guaranteed to be in {0, 1/2, 1}. Our observation is that A [0,1] , behaves differently than A {0,1/2,1} . Specifically, for the first, the expected squared error is maximized at {0, 1/2, 1}, while for the latter, the expected squared error is lower at these points. This suggests that by randomly choosing which of these algorithms to execute one can lower the cost.

In particular, we propose to multiplex between A [0,1] and A {0,1/2,1} as follows. With probability p, to be determined later, both Buffy and Angel use A [0,1] and otherwise A {0,1/2,1} , using the shared randomness to implicitly decide on the protocol. This means that the expected squared error becomes:

$E[( x -x) 2 ] = E[( x -x) 2 |running A [0,1] ] • p + E[( x -x) 2 |running A {0,1/2,1} ] • (1 -p).$We get that the cost, optimized for p = ϕ -1, is 6

$√ 10+11 √ 5-18 √ 2-17 24 ≈ 0.04644 (obtained for x ∈ {0, 1} ∪ 1 2 √ 2 , 1 -1 2 √ 2$). Notice that the cost of the algorithm is within 3.01% from the lower bound in Section 4.

Figure [4](#fig_4) illustrates how the non-hybrid (Section 6.2.2) ℓ = 3 algorithm has a lower cost than that of ℓ → ∞. However, as its worst-case expected squared error is not at x = 1/2, it does not multiplex as well with A {0,1/2,1} . Specifically, a hybrid algorithm that uses ℓ = 3 bits instead of the limit (ℓ → ∞) algorithm results in a higher cost of 102/361 -1/(3 √ 2) ≈ 0.04685 (which is obtained for p = 2/3). The above hybrid algorithms use unbounded shared randomness. In cases where we wish to use a small number of shared bits, we can approximate the better algorithm (that uses ℓ → ∞ for A [0,1] ); below we give a couple of examples.

Example I: Consider using ℓ = 4 bits. In this case, we use p = 3/4 and use the 2-bit algorithm from Section 6.2.2 as A [0,1] . The cost is then 1049-169 ), which improves over the 3-bit algorithm.

Example II: Consider using one random byte (ℓ = 8). In that case, we use p = 11/16 together with the 4-bit algorithm. The cost then becomes

1830635-1232945 √ 2 1858592 ≈ 0.04680 (obtained for x ∈ 109+6 √ 2 241 , 132-6 √ 2 241

). This further improves over the cost of Example I, over the best hybrid solution with ℓ = 3 (that uses unbounded randomness to represent the p = 2/3 value), and is within 1% of the unbounded shared randomness algorithm.

## N

Analysis of the Linear Sigmoid (Section 6.2.3)

First, we have (see Section 6.2) that the estimate function for X = 1 is:

$Z 1 (h) = 1 -Z 0 (1 -h) =        2α if h < h 0 2α + (1 -2α) • h-h0 1-2h0 if h ∈ [h 0 , 1 -h 0 ]$1 otherwise .

For x ∈ [α, 1 -α], denote by T -1 (x) = (1-2h0)(x-α)

## 1-2α

+ h 0 the value such that T (T -1 (x)) = x. We proceed with computing the expectation:

$E[ x|x < α]( =⇒ X = 0) = h 0 • (1 -2α) + 1-h0 h0 (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 -α E[ x|x > 1 -α]( =⇒ X = 1) = h 0 • (1 + 2α) + 1-h0 h0 2α + (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 + α E[ x|x ∈ [α, 1 -α]] = h 0 • (2α) + h 0 (1 -2α) + T -1 (x) h0 2α + (1 -2α) • h -h 0 1 -2h 0 dh + 1-h0 T -1 (x) (1 -2α) • h -h 0 1 -2h 0 dh = h 0 + 2α(T -1 (x) -h 0 ) + 1-h0 h0 (1 -2α) • h -h 0 1 -2h 0 dh = 1/2 + (-1 + 2h 0 )α + 2α(T -1 (x) -h 0 ) = 1/2 -α + 2αT -1 (x)$Therefore, we have:

$E[ x] =        1/2 -α if x < α 1/2 -α + 2α (1-2h0)(x-α) 1-2α + h 0 if x ∈ [α, 1 -α]$1/2 + α otherwise .

Next, we calculate the second moment of the estimate:

$E[( x) 2 |x < α]( =⇒ X = 0) = h 0 • (1 -2α) 2 + 1-h0h0$(1 -2α) • h -h 0 1 -2h 0 2 dh = 1/3 + h 0 /3 -4α/3 -4h 0 α/3 + 4α 2 /3 + 4h 0 α 2 /3

![Y (0.5) • 0.75 • (-0.25) + (1 -Y (0.5)) • 0.25 • (-0.75) + 0.25 = 0.25 -3/16 = 1/16.]()

![Figure4An illustration of the expected squared errors that motivate our choice of creating a hybrid of the optimal {0, 1/2, 1} and the biased ℓ → ∞ algorithms.]()

Other deterministic algorithms, e.g., that send 0 despite having |x -v0| > |x -v1|, can trivially be improved by an algorithm with the above form.

Notice that it is different than the value used for the x ∈ {0, 1/2, 1} case.

|x = 0.5] = E (1 X=0 • Z 0 + 1 X=1 • Z 1 ) 2 |x = 0.5

If for some h, A0(h) > A1(h), there exists an equivalent algorithm that replaces the role of X = 0 and X = 1 for this specific h.

