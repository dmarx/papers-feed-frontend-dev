<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating the Entropy of Linguistic Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-05">5 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aryaman</forename><surname>Arora</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgetown University ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
							<email>clara.meister@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgetown University ETH Zürich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<email>ryan.cotterell@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgetown University ETH Zürich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating the Entropy of Linguistic Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-05">5 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">5FCAA3AA8D61C58F8959791BC27A5FE5</idno>
					<idno type="arXiv">arXiv:2204.01469v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. Finally, we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is a natural connection between information theory, the mathematical study of communication systems, and linguistics, the study of human language-the primary vehicle that humans employ to communicate. Researchers have exploited this connection since information theory's inception <ref type="bibr" target="#b37">(Shannon, 1951;</ref><ref type="bibr" target="#b7">Cherry et al., 1953;</ref><ref type="bibr" target="#b17">Harris, 1991)</ref>. With the advent of modern computing, the number of information-theoretic linguistic studies has risen, exploring claims about language such as the optimality of the lexicon <ref type="bibr" target="#b30">(Piantadosi et al., 2011;</ref><ref type="bibr" target="#b31">Pimentel et al., 2021)</ref>, the complexity of morphological systems <ref type="bibr" target="#b8">(Cotterell et al., 2019;</ref><ref type="bibr" target="#b49">Wu et al., 2019;</ref><ref type="bibr" target="#b35">Rathi et al., 2021)</ref>, and the correlation between surprisal and language processing time <ref type="bibr" target="#b38">(Smith and Levy, 2013;</ref><ref type="bibr" target="#b3">Bentz et al., 2017;</ref><ref type="bibr" target="#b15">Goodkind and Bicknell, 2018;</ref><ref type="bibr" target="#b9">Cotterell et al., 2018;</ref><ref type="bibr">Meister et al., 2021, inter alia)</ref>. Minima in all the graphs indicate sign changes in the error of the estimate, from an under-to an over-estimate.</p><p>In information-theoretic linguistics, a fundamental quantity of research interest is entropy. Entropy is both useful to linguists in its own right, and is necessary for estimating other useful quantities, e.g., mutual information. However, the estimation of entropy from raw data can be quite challenging <ref type="bibr" target="#b29">(Paninski, 2003;</ref><ref type="bibr" target="#b28">Nowozin, 2015)</ref>, e.g., in expectation, the plug-in estimator underestimates entropy <ref type="bibr" target="#b24">(Miller, 1955)</ref>. Linguistic distributions often present additional challenges. For instance, many linguistic distributions, such as the unigram distribution, follow a power law <ref type="bibr" target="#b52">(Zipf, 1935;</ref><ref type="bibr" target="#b25">Mitzenmacher, 2004</ref>).<ref type="foot" target="#foot_0">foot_0</ref> Linguistics is not the only field with such nuances, and so a large number of entropy estimators have been proposed in other fields <ref type="bibr" target="#b6">(Chao and Shen, 2003;</ref><ref type="bibr">Archer et al., 2014, inter alia)</ref>. However, no work to date has attempted a practical comparison of these estimators on natural language data. This work fills this empirical void.</p><p>Our paper offers a large empirical comparison of the performance of 6 different entropy estimators on both synthetic and natural language data, an example of which is shown in Figure <ref type="figure">1</ref>. We find that <ref type="bibr" target="#b6">Chao and Shen's (2003)</ref> is the best estimator when very few data are available, but Nemenman et al. <ref type="bibr">'s (2002)</ref> is superior as more data become available. Both are significantly better (in terms of meansquared error) than the naïve plug-in estimator. Importantly, we also show that two recent studies <ref type="bibr" target="#b46">(Williams et al., 2021;</ref><ref type="bibr" target="#b21">McCarthy et al., 2020)</ref> show smaller effect sizes when a better estimator is employed; however, we are able to reproduce a significant effect in both replications. We recommend that future studies carefully consider their choice of entropy estimators, taking into account data availability and the nature of the underlying distribution.<ref type="foot" target="#foot_1">foot_1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Entropy and Language</head><p>Shannon entropy is a quantification of the uncertainty in a random variable. Given a (discrete) random variable X with probability distribution p over K possible outcomes X = {x k } K k=1 , the Shannon entropy of X is defined as</p><formula xml:id="formula_0">H(X) = H(p) def = - K k=1 p(x k ) log p(x k ) (1)</formula><p>Entropy has many uses throughout science and engineering; for instance, <ref type="bibr" target="#b36">Shannon (1948)</ref> originally proposed entropy as a lower bound on the compressibility of a stochastic source.</p><p>Yet the application of information-theoretic techniques to linguistics is not so straightforward: Information-theoretic measures are defined over probability distributions and, in the study of natural language, we typically only have access to samples from the distribution of interest, e.g., the phonotactic distribution in English, which permits word we cannot find in a corpus, like blick, rather than the true probabilities required in the computation of Eq. (1). Indeed, it is often the case that not all elements of X are even observed in available data-such as words that were coined after the a corpus was collected.</p><p>Rather, p must be approximated in order to estimate H(p). One solution is plug-in estimation: Given samples from p, the maximum-likelihood estimate for p is "plugged" into Eq. (1). However, as originally noted by <ref type="bibr" target="#b24">Miller (1955)</ref>, this strategy generally yields poor estimates. <ref type="foot" target="#foot_2">3</ref> It is thus necessary to derive more nuanced estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Statistical Estimation Theory</head><p>Statistical estimation theory provides us with the tools for estimating various quantities of interest based on samples from a distribution.</p><p>Central to this theory is the estimator: A statistic that approximates a property of the distribution our data is drawn from. More formally, let</p><formula xml:id="formula_1">D = { x (n) } N</formula><p>n=1 be samples from an unknown distribution p. Suppose we are interested in a quantity θ that can be computed as a function of the distribution p. An estimator θ(D) for θ is then a function of the data D that provides an approximation of θ.</p><p>Two properties of an estimator are often of interest: bias-the difference between the true value of θ and the expected value of our estimator θ(D) under p-and variance-how much θ(D) fluctuates from sample set to sample set:</p><formula xml:id="formula_2">bias( θ(D)) def = E p [ θ(D)] -θ (2) var( θ(D)) def = E p [( θ(D) -E p [ θ(D)]) 2 ]<label>(3)</label></formula><p>It is desirable to construct an estimator that has both low bias and low variance. However, the bias-variance trade-off tells us that we often have to pick one, and we should focus on a balance between the two. This trade-off is evinced through mean-squared error (MSE), a metric oft-employed for assessing estimator quality:</p><formula xml:id="formula_3">MSE( θ(D)) = bias( θ(D)) 2 + var( θ(D)) (4)</formula><p>To recognize the trade-oft note that, for any fixed MSE, a decrease in bias must be compensated with an increase in variance and vice versa. Indeed, it is important to recognize that there is typically no single estimator that is seen as "best." Different estimators balance the bias-variance trade-off differently, making their perceived quality specific to one's use-case. Importantly, the effectiveness of an estimator also depends on the domain of interest. Consequently, an empirical study of various entropy estimators, which this paper provides, is necessary in order to determine which entropy estimators are best suited for linguistic distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Plug-in Estimation of Entropy</head><p>A simple, two-step approach for estimating entropy is plug-in estimation. In the first step, we compute the maximum-likelihood estimate for p from our dataset D as follows</p><formula xml:id="formula_4">p MLE (x k ) def = N n=1 1{ x (n) = x k } N (5)</formula><p>In the second step, we plug Eq. ( <ref type="formula">5</ref>) into Eq. ( <ref type="formula">1</ref>) directly, which results in the estimator H MLE (D). So why is this a bad idea? While our probability estimates themselves are unbiased, entropy is a concave function. Consequently, by Jensen's inequality, this estimator is, in expectation, a lower bound on the true entropy (see App. E.1 for proof). Moreover, when N K, which is often the case in power-law distributed data, the estimate becomes quite unreliable <ref type="bibr" target="#b26">(Nemenman et al., 2002)</ref>. <ref type="bibr" target="#b24">-Miller (1955) and</ref><ref type="bibr" target="#b19">Madow (1948)</ref>. The first innovation in entropy estimation known to the authors is a simple fix derived from a first-order Taylor expansion of MLE (described above). The Miller-Madow estimator only involves a simple additive correction, which is shown below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Ensemble of Entropy Estimators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MM</head><formula xml:id="formula_5">H MM (D) def = H MLE (D) + K -1 2N (6)</formula><p>where K is size of the support of X . The Miller-Madow correction should seem intuitive in that we add K-1 2N ≥ 0 to compensate for the negative bias of the estimator. A full derivation of the Miller-Madow estimator is given in Proposition 2. <ref type="bibr">JACK-Zahl (1977)</ref>. Next we consider the jackknife, which is a common strategy used to correct for the bias of statistical estimators. In the case of entropy estimation, we can apply the jackknife out of the box to correct the bias inherent in the MLE estimator. Explicitly, this is done by averaging plug-in entropy estimates H MLE (D) albeit with the n th sample from the data removed; we denote this held-out plug-in estimator as H \n MLE (D). Averaging these "held-out" plug-in estimators results in the following simple entropy estimator</p><formula xml:id="formula_6">H JACK (D) def = N H MLE (D) - N -1 N N n=1 H \n MLE (D)<label>(7</label></formula><p>) Note that the jackknife is applicable to any estimator, not just H MLE (D), and, thus, can be combined with any of the other approaches mentioned.</p><p>HT- <ref type="bibr" target="#b18">Horvitz and Thompson (1952)</ref>. Horvitz-Thompson is a general scheme for building estimators that employs importance weighting in order to more efficiently estimate a function of a random variable. Importantly, this estimator gives us the ability to compensate for situations where the probability of an outcome is so low that it is often not observed in a sample, which is often the case for e.g., power-law distributions.</p><p>While a full exposition of HT estimators is outside of the scope of this work, in essence, we can divide the expected probability of a class by each class's estimated inclusion probability to compensate for such situations. Given the true probability of an outcome p(x k ), the probability that it occurs at least once in a sample of size N is 1 -(1p(x k )) N . The HT estimator for entropy is then defined as</p><formula xml:id="formula_7">H HT (D) def = - K k=1 p MLE (x k ) log p MLE (x k ) 1 -(1 -p MLE (x k )) N (8)</formula><p>using our MLE probability estimates p MLE (x k ).</p><p>CS- <ref type="bibr" target="#b6">Chao and Shen (2003)</ref>. Chao-Shen modifies HT by multiplying the MLE probability estimates by an estimate of sample coverage. Formally, let f 1 be the number of observed singletons<ref type="foot" target="#foot_3">foot_3</ref> in sample; our sample coverage can be estimated as</p><formula xml:id="formula_8">C = 1 -f 1 N .</formula><p>The CS estimator is then computed as:</p><formula xml:id="formula_9">H CS (D) def = - K k=1 C • p MLE (x k ) log C • p MLE (x k ) 1 -(1 -C • p MLE (x k )) N<label>(</label></formula><p>9) In the case that f 1 = N , we set f 1 = N -1 to ensure the estimated entropy is not 0.</p><p>WW- <ref type="bibr" target="#b47">Wolpert and Wolf (1995)</ref>. One family of entropy estimators in information theory is based on Bayesian principles. The first of these was the Wolpert-Wolf estimator, which uses a Dirichlet prior (with concentration parameter α and a uniform base distribution). This Bayesian estimator has a clean, closed form: Laplace smoothing), A = K k=1 α k , and ψ is the digamma function. A full derivation of Eq. ( <ref type="formula">10</ref>) is given in Proposition 3. Unfortunately, Eq. ( <ref type="formula">10</ref>) is very dependent on the choice of α: For large K, α almost completely determines the final entropy estimate, an observation first made by <ref type="bibr" target="#b26">Nemenman et al. (2002)</ref> which motivated their improved estimator described below. <ref type="bibr" target="#b26">-Nemenman et al. (2002)</ref>. Nemenman et al. (NSB) attempt to alleviate the Wolpert-Wolf estimator's dependence on α. They take α = α • 1, enforcing that the Dirichlet prior is symmetric, and develop a hyperprior over α that results in a nearuniform distribution over entropy. The hyperprior is given by</p><formula xml:id="formula_10">H WW (D | α) def = ψ A + 1 - K k=1 α k A ψ( α k + 1)<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NSB</head><formula xml:id="formula_11">p NSB (α) def = Kψ 1 (Kα + 1) -ψ 1 (α + 1) log K (11)</formula><p>where ψ 1 is the trigamma function. A full derivation of Eq. ( <ref type="formula">11</ref>) is given in Proposition 4. This choice of hyperprior mitigates the effect that the chosen α has on the entropy estimate. <ref type="bibr">Nemenman et al.'s (2002)</ref> entropy estimator is then the posterior mean of the Wolpert-Wolft estimator taken under p NSB :</p><formula xml:id="formula_12">H NSB (D) = ∞ 0 H WW (D | α • 1) p NSB (α) dα<label>(</label></formula><p>12) Typically, numerical integration is used to quickly compute the unidimensional integral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here we provide an evaluation of the entropy estimators presented in §3.2 on linguistic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Entropy of the Unigram Distribution</head><p>We start our study with a controlled experiment where we estimate the entropy of the truncated unigram distribution, the (finite) distribution over the frequent word tokens in a language without regard to context <ref type="bibr" target="#b2">(Baayen et al., 2016;</ref><ref type="bibr" target="#b10">Diessel, 2017;</ref><ref type="bibr" target="#b11">Divjak, 2019;</ref><ref type="bibr" target="#b27">Nikkarinen et al., 2021)</ref>. We renormalize the frequency counts of corpora in English, German, and Dutch (taken from CELEX; <ref type="bibr" target="#b1">Baayen et al., 1995)</ref>, as well as Mongolian and Tagalog (from Wikipedia<ref type="foot" target="#foot_4">foot_4</ref> ). We take this renormalization as a gold standard distribution, since we cannot access the underlying unigram distribution. We then draw samples of varying sizes (N ∈ {10 2 , 10 3 , 10 4 , 10 5 }) from the distribution of renormalized frequency counts to test the estimators' ability to recover the underlying distributions' entropy. While the renormalized frequency counts are not necessarily representative of the true unigram distribution, they nevertheless provide us with a controlled setting to benchmark various entropy estimators.</p><p>We evaluate the estimators on both bias and MSE, as defined in ( <ref type="formula">2</ref>) and (4), as well as mean absolute bias (MAB). To test the statistical significance of differences in metrics between entropy estimators, we use paired permutation tests <ref type="bibr" target="#b14">(Good, 2000)</ref> (sampling 1, 000 permutations) between pairs of estimators, checking MAB and MSE. We run <ref type="bibr">Tukey's test (1949)</ref> to judge the statistical significance of differences in MAB and MSE between all pairs of estimators, which found only a few insignificant comparisons when N was large.</p><p>Results are shown in Table <ref type="table" target="#tab_0">1</ref> and <ref type="table" target="#tab_0">Figure 1</ref>. We find that NSB (followed closely by CS) converges almost to the true entropy from below using with only a few samples. HT is the best estimator for N &lt; 2, 000, but as N increases it tends to overestimate entropy to the point where its bias is greater than that of MLE. Besides HT, all estimators at all tested sample sizes N have lower MAB and MSE than MLE.</p><p>Language n MLE CS MM JACK WW NSB Italian 16, 856 20.00% 15.56% 16.43% 14.09% 19.67% 11.41% Polish 15, 525 30.52% 23.48% 25.49% 21.75% 34.68% 17.07% Portuguese 7, 409 27.60% 20.76% 22.51% 18.81% 33.32% 14.18% Spanish 21, 408 20.50% 15.17% 16.44% 13.80% 21.04% 10.50% Arabic 2, 483 45.31% 38.49% 40.99% 37.93% 49.09% 34.82% Croatian 13, 856 31.35% 26.04% 26.62% 23.08% 35.66% 19.06% Greek 3, 305 41.58% 33.17% 36.39% 32.32% 48.80% 27.00%  <ref type="bibr" target="#b4">(Bond and Foster, 2013)</ref>. We rerun their experimental set-up using our full suite of entropy estimators to determine whether the relationship they posit remains significant, checking 3 more languages not in the original study.</p><p>We report results for normalized mutual information (dividing MI by maximum possible MI) in Table <ref type="table" target="#tab_2">2</ref>. We find that using NSB (the estimator we found most effective in §4.1) instead of MLE, nearly halves the measured effect in all languages. However, the effect remains statistically significant in 5 of 7 languages tested, including the 4 that were also in the original study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Replication of McCarthy et al. (2020)</head><p>Finally, we turn our attention to <ref type="bibr">McCarthy et al.'s (2020)</ref> study on the similarity between grammatical gender partitions between languages. Using information-theoretic measures, they found that closely related languages have more similar gender groupings of core lexical items. We replicate their experiment on Swadesh lists <ref type="bibr" target="#b40">(Swadesh, 1955)</ref> for 10 European languages with different estimators, and find that hierarchical clustering over both mutual (MI) and variational information (VI) produces the same trees as the original study. In this case, using NSB, our recommended estimator, results in a reduced estimate of MI (e.g. Croatian-Slovak: 0.54 with MLE → 0.46 with NSB), but significance test-ing with 1,000 permutations finds the same pairs were statistically significant for both MI and VI regardless of estimator: all pairs of Slavic languages and Romance languages, and Bulgarian-Spanish (see Figure <ref type="figure">2</ref>). Thus, we see a similar result here as in the previous replication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work presents the first empirical study comparing the performance of various entropy estimators for use with natural language distributions. From experiments on synthetic data (appendix) and natural data (CELEX), and two replication studies of recent papers in information-theoretic linguistics, we find that the oft-employed plug-in estimator of entropy can cause misleading results, e.g., the overestimates of effect sizes seen in both replication studies. The recommendation of our paper is that researchers should carefully consider their choice of entropy estimator based on data availability and the nature of the underlying distribution.</p><p>MAB MSE 10 1 10 2 10 3 10 4 10 1 10 2 10 3 10 4 2 HT WW WW WW WW WW WW JACK 5 MM WW WW JACK MM WW WW MM 10 JACK CS WW MM JACK WW WW MLE 100 CS CS JACK WW CS JACK JACK WW 1000 CS HT CS JACK CS HT CS JACK Table 4: Estimators with least MAB (mean absolute bias) and MSE (mean squared error) for various combinations of N and K sampling from Zipfian distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation</head><p>The code for each of the entropy estimators is implemented in Python using numpy <ref type="bibr" target="#b16">(Harris et al., 2020)</ref>, except for NSB which was taken from an existing efficient implementation in the ndd module <ref type="bibr" target="#b20">(Marsili, 2016)</ref>. We calculated entropies with base e (in nats).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments with simulated data</head><p>In our experiments with simulated data, we explore distributions sampled from a symmetric Dirichlet prior with varying number of classes K and known distributions of Zipfian form with various parameters.</p><p>Words in natural languages have a roughly Zipfian distribution, with probability inversely proportional to rank <ref type="bibr" target="#b52">(Zipf, 1935)</ref>, and a symmetric Dirichlet distribution is analogous to e.g. POS tag label distributions in natural language. Thus, studying synthetic data from such distributions as a start is useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Experiment 1: Symmetric Dirichlet distributions</head><p>We sample 1, 000 distributions from a symmetric Dirichlet distribution with variable number of classes K, i.e. with paramater α = [α 1 , . . . , α K ] = [1, . . . , 1]. We calculate entropy estimates on different sample sizes N . Since we know the parameters of the true distribution, we can compare estimates with the true entropy. We do pairwise comparisons of the MAB and MSE of estimators, using paired permutation tests to establish significance. Table <ref type="table" target="#tab_4">3</ref> shows our results, including significance tests. It is clear that when N K, all of the estimators have nearly converged to the true value and estimator choice does not matter. However, in the low-sample regime some estimators are indeed significantly better at approximating the true entropy. Our results are mixed as to which estimator is best in what context; the one found to be most frequently significantly better than other estimators was Chao-Shen. What is clear is that MLE is never the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Experiment 2: Zipfian distributions</head><p>We sample 1, 000 finite Zipfian distributions with K classes which obey Zipf's law, that the probability of an outcome is inverse proportional to its rank. The experimental setup is the same as in Experiment 1. A Zipfian distribution approximates (but is not a perfect model of) the distribution of tokens in natural language text in some languages, including English, which was the basis for the law being proposed. Compare similar experiments on infinite Zipf distributions by <ref type="bibr" target="#b51">Zhang (2012)</ref>. Results are in Table <ref type="table">4</ref>. <ref type="bibr" target="#b46">Williams et al. (2021)</ref> We used the following UD treebanks:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Replication of</head><p>• Arabic: PADT <ref type="bibr" target="#b39">(Smrž et al., 2008;</ref><ref type="bibr" target="#b41">Taji et al., 2017)</ref>; • Greek: GDT <ref type="bibr" target="#b32">(Prokopidis et al., 2005;</ref><ref type="bibr">Prokopidis and Papageorgiou, 2017)</ref>; • Italian: ISDT <ref type="bibr" target="#b5">(Bosco et al., 2013)</ref>, VIT <ref type="bibr" target="#b43">(Tonelli et al., 2008)</ref>; • Polish: PDB (Wróblewska, 2018); • Portuguese: GSD (McDonald et al., 2013), Bosque (Rademaker et al., 2017); • Spanish: AnCora (Taulé et al., 2008), GSD (McDonald et al., 2013). D Additional Figures hr sk ru bg uk fr ca it es pt Lang2 CS MI 0 0.1 0.2 0.3 0.4 0.5 HT JACK hr sk ru bg uk fr ca it es pt MLE hr sk ru bg uk fr ca it es pt hr sk ru bg uk fr ca it es pt MM hr sk ru bg uk fr ca it es pt NSB hr sk ru bg uk fr ca it es pt Lang1 WW Figure 2: Mutual information between the gender partitions of language pairs with various estimators, replicating McCarthy et al. (2020). 50 100 150 200 -1 0 1 2 Bias (nats) CS 50 100 150 200 HT 50 100 150 200 JACK 50 100 150 200 MLE 50 100 150 200 MM 50 100 150 200 NSB 50 100 150 200 Samples WW Estimator bias Figure 3: The distribution of bias for entropy over several estimators given variable sample size N , sampling from 100 distributions taken from a symmetric Dirichlet prior with K = 100. CS HT JACK MLE MM NSB WW Estimator 2 K: 2 MAB 0.25 0.50 0.75 K: 5 K: 10 K: 100 K: 1000 N: 10 CS HT JACK MLE MM NSB WW N: 100 CS HT JACK MLE MM NSB WW N: 1000 CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW Estimator 1 N: 10000 Pairwise MAB p-values (Dirichlet) CS HT JACK MLE MM NSB WW Estimator 2 K: 2 MSE 0.25 0.50 0.75 K: 5 K: 10 K: 100 K: 1000 N: 10 CS HT JACK MLE MM NSB WW N: 100 CS HT JACK MLE MM NSB WW N: 1000 CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW Estimator 1 N: 10000</p><p>Pairwise MSE p-values (Dirichlet)</p><p>Figure <ref type="figure" target="#fig_2">4</ref>: The heatmaps display the p-values calculated between pairs of estimators for mean absolute bias (MAB) and mean squared error (MSE) for Experiment 1. More purple values mean the estimator on the y-axis (Estimator 2) is better than the estimator on the x-axis (Estimator 1). Comparisons tend to become non-significant as N increases, since all the estimators gradually converge to the true entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Derivation of the Entropy Estimators</head><p>Let X = {x k } K k=1 be a finite set. Let p be a distribution over X . The entropy of p is defined as</p><formula xml:id="formula_13">H(p) def = - K k=1 p k log p k (13)</formula><p>Given a dataset of N samples D sampled i.i.d. from p, our goal is to estimate the entropy H(p) from samples D from the true distribution p. We will denote the count of an item n) . The maximum-likelihood estimate (MLE) of p given D is denoted</p><formula xml:id="formula_14">x k as c(x k ) = N n=1 1 x k = x (</formula><formula xml:id="formula_15">N n=1 1{ x (n) =x k } N</formula><p>. The plug-in estimate of H(p) is defined to be the estimate of H(p) obtained by plugging the MLE estimate p MLE directly into the definition of entropy, i.e.,</p><formula xml:id="formula_16">H MLE (D) = H( p MLE ) = - K k=1 p MLE (x k ) log p MLE (x k ) = - K k=1 c(x k ) N log c(x k ) N (<label>14</label></formula><formula xml:id="formula_17">)</formula><p>This section discusses the problems with Eq. ( <ref type="formula" target="#formula_16">14</ref>) as an estimator and provides detailed derivations of improved estimators found in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 The Plug-in Estimator is Negatively Biased</head><p>Proposition 1. The MLE entropy estimator in expectation underestimates true entropy, i.e.,</p><formula xml:id="formula_18">H MLE (D) = E K k=1 -p MLE (x k ) log p MLE (x k ) ≤ H(p)<label>(15)</label></formula><p>Proof. The result is a simple consequence of Jensen's inequality and some basic manipulations:</p><formula xml:id="formula_19">E K k=1 -p MLE (x k ) log p MLE (x k ) = K k=1 E[-p MLE (x k ) log p MLE (x k )] (linearity of expectation) ≤ - K k=1 E[ p MLE (x k )] log E[ p MLE (x k )] (Jensen's inequality) = - K k=1 p(x k ) log p(x k ) (E[ p MLE (x k )] = p(x k )) = H(p) (definition of entropy)</formula><p>This completes the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Miller-Madow</head><p>Proposition 2. Let p be a categorical distribution over X = {x 1 , . . . , x K }, i.e., a categorical distribution with support K. Let D be our dataset of size N sampled from p. Finally, let p MLE be the maximumlikelihood estimate computed on D. Then, we have</p><formula xml:id="formula_20">bias H MLE (D) def = E p H MLE (D) -H(p) (16) = - K -1 2N + o N -1 (17)</formula><p>Proof. We start by taking a first-order Taylor expansion and take an expectation of both sides.</p><formula xml:id="formula_21">H MLE (D) = H( p MLE , p) cross-entropy -KL( p MLE || p) (Lemma 1) (18) E p H MLE (D) = E p [H( p MLE , p)] -E p [KL( p MLE || p)] (expectation) (19) = E p - K k=1 p MLE (x k ) log p(x k ) -E p [KL( p MLE || p)] (defn. H(p, q)) (20) = - K k=1 E p [ p MLE (x k ) log p(x k )] -E p [KL( p MLE || p)] (linearity) (21) = - K k=1 E p [ p MLE (x k )] log p(x k ) -E p [KL( p MLE || p)] (algebra) (22) = - K k=1 p(x k ) log p(x k ) -E p [KL( p MLE || p)] (unbiased) (23) = H(p) -E p [KL( p MLE || p)] (defn. of H(p))<label>(24)</label></formula><p>(25)</p><p>This gives us:</p><formula xml:id="formula_22">E p H MLE (D) -H(p) = -E p [KL( p MLE || p)] (subtract H(p))<label>(26)</label></formula><p>Thus, we may compactly write the bias as:</p><formula xml:id="formula_23">bias H MLE (D) = E p [H( p MLE )] -H(p) (definition of bias) (27) = -E p [KL( p MLE || p)] (above computation) (28) ≤ 0 (non-negativity of KL)<label>(29)</label></formula><p>Now, we find a simpler expression for the remainder E p [KL( p MLE || p)]. Again, we start with a secondorder Taylor expansion</p><formula xml:id="formula_24">KL(p || q) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (Lemma 2) (<label>30</label></formula><formula xml:id="formula_25">)</formula><p>around the point ∆(x) = p(x)q(x). Define p MLE (x k ) = c(x k ) N where c(x k ) is the count of x k in the training set. We now simplify the first term:</p><formula xml:id="formula_26">E p K k=1 ∆(x k ) 2 2q(x k ) = E p K k=1 ( p MLE (x k ) -p(x k )) 2 2p(x k ) (definition of ∆(x k )) (31) = E p K k=1 ( c(x k ) N -p(x k )) 2 2p(x k ) (definition of MLE) (32) = E p K k=1 (c(x k ) -N p(x k )) 2 2N 2 p(x k ) (× N /N) (33) = 1 2N E p K k=1 (c(x k ) -N p(x k )) 2 N p(x k ) (pulling out 1 /2N) (34) = 1 2N E p     K k=1 c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N p(x k )     (exp. the binomial) (35) = 1 2N K k=1 E p c(x k ) 2 -2N p(x k )E p [c(x k )] + N 2 p(x k ) 2 N p(x k ) (lin. of expect.) (36) = 1 2N K k=1 N p k (1 -p(x k )) + N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N p(x k ) (moments of MLE) (37) = 1 2N K k=1 N p k (1 -p(x k )) N p(x k ) + 1 2N K k=1 N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N p(x k ) =0 (38) = 1 2N K k=1 $ $ $ $ N p(x k )(1 -p(x k )) $ $ $ $ N p(x k ) (39) = 1 2N K k=1 (1 -p(x k )) (algebra) (40) = 1 2N K k=1 1 =K - 1 2N K k=1 p(x k ) =1 (algebra) (41) = K -1 2N (42)</formula><p>Next, we simplify the second term, o ∆(x) 2 , in the MLE case:</p><formula xml:id="formula_27">E p o ∆(x) 2 = E p o ( p MLE (x k ) -p(x k )) 2 (definition of ∆) (43) = E p o c(x k ) N -p(x k ) 2 (definition of MLE) (44) = E p o (c(x k ) -N p(x k )) 2 N 2 (× N /N) (45) = E p o c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N 2 (46) = o E p c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N 2 (push exp. through) (47) = o     N p k (1 -p(x k )) + N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N 2     (48) = o N p(x k )(1 -p(x k )) N 2 (cancel terms) (49) = o p(x k )(1 -p(x k )) N (cancel N in fraction) (50) = o N -1 (ignore constants) (51)</formula><p>Putting it all together, we get that bias (H( p MLE )) = -K-1 2N + o N -1 which is the desired result.</p><p>Interestingly, it can be seen that the negative bias of the MLE gets worse as the number of classes K grows. Distributions with large K pop up frequently when dealing with natural language.</p><p>Corollary 1. The plug-in estimator of entropy is consistent.</p><p>Proof. From Proposition 2, we have bias (H( p MLE )) = -K-1 2N + o N -1 . Clearly, as N → 0, we have bias (H( p MLE )) → 0, so the estimator is consistent. One could also prove consistency through a simple application of the continuous mapping theorem.</p><p>Estimator 1 <ref type="bibr">(Miller-Madow)</ref>. Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then, the Miller-Madow estimator of H(p) is given by</p><formula xml:id="formula_28">H MM (D) def = H MLE (D) + K -1 2N<label>(52)</label></formula><p>The Miller-Madow estimator is biased, however it is consistent.</p><p>Lemma 1. The the first-order Taylor approximation of H MLE (D) around the distribution p is given by</p><formula xml:id="formula_29">H MLE (D) = H( p MLE , p) + R(p, p MLE )<label>(53)</label></formula><p>where the remainder R is given by</p><formula xml:id="formula_30">R(p, p MLE ) = -KL( p MLE || p)<label>(54)</label></formula><p>Proof. The result follows from direct computation. We start by taking the Taylor expansion of H( p MLE ) around H(p):</p><formula xml:id="formula_31">H MLE (D) = H(p) + K k=1 ∂ ∂p(x k ) H(p) p MLE (x k ) -p(x k ) + R(p, p MLE ) remainder<label>(55)</label></formula><p>Our first order term can then be rewritten as follows:</p><formula xml:id="formula_32">K k=1 ∂ ∂p(x k ) H(p) p MLE (x k ) -p(x k ) (56) = K k=1 ∂ ∂p(x k ) K k =1 -p(x k ) log p(x k ) p MLE (x k ) -p(x k ) (57) = K k=1 K k =1 - ∂ ∂p(x k ) p(x k ) log p(x k ) p MLE (x k ) -p(x k ) (linearity) (58) = K k=1 K k =1 ∂ ∂p(x k ) p(x k ) log p(x k ) p(x k ) -p MLE (x k ) (sign) (59) = K k=1 1 + log p(x k ) p(x k ) -p MLE (x k ) (60) = K k=1 p(x k ) -p MLE (x k ) + log p(x k ) (p(x k ) -p MLE (x k )) (61) = K k=1 p(x k ) -p MLE (x k ) + K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (62) = K k=1 p(x k ) =1 - K k=1 p MLE (x k ) =1 + K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (distrib. sum) (63) = K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (simplify) (64) = K k=1 log p(x k )p(x k ) -H(p) - K k=1 log p(x k ) p MLE (x k ) H(p, pMLE) (distrib. sum) (65) = H(p, p MLE ) -H(p)<label>(66)</label></formula><p>Plugging this back into our Taylor expansion, we get the following:</p><formula xml:id="formula_33">H MLE (D) = ¨Ḧ (p) - ¨Ḧ (p) + H(p, p MLE ) + R(p, p MLE )<label>(67)</label></formula><p>Now, we see that this implies</p><formula xml:id="formula_34">R(p, p MLE ) = H MLE (D) -H( p MLE , p) (algebra) (68) = - K k=1 p MLE (x k ) log p MLE (x k ) + K k=1 p MLE (x k ) log p(x k ) (defn.) (69) = - K k=1 ( p MLE (x k ) log p MLE (x k ) -p MLE (x k ) log p(x k )) (merge sums) (70) = - K k=1 p MLE (x k )(log p MLE (x k ) -log p(x k )) (factor out p MLE (x k )) (71) = - K k=1 p MLE (x k ) log p MLE (x k ) p(x k ) (log algebra) (72) = -KL( p MLE || p) (defn.)<label>(73)</label></formula><p>which is the desired result.</p><p>Lemma 2. Define ∆(x) = p(x)q(x). The second-order Taylor expansion of KL(p || q) around ∆(x) is given by</p><formula xml:id="formula_35">KL(p || q) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2<label>(74)</label></formula><p>Proof. Now we compute the series expansion of the KL-divergence. We first make a tricky substitution:</p><formula xml:id="formula_36">p(x) q(x) = q(x) + p(x) -q(x) q(x) = 1 + p(x) -q(x) q(x) = 1 + ∆(x) q(x)<label>(75)</label></formula><p>Now, we proceed with the derivation:</p><formula xml:id="formula_37">KL(p || q) = x∈X p(x) log p(x) q(x) (defn. of KL divergence) (76) = x∈X (q(x) + ∆(x)) log 1 + ∆(x) q(x) (Eq. (75)) (77) = x∈X (q(x) + ∆(x)) ∆(x) q(x) - ∆(x) 2 2q(x) 2 + o ∆(x) 2 (Taylor expansion) (78) = x∈X ∆(x) - ∆(x) 2 2q(x) + ∆(x) 2 q(x) - ∆(x) 3 2q(x) 2 + o ∆(x) 2 (distribute) (79) = x∈X ∆(x) - ∆(x) 2 2q(x) + ∆(x) 2 q(x) + o ∆(x) 2 (defn. of o) (80) = x∈X ∆(x) + ∆(x) 2 2q(x) + o ∆(x) 2 (algebra) (81) = x∈X ∆(x) =0 + x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (split sums) (82) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (83)</formula><p>which is the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Jackknife</head><p>The jackknife resampling method is used to estimate the bias of an estimator and correct for it, by sampling all subsamples of size N -1 from the available sample of size N , computing their average for the statistic being estimated. Generally, this reduces the order of the bias of an estimator from O(N -1 ) to at most O(N -2 ) <ref type="bibr" target="#b12">(Friedl and Stampfer, 2002)</ref>.</p><p>Estimator 2 (Jackknife). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Let H \n (D) be an estimate of the entropy from a sample with the n th observation held out. Then, the Jackknife estimator is given by</p><formula xml:id="formula_38">H JACK (D) def = N H MLE (D) - N -1 N N n=1 H \n MLE (D) (<label>84</label></formula><formula xml:id="formula_39">)</formula><p>This estimator is derived from the jackknife-resampled estimate of the bias of the MLE estimator, multiplied by N -1.</p><formula xml:id="formula_40">H JACK (D) -H MLE (D) = (N -1) H MLE (D) - 1 N N n=1 H \n MLE (D)<label>(85)</label></formula><p>E.4 <ref type="bibr">Horvitz-Thompson Horvitz and Thompson (HT;</ref><ref type="bibr" target="#b18">1952)</ref> is a common estimator given a finite universe, which is our case as K is finite. We omit a derivation a full here as it is well documented in other places <ref type="bibr" target="#b45">(Vieira, 2017)</ref>. However, we note that, in contrast to many applications of HT, the application of HT to entropy estimation results in a biased estimator as the function whose mean we seek to estimate is log p(x k ), which is dependent on the unknown distribution p.</p><p>Estimator 3 <ref type="bibr">(Horvitz-Thompson)</ref>. Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then the Horvitz-Thompson estimator is defined as</p><formula xml:id="formula_41">H HT (D) def = - K k=1 p MLE (x k ) log p MLE (x k ) 1 -(1 -p MLE (x k )) N (<label>86</label></formula><formula xml:id="formula_42">)</formula><p>where 1 -(1p MLE (x k )) N is an estimate of the inclusion probability, i.e., the probability that x k appears in a random sample D of size N .</p><p>We do not know of a simple expression for the bias of the Horvitz-Thompson entropy estimator, but one observation is that E p (1p MLE (x k )) N &gt; E p (1p(x k )) N when N &gt; 1 (justified by Jensen's inequality, since x N , N &gt; 1 is convex over [0, 1]); this is an overestimate of the true inclusion probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Chao-Shen</head><p>The Chao-Shen estimator builds upon Horvitz-Thompson by noting that that estimator does not correct for underestimation of number of classes K and resulting effect on estimates of p(x k ); i.e. 1-(1-p MLE (x k )) N is always 0 for a class not included in the sample even if the class is present in the true distribution. We can reweight the sample probabilities to compensate for missing classes using the notion of sample coverage.</p><p>Definition 1 (Sample coverage). We define the sample coverage as</p><formula xml:id="formula_43">C = K k=1 p(x k )1 x k ∈ D (87)</formula><p>Definitionally, (1 -C) is then the probability of sampling an x k not observed in the sample X . However, exact computation of Eq. ( <ref type="formula" target="#formula_44">88</ref>) is impossible as we do not know the true distribution p. Thus, <ref type="bibr" target="#b6">Chao and Shen (2003)</ref> fall back on a well-known estimator of C that uses a technique from <ref type="bibr">Good-Turing (1953)</ref> smoothing. Let f 1 be the number of classes with only one observation in the current sample, i.e, the number of singletons, then we can estimate the sample coverage as</p><formula xml:id="formula_44">C def = 1 - f 1 N<label>(88)</label></formula><p>The Chao-Shen estimator, described below, simply re-scales the MLE estimate of probability p MLE (x k ) in the HT estimator by C. This corrects for the observed underestimation of p's entropy by HT.</p><p>Estimator 4 (Chao-Shen). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Let C, an estimate of sample coverage, be defined as in Eq. ( <ref type="formula" target="#formula_44">88</ref>). The Chao-Shen estimator is then defined as</p><formula xml:id="formula_45">H CS (D) def = - K k=1 C • p MLE (x k ) log ( C • p MLE (x k )) 1 -(1 -C • p MLE (x k )) N (89) E.6 Wolpert-Wolf Fact 1 (Derivative of an exponent). d da x a = x a log x<label>(90)</label></formula><p>Fact 2 (Normalizer of a Dirichlet). The normalizer of a Dirichlet distribution is</p><formula xml:id="formula_46">δ K k=1 x k -1 K k=1 x α k dx = K k=1 Γ(α k ) Γ K k=1 α k (91)</formula><p>A relatively easy proof of this fact makes use of a Laplace transform.</p><p>Estimator 5 <ref type="bibr">(Wolpert-Wolf)</ref>. Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then, the Wolpert-Wolf estimator is given by</p><formula xml:id="formula_47">H WW (D | α) def = ψ A + 1 - K k=1 α k A ψ( α k + 1)<label>(92)</label></formula><p>where </p><formula xml:id="formula_48">c(x k ) def = N n=1 1{ x n = x k },</formula><formula xml:id="formula_49">E [H(p) | α] def = H(p) δ K k=1 p(x k ) -1 Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k -1 dp (93) = ψ (A + 1) - K k=1 α k A ψ(α k + 1)<label>(94)</label></formula><p>where</p><formula xml:id="formula_50">A def = K k=1 α k .</formula><p>Proof. Let Dirichlet(α 1 , . . . , α K ) be a Dirichlet posterior. The result follows by a series of manipulations:</p><formula xml:id="formula_51">E [H(p) | α] = H(p) δ K k=1 p(x k ) -1 Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k -1 dp (defn.) (95) = Γ (A) k=1 Γ(α k ) H(p) δ K k=1 p(x k ) -1 K k=1 p(x k ) α k -1 dp (96) = Γ (A) k=1 Γ(α k ) - K k=1 p(x k ) log p(x k ) δ K k=1 p(x k ) -1 K k=1 p α k -1 k dp (defn. H) (97) = - Γ (A) k=1 Γ(α k ) K k=1 p(x k ) log p(x k )δ K k=1 p(x k ) -1 K k=1 p(x k ) α k -1 dp (linear.) (98) = - Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k log p(x k )δ K k=1 p(x k ) -1 K j=1, j =k p(x j ) α j -1 dp (algebra) (99) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k p(x k ) α k δ K k=1 p(x k ) -1 K j=1, j =k p(x j ) α j -1 dp (fact #1) (100) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k δ K k=1 p(x k ) -1 p(x k ) α k K j=1, j =k p(x j ) α j -1 dp (algebra) (101) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k δ K k=1 p(x k ) -1 p(x k ) α k K j=1, j =k p(x j ) α j -1 dp (102) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k Γ(α k + 1) K j=1, j =k Γ(α j ) Γ K j=1 α j + 1 (fact #2) (103) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) d dα k Γ(α k + 1) Γ K j=1 α j + 1 (104) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k + 1)Γ K j=1 α j + 1 Γ K j=1 α j + 1 2 (derivative) (105) - ψ( K j=1 α j + 1)Γ(α k + 1)Γ( K j=1 α k + 1) Γ K j=1 α j + 1 2 = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k + 1) -ψ( K j=1 α j + 1)Γ(α k + 1) Γ K j=1 α j + 1 (simplify) (106) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k )α k -ψ( K j=1 α j + 1)Γ(α k )α k Γ K j=1 α j A (defn. Γ) (107) = - Γ (A) k=1 Γ(α k ) K k=1 Γ(α k ) Γ (A) K k=1 α k A ψ(α k + 1) - α k A ψ K k=1 α k + 1 (distrib.) (108) = - K k=1 α k A ψ(α k + 1) - α k A ψ K k=1 α k + 1 (cancel) (109) = - K k=1 α k A ψ(α k + 1) - α k A ψ (A + 1) (defn. A) (110) = - K k=1 α k A ψ(α k + 1) + K k=1 α k A ψ (A + 1) (distrib.) (111) = - K k=1 α k A ψ(α k + 1) + ψ (A + 1) ( a k = A) (112) = ψ (A + 1) - K k=1 α k A ψ(α k + 1) (rearr.) (113)</formula><p>which proves the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Nemenman-Shafee-Bialek</head><p>Estimator 6 (Nemenman-Shafee-Bialek). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Define the NSB density as</p><formula xml:id="formula_52">p NSB (α) def = Kψ 1 (Kα + 1) -ψ 1 (α + 1) log K (114)</formula><p>where ψ 1 is the trigramma function. Then, the NSB estimator is given by</p><formula xml:id="formula_53">H NSB (D) def = ∞ 0 H WW (D | α • 1) p NSB (α) dα<label>(115)</label></formula><p>The integral in Eq. ( <ref type="formula" target="#formula_53">115</ref>) is typically computed by numerical integration.</p><p>To derive the Nemenman-Shafee-Bialek (NSB) estimator, we start with the idea that we would like a prior over distributions such that the distribution over expected entropy is uniform. In other words, we are looking for a p NSB such that for α ∼ p NSB , the values of E p [H(p) | α] are uniformly distributed over [0, log K]. This is a good idea since, a-priori, we do not know entropy of p and, in the absence of any insight, we should assume the entropy could be anywhere in the range [0, log K]. We make the above intuition formal with the following proposition. Proof. First, we note that E p [H(p) | α] is a continuous, increasing function in α. We will not prove this formally, but it should make intuitive sense: α is a smoothing parameter and the more the distribution is smoothed, the more entropic it should be. From basic analysis, we know that a strictly continuous, increasing function has an inverse. The above means that we can view  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: A comparison of several estimators of the entropy of the unigram distribution across 5 languages. Minima in all the graphs indicate sign changes in the error of the estimate, from an under-to an over-estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>and we additionally defineα k def = c(x k ) + α k and A def = K k=1 α k . Proposition 3(Wolpert-Wolf). The expectation of entropy under a Dirichlet posterior Dirichlet(α) where parameter α is given by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Proposition 4 .</head><label>4</label><figDesc>Let p NSB be the NSB density given in Eq. (114). Then the following conditional expectationE p [H(p) | α]over [0, log K] when α ∼ p NSB (•), defined in Eq. (114).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>E p [H(p) | α] as a bijection from R ≥0 to the interval [0, log K]. Our goal is to reparameterize the Uniform distribution in terms of α. To that end, we define the function g -1 (α) def = E p [H(p) | α] : R ≥0 → [0, log K] and perform a change-of-variables transform on Eq. (118) using g -1 . We start with the continuous uniform over [0, log K], which is show below pa random variable and unrelated to the functional H(•); the choice of letter intentionally reminds one that the variable represents the expected entropy of under a random distribution. Now we apply the change-of-variables formula at H = g -1 (α) and manipulate: p(H) = p(g -1 (αthe prior p NSB (α) has the property that the expected entropy E p [H(p) | α] where α ∼ p NSB (•) is uniformly distributed over [0, log K], which we can see by reversing the above derivation. This proves the result. Nemenman et al. (2002) interpreted Proposition 4 in the following manner: As the variance of E p [H(p) | α], which is treated as a random variable since α is random, approaches 0, then the the NSB estimator implies a uniform prior over the entropy. Lemma 3 (NSB Derivative).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Kα + 1)ψ(α + 1)] = Kψ 1 (Kα + 1)ψ 1 (α + 1)(125)Proof. The proof follows by a straightforward computation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>10)where α k = c(x k ) + α k (for the histogram count c(x k ) of class k in the sample; this is analogous toThe best unigram entropy estimators on the corpora studied, tested on various N averaged over 100 samples. All differences are statistically significant on the permutation test; lighter color indicates fewer statistically significant comparisons on the Tukey test. Scale: significantly better than 6 5 4 3 2 1 0 other estimators.</figDesc><table><row><cell></cell><cell cols="2">MAB</cell><cell></cell><cell cols="2">MSE</cell></row><row><cell></cell><cell>10 2 10 3</cell><cell>10 4</cell><cell>10 5</cell><cell>10 2 10 3</cell><cell>10 4</cell><cell>10 5</cell></row><row><cell>English German Dutch</cell><cell cols="3">HT HT NSB NSB HT HT NSB CS HT HT NSB CS</cell><cell cols="3">HT HT NSB NSB HT HT NSB CS HT HT NSB CS</cell></row><row><cell cols="7">Mongolian NSB HT NSB NSB NSB HT NSB NSB Tagalog HT HT NSB NSB HT HT NSB NSB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Normalized mutual information, calculated with several estimators, between adjectives and the inanimate nouns they modify based on UD corpora. Colored-in cell means statistically significant NMI value.</figDesc><table><row><cell>4.2 Replication of Williams et al. (2021)</cell></row><row><cell>Next, we turn to a replication of Williams et al.'s</cell></row><row><cell>(2021) information-theoretic study on the associa-</cell></row><row><cell>tion between gendered inanimate nouns and their</cell></row><row><cell>modifying adjectives. They estimate mutual infor-</cell></row><row><cell>mation by using its familiar decomposition as the</cell></row></table><note><p>difference of two entropies:</p><p>MI(X; Y ) = H(X) -H(X | Y ).</p><p>The entropies H(X) and H(X | Y ) are estimated independently and then their difference is computed. We replicateWilliams et al.'s (2021)  </p><p>experiments using gold-parsed Universal Dependencies corpora, filtering out animate nouns with Multilingual WordNet</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Estimators with least MAB (mean absolute bias) and MSE (mean squared error) for various combinations of N and K sampling from symmetric Dirichlet. The lighter the color the fewer estimators the best estimator was found to be statistically significantly better than.</figDesc><table><row><cell cols="2">MAB</cell><cell></cell><cell cols="2">MSE</cell></row><row><cell>10 1 10 2</cell><cell cols="3">10 3 10 4 10 1 10 2</cell><cell>10 3 10 4</cell></row><row><cell cols="2">100 1000 NSB HT NSB CS CS CS</cell><cell>J J</cell><cell cols="2">CS CS HT NSB CS CS</cell><cell>J J</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>As<ref type="bibr" target="#b26">Nemenman et al. (2002)</ref> highlight, when estimating the entropy of a distribution that follows a power law, it is often possible to get an effectively meaningless estimate that is completely determined by the estimator's hyperparameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our code is available at https://github.com/ aryamanarora/entropy-estimation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A proof of this result in given in full in Proposition 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>A singleton (hapax legomenon) is an outcome which is observed only once in the sample.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We used dumps from November 1, 2021: Mongolian and Tagalog; the extracted counts are available in our repository.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Adina Williams</rs>, <rs type="person">Lucas Torroba Hennigen</rs>, <rs type="person">Tiago Pimentel</rs>, and the anonymous reviewers for feedback on the manuscript.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The authors foresee no ethical concerns with the research presented in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian entropy estimation for countable discrete distributions</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il Memming</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">W</forename><surname>Pillow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">81</biblScope>
			<biblScope unit="page" from="2833" to="2868" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gulikers</surname></persName>
		</author>
		<title level="m">CELEX2. Linguistic Data Consortium</title>
		<imprint>
			<publisher>Philadelphia</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frequency in lexical processing</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Milin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramscar</surname></persName>
		</author>
		<idno type="DOI">10.1080/02687038.2016.1147767</idno>
	</analytic>
	<monogr>
		<title level="j">Aphasiology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1174" to="1220" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The entropy of words-Learnability and expressivity across more than 1000 languages</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Ferrer-I Cancho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linking and extending an open multilingual Wordnet</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Converting Italian treebanks: Towards an Italian Stanford dependency treebank</title>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="61" to="69" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of Shannon&apos;s index of diversity when there are unseen species in sample</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Jen</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and Ecological Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Toward the logical description of languages in their phonemic aspect. Language</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><surname>Halle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Jakobson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1953">1953</date>
			<biblScope unit="page" from="34" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On the complexity and typology of inflectional morphological systems</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mans</forename><surname>Hulden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00271</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="327" to="342" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are all languages equally hard to language-model?</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="536" to="541" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Usage-based linguistics</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Diessel</surname></persName>
		</author>
		<idno type="DOI">10.1093/acrefore/9780199384655.013.363</idno>
	</analytic>
	<monogr>
		<title level="m">Oxford Research Encyclopedia of Linguistics</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Dagmar</forename><surname>Divjak</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781316084410</idno>
		<title level="m">Frequency in Language: Memory, Attention and Learning</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jackknife resampling</title>
		<author>
			<persName><forename type="first">Herwig</forename><surname>Friedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Stampfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Environmetrics</title>
		<meeting><address><addrLine>Chichester</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1089" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses</title>
		<author>
			<persName><forename type="first">I</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName><surname>Good</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-3235-1</idno>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predictive power of word surprisal for reading times is a linear function of language model quality</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Goodkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klinton</forename><surname>Bicknell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</title>
		<meeting>the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)<address><addrLine>Salt Lake City, Utah</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Array programming with NumPy</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Jarrod</forename><surname>Millman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéfan</forename><forename type="middle">J</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>Picus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marten</forename><forename type="middle">H</forename><surname>Van Kerkwijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Haldane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Fernández Del Río</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pearu</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Gérard-Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Sheppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hameer</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Gohlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-020-2649-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">585</biblScope>
			<biblScope unit="issue">7825</biblScope>
			<biblScope unit="page" from="357" to="362" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">A Theory of Language and Information: A Mathematical Approach, 1 edition</title>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the limiting distributions of estimates based on samples from finite universes</title>
		<author>
			<persName><forename type="first">G</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><surname>Madow</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177730149.full</idno>
	</analytic>
	<monogr>
		<title level="m">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1948">1948</date>
			<biblScope unit="page" from="535" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">simomarsili/ndd: Bayesian entropy estimation in Python -via the Nemenman-Schafee-Bialek algorithm</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Marsili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measuring the similarity of grammatical gender systems by comparing partitions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijia</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.456</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5664" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal Dependency annotation for multilingual parsing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Núria</forename><surname>Bertomeu Castelló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungmee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting the Uniform Information Density hypothesis</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="963" to="980" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Note on the bias of information estimates</title>
		<author>
			<persName><forename type="first">George</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory in Psychology: Problems and Methods</title>
		<meeting><address><addrLine>Glencoe, IL</addrLine></address></meeting>
		<imprint>
			<publisher>Free Press</publisher>
			<date type="published" when="1955">1955</date>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A brief history of generative models for power law and lognormal distributions</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internet Mathematics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="251" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entropy and inference, revisited</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ilya Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Shafee</surname></persName>
		</author>
		<author>
			<persName><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the unigram distribution</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Nikkarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damián</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.326</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3721" to="3729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Estimating discrete entropy</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimation of entropy and mutual information</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1191" to="1254" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Word lengths are optimized for efficient communication</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Tily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3526" to="3529" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How (non-)optimal is the lexicon?</title>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Nikkarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damián</forename><surname>Blasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.350</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4426" to="4438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Theoretical and practical issues in the construction of a Greek dependency treebank</title>
		<author>
			<persName><forename type="first">Prokopis</forename><surname>Prokopidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elina</forename><surname>Desipri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Koutsombogera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stelios</forename><surname>Piperidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting>the 4th Workshop on Treebanks and Linguistic Theories (TLT)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal Dependencies for Greek</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies</title>
		<meeting>the NoDaLiDa 2017 Workshop on Universal Dependencies<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>UDW</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universal Dependencies for Portuguese</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Livy</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cláudia</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valeria</forename><surname>De Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Dependency Linguistics</title>
		<meeting>the Fourth International Conference on Dependency Linguistics<address><addrLine>Pisa,Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Linköping University Electronic Press</publisher>
			<date type="published" when="2017">2017. Depling 2017</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An information-theoretic characterization of morphological fusion</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10115" to="10120" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
	</analytic>
	<monogr>
		<title level="j">The Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prediction and entropy of printed English</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="64" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The effect of word predictability on reading time is logarithmic</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.02.013</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="319" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Prague Arabic Dependency Treebank: A word on the million words</title>
		<author>
			<persName><forename type="first">Otakar</forename><surname>Smrž</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Bielický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iveta</forename><surname>Kouřilová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kráčmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Zemánek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Arabic and Local Languages</title>
		<meeting>the Workshop on Arabic and Local Languages<address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. Marrakech</date>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards greater accuracy in lexicostatistic dating</title>
		<author>
			<persName><forename type="first">Morris</forename><surname>Swadesh</surname></persName>
		</author>
		<idno type="DOI">10.1086/464321</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of American Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="137" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Universal Dependencies for Arabic</title>
		<author>
			<persName><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-1320</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Arabic Natural Language Processing Workshop</title>
		<meeting>the Third Arabic Natural Language Processing Workshop<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="166" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">AnCora: Multilevel annotated corpora for Catalan and Spanish</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Antònia</forename><surname>Mariona Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Enriching the venice Italian treebank with dependency and grammatical relations</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolfo</forename><surname>Delmonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonella</forename><surname>Bristot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08), Marrakech</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08), Marrakech<address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparing individual means in the analysis of variance</title>
		<author>
			<persName><forename type="first">John</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="114" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Estimating means in a finite universe</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the relationships between the grammatical genders of inanimate nouns and their co-occurring adjectives and verbs</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Wolf-Sonkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damián</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00355/98239/On-the-Relationships-Between-the-Grammatical</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="139" to="159" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimating functions of probability distributions from a finite set of samples</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevE.52.6841</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6841</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extended and enhanced Polish dependency bank in Universal Dependencies format</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Wróblewska</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</title>
		<meeting>the Second Workshop on Universal Dependencies (UDW 2018)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Morphological irregularity correlates with frequency</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy O'</forename><surname>Donnell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1505</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5117" to="5126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Jackknifing an index of diversity</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Zahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="907" to="913" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Entropy estimation in Turing&apos;s perspective</title>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1368" to="1389" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The Psycho-Biology of Language</title>
		<author>
			<persName><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<pubPlace>Houghton-Mifflin; New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
