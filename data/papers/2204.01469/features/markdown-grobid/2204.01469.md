# Estimating the Entropy of Linguistic Distributions

## Abstract

## 

Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. Finally, we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability.

## Introduction

There is a natural connection between information theory, the mathematical study of communication systems, and linguistics, the study of human language-the primary vehicle that humans employ to communicate. Researchers have exploited this connection since information theory's inception [(Shannon, 1951;](#b37)[Cherry et al., 1953;](#b7)[Harris, 1991)](#b17). With the advent of modern computing, the number of information-theoretic linguistic studies has risen, exploring claims about language such as the optimality of the lexicon [(Piantadosi et al., 2011;](#b30)[Pimentel et al., 2021)](#b31), the complexity of morphological systems [(Cotterell et al., 2019;](#b8)[Wu et al., 2019;](#b49)[Rathi et al., 2021)](#b35), and the correlation between surprisal and language processing time [(Smith and Levy, 2013;](#b38)[Bentz et al., 2017;](#b3)[Goodkind and Bicknell, 2018;](#b15)[Cotterell et al., 2018;](#b9)[Meister et al., 2021, inter alia)](#). Minima in all the graphs indicate sign changes in the error of the estimate, from an under-to an over-estimate.

In information-theoretic linguistics, a fundamental quantity of research interest is entropy. Entropy is both useful to linguists in its own right, and is necessary for estimating other useful quantities, e.g., mutual information. However, the estimation of entropy from raw data can be quite challenging [(Paninski, 2003;](#b29)[Nowozin, 2015)](#b28), e.g., in expectation, the plug-in estimator underestimates entropy [(Miller, 1955)](#b24). Linguistic distributions often present additional challenges. For instance, many linguistic distributions, such as the unigram distribution, follow a power law [(Zipf, 1935;](#b52)[Mitzenmacher, 2004](#b25)).[foot_0](#foot_0) Linguistics is not the only field with such nuances, and so a large number of entropy estimators have been proposed in other fields [(Chao and Shen, 2003;](#b6)[Archer et al., 2014, inter alia)](#). However, no work to date has attempted a practical comparison of these estimators on natural language data. This work fills this empirical void.

Our paper offers a large empirical comparison of the performance of 6 different entropy estimators on both synthetic and natural language data, an example of which is shown in Figure [1](#). We find that [Chao and Shen's (2003)](#b6) is the best estimator when very few data are available, but Nemenman et al. ['s (2002)](#) is superior as more data become available. Both are significantly better (in terms of meansquared error) than the naïve plug-in estimator. Importantly, we also show that two recent studies [(Williams et al., 2021;](#b46)[McCarthy et al., 2020)](#b21) show smaller effect sizes when a better estimator is employed; however, we are able to reproduce a significant effect in both replications. We recommend that future studies carefully consider their choice of entropy estimators, taking into account data availability and the nature of the underlying distribution.[foot_1](#foot_1)

## Entropy and Language

Shannon entropy is a quantification of the uncertainty in a random variable. Given a (discrete) random variable X with probability distribution p over K possible outcomes X = {x k } K k=1 , the Shannon entropy of X is defined as

$H(X) = H(p) def = - K k=1 p(x k ) log p(x k ) (1)$Entropy has many uses throughout science and engineering; for instance, [Shannon (1948)](#b36) originally proposed entropy as a lower bound on the compressibility of a stochastic source.

Yet the application of information-theoretic techniques to linguistics is not so straightforward: Information-theoretic measures are defined over probability distributions and, in the study of natural language, we typically only have access to samples from the distribution of interest, e.g., the phonotactic distribution in English, which permits word we cannot find in a corpus, like blick, rather than the true probabilities required in the computation of Eq. (1). Indeed, it is often the case that not all elements of X are even observed in available data-such as words that were coined after the a corpus was collected.

Rather, p must be approximated in order to estimate H(p). One solution is plug-in estimation: Given samples from p, the maximum-likelihood estimate for p is "plugged" into Eq. (1). However, as originally noted by [Miller (1955)](#b24), this strategy generally yields poor estimates. [3](#foot_2) It is thus necessary to derive more nuanced estimators.

## Statistical Estimation Theory

Statistical estimation theory provides us with the tools for estimating various quantities of interest based on samples from a distribution.

Central to this theory is the estimator: A statistic that approximates a property of the distribution our data is drawn from. More formally, let

$D = { x (n) } N$n=1 be samples from an unknown distribution p. Suppose we are interested in a quantity θ that can be computed as a function of the distribution p. An estimator θ(D) for θ is then a function of the data D that provides an approximation of θ.

Two properties of an estimator are often of interest: bias-the difference between the true value of θ and the expected value of our estimator θ(D) under p-and variance-how much θ(D) fluctuates from sample set to sample set:

$bias( θ(D)) def = E p [ θ(D)] -θ (2) var( θ(D)) def = E p [( θ(D) -E p [ θ(D)]) 2 ](3)$It is desirable to construct an estimator that has both low bias and low variance. However, the bias-variance trade-off tells us that we often have to pick one, and we should focus on a balance between the two. This trade-off is evinced through mean-squared error (MSE), a metric oft-employed for assessing estimator quality:

$MSE( θ(D)) = bias( θ(D)) 2 + var( θ(D)) (4)$To recognize the trade-oft note that, for any fixed MSE, a decrease in bias must be compensated with an increase in variance and vice versa. Indeed, it is important to recognize that there is typically no single estimator that is seen as "best." Different estimators balance the bias-variance trade-off differently, making their perceived quality specific to one's use-case. Importantly, the effectiveness of an estimator also depends on the domain of interest. Consequently, an empirical study of various entropy estimators, which this paper provides, is necessary in order to determine which entropy estimators are best suited for linguistic distributions.

## Plug-in Estimation of Entropy

A simple, two-step approach for estimating entropy is plug-in estimation. In the first step, we compute the maximum-likelihood estimate for p from our dataset D as follows

$p MLE (x k ) def = N n=1 1{ x (n) = x k } N (5)$In the second step, we plug Eq. ( [5](#)) into Eq. ( [1](#)) directly, which results in the estimator H MLE (D). So why is this a bad idea? While our probability estimates themselves are unbiased, entropy is a concave function. Consequently, by Jensen's inequality, this estimator is, in expectation, a lower bound on the true entropy (see App. E.1 for proof). Moreover, when N K, which is often the case in power-law distributed data, the estimate becomes quite unreliable [(Nemenman et al., 2002)](#b26). [-Miller (1955) and](#b24)[Madow (1948)](#b19). The first innovation in entropy estimation known to the authors is a simple fix derived from a first-order Taylor expansion of MLE (described above). The Miller-Madow estimator only involves a simple additive correction, which is shown below:

## An Ensemble of Entropy Estimators

## MM

$H MM (D) def = H MLE (D) + K -1 2N (6)$where K is size of the support of X . The Miller-Madow correction should seem intuitive in that we add K-1 2N ≥ 0 to compensate for the negative bias of the estimator. A full derivation of the Miller-Madow estimator is given in Proposition 2. [JACK-Zahl (1977)](#). Next we consider the jackknife, which is a common strategy used to correct for the bias of statistical estimators. In the case of entropy estimation, we can apply the jackknife out of the box to correct the bias inherent in the MLE estimator. Explicitly, this is done by averaging plug-in entropy estimates H MLE (D) albeit with the n th sample from the data removed; we denote this held-out plug-in estimator as H \n MLE (D). Averaging these "held-out" plug-in estimators results in the following simple entropy estimator

$H JACK (D) def = N H MLE (D) - N -1 N N n=1 H \n MLE (D)(7$) Note that the jackknife is applicable to any estimator, not just H MLE (D), and, thus, can be combined with any of the other approaches mentioned.

HT- [Horvitz and Thompson (1952)](#b18). Horvitz-Thompson is a general scheme for building estimators that employs importance weighting in order to more efficiently estimate a function of a random variable. Importantly, this estimator gives us the ability to compensate for situations where the probability of an outcome is so low that it is often not observed in a sample, which is often the case for e.g., power-law distributions.

While a full exposition of HT estimators is outside of the scope of this work, in essence, we can divide the expected probability of a class by each class's estimated inclusion probability to compensate for such situations. Given the true probability of an outcome p(x k ), the probability that it occurs at least once in a sample of size N is 1 -(1p(x k )) N . The HT estimator for entropy is then defined as

$H HT (D) def = - K k=1 p MLE (x k ) log p MLE (x k ) 1 -(1 -p MLE (x k )) N (8)$using our MLE probability estimates p MLE (x k ).

CS- [Chao and Shen (2003)](#b6). Chao-Shen modifies HT by multiplying the MLE probability estimates by an estimate of sample coverage. Formally, let f 1 be the number of observed singletons[foot_3](#foot_3) in sample; our sample coverage can be estimated as

$C = 1 -f 1 N .$The CS estimator is then computed as:

$H CS (D) def = - K k=1 C • p MLE (x k ) log C • p MLE (x k ) 1 -(1 -C • p MLE (x k )) N($9) In the case that f 1 = N , we set f 1 = N -1 to ensure the estimated entropy is not 0.

WW- [Wolpert and Wolf (1995)](#b47). One family of entropy estimators in information theory is based on Bayesian principles. The first of these was the Wolpert-Wolf estimator, which uses a Dirichlet prior (with concentration parameter α and a uniform base distribution). This Bayesian estimator has a clean, closed form: Laplace smoothing), A = K k=1 α k , and ψ is the digamma function. A full derivation of Eq. ( [10](#)) is given in Proposition 3. Unfortunately, Eq. ( [10](#)) is very dependent on the choice of α: For large K, α almost completely determines the final entropy estimate, an observation first made by [Nemenman et al. (2002)](#b26) which motivated their improved estimator described below. [-Nemenman et al. (2002)](#b26). Nemenman et al. (NSB) attempt to alleviate the Wolpert-Wolf estimator's dependence on α. They take α = α • 1, enforcing that the Dirichlet prior is symmetric, and develop a hyperprior over α that results in a nearuniform distribution over entropy. The hyperprior is given by

$H WW (D | α) def = ψ A + 1 - K k=1 α k A ψ( α k + 1)($
## NSB

$p NSB (α) def = Kψ 1 (Kα + 1) -ψ 1 (α + 1) log K (11)$where ψ 1 is the trigamma function. A full derivation of Eq. ( [11](#)) is given in Proposition 4. This choice of hyperprior mitigates the effect that the chosen α has on the entropy estimate. [Nemenman et al.'s (2002)](#) entropy estimator is then the posterior mean of the Wolpert-Wolft estimator taken under p NSB :

$H NSB (D) = ∞ 0 H WW (D | α • 1) p NSB (α) dα($12) Typically, numerical integration is used to quickly compute the unidimensional integral.

## Experiments

Here we provide an evaluation of the entropy estimators presented in §3.2 on linguistic data.

## Entropy of the Unigram Distribution

We start our study with a controlled experiment where we estimate the entropy of the truncated unigram distribution, the (finite) distribution over the frequent word tokens in a language without regard to context [(Baayen et al., 2016;](#b2)[Diessel, 2017;](#b10)[Divjak, 2019;](#b11)[Nikkarinen et al., 2021)](#b27). We renormalize the frequency counts of corpora in English, German, and Dutch (taken from CELEX; [Baayen et al., 1995)](#b1), as well as Mongolian and Tagalog (from Wikipedia[foot_4](#foot_4) ). We take this renormalization as a gold standard distribution, since we cannot access the underlying unigram distribution. We then draw samples of varying sizes (N ∈ {10 2 , 10 3 , 10 4 , 10 5 }) from the distribution of renormalized frequency counts to test the estimators' ability to recover the underlying distributions' entropy. While the renormalized frequency counts are not necessarily representative of the true unigram distribution, they nevertheless provide us with a controlled setting to benchmark various entropy estimators.

We evaluate the estimators on both bias and MSE, as defined in ( [2](#)) and (4), as well as mean absolute bias (MAB). To test the statistical significance of differences in metrics between entropy estimators, we use paired permutation tests [(Good, 2000)](#b14) (sampling 1, 000 permutations) between pairs of estimators, checking MAB and MSE. We run [Tukey's test (1949)](#) to judge the statistical significance of differences in MAB and MSE between all pairs of estimators, which found only a few insignificant comparisons when N was large.

Results are shown in Table [1](#tab_0) and [Figure 1](#tab_0). We find that NSB (followed closely by CS) converges almost to the true entropy from below using with only a few samples. HT is the best estimator for N < 2, 000, but as N increases it tends to overestimate entropy to the point where its bias is greater than that of MLE. Besides HT, all estimators at all tested sample sizes N have lower MAB and MSE than MLE.

Language n MLE CS MM JACK WW NSB Italian 16, 856 20.00% 15.56% 16.43% 14.09% 19.67% 11.41% Polish 15, 525 30.52% 23.48% 25.49% 21.75% 34.68% 17.07% Portuguese 7, 409 27.60% 20.76% 22.51% 18.81% 33.32% 14.18% Spanish 21, 408 20.50% 15.17% 16.44% 13.80% 21.04% 10.50% Arabic 2, 483 45.31% 38.49% 40.99% 37.93% 49.09% 34.82% Croatian 13, 856 31.35% 26.04% 26.62% 23.08% 35.66% 19.06% Greek 3, 305 41.58% 33.17% 36.39% 32.32% 48.80% 27.00%  [(Bond and Foster, 2013)](#b4). We rerun their experimental set-up using our full suite of entropy estimators to determine whether the relationship they posit remains significant, checking 3 more languages not in the original study.

We report results for normalized mutual information (dividing MI by maximum possible MI) in Table [2](#tab_2). We find that using NSB (the estimator we found most effective in §4.1) instead of MLE, nearly halves the measured effect in all languages. However, the effect remains statistically significant in 5 of 7 languages tested, including the 4 that were also in the original study.

## Replication of McCarthy et al. (2020)

Finally, we turn our attention to [McCarthy et al.'s (2020)](#) study on the similarity between grammatical gender partitions between languages. Using information-theoretic measures, they found that closely related languages have more similar gender groupings of core lexical items. We replicate their experiment on Swadesh lists [(Swadesh, 1955)](#b40) for 10 European languages with different estimators, and find that hierarchical clustering over both mutual (MI) and variational information (VI) produces the same trees as the original study. In this case, using NSB, our recommended estimator, results in a reduced estimate of MI (e.g. Croatian-Slovak: 0.54 with MLE → 0.46 with NSB), but significance test-ing with 1,000 permutations finds the same pairs were statistically significant for both MI and VI regardless of estimator: all pairs of Slavic languages and Romance languages, and Bulgarian-Spanish (see Figure [2](#)). Thus, we see a similar result here as in the previous replication.

## Conclusion

This work presents the first empirical study comparing the performance of various entropy estimators for use with natural language distributions. From experiments on synthetic data (appendix) and natural data (CELEX), and two replication studies of recent papers in information-theoretic linguistics, we find that the oft-employed plug-in estimator of entropy can cause misleading results, e.g., the overestimates of effect sizes seen in both replication studies. The recommendation of our paper is that researchers should carefully consider their choice of entropy estimator based on data availability and the nature of the underlying distribution.

MAB MSE 10 1 10 2 10 3 10 4 10 1 10 2 10 3 10 4 2 HT WW WW WW WW WW WW JACK 5 MM WW WW JACK MM WW WW MM 10 JACK CS WW MM JACK WW WW MLE 100 CS CS JACK WW CS JACK JACK WW 1000 CS HT CS JACK CS HT CS JACK Table 4: Estimators with least MAB (mean absolute bias) and MSE (mean squared error) for various combinations of N and K sampling from Zipfian distributions.

## A Implementation

The code for each of the entropy estimators is implemented in Python using numpy [(Harris et al., 2020)](#b16), except for NSB which was taken from an existing efficient implementation in the ndd module [(Marsili, 2016)](#b20). We calculated entropies with base e (in nats).

## B Experiments with simulated data

In our experiments with simulated data, we explore distributions sampled from a symmetric Dirichlet prior with varying number of classes K and known distributions of Zipfian form with various parameters.

Words in natural languages have a roughly Zipfian distribution, with probability inversely proportional to rank [(Zipf, 1935)](#b52), and a symmetric Dirichlet distribution is analogous to e.g. POS tag label distributions in natural language. Thus, studying synthetic data from such distributions as a start is useful.

## B.1 Experiment 1: Symmetric Dirichlet distributions

We sample 1, 000 distributions from a symmetric Dirichlet distribution with variable number of classes K, i.e. with paramater α = [α 1 , . . . , α K ] = [1, . . . , 1]. We calculate entropy estimates on different sample sizes N . Since we know the parameters of the true distribution, we can compare estimates with the true entropy. We do pairwise comparisons of the MAB and MSE of estimators, using paired permutation tests to establish significance. Table [3](#tab_4) shows our results, including significance tests. It is clear that when N K, all of the estimators have nearly converged to the true value and estimator choice does not matter. However, in the low-sample regime some estimators are indeed significantly better at approximating the true entropy. Our results are mixed as to which estimator is best in what context; the one found to be most frequently significantly better than other estimators was Chao-Shen. What is clear is that MLE is never the best choice.

## B.2 Experiment 2: Zipfian distributions

We sample 1, 000 finite Zipfian distributions with K classes which obey Zipf's law, that the probability of an outcome is inverse proportional to its rank. The experimental setup is the same as in Experiment 1. A Zipfian distribution approximates (but is not a perfect model of) the distribution of tokens in natural language text in some languages, including English, which was the basis for the law being proposed. Compare similar experiments on infinite Zipf distributions by [Zhang (2012)](#b51). Results are in Table [4](#). [Williams et al. (2021)](#b46) We used the following UD treebanks:

## C Replication of

• Arabic: PADT [(Smrž et al., 2008;](#b39)[Taji et al., 2017)](#b41); • Greek: GDT [(Prokopidis et al., 2005;](#b32)[Prokopidis and Papageorgiou, 2017)](#); • Italian: ISDT [(Bosco et al., 2013)](#b5), VIT [(Tonelli et al., 2008)](#b43); • Polish: PDB (Wróblewska, 2018); • Portuguese: GSD (McDonald et al., 2013), Bosque (Rademaker et al., 2017); • Spanish: AnCora (Taulé et al., 2008), GSD (McDonald et al., 2013). D Additional Figures hr sk ru bg uk fr ca it es pt Lang2 CS MI 0 0.1 0.2 0.3 0.4 0.5 HT JACK hr sk ru bg uk fr ca it es pt MLE hr sk ru bg uk fr ca it es pt hr sk ru bg uk fr ca it es pt MM hr sk ru bg uk fr ca it es pt NSB hr sk ru bg uk fr ca it es pt Lang1 WW Figure 2: Mutual information between the gender partitions of language pairs with various estimators, replicating McCarthy et al. (2020). 50 100 150 200 -1 0 1 2 Bias (nats) CS 50 100 150 200 HT 50 100 150 200 JACK 50 100 150 200 MLE 50 100 150 200 MM 50 100 150 200 NSB 50 100 150 200 Samples WW Estimator bias Figure 3: The distribution of bias for entropy over several estimators given variable sample size N , sampling from 100 distributions taken from a symmetric Dirichlet prior with K = 100. CS HT JACK MLE MM NSB WW Estimator 2 K: 2 MAB 0.25 0.50 0.75 K: 5 K: 10 K: 100 K: 1000 N: 10 CS HT JACK MLE MM NSB WW N: 100 CS HT JACK MLE MM NSB WW N: 1000 CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW Estimator 1 N: 10000 Pairwise MAB p-values (Dirichlet) CS HT JACK MLE MM NSB WW Estimator 2 K: 2 MSE 0.25 0.50 0.75 K: 5 K: 10 K: 100 K: 1000 N: 10 CS HT JACK MLE MM NSB WW N: 100 CS HT JACK MLE MM NSB WW N: 1000 CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW Estimator 1 N: 10000

Pairwise MSE p-values (Dirichlet)

Figure [4](#fig_2): The heatmaps display the p-values calculated between pairs of estimators for mean absolute bias (MAB) and mean squared error (MSE) for Experiment 1. More purple values mean the estimator on the y-axis (Estimator 2) is better than the estimator on the x-axis (Estimator 1). Comparisons tend to become non-significant as N increases, since all the estimators gradually converge to the true entropy.

## E Derivation of the Entropy Estimators

Let X = {x k } K k=1 be a finite set. Let p be a distribution over X . The entropy of p is defined as

$H(p) def = - K k=1 p k log p k (13)$Given a dataset of N samples D sampled i.i.d. from p, our goal is to estimate the entropy H(p) from samples D from the true distribution p. We will denote the count of an item n) . The maximum-likelihood estimate (MLE) of p given D is denoted

$x k as c(x k ) = N n=1 1 x k = x ($$N n=1 1{ x (n) =x k } N$. The plug-in estimate of H(p) is defined to be the estimate of H(p) obtained by plugging the MLE estimate p MLE directly into the definition of entropy, i.e.,

$H MLE (D) = H( p MLE ) = - K k=1 p MLE (x k ) log p MLE (x k ) = - K k=1 c(x k ) N log c(x k ) N (14$$)$This section discusses the problems with Eq. ( [14](#formula_16)) as an estimator and provides detailed derivations of improved estimators found in the literature.

## E.1 The Plug-in Estimator is Negatively Biased

Proposition 1. The MLE entropy estimator in expectation underestimates true entropy, i.e.,

$H MLE (D) = E K k=1 -p MLE (x k ) log p MLE (x k ) ≤ H(p)(15)$Proof. The result is a simple consequence of Jensen's inequality and some basic manipulations:

$E K k=1 -p MLE (x k ) log p MLE (x k ) = K k=1 E[-p MLE (x k ) log p MLE (x k )] (linearity of expectation) ≤ - K k=1 E[ p MLE (x k )] log E[ p MLE (x k )] (Jensen's inequality) = - K k=1 p(x k ) log p(x k ) (E[ p MLE (x k )] = p(x k )) = H(p) (definition of entropy)$This completes the result.

## E.2 Miller-Madow

Proposition 2. Let p be a categorical distribution over X = {x 1 , . . . , x K }, i.e., a categorical distribution with support K. Let D be our dataset of size N sampled from p. Finally, let p MLE be the maximumlikelihood estimate computed on D. Then, we have

$bias H MLE (D) def = E p H MLE (D) -H(p) (16) = - K -1 2N + o N -1 (17)$Proof. We start by taking a first-order Taylor expansion and take an expectation of both sides.

$H MLE (D) = H( p MLE , p) cross-entropy -KL( p MLE || p) (Lemma 1) (18) E p H MLE (D) = E p [H( p MLE , p)] -E p [KL( p MLE || p)] (expectation) (19) = E p - K k=1 p MLE (x k ) log p(x k ) -E p [KL( p MLE || p)] (defn. H(p, q)) (20) = - K k=1 E p [ p MLE (x k ) log p(x k )] -E p [KL( p MLE || p)] (linearity) (21) = - K k=1 E p [ p MLE (x k )] log p(x k ) -E p [KL( p MLE || p)] (algebra) (22) = - K k=1 p(x k ) log p(x k ) -E p [KL( p MLE || p)] (unbiased) (23) = H(p) -E p [KL( p MLE || p)] (defn. of H(p))(24)$(25)

This gives us:

$E p H MLE (D) -H(p) = -E p [KL( p MLE || p)] (subtract H(p))(26)$Thus, we may compactly write the bias as:

$bias H MLE (D) = E p [H( p MLE )] -H(p) (definition of bias) (27) = -E p [KL( p MLE || p)] (above computation) (28) ≤ 0 (non-negativity of KL)(29)$Now, we find a simpler expression for the remainder E p [KL( p MLE || p)]. Again, we start with a secondorder Taylor expansion

$KL(p || q) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (Lemma 2) (30$$)$around the point ∆(x) = p(x)q(x). Define p MLE (x k ) = c(x k ) N where c(x k ) is the count of x k in the training set. We now simplify the first term:

$E p K k=1 ∆(x k ) 2 2q(x k ) = E p K k=1 ( p MLE (x k ) -p(x k )) 2 2p(x k ) (definition of ∆(x k )) (31) = E p K k=1 ( c(x k ) N -p(x k )) 2 2p(x k ) (definition of MLE) (32) = E p K k=1 (c(x k ) -N p(x k )) 2 2N 2 p(x k ) (× N /N) (33) = 1 2N E p K k=1 (c(x k ) -N p(x k )) 2 N p(x k ) (pulling out 1 /2N) (34) = 1 2N E p     K k=1 c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N p(x k )     (exp. the binomial) (35) = 1 2N K k=1 E p c(x k ) 2 -2N p(x k )E p [c(x k )] + N 2 p(x k ) 2 N p(x k ) (lin. of expect.) (36) = 1 2N K k=1 N p k (1 -p(x k )) + N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N p(x k ) (moments of MLE) (37) = 1 2N K k=1 N p k (1 -p(x k )) N p(x k ) + 1 2N K k=1 N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N p(x k ) =0 (38) = 1 2N K k=1 $ $ $ $ N p(x k )(1 -p(x k )) $ $ $ $ N p(x k ) (39) = 1 2N K k=1 (1 -p(x k )) (algebra) (40) = 1 2N K k=1 1 =K - 1 2N K k=1 p(x k ) =1 (algebra) (41) = K -1 2N (42)$Next, we simplify the second term, o ∆(x) 2 , in the MLE case:

$E p o ∆(x) 2 = E p o ( p MLE (x k ) -p(x k )) 2 (definition of ∆) (43) = E p o c(x k ) N -p(x k ) 2 (definition of MLE) (44) = E p o (c(x k ) -N p(x k )) 2 N 2 (× N /N) (45) = E p o c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N 2 (46) = o E p c(x k ) 2 -2c(x k )N p(x k ) + N 2 p(x k ) 2 N 2 (push exp. through) (47) = o     N p k (1 -p(x k )) + N 2 p(x k ) 2 -2N 2 p(x k ) 2 + N 2 p(x k ) 2 N 2     (48) = o N p(x k )(1 -p(x k )) N 2 (cancel terms) (49) = o p(x k )(1 -p(x k )) N (cancel N in fraction) (50) = o N -1 (ignore constants) (51)$Putting it all together, we get that bias (H( p MLE )) = -K-1 2N + o N -1 which is the desired result.

Interestingly, it can be seen that the negative bias of the MLE gets worse as the number of classes K grows. Distributions with large K pop up frequently when dealing with natural language.

Corollary 1. The plug-in estimator of entropy is consistent.

Proof. From Proposition 2, we have bias (H( p MLE )) = -K-1 2N + o N -1 . Clearly, as N → 0, we have bias (H( p MLE )) → 0, so the estimator is consistent. One could also prove consistency through a simple application of the continuous mapping theorem.

Estimator 1 [(Miller-Madow)](#). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then, the Miller-Madow estimator of H(p) is given by

$H MM (D) def = H MLE (D) + K -1 2N(52)$The Miller-Madow estimator is biased, however it is consistent.

Lemma 1. The the first-order Taylor approximation of H MLE (D) around the distribution p is given by

$H MLE (D) = H( p MLE , p) + R(p, p MLE )(53)$where the remainder R is given by

$R(p, p MLE ) = -KL( p MLE || p)(54)$Proof. The result follows from direct computation. We start by taking the Taylor expansion of H( p MLE ) around H(p):

$H MLE (D) = H(p) + K k=1 ∂ ∂p(x k ) H(p) p MLE (x k ) -p(x k ) + R(p, p MLE ) remainder(55)$Our first order term can then be rewritten as follows:

$K k=1 ∂ ∂p(x k ) H(p) p MLE (x k ) -p(x k ) (56) = K k=1 ∂ ∂p(x k ) K k =1 -p(x k ) log p(x k ) p MLE (x k ) -p(x k ) (57) = K k=1 K k =1 - ∂ ∂p(x k ) p(x k ) log p(x k ) p MLE (x k ) -p(x k ) (linearity) (58) = K k=1 K k =1 ∂ ∂p(x k ) p(x k ) log p(x k ) p(x k ) -p MLE (x k ) (sign) (59) = K k=1 1 + log p(x k ) p(x k ) -p MLE (x k ) (60) = K k=1 p(x k ) -p MLE (x k ) + log p(x k ) (p(x k ) -p MLE (x k )) (61) = K k=1 p(x k ) -p MLE (x k ) + K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (62) = K k=1 p(x k ) =1 - K k=1 p MLE (x k ) =1 + K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (distrib. sum) (63) = K k=1 log p(x k ) (p(x k ) -p MLE (x k )) (simplify) (64) = K k=1 log p(x k )p(x k ) -H(p) - K k=1 log p(x k ) p MLE (x k ) H(p, pMLE) (distrib. sum) (65) = H(p, p MLE ) -H(p)(66)$Plugging this back into our Taylor expansion, we get the following:

$H MLE (D) = ¨Ḧ (p) - ¨Ḧ (p) + H(p, p MLE ) + R(p, p MLE )(67)$Now, we see that this implies

$R(p, p MLE ) = H MLE (D) -H( p MLE , p) (algebra) (68) = - K k=1 p MLE (x k ) log p MLE (x k ) + K k=1 p MLE (x k ) log p(x k ) (defn.) (69) = - K k=1 ( p MLE (x k ) log p MLE (x k ) -p MLE (x k ) log p(x k )) (merge sums) (70) = - K k=1 p MLE (x k )(log p MLE (x k ) -log p(x k )) (factor out p MLE (x k )) (71) = - K k=1 p MLE (x k ) log p MLE (x k ) p(x k ) (log algebra) (72) = -KL( p MLE || p) (defn.)(73)$which is the desired result.

Lemma 2. Define ∆(x) = p(x)q(x). The second-order Taylor expansion of KL(p || q) around ∆(x) is given by

$KL(p || q) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2(74)$Proof. Now we compute the series expansion of the KL-divergence. We first make a tricky substitution:

$p(x) q(x) = q(x) + p(x) -q(x) q(x) = 1 + p(x) -q(x) q(x) = 1 + ∆(x) q(x)(75)$Now, we proceed with the derivation:

$KL(p || q) = x∈X p(x) log p(x) q(x) (defn. of KL divergence) (76) = x∈X (q(x) + ∆(x)) log 1 + ∆(x) q(x) (Eq. (75)) (77) = x∈X (q(x) + ∆(x)) ∆(x) q(x) - ∆(x) 2 2q(x) 2 + o ∆(x) 2 (Taylor expansion) (78) = x∈X ∆(x) - ∆(x) 2 2q(x) + ∆(x) 2 q(x) - ∆(x) 3 2q(x) 2 + o ∆(x) 2 (distribute) (79) = x∈X ∆(x) - ∆(x) 2 2q(x) + ∆(x) 2 q(x) + o ∆(x) 2 (defn. of o) (80) = x∈X ∆(x) + ∆(x) 2 2q(x) + o ∆(x) 2 (algebra) (81) = x∈X ∆(x) =0 + x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (split sums) (82) = x∈X ∆(x) 2 2q(x) + o ∆(x) 2 (83)$which is the desired result.

## E.3 Jackknife

The jackknife resampling method is used to estimate the bias of an estimator and correct for it, by sampling all subsamples of size N -1 from the available sample of size N , computing their average for the statistic being estimated. Generally, this reduces the order of the bias of an estimator from O(N -1 ) to at most O(N -2 ) [(Friedl and Stampfer, 2002)](#b12).

Estimator 2 (Jackknife). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Let H \n (D) be an estimate of the entropy from a sample with the n th observation held out. Then, the Jackknife estimator is given by

$H JACK (D) def = N H MLE (D) - N -1 N N n=1 H \n MLE (D) (84$$)$This estimator is derived from the jackknife-resampled estimate of the bias of the MLE estimator, multiplied by N -1.

$H JACK (D) -H MLE (D) = (N -1) H MLE (D) - 1 N N n=1 H \n MLE (D)(85)$E.4 [Horvitz-Thompson Horvitz and Thompson (HT;](#)[1952)](#b18) is a common estimator given a finite universe, which is our case as K is finite. We omit a derivation a full here as it is well documented in other places [(Vieira, 2017)](#b45). However, we note that, in contrast to many applications of HT, the application of HT to entropy estimation results in a biased estimator as the function whose mean we seek to estimate is log p(x k ), which is dependent on the unknown distribution p.

Estimator 3 [(Horvitz-Thompson)](#). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then the Horvitz-Thompson estimator is defined as

$H HT (D) def = - K k=1 p MLE (x k ) log p MLE (x k ) 1 -(1 -p MLE (x k )) N (86$$)$where 1 -(1p MLE (x k )) N is an estimate of the inclusion probability, i.e., the probability that x k appears in a random sample D of size N .

We do not know of a simple expression for the bias of the Horvitz-Thompson entropy estimator, but one observation is that E p (1p MLE (x k )) N > E p (1p(x k )) N when N > 1 (justified by Jensen's inequality, since x N , N > 1 is convex over [0, 1]); this is an overestimate of the true inclusion probability.

## E.5 Chao-Shen

The Chao-Shen estimator builds upon Horvitz-Thompson by noting that that estimator does not correct for underestimation of number of classes K and resulting effect on estimates of p(x k ); i.e. 1-(1-p MLE (x k )) N is always 0 for a class not included in the sample even if the class is present in the true distribution. We can reweight the sample probabilities to compensate for missing classes using the notion of sample coverage.

Definition 1 (Sample coverage). We define the sample coverage as

$C = K k=1 p(x k )1 x k ∈ D (87)$Definitionally, (1 -C) is then the probability of sampling an x k not observed in the sample X . However, exact computation of Eq. ( [88](#formula_44)) is impossible as we do not know the true distribution p. Thus, [Chao and Shen (2003)](#b6) fall back on a well-known estimator of C that uses a technique from [Good-Turing (1953)](#) smoothing. Let f 1 be the number of classes with only one observation in the current sample, i.e, the number of singletons, then we can estimate the sample coverage as

$C def = 1 - f 1 N(88)$The Chao-Shen estimator, described below, simply re-scales the MLE estimate of probability p MLE (x k ) in the HT estimator by C. This corrects for the observed underestimation of p's entropy by HT.

Estimator 4 (Chao-Shen). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Let C, an estimate of sample coverage, be defined as in Eq. ( [88](#formula_44)). The Chao-Shen estimator is then defined as

$H CS (D) def = - K k=1 C • p MLE (x k ) log ( C • p MLE (x k )) 1 -(1 -C • p MLE (x k )) N (89) E.6 Wolpert-Wolf Fact 1 (Derivative of an exponent). d da x a = x a log x(90)$Fact 2 (Normalizer of a Dirichlet). The normalizer of a Dirichlet distribution is

$δ K k=1 x k -1 K k=1 x α k dx = K k=1 Γ(α k ) Γ K k=1 α k (91)$A relatively easy proof of this fact makes use of a Laplace transform.

Estimator 5 [(Wolpert-Wolf)](#). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Then, the Wolpert-Wolf estimator is given by

$H WW (D | α) def = ψ A + 1 - K k=1 α k A ψ( α k + 1)(92)$where 

$c(x k ) def = N n=1 1{ x n = x k },$$E [H(p) | α] def = H(p) δ K k=1 p(x k ) -1 Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k -1 dp (93) = ψ (A + 1) - K k=1 α k A ψ(α k + 1)(94)$where

$A def = K k=1 α k .$Proof. Let Dirichlet(α 1 , . . . , α K ) be a Dirichlet posterior. The result follows by a series of manipulations:

$E [H(p) | α] = H(p) δ K k=1 p(x k ) -1 Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k -1 dp (defn.) (95) = Γ (A) k=1 Γ(α k ) H(p) δ K k=1 p(x k ) -1 K k=1 p(x k ) α k -1 dp (96) = Γ (A) k=1 Γ(α k ) - K k=1 p(x k ) log p(x k ) δ K k=1 p(x k ) -1 K k=1 p α k -1 k dp (defn. H) (97) = - Γ (A) k=1 Γ(α k ) K k=1 p(x k ) log p(x k )δ K k=1 p(x k ) -1 K k=1 p(x k ) α k -1 dp (linear.) (98) = - Γ (A) k=1 Γ(α k ) K k=1 p(x k ) α k log p(x k )δ K k=1 p(x k ) -1 K j=1, j =k p(x j ) α j -1 dp (algebra) (99) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k p(x k ) α k δ K k=1 p(x k ) -1 K j=1, j =k p(x j ) α j -1 dp (fact #1) (100) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k δ K k=1 p(x k ) -1 p(x k ) α k K j=1, j =k p(x j ) α j -1 dp (algebra) (101) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k δ K k=1 p(x k ) -1 p(x k ) α k K j=1, j =k p(x j ) α j -1 dp (102) = - Γ (A) k=1 Γ(α k ) K k=1 d dα k Γ(α k + 1) K j=1, j =k Γ(α j ) Γ K j=1 α j + 1 (fact #2) (103) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) d dα k Γ(α k + 1) Γ K j=1 α j + 1 (104) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k + 1)Γ K j=1 α j + 1 Γ K j=1 α j + 1 2 (derivative) (105) - ψ( K j=1 α j + 1)Γ(α k + 1)Γ( K j=1 α k + 1) Γ K j=1 α j + 1 2 = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k + 1) -ψ( K j=1 α j + 1)Γ(α k + 1) Γ K j=1 α j + 1 (simplify) (106) = - Γ (A) k=1 Γ(α k ) K k=1 K j=1, j =k Γ(α j ) ψ(α k + 1)Γ(α k )α k -ψ( K j=1 α j + 1)Γ(α k )α k Γ K j=1 α j A (defn. Γ) (107) = - Γ (A) k=1 Γ(α k ) K k=1 Γ(α k ) Γ (A) K k=1 α k A ψ(α k + 1) - α k A ψ K k=1 α k + 1 (distrib.) (108) = - K k=1 α k A ψ(α k + 1) - α k A ψ K k=1 α k + 1 (cancel) (109) = - K k=1 α k A ψ(α k + 1) - α k A ψ (A + 1) (defn. A) (110) = - K k=1 α k A ψ(α k + 1) + K k=1 α k A ψ (A + 1) (distrib.) (111) = - K k=1 α k A ψ(α k + 1) + ψ (A + 1) ( a k = A) (112) = ψ (A + 1) - K k=1 α k A ψ(α k + 1) (rearr.) (113)$which proves the result.

## E.7 Nemenman-Shafee-Bialek

Estimator 6 (Nemenman-Shafee-Bialek). Let p be a categorical over K categories. We seek to estimate the entropy H(p). Let D be our dataset of size N sampled from p. Define the NSB density as

$p NSB (α) def = Kψ 1 (Kα + 1) -ψ 1 (α + 1) log K (114)$where ψ 1 is the trigramma function. Then, the NSB estimator is given by

$H NSB (D) def = ∞ 0 H WW (D | α • 1) p NSB (α) dα(115)$The integral in Eq. ( [115](#formula_53)) is typically computed by numerical integration.

To derive the Nemenman-Shafee-Bialek (NSB) estimator, we start with the idea that we would like a prior over distributions such that the distribution over expected entropy is uniform. In other words, we are looking for a p NSB such that for α ∼ p NSB , the values of E p [H(p) | α] are uniformly distributed over [0, log K]. This is a good idea since, a-priori, we do not know entropy of p and, in the absence of any insight, we should assume the entropy could be anywhere in the range [0, log K]. We make the above intuition formal with the following proposition. Proof. First, we note that E p [H(p) | α] is a continuous, increasing function in α. We will not prove this formally, but it should make intuitive sense: α is a smoothing parameter and the more the distribution is smoothed, the more entropic it should be. From basic analysis, we know that a strictly continuous, increasing function has an inverse. The above means that we can view  

![Figure 1: A comparison of several estimators of the entropy of the unigram distribution across 5 languages. Minima in all the graphs indicate sign changes in the error of the estimate, from an under-to an over-estimate.]()

![and we additionally defineα k def = c(x k ) + α k and A def = K k=1 α k . Proposition 3(Wolpert-Wolf). The expectation of entropy under a Dirichlet posterior Dirichlet(α) where parameter α is given by]()

![Let p NSB be the NSB density given in Eq. (114). Then the following conditional expectationE p [H(p) | α]over [0, log K] when α ∼ p NSB (•), defined in Eq. (114).]()

![E p [H(p) | α] as a bijection from R ≥0 to the interval [0, log K]. Our goal is to reparameterize the Uniform distribution in terms of α. To that end, we define the function g -1 (α) def = E p [H(p) | α] : R ≥0 → [0, log K] and perform a change-of-variables transform on Eq. (118) using g -1 . We start with the continuous uniform over [0, log K], which is show below pa random variable and unrelated to the functional H(•); the choice of letter intentionally reminds one that the variable represents the expected entropy of under a random distribution. Now we apply the change-of-variables formula at H = g -1 (α) and manipulate: p(H) = p(g -1 (αthe prior p NSB (α) has the property that the expected entropy E p [H(p) | α] where α ∼ p NSB (•) is uniformly distributed over [0, log K], which we can see by reversing the above derivation. This proves the result. Nemenman et al. (2002) interpreted Proposition 4 in the following manner: As the variance of E p [H(p) | α], which is treated as a random variable since α is random, approaches 0, then the the NSB estimator implies a uniform prior over the entropy. Lemma 3 (NSB Derivative).]()

![Kα + 1)ψ(α + 1)] = Kψ 1 (Kα + 1)ψ 1 (α + 1)(125)Proof. The proof follows by a straightforward computation:]()

![10)where α k = c(x k ) + α k (for the histogram count c(x k ) of class k in the sample; this is analogous toThe best unigram entropy estimators on the corpora studied, tested on various N averaged over 100 samples. All differences are statistically significant on the permutation test; lighter color indicates fewer statistically significant comparisons on the Tukey test. Scale: significantly better than 6 5 4 3 2 1 0 other estimators.]()

![Normalized mutual information, calculated with several estimators, between adjectives and the inanimate nouns they modify based on UD corpora. Colored-in cell means statistically significant NMI value.]()

![Estimators with least MAB (mean absolute bias) and MSE (mean squared error) for various combinations of N and K sampling from symmetric Dirichlet. The lighter the color the fewer estimators the best estimator was found to be statistically significantly better than.]()

As[Nemenman et al. (2002)](#b26) highlight, when estimating the entropy of a distribution that follows a power law, it is often possible to get an effectively meaningless estimate that is completely determined by the estimator's hyperparameters.

Our code is available at https://github.com/ aryamanarora/entropy-estimation.

A proof of this result in given in full in Proposition 1.

A singleton (hapax legomenon) is an outcome which is observed only once in the sample.

We used dumps from November 1, 2021: Mongolian and Tagalog; the extracted counts are available in our repository.

