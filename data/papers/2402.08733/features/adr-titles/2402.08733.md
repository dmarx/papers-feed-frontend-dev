- Decision to define second-order calibration as an extension of ordinary calibration
- Choice to train models on pairs of responses instead of single responses
- Decision to allow models to "cheat" by observing one response while predicting another
- Assumption that cheating behavior indicates gaps in knowledge
- Choice of metrics for evaluating model performance (e.g., cross-entropy, marginal likelihood)
- Decision to use paired responses for constructing confidence intervals
- Choice to focus on epistemic versus aleatoric uncertainty
- Decision to demonstrate empirical performance on specific datasets (e.g., CIFAR-10H)
- Choice of model architecture (e.g., Transformer sequence models)
- Decision to apply the approach to various tasks (e.g., image classification, language modeling)
- Assumption that existing epistemic uncertainty quantification techniques are insufficient
- Decision to propose a novel cheat-corrected epistemic confidence metric
- Choice to evaluate the impact of model capacity and training data on calibration
- Decision to explore implications of miscalibration in binary response scenarios
- Choice to compare against existing uncertainty quantification baselines
- Decision to define and measure grouping loss in calibration assessments
- Assumption that second-order calibration can be achieved without assumptions about p Y|X
- Decision to use statistical hallucination tests for model evaluation
- Choice to focus on the implications of underfitting in model training
- Decision to document theoretical results and proofs in an appendix