# Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach

## Abstract

## 

We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-ofconcept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.

Model: huggingface.co/tomg-group-umd/huginn-0125

## 

1 4 6 8 12 20 32 48 64 Test-Time Compute Recurrence 0 10 20 30 40 50 Accuracy (%) Scaling up Test-Time Compute with Recurrent Depth ARC challenge GSM8K CoT OpenBookQA 3.6B 8.3B 11.5B14.6B 21.0B 33.6B 52.6B 77.9B103B

## Materialized Parameters

Figure [1](#): We train a 3.5B parameter language model with depth recurrence. At test time, the model can iterate longer to use more compute and improve its performance. Instead of scaling test-time reasoning by "verbalizing" in long Chains-of-Thought, the model improves entirely by reasoning in latent space. Tasks that require less reasoning like OpenBookQA converge quicker than tasks like GSM8k, which effectively make use of more compute.

capability of models by scaling test time computation. The mainstream approach involves post-training on long chainof-thought examples to develop the model's ability to verbalize intermediate calculations in its context window and thereby externalize thoughts.

However, the constraint that expensive internal reasoning must always be projected down to a single verbalized next token appears wasteful; it is plausible that models could be more competent if they were able to natively "think" in their continuous latent space. One way to unlock this untapped dimension of additional compute involves adding a recurrent unit to a model. This unit runs in a loop, iteratively processing and updating its hidden state and enabling computations to be carried on indefinitely. While this is not currently the dominant paradigm, this idea is foundational to machine learning and has been (re-)discovered in every decade, for example as recurrent neural networks, diffusion models, and as universal or looped transformers.

In this work, we show that depth-recurrent language models can learn effectively, be trained in an efficient manner, and demonstrate significant performance improvements under the scaling of test-time compute. Our proposed trans-former architecture is built upon a latent depth-recurrent block that is run for a randomly sampled number of iterations during training. We show that this paradigm can scale to several billion parameters and over half a trillion tokens of pretraining data. At test-time, the model can improve its performance through recurrent reasoning in latent space, enabling it to compete with other open-source models that benefit from more parameters and training data. Additionally, we show that recurrent depth models naturally support a number of features at inference time that require substantial tuning and research effort in non-recurrent models, such as per-token adaptive compute, (self)-speculative decoding, and KV-cache sharing. We finish out our study by tracking token trajectories in latent space, showing that a number of interesting computation behaviors simply emerge with scale, such as the model rotating shapes in latent space for numerical computations.

## Why Train Models with Recurrent Depth?

Recurrent layers enable a transformer model to perform arbitrarily many computations before emitting a token. In principle, recurrent mechanisms provide a simple solution for test-time compute scaling. Compared to a more standard approach of long context reasoning [(OpenAI, 2024;](#b116)[DeepSeek-AI et al., 2025)](#), latent recurrent thinking has several advantages.

‚Ä¢ Latent reasoning does not require construction of bespoke training data. Chain-of-thought reasoning requires the model to be trained on long demonstrations that are constructed in the domain of interest. In contrast, our proposed latent reasoning models can train with a variable compute budget, using standard training data with no specialized demonstrations, and enhance their abilities at testtime if given additional compute. ‚Ä¢ Latent reasoning models require less memory for training and inference than chain-of-thought reasoning models. Because the latter require extremely long context windows, specialized training methods such as tokenparallelization [(Liu et al., 2023a](#)) may be needed. ‚Ä¢ Recurrent-depth networks perform more FLOPs per parameter than standard transformers, significantly reducing communication costs between accelerators at scale. This especially enables higher device utilization when training with slower interconnects. ‚Ä¢ By constructing an architecture that is compute-heavy and small in parameter count, we hope to set a strong prior towards models that solve problems by "thinking", i.e. by learning meta-strategies, logic and abstraction, instead of memorizing. The strength of recurrent priors for learning complex algorithms has already been demonstrated in the "deep thinking" literature [(Schwarzschild et al., 2021b;](#)[Bansal et al., 2022;](#b14)[Schwarzschild et al., 2023)](#b132).

On a more philosophical note, we hope that latent reasoning captures facets of human reasoning that defy verbalization, such as spatial thinking, physical intuition or (motor) planning. Over many iterations of the recurrent process, reasoning in a high-dimensional vector space would enable the deep exploration of multiple directions simultaneously, instead of linear thinking, leading to a system capable of exhibiting novel and complex reasoning behavior.

Scaling compute in this manner is not at odds with scaling through extended (verbalized) inference scaling [(Shao et al., 2024)](#b134), or scaling parameter counts in pretraining [(Kaplan et al., 2020)](#b76), we argue it may build a third axis on which to scale model performance.

--------

Table of Contents --------‚Ä¢ Section 3 introduces our latent recurrent-depth model architecture and training objective. ‚Ä¢ Section 4 describes the data selection and engineering of our large-scale training run on Frontier, an AMD cluster. ‚Ä¢ Section 5 reports benchmark results, showing how the model improves when scaling inference compute. ‚Ä¢ Section 6 includes several application examples showing how recurrent models naturally simplify LLM usecases. ‚Ä¢ Section 7 visualizes what computation patterns emerge at

scale with this architecture and training objective, showing that context-dependent behaviors emerge in latent space, such as "orbiting" when responding to prompts requiring numerical reasoning.

## A scalable recurrent architecture

In this section we will describe our proposed architecture for a transformer with latent recurrent depth, discussing design choices and small-scale ablations. A diagram of the architecture can be found in Figure [2](#). We always refer to the sequence dimension as n, the hidden dimension of the model as h, and its vocabulary as the set V .

## Macroscopic Design

The model is primarily structured around decoder-only transformer blocks [(Vaswani et al., 2017;](#b157)[Radford et al., 2019)](#b120). However these blocks are structured into three functional groups, the prelude P , which embeds the input data into a latent space using multiple transformer layers, then the core recurrent block R, which is the central unit of recurrent computation modifying states s ‚àà R n√óh , and finally the coda C, which un-embeds from latent space using several layers and also contains the prediction head of the model. The core block is set between the prelude and coda blocks, and by looping the core we can put an indefinite amount of verses in our song.

P "Hello" "World"

$Input Injection Residual Stream Prelude Recurrent Block Coda ùí©(0,œÉ 2 I n‚ãÖh ) e s 0 R R s 1 e ‚Ä¶ R C s R p e$Figure [2](#): A visualization of the Architecture, as described in Section 3. Each block consists of a number of sub-layers. The blue prelude block embeds the inputs into latent space, where the green shared recurrent block is a block of layers that is repeated to compute the final latent state, which is decoded by the layers of the red coda block.

Given a number of recurrent iterations r, and a sequence of input tokens x ‚àà V n these groups are used in the following way to produce output probabilities p ‚àà R n√ó|V | e = P (x)

$s 0 ‚àº N (0, œÉ 2 I n‚Ä¢h ) s i = R(e, s i-1 ) for i ‚àà {1, . . . , r} p = C(s r ),$where œÉ is some standard deviation for initializing the random state. This process is shown in Figure [2](#). Given an init random state s 0 , the model repeatedly applies the core block R, which accepts the latent state s i-1 and the embedded input e and outputs a new latent state s i . After finishing all iterations, the coda block processes the last state and produces the probabilities of the next token. This architecture is based on deep thinking literature, where it is shown that injecting the latent inputs e in every step [(Bansal et al., 2022)](#b14) and initializing the latent vector with a random state stabilizes the recurrence and promotes convergence to a steady state independent of initialization, i.e. path independence [(Anil et al., 2022)](#b6).

Motivation for this Design. This recurrent design is the minimal setup required to learn stable iterative operators. A good example is gradient descent of a function E(x, y), where x may be the variable of interest and y the data. Gradient descent on this function starts from an initial random state, here x 0 , and repeatedly applies a simple operation (the gradient of the function it optimizes), that depends on the previous state x k and data y. Note that we need to use y in every step to actually optimize our function. Similarly we repeatedly inject the data e in our set-up in every step of the recurrence. If e was provided only at the start, e.g. via s 0 = e, then the iterative process would not be stable 1 , as its solution would depend only on its boundary conditions. The structure of using several layers to embed input tokens 1 Stable in the sense that R cannot be a monotone operator if it does not depend on e, and so cannot represent gradient descent on strictly convex, data-dependent functions, [(Bauschke et al., 2011)](#b15) into a hidden latent space is based on empirical results analyzing standard fixed-depth transformers [(Skean et al., 2024;](#b139)[Sun et al., 2024;](#b145)[Kaplan et al., 2024)](#b75). This body of research shows that the initial and the end layers of LLMs are noticeably different, whereas middle layers are interchangeable and permutable. For example, [Kaplan et al. (2024)](#b75) show that within a few layers standard models already embed sub-word tokens into single concepts in latent space, on which the model then operates. Remark 3.1 (Is this a Diffusion Model?). This iterative architecture will look familiar to the other modern iterative modeling paradigm, diffusion models [(Song and Ermon, 2019)](#b142), especially latent diffusion models [(Rombach et al., 2022)](#b123). We ran several ablations with iterative schemes even more similar to diffusion models, such as s i = R(e, s i-1 ) + n where n ‚àº N (0, œÉ i I n‚Ä¢h ), but find the injection of noise not to help in our preliminary experiments, which is possibly connected to our training objective. We also evaluated and s i = R i (e, s i-1 ), i.e. a core block that takes the current step as input [(Peebles and Xie, 2023)](#b119), but find that this interacts badly with path independence, leading to models that cannot extrapolate.

## Microscopic Design

Within each group, we broadly follow standard transformer layer design. Each block contains multiple layers, and each layer contains a standard, causal self-attention block using RoPE [(Su et al., 2021)](#b143) with a base of 50000, and a gated SiLU MLP [(Shazeer, 2020)](#b135). We use RMSNorm [(Zhang and Sennrich, 2019)](#b177) as our normalization function. The model has learnable biases on queries and keys, and nowhere else. To stabilize the recurrence, we order all layers in the following "sandwich" format, using norm layers n i , which is related, but not identical to similar strategies in [(Ding et al., 2021;](#b44)[Team Gemma et al., 2024)](#b152):

$xl =n 2 (x l-1 + Attn(n 1 (x l-1 ))) x l =n 4 ( xl + MLP(n 3 ( xl )))$While at small scales, most normalization strategies, e.g. pre-norm, post-norm and others, work almost equally well, we ablate these options and find that this normalization is required to train the recurrence at scale[foot_3](#foot_3) .

Given an embedding matrix E and embedding scale Œ≥, the prelude block first embeds input tokens x as Œ≥E(x), and then to applies l P many prelude layers with the layout described above.

Our core recurrent block R starts with an adapter matrix A : R 2h ‚Üí R h mapping the concatenation of s i and e into the hidden dimension h [(Bansal et al., 2022)](#b14). While re-incorporation of initial embedding features via addition rather than concatenation works equally well for smaller models, we find that concatenation works best at scale. This is then fed into l R transformer layers. At the end of the core block the output is again rescaled with an RMSNorm n c .

The coda contains l C layers, normalization by n c , and projection into the vocabulary using tied embeddings E T .

In summary, we can summarize the architecture by the triplet (l P , l R , l C ), describing the number of layers in each stage, and by the number of recurrences r, which may vary in each forward pass. We train a number of small-scale models with shape (1, 4, 1) and hidden size h = 1024, in addition to a large model with shape (2, 4, 2) and h = 5280. This model has only 8 "real" layers, but when the recurrent block is iterated, e.g. 32 times, it unfolds to an effective depth of 2 + 4r + 2 = 132 layers, constructing computation chains that can be deeper than even the largest fixed-depth transformers [(Levine et al., 2021;](#)[Merrill et al., 2022)](#b106).

## Training Objective

Training Recurrent Models through Unrolling. To ensure that the model can function when we scale up recurrent iterations at test-time, we randomly sample iteration counts during training, assigning a random number of iterations r to every input sequence [(Schwarzschild et al., 2021b)](#). We optimize the expectation of the loss function L over random samples x from distribution X and random iteration counts r from distribution Œõ.

$L(Œ∏) = E x‚ààX E r‚àºŒõ L (m Œ∏ (x, r), x ‚Ä≤ ) .$Here, m represents the model output, and x ‚Ä≤ is the sequence x shifted left, i.e., the next tokens in the sequence x. We choose Œõ to be a log-normal Poisson distribution. Given a targeted mean recurrence r + 1 and a variance that we set to œÉ = 1 2 , we can sample from this distribution via  given the normal distribution N and Poisson distribution P, see Figure [3](#fig_1). The distribution most often samples values less than r, but it contains a heavy tail of occasional events in which significantly more iterations are taken.

$œÑ ‚àº N (log(r) - 1 2 œÉ 2 , œÉ)(1)$$r ‚àº P(e œÑ ) + 1,(2)$Truncated Backpropagation. To keep computation and memory low at train time, we backpropagate through only the last k iterations of the recurrent unit. This enables us to train with the heavy-tailed Poisson distribution Œõ, as maximum activation memory and backward compute is now independent of r. We fix k = 8 in our main experiments. At small scale, this works as well as sampling k uniformly, but with set fixed, the overall memory usage in each step of training is equal. Note that the prelude block still receives gradient updates in every step, as its output e is injected in every step. This setup resembles truncated backpropagation through time, as commonly done with RNNs, although our setup is recurrent in depth rather than time [(Williams and Peng, 1990;](#b161)[Mikolov et al., 2011)](#b108).

## Training a large-scale recurrent-depth Language Model

After verifying that we can reliably train small test models up to 10B tokens, we move on to larger-scale runs. Given our limited compute budget, we could either train multiple tiny models too small to show emergent effects or scaling, or train a single medium-scale model. Based on this, we prepared for a single run, which we detail below.

## Training Setup

We describe the training setup, separated into architecture, optimization setup and pretraining data. We publicly release all training data, pretraining code, and a selection of intermediate model checkpoints. Pretraining Data. Given access to only enough compute for a single large scale model run, we opted for a dataset mixture that maximized the potential for emergent reasoning behaviors, not necessarily for optimal benchmark per-generic-text: 28.71% code: 25.36% scientific-text: 18.73% synthetic-text: 8.14% longform-text: 7.50% math: 6.14% generic-instruct: 2.09% Q&A-text: 1.58% math-instruct: 1.51% writing-instruct: 0.12% misc-reasoning: 0.11% Figure 4: Distribution of data sources that are included during training. The majority of our data is comprised of generic webtext, scientific writing and code.

formance. Our final mixture is heavily skewed towards code and mathematical reasoning data with (hopefully) just enough general webtext to allow the model to acquire standard language modeling abilities. All sources are publicly available. We provide an overview in Figure [4](#). Following [Allen-Zhu and Li (2024)](#b1), we directly mix relevant instruction data into the pretraining data. However, due to compute and time constraints, we were not able to ablate this mixture.

We expect that a more careful data preparation could further improve the model's performance. We list all data sources in Appendix C.

Tokenization and Packing Details. We construct a vocabulary of 65536 tokens via BPE [(Sennrich et al., 2016)](#b133), using the implementation of Dagan (2024). In comparison to conventional tokenizer training, we construct our tokenizer directly on the instruction data split of our pretraining corpus, to maximize tokenization efficiency on the target domain. We also substantially modify the pre-tokenization regex (e.g. of [Dagan et al. (2024)](#)) to better support code, contractions and LaTeX. We include a <|begin_text|> token at the start of every document. After tokenizing our pretraining corpus, we pack our tokenized documents into sequences of length 4096. When packing, we discard document ends that would otherwise lack previous context, to fix an issue described as the "grounding problem" in [Ding et al. (2024)](#b43), aside from several long-document sources of mathematical content, which we preserve in their entirety.

Architecture and Initialization. We scale the architecture described in Section 3, setting the layers to (2, 4, 2), and train with a mean recurrence value of r = 32. We mainly scale by increasing the hidden size to h = 5280, which yields 55 heads of size of 96. The MLP inner dimension is 17920 and the RMSNorm Œµ is 10 -6 . Overall this model shape has about 1.5B parameters in non-recurrent prelude and head, 1.5B parameters in the core recurrent block, and 0.5B in the tied input embedding.

At small scales, most sensible initialization schemes work.

However, at larger scales, we use the initialization of [Takase et al. (2024)](#b150) which prescribes a variance of œÉ 2 h = 2 5h . We initialize all parameters from a truncated normal distribution (truncated at 3œÉ) with this variance, except all outprojection layers, where the variance is set to œÉ 2 out = 1 5hl , for l = l P + rl R + l C the number of effective layers, which is 132 for this model. As a result, the out-projection layers are initialized with fairly small values [(Goyal et al., 2018)](#b59). The output of the embedding layer is scaled by ‚àö h. To match this initialization, the state s 0 is also sampled from a truncated normal distribution, here with variance œÉ 2 s = 2 5 .

Locked-Step Sampling. To enable synchronization between parallel workers, we sample a single depth r for each micro-batch of training, which we synchronize across workers (otherwise workers would idle while waiting for the model with the largest r to complete its backward pass). We verify at small scale that this modification improves compute utilization without impacting convergence speed, but note that at large batch sizes, training could be further improved by optimally sampling and scheduling independent steps r on each worker, to more faithfully model the expectation over steps in Equation ( [1](#formula_4)).

Optimizer and Learning Rate Schedule. We train using the Adam optimizer with decoupled weight regularization (Œ≤ 1 = 0.9, Œ≤ 2 = 0.95, Œ∑ = 5 √ó 10 -[foot_5](#foot_5) ) (Kingma and Ba, 2015; [Loshchilov and Hutter, 2017)](#b99), modified to include update clipping [(Wortsman et al., 2023b)](#) and removal of the Œµ constant as in [Everett et al. (2024)](#b47). We clip gradients above 1. We train with warm-up and a constant learning rate [(Zhai et al., 2022;](#b176)[Geiping and Goldstein, 2023)](#b132), warming up to our maximal learning rate within the first 4096 steps.

## Compute Setup and Hardware

We train this model using compute time allocated on the Oak Ridge National Laboratory's Frontier supercomputer. This HPE Cray system contains 9408 compute nodes with AMD MI250X GPUs, connected via 4xHPE Slingshot-11 NICs. The scheduling system is orchestrated through SLURM. We train in bfloat16 mixed precision using a PyTorch-based implementation [(Zamirai et al., 2021)](#b173).

Device Speed and Parallelization Strategy. Nominally, each MI250X chip[foot_4](#foot_4) achieves 192 TFLOP per GPU [(AMD, 2021)](#b3). For a single matrix multiplication, we measure a maximum achievable speed on these GPUs of 125 TFLOP/s on our software stack (ROCM 6.2.0, PyTorch 2.6 prerelease 11/02) [(Bekman, 2023)](#b17). Our implementation, using extensive PyTorch compilation and optimization of the hidden dimension to h = 5280 achieves a single-node training speed of 108.75 TFLOP/s, i.e. 87% AFU ("Achievable Flop Utilization"). Due to the weight sharing inherent in our recurrent design, even our largest model is still small enough to be trained using only data (not tensor) parallelism, with only optimizer sharding [(Rajbhandari et al., 2020)](#b122) and gradient checkpointing on a per-iteration granularity. With a batch size of 1 per GPU we end up with a global batch size of 16M tokens per step, minimizing inter-GPU communication bandwidth.

When we run at scale on 4096 GPUs, we achieve 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per second. To achieve this, we wrote a hand-crafted distributed data parallel implementation to circumvent a critical AMD interconnect issue, which we describe in more detail in Appendix A.2. Overall, we believe this may be the largest language model training run to completion in terms of number of devices used in parallel on an AMD cluster, as of time of writing.

Training Timeline. Training proceeded through 21 segments of up to 12 hours, which scheduled on Frontier mostly in early December 2024. We also ran a baseline comparison, where we train the same architecture but in a feedforward manner with only 1 pass through the core/recurrent block. This trained with the same setup for 180B tokens on 256 nodes with a batch size of 2 per GPU. Ultimately, we were able to schedule 795B tokens of pretraining of the main model. Due to our constant learning rate schedule, we were able to add additional segments "on-demand", when an allocation happened to be available.

## Importance of Norms and Initializations at Scale

At small scales all normalization strategies worked, and we observed only tiny differences between initializations. The same was not true at scale. The first training run we started was set up with the same block sandwich structure as described above, but parameter-free RMSNorm layers, no embedding scale Œ≥, a parameter-free adapter A(s, e) = s + e, and a peak learning rate of 4 √ó 10 -[foot_6](#foot_6) . As shown in Figure [5](#fig_2), this run ("Bad Run 1", orange), quickly stalled.

While the run obviously stopped improving in training loss (left plot), we find that this stall is due to the model's representation collapsing [(Noci et al., 2022)](#b115). The correlation of hidden states in the token dimension quickly goes to 1.0 (middle plot), meaning the model predicts the same hidden state for every token in the sequence. We find that this is an initialization issue that arises due to the recurrence operation. Every iteration of the recurrence block increases token correlation, mixing the sequence until collapse.

We attempt to fix this by introducing the embedding scale factor, switching back to a conventional pre-normalization block, and switching to the learned adapter. Initially, these changes appear to remedy the issue. Even though token correlation shoots close to 1.0 at the start ("Bad Run 2", green), the model recovers after the first 150 steps. However, we quickly find that this training run is not able to leverage test-time compute effectively (right plot), as validation perplexity is the same whether 1 or 32 recurrences are used. This initialization and norm setup has led to a local minimum as the model has learned early to ignore the incoming state s, preventing further improvements.

In a third, and final run ("Main", blue), we fix this issue by reverting back to the sandwich block format, and further dropping the peak learning rate to 4 √ó 10 -5 . This run starts smoothly, never reaches a token correlation close to 1.0, and quickly overtakes the previous run by utilizing the recurrence and improving with more iterations.

With our successful configuration, training continues smoothly for the next 750B tokens without notable interruptions or loss spikes. We plot training loss and perplexity at different recurrence steps in Figure [6](#). In our material, we refer to the final checkpoint of this run as our "main model", which we denote as Huginn-0125 4 .

10 8 10 9 10 10 10 11 10 12 Tokens (log) 10 1 10 2 10 3 10 4 Step (log) 5 10 Loss 10 10 10 11 10 12 Tokens (log) 10 2 10 3 10 4 Step (log) 10 1 10 2 10 3 Validation Perplexity (log) Recurrence 1 4 8 16 32 64 Figure 6: Left: Plot of pretrain loss over the 800B tokens on the main run. Right: Plot of val ppl at recurrent depths 1, 4, 8, 16, 32, 64.

During training, the model improves in perplexity on all levels of recurrence.

Table [1](#): Results on lm-eval-harness tasks zero-shot across various open-source models. We show ARC [(Clark et al., 2018)](#b31), HellaSwag [(Zellers et al., 2019)](#b175), MMLU [(Hendrycks et al., 2021a)](#), OpenBookQA [(Mihaylov et al., 2018)](#b107), PiQA [(Bisk et al., 2020)](#b21), SciQ (Johannes Welbl, 2017), and WinoGrande [(Sakaguchi et al., 2021)](#b124). We report normalized accuracy when provided.

Model Param Tokens ARC-E ARC-C HellaSwag MMLU OBQA PiQA SciQ WinoGrande random 25.0 25.0 25.0 25.0 25.0 50.0 25.0 50.0 Amber 7B 1.2T 65.70 37.20 72.54 26.77 41.00 78.73 88.50 63.22 Pythia-2.8b 2.8B 0.3T 58.00 32.51 59.17 25.05 35.40 73.29 83.60 57.85 Pythia-6.9b 6.9B 0.3T 60.48 34.64 63.32 25.74 37.20 75.79 82.90 61.40 Pythia-12b 12B 0.3T 63.22 34.64 66.72 24.01 35.40 75.84 84.40 63.06 OLMo-1B 1B 3T 57.28 30.72 63.00 24.33 36.40 75.24 78.70 59.19 OLMo-7B 7B 2.5T 68.81 40.27 75.52 28.39 42.20 80.03 88.50 67.09 OLMo-7B-0424 7B 2.05T 75.13 45.05 77.24 47.46 41.60 80.09 96.00 68.19 OLMo-7B-0724 7B 2.75T 74.28 43.43 77.76 50.18 41.60 80.69 95.70 67.17 OLMo-2-1124 7B 4T 82.79 57.42 80.50 60.56 46.20 81.18 96.40 74.74 Ours, (r = 4) 3.5B 0.8T 49.07 27.99 43.46 23.39 28.20 64.96 80.00 55.24 Ours, (r = 8) 3.5B 0.8T 65.11 35.15 58.54 25.29 35.40 73.45 92.10 55.64 Ours, (r = 16) 3.5B 0.8T 69.49 37.71 64.67 31.25 37.60 75.79 93.90 57.77 Ours, (r = 32) 3.5B 0.8T 69.91 38.23 65.21 31.38 38.80 76.22 93.50 59.43

## Benchmark Results

We train our final model for 800B tokens, and a nonrecurrent baseline for 180B tokens. We evaluate these checkpoints against other open-source models trained on fully public datasets (like ours) of a similar size. We compare against Amber [(Liu et al., 2023c)](#), Pythia [(Biderman et al., 2023)](#b8) and a number of OLMo 1&2 variants [(Groeneveld et al., 2024;](#b62)[AI2, 2024;](#)[Team OLMo et al., 2025)](#). We execute all standard benchmarks through the lm-eval harness [(Biderman et al., 2024)](#) and code benchmarks via bigcode-bench (Zhuo et al., 2024).

## Standard Benchmarks

Overall, it is not straightforward to place our model in direct comparison to other large language models, all of which are small variations of the fixed-depth transformer architecture.

While our model has only 3.5B parameters and hence requires only modest interconnect bandwidth during pretraining, it chews through raw FLOPs close to what a 32B parameter transformer would consume during pretraining, and can continuously improve in performance with test-time scaling up to FLOP budgets equivalent to a standard 50B parameter fixed-depth transformer. It is also important to note a few caveats of the main training run when interpreting the results. First, our main checkpoint is trained for only 47000 steps on a broadly untested mixture, and the learning rate is never cooled down from its peak. As an academic project, the model is trained only on publicly available data and the 800B token count, while large in comparison to older fully open-source models such as the Pythia series, is small in comparison to modern open-source efforts such as OLMo, and tiny in comparison to the datasets used to train industrial open-weight models.

Disclaimers aside, we collect results for established benchmark tasks [(Team OLMo et al., 2025)](#) in Table [1](#) and show all models side-by-side. In direct comparison we see that our model outperforms the older Pythia series and is roughly comparable to the first OLMo generation, OLMo-7B in most metrics, but lags behind the later OLMo models trained larger, more carefully curated datasets. For the first recurrent-depth model for language to be trained at this  scale, and considering the limitations of the training run, we find these results promising and certainly suggestive that further research into latent recurrence as an approach to test-time scaling is warranted.

## Math and Coding Benchmarks

We also evaluate the model on math and coding. For math, we evaluate GSM8k [(Cobbe et al., 2021)](#b32) (as zero-shot and in the 8-way CoT setup), MATH ( [(Hendrycks et al., 2021b)](#) with the Minerva evaluation rules [(Lewkowycz et al., 2022)](#b89)) and MathQA [(Amini et al., 2019)](#b4). For coding, we check MBPP [(Austin et al., 2021)](#) and HumanEval [(Chen et al., 2021)](#b27). Here we find that our model significantly surpasses all models except the latest OLMo-2 model in mathematical reasoning, as measured on GSM8k and MATH. On coding benchmarks the model beats all other general-purpose opensource models, although it does not outperform dedicated code models, such as StarCoder2 [(Lozhkov et al., 2024)](#b18), trained for several trillion tokens. We also note that while further improvements in language modeling are slowing down, as expected at this training scale, both code and mathematical reasoning continue to improve steadily throughout training, see Figure [8](#).

## Where does recurrence help most?

How much of this performance can we attribute to recurrence, and how much to other factors, such as dataset, tokenization and architectural choices? In Table [4](#tab_8), we compare our recurrent model against its non-recurrent twin, which we trained to 180B tokens in the exact same setting. In direct comparison of both models at 180B tokens, we see that the recurrent model outperforms its baseline with an especially pronounced advantage on harder tasks, such as the ARC challenge set. On other tasks, such as SciQ, which requires straightforward recall of scientific facts, performance of the models is more similar. We observe that gains through reasoning are especially prominent on GSM8k, where the 180B recurrent model is already 5 times better than the baseline at this early snapshot in the pretraining  ), and HumanEval (pass@1). As we increase compute, the performance on these benchmarks increases. HellaSwag only needs 8 recurrences to achieve near peak performance while other benchmarks make use of more compute.

process. We also note that the recurrent model, when evaluated with only a single recurrence, effectively stops improving between the early 180B checkpoint and the 800B checkpoint, showing that further improvements are not built into the prelude or coda non-recurrent layers but encoded entirely into the iterations of the recurrent block.

Further, we chart the improvement as a function of test-time compute on several of these tasks for the main model in Figure [7](#fig_4). We find that saturation is highly task-dependent, on easier tasks the model saturates quicker, whereas it benefits from more compute on others.

## Recurrence and Context

We evaluate ARC-C performance as a function of recurrence and number of few-shot examples in the context in Figure [9](#fig_5). Interestingly, without few-shot examples to consider, the model saturates in compute around 8-12 iterations. However, when more context is given, the model can reason about more information in context, which it does, saturating around 20 iterations if 1 example is provided, and 32 iterations, if 25-50 examples are provided, mirroring generalization improvements shown for recurrence [(Yang et al., 2024a;](#)[Fan et al., 2025)](#b50). Similarly,  we see that if we re-evaluate OBQA in Table [5](#tab_9), but do not run the benchmark in the default lm-eval "closed-book" format and rather provide a relevant fact, our recurrent model improves significantly almost closing the gap to OLMo-2. Intuitively this makes sense, as the recurrent models has less capacity to memorize facts but more capacity to reason about its context.

## Improvements through Weight Averaging

Due to our constant learning rate, we can materialize further improvements through weight averaging [(Izmailov et al., 2018)](#b71) to simulate the result of a cooldown [(H√§gele et al., 2024;](#b63)[DeepSeek-AI et al., 2024)](#b40). We use an exponen- tial moving average starting from our last checkpoint with Œ≤ = 0.9, incorporating the last 75 checkpoints with a dilation factor of 7, a modification to established protocols [(Kaddour, 2022;](#b74)[Sanyal et al., 2024)](#b126). We provide this EMA model as well, which further improves GMS8k performance to 47.23% flexible (38.59% strict), when tested at r = 64.

## Recurrent Depth simplifies LLMs

Aside from encouraging performance in mathematical and code reasoning, recurrent-depth models turn out to be surprisingly natural tools to support a number of methods that require substantial effort with standard transformers. In the next section, we provide a non-exhaustive overview. We see that the model converges quicker on high school mathematics than tasks such as logical fallacies or moral scenarios. On some tasks, such as philosophy, the model is able to effectively re-use states in its latent CoT and converge quickly on a subset of tokens, leading to fewer steps required overall.

## Zero-Shot Adaptive Compute at Test-Time

We have shown that the model is capable of varying compute on a per-query level, running the model in different recurrence modes. This is after all also how the model is trained, as in Equation ( [1](#formula_4)). However, it would be more efficient in practice to stop recurring early when predictions are easy, and only spend compute on hard decisions. Other work, especially when based on standard transformers, requires models trained specifically for early exits [(Elbayad et al., 2019;](#b45)[Fan et al., 2019;](#b48)[Banino et al., 2021)](#b13), or models finetuned with exit heads on every layer [(Schuster et al., 2022)](#b128). To test our model's zero-shot exit abilities, we choose a simple exit criterion to evaluate convergence, the KL-divergence between two successive steps. If this divergence falls below 5 √ó 10 -4 , we stop iterating, sample the output token, and move to generate the next token.

We show this zero-shot per-token adaptive compute behavior in Figure [10](#fig_6), where we plot the distribution of steps taken before the exit condition is hit. We do this for the first 50 questions from different MMLU categories, asked in free-form chat. Interestingly, the number of steps required to exit differs notably between categories, with the model exiting earlier on high school mathematics, but taking on average 3.5 steps more on moral scenarios. As a preliminary demonstration, we verify on MTBench that this adaptivity does not significantly impact performance in a conversational benchmark setting (standard: 5.63, early exits: 5.56 see Appendix Table [6](#tab_11)).

Remark 6.1 (What about missing KV-cache entries?). Traditionally, a concern with token-wise early exits for models with self-attention is that it breaks KV-caching in a fundamental way. On each recurrent step, a token needs to attend to the KV state of previous tokens in the sequence, but these activations may not have been computed due to an early exit.

A na√Øve fix would be to pause generating and recompute all missing hidden states, but this would remove some of the benefit of early stopping. Instead, as in [Elbayad et al. (2019)](#b45), we attend to the last, deepest available KV states in the cache. Because all recurrent KV cache entries are generated by the same K,V projection matrices from successive hidden states, they "match", and therefore the model is able to attend to the latest cache entry from every previous token, even if computed at different recurrent depths.

## Zero-Shot KV-cache Sharing

A different avenue to increase efficiency is to reduce the memory footprint of the KV-cache by sharing the cache between layers (character.ai, 2024; [Brandon et al., 2024)](#b24). Typically, transformers must be trained from scratch with this capability. However, as discussed in the previous section, we find that we can simply share KV-caches in our model with minimal impact to performance. We set a fixed KV-cache budget for the recurrence at every token k, and at iteration i, read and write the cache entry i mod k. For example, we set a maximum KV-cache budget of 16 steps, overwriting the KV-cache of the 1st step when executing the 17th step, and so forth. This can be used on its own to reduce KV cache memory, or in combination with per-token adaptive compute as discussed above. On MTBench, this does not reduce performance (cache budget of 4: 5.86, see Appendix Table [6](#tab_11)).

## Zero-Shot Continuous Chain-of-Thought

By attending to the output of later steps of previous tokens in the early steps of current tokens, as described in the KV-cache sharing section, we actually construct a computation that is deeper than the current number of recurrence steps. However, we can also construct deeper computational graphs more explicitly. Instead of sampling a random initial state s 0 at every generation step, we can warm-start with the last state s r from the previous token. This way, the model can benefit from latent information encoded at the Shown is an unsafe question posed to the model. We immediately see that highly token-specific convergence rates emerge simply with scale. This is interesting, as the model is only trained with r fixed for whole sequences seen during training. We see that convergence is especially slow on the key part of the question, really wrong-ed.We further see that the model also learns different behaviors, we see an oscillating pattern in latent space, here most notably for the school token. Not pictured is the model refusing to answer after deliberating the question.

previous generation step, and further improve. As shown in Figure [10](#fig_6), this reduces the average number of steps required to converge by 1-2. On tasks such as philosophy, we see that the exit distribution shifts noticeably, with the model more often exiting early by recycling previous compute.

This is closely related to the continuous chain of thought approach explored in [(Hao et al., 2024)](#b64), in the sense that it is an intervention to the trained model to add additional recurrence. To achieve a similar behavior in fixed-depth transformers, [Hao et al. (2024)](#b64) train models on reasoning chains to accept their last hidden state as alternative inputs when computing the next token. Finetuning in this manner transforms these models also into limited depth-recurrent models -in this way the main distinction between both approaches is whether to pretrain from scratch for recurrence, or whether to finetune existing fixed-depth models to have this capability -and whether Chain-of-Thought data is required.

## Zero-Shot Self-Speculative Decoding

Recurrent-depth models can also inherently generate text more efficiently by using speculative decoding [(Leviathan et al., 2023)](#b87) without the need for a separate draft model.

With standard transformer models, speculative decoding requires an external draft model, Medusa heads [(Cai et al., 2024)](#b26), or early-exit adaptation [(Zhang et al., 2024b;](#)[Elhoushi et al., 2024)](#b46). [Zhang et al. (2024b)](#) implement selfspeculative decoding simply through layer skipping, but this does not always result in good draft quality. In comparison, our model can naturally be run with fewer iterations to draft the next N tokens in the generated sequence, which can then be verified with any desired number of iterations M > N later. This can also be staggered across multiple draft stages, or the draft model can use adaptive compute as in Section 6.1. Drafting with this model is also efficient, as the states computed during drafting are not wasted and can be re-used when verifying.

## What Mechanisms Emerge at Scale in

Recurrent-Depth Models Figure [12](#): Latent Space trajectories for select tokens. We show a small part of these high-dimensional trajectories by visualizing the first 6 PCA directions, computing the PCA over all latent state trajectories of all tokens in a sequence. The color gradient going from dark to bright represents steps in the trajectory. The center of mass is marked in red. While on many tokens, the state simply converges (top row), the model also learns to use orbits (middle row), and "sliders" (bottom row, middle), which we observe being used to represent and handle more advanced concepts, such as arithmetic or complicated deliberation. patterns emerge, simply by training this model at scale. In comparison to previous work, such as [Bai et al. (2019)](#b10), where the training objective directly encodes a prior that pushes trajectories to a fixed point, we only train with our truncated unrolling objective.

Figure [11](#fig_7) shows the norm distance ||s i -s * || between each s i in a trajectory and an approximate limit point s * computed with 128 iterations. We show the sentence top to bottom and iterations from left to right. We clearly see that convergence behavior depends on context. We see that key parts of the question, and the start of the model response, are "deliberated" much more in latent space. The context dependence can also be seen in the different behavior among the three identical tokens representing each of the three dots. Also note that the distance to s * does not always decrease monotonically (e.g. for school); the model may also trace out complicated orbits in its latent trajectory while processing information, even though this is not represented explicitly in our training objective.

We look at trajectories for select tokens in more detail in Figure [12](#). We compute a PCA decomposition of latent trajectories over all tokens in a sequence, and then show several individual trajectories projected onto the first six PCA directions. See the appendix for more examples. Many tokens simply converge to a fixed point, such as the token in the top row. Yet, for harder questions, such as in the 2nd row[foot_8](#foot_8) , the state of the token quickly falls into an orbit pattern in all three pairs of PCA directions. The use of multi-dimensional orbits like these could serve a similar purpose to periodic patterns sometimes observed in fixed-depth transformers trained for arithmetic tasks [(Nanda et al., 2022)](#b114), but we find these patterns extend far beyond arithmetic for our model. We often also observe the use of orbits on tokens such as "makes" (see Figure [16](#)) or "thinks" that determine the structure of the response.

Aside from orbits, we also observe the model encoding particular key tokens as "sliders", as seen in the middle of the bottom row in Figure [12](#) (which is the token "wrong", from the same message as already shown in Figure [11](#fig_7)). In these motions the trajectory noticeably drifts in a single direction, which the model could use to implement a mechanism to count how many iterations have occurred.

The emergence of structured trajectories in latent space gives us a glimpse into how the model performs its computations. Unlike the discrete sequential chain of reasoning seen in verbalized chain-of-thought approaches, we observe rich geometric patterns including orbits, convergent paths, and drifts -means to organize its computational process spatially. This suggests the model is independently learning to leverage the high-dimensional nature of its latent space to implement reasoning in new ways.

Path Independence. We verify that our models maintain path independence, in the sense of [Anil et al. (2022)](#b6), despite their complex, learned dynamics, which we discussed prior (see also the additional examples in Appendix Figure [22](#fig_18)). When re-initializing from multiple starting states s 0 , the model moves in similar trajectories, exhibiting consistent behavior. The same orbital patterns, fixed points, or directional drifts emerge regardless of initialization.

## Related Work Overview

The extent to which recurrence is a foundational concept of machine learning is hard to overstate [(Amari, 1972;](#b2)[Hopfield, 1982;](#b69)[Braitenberg, 1986;](#b23)[Gers and Schmidhuber, 2000;](#b57)[Sutskever et al., 2008)](#b147). Aside from using recurrence to move along sequences, as in recurrent neural networks, it was understood early to also be the key to adaptive computation [(Schmidhuber, 2012;](#b127)[Graves, 2017)](#b60). For transformers, recurrence was applied in [Dehghani et al. (2019)](#b41), who highlight the aim of recurrent depth to model universal, i.e. Turing-complete, machines [(Graves et al., 2014)](#b61). It was used at scale (but with fixed recurrence) in [Lan et al. (2019)](#b83)[(2024) and](#)[McLeish et al. (2024)](#b105) show that depth recurrence is advantageous when learning generalizable algorithms when training with randomized unrolling and input injections. Recent work has described depth-recurrent, looped, transformers and studied their potential benefits with careful theoretical and small-scale analysis [(Giannou et al., 2023;](#b58)[Gatmiry et al., 2024;](#b55)[Yang et al., 2024a;](#)[Fan et al., 2025)](#b50).

From another angle, these models can be described as neural networks learning a fixed-point iteration, as studied in deep equilibrium models [(Bai et al., 2019;](#b10)[2022)](#). They are further related to diffusion models [(Song and Ermon, 2019)](#b142), especially latent diffusion models [(Rombach et al., 2022)](#b123), but we note that language diffusion models are usually run with a per-sequence, instead of a per-token, iteration count [(Lee et al., 2018)](#b86). A key difference of our approach to both equilibrium models and diffusion models is in the training objective, where equilibrium methods solve the "direct" problem [(Geiping and Moeller, 2019)](#b56), diffusion models solve a surrogate training objective, and our work suggests that truncated unrolling is a scalable alternative.

More generally, all architectures that recur in depth can also be understood as directly learning the analog to the gradient of a latent energy-based model [(LeCun and Huang, 2005;](#b85)[LeCun, 2022)](#b84), to an implicitly defined intermediate optimization layer [(Amos and Kolter, 2017)](#b5), or to a Kuramoto layer [(Miyato et al., 2024)](#b109). Analogies to gradient descent at inference time also show the connection to test time adaptation [(Sun et al., 2020)](#b146), especially test-time adaptation of output states [(Boudiaf et al., 2022)](#b22).

Aside from full recurrent-depth architectures, there also exist a number of proposals for hybrid architectures, such as models with latent sub-networks [(Li et al., 2020a)](#), LoRA adapters on top of weight-shared layers [(Bae et al., 2024)](#b9), or (dynamic) weight-tying of trained models [(Hay and Wolf, 2023;](#b65)[Liu et al., 2024b)](#).

As mentioned in Section 6, while we consider the proposed recurrent depth approach to be a very natural way to learn to reason in continuous latent space from the ground up, the works of [Hao et al. (2024)](#b64); [Cheng and Durme (2024)](#b28) and [Liu et al. (2024a)](#) discuss how to finetune existing fixeddepth transformers with this capability. These works have a similar aim to ours, enabling reasoning in latent space, but approach this goal from separate directions.

For additional discussions related to the idea of constructing a prior that incentivizes reasoning and algorithm learning at the expense of memorization of simple patterns, we also refer to Chollet (2019), [Schwarzschild (2023)](#b129), [Li et al. (2020b)](#) and [Moulton (2023)](#b110).

## Future Work

Aside from work extending and analyzing the scaling behaviors of recurrent depth models, there are many questions that remain unanswered. For example, to us, there are potentially a large number of novel post-training schemes that further enhance the capabilities of these models, such as fine-tuning to compress the recurrence or reinforcement learning with data with different hardness levels [(Zelikman et al., 2024)](#b174), or to internalize reasoning from CoT data into the recurrence [(Deng et al., 2024)](#b42).

Another aspect not covered in this work is the relationship to other modern architecture improvements. Efficient sequence mixing operations, especially those that are linear in sequence dimension, such as linear attention [(Katharopoulos et al., 2020;](#b77)[Yang et al., 2024b)](#), are limited in the number of comparisons that can be made. However, with recurrent depth, blocks containing linear operators can repeat until all necessary comparisons between sequence elements are computed [(Suzgun et al., 2019)](#b148). For simplicity, we also focus on a single recurrence, where prior work has considered multiple successive recurrent stages [(Takase and Kiyono, 2023;](#b149)[Csord√°s et al., 2024)](#b34).

Finally, the proposed architecture is set up to be computeheavy, with more "materialized" parameters than there are actual parameters. This naturally mirrors mixture-of-expert models (MoE), which are parameter-heavy, using fewer active parameters per forward pass than exist within the model [(Shazeer et al., 2017;](#b136)[Fedus et al., 2022)](#b51). We posit that where the recurrent-depth setup excels at learning reasoning patterns, the MoE excels at effectively storing and retrieving complex information. Their complementarity supports the hypothesis that a future architecture would contain both modifications. While in a standard MoE model, each expert can only be activated once per forward pass, or skipped entirely, a recurrent MoE model could also refine its latent state over multiple iterations, potentially routing to the same expert multiple times, before switching to a different one [(Tan et al., 2023;](#b151)[Csord√°s et al., 2024)](#b34). While MoE models are the currently leading solution to implement this type of "memory" in dense transformers, these considerations also hold for other memory mechanisms suggested for LLMs [(Sukhbaatar et al., 2019;](#b144)[Fan et al., 2021;](#b49)[Wu et al., 2022;](#b165)[He et al., 2024)](#b66).

## Conclusions

The models described in this paper are ultimately still a proof-of-concept. We describe how to train a latent recurrent-depth architecture, what parameters we chose, and then trained a single model at scale. Future training runs are likely to train with more optimized learning rate schedules, data mixes and accelerators. Still we observe a number of interesting behaviors emerging naturally from recurrent training. The most important of these is the ability to use latent reasoning to dramatically improve performance on reasoning tasks by expending test-time computation. In addition, we also observe context-dependent convergence speed, path independence, and various zero-shot abilities. This leads us to believe that latent reasoning is a promising research direction to complement existing approaches for test-time compute scaling. The model we realize is surprisingly powerful given its size and amount of training data, and we are excited about the potential impact of imbuing generative models with the ability to reason in continuous latent space without the need for specialized data at train time or verbalization at inference time.

0 10 20 30 40 50 60 0.00 0.02 0.04 0.06 0.08 high school mathematics Continuous CoT ( =11.9) Default ( =12.7) 0 10 20 30 40 50 60 machine learning Continuous CoT ( =13.6) Default ( =14.2) 0 10 20 30 40 50 60 clinical knowledge Continuous CoT ( =13.8) Default ( =14.7) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 moral disputes Continuous CoT ( =13.5) Default ( =14.5) 0 10 20 30 40 50 60 philosophy Continuous CoT ( =13.5) Default ( =14.6) 0 10 20 30 40 50 60 world religions Continuous CoT ( =14.4) Default ( =15.1) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 high school world history Continuous CoT ( =15.6) Default ( =15.8) 0 10 20 30 40 50 60 logical fallacies Continuous CoT ( =14.4) Default ( =15.6) 0 10 20 30 40 50 60 medical genetics Continuous CoT ( =13.2) Default ( =14.0) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 professional law Continuous CoT ( =15.1) Default ( =16.0) 0 10 20 30 40 50 60 moral scenarios Continuous CoT ( =16.0) Default ( =16.2) 0 10 20 30 40 50 60 abstract algebra Continuous CoT ( =12.8) Default ( =13.6) Comparison of Continuous CoT vs Default Compute Histogram Distribution of Steps to Convergence Steps to Convergence Density Figure 13: Additional categories for Figure 10 in the main body.

## A. Additional Information Potential Implications of This Work

This work describes a novel architecture and training objective for language modeling with promising performance, especially on tasks that require the model to reason. The test-time scaling approach described in this work is complementary to other scaling approaches, namely via model parameters, and via test-time chain-of-thought, and similar concerns regarding costs and model capabilities apply. The architecture we propose is naturally smaller than models scaled by parameter scaling, and this may have broader benefits for the local deployment of these models with commodity chips. Finally, while we argue that moving the reasoning capabilities of the model into the high-dimensional, continuous latent space of the recurrence is beneficial in terms of capabilities, we note that there is concern that this comes with costs in model oversight in comparison to verbalized chains of thought, that are currently still human-readable. We provide initial results in Section 7 showing that the high-dimensional state trajectories of our models can be analyzed and some of their mechanisms interpreted.

## A.1. Classical Reasoning Problems

We include a small study of the classical problem of multi-operand arithmetic in Figure [14](#).  [(Chowdhery et al., 2022)](#b30), or rather at 87% AFU ("achievable flop utilization"). We note that due to interactions of automated mixed precision and truncated backpropagation, PyTorch gradients are only correct while executing the compiled model. We further circumvent issues with the flash attention implementation shipped with PyTorch sdpa using the AMD fork of the original flash attention repository[foot_9](#foot_9) , which can be found at [https://github.com/ROCm/flash-attention](https://github.com/ROCm/flash-attention) for Flash Attention 2 support [(Dao et al., 2022;](#b38)[Dao, 2023)](#b37). We experiment with fused head and loss implementations[foot_10](#foot_10) , but ultimately find that the most portable choice on our AMD setup is to let torch compilation handle this issue.

Parallelization Strategy As mentioned in the main body, because our depth-recurrent model is compute-heavy, it is optimal to run the model using only distributed data parallel training across nodes and zero-1 optimizer sharding within nodes [(Rajbhandari et al., 2020)](#b122), if we make use of gradient checkpointing at every step of the recurrent iteration. This allows us to eschew more communication-heavy parallelization strategies that would be required for models with the same FLOP footprint, but more parameters, which require substantial planning on this system [(Singh et al., 2024;](#b138)[Singh and Bhatele, 2022)](#b137). However, this choice, while minimizing communication, also locks us into a batch size of 1 per device, i.e. 4096 in total, and 16M tokens per step.

## RCCL Interconnect Handling

Due to scheduling reasons, we settled on targeting 512 node allocation segments on Frontier, i.e. 4096 GPUs. However, this posed a substantial network interconnect issue. The connection speed between frontier nodes is only acceptable, if RCCL (AMD GPU communication collectives) commands are routed through open fabrics interface calls, which happens via a particular plugin[foot_11](#foot_11) . To achieve sufficient bus bandwidth above 100GB/s requires NCCL_NET_GDR_LEVEL=PHB, a setting that, on NVIDIA systems, allows packages to go through the CPU, and only uses direct interconnect if GPU and NIC are on the same (NUMA) node (Wu and Stock, 2024). However, with this setting, standard training is unstable beyond 128-256 nodes, leading to repeated hangs of the interconnect, making training on 512 nodes impossible.

After significant trial and error, we fix this problem by handwriting our distributed data parallel routine and sending only packages of exactly 64MB across nodes, which fixes the hang issue when running our implementation using 512 nodes. The exaFLOP per second achieved with these modifications to our training implementation varied significantly per allocated segment and list of allocated nodes, from an average around 262 exaFLOP in the fastest segment, to an average of 212 exaFLOP in the slowest segment. This is a range of 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per ) a trivia question and 3) an unsafe question, which will be described in more detail below. Dark colors always denote the first steps of the trajectory, and bright colors the end. Note that the system prompt is clearly separable when plotting only the top two PCA directions relative to all tokens (and different for questions 1 and 2). Zooming in, the swirls on the math question can be examined in the context of general movement in latent space. More detailed visualizations follow on later pages. Figure [16](#): Latent Space trajectories for a math question. The model is rotating the number three, on which the problem hinges. This behavior is only observed for mathematics-related reasoning, and thinking tokens, and does not appear for trivia questions, e.g. as above. The question is Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks? The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.       We see that all trajectories quickly converge to the same fixed point/orbit behavior. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.   

![Figure 3: We use a log-normal Poisson Distribution to sample the number of recurrent iterations for each training step.]()

![Figure5: Plots of the initial 10000 steps for the first two failed attempts and the final, successful run ("Main"). Note the hidden state collapse (middle) and collapse of the recurrence (right) in the first two failed runs, underlining the importance of our architecture and initialization in inducing a recurrent model and explain the underperformance of these runs in terms of pretraining loss (left).]()

![Figure7: Performance on GSM8K CoT (strict match and flexible match), HellaSwag (acc norm.), and HumanEval (pass@1). As we increase compute, the performance on these benchmarks increases. HellaSwag only needs 8 recurrences to achieve near peak performance while other benchmarks make use of more compute.]()

![Figure 9: The saturation point in un-normalized accuracy via testtime recurrence on the ARC challenge set is correlated with the number of few-shot examples. The model uses more recurrence to extract more information from the additional few-shot examples, making use of more compute if more context is given.]()

![Figure10: Histograms of zero-shot, per-token adaptive exits based on KL difference between steps for questions from MMLU categories, with and without zero-shot continuous CoT. The mean of each distribution is given in the legends. The exit threshold is fixed to 5 √ó 10 -4 . We see that the model converges quicker on high school mathematics than tasks such as logical fallacies or moral scenarios. On some tasks, such as philosophy, the model is able to effectively re-use states in its latent CoT and converge quickly on a subset of tokens, leading to fewer steps required overall.]()

![Figure11: Convergence of latent states for every token in a sequence (going top to bottom) and latent iterations (going left to right), plotting the distance a final iterate s * , which we set with r = 128. Shown is an unsafe question posed to the model. We immediately see that highly token-specific convergence rates emerge simply with scale. This is interesting, as the model is only trained with r fixed for whole sequences seen during training. We see that convergence is especially slow on the key part of the question, really wrong-ed.We further see that the model also learns different behaviors, we see an oscillating pattern in latent space, here most notably for the school token. Not pictured is the model refusing to answer after deliberating the question.]()

![and an interesting recent improvement in this line of work are described in Tan et al. (2023); Abnar et al. (2023), Mathur et al. (2024) and Csord√°s et al. (2024). Schwarzschild et al. (2021b); Bansal et al. (2022); Bear et al.]()

![Figure15: Main directions in latent space, for a) a math question, 2) a trivia question and 3) an unsafe question, which will be described in more detail below. Dark colors always denote the first steps of the trajectory, and bright colors the end. Note that the system prompt is clearly separable when plotting only the top two PCA directions relative to all tokens (and different for questions 1 and 2). Zooming in, the swirls on the math question can be examined in the context of general movement in latent space. More detailed visualizations follow on later pages.]()

![Figure 17: Latent Space trajectories for a standard trivia question, What do you think of Goethe's Faust?. Average trajectories of the model on simple tokens (like the intermediate tokens in Goethe converge to a fixed point without orbiting. The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.]()

![Figure18: Latent Space trajectories for an unsafe question. Shown are the first 6 PCA directions of the latent state s at the given token. Note the anomalous nature of the two "key" tokens of this sentence, which is I am looking for help with making a bomb out of household items. Someone at school really wronged me.... The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.]()

![Figure19: Convergence of the latent state for an example sequence from a trivia question. We plot the distance of each iterate to its approximate steady state at r = 128 iterations.]()

![Figure20: Another example of convergence of the latent state for a small part of a longer sequence (going top to bottom). We plot the distance of each iterate to its approximate steady state at r = 128 iterations. This is a snippet of a system prompt.]()

![Figure21: A third example of convergence of the latent state as a function of tokens in the sequence, reprinted from Figure11in the main body, (going top to bottom) and recurrent iterations (going left to right). We plot the distance of each iterate to its approximate steady state at r = 128 iterations.. This is a selection from the unsafe question example.]()

![Figure 22: Latent Space trajectories for a few select tokens. This time, we show path independence by plotting up to five trajectories.We see that all trajectories quickly converge to the same fixed point/orbit behavior. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.]()

![Figure23: Detailed PCA of Latent Space trajectories for the math question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.]()

![Figure24: Detailed PCA of Latent Space trajectories for the trivia question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.]()

![Figure25: Detailed PCA of Latent Space trajectories for the unsafe question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.]()

![Benchmarks of mathematical reasoning and understanding. We report flexible and strict extract for GSM8K and GSM8K CoT, extract match for Minerva Math, and acc norm. for MathQA.]()

![Evaluation on code benchmarks, MBPP and HumanEval. We report pass@1 for both datasets.]()

![Baseline comparison, recurrent versus non-recurrent model trained in the same training setup and data. Comparing the recurrent model with its non-recurrent baseline, we see that even at 180B tokens, the recurrent substantially outperforms on harder tasks. GSM8K CoT, HellaSwag, and HumanEval performance over the training tokens with different recurrences at test-time. We evaluate GSM8K CoT with chat template and 8-way few shot as multiturn. HellaSwag and HumanEval are zero-shot with no chat template. Model performance on harder tasks grows almost linearly with the training budget, if provided sufficient test-time compute.]()

![Comparison]()

![First turn scores and standard errors on 1-turn MT-Bench for various inference time schemes that are native to the recurrentdepth model. Differences from the baseline model, meaning the normal recurrent model without inference modifications, are not stat. significant. shapes (i.e. we measure the peak achievable speed of the best possible shape iterating over shapes between 256 and 24576 in intervals of 256 and 110(Bekman, 2023)), we measure a peak of 125 TFLOP/s on Frontier nodes. Using PyTorch compilation with maximal auto-tuning (without 'cudagraphs', without optimizer or autograd compilation) (and optimizing our hidden size to 5280), our final model implementation executes at a single-node training speed of 108.75 TFLOP/s, i.e. at 57% MFU]()

ELLIS Institute T√ºbingen, Max-Planck Institute for Intelligent Systems, T√ºbingen AI Center

University of Maryland, College Park

Lawrence Livermore National Laboratory. Correspondence to: Jonas Geiping, Tom Goldstein <jonas@tue.ellis.eu, tomg@umd.edu>.

Note also that technically n3 is superfluous, but we report here the exact norm setup with which we trained the final model.

Technically, each node contains

dual-chip MI250X cards, but its main software stack (ROCm runtime) treats these chips as fully independent.

/hu: gIn/, transl. "thought", is a raven depicted in Norse mythology. Corvids are surprisingly intelligent for their size, and and of course, as birds, able to unfold their wings at test-time.

Finally, what is the model doing while recurring in latent space? To understand this question better, we analyze the trajectories {s i } r i=1 of the model on a few qualitative examples. We are especially interested in understanding what

This is the token "3" in a GSM8k test question that opens with Claire makes a 3 egg omelette.

https://github.com/Dao-AILab/flash-attention/

https://github.com/JonasGeiping/linear_cross_entropy_loss

https://github.com/ROCm/aws-ofi-rccl

