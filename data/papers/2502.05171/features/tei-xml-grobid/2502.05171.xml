<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title>
				<funder ref="#_mw7n4qD">
					<orgName type="full">U.S. Department of Energy</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA TIAMAT</orgName>
				</funder>
				<funder ref="#_Ak89umx">
					<orgName type="full">LLNL</orgName>
				</funder>
				<funder ref="#_mxg6azs">
					<orgName type="full">ONR</orgName>
				</funder>
				<funder>
					<orgName type="full">Capital One Bank</orgName>
				</funder>
				<funder>
					<orgName type="full">Hector II foundation</orgName>
				</funder>
				<funder ref="#_CU4HTkC">
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder>
					<orgName type="full">Novel Computational Impact on Theory and Experiment (INCITE) Program</orgName>
				</funder>
				<funder ref="#_6ttzJF3">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">MPI Intelligent Systems compute cluster</orgName>
				</funder>
				<funder ref="#_vWtrd8s">
					<orgName type="full">U.S. Department of Energy&apos;s (DOE) Innovative</orgName>
				</funder>
				<funder ref="#_xNaUnvE #_JThECxZ">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_wNR43zt">
					<orgName type="full">Office of Science of the U.S. Department of Energy</orgName>
				</funder>
				<funder ref="#_MEh3q3r">
					<orgName type="full">Lawrence Livermore National Security, LLC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-17">17 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>Mcleish</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Neel</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">R</forename><surname>Bartoldson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Bhatele</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
						</author>
						<title level="a" type="main">Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-17">17 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">98D19B88CB625A1EA974ED81378D3E0F</idno>
					<idno type="arXiv">arXiv:2502.05171v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-ofconcept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.</p><p>Model: huggingface.co/tomg-group-umd/huginn-0125</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 4 6 8 12 20 32 48 64 Test-Time Compute Recurrence 0 10 20 30 40 50 Accuracy (%) Scaling up Test-Time Compute with Recurrent Depth ARC challenge GSM8K CoT OpenBookQA 3.6B 8.3B 11.5B14.6B 21.0B 33.6B 52.6B 77.9B103B</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materialized Parameters</head><p>Figure <ref type="figure">1</ref>: We train a 3.5B parameter language model with depth recurrence. At test time, the model can iterate longer to use more compute and improve its performance. Instead of scaling test-time reasoning by "verbalizing" in long Chains-of-Thought, the model improves entirely by reasoning in latent space. Tasks that require less reasoning like OpenBookQA converge quicker than tasks like GSM8k, which effectively make use of more compute.</p><p>capability of models by scaling test time computation. The mainstream approach involves post-training on long chainof-thought examples to develop the model's ability to verbalize intermediate calculations in its context window and thereby externalize thoughts.</p><p>However, the constraint that expensive internal reasoning must always be projected down to a single verbalized next token appears wasteful; it is plausible that models could be more competent if they were able to natively "think" in their continuous latent space. One way to unlock this untapped dimension of additional compute involves adding a recurrent unit to a model. This unit runs in a loop, iteratively processing and updating its hidden state and enabling computations to be carried on indefinitely. While this is not currently the dominant paradigm, this idea is foundational to machine learning and has been (re-)discovered in every decade, for example as recurrent neural networks, diffusion models, and as universal or looped transformers.</p><p>In this work, we show that depth-recurrent language models can learn effectively, be trained in an efficient manner, and demonstrate significant performance improvements under the scaling of test-time compute. Our proposed trans-former architecture is built upon a latent depth-recurrent block that is run for a randomly sampled number of iterations during training. We show that this paradigm can scale to several billion parameters and over half a trillion tokens of pretraining data. At test-time, the model can improve its performance through recurrent reasoning in latent space, enabling it to compete with other open-source models that benefit from more parameters and training data. Additionally, we show that recurrent depth models naturally support a number of features at inference time that require substantial tuning and research effort in non-recurrent models, such as per-token adaptive compute, (self)-speculative decoding, and KV-cache sharing. We finish out our study by tracking token trajectories in latent space, showing that a number of interesting computation behaviors simply emerge with scale, such as the model rotating shapes in latent space for numerical computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Why Train Models with Recurrent Depth?</head><p>Recurrent layers enable a transformer model to perform arbitrarily many computations before emitting a token. In principle, recurrent mechanisms provide a simple solution for test-time compute scaling. Compared to a more standard approach of long context reasoning <ref type="bibr" target="#b116">(OpenAI, 2024;</ref><ref type="bibr">DeepSeek-AI et al., 2025)</ref>, latent recurrent thinking has several advantages.</p><p>• Latent reasoning does not require construction of bespoke training data. Chain-of-thought reasoning requires the model to be trained on long demonstrations that are constructed in the domain of interest. In contrast, our proposed latent reasoning models can train with a variable compute budget, using standard training data with no specialized demonstrations, and enhance their abilities at testtime if given additional compute. • Latent reasoning models require less memory for training and inference than chain-of-thought reasoning models. Because the latter require extremely long context windows, specialized training methods such as tokenparallelization <ref type="bibr">(Liu et al., 2023a</ref>) may be needed. • Recurrent-depth networks perform more FLOPs per parameter than standard transformers, significantly reducing communication costs between accelerators at scale. This especially enables higher device utilization when training with slower interconnects. • By constructing an architecture that is compute-heavy and small in parameter count, we hope to set a strong prior towards models that solve problems by "thinking", i.e. by learning meta-strategies, logic and abstraction, instead of memorizing. The strength of recurrent priors for learning complex algorithms has already been demonstrated in the "deep thinking" literature <ref type="bibr">(Schwarzschild et al., 2021b;</ref><ref type="bibr" target="#b14">Bansal et al., 2022;</ref><ref type="bibr" target="#b132">Schwarzschild et al., 2023)</ref>.</p><p>On a more philosophical note, we hope that latent reasoning captures facets of human reasoning that defy verbalization, such as spatial thinking, physical intuition or (motor) planning. Over many iterations of the recurrent process, reasoning in a high-dimensional vector space would enable the deep exploration of multiple directions simultaneously, instead of linear thinking, leading to a system capable of exhibiting novel and complex reasoning behavior.</p><p>Scaling compute in this manner is not at odds with scaling through extended (verbalized) inference scaling <ref type="bibr" target="#b134">(Shao et al., 2024)</ref>, or scaling parameter counts in pretraining <ref type="bibr" target="#b76">(Kaplan et al., 2020)</ref>, we argue it may build a third axis on which to scale model performance.</p><p>--------</p><p>Table of Contents --------• Section 3 introduces our latent recurrent-depth model architecture and training objective. • Section 4 describes the data selection and engineering of our large-scale training run on Frontier, an AMD cluster. • Section 5 reports benchmark results, showing how the model improves when scaling inference compute. • Section 6 includes several application examples showing how recurrent models naturally simplify LLM usecases. • Section 7 visualizes what computation patterns emerge at</p><p>scale with this architecture and training objective, showing that context-dependent behaviors emerge in latent space, such as "orbiting" when responding to prompts requiring numerical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A scalable recurrent architecture</head><p>In this section we will describe our proposed architecture for a transformer with latent recurrent depth, discussing design choices and small-scale ablations. A diagram of the architecture can be found in Figure <ref type="figure">2</ref>. We always refer to the sequence dimension as n, the hidden dimension of the model as h, and its vocabulary as the set V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Macroscopic Design</head><p>The model is primarily structured around decoder-only transformer blocks <ref type="bibr" target="#b157">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b120">Radford et al., 2019)</ref>. However these blocks are structured into three functional groups, the prelude P , which embeds the input data into a latent space using multiple transformer layers, then the core recurrent block R, which is the central unit of recurrent computation modifying states s ∈ R n×h , and finally the coda C, which un-embeds from latent space using several layers and also contains the prediction head of the model. The core block is set between the prelude and coda blocks, and by looping the core we can put an indefinite amount of verses in our song.</p><p>P "Hello" "World"</p><formula xml:id="formula_0">Input Injection Residual Stream Prelude Recurrent Block Coda 𝒩(0,σ 2 I n⋅h ) e s 0 R R s 1 e … R C s R p e</formula><p>Figure <ref type="figure">2</ref>: A visualization of the Architecture, as described in Section 3. Each block consists of a number of sub-layers. The blue prelude block embeds the inputs into latent space, where the green shared recurrent block is a block of layers that is repeated to compute the final latent state, which is decoded by the layers of the red coda block.</p><p>Given a number of recurrent iterations r, and a sequence of input tokens x ∈ V n these groups are used in the following way to produce output probabilities p ∈ R n×|V | e = P (x)</p><formula xml:id="formula_1">s 0 ∼ N (0, σ 2 I n•h ) s i = R(e, s i-1 ) for i ∈ {1, . . . , r} p = C(s r ),</formula><p>where σ is some standard deviation for initializing the random state. This process is shown in Figure <ref type="figure">2</ref>. Given an init random state s 0 , the model repeatedly applies the core block R, which accepts the latent state s i-1 and the embedded input e and outputs a new latent state s i . After finishing all iterations, the coda block processes the last state and produces the probabilities of the next token. This architecture is based on deep thinking literature, where it is shown that injecting the latent inputs e in every step <ref type="bibr" target="#b14">(Bansal et al., 2022)</ref> and initializing the latent vector with a random state stabilizes the recurrence and promotes convergence to a steady state independent of initialization, i.e. path independence <ref type="bibr" target="#b6">(Anil et al., 2022)</ref>.</p><p>Motivation for this Design. This recurrent design is the minimal setup required to learn stable iterative operators. A good example is gradient descent of a function E(x, y), where x may be the variable of interest and y the data. Gradient descent on this function starts from an initial random state, here x 0 , and repeatedly applies a simple operation (the gradient of the function it optimizes), that depends on the previous state x k and data y. Note that we need to use y in every step to actually optimize our function. Similarly we repeatedly inject the data e in our set-up in every step of the recurrence. If e was provided only at the start, e.g. via s 0 = e, then the iterative process would not be stable 1 , as its solution would depend only on its boundary conditions. The structure of using several layers to embed input tokens 1 Stable in the sense that R cannot be a monotone operator if it does not depend on e, and so cannot represent gradient descent on strictly convex, data-dependent functions, <ref type="bibr" target="#b15">(Bauschke et al., 2011)</ref> into a hidden latent space is based on empirical results analyzing standard fixed-depth transformers <ref type="bibr" target="#b139">(Skean et al., 2024;</ref><ref type="bibr" target="#b145">Sun et al., 2024;</ref><ref type="bibr" target="#b75">Kaplan et al., 2024)</ref>. This body of research shows that the initial and the end layers of LLMs are noticeably different, whereas middle layers are interchangeable and permutable. For example, <ref type="bibr" target="#b75">Kaplan et al. (2024)</ref> show that within a few layers standard models already embed sub-word tokens into single concepts in latent space, on which the model then operates. Remark 3.1 (Is this a Diffusion Model?). This iterative architecture will look familiar to the other modern iterative modeling paradigm, diffusion models <ref type="bibr" target="#b142">(Song and Ermon, 2019)</ref>, especially latent diffusion models <ref type="bibr" target="#b123">(Rombach et al., 2022)</ref>. We ran several ablations with iterative schemes even more similar to diffusion models, such as s i = R(e, s i-1 ) + n where n ∼ N (0, σ i I n•h ), but find the injection of noise not to help in our preliminary experiments, which is possibly connected to our training objective. We also evaluated and s i = R i (e, s i-1 ), i.e. a core block that takes the current step as input <ref type="bibr" target="#b119">(Peebles and Xie, 2023)</ref>, but find that this interacts badly with path independence, leading to models that cannot extrapolate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Microscopic Design</head><p>Within each group, we broadly follow standard transformer layer design. Each block contains multiple layers, and each layer contains a standard, causal self-attention block using RoPE <ref type="bibr" target="#b143">(Su et al., 2021)</ref> with a base of 50000, and a gated SiLU MLP <ref type="bibr" target="#b135">(Shazeer, 2020)</ref>. We use RMSNorm <ref type="bibr" target="#b177">(Zhang and Sennrich, 2019)</ref> as our normalization function. The model has learnable biases on queries and keys, and nowhere else. To stabilize the recurrence, we order all layers in the following "sandwich" format, using norm layers n i , which is related, but not identical to similar strategies in <ref type="bibr" target="#b44">(Ding et al., 2021;</ref><ref type="bibr" target="#b152">Team Gemma et al., 2024)</ref>:</p><formula xml:id="formula_2">xl =n 2 (x l-1 + Attn(n 1 (x l-1 ))) x l =n 4 ( xl + MLP(n 3 ( xl )))</formula><p>While at small scales, most normalization strategies, e.g. pre-norm, post-norm and others, work almost equally well, we ablate these options and find that this normalization is required to train the recurrence at scale<ref type="foot" target="#foot_3">foot_3</ref> .</p><p>Given an embedding matrix E and embedding scale γ, the prelude block first embeds input tokens x as γE(x), and then to applies l P many prelude layers with the layout described above.</p><p>Our core recurrent block R starts with an adapter matrix A : R 2h → R h mapping the concatenation of s i and e into the hidden dimension h <ref type="bibr" target="#b14">(Bansal et al., 2022)</ref>. While re-incorporation of initial embedding features via addition rather than concatenation works equally well for smaller models, we find that concatenation works best at scale. This is then fed into l R transformer layers. At the end of the core block the output is again rescaled with an RMSNorm n c .</p><p>The coda contains l C layers, normalization by n c , and projection into the vocabulary using tied embeddings E T .</p><p>In summary, we can summarize the architecture by the triplet (l P , l R , l C ), describing the number of layers in each stage, and by the number of recurrences r, which may vary in each forward pass. We train a number of small-scale models with shape (1, 4, 1) and hidden size h = 1024, in addition to a large model with shape (2, 4, 2) and h = 5280. This model has only 8 "real" layers, but when the recurrent block is iterated, e.g. 32 times, it unfolds to an effective depth of 2 + 4r + 2 = 132 layers, constructing computation chains that can be deeper than even the largest fixed-depth transformers <ref type="bibr">(Levine et al., 2021;</ref><ref type="bibr" target="#b106">Merrill et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Objective</head><p>Training Recurrent Models through Unrolling. To ensure that the model can function when we scale up recurrent iterations at test-time, we randomly sample iteration counts during training, assigning a random number of iterations r to every input sequence <ref type="bibr">(Schwarzschild et al., 2021b)</ref>. We optimize the expectation of the loss function L over random samples x from distribution X and random iteration counts r from distribution Λ.</p><formula xml:id="formula_3">L(θ) = E x∈X E r∼Λ L (m θ (x, r), x ′ ) .</formula><p>Here, m represents the model output, and x ′ is the sequence x shifted left, i.e., the next tokens in the sequence x. We choose Λ to be a log-normal Poisson distribution. Given a targeted mean recurrence r + 1 and a variance that we set to σ = 1 2 , we can sample from this distribution via  given the normal distribution N and Poisson distribution P, see Figure <ref type="figure" target="#fig_1">3</ref>. The distribution most often samples values less than r, but it contains a heavy tail of occasional events in which significantly more iterations are taken.</p><formula xml:id="formula_4">τ ∼ N (log(r) - 1 2 σ 2 , σ)<label>(1)</label></formula><formula xml:id="formula_5">r ∼ P(e τ ) + 1,<label>(2)</label></formula><p>Truncated Backpropagation. To keep computation and memory low at train time, we backpropagate through only the last k iterations of the recurrent unit. This enables us to train with the heavy-tailed Poisson distribution Λ, as maximum activation memory and backward compute is now independent of r. We fix k = 8 in our main experiments. At small scale, this works as well as sampling k uniformly, but with set fixed, the overall memory usage in each step of training is equal. Note that the prelude block still receives gradient updates in every step, as its output e is injected in every step. This setup resembles truncated backpropagation through time, as commonly done with RNNs, although our setup is recurrent in depth rather than time <ref type="bibr" target="#b161">(Williams and Peng, 1990;</ref><ref type="bibr" target="#b108">Mikolov et al., 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training a large-scale recurrent-depth Language Model</head><p>After verifying that we can reliably train small test models up to 10B tokens, we move on to larger-scale runs. Given our limited compute budget, we could either train multiple tiny models too small to show emergent effects or scaling, or train a single medium-scale model. Based on this, we prepared for a single run, which we detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Setup</head><p>We describe the training setup, separated into architecture, optimization setup and pretraining data. We publicly release all training data, pretraining code, and a selection of intermediate model checkpoints. Pretraining Data. Given access to only enough compute for a single large scale model run, we opted for a dataset mixture that maximized the potential for emergent reasoning behaviors, not necessarily for optimal benchmark per-generic-text: 28.71% code: 25.36% scientific-text: 18.73% synthetic-text: 8.14% longform-text: 7.50% math: 6.14% generic-instruct: 2.09% Q&amp;A-text: 1.58% math-instruct: 1.51% writing-instruct: 0.12% misc-reasoning: 0.11% Figure 4: Distribution of data sources that are included during training. The majority of our data is comprised of generic webtext, scientific writing and code.</p><p>formance. Our final mixture is heavily skewed towards code and mathematical reasoning data with (hopefully) just enough general webtext to allow the model to acquire standard language modeling abilities. All sources are publicly available. We provide an overview in Figure <ref type="figure">4</ref>. Following <ref type="bibr" target="#b1">Allen-Zhu and Li (2024)</ref>, we directly mix relevant instruction data into the pretraining data. However, due to compute and time constraints, we were not able to ablate this mixture.</p><p>We expect that a more careful data preparation could further improve the model's performance. We list all data sources in Appendix C.</p><p>Tokenization and Packing Details. We construct a vocabulary of 65536 tokens via BPE <ref type="bibr" target="#b133">(Sennrich et al., 2016)</ref>, using the implementation of Dagan (2024). In comparison to conventional tokenizer training, we construct our tokenizer directly on the instruction data split of our pretraining corpus, to maximize tokenization efficiency on the target domain. We also substantially modify the pre-tokenization regex (e.g. of <ref type="bibr">Dagan et al. (2024)</ref>) to better support code, contractions and LaTeX. We include a &lt;|begin_text|&gt; token at the start of every document. After tokenizing our pretraining corpus, we pack our tokenized documents into sequences of length 4096. When packing, we discard document ends that would otherwise lack previous context, to fix an issue described as the "grounding problem" in <ref type="bibr" target="#b43">Ding et al. (2024)</ref>, aside from several long-document sources of mathematical content, which we preserve in their entirety.</p><p>Architecture and Initialization. We scale the architecture described in Section 3, setting the layers to (2, 4, 2), and train with a mean recurrence value of r = 32. We mainly scale by increasing the hidden size to h = 5280, which yields 55 heads of size of 96. The MLP inner dimension is 17920 and the RMSNorm ε is 10 -6 . Overall this model shape has about 1.5B parameters in non-recurrent prelude and head, 1.5B parameters in the core recurrent block, and 0.5B in the tied input embedding.</p><p>At small scales, most sensible initialization schemes work.</p><p>However, at larger scales, we use the initialization of <ref type="bibr" target="#b150">Takase et al. (2024)</ref> which prescribes a variance of σ 2 h = 2 5h . We initialize all parameters from a truncated normal distribution (truncated at 3σ) with this variance, except all outprojection layers, where the variance is set to σ 2 out = 1 5hl , for l = l P + rl R + l C the number of effective layers, which is 132 for this model. As a result, the out-projection layers are initialized with fairly small values <ref type="bibr" target="#b59">(Goyal et al., 2018)</ref>. The output of the embedding layer is scaled by √ h. To match this initialization, the state s 0 is also sampled from a truncated normal distribution, here with variance σ 2 s = 2 5 .</p><p>Locked-Step Sampling. To enable synchronization between parallel workers, we sample a single depth r for each micro-batch of training, which we synchronize across workers (otherwise workers would idle while waiting for the model with the largest r to complete its backward pass). We verify at small scale that this modification improves compute utilization without impacting convergence speed, but note that at large batch sizes, training could be further improved by optimally sampling and scheduling independent steps r on each worker, to more faithfully model the expectation over steps in Equation ( <ref type="formula" target="#formula_4">1</ref>).</p><p>Optimizer and Learning Rate Schedule. We train using the Adam optimizer with decoupled weight regularization (β 1 = 0.9, β 2 = 0.95, η = 5 × 10 -<ref type="foot" target="#foot_5">foot_5</ref> ) (Kingma and Ba, 2015; <ref type="bibr" target="#b99">Loshchilov and Hutter, 2017)</ref>, modified to include update clipping <ref type="bibr">(Wortsman et al., 2023b)</ref> and removal of the ε constant as in <ref type="bibr" target="#b47">Everett et al. (2024)</ref>. We clip gradients above 1. We train with warm-up and a constant learning rate <ref type="bibr" target="#b176">(Zhai et al., 2022;</ref><ref type="bibr" target="#b132">Geiping and Goldstein, 2023)</ref>, warming up to our maximal learning rate within the first 4096 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Compute Setup and Hardware</head><p>We train this model using compute time allocated on the Oak Ridge National Laboratory's Frontier supercomputer. This HPE Cray system contains 9408 compute nodes with AMD MI250X GPUs, connected via 4xHPE Slingshot-11 NICs. The scheduling system is orchestrated through SLURM. We train in bfloat16 mixed precision using a PyTorch-based implementation <ref type="bibr" target="#b173">(Zamirai et al., 2021)</ref>.</p><p>Device Speed and Parallelization Strategy. Nominally, each MI250X chip<ref type="foot" target="#foot_4">foot_4</ref> achieves 192 TFLOP per GPU <ref type="bibr" target="#b3">(AMD, 2021)</ref>. For a single matrix multiplication, we measure a maximum achievable speed on these GPUs of 125 TFLOP/s on our software stack (ROCM 6.2.0, PyTorch 2.6 prerelease 11/02) <ref type="bibr" target="#b17">(Bekman, 2023)</ref>. Our implementation, using extensive PyTorch compilation and optimization of the hidden dimension to h = 5280 achieves a single-node training speed of 108.75 TFLOP/s, i.e. 87% AFU ("Achievable Flop Utilization"). Due to the weight sharing inherent in our recurrent design, even our largest model is still small enough to be trained using only data (not tensor) parallelism, with only optimizer sharding <ref type="bibr" target="#b122">(Rajbhandari et al., 2020)</ref> and gradient checkpointing on a per-iteration granularity. With a batch size of 1 per GPU we end up with a global batch size of 16M tokens per step, minimizing inter-GPU communication bandwidth.</p><p>When we run at scale on 4096 GPUs, we achieve 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per second. To achieve this, we wrote a hand-crafted distributed data parallel implementation to circumvent a critical AMD interconnect issue, which we describe in more detail in Appendix A.2. Overall, we believe this may be the largest language model training run to completion in terms of number of devices used in parallel on an AMD cluster, as of time of writing.</p><p>Training Timeline. Training proceeded through 21 segments of up to 12 hours, which scheduled on Frontier mostly in early December 2024. We also ran a baseline comparison, where we train the same architecture but in a feedforward manner with only 1 pass through the core/recurrent block. This trained with the same setup for 180B tokens on 256 nodes with a batch size of 2 per GPU. Ultimately, we were able to schedule 795B tokens of pretraining of the main model. Due to our constant learning rate schedule, we were able to add additional segments "on-demand", when an allocation happened to be available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Importance of Norms and Initializations at Scale</head><p>At small scales all normalization strategies worked, and we observed only tiny differences between initializations. The same was not true at scale. The first training run we started was set up with the same block sandwich structure as described above, but parameter-free RMSNorm layers, no embedding scale γ, a parameter-free adapter A(s, e) = s + e, and a peak learning rate of 4 × 10 -<ref type="foot" target="#foot_6">foot_6</ref> . As shown in Figure <ref type="figure" target="#fig_2">5</ref>, this run ("Bad Run 1", orange), quickly stalled.</p><p>While the run obviously stopped improving in training loss (left plot), we find that this stall is due to the model's representation collapsing <ref type="bibr" target="#b115">(Noci et al., 2022)</ref>. The correlation of hidden states in the token dimension quickly goes to 1.0 (middle plot), meaning the model predicts the same hidden state for every token in the sequence. We find that this is an initialization issue that arises due to the recurrence operation. Every iteration of the recurrence block increases token correlation, mixing the sequence until collapse.</p><p>We attempt to fix this by introducing the embedding scale factor, switching back to a conventional pre-normalization block, and switching to the learned adapter. Initially, these changes appear to remedy the issue. Even though token correlation shoots close to 1.0 at the start ("Bad Run 2", green), the model recovers after the first 150 steps. However, we quickly find that this training run is not able to leverage test-time compute effectively (right plot), as validation perplexity is the same whether 1 or 32 recurrences are used. This initialization and norm setup has led to a local minimum as the model has learned early to ignore the incoming state s, preventing further improvements.</p><p>In a third, and final run ("Main", blue), we fix this issue by reverting back to the sandwich block format, and further dropping the peak learning rate to 4 × 10 -5 . This run starts smoothly, never reaches a token correlation close to 1.0, and quickly overtakes the previous run by utilizing the recurrence and improving with more iterations.</p><p>With our successful configuration, training continues smoothly for the next 750B tokens without notable interruptions or loss spikes. We plot training loss and perplexity at different recurrence steps in Figure <ref type="figure">6</ref>. In our material, we refer to the final checkpoint of this run as our "main model", which we denote as Huginn-0125 4 .</p><p>10 8 10 9 10 10 10 11 10 12 Tokens (log) 10 1 10 2 10 3 10 4 Step (log) 5 10 Loss 10 10 10 11 10 12 Tokens (log) 10 2 10 3 10 4 Step (log) 10 1 10 2 10 3 Validation Perplexity (log) Recurrence 1 4 8 16 32 64 Figure 6: Left: Plot of pretrain loss over the 800B tokens on the main run. Right: Plot of val ppl at recurrent depths 1, 4, 8, 16, 32, 64.</p><p>During training, the model improves in perplexity on all levels of recurrence.</p><p>Table <ref type="table">1</ref>: Results on lm-eval-harness tasks zero-shot across various open-source models. We show ARC <ref type="bibr" target="#b31">(Clark et al., 2018)</ref>, HellaSwag <ref type="bibr" target="#b175">(Zellers et al., 2019)</ref>, MMLU <ref type="bibr">(Hendrycks et al., 2021a)</ref>, OpenBookQA <ref type="bibr" target="#b107">(Mihaylov et al., 2018)</ref>, PiQA <ref type="bibr" target="#b21">(Bisk et al., 2020)</ref>, SciQ (Johannes Welbl, 2017), and WinoGrande <ref type="bibr" target="#b124">(Sakaguchi et al., 2021)</ref>. We report normalized accuracy when provided.</p><p>Model Param Tokens ARC-E ARC-C HellaSwag MMLU OBQA PiQA SciQ WinoGrande random 25.0 25.0 25.0 25.0 25.0 50.0 25.0 50.0 Amber 7B 1.2T 65.70 37.20 72.54 26.77 41.00 78.73 88.50 63.22 Pythia-2.8b 2.8B 0.3T 58.00 32.51 59.17 25.05 35.40 73.29 83.60 57.85 Pythia-6.9b 6.9B 0.3T 60.48 34.64 63.32 25.74 37.20 75.79 82.90 61.40 Pythia-12b 12B 0.3T 63.22 34.64 66.72 24.01 35.40 75.84 84.40 63.06 OLMo-1B 1B 3T 57.28 30.72 63.00 24.33 36.40 75.24 78.70 59.19 OLMo-7B 7B 2.5T 68.81 40.27 75.52 28.39 42.20 80.03 88.50 67.09 OLMo-7B-0424 7B 2.05T 75.13 45.05 77.24 47.46 41.60 80.09 96.00 68.19 OLMo-7B-0724 7B 2.75T 74.28 43.43 77.76 50.18 41.60 80.69 95.70 67.17 OLMo-2-1124 7B 4T 82.79 57.42 80.50 60.56 46.20 81.18 96.40 74.74 Ours, (r = 4) 3.5B 0.8T 49.07 27.99 43.46 23.39 28.20 64.96 80.00 55.24 Ours, (r = 8) 3.5B 0.8T 65.11 35.15 58.54 25.29 35.40 73.45 92.10 55.64 Ours, (r = 16) 3.5B 0.8T 69.49 37.71 64.67 31.25 37.60 75.79 93.90 57.77 Ours, (r = 32) 3.5B 0.8T 69.91 38.23 65.21 31.38 38.80 76.22 93.50 59.43</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Benchmark Results</head><p>We train our final model for 800B tokens, and a nonrecurrent baseline for 180B tokens. We evaluate these checkpoints against other open-source models trained on fully public datasets (like ours) of a similar size. We compare against Amber <ref type="bibr">(Liu et al., 2023c)</ref>, Pythia <ref type="bibr" target="#b8">(Biderman et al., 2023)</ref> and a number of OLMo 1&amp;2 variants <ref type="bibr" target="#b62">(Groeneveld et al., 2024;</ref><ref type="bibr">AI2, 2024;</ref><ref type="bibr">Team OLMo et al., 2025)</ref>. We execute all standard benchmarks through the lm-eval harness <ref type="bibr">(Biderman et al., 2024)</ref> and code benchmarks via bigcode-bench (Zhuo et al., 2024).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Standard Benchmarks</head><p>Overall, it is not straightforward to place our model in direct comparison to other large language models, all of which are small variations of the fixed-depth transformer architecture.</p><p>While our model has only 3.5B parameters and hence requires only modest interconnect bandwidth during pretraining, it chews through raw FLOPs close to what a 32B parameter transformer would consume during pretraining, and can continuously improve in performance with test-time scaling up to FLOP budgets equivalent to a standard 50B parameter fixed-depth transformer. It is also important to note a few caveats of the main training run when interpreting the results. First, our main checkpoint is trained for only 47000 steps on a broadly untested mixture, and the learning rate is never cooled down from its peak. As an academic project, the model is trained only on publicly available data and the 800B token count, while large in comparison to older fully open-source models such as the Pythia series, is small in comparison to modern open-source efforts such as OLMo, and tiny in comparison to the datasets used to train industrial open-weight models.</p><p>Disclaimers aside, we collect results for established benchmark tasks <ref type="bibr">(Team OLMo et al., 2025)</ref> in Table <ref type="table">1</ref> and show all models side-by-side. In direct comparison we see that our model outperforms the older Pythia series and is roughly comparable to the first OLMo generation, OLMo-7B in most metrics, but lags behind the later OLMo models trained larger, more carefully curated datasets. For the first recurrent-depth model for language to be trained at this  scale, and considering the limitations of the training run, we find these results promising and certainly suggestive that further research into latent recurrence as an approach to test-time scaling is warranted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Math and Coding Benchmarks</head><p>We also evaluate the model on math and coding. For math, we evaluate GSM8k <ref type="bibr" target="#b32">(Cobbe et al., 2021)</ref> (as zero-shot and in the 8-way CoT setup), MATH ( <ref type="bibr">(Hendrycks et al., 2021b)</ref> with the Minerva evaluation rules <ref type="bibr" target="#b89">(Lewkowycz et al., 2022)</ref>) and MathQA <ref type="bibr" target="#b4">(Amini et al., 2019)</ref>. For coding, we check MBPP <ref type="bibr">(Austin et al., 2021)</ref> and HumanEval <ref type="bibr" target="#b27">(Chen et al., 2021)</ref>. Here we find that our model significantly surpasses all models except the latest OLMo-2 model in mathematical reasoning, as measured on GSM8k and MATH. On coding benchmarks the model beats all other general-purpose opensource models, although it does not outperform dedicated code models, such as StarCoder2 <ref type="bibr" target="#b18">(Lozhkov et al., 2024)</ref>, trained for several trillion tokens. We also note that while further improvements in language modeling are slowing down, as expected at this training scale, both code and mathematical reasoning continue to improve steadily throughout training, see Figure <ref type="figure">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Where does recurrence help most?</head><p>How much of this performance can we attribute to recurrence, and how much to other factors, such as dataset, tokenization and architectural choices? In Table <ref type="table" target="#tab_8">4</ref>, we compare our recurrent model against its non-recurrent twin, which we trained to 180B tokens in the exact same setting. In direct comparison of both models at 180B tokens, we see that the recurrent model outperforms its baseline with an especially pronounced advantage on harder tasks, such as the ARC challenge set. On other tasks, such as SciQ, which requires straightforward recall of scientific facts, performance of the models is more similar. We observe that gains through reasoning are especially prominent on GSM8k, where the 180B recurrent model is already 5 times better than the baseline at this early snapshot in the pretraining  ), and HumanEval (pass@1). As we increase compute, the performance on these benchmarks increases. HellaSwag only needs 8 recurrences to achieve near peak performance while other benchmarks make use of more compute.</p><p>process. We also note that the recurrent model, when evaluated with only a single recurrence, effectively stops improving between the early 180B checkpoint and the 800B checkpoint, showing that further improvements are not built into the prelude or coda non-recurrent layers but encoded entirely into the iterations of the recurrent block.</p><p>Further, we chart the improvement as a function of test-time compute on several of these tasks for the main model in Figure <ref type="figure" target="#fig_4">7</ref>. We find that saturation is highly task-dependent, on easier tasks the model saturates quicker, whereas it benefits from more compute on others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrence and Context</head><p>We evaluate ARC-C performance as a function of recurrence and number of few-shot examples in the context in Figure <ref type="figure" target="#fig_5">9</ref>. Interestingly, without few-shot examples to consider, the model saturates in compute around 8-12 iterations. However, when more context is given, the model can reason about more information in context, which it does, saturating around 20 iterations if 1 example is provided, and 32 iterations, if 25-50 examples are provided, mirroring generalization improvements shown for recurrence <ref type="bibr">(Yang et al., 2024a;</ref><ref type="bibr" target="#b50">Fan et al., 2025)</ref>. Similarly,  we see that if we re-evaluate OBQA in Table <ref type="table" target="#tab_9">5</ref>, but do not run the benchmark in the default lm-eval "closed-book" format and rather provide a relevant fact, our recurrent model improves significantly almost closing the gap to OLMo-2. Intuitively this makes sense, as the recurrent models has less capacity to memorize facts but more capacity to reason about its context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Improvements through Weight Averaging</head><p>Due to our constant learning rate, we can materialize further improvements through weight averaging <ref type="bibr" target="#b71">(Izmailov et al., 2018)</ref> to simulate the result of a cooldown <ref type="bibr" target="#b63">(Hägele et al., 2024;</ref><ref type="bibr" target="#b40">DeepSeek-AI et al., 2024)</ref>. We use an exponen- tial moving average starting from our last checkpoint with β = 0.9, incorporating the last 75 checkpoints with a dilation factor of 7, a modification to established protocols <ref type="bibr" target="#b74">(Kaddour, 2022;</ref><ref type="bibr" target="#b126">Sanyal et al., 2024)</ref>. We provide this EMA model as well, which further improves GMS8k performance to 47.23% flexible (38.59% strict), when tested at r = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Recurrent Depth simplifies LLMs</head><p>Aside from encouraging performance in mathematical and code reasoning, recurrent-depth models turn out to be surprisingly natural tools to support a number of methods that require substantial effort with standard transformers. In the next section, we provide a non-exhaustive overview. We see that the model converges quicker on high school mathematics than tasks such as logical fallacies or moral scenarios. On some tasks, such as philosophy, the model is able to effectively re-use states in its latent CoT and converge quickly on a subset of tokens, leading to fewer steps required overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Zero-Shot Adaptive Compute at Test-Time</head><p>We have shown that the model is capable of varying compute on a per-query level, running the model in different recurrence modes. This is after all also how the model is trained, as in Equation ( <ref type="formula" target="#formula_4">1</ref>). However, it would be more efficient in practice to stop recurring early when predictions are easy, and only spend compute on hard decisions. Other work, especially when based on standard transformers, requires models trained specifically for early exits <ref type="bibr" target="#b45">(Elbayad et al., 2019;</ref><ref type="bibr" target="#b48">Fan et al., 2019;</ref><ref type="bibr" target="#b13">Banino et al., 2021)</ref>, or models finetuned with exit heads on every layer <ref type="bibr" target="#b128">(Schuster et al., 2022)</ref>. To test our model's zero-shot exit abilities, we choose a simple exit criterion to evaluate convergence, the KL-divergence between two successive steps. If this divergence falls below 5 × 10 -4 , we stop iterating, sample the output token, and move to generate the next token.</p><p>We show this zero-shot per-token adaptive compute behavior in Figure <ref type="figure" target="#fig_6">10</ref>, where we plot the distribution of steps taken before the exit condition is hit. We do this for the first 50 questions from different MMLU categories, asked in free-form chat. Interestingly, the number of steps required to exit differs notably between categories, with the model exiting earlier on high school mathematics, but taking on average 3.5 steps more on moral scenarios. As a preliminary demonstration, we verify on MTBench that this adaptivity does not significantly impact performance in a conversational benchmark setting (standard: 5.63, early exits: 5.56 see Appendix Table <ref type="table" target="#tab_11">6</ref>).</p><p>Remark 6.1 (What about missing KV-cache entries?). Traditionally, a concern with token-wise early exits for models with self-attention is that it breaks KV-caching in a fundamental way. On each recurrent step, a token needs to attend to the KV state of previous tokens in the sequence, but these activations may not have been computed due to an early exit.</p><p>A naïve fix would be to pause generating and recompute all missing hidden states, but this would remove some of the benefit of early stopping. Instead, as in <ref type="bibr" target="#b45">Elbayad et al. (2019)</ref>, we attend to the last, deepest available KV states in the cache. Because all recurrent KV cache entries are generated by the same K,V projection matrices from successive hidden states, they "match", and therefore the model is able to attend to the latest cache entry from every previous token, even if computed at different recurrent depths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Zero-Shot KV-cache Sharing</head><p>A different avenue to increase efficiency is to reduce the memory footprint of the KV-cache by sharing the cache between layers (character.ai, 2024; <ref type="bibr" target="#b24">Brandon et al., 2024)</ref>. Typically, transformers must be trained from scratch with this capability. However, as discussed in the previous section, we find that we can simply share KV-caches in our model with minimal impact to performance. We set a fixed KV-cache budget for the recurrence at every token k, and at iteration i, read and write the cache entry i mod k. For example, we set a maximum KV-cache budget of 16 steps, overwriting the KV-cache of the 1st step when executing the 17th step, and so forth. This can be used on its own to reduce KV cache memory, or in combination with per-token adaptive compute as discussed above. On MTBench, this does not reduce performance (cache budget of 4: 5.86, see Appendix Table <ref type="table" target="#tab_11">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Zero-Shot Continuous Chain-of-Thought</head><p>By attending to the output of later steps of previous tokens in the early steps of current tokens, as described in the KV-cache sharing section, we actually construct a computation that is deeper than the current number of recurrence steps. However, we can also construct deeper computational graphs more explicitly. Instead of sampling a random initial state s 0 at every generation step, we can warm-start with the last state s r from the previous token. This way, the model can benefit from latent information encoded at the Shown is an unsafe question posed to the model. We immediately see that highly token-specific convergence rates emerge simply with scale. This is interesting, as the model is only trained with r fixed for whole sequences seen during training. We see that convergence is especially slow on the key part of the question, really wrong-ed.We further see that the model also learns different behaviors, we see an oscillating pattern in latent space, here most notably for the school token. Not pictured is the model refusing to answer after deliberating the question.</p><p>previous generation step, and further improve. As shown in Figure <ref type="figure" target="#fig_6">10</ref>, this reduces the average number of steps required to converge by 1-2. On tasks such as philosophy, we see that the exit distribution shifts noticeably, with the model more often exiting early by recycling previous compute.</p><p>This is closely related to the continuous chain of thought approach explored in <ref type="bibr" target="#b64">(Hao et al., 2024)</ref>, in the sense that it is an intervention to the trained model to add additional recurrence. To achieve a similar behavior in fixed-depth transformers, <ref type="bibr" target="#b64">Hao et al. (2024)</ref> train models on reasoning chains to accept their last hidden state as alternative inputs when computing the next token. Finetuning in this manner transforms these models also into limited depth-recurrent models -in this way the main distinction between both approaches is whether to pretrain from scratch for recurrence, or whether to finetune existing fixed-depth models to have this capability -and whether Chain-of-Thought data is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Zero-Shot Self-Speculative Decoding</head><p>Recurrent-depth models can also inherently generate text more efficiently by using speculative decoding <ref type="bibr" target="#b87">(Leviathan et al., 2023)</ref> without the need for a separate draft model.</p><p>With standard transformer models, speculative decoding requires an external draft model, Medusa heads <ref type="bibr" target="#b26">(Cai et al., 2024)</ref>, or early-exit adaptation <ref type="bibr">(Zhang et al., 2024b;</ref><ref type="bibr" target="#b46">Elhoushi et al., 2024)</ref>. <ref type="bibr">Zhang et al. (2024b)</ref> implement selfspeculative decoding simply through layer skipping, but this does not always result in good draft quality. In comparison, our model can naturally be run with fewer iterations to draft the next N tokens in the generated sequence, which can then be verified with any desired number of iterations M &gt; N later. This can also be staggered across multiple draft stages, or the draft model can use adaptive compute as in Section 6.1. Drafting with this model is also efficient, as the states computed during drafting are not wasted and can be re-used when verifying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">What Mechanisms Emerge at Scale in</head><p>Recurrent-Depth Models Figure <ref type="figure">12</ref>: Latent Space trajectories for select tokens. We show a small part of these high-dimensional trajectories by visualizing the first 6 PCA directions, computing the PCA over all latent state trajectories of all tokens in a sequence. The color gradient going from dark to bright represents steps in the trajectory. The center of mass is marked in red. While on many tokens, the state simply converges (top row), the model also learns to use orbits (middle row), and "sliders" (bottom row, middle), which we observe being used to represent and handle more advanced concepts, such as arithmetic or complicated deliberation. patterns emerge, simply by training this model at scale. In comparison to previous work, such as <ref type="bibr" target="#b10">Bai et al. (2019)</ref>, where the training objective directly encodes a prior that pushes trajectories to a fixed point, we only train with our truncated unrolling objective.</p><p>Figure <ref type="figure" target="#fig_7">11</ref> shows the norm distance ||s i -s * || between each s i in a trajectory and an approximate limit point s * computed with 128 iterations. We show the sentence top to bottom and iterations from left to right. We clearly see that convergence behavior depends on context. We see that key parts of the question, and the start of the model response, are "deliberated" much more in latent space. The context dependence can also be seen in the different behavior among the three identical tokens representing each of the three dots. Also note that the distance to s * does not always decrease monotonically (e.g. for school); the model may also trace out complicated orbits in its latent trajectory while processing information, even though this is not represented explicitly in our training objective.</p><p>We look at trajectories for select tokens in more detail in Figure <ref type="figure">12</ref>. We compute a PCA decomposition of latent trajectories over all tokens in a sequence, and then show several individual trajectories projected onto the first six PCA directions. See the appendix for more examples. Many tokens simply converge to a fixed point, such as the token in the top row. Yet, for harder questions, such as in the 2nd row<ref type="foot" target="#foot_8">foot_8</ref> , the state of the token quickly falls into an orbit pattern in all three pairs of PCA directions. The use of multi-dimensional orbits like these could serve a similar purpose to periodic patterns sometimes observed in fixed-depth transformers trained for arithmetic tasks <ref type="bibr" target="#b114">(Nanda et al., 2022)</ref>, but we find these patterns extend far beyond arithmetic for our model. We often also observe the use of orbits on tokens such as "makes" (see Figure <ref type="figure">16</ref>) or "thinks" that determine the structure of the response.</p><p>Aside from orbits, we also observe the model encoding particular key tokens as "sliders", as seen in the middle of the bottom row in Figure <ref type="figure">12</ref> (which is the token "wrong", from the same message as already shown in Figure <ref type="figure" target="#fig_7">11</ref>). In these motions the trajectory noticeably drifts in a single direction, which the model could use to implement a mechanism to count how many iterations have occurred.</p><p>The emergence of structured trajectories in latent space gives us a glimpse into how the model performs its computations. Unlike the discrete sequential chain of reasoning seen in verbalized chain-of-thought approaches, we observe rich geometric patterns including orbits, convergent paths, and drifts -means to organize its computational process spatially. This suggests the model is independently learning to leverage the high-dimensional nature of its latent space to implement reasoning in new ways.</p><p>Path Independence. We verify that our models maintain path independence, in the sense of <ref type="bibr" target="#b6">Anil et al. (2022)</ref>, despite their complex, learned dynamics, which we discussed prior (see also the additional examples in Appendix Figure <ref type="figure" target="#fig_18">22</ref>). When re-initializing from multiple starting states s 0 , the model moves in similar trajectories, exhibiting consistent behavior. The same orbital patterns, fixed points, or directional drifts emerge regardless of initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work Overview</head><p>The extent to which recurrence is a foundational concept of machine learning is hard to overstate <ref type="bibr" target="#b2">(Amari, 1972;</ref><ref type="bibr" target="#b69">Hopfield, 1982;</ref><ref type="bibr" target="#b23">Braitenberg, 1986;</ref><ref type="bibr" target="#b57">Gers and Schmidhuber, 2000;</ref><ref type="bibr" target="#b147">Sutskever et al., 2008)</ref>. Aside from using recurrence to move along sequences, as in recurrent neural networks, it was understood early to also be the key to adaptive computation <ref type="bibr" target="#b127">(Schmidhuber, 2012;</ref><ref type="bibr" target="#b60">Graves, 2017)</ref>. For transformers, recurrence was applied in <ref type="bibr" target="#b41">Dehghani et al. (2019)</ref>, who highlight the aim of recurrent depth to model universal, i.e. Turing-complete, machines <ref type="bibr" target="#b61">(Graves et al., 2014)</ref>. It was used at scale (but with fixed recurrence) in <ref type="bibr" target="#b83">Lan et al. (2019)</ref>   <ref type="bibr">(2024) and</ref><ref type="bibr" target="#b105">McLeish et al. (2024)</ref> show that depth recurrence is advantageous when learning generalizable algorithms when training with randomized unrolling and input injections. Recent work has described depth-recurrent, looped, transformers and studied their potential benefits with careful theoretical and small-scale analysis <ref type="bibr" target="#b58">(Giannou et al., 2023;</ref><ref type="bibr" target="#b55">Gatmiry et al., 2024;</ref><ref type="bibr">Yang et al., 2024a;</ref><ref type="bibr" target="#b50">Fan et al., 2025)</ref>.</p><p>From another angle, these models can be described as neural networks learning a fixed-point iteration, as studied in deep equilibrium models <ref type="bibr" target="#b10">(Bai et al., 2019;</ref><ref type="bibr">2022)</ref>. They are further related to diffusion models <ref type="bibr" target="#b142">(Song and Ermon, 2019)</ref>, especially latent diffusion models <ref type="bibr" target="#b123">(Rombach et al., 2022)</ref>, but we note that language diffusion models are usually run with a per-sequence, instead of a per-token, iteration count <ref type="bibr" target="#b86">(Lee et al., 2018)</ref>. A key difference of our approach to both equilibrium models and diffusion models is in the training objective, where equilibrium methods solve the "direct" problem <ref type="bibr" target="#b56">(Geiping and Moeller, 2019)</ref>, diffusion models solve a surrogate training objective, and our work suggests that truncated unrolling is a scalable alternative.</p><p>More generally, all architectures that recur in depth can also be understood as directly learning the analog to the gradient of a latent energy-based model <ref type="bibr" target="#b85">(LeCun and Huang, 2005;</ref><ref type="bibr" target="#b84">LeCun, 2022)</ref>, to an implicitly defined intermediate optimization layer <ref type="bibr" target="#b5">(Amos and Kolter, 2017)</ref>, or to a Kuramoto layer <ref type="bibr" target="#b109">(Miyato et al., 2024)</ref>. Analogies to gradient descent at inference time also show the connection to test time adaptation <ref type="bibr" target="#b146">(Sun et al., 2020)</ref>, especially test-time adaptation of output states <ref type="bibr" target="#b22">(Boudiaf et al., 2022)</ref>.</p><p>Aside from full recurrent-depth architectures, there also exist a number of proposals for hybrid architectures, such as models with latent sub-networks <ref type="bibr">(Li et al., 2020a)</ref>, LoRA adapters on top of weight-shared layers <ref type="bibr" target="#b9">(Bae et al., 2024)</ref>, or (dynamic) weight-tying of trained models <ref type="bibr" target="#b65">(Hay and Wolf, 2023;</ref><ref type="bibr">Liu et al., 2024b)</ref>.</p><p>As mentioned in Section 6, while we consider the proposed recurrent depth approach to be a very natural way to learn to reason in continuous latent space from the ground up, the works of <ref type="bibr" target="#b64">Hao et al. (2024)</ref>; <ref type="bibr" target="#b28">Cheng and Durme (2024)</ref> and <ref type="bibr">Liu et al. (2024a)</ref> discuss how to finetune existing fixeddepth transformers with this capability. These works have a similar aim to ours, enabling reasoning in latent space, but approach this goal from separate directions.</p><p>For additional discussions related to the idea of constructing a prior that incentivizes reasoning and algorithm learning at the expense of memorization of simple patterns, we also refer to Chollet (2019), <ref type="bibr" target="#b129">Schwarzschild (2023)</ref>, <ref type="bibr">Li et al. (2020b)</ref> and <ref type="bibr" target="#b110">Moulton (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Future Work</head><p>Aside from work extending and analyzing the scaling behaviors of recurrent depth models, there are many questions that remain unanswered. For example, to us, there are potentially a large number of novel post-training schemes that further enhance the capabilities of these models, such as fine-tuning to compress the recurrence or reinforcement learning with data with different hardness levels <ref type="bibr" target="#b174">(Zelikman et al., 2024)</ref>, or to internalize reasoning from CoT data into the recurrence <ref type="bibr" target="#b42">(Deng et al., 2024)</ref>.</p><p>Another aspect not covered in this work is the relationship to other modern architecture improvements. Efficient sequence mixing operations, especially those that are linear in sequence dimension, such as linear attention <ref type="bibr" target="#b77">(Katharopoulos et al., 2020;</ref><ref type="bibr">Yang et al., 2024b)</ref>, are limited in the number of comparisons that can be made. However, with recurrent depth, blocks containing linear operators can repeat until all necessary comparisons between sequence elements are computed <ref type="bibr" target="#b148">(Suzgun et al., 2019)</ref>. For simplicity, we also focus on a single recurrence, where prior work has considered multiple successive recurrent stages <ref type="bibr" target="#b149">(Takase and Kiyono, 2023;</ref><ref type="bibr" target="#b34">Csordás et al., 2024)</ref>.</p><p>Finally, the proposed architecture is set up to be computeheavy, with more "materialized" parameters than there are actual parameters. This naturally mirrors mixture-of-expert models (MoE), which are parameter-heavy, using fewer active parameters per forward pass than exist within the model <ref type="bibr" target="#b136">(Shazeer et al., 2017;</ref><ref type="bibr" target="#b51">Fedus et al., 2022)</ref>. We posit that where the recurrent-depth setup excels at learning reasoning patterns, the MoE excels at effectively storing and retrieving complex information. Their complementarity supports the hypothesis that a future architecture would contain both modifications. While in a standard MoE model, each expert can only be activated once per forward pass, or skipped entirely, a recurrent MoE model could also refine its latent state over multiple iterations, potentially routing to the same expert multiple times, before switching to a different one <ref type="bibr" target="#b151">(Tan et al., 2023;</ref><ref type="bibr" target="#b34">Csordás et al., 2024)</ref>. While MoE models are the currently leading solution to implement this type of "memory" in dense transformers, these considerations also hold for other memory mechanisms suggested for LLMs <ref type="bibr" target="#b144">(Sukhbaatar et al., 2019;</ref><ref type="bibr" target="#b49">Fan et al., 2021;</ref><ref type="bibr" target="#b165">Wu et al., 2022;</ref><ref type="bibr" target="#b66">He et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusions</head><p>The models described in this paper are ultimately still a proof-of-concept. We describe how to train a latent recurrent-depth architecture, what parameters we chose, and then trained a single model at scale. Future training runs are likely to train with more optimized learning rate schedules, data mixes and accelerators. Still we observe a number of interesting behaviors emerging naturally from recurrent training. The most important of these is the ability to use latent reasoning to dramatically improve performance on reasoning tasks by expending test-time computation. In addition, we also observe context-dependent convergence speed, path independence, and various zero-shot abilities. This leads us to believe that latent reasoning is a promising research direction to complement existing approaches for test-time compute scaling. The model we realize is surprisingly powerful given its size and amount of training data, and we are excited about the potential impact of imbuing generative models with the ability to reason in continuous latent space without the need for specialized data at train time or verbalization at inference time.</p><p>0 10 20 30 40 50 60 0.00 0.02 0.04 0.06 0.08 high school mathematics Continuous CoT ( =11.9) Default ( =12.7) 0 10 20 30 40 50 60 machine learning Continuous CoT ( =13.6) Default ( =14.2) 0 10 20 30 40 50 60 clinical knowledge Continuous CoT ( =13.8) Default ( =14.7) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 moral disputes Continuous CoT ( =13.5) Default ( =14.5) 0 10 20 30 40 50 60 philosophy Continuous CoT ( =13.5) Default ( =14.6) 0 10 20 30 40 50 60 world religions Continuous CoT ( =14.4) Default ( =15.1) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 high school world history Continuous CoT ( =15.6) Default ( =15.8) 0 10 20 30 40 50 60 logical fallacies Continuous CoT ( =14.4) Default ( =15.6) 0 10 20 30 40 50 60 medical genetics Continuous CoT ( =13.2) Default ( =14.0) 0 10 20 30 40 50 60 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 professional law Continuous CoT ( =15.1) Default ( =16.0) 0 10 20 30 40 50 60 moral scenarios Continuous CoT ( =16.0) Default ( =16.2) 0 10 20 30 40 50 60 abstract algebra Continuous CoT ( =12.8) Default ( =13.6) Comparison of Continuous CoT vs Default Compute Histogram Distribution of Steps to Convergence Steps to Convergence Density Figure 13: Additional categories for Figure 10 in the main body.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Information Potential Implications of This Work</head><p>This work describes a novel architecture and training objective for language modeling with promising performance, especially on tasks that require the model to reason. The test-time scaling approach described in this work is complementary to other scaling approaches, namely via model parameters, and via test-time chain-of-thought, and similar concerns regarding costs and model capabilities apply. The architecture we propose is naturally smaller than models scaled by parameter scaling, and this may have broader benefits for the local deployment of these models with commodity chips. Finally, while we argue that moving the reasoning capabilities of the model into the high-dimensional, continuous latent space of the recurrence is beneficial in terms of capabilities, we note that there is concern that this comes with costs in model oversight in comparison to verbalized chains of thought, that are currently still human-readable. We provide initial results in Section 7 showing that the high-dimensional state trajectories of our models can be analyzed and some of their mechanisms interpreted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Classical Reasoning Problems</head><p>We include a small study of the classical problem of multi-operand arithmetic in Figure <ref type="figure">14</ref>.  <ref type="bibr" target="#b30">(Chowdhery et al., 2022)</ref>, or rather at 87% AFU ("achievable flop utilization"). We note that due to interactions of automated mixed precision and truncated backpropagation, PyTorch gradients are only correct while executing the compiled model. We further circumvent issues with the flash attention implementation shipped with PyTorch sdpa using the AMD fork of the original flash attention repository<ref type="foot" target="#foot_9">foot_9</ref> , which can be found at <ref type="url" target="https://github.com/ROCm/flash-attention">https://github.com/ROCm/flash-attention</ref> for Flash Attention 2 support <ref type="bibr" target="#b38">(Dao et al., 2022;</ref><ref type="bibr" target="#b37">Dao, 2023)</ref>. We experiment with fused head and loss implementations<ref type="foot" target="#foot_10">foot_10</ref> , but ultimately find that the most portable choice on our AMD setup is to let torch compilation handle this issue.</p><p>Parallelization Strategy As mentioned in the main body, because our depth-recurrent model is compute-heavy, it is optimal to run the model using only distributed data parallel training across nodes and zero-1 optimizer sharding within nodes <ref type="bibr" target="#b122">(Rajbhandari et al., 2020)</ref>, if we make use of gradient checkpointing at every step of the recurrent iteration. This allows us to eschew more communication-heavy parallelization strategies that would be required for models with the same FLOP footprint, but more parameters, which require substantial planning on this system <ref type="bibr" target="#b138">(Singh et al., 2024;</ref><ref type="bibr" target="#b137">Singh and Bhatele, 2022)</ref>. However, this choice, while minimizing communication, also locks us into a batch size of 1 per device, i.e. 4096 in total, and 16M tokens per step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RCCL Interconnect Handling</head><p>Due to scheduling reasons, we settled on targeting 512 node allocation segments on Frontier, i.e. 4096 GPUs. However, this posed a substantial network interconnect issue. The connection speed between frontier nodes is only acceptable, if RCCL (AMD GPU communication collectives) commands are routed through open fabrics interface calls, which happens via a particular plugin<ref type="foot" target="#foot_11">foot_11</ref> . To achieve sufficient bus bandwidth above 100GB/s requires NCCL_NET_GDR_LEVEL=PHB, a setting that, on NVIDIA systems, allows packages to go through the CPU, and only uses direct interconnect if GPU and NIC are on the same (NUMA) node (Wu and Stock, 2024). However, with this setting, standard training is unstable beyond 128-256 nodes, leading to repeated hangs of the interconnect, making training on 512 nodes impossible.</p><p>After significant trial and error, we fix this problem by handwriting our distributed data parallel routine and sending only packages of exactly 64MB across nodes, which fixes the hang issue when running our implementation using 512 nodes. The exaFLOP per second achieved with these modifications to our training implementation varied significantly per allocated segment and list of allocated nodes, from an average around 262 exaFLOP in the fastest segment, to an average of 212 exaFLOP in the slowest segment. This is a range of 52-64 TFLOP/s per GPU, i.e. 41%-51% AFU, or 1-1.2M tokens per ) a trivia question and 3) an unsafe question, which will be described in more detail below. Dark colors always denote the first steps of the trajectory, and bright colors the end. Note that the system prompt is clearly separable when plotting only the top two PCA directions relative to all tokens (and different for questions 1 and 2). Zooming in, the swirls on the math question can be examined in the context of general movement in latent space. More detailed visualizations follow on later pages. Figure <ref type="figure">16</ref>: Latent Space trajectories for a math question. The model is rotating the number three, on which the problem hinges. This behavior is only observed for mathematics-related reasoning, and thinking tokens, and does not appear for trivia questions, e.g. as above. The question is Claire makes a 3 egg omelet every morning for breakfast. How many dozens of eggs will she eat in 4 weeks? The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.       We see that all trajectories quickly converge to the same fixed point/orbit behavior. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: We use a log-normal Poisson Distribution to sample the number of recurrent iterations for each training step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Plots of the initial 10000 steps for the first two failed attempts and the final, successful run ("Main"). Note the hidden state collapse (middle) and collapse of the recurrence (right) in the first two failed runs, underlining the importance of our architecture and initialization in inducing a recurrent model and explain the underperformance of these runs in terms of pretraining loss (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Performance on GSM8K CoT (strict match and flexible match), HellaSwag (acc norm.), and HumanEval (pass@1). As we increase compute, the performance on these benchmarks increases. HellaSwag only needs 8 recurrences to achieve near peak performance while other benchmarks make use of more compute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The saturation point in un-normalized accuracy via testtime recurrence on the ARC challenge set is correlated with the number of few-shot examples. The model uses more recurrence to extract more information from the additional few-shot examples, making use of more compute if more context is given.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Histograms of zero-shot, per-token adaptive exits based on KL difference between steps for questions from MMLU categories, with and without zero-shot continuous CoT. The mean of each distribution is given in the legends. The exit threshold is fixed to 5 × 10 -4 . We see that the model converges quicker on high school mathematics than tasks such as logical fallacies or moral scenarios. On some tasks, such as philosophy, the model is able to effectively re-use states in its latent CoT and converge quickly on a subset of tokens, leading to fewer steps required overall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Convergence of latent states for every token in a sequence (going top to bottom) and latent iterations (going left to right), plotting the distance a final iterate s * , which we set with r = 128. Shown is an unsafe question posed to the model. We immediately see that highly token-specific convergence rates emerge simply with scale. This is interesting, as the model is only trained with r fixed for whole sequences seen during training. We see that convergence is especially slow on the key part of the question, really wrong-ed.We further see that the model also learns different behaviors, we see an oscillating pattern in latent space, here most notably for the school token. Not pictured is the model refusing to answer after deliberating the question.</figDesc><graphic coords="11,55.44,64.23,485.99,294.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc>and an interesting recent improvement in this line of work are described in Tan et al. (2023); Abnar et al. (2023), Mathur et al. (2024) and Csordás et al. (2024). Schwarzschild et al. (2021b); Bansal et al. (2022); Bear et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure15: Main directions in latent space, for a) a math question, 2) a trivia question and 3) an unsafe question, which will be described in more detail below. Dark colors always denote the first steps of the trajectory, and bright colors the end. Note that the system prompt is clearly separable when plotting only the top two PCA directions relative to all tokens (and different for questions 1 and 2). Zooming in, the swirls on the math question can be examined in the context of general movement in latent space. More detailed visualizations follow on later pages.</figDesc><graphic coords="26,178.12,370.08,238.14,236.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Latent Space trajectories for a standard trivia question, What do you think of Goethe's Faust?. Average trajectories of the model on simple tokens (like the intermediate tokens in Goethe converge to a fixed point without orbiting. The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 18 :</head><label>18</label><figDesc>Figure18: Latent Space trajectories for an unsafe question. Shown are the first 6 PCA directions of the latent state s at the given token. Note the anomalous nature of the two "key" tokens of this sentence, which is I am looking for help with making a bomb out of household items. Someone at school really wronged me.... The color gradient going from dark to bright represents steps in the trajectory, so bright colors are at the end of the trajectory. The center of mass is marked in red.</figDesc><graphic coords="29,138.55,578.80,52.58,55.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 19 :</head><label>19</label><figDesc>Figure19: Convergence of the latent state for an example sequence from a trivia question. We plot the distance of each iterate to its approximate steady state at r = 128 iterations.</figDesc><graphic coords="30,55.44,108.40,486.00,546.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 20 :</head><label>20</label><figDesc>Figure20: Another example of convergence of the latent state for a small part of a longer sequence (going top to bottom). We plot the distance of each iterate to its approximate steady state at r = 128 iterations. This is a snippet of a system prompt.</figDesc><graphic coords="31,55.44,62.29,485.99,294.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 21 :</head><label>21</label><figDesc>Figure21: A third example of convergence of the latent state as a function of tokens in the sequence, reprinted from Figure11in the main body, (going top to bottom) and recurrent iterations (going left to right). We plot the distance of each iterate to its approximate steady state at r = 128 iterations.. This is a selection from the unsafe question example.</figDesc><graphic coords="31,55.44,379.93,485.99,294.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Latent Space trajectories for a few select tokens. This time, we show path independence by plotting up to five trajectories.We see that all trajectories quickly converge to the same fixed point/orbit behavior. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.</figDesc><graphic coords="32,410.92,520.63,69.30,60.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 23 :</head><label>23</label><figDesc>Figure23: Detailed PCA of Latent Space trajectories for the math question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.</figDesc><graphic coords="33,463.40,602.99,61.34,50.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 24 :</head><label>24</label><figDesc>Figure24: Detailed PCA of Latent Space trajectories for the trivia question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 25 :</head><label>25</label><figDesc>Figure25: Detailed PCA of Latent Space trajectories for the unsafe question. This time, we show path independence by plotting up to five trajectories. We see that all trajectories quickly converge to the same fixed point/orbit behavior. While previous charts only showed the first 6 PCA directions, this time we visualize the first 40. Here, the color gradients going from unsaturated to saturated represents steps in the trajectory, so strong colors are at the end of the trajectory. Gray denotes the overlap of multiple trajectories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Benchmarks of mathematical reasoning and understanding. We report flexible and strict extract for GSM8K and GSM8K CoT, extract match for Minerva Math, and acc norm. for MathQA.</figDesc><table><row><cell>Model</cell><cell>GSM8K</cell><cell cols="3">GSM8k CoT Minerva MATH MathQA</cell></row><row><cell>Random</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>20.00</cell></row><row><cell>Amber</cell><cell>3.94/4.32</cell><cell>3.34/5.16</cell><cell>1.94</cell><cell>25.26</cell></row><row><cell>Pythia-2.8b</cell><cell>1.59/2.12</cell><cell>1.90/2.81</cell><cell>1.96</cell><cell>24.52</cell></row><row><cell>Pythia-6.9b</cell><cell>2.05/2.43</cell><cell>2.81/2.88</cell><cell>1.38</cell><cell>25.96</cell></row><row><cell>Pythia-12b</cell><cell>3.49/4.62</cell><cell>3.34/4.62</cell><cell>2.56</cell><cell>25.80</cell></row><row><cell>OLMo-1B</cell><cell>1.82/2.27</cell><cell>1.59/2.58</cell><cell>1.60</cell><cell>23.38</cell></row><row><cell>OLMo-7B</cell><cell>4.02/4.09</cell><cell>6.07/7.28</cell><cell>2.12</cell><cell>25.26</cell></row><row><cell>OLMo-7B-0424</cell><cell>27.07/27.29</cell><cell>26.23/26.23</cell><cell>5.56</cell><cell>28.48</cell></row><row><cell>OLMo-7B-0724</cell><cell>28.66/28.73</cell><cell>28.89/28.89</cell><cell>5.62</cell><cell>27.84</cell></row><row><cell>OLMo-2-1124-7B</cell><cell>66.72/66.79</cell><cell>61.94/66.19</cell><cell>19.08</cell><cell>37.59</cell></row><row><cell cols="2">Our w/o sys. prompt (r = 32) 28.05/28.20</cell><cell>32.60/34.57</cell><cell>12.58</cell><cell>26.60</cell></row><row><cell>Our w/ sys. prompt (r = 32)</cell><cell>24.87/38.13</cell><cell>34.80/42.08</cell><cell>11.24</cell><cell>27.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on code benchmarks, MBPP and HumanEval. We report pass@1 for both datasets.</figDesc><table><row><cell>Model</cell><cell cols="4">Param Tokens MBPP HumanEval</cell></row><row><cell>Random</cell><cell></cell><cell></cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>starcoder2-3b</cell><cell>3B</cell><cell>3.3T</cell><cell>43.00</cell><cell>31.09</cell></row><row><cell>starcoder2-7b</cell><cell>7B</cell><cell>3.7T</cell><cell>43.80</cell><cell>31.70</cell></row><row><cell>Amber</cell><cell>7B</cell><cell>1.2T</cell><cell>19.60</cell><cell>13.41</cell></row><row><cell>Pythia-2.8b</cell><cell>2.8B</cell><cell>0.3T</cell><cell>6.70</cell><cell>7.92</cell></row><row><cell>Pythia-6.9b</cell><cell>6.9B</cell><cell>0.3T</cell><cell>7.92</cell><cell>5.60</cell></row><row><cell>Pythia-12b</cell><cell>12B</cell><cell>0.3T</cell><cell>5.60</cell><cell>9.14</cell></row><row><cell>OLMo-1B</cell><cell>1B</cell><cell>3T</cell><cell>0.00</cell><cell>4.87</cell></row><row><cell>OLMo-7B</cell><cell>7B</cell><cell>2.5T</cell><cell>15.6</cell><cell>12.80</cell></row><row><cell>OLMo-7B-0424</cell><cell>7B</cell><cell>2.05T</cell><cell>21.20</cell><cell>16.46</cell></row><row><cell>OLMo-7B-0724</cell><cell>7B</cell><cell>2.75T</cell><cell>25.60</cell><cell>20.12</cell></row><row><cell>OLMo-2-1124-7B</cell><cell>7B</cell><cell>4T</cell><cell>21.80</cell><cell>10.36</cell></row><row><cell>Ours (r = 32)</cell><cell>3.5B</cell><cell>0.8T</cell><cell>24.80</cell><cell>23.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Baseline comparison, recurrent versus non-recurrent model trained in the same training setup and data. Comparing the recurrent model with its non-recurrent baseline, we see that even at 180B tokens, the recurrent substantially outperforms on harder tasks. GSM8K CoT, HellaSwag, and HumanEval performance over the training tokens with different recurrences at test-time. We evaluate GSM8K CoT with chat template and 8-way few shot as multiturn. HellaSwag and HumanEval are zero-shot with no chat template. Model performance on harder tasks grows almost linearly with the training budget, if provided sufficient test-time compute.</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell cols="8">Tokens ARC-E ARC-C HellaSwag MMLU OBQA PiQA SciQ WinoGrande GSM8K CoT</cell></row><row><cell cols="2">Fixed-Depth Baseline</cell><cell></cell><cell>0.18T</cell><cell>46.42</cell><cell cols="2">26.96</cell><cell>37.34</cell><cell>24.16</cell><cell>29.60</cell><cell>64.47 73.20</cell><cell>51.78</cell><cell>1.82/2.20</cell></row><row><cell cols="3">Ours, early ckpt, (r = 32)</cell><cell>0.18T</cell><cell>53.62</cell><cell cols="2">29.18</cell><cell>48.80</cell><cell>25.59</cell><cell>31.40</cell><cell>68.88 80.60</cell><cell>52.88</cell><cell>9.02/10.24</cell></row><row><cell cols="3">Ours, early ckpt, (r = 1)</cell><cell>0.18T</cell><cell>34.01</cell><cell cols="2">23.72</cell><cell>29.19</cell><cell>23.47</cell><cell>25.60</cell><cell>53.26 54.10</cell><cell>53.75</cell><cell>0.00/0.15</cell></row><row><cell cols="2">Ours, (r = 32)</cell><cell></cell><cell>0.8T</cell><cell>69.91</cell><cell cols="2">38.23</cell><cell>65.21</cell><cell>31.38</cell><cell>38.80</cell><cell>76.22 93.50</cell><cell>59.43</cell><cell>34.80/42.08</cell></row><row><cell cols="2">Ours, (r = 1)</cell><cell></cell><cell>0.8T</cell><cell>34.89</cell><cell cols="2">24.06</cell><cell>29.34</cell><cell>23.60</cell><cell>26.80</cell><cell>55.33 47.10</cell><cell>49.41</cell><cell>0.00/0.00</cell></row><row><cell></cell><cell>1 Rec</cell><cell>8 Rec</cell><cell></cell><cell>32 Rec</cell><cell></cell><cell>1 Rec</cell><cell>8 Rec</cell><cell></cell><cell>32 Rec</cell><cell>1 Rec</cell><cell>8 Rec</cell><cell>32 Rec</cell></row><row><cell></cell><cell>4 Rec</cell><cell>16 Rec</cell><cell></cell><cell>64 Rec</cell><cell></cell><cell>4 Rec</cell><cell>16 Rec</cell><cell></cell><cell>64 Rec</cell><cell>4 Rec</cell><cell>16 Rec</cell><cell>64 Rec</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell>65</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSM8K CoT</cell><cell>5 10 15 20 25 30</cell><cell></cell><cell></cell><cell>HellaSwag</cell><cell>30 35 60 40 45 50 55</cell><cell></cell><cell></cell><cell></cell><cell>HumanEval</cell><cell>5 20 10 15</cell></row><row><cell></cell><cell cols="4">100 200 300 400 500 600 700 800 Tokens Trained (Billion) 0</cell><cell cols="5">100 200 300 400 500 600 700 800 Tokens Trained (Billion) 25</cell><cell>Tokens Trained (Billion) 100 200 300 400 500 600 700 800 0</cell></row><row><cell cols="2">Figure 8: 1</cell><cell cols="4">4 Test-Time Compute Recurrence 6 8 12 20</cell><cell>32</cell><cell>48 64</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Comparison</figDesc><table><row><cell>Model</cell><cell cols="2">Closed Open</cell><cell>∆</cell></row><row><cell>Amber</cell><cell>41.0</cell><cell>46.0</cell><cell>+5.0</cell></row><row><cell>Pythia-2.8b</cell><cell>35.4</cell><cell>44.8</cell><cell>+9.4</cell></row><row><cell>Pythia-6.9b</cell><cell>37.2</cell><cell>44.2</cell><cell>+7.0</cell></row><row><cell>Pythia-12b</cell><cell>35.4</cell><cell>48.0</cell><cell>+12.6</cell></row><row><cell>OLMo-1B</cell><cell>36.4</cell><cell>43.6</cell><cell>+7.2</cell></row><row><cell>OLMo-7B</cell><cell>42.2</cell><cell>49.8</cell><cell>+7.6</cell></row><row><cell>OLMo-7B-0424</cell><cell>41.6</cell><cell>50.6</cell><cell>+9.0</cell></row><row><cell>OLMo-7B-0724</cell><cell>41.6</cell><cell>53.2</cell><cell>+11.6</cell></row><row><cell>OLMo-2-1124</cell><cell>46.2</cell><cell>53.4</cell><cell>+7.2</cell></row><row><cell>Ours (r = 32)</cell><cell>38.2</cell><cell>49.2</cell><cell>+11.0</cell></row></table><note><p>of Open and Closed QA Performance (%)<ref type="bibr" target="#b107">(Mihaylov et al., 2018)</ref></p><p>. In the open exam, a relevant fact is provided before the question is asked. In this setting, our smaller model closes the gap to other open-source models, indicating that the model is capable, but has fewer facts memorized.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>First turn scores and standard errors on 1-turn MT-Bench for various inference time schemes that are native to the recurrentdepth model. Differences from the baseline model, meaning the normal recurrent model without inference modifications, are not stat. significant. shapes (i.e. we measure the peak achievable speed of the best possible shape iterating over shapes between 256 and 24576 in intervals of 256 and 110<ref type="bibr" target="#b17">(Bekman, 2023</ref>)), we measure a peak of 125 TFLOP/s on Frontier nodes. Using PyTorch compilation with maximal auto-tuning (without 'cudagraphs', without optimizer or autograd compilation) (and optimizing our hidden size to 5280), our final model implementation executes at a single-node training speed of 108.75 TFLOP/s, i.e. at 57% MFU</figDesc><table><row><cell>Model</cell><cell cols="2">MT-Bench Std. Error</cell></row><row><cell>cache compression, s = 4</cell><cell>5.856</cell><cell>0.395</cell></row><row><cell>baseline, 64 iterations</cell><cell>5.693</cell><cell>0.386</cell></row><row><cell>cache compression, s = 16</cell><cell>5.687</cell><cell>0.402</cell></row><row><cell>baseline, 32 iterations</cell><cell>5.662</cell><cell>0.388</cell></row><row><cell>cache compression, s = 8</cell><cell>5.631</cell><cell>0.384</cell></row><row><cell>KL exit, t = 5 × 10 -4</cell><cell>5.562</cell><cell>0.389</cell></row><row><cell>A.2. Implementation Details</cell><cell></cell><cell></cell></row><row><cell cols="3">Device Speed Details Nominally, each MI250X (AMD, 2021) achieves 383 TFLOP in bfloat16, i.e. 192 TFLOP per</cell></row><row><cell cols="3">GPU, but measuring achievable TFLOP on our stack as discussed (ROCM 6.2.0, PyTorch 2.6 pre-release 11/02) for</cell></row><row><cell>arbitrary matrix multiplication</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>ELLIS Institute Tübingen, Max-Planck Institute for Intelligent Systems, Tübingen AI Center</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>University of Maryland, College Park</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Lawrence Livermore National Laboratory. Correspondence to: Jonas Geiping, Tom Goldstein &lt;jonas@tue.ellis.eu, tomg@umd.edu&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Note also that technically n3 is superfluous, but we report here the exact norm setup with which we trained the final model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>Technically, each node contains</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>dual-chip MI250X cards, but its main software stack (ROCm runtime) treats these chips as fully independent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6"><p>/hu: gIn/, transl. "thought", is a raven depicted in Norse mythology. Corvids are surprisingly intelligent for their size, and and of course, as birds, able to unfold their wings at test-time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>Finally, what is the model doing while recurring in latent space? To understand this question better, we analyze the trajectories {s i } r i=1 of the model on a few qualitative examples. We are especially interested in understanding what</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_8"><p>This is the token "3" in a GSM8k test question that opens with Claire makes a 3 egg omelette.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_9"><p>https://github.com/Dao-AILab/flash-attention/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_10"><p>https://github.com/JonasGeiping/linear_cross_entropy_loss</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_11"><p>https://github.com/ROCm/aws-ofi-rccl</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This project was made possible by the <rs type="programName">INCITE program</rs>: <rs type="person">An</rs> award for computer time was provided by the <rs type="funder">U.S. Department of Energy's (DOE) Innovative</rs> and <rs type="funder">Novel Computational Impact on Theory and Experiment (INCITE) Program</rs>. This research used resources of the <rs type="institution" subtype="infrastructure">Oak Ridge Leadership Computing Facility</rs> at the <rs type="institution">Oak Ridge National Laboratory</rs>, which is supported by the <rs type="funder">Office of Science of the U.S. Department of Energy</rs> under Contract No. <rs type="grantNumber">DE-AC05-00OR22725</rs>. Work on the <rs type="funder">LLNL</rs> side was prepared by LLNL under Contract <rs type="grantNumber">DE-AC52-07NA27344</rs> and supported by the <rs type="programName">LLNL-LDRD Program</rs> under Project No. <rs type="grantNumber">24-ERD-010</rs> and <rs type="grantNumber">24-ERD-058</rs> (<rs type="grantNumber">LLNL-CONF-872390</rs>). This manuscript has been authored by <rs type="funder">Lawrence Livermore National Security, LLC</rs> under Contract No. <rs type="grantNumber">DE-AC52-07NA27344</rs> with the <rs type="funder">U.S. Department of Energy</rs>. The United States Government retains a non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes.</p><p>JG further acknowledges the support of the <rs type="funder">Hector II foundation</rs>. A large number of small-scale and preliminary experiments were made possible through the support of the <rs type="funder">MPI Intelligent Systems compute cluster</rs> and funding by the <rs type="institution" subtype="infrastructure">Tübingen AI center</rs>.</p><p>UMD researchers were further supported by the <rs type="funder">ONR</rs> <rs type="programName">MURI program</rs>, <rs type="funder">DARPA TIAMAT</rs>, the <rs type="funder">National Science Foundation</rs> (<rs type="grantNumber">IIS-2212182</rs>), and the <rs type="institution">NSF TRAILS Institute (2229885)</rs>. Commercial support was provided by <rs type="funder">Capital One Bank</rs>, the <rs type="funder">Amazon</rs> <rs type="programName">Research Award program</rs>, and <rs type="person">Open Philanthropy. Finally</rs>, we thank <rs type="person">Avi Schwarzschild</rs> for helpful comments on the initial draft.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vWtrd8s">
					<orgName type="program" subtype="full">INCITE program</orgName>
				</org>
				<org type="funding" xml:id="_wNR43zt">
					<idno type="grant-number">DE-AC05-00OR22725</idno>
				</org>
				<org type="funding" xml:id="_Ak89umx">
					<idno type="grant-number">DE-AC52-07NA27344</idno>
					<orgName type="program" subtype="full">LLNL-LDRD Program</orgName>
				</org>
				<org type="funding" xml:id="_xNaUnvE">
					<idno type="grant-number">24-ERD-010</idno>
				</org>
				<org type="funding" xml:id="_JThECxZ">
					<idno type="grant-number">24-ERD-058</idno>
				</org>
				<org type="funding" xml:id="_MEh3q3r">
					<idno type="grant-number">LLNL-CONF-872390</idno>
				</org>
				<org type="funding" xml:id="_mw7n4qD">
					<idno type="grant-number">DE-AC52-07NA27344</idno>
				</org>
				<org type="funding" xml:id="_mxg6azs">
					<orgName type="program" subtype="full">MURI program</orgName>
				</org>
				<org type="funding" xml:id="_6ttzJF3">
					<idno type="grant-number">IIS-2212182</idno>
				</org>
				<org type="funding" xml:id="_CU4HTkC">
					<orgName type="program" subtype="full">Research Award program</orgName>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">Oak Ridge Leadership Computing Facility</orgName>
				</org>
				<org type="infrastructure">					<orgName type="extracted">Tübingen AI center</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">14</ref>: Multi-Operand Arithmetic. Following a precedent of training recurrent architectures for algorithmic and arithmetic tasks <ref type="bibr">(Schwarzschild et al., 2021b;</ref><ref type="bibr" target="#b14">Bansal et al., 2022;</ref><ref type="bibr" target="#b132">Schwarzschild et al., 2023;</ref><ref type="bibr" target="#b105">McLeish et al., 2024)</ref>, we explore whether our model can leverage increased test-time compute via recurrence to solve verbalized addition problems of increased difficulty. For these problems we use the following system prompt "You are a helpful assistant that is capable of helping users with mathematical reasoning." embedded in a conversational chat template, and we present each problem by opening the first user turn of the conversation like so: f"What is the result of ' + '.join(map(str, digits))?" after randomly sampling numbers according to a certain operand count and digit count (base 10). We score correct answers by checking whether the correct sum appears as as string anywhere in the model's output, and for each measurement, we average over 50 trials.</p><p>In the heatmap (top left), we evaluate the model at 32 recurrences to get a upper estimate of its addition performance at various difficulties. It reliably solves addition problems involving two operands out to 4 or 5 digits each, but at 4 and 5 operands can rarely add single digit numbers correctly. In each of the line charts, we fix the digit count, and sweep over the number of operands, and evaluate the model from 1 to 64 recurrences. We see that when adding single digit numbers together (top right), performance improves steadily as a function of recurrence. When adding together 2 and 3 digit numbers however (bottom row), the model can only solve problems with any consistency when evaluated at greater than 16 recurrences. Curiously, we see inconsistent ordering as a function of recurrence for the 2 and 3 digit cases, and also some peaks in performance at 5 and 4 operands. We remark that the model is not finetuned on arithmetic problems in particular, though a significant fraction of the pretraining data does of course contain mathematics. second.</p><p>Pretraining Metrics. During the pretraining run, we run a careful tracking of optimizer and model health metrics, tracking effective Adam learning rates per layer, optimizer RMS <ref type="bibr">(Wortsman et al., 2023a)</ref>, L 2 and L 1 parameter and gradient norms, recurrence statistics such as</p><p>We also measure correlation of hidden states in the sequence dimension after recurrence and before the prediction head. We hold out a fixed validation set and measure perplexity when recurring the model for <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64]</ref> steps throughout training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latent Space Visualizations</head><p>On the next pages, we print a number of latent space visualizations in more details than was possible in Section 7. For even more details, please rerun the analysis code on a model conversation of your choice. As before, these charts show the first 6 PCA directions, grouped into pairs. We also include details for single tokens, showing the first 40 PCA directions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pretraining Data</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Etai Littwin, Jiatao Gu, Josh Susskind, and Samy Bengio. 2023. Adaptivity and Modularity for Efficient Generalization Over Task Complexity</title>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantel</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Thilak</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.08866</idno>
		<idno>arxiv:2310.08866</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Physics of language models: Part 3.1, knowledge storage and extraction</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno>AI2. 2024. OLMo 1.7-7B: A 24</idno>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
	<note>point improvement on MMLU</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements</title>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
		<idno type="DOI">10.1109/T-C.1972.223477</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1197" to="1206" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><surname>Amd</surname></persName>
		</author>
		<title level="m">AMD Instinct™ MI250X Accelerators</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13319</idno>
		<title level="m">Mathqa: Towards interpretable math word problem solving with operation-based formalisms</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable Optimization as a Layer in Neural Networks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Path Independent Equilibrium Models Can Better Exploit Test-Time Computation</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwini</forename><surname>Pokle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Treutlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grosse</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Quoc Le, and 1 others. 2021. Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">Marcus</forename><surname>Mcaleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Llemma: An Open Language Model for Mathematics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Sangmin</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.20672</idno>
		<title level="m">Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Equilibrium Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Shaojie Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Deep Equilibrium Solvers</title>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linzhi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.07055</idno>
		<idno>arxiv:2408.07055</idno>
		<title level="m">LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pon-derNet: Learning to Ponder</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th ICML Workshop on Automated Machine Learning (AutoML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end Algorithm Synthesis with Recurrent Networks: Extrapolation without Overthinking</title>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Borgnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyad</forename><surname>Emam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Heinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Bauschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfu</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.4688</idno>
		<title level="m">Firmly nonexpansive mappings and maximally monotone operators: Correspondence and duality</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>math</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rethinking Deep Thinking: Stable Learning of Algorithms using Lipschitz Constraints</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Prügel-Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.23451</idno>
		<idno>arxiv:2410.23451</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Stas</forename><surname>Bekman</surname></persName>
		</author>
		<title level="m">Machine Learning Engineering Open Book</title>
		<imprint>
			<publisher>Stasosphere Online Inc</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werra</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>SmolLM-corpus</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.01373</idno>
		<idno>arxiv:2304.01373</idno>
		<title level="m">Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</title>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alham</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidney</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Clive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julen</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Etxaniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Fattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Zosa Forde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimansa</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename><forename type="middle">Y</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.14782</idno>
		<imprint/>
	</monogr>
	<note>and 11 others. 2024. Lessons from the Trenches on Reproducible Evaluation of Language Models. arxiv:2405.14782[cs</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parameter-Free Online Test-Time Adaptation</title>
		<author>
			<persName><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8344" to="8353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Vehicles: Experiments in Synthetic Psychology</title>
		<author>
			<persName><forename type="first">Valentino</forename><surname>Braitenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</title>
		<author>
			<persName><forename type="first">William</forename><surname>Brandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Ragan</forename><surname>Kelly</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.12981</idno>
		<idno>arxiv:2405.12981</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Digitised Books. c. 1510 -c. 1900. JSONL (OCR Derived Text + Metadata)</title>
		<idno type="DOI">10.23636/r7w6-zy15</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>British Library</publisher>
		</imprint>
		<respStmt>
			<orgName>British Library Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</title>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongwu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-First International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>character.ai. 2024. Optimizing AI Inference at Character.AI</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scott Gray, and 39 others</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Compressed Chain of Thought: Efficient Reasoning Through Dense Representations</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.13171</idno>
		<idno>arxiv:2412.13171</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GoodWiki dataset. François Chollet</title>
		<author>
			<persName><forename type="first">Euirim</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1911.01547</idno>
		<idno>arxiv:1911.01547</idno>
		<imprint>
			<date type="published" when="2019">2023. 2019</date>
		</imprint>
	</monogr>
	<note>On the Measure of Intelligence cs</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Vinodkumar Prabhakaran, and 48 others</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
	</analytic>
	<monogr>
		<title level="m">PaLM: Scaling Language Modeling with Pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457v1</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Training Verifiers to Solve Math Word Problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2110.14168</idno>
		<idno>arxiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Openphi/textbooks • Datasets at Hugging Face</title>
		<author>
			<persName><forename type="first">Owen</forename><surname>Colegrove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vik</forename><surname>Paruchuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Openphi-Team</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">MoEUT: Mixtureof-Experts Universal Transformers</title>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gautier Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Bpeasy</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Getting the most out of your tokenizer for pre-training and domain adaptation</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Gautier Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><surname>Rozière</surname></persName>
		</author>
		<idno>arxiv:2402.01035</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.08691</idno>
		<idno>arxiv:2307.08691</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.14135</idno>
		<idno>arxiv:2205.14135</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">others. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingkai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoshu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2501.12948</idno>
		<idno>arxiv:2501.12948</idno>
		<imprint>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepseek-Ai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengda</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fucong</forename><surname>Dai</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.19437</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
	<note>DeepSeek-V3 Technical Report. arxiv:2412.19437[cs</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1807.03819</idno>
		<idno>arxiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Universal Transformers</orgName>
		</respStmt>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.14838</idno>
		<idno>arxiv:2405.14838</idno>
		<title level="m">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fewer Truncations Improve Language Modeling</title>
		<author>
			<persName><forename type="first">Hantian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-First International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CogView: Mastering Text-to-Image Generation via Transformers</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19822" to="19835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Depth-Adaptive Transformer</title>
		<author>
			<persName><forename type="first">Maha</forename><surname>Elbayad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Elhoushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Hosmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Wasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anas</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Acun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">A</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carole-Jean</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2404.16710</idno>
		<idno>arxiv:2404.16710</idno>
		<title level="m">LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Katie</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.05872</idno>
		<idno>arxiv:2407.05872</idno>
		<title level="m">Scaling Exponents Across Parameterizations and Optimizers</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Reducing Transformer Depth on Demand with Structured Dropout</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1909.11556</idno>
		<idno>arxiv:1909.11556</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Addressing Some Limitations of Transformers with Feedback Memory</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2002.09402</idno>
		<idno>arxiv:2002.09402</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Looped Transformers for Length Generalization</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2101.03961</idno>
		<idno>arxiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<author>
			<persName><forename type="first">Xidong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mguni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ChessGPT: Bridging Policy Learning and Language Modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7216" to="7262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gabarain</surname></persName>
		</author>
		<title level="m">Locutusque/hercules-v5</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m">Hugging Face</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning? Jonas Geiping and Tom Goldstein</title>
		<author>
			<persName><forename type="first">Khashayar</forename><surname>Gatmiry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.08292</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="page" from="11117" to="11143" />
		</imprint>
	</monogr>
	<note>Cramming: Training a Language Model on a single GPU in one day</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parametric Majorization for Data-Driven Energy Minimization Methods</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10262" to="10273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2000.861302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</title>
		<meeting>the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Looped Transformers as Programmable Computers</title>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Giannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jy-Yong</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11398" to="11442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Yangqing Jia, and Kaiming He</title>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno>arxiv:1706.02677</idno>
	</analytic>
	<monogr>
		<title level="m">Large Minibatch SGD: Training ImageNet in 1 Hour</title>
		<imprint>
			<publisher>Accurate</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Adaptive Computation Time for Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1603.08983</idno>
		<idno>arxiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>arxiv:1410.5401</idno>
		<title level="m">Neural Turing Machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Yuling Gu, Jack Hessel, and 24 others</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><surname>Elazar</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.00838</idno>
		<idno>arxiv:2402.00838</idno>
	</analytic>
	<monogr>
		<title level="m">Accelerating the Science of Language Models</title>
		<meeting><address><addrLine>OLMo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hägele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elie</forename><surname>Bakouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Efficient Systems for Foundation Models II @ ICML2024</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dijia</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.06769</idno>
		<idno>arxiv:2412.06769</idno>
		<title level="m">Training Large Language Models to Reason in a Continuous Latent Space</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic Layer Tying for Parameter-Efficient Transformers</title>
		<author>
			<persName><forename type="first">David</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Zexue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.13449</idno>
		<idno>arxiv:2402.13449</idno>
		<title level="m">Towards Large Language Models with Training-Free Consolidated Associative Memory</title>
		<meeting><address><addrLine>CAMELoT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">2021a. Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dawn Song, and Jacob Steinhardt. 2021b. Measuring Massive Multitask Language Understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Jiewen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.03350</idno>
		<idno>arxiv:2408.03350</idno>
		<title level="m">miniCTX: Neural Theorem Proving with</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Long-)Contexts.</note>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Averaging weights leads to wider optima and better generalization: 34th Conference on Uncertainty in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2018. 34th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateja</forename><surname>Jamnik</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.03755</idno>
		<idno>arxiv:2311.03755</idno>
		<title level="m">Multilingual Mathematical Autoformalization</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.14981</idno>
		<idno>arxiv:2209.14981</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Guy</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matanel</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.05864</idno>
		<idno>arxiv:2410.05864</idno>
		<title level="m">From Tokens to Words: On the Inner Lexicon of LLMs</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Scaling Laws for Neural Language Models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2001.08361</idno>
		<idno>arxiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kenney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>ArXivDLInstruct</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models</title>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyoung</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.emnlp-main.248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4334" to="4353" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">BookSum: A Collection of Datasets for Long-form Narrative Summarization</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Kryściński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2105.08209</idno>
		<idno>arxiv:2105.08209</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Caiming Xiong, and Dragomir Radev cs</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuotao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senqiao</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.18629</idno>
		<idno>arxiv:2406.18629</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Xiangru Peng, and Jiaya Jia cs</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Kevin Gimpel, Piyush Sharma, and Radu Soricut</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">A Path Towards Autonomous Machine Intelligence. Preprint, Version 0.9</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">62</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Loss functions for discriminative training of energy-based models</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2005 -Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1149</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fast Inference from Transformers via Speculative Decoding</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Leviathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matan</forename><surname>Kalman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19274" to="19286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Or Sharir, Hofit Bata, and Amnon Shashua. 2021. The Depth-to-Width Interplay in Self-Attention</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2006.12467</idno>
		<idno>arxiv:2006.12467</idno>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Solving quantitative reasoning problems with language models</title>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinay</forename><surname>Ramasesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Gutman-Solo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.14858</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtian</forename><surname>Zi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Marone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Gontier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ho</forename><surname>Yee</surname></persName>
		</author>
		<title level="m">StarCoder: May the source be with you! Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Yuqing Tang, and Xiang Kong. 2020a. Deep Transformers with Latent Depth</title>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2009.13102</idno>
		<idno>arxiv:2009.13102</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Pushmeet Kohli, and Oriol Vinyals. 2020b. Strong Generalization and Efficiency in Neural Programs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2007.03629</idno>
		<idno>arxiv:2007.03629</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pangarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ranjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A top-quality LLM pre-training dataset requires the perfect blend</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">2023a. Ring attention with blockwise transformers for near-infinite context</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01889</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">2024a. Deliberation in Latent Space via Differentiable Cache Augmentation</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.17747</idno>
		<idno>arxiv:2412.17747</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">2023b. We-bGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3580305.3599931</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;23</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;23<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="4549" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">MobileLLM: Optimizing Subbillion Parameter Language Models for On-Device Use Cases</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Krishnamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno>arxiv:2402.14905</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurick</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pangarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guowei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fajri</forename><surname>Koto</surname></persName>
		</author>
		<title level="m">Liping Tang, Nikhil Ranjan, and 9 others. 2023c. LLM360: Towards fully transparent open-source LLMs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled Weight Decay Regularization.</note>
	<note>cs, math</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cassano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Lamy-Poirier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Pykhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kocetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Abulkhanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2402.19173</idno>
		<imprint/>
	</monogr>
	<note>Indraneil Paul, and 47 others. 2024. StarCoder 2 and The Stack v2: The Next Generation</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houxing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjie</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.08196</idno>
		<idno>arxiv:2410.08196</idno>
	</analytic>
	<monogr>
		<title level="m">Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Majstorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>Selected Digitized Books | The Library of Congress</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Tom Goldstein, Avi Schwarzschild, and Petar Veličković. 2024. The CLRS-Text Algorithmic Reasoning Language Benchmark</title>
		<author>
			<persName><forename type="first">Larisa</forename><surname>Markeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Mcleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfried</forename><surname>Bounsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kozlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Vitvitskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.04229</idno>
		<idno>arxiv:2406.04229</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">MIND over Body: Adaptive Thinking using Dynamic Computation</title>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><forename type="middle">M</forename><surname>Plis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Transformers Can Do Arithmetic with the Right Embeddings</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Michael Mcleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">R</forename><surname>Bartoldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavya</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Bhatele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Saturated Transformers are Constant-Depth Threshold Circuits</title>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00493</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="843" to="856" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Černocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2011.5947611</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Artificial Kuramoto Oscillatory Neurons</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sindy</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirteenth International Conference on Learning Representations</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">The Many Ways that Digital Minds Can Know</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Moulton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Oc-toPack: Instruction Tuning Code Large Language Models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armel</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinkai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swayam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2308.07124</idno>
		<idno>arxiv:2308.07124</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Tiny-textbooks</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Pham</surname></persName>
		</author>
		<idno type="DOI">10.57967/hf/1126</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Revision 14de7ba</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Tiny-strange-textbooks</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Pham</surname></persName>
		</author>
		<idno type="DOI">10.57967/hf/1612</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Revision 6f304f1</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Progress measures for grokking via mechanistic interpretability</title>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sotiris</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidak Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">New reasoning models: Openai o1-preview and o1-mini</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/research/o1-preview-and-o1-mini" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2203.02155</idno>
		<idno>arxiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text</title>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Scalable Diffusion Models with Transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1911.05507</idno>
		<idno>arxiv:1911.05507</idno>
		<title level="m">Compressive Transformers for Long-Range Sequence Modelling</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">ZeRO: Memory optimizations Toward Training Trillion Parameter Models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41405.2020.00024</idno>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">High-Resolution Image Synthesis With Latent Diffusion Models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Björn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">WinoGrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3474381</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Nihal Nayak, Debajyoti Datta, and 21 others. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Chhablani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Early weight averaging meets high learning rates for LLM pre-training</title>
		<author>
			<persName><forename type="first">Sunny</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atula</forename><surname>Tejaswi Neerkaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Language Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Abhishek Kumar, and sujay sanghavi</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Self-Delimiting Neural Networks</title>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1210.0118</idno>
		<idno>arxiv:1210.0118</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Confident Adaptive Language Modeling</title>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">Deep Thinking Systems: Logical Extrapolation with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>College Park, College Park</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. 2021a. Datasets for Studying Generalization from Easy to Hard Examples</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Borgnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2108.06011</idno>
		<idno>arxiv:2108.06011</idno>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">2021b. Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Borgnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uzi</forename><surname>Vishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6695" to="6706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Algorithm Design for Learned Algorithms</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Michael Mcleish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakash</forename><surname>Chandnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<title level="m">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<imprint>
			<date type="published" when="2024">1 others. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">GLU Variants Improve Transformer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2002.05202</idno>
		<idno>arxiv:2002.05202</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1701.06538</idno>
		<idno>arxiv:1701.06538</idno>
		<title level="m">Outrageously Large Neural Networks: The Sparsely-Gated Mixtureof-Experts Layer</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Bhatele</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS53621.2022.00065</idno>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="606" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajwal</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Kirchenbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manli</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Bhatele</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC41406.2024.00010</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis SC</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="36" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Does Representation Matter? Exploring Intermediate Layers in Large Language Models</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Skean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Rifat Arefin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2412.09563</idno>
		<idno>arxiv:2412.09563</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<title level="m" type="main">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">Robert</forename><surname>Steeves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Niklas Muennighoff, and 17 others. 2024. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lucy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.840</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15725" to="15788" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05600</idno>
		<title level="m">Generative Modeling by Estimating Gradients the Data Distribution</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">RoFormer: Enhanced Transformer with Rotary Position Embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno>arxiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<title level="m" type="main">Augmenting Self-attention with Persistent Memory</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.01470</idno>
		<idno>arxiv:1907.01470</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Aakash Kumar Nain, and Llion Jones</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Pickett</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.09298</idno>
		<idno>arxiv:2407.09298</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Transformer Layers as Painters cs</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Test-Time Training with Self-Supervision for Generalization under Distribution Shifts</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">The Recurrent Temporal Restricted Boltzmann Machine</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1911.03329</idno>
		<idno>arxiv:1911.03329</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Lessons on Parameter Sharing across Layers in Transformers</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2104.06022</idno>
		<idno>arxiv:2104.06022</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Spike No More: Stabilizing the Pre-training of Large Language Models</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<idno>arxiv:2312.16903</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenfang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.07096</idno>
		<idno>arxiv:2310.07096</idno>
		<title level="m">Sparse Universal Transformer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Improving Open Language Models at a Practical Size</title>
		<author>
			<persName><forename type="first">Team</forename><surname>Gemma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Ramé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pouya</forename><surname>Tafti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abe</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Casbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabela</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charline</forename><forename type="middle">Le</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sammy</forename><surname>Jerome</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2408.00118</idno>
		<idno>arxiv:2408.00118</idno>
	</analytic>
	<monogr>
		<title level="j">Gemma</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Team</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taira</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faeze</forename><surname>Brahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2501.00656</idno>
		<imprint/>
	</monogr>
	<note>and 21 others. 2025. 2 OLMo 2 Furious. arxiv:2501.00656[cs</note>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Llama-2-7B-32K-Instruct -and fine-tuning for Llama-2 models with Together API</title>
		<author>
			<persName><surname>Togetherai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kisacanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexan</forename><surname>Ayrapetyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2410.01560</idno>
		<idno>arxiv:2410.01560</idno>
		<title level="m">Accelerating AI for Math with Massive Open-Source Instruction Data</title>
		<imprint>
			<biblScope unit="volume">2024</biblScope>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Narenthiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint/>
	</monogr>
	<note>Daria Gitman, Fei Jia, and Igor Gitman. 2024b</note>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
			<affiliation>
				<orgName type="collaboration">Illia Polosukhin</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">MathPile: A Billion-Token-Scale Pretraining Corpus for Math</title>
		<author>
			<persName><forename type="first">Zengzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Egert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makesh Narsimhan</forename><surname>Sreedhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.08673</idno>
		<idno>arxiv:2406.08673</idno>
		<title level="m">Opensource dataset for training top-performing reward models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">RedPajama: An Open Dataset for Training Large Language Models</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Chalamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1990.2.4.490</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="490" to="501" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">2023a. Stable and low-precision training for large-scale vision-language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10271" to="10298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">2023b. Stable and low-precision training for large-scale vision-language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Enhancing PyTorch Performance on Frontier with the RCCL OFI-Plugin</title>
		<author>
			<persName><forename type="first">Mengshiou</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Memorizing Transformers</title>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Norman Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delesley</forename><surname>Hutchins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover</title>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.17227</idno>
		<idno>arxiv:2407.17227</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Magpie: Alignment Data Synthesis from Scratch Prompting Aligned LLMs with Nothing</title>
		<author>
			<persName><forename type="first">Zhangchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radha</forename><surname>Poovendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.08464</idno>
		<idno>arxiv:2406.08464</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">LeanDojo: Theorem Proving with Retrieval-Augmented Language Models</title>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">M</forename><surname>Swope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Chalamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Godil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">and Dimitris Papailiopoulos. 2024a. Looped Transformers are Better at Learning Learning Algorithms</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">2024b. Parallelizing Linear Transformers with the Delta Rule over Sequence Length</title>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eighth Annual Conference on Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Lean Workbook: A large-scale Lean problem set formalized from natural language math problems</title>
		<author>
			<persName><forename type="first">Huaiyuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models</title>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">Revisiting BFloat16 Training</title>
		<author>
			<persName><forename type="first">Pedram</forename><surname>Zamirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2010.06192</idno>
		<idno>arxiv:2010.06192</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title level="m" type="main">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zelikman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georges</forename><surname>Harik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varuna</forename><surname>Jayasiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.09629</idno>
		<idno>arxiv:2403.09629</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Scaling Vision Transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12104" to="12113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Root Mean Square Layer Normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leuang</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qunshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raven</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuney</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrun</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.19327</idno>
		<idno>arxiv:2405.19327</idno>
		<title level="m">Bill Lin, and 26 others. 2024a. MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series</title>
		<imprint/>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Gang Chen, and Sharad Mehrotra. 2024b. Draft&amp; Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidan</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.607</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11263" to="11282" />
		</imprint>
	</monogr>
	<note>Long Papers) Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Autonomous Data Selection with Language Models for Mathematical Texts</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-acl.762</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2024</title>
		<meeting><address><addrLine>Bangkok</addrLine></address></meeting>
		<imprint>
			<publisher>Thailand. Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="12834" to="12859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<title level="m" type="main">Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2409.17115</idno>
		<idno>arxiv:2409.17115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Terry</forename><surname>Yue Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><forename type="middle">Chien</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratnadira</forename><surname>Widyasari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imam</forename><surname>Nur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bani</forename><surname>Yusuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haolan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junda</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Indraneil</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Armel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoheng</forename><surname>Zebaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Ding</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.15877</idno>
		<imprint/>
	</monogr>
	<note>and 14 others. 2024. BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
