{
  "arxivId": "2310.17157",
  "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
  "authors": "Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen",
  "abstract": "Large language models (LLMs) with hundreds of billions of parameters have\nsparked a new wave of exciting AI applications. However, they are\ncomputationally expensive at inference time. Sparsity is a natural approach to\nreduce this cost, but existing methods either require costly retraining, have\nto forgo LLM's in-context learning ability, or do not yield wall-clock time\nspeedup on modern hardware. We hypothesize that contextual sparsity, which are\nsmall, input-dependent sets of attention heads and MLP parameters that yield\napproximately the same output as the dense model for a given input, can address\nthese issues. We show that contextual sparsity exists, that it can be\naccurately predicted, and that we can exploit it to speed up LLM inference in\nwall-clock time without compromising LLM's quality or in-context learning\nability. Based on these insights, we propose DejaVu, a system that uses a\nlow-cost algorithm to predict contextual sparsity on the fly given inputs to\neach layer, along with an asynchronous and hardware-aware implementation that\nspeeds up LLM inference. We validate that DejaVu can reduce the inference\nlatency of OPT-175B by over 2X compared to the state-of-the-art\nFasterTransformer, and over 6X compared to the widely used Hugging Face\nimplementation, without compromising model quality. The code is available at\nhttps://github.com/FMInference/DejaVu.",
  "url": "https://arxiv.org/abs/2310.17157",
  "issue_number": 352,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/352",
  "created_at": "2025-01-04T15:02:30.861014",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 19,
  "last_read": "2025-01-04T15:02:30.861843",
  "last_visited": "2024-12-28T06:14:06.334Z",
  "main_tex_file": null,
  "published_date": "2023-10-26T05:01:09Z",
  "arxiv_tags": [
    "cs.LG"
  ]
}