<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-10-26">26 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zichang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-26">26 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">3F8B031D54896B0C893B131D42D58724</idno>
					<idno type="arXiv">arXiv:2310.17157v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DEJAVU, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardwareaware implementation that speeds up LLM inference. We validate that DEJAVU can reduce the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs), such as GPT-3, PaLM, and OPT have demonstrated that an immense number of parameters unleashes impressive performance and emergent in-context-learning abilities-they can perform a task by conditioning on input-output examples, without updating their parameters <ref type="bibr" target="#b19">(Bommasani et al., 2021;</ref><ref type="bibr" target="#b91">Liang et al., 2022;</ref><ref type="bibr" target="#b24">Brown et al., 2020;</ref><ref type="bibr" target="#b101">Min et al., 2022;</ref><ref type="bibr" target="#b25">Chan et al., 2022)</ref>. However, they are very expensive at inference time, especially for latency-sensitive applications <ref type="bibr" target="#b109">(Pope et al., 2022</ref>). An ideal inference-time model should use less computation and memory while maintaining the performance and special abilities of pre-trained LLMs. The simplest and most natural approach is sparsification or pruning, which has a long history before the LLM era <ref type="bibr" target="#b82">(LeCun et al., 1989)</ref>. Unfortunately, speeding up inference-time sparse LLMs in wall-clock time while maintaining quality and in-context learning abilities remains a challenging problem.</p><p>While sparsity and pruning have been well-studied, they have not seen wide adoption on LLMs due to the poor quality and efficiency trade-offs on modern hardware such as GPUs. First, it is infeasible to retrain or iteratively prune models at the scale of hundreds of billions of parameters. Thus, methods in iterative pruning and lottery ticket hypothesis <ref type="bibr" target="#b84">(Lee et al., 2018;</ref><ref type="bibr" target="#b50">Frankle &amp; Carbin, 2018)</ref> can only be applied to smaller-scale models. Second, it is challenging to find sparsity that preserves the in-context learning ability of LLMs. Many works have shown the effectiveness of task-dependent pruning <ref type="bibr" target="#b99">(Michel et al., 2019;</ref><ref type="bibr" target="#b13">Bansal et al., 2022)</ref>, but maintaining different models for each task conflicts with the task independence goal of LLMs. Lastly, it is hard to achieve wall-clock time speed-up with unstructured sparsity due to its well-known difficulty with modern hardware <ref type="bibr" target="#b69">(Hooker, 2021)</ref>. For example, recent development in zero-shot pruning like SparseGPT <ref type="bibr" target="#b51">(Frantar &amp; Alistarh, 2023)</ref> finds 60% unstructured sparsity but does not yet lead to any wall-clock time speedup.</p><p>An ideal sparsity for LLMs should (i) not require model retraining, (ii) preserve quality and in-context learning ability, and (iii) lead to speed-up in wall-clock time on modern hardware. To achieve such demanding requirements, we go beyond static sparsity in previous works (e.g., structured/unstructured weight pruning). We instead envision contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that lead to (approximately) the same output as the full model for an input. Inspired by the connections between LLMs, Hidden Markov Models <ref type="bibr" target="#b137">(Xie et al., 2022;</ref><ref type="bibr" target="#b14">Baum &amp; Petrie, 1966)</ref>, and the classic Viterbi algorithm <ref type="bibr" target="#b131">(Viterbi, 1967)</ref>, we hypothesize that for pre-trained LLMs, contextual sparsity exists given any input.</p><p>The hypothesis, if true, would enable us to cut off specific attention heads and MLP parameters (structured sparsity) on the fly for inference-time, without modifying pre-trained models. However, there are three challenges.</p><p>Existence: It is nontrivial to verify if such contextual sparsity exists, and naive verification can be prohibitively expensive.</p><p>Prediction: Even if contextual sparsity exists, it is challenging to predict the sparsity for a given input in advance.</p><p>Efficiency: Even if the sparsity can be predicted, it might be difficult to achieve end-to-end wall-clock time speedup. Taking OPT-175B as an example, the latency of one MLP block is only 0.2 ms on an 8×A100 80GB machine. Without a fast prediction and optimized implementation, the overhead can easily increase the LLM latency rather than reduce it.</p><p>In this work, we address these challenges as follows:</p><p>Existence: Fortunately, we verify the existence of contextual sparsity with a surprisingly simple approach. To achieve essentially the same output, contextual sparsity is on average 85% structured sparse and thereby potentially leads to a 7× parameter reduction for each specific input while maintaining accuracy (Figure <ref type="figure" target="#fig_10">1(a)</ref>). During explorations of contextual sparsity, we make important empirical observations and build a theoretical understanding of major components in LLMs that help address the prediction and efficiency challenge. . DEJAVU uses lookahead predictors to side-step prediction costs: given the input to the attention layer at block k, they (asynchronously) predict the contextual sparsity for the MLP at block k, and given the input to the MLP at block k, they predict the sparsity for the attention head at the next layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deja Vu</head><p>Prediction: We discover that contextual sparsity depends not only on individual input tokens (i.e., non-contextual dynamic sparsity) but also on their interactions (contextual dynamic sparsity). Figure <ref type="figure" target="#fig_10">1</ref> <ref type="bibr">(b)</ref> shows that with pure dynamic information, sparsity prediction is inaccurate. Only with token embeddings with sufficient contextual information can we predict sparsity accurately. Another finding is that contextual dynamic sparsity for every layer can be predicted based on the "similarity" between layer parameters (heads/MLP) and the output from the previous layer, which carries the immediate contextual mixture of token embeddings.</p><p>Efficiency: Because at inference time, model parameters are static, inspired by the classical nearest neighbor search (NNS) literature and its applications in efficient deep learning, it is possible to formulate the above similarity-based prediction as an NNS problem <ref type="bibr">(Indyk &amp; Motwani, 1998b;</ref><ref type="bibr" target="#b143">Zhang et al., 2018;</ref><ref type="bibr">Chen et al., 2020a)</ref>. However, as mentioned, the overhead might be difficult to overcome as we would need to perform on-the-fly predictions before every layer. Luckily, we exploit a phenomenon of LLM where token embeddings change slowly across layers due to residual connections (wellknown in computer vision <ref type="bibr" target="#b65">(He et al., 2016)</ref>). Since the inputs to a few consecutive layers are very similar, we can design an asynchronous lookahead predictor (Figure <ref type="figure">2</ref>).</p><p>Based on our findings, we present a system, DEJAVU, that exploits contextual sparsity and realizes efficient LLMs for latency-sensitive applications.</p><p>• In Section 4.1 and Section 4.2, we present a low-cost learning-based algorithm to predict sparsity on the fly.</p><p>Given the input to a specific layer, it predicts a relevant subset of attention (heads) or MLP parameters in the next layer and only loads them for the computation.</p><p>• In Section 4.3, we propose an asynchronous predictor (similar to classic branch predictor <ref type="bibr" target="#b118">(Smith, 1998)</ref>) to avoid the sequential overhead. A theoretical guarantee justifies that the cross-layer design suffices for accurate sparsity prediction.</p><p>After integrating hardware-aware implementation of sparse matrix multiply (Section 4.4), DEJAVU (written mostly in Python) can reduce latency of open-source LLMs such as OPT-175B by over 2× end-to-end without quality degradation compared to the state-of-the-art library Faster-Transformer from Nvidia (written entirely in C++/CUDA), and over 2× compared to the widely used Hugging Face implementation at small batch sizes. Furthermore, we show several ablations on different components of DEJAVU and its compatibility with quantization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work and Problem Formulation</head><p>We first briefly discuss the rich literature on efficient inference. Then, we introduce the latency breakdown in our setting. Last, we provide a formal problem formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quantization, Pruning, Distillation for Inference</head><p>Various relaxations have been studied for decades for model inference in machine learning. There are three main techniques: quantization <ref type="bibr" target="#b63">(Han et al., 2015;</ref><ref type="bibr" target="#b75">Jacob et al., 2018;</ref><ref type="bibr" target="#b103">Nagel et al., 2019;</ref><ref type="bibr" target="#b144">Zhao et al., 2019)</ref>, pruning or sparsity <ref type="bibr" target="#b102">(Molchanov et al., 2016;</ref><ref type="bibr" target="#b92">Liu et al., 2018;</ref><ref type="bibr" target="#b68">Hoefler et al., 2021)</ref>, and distillation <ref type="bibr" target="#b67">(Hinton et al., 2015;</ref><ref type="bibr" target="#b127">Tang et al., 2019;</ref><ref type="bibr" target="#b129">Touvron et al., 2021)</ref>. They are orthogonal areas and usually excel in different settings. Recently, there is active research attempting to apply one or a combination of such techniques in LLM inference <ref type="bibr" target="#b139">(Yao et al., 2022;</ref><ref type="bibr" target="#b108">Park et al., 2022;</ref><ref type="bibr" target="#b46">Dettmers et al., 2022;</ref><ref type="bibr" target="#b52">Frantar et al., 2022;</ref><ref type="bibr" target="#b51">Frantar &amp; Alistarh, 2023;</ref><ref type="bibr" target="#b13">Bansal et al., 2022;</ref><ref type="bibr" target="#b136">Xiao et al., 2022)</ref>. More discussion is presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LLM Inference Latency Breakdown</head><p>The generative procedure of LLMs consists of two phases: (i) the prompt phase takes an input sequence to generate the keys and values (KV cache) for each transformer block of LLMs, which is similar to the forwarding pass of LLMs training; and (ii) the token generation phase utilizes and updates the KV cache to generate tokens step by step, where the current token generation depends on previously generated tokens.</p><p>This paper studies the setting where the token generation phase easily dominates the end-to-end inference time. As shown in Table <ref type="table" target="#tab_0">1</ref>, generating a sequence of length 128 takes much longer time than processing a sequence of length 128 as prompt due to I/O latency of loading model parameters.</p><p>In addition, Table <ref type="table" target="#tab_1">2</ref> shows that attention and MLP are both bottlenecks in LLMs, e.g., in 175B models, loading MLP parameters takes around 2 3 of the total I/O and attention heads take the other 1 3 . Further, in the tensor-parallel regime, there are two communications between GPUs, one after the attention block, and the other one after the MLP block. As shown in Table <ref type="table" target="#tab_2">3</ref>, communication between GPUs takes around 15 % token generation latency. This paper focuses on making attention and MLP more efficient. Communication cost implies that the upper bound of such speed-up is around 6× when skipping all transformer blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Problem Formulation</head><p>The goal is to reduce the generation latency of LLMs by exploiting contextual sparsity. In the following, we formally define the sparsified attention and MLP blocks.</p><p>Sparsified MLP: There are two linear layers in one MLP block, W 1 , W 2 ∈ R d×4d . Denote y ∈ R 1×d as the input to the MLP block in the current generation step. Let each column (the weight of i-th neuron) of linear layers be W 1 i , W 2 i ∈ R d×1 . With contextual sparsity, only a small set of them are required for computation. Let S M ⊆ [4d] denote such set of neurons for input y. The sparsified MLP computation is</p><formula xml:id="formula_0">MLP S M (y) = σ(yW 1 S M )(W 2 S M ) ⊤ , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where σ is the activation function, e.g., ReLU, GeLU. Note that since the computation in the first linear results in sparse activations, the second linear layer is also sparsified.</p><p>Sparsified Attention: Let X ∈ R n×d denote the embeddings of all tokens (e.g., prompts and previously generated tokens). Let y ∈ R 1×d be the input to the Multi-Head-Attention (MHA) in the current generation step. Suppose there are h heads. For each i ∈ [h], we use</p><formula xml:id="formula_2">W K i ,W Q i ,W V i ∈ R d×d h to</formula><p>denote key, query, value projections for the i-th head, and</p><formula xml:id="formula_3">W O i ∈ R d h ×d for output projections.</formula><p>With contextual sparsity, we denote S A as a small set of attention heads leading to approximately the same output as the full attention for input y. Following the notation system in <ref type="bibr" target="#b2">(Alman &amp; Song, 2023)</ref>, sparsified MHA computation can be formally written as</p><formula xml:id="formula_4">MHA S A (y) = i∈S A H i (y) 1×d h W O i d h ×d</formula><p>, where H i (y) : R d → R d h and D i (y) ∈ R can be written as</p><formula xml:id="formula_5">H i (y) := D i (y) -1 exp(yW Q i (W K i ) ⊤ X ⊤ )XW V i , (2) D i (y) := exp(yW Q i (W K i ) ⊤ X ⊤ )1 n .</formula><p>For both MLP and Attention, given a compute budget, the goal is to find S M and S A that minimize the error between the sparse approximation and full computation. We can zero out over 95% of MLP parameters for a given token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pre-trained LLMs are Contextually Sparse</head><p>In this section, we present several key observations and theoretical understandings of sparsity in LLMs, upon which the DEJAVU design is based. We first test the contextual sparsity hypothesis and verify that contextual sparsity exists in pretrained LLMs in Section 3.1. Then, we build an understanding of why contextual sparsity happens naturally even when LLMs are densely trained in Section 3.2. Finally, we present an observation on residual connections and explain their relationship to contextual sparsity analytically in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contextual Sparsity Hypothesis</head><p>Inspired by prior pruning literature <ref type="bibr" target="#b102">(Molchanov et al., 2016)</ref>, we find a surprisingly simple method is sufficient to study and verify our hypothesis. In this section, we describe the testing procedure, observation details, and insights of this study.</p><p>Verification: Our test is performed on OPT-175B, 66B, and 30B models and various downstream datasets such as Open-BookQA <ref type="bibr" target="#b100">(Mihaylov et al., 2018)</ref> and Wiki-Text <ref type="bibr" target="#b98">(Merity et al., 2016)</ref>. We find the contextual sparsity for every input example with two forward passes of the model. In the first pass, we record a subset of parameters, specifically which attention heads and MLP neurons yield large output norms for the input. In the second pass, each input example only uses the recorded subset of parameters for the computation. Surprisingly, these two forward passes lead to similar prediction or performance on all in-context learning and language modeling tasks.</p><p>Observation: Figure <ref type="figure">3</ref> shows that on average, we can impose up to 80% sparsity on attention heads and 95% sparsity on MLP neurons. As mentioned in Section 2, OPT-175B model has 2× MLP parameters than those of attention blocks. Therefore total sparsity here is around 85%. Since these are all structured sparsity (heads and neurons), predicting them accurately could potentially lead to 7× speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insight:</head><p>It is intuitive that we can find contextual sparsity in MLP blocks at inference time because of their activation functions, e.g., ReLU or GeLU <ref type="bibr" target="#b80">(Kurtz et al., 2020)</ref>. Similar observations were made by <ref type="bibr">(Li et al., 2022)</ref>. However, it is surprising that we can find contextual sparsity in attention layers. Note that, finding contextual sparsity in attention is not the same as head pruning. We cross-check that different examples have different contextual sparsity. Although 80% of the parameters are not included in the paths for a given example, they might be used by other examples. Next, we will try to understand why contextual sparsity exists in attention blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token Clustering in Attention Layers</head><p>In the previous section, we have verified that there exists contextual sparsity for a given input in LLMs. In this section, we try to understand the reason for such phenomena, especially in attention layers. We first show an in-depth observation of attention. Then we present a hypothesis that self-attentions are conceptually clustering algorithms. Last we show analytical evidence to support this hypothesis.</p><p>Observation: Figure <ref type="figure" target="#fig_3">4</ref> shows the attention map of three different heads from the same layer for an example input. The next token it should predict is "Truck". Darker color represents higher attention scores. We observe that the middle head is a relatively uniform token-mixing head while the top and bottom ones are "heavy hitter" attention heads (with high attention to "like" and "shipping"). Unsurprisingly, only selecting heavy hitter heads but not uniform heads does not affect the prediction, since uniform heads do not model or encode important token interactions. In the next section, we will also explain in detail how the criteria for selecting uniform attention heads and heads with small output norms are highly correlated.</p><p>Hypothesis: We hypothesize that the attention head is performing mean-shift clustering <ref type="bibr" target="#b45">(Derpanis, 2005)</ref>.</p><p>Recall the notation defined in Section 2.3. For i-th head at current layer, X = [x 1 , ... , x n ] ⊤ ∈ R n×d are the token embeddings in the previous time steps. XW K i and XW V i are the projection of embedding. For an input embedding y, the output ỹi = H i (y), where H i (y) is defined in Eq. 2.</p><p>For each i ∈ [h], if we let K i (x j ,y) := exp(yW Q i (W K i ) ⊤ x j ) measure the similarity between x j and y, and define m i (y) := j Ki(xj ,y)xj j Ki(xj ,y) , then we have ỹi = m i (y)W V i . Further, if we set W V i = I and consider the residue connection followed by layer norm, then in the next layer, the embedding ŷi of the current token becomes ŷi = Normalize(y + ỹi ) = Normalize(y+m i (y)), which has a fixed point y = γm i (y) for any scalar γ. This iteration bears a resemblance to meanshift clustering, which simply performs iteration y ← m i (y) until convergence. This has an obvious fixed point y = m i (y).</p><p>Therefore, the self-attention head can be regarded as one mean-shift step to push input embeddings of different tokens together, if they are already neighbors in a projection space specified by W Q i (W K i ) ⊤ . Different heads learn different projection spaces to perform clustering. These dynamics explain the precise reason why token embeddings tend to cluster after going through more layers, resulting in high attention scores among cluster members, and low scores for non-members. Furthermore, the cluster patterns are different at different heads (More details in Appendix K).</p><p>The above analysis not only provides an understanding of why contextual sparsity exists naturally in pre-trained LLMs, but also inspires our design of "similarity"-based sparsity prediction for DEJAVU in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Slowly Changing Embeddings across Layers</head><p>We first present our observation that embeddings change slowly across consecutive layers. Then we provide a detailed analysis on the phenomenon. Finally, we show its close connection with contextual sparsity. Details are in Section B.</p><p>High similar embeddings in consecutive layers: In Figure <ref type="figure" target="#fig_4">5</ref>(a), we show that for the same given input, the cosine similarity between embeddings or activations in two consecutive layers is exceptionally high on 7 different sizes of OPT models. Specifically, we collect activations from each layer while performing OPT model inference on C4 validation set <ref type="bibr" target="#b113">(Raffel et al., 2019)</ref>. Taking OPT-175B as an example, starting from the second layer, the similarity between any two consecutive layers is around 0.99, which indicates that when an input is passed through the model, the direction of its embedding changes slowly. Interestingly, the most drastic change happens in the first layer. Furthermore, we increase the gap and investigate the similarity between the embedding at layer l and at layer l + n shown in Figure <ref type="figure" target="#fig_4">5</ref>(b). As we increase the gap, the similarity decreases as expected while the differences in cosine similarity between various choices ∥X∥ is significantly higher than ∥F (X)∥, which explains the slowly changing embedding.</p><p>of n are smaller at the shallower layer. We plot the mean similarity, and the standard deviation is indicated by the shading. Similar plots on more models are presented in Appendix B.</p><p>Connection to residuals: We verify that the high similarity in embeddings in LLM inference is due to the residual connection. We first dissect the computation graph inside each transformer layer to understand the cause behind this phenomenon. There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as X +F (X), where F is either the Multi-Head Attention or two MLP Layers. In Figure <ref type="figure" target="#fig_4">5</ref>(c) and Figure <ref type="figure" target="#fig_4">5</ref>(d), indeed we can see that ∥X∥ is significantly greater than ∥F (X)∥, confirming that embeddings are changing slowly because the residual norm is large.</p><p>Connection to Contextual Sparsity: We take a step deeper trying to understand the reason behind the large residual norm with mathematical modeling. We discover that one possible reason for small ∥F (X)∥ is due to high sparsity. For the MLP Block, high sparsity may contribute to the small norm of F (X) because a large portion of outputs have small norms. Similar reasoning applies to the Attention Block, and thus a large number of attention heads yield small norm outputs.</p><p>Residual Two Sides Bound: Besides empirical reasoning, we formally define the computation of LLMs mathematically. Under our computation model, we can show that a shrinking property which is observed by our practical experiments.</p><p>Proofs are in Appendix G, H, I.</p><p>Lemma 3.1 (Informal). Let 0 &lt; ϵ 1 &lt; ϵ 2 &lt; 1 be the lower and upper bound of the shrinking factor. Let x be the y be the output. We have the residual connection y = x+F (x). For the MLP block F (x), we have ϵ 1 ≤ ∥y -x∥ 2 ≤ ϵ 2 . For the attention block F (x), we have ϵ 1 ≤ ∥y-x∥ 2 ≤ ϵ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DEJAVU</head><p>In this section, we present our framework for inference-time contextual sparsity search for LLMs. We introduce the sparsity predictor for MLPs in Section 4.1 and for attention heads in Section 4.2. DEJAVU's workflow is shown in Figure <ref type="figure">2</ref>. Section 4.3 discusses exploiting our observation on LLMs to avoid the sparse prediction overhead with theoretical guarantees. In Section 4.4, we present our optimized implementation that enables end-to-end latency reduction. More details are presented in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Contextual Sparsity Prediction in MLP Blocks</head><p>As explained in Section 2, MLP blocks are one of the major bottlenecks for the LLM generation ( 2 3 of the FLOPs and IOs). In this section, we discuss how we achieve wall-clock time speed-up with contextual sparsity in the MLP blocks.</p><p>Challenge Figure <ref type="figure">3</ref> <ref type="bibr">(b)</ref> shows that for a given token, the contextual sparsity of 95% is possible. The contextual sparsity in the MLP block can be identified after computing the activation. However, this only demonstrates the existence of contextual sparsity but brings no benefits in terms of efficiency. A fast and precise prediction is needed to exploit contextual sparsity for end-to-end efficiency. The naive way is to select a subset of neurons randomly. Unsurprisingly, random selection fails to identify the accurate contextual sparsity, resulting in drastic model degradation.</p><p>A Near-Neighbor Search Problem: Recall that we verify the existence of contextual sparsity by recording which neurons yield significant norms. Essentially, given the input, the goal is to search for the neurons that have high inner products with the input, because the activation function "filters" low activation. Thus, we formulate the contextual sparsity prediction of an MLP layer as the classical near-neighbor search problem under the inner product metric.</p><p>Definition 4.1 (Approximate MaxIP in MLP). Let c ∈ (0,1) and τ ∈ (0,1) denote two parameters. Given an n-vector dataset W 1 ⊂ S d-1 on a unit sphere, the objective of the (c, τ )-MaxIP is to construct a data structure that, given a query y ∈ S d-1 such that max w∈W 1 ⟨y,w⟩ ≥ τ , it retrieves a vector z from W 1 that satisfies ⟨y,z⟩ ≥ c•max w∈W 1 ⟨y,w⟩.</p><p>Remark 4.2. Our W 1 (first linear layer) and y (input embedding) in MLP blocks can be viewed as the dataset and query in Definition 4.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design</head><p>The standard state-of-the-art near-neighbor search methods and implementations slow down the computation. Take OPT-175B where d is 12288 as an example. HNSW <ref type="bibr" target="#b96">(Malkov &amp; Yashunin, 2018)</ref> requires more than 10ms, and FAISS <ref type="bibr" target="#b77">(Johnson et al., 2019)</ref> requires more than 4ms, while the MLP computation is only 0.2ms. The high dimensionality and complications of data structure implementation on GPU make the search time longer than the MLP computation. Therefore, we choose a neural network classifier as our near-neighbor search method to exploit the fast matrix multiplication on GPU. For each MLP block, we train a small twolayer fully connected network to predict contextual sparsity.</p><p>Collecting training data is straightforward because we know the contextual sparsity using dense computation. The training algorithm is summarized in Algorithm 1. The sparsified computation in W 1 has two steps: (1) Given y, the sparsity predictor SP M predicts a set S M of important neurons in weights W 1 . (2) Compute the sparsified MLP defined in Eq. equation 1. Note here the sparsity in MLP is highly structured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Sparse Predictor Training</head><p>Input: A pre-trained LLM block with parameter set M , token embedding set at block</p><formula xml:id="formula_6">M = {x i } i∈[N ] , threshold t Sparse Predictor SP P + ← ∅, P -← ∅ for i = 1 → N do P + ← P + ∪{(x i ,m r ) | m r ∈ M,m r (x i ) ≥ t} P -← P -∪{(x i ,m r ) | m r ∈ M,m r (x i ) &lt; t} end for SP ← TRAIN(P + ,P -,L)</formula><p>▷ L is a loss function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Contextual Sparsity Prediction in Attention Blocks</head><p>Attention blocks take around 30% I/Os in the generation. In this section, we describe how DEJAVU exploits contextual sparsity to speed up the Attention blocks.</p><p>Challenge: As discussed in Section 3.1, only a few heads perform important computations for a given input token. Similar to the MLP blocks, a fast selection of attention heads without full computation is required to reduce end-to-end latency. Furthermore, one particular challenge of sparse prediction in attention blocks is attention's dependence on previous tokens. On the one hand, it is unclear whether the past token's key and value caches are needed for sparse prediction. On the other hand, it is unclear how to handle the missing KV cache of past tokens for the current token computation at the selected head.</p><p>A Near-Neighbor Search Problem: Head prediction can also be formulated as a near-neighbor search problem based on our understanding in Section 3.2. Since each head is performing mean-shift clustering, after the first few layers, the current token embedding alone is sufficient for the prediction thanks to the token-mixing nature of the transformer. Therefore, the prediction can be based on the similarity between y and head parameters.</p><p>Approach: We design our attention sparse predictor to be the same architecture as the MLP sparse predictor. Each head is regarded as one class and a similar training process is used (Algorithm 1). Then, similar to how MLP prediction is performed, the attention sparsity predictor SP A selects a set S A of heads H i (see Eq. equation 2). To address the problem of missing KV cache for a past token, we exploit the fact that the generation latency is I/O bounded while computation is essentially "free". Specifically, for the predicted attention head of input y, we compute the corresponding keys, and values and store them in the KV cache. But we also save a copy of y for all the other non-selected heads. Then during the future token generation, if there is missing KV cache in the selected heads, we could load stored token embeddings and compute the keys and values together. This requires almost minimal extra memory access (the main cost is loading the weight matrices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reducing Overhead with Asynchronous Execution</head><p>Sparse prediction overhead may easily increase the end-toend latency rather than reduce it despite the reduction in FLOPs. Therefore, we introduce a look-ahead sparse prediction method, inspired by our observations in Section 3.3.</p><p>Challenge: Denote y l ∈ R d as the input to transformer layer l. We can write the computation at layer l as y l ← MHA l (y l ), y l ← MLP l ( y l ). With predictors SP l A and SP l M , the computation at the transformer layer l can be re-written as</p><formula xml:id="formula_7">S l A ← SP l A (y l ), y l ← MHA l S l A (y l ), S l M ← SP l M ( y l ), y l ← MLP l S l M ( y l )</formula><p>where set S l A is the contextual sparsity for the Attention block, and set S l M is the contextual sparsity for the MLP block at l-th layer. Note that the computation at Attention and MLP blocks have to wait for the sparse predictor decision. This overhead potentially outweighs the saving from Attention and MLP blocks in terms of latency.</p><p>Approach: In Section 3.3, we present the slowly evolving embedding phenomenon, which provides opportunities to relax the sequential computation to parallel computation. Along with the observation of low computation intensity during generation, we parallel the sparse prediction with the computation of each block ( See Figure <ref type="figure">2</ref>). The computation can be written as follows:</p><formula xml:id="formula_8">y l ← MHA l S l A (y l ), y l ← MLP l S l M ( y l ), S l+1 A ← SP l A (y l ), S l+1 M ← SP l M (y l ), We remark S l+1</formula><p>A and S l+1 M can be computed in parallel with y l or y l , while the previous 4 steps are sequential.</p><p>Theoretical guarantee: The sparse predictor can make further cross-layer decisions because of the residual connection. We present an informal lemma statement regarding crosslayer prediction. It is well-known that MaxIP is equivalent to ℓ 2 nearest neighbor search. For convenience, we use MaxIP here. We include more discussions and proofs in Section J.</p><p>Lemma 4.3 (Informal). Let ϵ ∈ (0,1). Let y l be input at l-th layer. Let y l-1 be the input at (l-1)-th layer. Suppose that ∥y l -y l-1 ∥ 2 ≤ ϵ. For any parameters c, τ such that ϵ &lt; O(cτ ). Then we can show that, solving MaxIP(c,τ ) is sufficient to solve MaxIP(0.99c,τ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hardware-efficient Implementation</head><p>We describe how DEJAVU is implemented in a hardwareefficient manner to realize the theoretical speedup of contextual sparsity. Taking into account hardware characteristics leads to over 2× speedup compared to an optimized dense model, and 4× faster than a standard sparse implementation.</p><p>We highlight some hardware characteristics of GPUs:</p><p>• Small-batch generation is bottlenecked by GPU memory I/Os <ref type="bibr" target="#b107">(NVIDIA, 2022;</ref><ref type="bibr" target="#b74">Ivanov et al., 2021;</ref><ref type="bibr" target="#b40">Dao et al., 2022)</ref>. This is because of low arithmetic intensity. For each element loaded from GPU memory, only a small number of floating point operations are performed.</p><p>• GPUs are block-oriented devices: loading a single byte of memory takes the same time as loading a block of memory around that same address <ref type="bibr" target="#b64">(Harris, 2013)</ref>. The block size is usually 128 bytes for NVIDIA GPUs <ref type="bibr" target="#b38">(Cook, 2012)</ref>.</p><p>These characteristics present some challenges in implementing contextual sparsity. However, they can be addressed with classical techniques in GPU programming.</p><p>Kernel fusion: A standard implementation of sparse matrix-vector multiply (e.g., in PyTorch) that separately indexes a subset of the matrix W 1 S M before multiplying with input y would incur 3× the amount of memory I/Os. Therefore, to avoid such overhead, we fuse the indexing and the multiplication step. Specifically, we load a subset of W 1 S M to memory, along with y, perform the multiply, then write down the result. This fused implementation (in Triton <ref type="bibr" target="#b128">(Tillet et al., 2019)</ref>) yields up to 4× speedup compared to a standard PyTorch implementation (Appendix E).</p><p>Memory coalescing: In the dense implementation, the weight matrices of two linear layers in MLP are stored as (W 1 ) ⊤ and W 2 so that no extra transpose operation is needed. They are conventionally stored in row-major format. In the sparse implementation, it allows us to load (W 1 S M ) ⊤ optimally (the second dimension is contiguous in memory). However, for cases where we need to load (W 2 S M ), this format significantly slows down memory loading, as indices in S M point to non-contiguous memory. We simply store these matrices in column-major format (i.e., store (W 2 ) ⊤ in row-major format), then use the same fused kernel above. Similarly, in attention blocks, we store attention output projection W O column-major format.</p><p>These two techniques (kernel fusion and memorycoalescing) make DEJAVU hardware-efficient, yielding up to 2× speedup end-to-end compared to the state-of-the-art FasterTransformer (Section 5.1). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>In Section 5.1, we present the end-to-end results that show DEJAVU achieves over 2× reduction in token generation latency compared to the state-of-the-art FasterTransformer and over 6× compared to Hugging Face with no accuracy loss. In Section 5.2, we perform a list of ablation studies such as independent evaluation on the inference-time contextual sparsity of the MLP block and the Attention block (Details are presented in Section C). At last, we present the additional results to demonstrate the future possibility of sparsifying the entire LLMs via layer skipping in Section C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">End-to-End Result</head><p>Experiment Setting: We compare the accuracy of DE-JAVU-OPT against the original OPT model on two language modeling datasets Wiki-Text <ref type="bibr" target="#b98">(Merity et al., 2016)</ref> and C4 <ref type="bibr" target="#b113">(Raffel et al., 2019)</ref> and seven few-shot downstream tasks: CB (de Marneffe et al., 2019), COPA <ref type="bibr" target="#b59">(Gordon et al., 2012)</ref>, Lambada <ref type="bibr" target="#b112">(Radford et al., 2019)</ref>, OpenBookQA <ref type="bibr" target="#b100">(Mihaylov et al., 2018)</ref>, PIQA <ref type="bibr" target="#b17">(Bisk et al., 2020)</ref>, RTE <ref type="bibr" target="#b57">(Giampiccolo et al., 2007)</ref>, <ref type="bibr">Winogrande (ai2, 2019)</ref>. We use lm-evalharness <ref type="bibr" target="#b54">(Gao et al., 2021)</ref> for zero-shot and five-shot tasks. We collect training data for the sparsity predictor using 500 random data points from the C4 training dataset. Our experiments are conducted on NVIDIA A100 80GB GPU servers.</p><p>No accuracy drop until 75% sparsity: In Figure <ref type="figure" target="#fig_11">6</ref>, we present DEJAVU-OPT-175B's accuracy trend. In a zero-shot setting, the average accuracy across tasks does not drop until 75% sparsity. A similar trend can be observed for the five-shot setting, which verifies the model's ability for in-context learning. This result is exceptionally encouraging given our observation in Figure <ref type="figure" target="#fig_10">1(a)</ref>, where we could impose 85% sparsity when allowed full computation.</p><p>Over 2× latency reduction: Figure <ref type="figure">7</ref> presents the latency speed-up for the token generation with OPT-175B at batch size 1, where DEJAVU achieves the best performance. At around 75% sparsity, DEJAVU speeds up generation by 1.8-2× compared to the state-of-the-art FasterTransformers (FT)<ref type="foot" target="#foot_8">foot_8</ref> and by 4.8-6× to Hugging Face (HF) implementation<ref type="foot" target="#foot_9">foot_9</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Results</head><p>Contextual Sparsity for Larger Batches: Although this paper focuses on latency-sensitive settings, we demonstrate that DEJAVU generalizes to larger batches. we present the Union contextual sparsity (fraction of neurons/heads that are not used by any of the inputs in the batch) of different batches sizes for MLP and Attention blocks, respectively, in Figure <ref type="figure">8</ref> and 11. The union operation is essential to realize a fast sparse GEMM. Surprisingly the number of MLP neurons and Attention heads that DEJAVU activated does not grow linearly with the batch size. This suggests a power law distribution rather than a uniform distribution of parameter access from all input examples. This provides an opportunity for potentially extending Dejavu to the high-throughout setting. For example, we can first pre-process the inputs and batch similar inputs to enjoy a higher level of union contextual sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual sparsity on MLP blocks:</head><p>We study the contextual sparsification of the MLP block in OPT-175B. We leave the Attention block as dense computation. Table <ref type="table" target="#tab_3">4</ref> shows the model performance at 85% sparsity. The MLP sparse predictor introduces no accuracy loss on both zero-shot tasks and language modeling. In the training of the MLP sparse predictor, we observe that the sparse predictor achieves high validation accuracy. The shallow layer seems easier to model because the predictor has validation accuracy over 99% in the shallow layers and drops to around 93% in the ending layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual sparsity on attention blocks:</head><p>In this section, we study the sparse predictor for the Attention block on OPT-175B and leave the MLP block as dense computation. Table <ref type="table" target="#tab_3">4</ref> displays the test accuracy on zero-shot tasks and perplexity on the language modeling datasets. In summary, the Attention sparse predictor introduces no accuracy loss at around 50% sparsity. During the training of the Attention sparse predictor, we observe different trends compared to the MLP sparse predictor. The validation accuracy is around 93% in the middle layers and near 99% in the shallow and deep layers.</p><p>Contextual Sparsity on Smaller Models: Our main experiments focus on OPT-175B. Here, we verify DEJAVU's effectiveness on a smaller model, specifically OPT-66B. In Table <ref type="table" target="#tab_4">5</ref>, we summarize the accuracy on zero-shot task at 50% sparsity. Similar to DEJAVU-OPT-175B, we notice no accuracy loss.</p><p>Contextual Sparsity on Other Models: We expand the evaluation to another model family. In Table <ref type="table" target="#tab_5">6</ref>, we summarize the accuracy at attention sparsity 50% and MLP sparsity 30%. Similar to OPT family, we notice no accuracy loss. The lower sparsity level in MLP is due to the difference in activation function.</p><p>Non-Contextual Sparsity: As we mentioned in Section 1, one could predict sparsity without contextual information.</p><p>For non-contextual sparsity, we rely on the original embedding at the input layer. At every block, we first pass the original embedding to record a subset of parameters yielding a large norm. In the second pass, the embedding at every layer only uses the recorded subset. As shown in Figure <ref type="figure" target="#fig_10">1</ref>, non-contextual prediction is not sufficient and leads to accuracy losses even at 50% sparsity. This result verifies our design choices of relying on the activation at every layer as input to make contextual sparsity predictions.</p><p>Compatibility with Quantization: Quantization is another promising direction for efficient language models. We investigate the possibility of combining contextual sparsity with quantization techniques. For DEJAVU-OPT-175B, we set the entire model sparsity at 75%. For quantization, we apply 4-bit quantization on model weights (W4A16). As shown in Table <ref type="table" target="#tab_6">7</ref>, the combination of quantization and DEJAVU almost always achieves better accuracy than DEJAVU or quantization alone. This suggests that the approximation errors from these two directions do not get compounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our main goal is to make LLM inference efficient so that their powerful in-context learning abilities can be used in more application domains. We observe that contextual sparsity can be accurately predicted with lightweight learning-based algorithms. This motivated us to design DEJAVU that uses asynchronous lookahead predictors and hardware-efficient sparsity to speed up LLM inference in wall-clock time. Our encouraging empirical results validate that contextual sparsity can reduce inference latency by over 2× compared to the state-of-the-art FasterTransformer without model quality drops. Our method is a step towards making LLMs more accessible to the general community, which could unlock exciting new AI applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents:</head><p>In Section A, we present an extended discussion on LLM inference and related works. In Section B, we provide more observation plots for slowly changing activation and further observation on the possibility of sparsifying LLMs via layer skipping. In Section C, we provide experiment details. In Section D, we demonstrate implementation details. In Section E, we provide detailed benchmarks regarding our implementation. In Section F, we define some basic notations and definitions.</p><p>In Section G, we define subspace embedding and show the norm preserving. In Section H, we introduce distances, angles, and inner product. In Section I, we provide the distance between different functions. In Section J, we provide the Near-neighbor Search data structure. In Section K, we discuss self-attention as a clustering algorithm in depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>Generative LLM inference. Taking OPT-175B as an example, assume 6 A100 80GB PCIe, based on the hardware specifications, we compare two main phases of inference time LLM, namely prompting and token generation in Table <ref type="table" target="#tab_0">1</ref>, and two major components, namely Multi-Head-Attention block and MLP block in Table <ref type="table" target="#tab_1">2</ref>. In practice, the token generation phase usually dominates the end-to-end test latency due to IO latency. Generating only two tokens is about the same latency as prompting. Further, during token generation, the MLP block is 2 × more expensive in both FLOPs and IO access. The hardware is often at low utilization because memory reads and writes are more limited on modern hardware than tensor core computation.</p><p>Given the rapid development of LLM, there is an emergence of systems that are specialized for LLM inference, such as Faster Transformer (NVIDIA), Orca <ref type="bibr" target="#b140">(Yu et al., 2022)</ref>, LightSeq <ref type="bibr">(Wang et al., 2021)</ref>, PaLM inference <ref type="bibr" target="#b109">(Pope et al., 2022)</ref>, TurboTransformers <ref type="bibr" target="#b49">(Fang et al., 2021)</ref>, and Deepspeed-Inference <ref type="bibr" target="#b5">(Aminabadi et al., 2022)</ref>. In practice, the token generation phase usually dominates the end-to-end inference time. Although the state-of-the-art systems introduce some helpful system optimizations for speedup, there is a lack of careful algorithm and system co-design to unleash the full potential of hardware efficiency during the LLM inference computation.</p><p>Near-neighbor Search for Efficient Deep Neural Networks. Near-neighbor Search is a well-studied problem with wide applications in recommendation system <ref type="bibr" target="#b138">(Xue et al., 2017;</ref><ref type="bibr" target="#b62">Hall &amp; Attenberg, 2015)</ref>, question answering <ref type="bibr" target="#b21">(Boytsov et al., 2016;</ref><ref type="bibr" target="#b116">Seo et al., 2019;</ref><ref type="bibr" target="#b26">Chang et al., 2020)</ref> and natural language processing <ref type="bibr" target="#b16">(Bengio et al., 2003;</ref><ref type="bibr" target="#b83">Lee et al., 2016)</ref>. There has been a line of work using Near-neighbor Search techniques such as Locality-sensitive hashing <ref type="bibr" target="#b58">(Gionis et al., 1999)</ref> and Graph-based indexing <ref type="bibr" target="#b95">(Malkov et al., 2014)</ref> for efficient deep neural network training or inference <ref type="bibr" target="#b143">(Zhang et al., 2018;</ref><ref type="bibr" target="#b28">Chen et al., 2019;</ref><ref type="bibr">2020a;</ref><ref type="bibr" target="#b79">Kitaev et al., 2020;</ref><ref type="bibr">Chen et al., 2021b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b93">Liu et al., 2022)</ref>.</p><p>Quantization, pruning, distillation for LLM inference. Various system relaxations have been studied for decades for model inference in machine learning. For example, quantization <ref type="bibr" target="#b63">(Han et al., 2015;</ref><ref type="bibr" target="#b75">Jacob et al., 2018;</ref><ref type="bibr" target="#b103">Nagel et al., 2019;</ref><ref type="bibr" target="#b144">Zhao et al., 2019)</ref>, pruning <ref type="bibr" target="#b102">(Molchanov et al., 2016;</ref><ref type="bibr" target="#b92">Liu et al., 2018;</ref><ref type="bibr" target="#b66">He et al., 2019;</ref><ref type="bibr" target="#b68">Hoefler et al., 2021)</ref>, and distillation <ref type="bibr" target="#b67">(Hinton et al., 2015;</ref><ref type="bibr" target="#b34">Cho &amp; Hariharan, 2019;</ref><ref type="bibr" target="#b127">Tang et al., 2019;</ref><ref type="bibr" target="#b129">Touvron et al., 2021)</ref> have been applied to speed up the inference of the machine learning model. Active research has recently attempted to apply such techniques in LLM inference. For example, zeroQuant <ref type="bibr" target="#b139">(Yao et al., 2022)</ref> and nuQmm <ref type="bibr" target="#b108">(Park et al., 2022)</ref> implement customized CUDA kernels to support tenor-wise or group-wise quantization for LLM inference; LLM.int8 <ref type="bibr" target="#b46">(Dettmers et al., 2022)</ref> adopts a mixed INT8/FP16 computation to diminish the influence of activation outliers; SmoothQuant <ref type="bibr" target="#b136">(Xiao et al., 2022)</ref> enables efficient 8-bit weight and activation for LLM inference; GPTQ <ref type="bibr" target="#b52">(Frantar et al., 2022)</ref> adopts a one-shot weight quantization method based on approximate second-order information for accuracy and efficiency; SparseGPT <ref type="bibr" target="#b51">(Frantar &amp; Alistarh, 2023)</ref> introduces an approximate sparse regression solver to enable the sparsity in LLM inference; <ref type="bibr" target="#b13">(Bansal et al., 2022)</ref> has reported that a small set of attention heads can perform primitive induction operations associated with in-context learning, and use this property to prune LLM for acceleration.</p><p>Residual connections in neural networks. Residual connection shows great advantages for neural network generalization, it provides additional paths for activations to reach the latter parts of the neural network by skipping some layers <ref type="bibr" target="#b65">(He et al., 2016)</ref>. The advancement of residual connections can be viewed as ensembles of multiple shallow neural networks <ref type="bibr" target="#b130">(Veit et al., 2016)</ref>. Plenty of active research has discussed the effectiveness of residual connections <ref type="bibr" target="#b12">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b15">Bello et al., 2021;</ref><ref type="bibr" target="#b1">Allen-Zhu &amp; Li, 2019;</ref><ref type="bibr" target="#b53">Frei et al., 2019)</ref>. However, as far as we know, there is no former work that leverages the property of residual connections to improve the efficiency of LLM inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Observation on Slowly Changing Observation</head><p>First, we present more plots on the cosine similarity between representations. Figure <ref type="figure">9</ref> plots the cosine similarity between activation across layers on OPT family. It is evident that similarity is high for the larger models.</p><p>There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as X +F (X), where F is either the Multi-Head Attention or two MLP Layer. Figure <ref type="figure" target="#fig_10">10</ref> plots the cosine similarity between X and X +F (X), which is close to 1.0, and the cosine similarity between . Cosine similarity between X and F (X), and the cosine similarity between X and X ′ in orange color. L2 norm of X and F (X) and X after layer normalization in purple on the right. Except on the first layer, ∥X∥ is significantly higher than ∥F (X)∥. ∥F (X)∥ is higher at the first layer, which corresponds to the low cosine similarity at the first layer.</p><p>For example, in our exploration stage mentioned in Section 4.1, we adopt HNSW, a state-of-art near-neighbor search method, to predict MLP sparse pattern, and we can see from the following table there is no drop in the perplexity at 90 % sparsity ratio. However, due to the high dimensionality of embedding and HNSW's reliance on CPU, the time HNSW took to identify the sparsity pattern is 10ms, which is longer than the MLP computation.</p><p>In our paper, we choose a neural network classifier as our near neighbor search method to take advantage of the fast matrix multiplication on GPU. And training such classifiers to predict sparsity patterns is not only cheaper in terms of training cost but also inherently different from the method concept. Improving the inference efficiency of Transformer models is a challenging task due to their sequential execution of Transformer layers. Each sub-block depends on the output of the previous one, leading to low hardware efficiency, particularly during the token generation phase where each forward pass is computed for only one token. However, the sequential execution of blocks and sub-blocks yields computation bubbles, and the latter involves a large amount of communication overhead.</p><p>Here, we present an interesting observation that can potentially alleviate these challenges. We found that the activation of the model changes slowly across blocks. Specifically, the cosine similarity of activations between adjacent blocks is often above 0.99. This suggests that the blocks might take the previous activation as input -parallelize or reorder the blockswithout significantly affecting the output. Slowly changing activations suggest that it may be possible to parallelize, reorder, or even skip blocks while maintaining a similar output. Some existing models, such as GPT-J <ref type="bibr" target="#b132">(Wang &amp; Komatsuzaki, 2021)</ref>, GPT-NeoX <ref type="bibr" target="#b18">(Black et al., 2022)</ref>, and PaLM <ref type="bibr" target="#b35">(Chowdhery et al., 2022)</ref> already placed the Attention block and MLP block in parallel in training to facilitate parallel computation and reduce the communication overhead.</p><p>Here we investigate the possibility at inference time. And surprisingly, we found parallelizing those blocks for models that are trained in a sequence manner will not hurt the performance of downstream tasks significantly. And surprisingly, we found parallelizing those blocks for models that are trained in a sequence manner will not hurt the performance of downstream tasks significantly. TableC.3 presents some preliminary results of OPT-175B and Bloom</p><p>Given the activation y and Transformer layer l, we have:</p><formula xml:id="formula_9">y l ← y l +MHA l (y l )</formula><p>y l ← y l +MLP l ( y l ) Parallelizing two blocks refers to placing the Attention and MLP blocks in parallel, i.e.: y l ← y+MHA l (y l )+MLP l (y l ) Parallelizing four blocks then parallelize the blocks of two Transformer layers, defined as follows:</p><p>y l+1 ← y l +MHA l (y l )+MLP l (y l )+MHA l+1 (y l )+MLP l+1 (y l ) Skipping layers is straightforward, which drops an entire Transformer layer for every n layers.</p><p>We are surprised to find that parallel two layers preserve accuracy on a series of tasks across models. Besides, randomly skipping 25% layers doesn't lead to catastrophic quality. Our findings suggest from the downstream task perspective, the activation patterns within the model are relatively consistent across different blocks, providing a potential avenue for future research on model compression and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation Details</head><p>Figure <ref type="figure" target="#fig_7">12</ref> presents a more detailed workflow of DEJAVU. The left diagram shows how an input y performs the sparse MHA with selected indices 0,3, predicted by the head predictor. Similarly, the right diagram shows how an input y performs the sparse MLP with selected indices 0,2, predicted by the neuron predictor of that layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Head Index</head><formula xml:id="formula_10">! ! = {0,3} Attention with " " ! # , " " ! $ , " " ! % $ % &amp; ($) " " ! ' Output Projection )%* " ! ($) % ( ($) " " " ) ! * = {0,2} $ σ($" " " + ) " " " + MLP " " ($)</formula><p>Selected Neurons Index Sparsified Attention Sparsified MLP Next, we will present a general explanation of two optimizations we used in DEJAVU implementation. Kernel fusion: A standard implementation of sparse matrix-vector multiply (e.g., W x in PyTorch) that separately indexes a subset of the matrix W [idx,:] before multiplying with input x would incur 3× the amount of memory IOs: one to load a subset of W from GPU memory, one to write that subset to a different contiguous region in memory, and one to load that (now contiguous) subset in again to multiply with x. Similarly, to use sparse matrix multiply routines (e.g., cuSparse), we would first need to convert W [idx,:] to sparse format, again incurring more memory IOs. We instead fuse the indexing and the multiplication step: we load a subset of W [idx,:] to memory, along with x, perform the multiply, then write down the result. This fused implementation (in Triton <ref type="bibr" target="#b128">(Tillet et al., 2019)</ref>) yields up to 4× speedup compared to a standard PyTorch implementation (Section E). Memory coalescing: the weight matrices are conventionally stored in row-major format. This allows us to load W [idx,:] optimally (as the second dimension is contiguous in memory). However, for cases where we need to load W [:,idx] (attention output projection and the 2nd weight matrix in the MLP) this format significantly slows down memory loading, as idx could contain indices pointing to non-contiguous memory. A simple solution is to store these matrices in column-major format (i.e., storing W ⊤ in contiguous row-major format), then use the same fused kernel above. This transposition is done once when loading the model, and incurs no added cost during generation. We validate that our hardware-aware implementation of sparse MLP and sparse attention (Section 4.4) yields wall-clock speed up compared to both dense MLP/attention and compared to the standard implementation in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Benchmarking</head><p>Recall that our implementation fuses the sparse indexing and the multiplication (W 1 S M ) ⊤ y for weight matrices (W 1 ) ⊤ and input vector y. In cases where we need to index W 2 S M , we store the transpose of W 2 to ensure memory coalescing. For the baseline implementation in PyTorch, we index (W 1 S M ) ⊤ as a separate operation before multiplying with y, which incurs more memory reads/writes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarly, we fuse the sparse indexing and the multiplication (W</head><formula xml:id="formula_11">Q S A ) ⊤ y, (W K S A ) ⊤ y, (W V S A ) ⊤ y for weight matrices (W Q ) ⊤ , (W K ) ⊤ , (W V ) ⊤</formula><p>and input vector y. Note we usually concatenate all three matrices in the standard implementation, but we separate them here for clarity. In cases where we need to index W O S A , we store the transpose of W O to ensure memory coalescing. In Figure <ref type="figure" target="#fig_8">13</ref> and Figure <ref type="figure" target="#fig_9">14</ref>, our sparse MLP and attention implementations are 4-5× faster than the baseline implementation in Pytorch, and remains faster than the dense version for density up to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Notations and Basic Definitions</head><p>For a positive integer n, let [n] := {1,2,•••,n}. For a matrix A ∈ R n×n , let A i,: and A :,j be two column vectors corresponding to the i-th row and the j-th column of A respectively, and A i,j be the entry at the i-th row and the j-th column. For a vector x ∈ R n , let √ x ∈ R n denote the vector with the i-th entry being √ x i and diag(x) ∈ R n×n denote the diagonal matrix with the i-th diagonal entry being x i . For two matrices A,W ∈ R n×n , let ∥A∥ W := (</p><formula xml:id="formula_12">n i=1 n j=1 W i,j A 2 i,j ) 1/2 and W •A denote the matrix where (W •A) i,j = W i,j A i,j . For matrix W ∈ R n×n , let D Wi := diag(W i,: ) with i ∈ [n].</formula><p>For two vectors x ∈ R n and w ∈ R n ≥0 , let ∥x∥ w := ( n i=1 w i x 2 i ) 1/2 . For a vector x, we denote ∥x∥ 2 := ( ℓ 2 norm. We denote ∥x∥ p := ( n i=1 |x i | p ) 1/p as its ℓ p norm. For a square matrix A, we denote tr[A] as the trace of matrix A. For a matrix A ∈ R n×k (suppose n ≥ k), we use ∥A∥ to denote its spectral norm, i.e., ∥A∥ = sup x ∥Ax∥ 2 /∥x∥ 2 . We use ∥A∥ F to denote its Frobenius norm ∥A∥ F := ( n i=1 k j=1 A 2 i,j ) 1/2 . Suppose matrix A ∈ R n×k has SVD decomposition U ΣV ⊤ where U ∈ R n×k (this matrix has orthonormal columns), Σ ∈ R k×k is a diagonal matrix, and V ∈ R k×k . We call columns of U are singular vectors. We use A † ∈ R k×n to denote the Moore-Penrose pseudoinverse, then A † = V Σ -1 U ⊤ . Suppose Σ ∈ R k×k is sorted diagonal matrix, let σ 1 ,•••,σ k denote the diagonal entries of Σ. Then we call σ i the i-th singular value of matrix, and we write it as σ i (A).</p><p>For any symmetric matrix B ∈ R k×k , we define its eigenvalue decomposition as U ΛU ⊤ , where Λ is a diagonal matrix. Let λ 1 ,•••,λ k denote the entries on diagonal of Λ ∈ R k×k . We say λ i is the i-th eigenvalue. Usually we write it as λ i (B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The connection between eigenvalues and singular values is</head><formula xml:id="formula_13">σ 2 i (A) = λ i (A ⊤ A)</formula><p>We use notation A ⪰ 0 to denote that matrix A is positive semidefinite (psd). Mathematically, A ⪰ 0 means for all vectors x, we have x ⊤ Ax ≥ 0.</p><p>Similarly, for two squarer matrices A and B, we use A ⪰ B to denote the case where for all vectors x, x ⊤ Ax ≥ x ⊤ Bx.</p><p>We use Pr[] and E[] for probability and expectation. We denote max{a,b} as the maximum between a and b. We denote min{a,b} (resp. max{a,b}) as the minimum (reps. maximum) between a and b.</p><p>Throughout, for non-negative real numbers a and b, we use the notation a</p><formula xml:id="formula_14">= (1±ϵ)b if a ∈ [(1-ϵ)b,(1+ϵ)b].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Subspace Embeddings and Norm Preserving</head><p>In Section G.1, we show the norm preserving of the soft-max functions. In Section G.2, we show the norm preserving of the ReLU function. In Section G.3, we introduce the folded Guassian distribution. In Section G.4, we introduce the ℓ 2 subspace embedding. In Section G.5, we introduce the ℓ 1 subspace embedding. In Section G.6, we introduce different sketching matrices for subspace embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Soft-Max Functions</head><p>Let K ∈ R s×d and V ∈ R d×s . Inspired by the softmax unit in the attention scheme of large language models. The softmax related regression has been studied in many settings <ref type="bibr" target="#b141">(Zandieh et al., 2023;</ref><ref type="bibr" target="#b2">Alman &amp; Song, 2023;</ref><ref type="bibr" target="#b23">Brand et al., 2023;</ref><ref type="bibr" target="#b90">Li et al., 2023b;</ref><ref type="bibr">Deng et al., 2023b;</ref><ref type="bibr">a;</ref><ref type="bibr">Gao et al., 2023a;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr">Gao et al., 2023b)</ref>. In this work, we follow the standard softmax definition. We define σ 1 : R s → R s to be a softmax function, i.e., for any vector y ∈ R s , the σ(y) can be written as</p><formula xml:id="formula_15">σ 1 (y) i = exp(y i ) d j=1 exp(y j ) , ∀i ∈ [d]</formula><p>The standard softmax is ℓ 1 version. In this work, we also consider the ℓ 2 generalization. We define σ 2 : R s → R s to be a softmax function (ℓ 2 version), i.e., for any vector y ∈ R s , the σ(y) can be written as</p><formula xml:id="formula_16">σ 2 (y) i = exp(y i ) ( d j=1 exp(2y j )) 1/2 , ∀i ∈ [d]</formula><p>We define function f :</p><formula xml:id="formula_17">R d → R d f (x) = V •(σ(K •x))<label>(3)</label></formula><p>Definition G.1. We say X ⊂ R d is a rank-k subspace, if there is an orthonormal basis U ∈ R d×k , for any x ∈ X , there is y ∈ R k such that x = U y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We can have</head><p>Lemma G.2. Let τ ∈ (0,1). Let X ⊂ R d denote a subspace with rank k. Let f be defined based on</p><formula xml:id="formula_18">σ 2 function. Let V is a random Gaussian matrices with d ≥ Ω(ϵ -2 (k+log(1/δ))) rows. Let V = τ V , then we have with probability 1-δ (1-ϵ)τ ∥x∥ 2 ≤ ∥f (x)∥ ≤ (1+ϵ)τ ∥x∥ 2 . for all unit vectors x ∈ X . Further, if d = O(k+log(1/δ)), then we have 0.5τ ∥x∥ 2 ≤ ∥f (x)∥ ≤ 2τ ∥x∥ 2 .</formula><p>Remark G.3. The above condition implies that f is a shrinking operator but also not shrinking arbitrarily small.</p><p>Proof. Given d ≥ Ω(ϵ -2 (k+log(1/δ))), by using Lemma G.11 , we have (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 As the input of the function f here is the output of a softmax function (ℓ 2 version), we know that ∥y∥ 2 = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus, we have</head><p>(1-ϵ) ≤ ∥V y∥ 2 ≤ (1+ϵ) By rescaling V , we have</p><formula xml:id="formula_19">(1-ϵ)∥x∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥x∥ 2 . Lemma G.4. Let τ ∈ (0,1). Let X ⊂ R d denote a subspace with rank k. Let f be defined based on σ 1 function. Suppose V is a random Gaussian matrix with d ≥ Ω((k+log(1/δ))) rows. Let V = 1 2 τ V . Then we have 1 4 √ s τ •∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ τ •∥x∥ 2</formula><p>for all unit vectors x.</p><p>Proof. By property of subspace embedding, we know that if d ≥ Ω(ϵ -2 (s+log(1/δ))), (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 By property of function of f , we know we only need to care ∥y∥ 1 = 1, this implies that</p><formula xml:id="formula_20">1 √ s ∥y∥ 1 ≤ ∥y∥ 2 ≤ ∥y∥ 1</formula><p>On one hand, we have</p><formula xml:id="formula_21">∥V y∥ 2 ≤ (1+ϵ)•∥y∥ 2 ≤ (1+ϵ)•∥y∥ 1 = (1+ϵ),<label>(4)</label></formula><p>where the first step follows from ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 , the second step follows from ∥y∥ 2 ≤ ∥y∥ 1 and the last step follows from ∥y∥ 1 = 1.</p><p>On the other hand, we have</p><formula xml:id="formula_22">∥V y∥ 2 ≥ (1-ϵ)∥y∥ 2 ≥ 1 √ s (1-ϵ)∥y∥ 1 = 1 √ s (1-ϵ),<label>(5)</label></formula><p>where the first step follows from (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 , the second step follows from 1 √ s ∥y∥ 1 ≤ ∥y∥ 2 and the last step follows from ∥y∥ 1 = 1.</p><p>Combining Eq. ( <ref type="formula" target="#formula_22">5</ref>)and Eq. ( <ref type="formula" target="#formula_21">4</ref>) together, we have</p><formula xml:id="formula_23">(1-ϵ) 1 √ s ≤ ∥V y∥ 2 ≤ (1+ϵ) Choosing ϵ = 1/2, we have 1 2 √ s ≤ ∥V y∥ 2 ≤ 2. By V = 1 2 τ V and ∥x∥ 2 = 1, we have 1 4 √ s τ ∥x∥ 2 ≤ ∥V y∥ 2 ≤ τ ∥x∥ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 ReLU Functions</head><p>We use ϕ : R → R to denote ReLU function, i.e., ϕ(z) = max{z,0}.</p><p>We define function g :</p><formula xml:id="formula_24">R d → R d g(x) = V •(ϕ(K •x))<label>(6)</label></formula><p>Let K ∈ R s×d and V ∈ R d×s .</p><p>Lemma G.5. Let X ⊂ R d denote a rank-k subspace. Let K denote a random Gaussian matrix. Let V denote a random Gaussian matrix. Let s ≥ Ω(ϵ -2 klog(1/(δϵ))). Let d ≥ Ω(ϵ -2 (k + log(1/δ))). Then we know with high probability 1 -δ, for all unit vector x ∈ X</p><formula xml:id="formula_25">(1-ϵ)∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ)∥x∥ 2 Proof. Suppose s ≥ Ω(ϵ -2 log(1/δ)).</formula><p>Using Lemma G.6, Fact G.7, we can show that for each fixed</p><formula xml:id="formula_26">(1-ϵ)∥x∥ 2 ≤ ∥ϕ(Kx)∥ 2 ≤ (1+ϵ)∥x∥ 2 holds with probability 1-δ.</formula><p>By a standard ϵ-net argument (Lemma G.9), the net points in X is at most (10/ϵ) O(k) .</p><p>Taking a union bound over all the net points, we can show that for all x ∈ X</p><p>(1-ϵ)∥x∥ 2 ≤ ∥ϕ(Kx)∥ 2 ≤ (1+ϵ)∥x∥ 2 holds with probability 1-δ/2 and s ≥ Ω(ϵ -2 klog(1/(δϵ))).</p><p>Further, we using Lemma G.11, we can show that</p><p>(1-ϵ)∥ϕ(Kx)∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ)∥ϕ(Kx)∥ 2 holds with probability 1-δ/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining together,</head><p>(1-ϵ) 2 ∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ) 2 ∥x∥ 2 G.5 ℓ 1 subspace embedding When p = 1, using Cauchy random variables, Sohler and Woodruff <ref type="bibr" target="#b119">(Sohler &amp; Woodruff, 2011)</ref> showed there exist ℓ 1 oblivious subspace embeddings with O(dlogd) rows and κ = O(dlogd). This approach was generalized by using p-stable random variables in work of <ref type="bibr" target="#b97">Meng and Mahoney (Meng &amp; Mahoney, 2013)</ref> to ℓ p -norms when 1 &lt; p &lt; 2, where they showed there exist ℓ p oblivious subspace embeddings with O(dlogd) rows and κ = O((dlogd) 1/p ). Unlike the case when p = 2, due to the large distortion</p><p>In <ref type="bibr" target="#b133">(Wang &amp; Woodruff, 2018)</ref>, they show for every 1 ≤ p &lt; 2, any oblivious subspace embedding with dimension r has distortion κ = Ω(</p><formula xml:id="formula_27">1 ( 1 d ) 1/p •log 2/p r+( r n ) 1/p-1/2</formula><p>). They also give sparse oblivious subspace embeddings for every 1 ≤ p &lt; 2 which are optimal in dimension and distortion, up to poly (logd) factors. Importantly for p = 1, they achieve r = O(dlogd),κ = O(dlogd) and s = O(logd) non-zero entries per column.</p><p>Definition G.10 (ℓ 1 subspace embedding). Let 0 &lt; α &lt; β be parameters. We will say a matrix S is an ℓ 1 subspace embedding for an n×d matrix A if there are constants c 1 ,c 2 &gt; 0 so that for all x ∈ R d , ∥Ax∥ ≤ ∥SAx∥ 1 ≤ d c1 ∥Ax∥ 1 , and S has at most d c2 rows.</p><formula xml:id="formula_28">G.6 Random Matrices Matrices b Time for R•A Reference Random Gaussian ϵ -2 (d+log(1/δ)) T mat (b,n,d) Thm. 6 of (Woodruff, 2014) SRHT ϵ -2 ( √ d+ √ logn) 2 log(d/δ) ndlog(ϵ -1 d(logn)) Thm. 7 of (Woodruff, 2014) AMS ϵ -2 (d+log(1/δ)) T mat (b,n,d) Follow from JL guarantee Count-sketch ϵ -2 δ -1 d 2 nnz(A) Thm. 9 of (Woodruff, 2014) Sparse embedding ϵ -2 d• poly log(d/(ϵδ)) ϵ -1 nnz(A) poly log(d/(ϵδ)) Thm. 10 (2) of (Woodruff, 2014) Sparse embedding ϵ -2 d 1+γ</formula><p>ϵ -1 nnz(A)poly(1/γ) Thm. 10 (1) of <ref type="bibr" target="#b135">(Woodruff, 2014)</ref> Table <ref type="table">9</ref>. Summary for different sketching matrices for subspace embedding. The sketching matrix R has size b×n. The vectors are from the column subspace of matrix A with size n×d.ϵ ∈ (0,1) is the error parameter, and δ ∈ (0,1) is the probability parameter. Tmat (a,b,c) denotes the running time of fast matrix multiplication of two matrices with size a×b and b×c. In the first sparse embedding matrix, each column has s ≥ ϵ -1 poly log(d/(ϵδ)) non-zero entries; In the second sparse embedding matrix, each column has s ≥ ϵ -1 poly (1/γ) non-zero entries, γ &gt; 0 is a tunable parameter that gives different trade-offs, and δ can be as small as 1/ poly (d). For count-sketch matrices, the subspace embedding guarantee is proved from JL moment property, instead of directly from JL guarantee.</p><p>Lemma G.11 (Theorem 6 of <ref type="bibr" target="#b135">(Woodruff, 2014)</ref>). Let 0 &lt; ϵ,δ &lt; 1 and S = 1 √ k R ∈ R k×n where the entries R i,j of R are independent standard normal random variables. Then if k = Θ(ϵ -2 (d+log(1/δ))), then for any fixed n×d matrix A, with probability 1-δ,S is a (1±ϵ)ℓ 2 -subspace embedding for A, that is, simultaneously for all x ∈ R d ,∥SAx∥ 2 = (1±ϵ)∥Ax∥ 2 . Here C &gt; 0 is an absolute constant.</p><p>We consider several standard sketching matrices:</p><p>1. Random Gaussian matrices.</p><p>2. Subsampled randomized Hadamard/Fourier transform (SRHT) matrices <ref type="bibr" target="#b94">(Lu et al., 2013)</ref>.</p><p>3. AMS sketch matrices <ref type="bibr" target="#b4">(Alon et al., 1996)</ref>, random {-1,+1} per entry. 4. Count-Sketch matrices <ref type="bibr" target="#b27">(Charikar et al., 2002)</ref>, each column only has one non-zero entry, and is -1,+1 half probability each.</p><p>5. Sparse embedding matrices <ref type="bibr" target="#b104">(Nelson &amp; Nguyên, 2013)</ref>, each column only has s non-zero entries, and each entry is</p><formula xml:id="formula_29">-1 √ s ,+ 1 √ s half probability each.</formula><p>6. Uniform sampling matrices.</p><p>Definition G.12 (Random Gaussian matrix). We say R ∈ R b×n is a random Gaussian matrix if all entries are sampled from N (0,1/b) independently.</p><p>Definition G.13 (Subsampled randomized Hadamard/Fourier transform matrix <ref type="bibr" target="#b94">(Lu et al., 2013)</ref>). We say R ∈ R b×n is a subsampled randomized Hadamard transform (SRHT) matrix<ref type="foot" target="#foot_11">foot_11</ref> if it is of the form R = n/bSHD, where S ∈ R b×n is a random matrix whose rows are b uniform samples (without replacement) from the standard basis of R n ,H ∈ R n×n is a normalized Walsh-Hadamard matrix, and D ∈ R n×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.</p><p>Definition G.14 (AMS sketch matrix <ref type="bibr" target="#b4">(Alon et al., 1996)</ref>). Let h 1 ,h 2 ,•••,h b be b random hash functions picking from a 4-wise independent hash family H = {h :</p><formula xml:id="formula_30">[n] → {-1 √ b ,+ 1 √ b }}.</formula><p>Then R ∈ R b×n is a AMS sketch matrix if we set R i,j = h i (j) Definition G.15 (Count-sketch matrix <ref type="bibr" target="#b27">(Charikar et al., 2002)</ref>). Let h : [n] → [b] be a random 2-wise independent hash function and σ : [n] → {-1,+1} be a random 4-wise independent hash function. Then R ∈ R b×n is a count-sketch matrix if we set R h(i),i = σ(i) for all i ∈ [n] and other entries to zero. Definition G.16 (Sparse embedding matrix I <ref type="bibr" target="#b104">(Nelson &amp; Nguyên, 2013)</ref>). We say R ∈ R b×n is a sparse embedding matrix with parameter s if each column has exactly s non-zero elements being ±1/ √ s uniformly at random, whose locations are picked uniformly at random without replacement (and independent across columns)<ref type="foot" target="#foot_12">foot_12</ref> . Definition G.17 (Sparse embedding matrix II <ref type="bibr" target="#b104">(Nelson &amp; Nguyên, 2013)</ref>). Let h : [n] × [s] → [b/s] be a random 2-wise independent hash function and σ : [n]×[s] → {-1,1} be a 4-wise independent. Then R ∈ R b×n is a sparse embedding matrix II with parameter s if we set R (j-1)b/s+h(i,j),i = σ(i,j)/ √ s for all (i,j) ∈ [n]×[s] and all other entries to zero<ref type="foot" target="#foot_13">foot_13</ref> .</p><p>Definition G.18 (Uniform sampling matrix). We say R ∈ R b×n is a uniform sampling matrix if it is of the form R = n/bSD, where S ∈ R b×n is a random matrix whose rows are b uniform samples (without replacement) from the standard basis of R n , and D ∈ R n×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Distances, Angles, and Inner Product</head><p>Most of the properties in this section are very standard in literature, e.g., see <ref type="bibr" target="#b61">(Gu et al., 2023)</ref>.</p><p>Let U ∈ R n×k denote an orthonormal basis, we use</p><formula xml:id="formula_31">U ⊥ ∈ R n×(n-k) denote the matrix such that U U ⊤ +U ⊥ U ⊤ ⊥ = I n . Definition H.1. Let X ∈ R n×k and Y ∈ R n×k .</formula><p>For any matrix X, and for orthogonal matrix</p><formula xml:id="formula_32">Y (Y ⊤ Y = I k ) we define • tanθ(Y,X) := ∥Y ⊤ ⊥ X(Y ⊤ X) -1 ∥</formula><p>For orthogonal matrices Y and X (Y ⊤ Y = I k and X ⊤ X = I k ), we define</p><p>• cosθ(Y,X) := σ min (Y ⊤ X).</p><p>-It is obvious that cos(Y,X) = 1/∥(Y ⊤ X) -1 ∥ and cos(Y,X) ≤ 1.</p><p>• sinθ(Y,X) := ∥(I -Y Y ⊤ )X∥.</p><p>-</p><formula xml:id="formula_33">It is obvious that sinθ(Y,X) = ∥Y ⊥ Y ⊤ ⊥ X∥ = ∥Y ⊤ ⊥ X∥ and sinθ(Y,X) ≤ 1. • dist(Y,X) := min Q∈O k ∥Y Q-X∥</formula><p>where O k is the set of k×k orthogonal matrices.</p><p>Lemma H.2 (Structural lemma for orthogonal matrices). Let X,Y ∈ R n×k be orthogonal matrices. Then</p><formula xml:id="formula_34">(Y ⊤ X) ⊥ = Y ⊤ ⊥ X. Proof. Let us first compute the Gram of Y ⊤ X, which is X ⊤ Y Y ⊤ X = X ⊤ (I -Y ⊥ Y ⊤ ⊥ )X = X ⊤ X -X ⊤ Y ⊥ Y ⊤ ⊥ X = I k -X ⊤ Y ⊥ Y ⊤ ⊥ X,</formula><p>We also have</p><formula xml:id="formula_35">∥x+y∥ 2 ≤ ∥x∥ 2 +∥y∥ 2 ≤ 1+ϵ<label>(7)</label></formula><p>We have</p><formula xml:id="formula_36">(1-ϵ) 2 ≥ 1-2ϵ<label>(8)</label></formula><p>We also have</p><formula xml:id="formula_37">1 (1+ϵ) 2 ≥ 1-3ϵ<label>(9)</label></formula><p>where ϵ ∈ (0,0.1).</p><p>Combining Eq. ( <ref type="formula" target="#formula_36">8</ref>) and Eq. ( <ref type="formula" target="#formula_37">9</ref>), we have</p><formula xml:id="formula_38">1 (1+ϵ) 2 •(1-ϵ) 2 ≥ (1-2ϵ)•(1-3ϵ) = 1-5ϵ+6ϵ 2 ≥ 1-5ϵ+ϵ = 1-4ϵ</formula><p>(10) where the first step follows from Eq. ( <ref type="formula" target="#formula_36">8</ref>) and Eq. ( <ref type="formula" target="#formula_37">9</ref>) and the rest of them follow from simple algebra.</p><p>Finally, we have</p><formula xml:id="formula_39">1-⟨x,z⟩ 2 = 1-⟨x, x+y ∥x+y∥ 2 ⟩ 2 = 1- 1 ∥x+y∥ 2 2 ⟨x,x+y⟩ 2 = 1- 1 ∥x+y∥ 2 2 •(∥x∥ 2 2 +⟨x,y⟩) 2 = 1- 1 ∥x+y∥ 2 2 •(1+⟨x,y⟩) 2 ≤ 1- 1 (1+ϵ) 2 •(1+⟨x,y⟩) 2 ≤ 1- 1 (1+ϵ) 2 •(1-ϵ) 2 ≤ 1-(1-4ϵ) = 4ϵ,</formula><p>where the first step follow the definition of z, the second step follows from the reorganization, the third step follows from the definition of inner product, the fourth step follows from ∥x∥ 2 = 1, the fifth step follows from Eq. ( <ref type="formula" target="#formula_35">7</ref>), the sixth step follows from 1+⟨x,y⟩ ≥ 1-|⟨x,y⟩| ≥ 1-∥x∥ 2 •∥y∥ 2 ≥ 1-ϵ, the seventh step follows from Eq. ( <ref type="formula">10</ref>) and the last step follows from simple algebra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Function Approximations</head><p>We first we show the function approximation for two operators in Section I.1, which means that there are two functions. Then we show the function approximations for four operators in Section I.2. Assume the the following conditions</p><formula xml:id="formula_40">• Condition 1a. f 1 is a linear function • Condition 1b. ∥f 1 (x)∥ 2 ≤ ϵ 1 ∥x∥ 2 (f 1 is shrinking) • Condition 1c. ∥f 1 (x)-f 1 (y)∥ 2 ≤ L 1 ∥x-y∥ 2 (f 1 is Lipschitz) • Condition 2a. f 2 is a linear function • Condition 2b. ∥f 2 (x)∥ 2 ≤ ϵ 2 ∥x∥ 2 (f 2 is shrinking) • Condition 2c. ∥f 2 (x)-f 2 (y)∥ 2 ≤ L 2 ∥x-y∥ 2 (f 2 is Lipschitz)</formula><p>We define three functions</p><formula xml:id="formula_41">• g 1 (x) =: (I +f 1 )•(I +f 2 )(x) = x+f 2 (x)+f 1 (x+f 2 (x)) • g 2 (x) =: (I +f 2 )•(I +f 1 )(x) = x+f 1 (x)+f 2 (x+f 1 (x)) • g 3 (x) =: (I +f 1 +f 2 )(x) = x+f 1 (x)+f 2 (x)</formula><p>Then we can show that</p><formula xml:id="formula_42">• Part 1. ∥g 1 (x)-g 2 (x)∥ 2 ≤ 2ϵ 1 ϵ 2 ∥x∥ 2 (if f 1 and f 2 are linear functions) • Part 2. ∥g 1 (x)-g 2 (x)∥ 2 ≤ (ϵ 2 •L 1 +ϵ 1 •L 2 )∥x∥ 2 (if f 1 and f 2 are Lipschitz functions) • Part 3. ∥g 1 (x)-g 3 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 (if f 1 is a linear function) • Part 4. ∥g 1 (x)-g 3 (x)∥ 2 ≤ ϵ 2 •L 1 ∥x∥ 2 (if f 1 is a Lipschitz function) • Part 5. ∥g 2 (x)-g 3 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 (if f 2 is a linear function) • Part 6. ∥g 2 (x)-g 3 (x)∥ 2 ≤ ϵ 1 •L 2 ∥x∥ 2 (if f 2 is a Lipschitz function)</formula><p>Proof. Part 1.</p><p>We have</p><formula xml:id="formula_43">∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 +ϵ 1 ϵ 2 ∥x∥ 2 = 2ϵ 1 ϵ 2 ∥x∥ 2</formula><p>where the first step follows from triangular inequality, the second step follows from Part 3 and Part 5 and the last step follows from simple algebra.</p><p>Part 2.</p><p>We have</p><formula xml:id="formula_44">∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ ϵ 2 •L 1 ∥x∥ 2 +ϵ 1 •L 2 ∥x∥ 2 = (ϵ 2 •L 1 +ϵ 1 •L 2 )∥x∥ 2</formula><p>where the first step follows from triangular inequality, the second step follows from Part 4 and Part 6 and the last step follows from simple algebra.</p><p>Part 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We have ∥g</head><formula xml:id="formula_45">1 (x)-g 3 (x)∥ 2 = ∥f 1 (x+f 2 (x))-f 1 (x)∥ 2 = ∥f 1 (x+f 2 (x)-x)∥ 2 = ∥f 1 (f 2 (x))∥ 2 ≤ ϵ 1 •∥f 2 (x)∥ 2 ≤ ϵ 1 •ϵ 2 •∥x∥ 2</formula><p>, where the first step follows from the definition of g 1 and g 3 , the second step follows from the fact that f 1 is a linear function, the third step follows from simple algebra, the fourth step follows from Condition 1b and the last step follows from Condition 2b.</p><p>Part 4.</p><formula xml:id="formula_46">∥g 1 (x)-g 3 (x)∥ 2 = ∥f 1 (x+f 2 (x))-f 1 (x)∥ 2 ≤ L 1 •∥x+f 2 (x)-x∥ 2 = L 1 •∥f 2 (x)∥ 2 ≤ L 1 •ϵ 2 ∥x∥ 2</formula><p>, where the first step follows from definition of g 1 and g 3 , the second step follows from Condition 1c, the third step follows from simple algebra and the last step follows from Condition 2b.</p><p>Part 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We have ∥g</head><formula xml:id="formula_47">2 (x)-g 3 (x)∥ 2 = ∥f 2 (x+f 1 (x))-f 2 (x)∥ 2 = ∥f 2 (x+f 1 (x)-x)∥ 2 = ∥f 2 (f 1 (x))∥ 2 ≤ ϵ 2 •∥f 1 (x)∥ 2 ≤ ϵ 2 •ϵ 1 •∥x∥ 2</formula><p>, where the first step follows from the definition of g 2 and g 3 , the second step follows from the fact that f 2 is a linear function, the third step follows from simple algebra, the fourth step follows from Condition 2b and the last step follows from Condition 1b. Part 6.</p><formula xml:id="formula_48">∥g 2 (x)-g 3 (x)∥ 2 = ∥f 2 (x+f 1 (x))-f 2 (x)∥ 2 ≤ L 2 •∥x+f 1 (x)-x∥ 2 = L 2 •∥f 1 (x)∥ 2 ≤ L 2 •ϵ 1 ∥x∥ 2 ,</formula><p>where the first step follows from definition of g 1 and g 3 , the second step follows from Condition 2c, the third step follows from simple algebra and the last step follows from Condition 1b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Function Approximations for Four Operators</head><p>Lemma I.2. For each i ∈ [4], we assume the following conditions</p><formula xml:id="formula_49">• i(a) f i is a linear function • i(b) ∥f i (x)∥ 2 ≤ ϵ i ∥x∥ 2 (f i is shriking) • i(c) ∥f i (x)-f i (y)∥ 2 ≤ L i ∥x-y∥ 2 (f i is Lipschitz)</formula><p>We define three functions</p><formula xml:id="formula_50">• g 1 (x) := (I +f 1 )•(I +f 2 )•(I +f 3 )•(I +f 4 )(x) • g 2 (x) := (I +f 1 )•(I +f 3 )•(I +f 2 )•(I +f 4 )(x) • g 3 (x) := (I +f 1 +f 2 +f 3 +f 4 )(x)</formula><p>Then, we can show that</p><formula xml:id="formula_51">• Part 1. ∥g 1 (x)-g 2 (x)∥ 2 ≤ 2(ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 2. ∥g 1 (x) -g 2 (x)∥ 2 ≤ (2L 1 ϵ 2 + 2L 1 ϵ 3 + 2L 1 ϵ 4 + L 2 ϵ 3 + 2L 2 ϵ 4 + 2L 3 ϵ 4 + 2L 1 ϵ 2 ϵ 3 + 2L 1 ϵ 2 ϵ 4 + 2L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 2 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) • Part 3. ∥g 1 (x)-g 3 (x)∥ 2 ≤ (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 4. ∥g 1 (x) -g 3 (x)∥ 2 ≤ (L 1 ϵ 2 + L 1 ϵ 3 + L 1 ϵ 4 + L 2 ϵ 3 + L 2 ϵ 4 + L 3 ϵ 4 + L 1 ϵ 2 ϵ 3 + L 1 ϵ 2 ϵ 4 + L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 + L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) • Part 5. ∥g 2 (x)-g 3 (x)∥ 2 ≤ (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 6.∥g 2 (x) -g 3 (x)∥ 2 ≤ (L 1 ϵ 2 + L 1 ϵ 3 + L 1 ϵ 4 + L 2 ϵ 4 + L 3 ϵ 2 + L 3 ϵ 4 + L 1 ϵ 2 ϵ 3 + L 1 ϵ 2 ϵ 4 + L 1 ϵ 3 ϵ 4 + L 3 ϵ 2 ϵ 4 + L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) Proof. Part 1.</formula><p>We have</p><formula xml:id="formula_52">∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ 2(ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2</formula><p>where the first step follows from triangular inequality and the last step follows from Part 3 and Part 5.</p><p>Part 2.</p><p>We have</p><formula xml:id="formula_53">∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ (2L 1 ϵ 2 +2L 1 ϵ 3 +2L 1 ϵ 4 +L 2 ϵ 3 +2L 2 ϵ 4 +2L 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 +2L 1 ϵ 2 ϵ 4 +2L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 2 ϵ 4</formula><p>)∥x∥ 2 where the first step follows from triangular inequality and the last step follows from Part 4 and Part 6.</p><p>Part 3. We have ∥g</p><formula xml:id="formula_54">1 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 2 )•(I +f 3 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+ f 3 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 3 (f 4 (x))+f 2 (f 4 (x)+f 3 (x+f 4 (x)))+f 1 (f 4 (x)+ f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))))∥ 2 = ∥f 3 (f 4 (x))+f 2 (f 4 (x))+f 2 (f 3 (x))+f 2 (f 3 (f 4 (x)))+ f 1 (f 4 (x))+f 1 (f 3 (x))+f 1 (f 3 (f 4 (x)))+f 1 (f 2 (x))+f 1 (f 2 (f 4 (x))) +f 1 (f 2 (f 3 (x)))+f 1 (f 2 (f 3 (f 4 (x)))))∥ 2 ≤ ∥f 3 (f 4 (x))∥ 2 +∥f 2 (f 4 (x))∥ 2 +∥f 2 (f 3 (x))∥ 2 +∥f 2 (f 3 (f 4 (x)))∥ 2 + ∥f 1 (f 4 (x))∥ 2 +∥f 1 (f 3 (x))∥ 2 +∥f 1 (f 3 (f 4 (x)))∥ 2 +∥f 1 (f 2 (x))∥ 2 +∥f 1 (f 2 (f 4 (x)))∥ 2 + ∥f 1 (f 2 (f 3 (x)))∥ 2 +∥f 1 (f 2 (f 3 (f 4 (x))))∥ 2 ≤ (ϵ 3 ϵ 4 +ϵ 2 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 4 +ϵ 1 ϵ 3 +ϵ 1 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 = (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 ,</formula><p>where the first step follows from the definition of g 1 and g 3 , the second step follows from simple algebra, the third step follows from reorganization, the fourth step follows from the fact that all f i ,∀i ∈ [4] are linear function, the fifth step follows from triangular inequality, the sixth step follows from i(b) and the last step follows from reorganization.</p><p>Part 4. We have</p><formula xml:id="formula_55">∥g 1 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 2 )•(I +f 3 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+ f 3 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x))) +f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -f 1 (x)-f 2 (x)-f 3 (x))∥ 2 = ∥f 3 (x+f 4 (x))-f 3 (x)+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))-f 2 (x) +f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x))))-f 1 (x)∥ ≤ L 3 ∥x+f 4 (x)-x∥ 2 +L 2 ∥x+f 4 (x)+f 3 (x+f 4 (x))-x∥ 2 +L 1 ∥x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))-x∥ 2 ≤ L 3 ∥f 4 (x)∥ 2 +L 2 ∥f 4 (x)+f 3 (x+f 4 (x))∥ 2 +L 1 ∥f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x+f 4 (x)∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x+f 4 (x)∥ 2 +L 1 ϵ 2 ∥x+f 4 (x)+f 3 (x+f 4 (x))∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x∥+L 2 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥ 2 +L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 3 ∥x+f 4 (x)∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x∥+L 2 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥ 2 +L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 3 ϵ 4 ∥x∥ 2 = (L 3 ϵ 4 +L 2 ϵ 4 +L 2 ϵ 3 +L 2 ϵ 3 ϵ 4 +L 1 ϵ 4 +L 1 ϵ 3 +L 1 ϵ 3 ϵ 4 +L 1 ϵ 2 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 = (L 1 ϵ 2 +L 1 ϵ 3 +L 1 ϵ 4 +L 2 ϵ 3 +L 2 ϵ 4 +L 3 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 3 ϵ 4 +L 2 ϵ 3 ϵ 4 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2</formula><p>where the first step follows from the definition of g 1 and g 3 , the second step follows from simple algebra, the third step follows from simple algebra, the fourth step follows from reorganization, the fifth step follows from the fact that all f i ,∀i ∈ [4] are Lipschitz functions, the sixth step follows from simple algebra, the seventh step follows from i(b), the eighth step follows from triangular inequality, the ninth step follows from i(b), the tenth step follows from i(b) and the last step follows from reorganization.</p><p>Part 5. We have</p><formula xml:id="formula_56">∥g 2 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 3 )•(I +f 2 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+ f 2 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 2 (f 4 (x))+f 3 (f 4 (x))+ f 3 (f 2 (x+f 4 (x)</formula><p>))+f 1 (f 4 (x))+f 1 (f 2 (x+f 4 (x)))+f 1 (f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))))∥ 2 ≤ (ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 3 ϵ 2 +ϵ 3 ϵ 2 ϵ 4 +ϵ 1 ϵ 4 +ϵ 1 ϵ 2 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 +ϵ 1 ϵ 3 ϵ 4 +ϵ 1 ϵ 3 ϵ 2 +ϵ 1 ϵ 3 ϵ 2 ϵ 4 )∥x∥ 2 = (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 , where the first step follows from the definition of g 2 and g 3 , the second step follows from simple algebra, the third step follows from the fact that all f i ,∀i ∈ [4] are linear function, the fourth step follows from triangular inequality and i(b), and the last step follows from reorganization. ))-f 1 (x)∥ 2 ≤ L 2 ϵ 4 ∥x∥ 2 +L 3 ϵ 4 ∥x∥ 2 +L 3 ϵ 2 ∥x+f 4 (x)∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x+f 4 (x)∥ 2 +L 1 ϵ 3 ∥x+f 4 (x)+f 2 (x+f 4 (x))∥ 2 ≤ L 2 ϵ 4 ∥x∥ 2 +L 3 ϵ 4 ∥x∥ 2 +L 3 ϵ 2 ∥x∥ 2 +L 3 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥+L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ϵ 2 ∥x∥ 2 +L 1 ϵ 3 ϵ 2 ϵ 4 ∥x∥ 2 = (L 1 ϵ 2 +L 1 ϵ 3 +L 1 ϵ 4 +L 2 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 3 ϵ 4 +L 3 ϵ 2 ϵ 4 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 where the first step follows from the definition of g 2 and g 3 , the second step follows from simple algebra, the third step follows from reorganization, the fourth step follows from triangular inequality, the fifth step follows from the fact that all f i ,∀i ∈ [4] are Lipschitz functions and i(b), the sixth step follows from triangular inequality, and the last step follows from reorganization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Nearest Neighbor Search Data Structure</head><p>We use the reduction-based approximate MaxIP method with LSH data-structure to achieve sublinear iteration cost. Note that we choose this method due to its clear theoretical guarantee on the retrieval results. It is well-known that an LSH data-structures is used for approximate nearest neighbor problem. The following definition of approximate nearest neighbor search is very standard in literature <ref type="bibr" target="#b11">(Arya &amp; Mount, 1993;</ref><ref type="bibr">Indyk &amp; Motwani, 1998a;</ref><ref type="bibr" target="#b41">Datar et al., 2004;</ref><ref type="bibr" target="#b7">Andoni et al., 2014;</ref><ref type="bibr">2015;</ref><ref type="bibr">Andoni &amp; Razenshteyn, 2015;</ref><ref type="bibr" target="#b73">Indyk &amp; Wagner, 2018;</ref><ref type="bibr" target="#b9">Andoni et al., 2017;</ref><ref type="bibr" target="#b33">2018;</ref><ref type="bibr" target="#b48">Dong et al., 2019;</ref><ref type="bibr">Chen et al., 2020b;</ref><ref type="bibr">Li &amp; Li, 2022;</ref><ref type="bibr" target="#b86">Li et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.1 LSH and MaxIP</head><p>We start with the defining the Approximate Nearest Neighbor (ANN) problem <ref type="bibr" target="#b11">(Arya &amp; Mount, 1993;</ref><ref type="bibr">Indyk &amp; Motwani, 1998a;</ref><ref type="bibr" target="#b41">Datar et al., 2004;</ref><ref type="bibr" target="#b7">Andoni et al., 2014;</ref><ref type="bibr">2015;</ref><ref type="bibr">Andoni &amp; Razenshteyn, 2015;</ref><ref type="bibr" target="#b73">Indyk &amp; Wagner, 2018;</ref><ref type="bibr" target="#b9">Andoni et al., 2017;</ref><ref type="bibr" target="#b33">2018;</ref><ref type="bibr" target="#b48">Dong et al., 2019;</ref><ref type="bibr">Chen et al., 2020b)</ref> as: Definition J.1 (Approximate Nearest Neighbor (ANN)). Let c &gt; 1 and r ∈ (0,2) denote two parameters. Given an n-vector set Y ⊂ S d-1 on a unit sphere, the objective of the (c,r)-Approximate Nearest Neighbor (ANN) is to construct a data structure that, for any query x ∈ S d-1 such that min y∈Y ∥y-x∥ 2 ≤ r, it returns a vector z from Y that satisfies ∥z-x∥ 2 ≤ c•r.</p><p>The ANN problem can be solved via locality sensitive hashing (LSH) <ref type="bibr">(Indyk &amp; Motwani, 1998a;</ref><ref type="bibr" target="#b41">Datar et al., 2004;</ref><ref type="bibr" target="#b73">Indyk &amp; Wagner, 2018)</ref>. In this paper, we use the standard definitions of LSH (see <ref type="bibr">Indyk and Motwani (Indyk &amp; Motwani, 1998a)</ref>).</p><p>Definition J.2 (Locality Sensitive Hashing). Let c &gt; 1 denote a parameter. Let p 1 ,p 2 ∈ (0,1) denote two parameters and p 1 &gt; p 2 . We say a function family H is (r,c • r,p 1 ,p 2 )-sensitive if and only if, for any vectors x,y ∈ R d , for any h chosen uniformly at random from H, we have:</p><p>• if ∥x-y∥ 2 ≤ r, then Pr h∼H [h(x) = h(y)] ≥ p 1 ,</p><p>• if ∥x-y∥ 2 ≥ c•r, then Pr h∼H [h(x) = h(y)] ≤ p 2 .</p><p>Next, we show that LSH solves ANN problem with sublinear query time complexity.</p><p>Theorem J.3 <ref type="bibr" target="#b9">(Andoni, Laarhoven, Razenshteyn and Waingarten (Andoni et al., 2017)</ref>). Let c &gt; 1 and r ∈ (0,2) denote two parameters. One can solve (c,r)-ANN on a unit sphere in query time O(d•n ρ ) using preprocessing time O(dn 1+o(1) ) and space O(n 1+o(1) +dn), where ρ = 2 c 2 -1 c 4 +o(1).</p><p>time complexity. We refer readers to Section 8.2 in <ref type="bibr" target="#b117">(Shrivastava et al., 2021)</ref> for more details<ref type="foot" target="#foot_15">foot_15</ref> . Moreover, Corollary J.9 could be applied to projected MaxIP problem.</p><p>Theorem J.10. Let c ∈ (0,1) and τ ∈ (0,1). Let ϕ,ψ : R d → R k denote two transforms. Let T ϕ denote the time to compute ϕ(x) and T ψ denote the time to compute ψ(y). Given a set of n-points Y ∈ R d with ψ(Y ) ⊂ S k-1 on the sphere, one can construct a data structure with O(dn 1+o(1) + T ψ n) preprocessing time and O(n 1+o(1) + dn) space so that for any query x ∈ R d with ϕ(x) ∈ S k-1 , we take query time complexity O(d•n ρ +T ϕ ) to solve (c,ϕ,ψ,τ )-MaxIP with respect to (x,Y ) with probability at least 0.9, where ρ := 2(1-τ ) 2 (1-cτ ) 2 -(1-τ ) 4 (1-cτ ) 4 +o(1).</p><p>Proof. The preprocessing phase can be decomposed in two parts.</p><p>• It takes O(T ψ n) time to transform every y ∈ Y into ψ(y).</p><p>• It takes O(O(dn 1+o(1) ) time and O(dn 1+o(1) +dn) to index every ψ(y) into LSH using Theorem J.9.</p><p>The query phase can be decomposed in two parts.</p><p>• It takes O(T ϕ ) time to transform every x ∈ R d into ϕ(x).</p><p>• It takes O(d•n ρ ) time perform query for ϕ(x) in LSH using Theorem J.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Self-attention layer as a clustering algorithm</head><p>The self-attention layer in the Transformer looks like mean-shift clustering. Suppose {(x j ,v j )} are a bunch of key and value pairs and q is the query. Note that q = W q x, k = W k x and v = W v x are computed by three projection matrices W k , W q and W v from a common x. Then from self-attention we have:</p><formula xml:id="formula_57">v = j p j v j = j exp(x ⊺ W ⊺ q W k x j )W v x j j exp(x ⊺ W ⊺ q W k x j ) = W v j exp(x ⊺ W ⊺ q W k x j )x j j exp(x ⊺ W ⊺ q W k x j )<label>(14)</label></formula><p>where ∼ (q,k j ) := exp(q ⊺ k j ) = exp(x ⊺ W ⊺ q W k x j ) and p j =∼ (q,k j )/ j ∼ (q,k j ). On the other hand, mean-shift clustering looks like the following: m(x) = j K(x j ,x)x j j K(x j ,x)</p><p>where K(x j ,x) is a kernel matrix that measure the similarity between x j and x. According to the mean-shift algorithm, in the next iteration, we will simply replace x with m(x).</p><p>So in some sense, self-attention is just to do some kind of clustering for the input embedding q and k, plus a transformation of the embedding to another place. The term "projection" is due to the fact that there is a projection matrix W v on x for the next level.</p><p>Residue connection and LayerNorm. Compared to mean-shift, Transformer layer has residue connection. Therefore, for single-headed attention, what you actually get is v+x, followed by a LayerNorm. For the residue connection, the mean-shift analog already shows the output m(x) contains x+ part. The reason why we need residue connection is that the self-attention part might only model the "change" of x in the mean-shift picture, rather than the full update of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L The role of self-attention</head><p>Consider we have a vocabulary of size m and d dimensional embedding space. In practice, many papers in NLP have reported clustering behaviors of word embeddings: such a clustering of word embedding naturally occurs after training.</p><p>An explanation for the above phenomenon is that, by grouping these word embedding together, we might generalize better, since similarity in word now can transfer (e.g., A linked to B, B linked to C, then A might link to C as well) and generalization follows.</p><p>Let's treat it as a fact and focus on how this is achieved and how self-attention plays a role here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Accuracy-Efficiency Trade-offs Figure1. (1) LLMs have up to 85% contextual sparsity for a given input. (2) Contextual sparsity has much better efficiency-accuracy trade-offs (up to 7×) than non-contextual sparsity or static sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Figure2. DEJAVU uses lookahead predictors to side-step prediction costs: given the input to the attention layer at block k, they (asynchronously) predict the contextual sparsity for the MLP at block k, and given the input to the MLP at block k, they predict the sparsity for the attention head at the next layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>Figure 3. In Figure (a), we plot the percentage of not-activated attention heads. By only keeping heads that yield large output norms, we can silence over 80% attention heads for a given token. In Figure (b), we plot the average sparsity we impose on MLP layers.We can zero out over 95% of MLP parameters for a given token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. We visualize the attention scores of three different heads for an exemplary sentence. Head 42 and Head 44 give heavy attention scores on particular tokens while Head 43 is more uniform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Slowly Changing Embedding. Figure (a) shows the median cosine similarity between representations at two consecutive layers across all layers for different OPT models. All models show a similarity greater than 95%. Figure (b) shows cosine similarity stays high even a few layers apart. For the residual connection X ′ = X +F (X) inside each block, we plot the ℓ2 norm of X and F (X) in Figure (c) and Figure (d).∥X∥ is significantly higher than ∥F (X)∥, which explains the slowly changing embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure6. Accuracy Trend for DEJAVU-OPT-175B. This figure shows the accuracy of DEJAVU-OPT-175B on language modeling datasets and downstream tasks when we set different sparsity at test time. In general, DEJAVU-OPT-175B incurs no accuracy drop until 75% sparsity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Figure10. Cosine similarity between X and F (X), and the cosine similarity between X and X ′ in orange color. L2 norm of X and F (X) and X after layer normalization in purple on the right. Except on the first layer, ∥X∥ is significantly higher than ∥F (X)∥. ∥F (X)∥ is higher at the first layer, which corresponds to the low cosine similarity at the first layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Detailed diagram on the sparsified computation process of MLP and Attention. Notation refers to Section 2.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 .</head><label>13</label><figDesc>Figure13. Speed benchmarking of the MLP layer of OPT-175B on 8xA100s. Our sparse implementation is up to 4.5× faster than the baseline implementation in PyTorch. Our sparse MLP implementation remains faster than dense MLP for density up to 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Figure14. Speed benchmarking of the attention layer of OPT-175B on 8xA100s. Our sparse implementation is up to 5× faster than the baseline implementation in PyTorch. Our sparse attention implementation remains faster than dense MLP for density up to 0.8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>I. 1</head><label>1</label><figDesc>Function Approximations for Two Operators Lemma I.1. Let f 1 : R d → R d and let f 2 : R d → R d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Part 6 .</head><label>6</label><figDesc>We have ∥g 2 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 3 )•(I +f 2 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))) +f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 2 (x+f 4 (x))-f 2 (x)+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))-f 3 (x) +f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))))-f 1 (x)∥ 2 ≤ ∥f 2 (x+f 4 (x))-f 2 (x)∥ 2 +∥f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))-f 3 (x)∥ 2 +∥f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Theoretical breakdown for prompting versus token generation (tensor model parallelism on 8 A100-80G GPUs).</figDesc><table><row><cell></cell><cell>TFLOPs</cell><cell>I/O</cell><cell cols="2">Compute Latency (ms) I/O Latency (ms)</cell></row><row><cell>Prompting 128</cell><cell>44.6</cell><cell>330 GB</cell><cell>17.87</cell><cell>20.6</cell></row><row><cell>Token Generation 128</cell><cell>44.6</cell><cell>41 TB</cell><cell>17.87</cell><cell>2600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Theoretical breakdown for Attention block versus MLP block in one transformer layer when generating one token (tensor model parallelism on 8 A100-80G GPUs).</figDesc><table><row><cell></cell><cell cols="4">GFLOPs I/O (GB) Compute Latency (ms) I/O Latency (ms)</cell></row><row><cell>Attention Block</cell><cell>1.21</cell><cell>1.12</cell><cell>0.00048</cell><cell>0.07</cell></row><row><cell>MLP Block</cell><cell>2.41</cell><cell>2.25</cell><cell>0.00096</cell><cell>0.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Latency breakdown of generating 1 token under the setting of batch size 1 and prompt length 128 on 8 A100-80GB.</figDesc><table><row><cell cols="4">All Reduce MLP Block Attention Block (ms) Others</cell></row><row><cell>6 ms</cell><cell>19ms</cell><cell>13ms</cell><cell>2ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Accuracy of zero-shot tasks and language modeling when sparsifying the MLP block and the Attention block separately. The sparsity is set at 85% for MLP-block and 50% for Attention-block. DEJAVU incurs no accuracy drop across the boards.</figDesc><table><row><cell>Model</cell><cell>CB</cell><cell cols="6">COPA Lambada OpenBookQA PIQA RTE Winogrande Wikitext</cell><cell>C4</cell></row><row><cell>OPT-175B</cell><cell cols="2">0.3523 0.86</cell><cell>0.7584</cell><cell>0.446</cell><cell>0.8096 0.6029</cell><cell>0.7261</cell><cell>10.8221 7.7224</cell></row><row><cell>DEJAVU-MLP-OPT-175B</cell><cell cols="2">0.3544 0.85</cell><cell>0.7619</cell><cell>0.446</cell><cell>0.8096 0.6065</cell><cell>0.7206</cell><cell>10.7988 7.7393</cell></row><row><cell cols="3">DEJAVU-Attention-OPT-175B 0.3544 0.86</cell><cell>0.7586</cell><cell>0.4460</cell><cell>0.8063 0.5921</cell><cell>0.7245</cell><cell>10.8696 7.7393</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>DEJAVU-OPT66B on zero-shot downstream task.</figDesc><table><row><cell>Model</cell><cell>CB</cell><cell cols="5">COPA Lambada OpenBookQA PIQA RTE Winogrande</cell></row><row><cell>OPT-66B</cell><cell cols="2">0.3928 0.87</cell><cell>0.7508</cell><cell>0.426</cell><cell>0.7921 0.6028</cell><cell>0.6890</cell></row><row><cell cols="3">DEJAVU-OPT-66B 0.4285 0.87</cell><cell>0.7458</cell><cell>0.434</cell><cell>0.7933 0.5884</cell><cell>0.6898</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>DEJAVU-BLOOM on zero-shot downstream task.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">CB COPA OpenBookQA PIQA RTE Winogrande Lambada</cell></row><row><cell>BLOOM</cell><cell></cell><cell cols="2">0.455 0.8</cell><cell>0448</cell><cell cols="2">0.79 0.617</cell><cell>0.704</cell><cell>0.677</cell></row><row><cell cols="4">Dejavu-BLOOM 0.448 0.8</cell><cell>0.44</cell><cell cols="2">0.787 0.606</cell><cell>0.710</cell><cell>0.675</cell></row><row><cell>Union Contextual Sparsity</cell><cell>0.5 0.6 0.7 0.8 0.9 1.0</cell><cell cols="2">Batch size 2 4 8 16 32</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>20</cell><cell cols="2">40 Transformer Layer 60</cell><cell>80</cell><cell>96</cell></row></table><note><p>Figure 8. Union contextual sparsity with larger batch size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>DEJAVU-OPT-175B with 4-bit quantization.</figDesc><table><row><cell></cell><cell cols="5">CB COPA OpenBookQA PIQA RTE Winogrande Lambada</cell></row><row><cell>OPT-175B</cell><cell>0.352 0.86</cell><cell>0.446</cell><cell>0.809 0.602</cell><cell>0.726</cell><cell>0.758</cell></row><row><cell>Dejavu-OPT-175B</cell><cell>0.402 0.85</cell><cell>0.450</cell><cell>0.802 0.592</cell><cell>0.726</cell><cell>0.753</cell></row><row><cell>OPT-175B + W4A16</cell><cell>0.356 0.85</cell><cell>0.44</cell><cell>0.806 0.574</cell><cell>0.714</cell><cell>0.757</cell></row><row><cell cols="2">Dejavu-OPT-175B + W4A16 0.365 0.86</cell><cell>0.452</cell><cell>0.805 0.592</cell><cell>0.726</cell><cell>0.754</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Sparsify from the Depth: Skipping or parallel entire transformer blocks may not lead to catastrophic drop in accuracy at test time.</figDesc><table><row><cell>Union Contextual Sparsity</cell><cell>0.5 0.6 0.7 0.8 0.9 1.0</cell><cell cols="2">Batch size 2 4 8 16 32</cell><cell></cell><cell></cell><cell></cell><cell>Union Contextual Sparsity</cell><cell>0.4 0.5 0.6 0.7 0.8 0.9</cell><cell cols="2">Batch size 2 4 8 16 32</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>20</cell><cell cols="2">40 Transformer Layer 60</cell><cell>80</cell><cell>96</cell><cell></cell><cell>0</cell><cell>20</cell><cell>40 Transformer Layer 60</cell><cell>80</cell><cell>96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) MLP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Figure 11. Union contextual sparsity with larger batch size.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">OPT-1.3B OPT-1.3B + HNSW</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Hellaswag</cell><cell>0.4154</cell><cell></cell><cell></cell><cell>0.4314</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>C4</cell><cell></cell><cell>14.2</cell><cell></cell><cell></cell><cell>14.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Model</cell><cell cols="6">COPA Hellaswag Lambada OpenBookQA PIQA Winogrande</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">OPT-175B 0.8600</cell><cell>0.7814</cell><cell>0.7584</cell><cell></cell><cell cols="2">0.4460</cell><cell>0.8096</cell><cell>0.7261</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Parallel 2 0.8300</cell><cell>0.7737</cell><cell>0.7762</cell><cell></cell><cell cols="2">0.4520</cell><cell>0.8030</cell><cell>0.7096</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Parallel 4 0.5200</cell><cell>0.2519</cell><cell>0</cell><cell></cell><cell cols="2">0.2720</cell><cell>0.5092</cell><cell>0.4870</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Skip 2/8 0.8000</cell><cell>0.7112</cell><cell>0.6387</cell><cell></cell><cell cols="2">0.4220</cell><cell>0.7840</cell><cell>0.6630</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Skip 2/4 0.6900</cell><cell>0.4409</cell><cell>0.0240</cell><cell></cell><cell cols="2">0.3400</cell><cell>0.6882</cell><cell>0.5383</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Bloom</cell><cell>0.8000</cell><cell>0.7460</cell><cell>0.6771</cell><cell></cell><cell cols="2">0.4480</cell><cell>0.7949</cell><cell>0.7040</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Parallel 2 0.8100</cell><cell>0.7404</cell><cell>0.6992</cell><cell></cell><cell cols="2">0.4360</cell><cell>0.7813</cell><cell>0.7048</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Parallel 4 0.6200</cell><cell>0.3176</cell><cell>0.1325</cell><cell></cell><cell cols="2">0.2720</cell><cell>0.5593</cell><cell>0.5217</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Skip 2/8 0.7900</cell><cell>0.6829</cell><cell>0.5936</cell><cell></cell><cell cols="2">0.4120</cell><cell>0.7699</cell><cell>0.6614</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">-Skip 2/4 0.6600</cell><cell>0.5538</cell><cell>0.3023</cell><cell></cell><cell cols="2">0.3580</cell><cell>0.7046</cell><cell>0.5549</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Setting</cell><cell cols="4">Wiki(ppl) C4(ppl)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Baseline</cell><cell></cell><cell cols="2">11.57</cell><cell>10.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Skip every 2 layers</cell><cell cols="2">21.16</cell><cell>16.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Skip every 4 layers</cell><cell cols="2">13.45</cell><cell>11.37</cell></row></table><note><p>C.3 Future Possibility: Skipping Layer</p><p>Deja Vu currently sparsifies from the perspective of model width. Here, we explore the possibility of sparsification from model depth. As observed in Section 3, we show that the activation of large language models changes slowly across blocks. This property can be leveraged to increase the efficiency of a trained model by parallelizing, reordering, or skipping certain intermediate sub-blocks without significantly impacting the overall accuracy.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Rice University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Zhe Jiang University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Stanford University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>University of California, San Diego</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>ETH Zurich</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Adobe Research</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Meta AI (FAIR)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Carnegie Mellon University. Correspondence to: Zichang Liu &lt;zl71@rice.edu&gt;, Tri Dao &lt;trid@stanford.edu&gt;, Tianyi Zhou &lt;t8zhou@ucsd.edu&gt;, Zhao Song &lt;zsong@adobe.com&gt;, Beidi Chen &lt;beidic@andrew.cmu.edu&gt;.Proceedings of the40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_8"><p>http://github.com/NVIDIA/FasterTransformer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>http://github.com/huggingface/transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_10"><p>We remark that sketching technique has widely applied to many applications such as linear regression, low-rank approximation<ref type="bibr" target="#b36">(Clarkson &amp; Woodruff, 2013;</ref><ref type="bibr" target="#b104">Nelson &amp; Nguyên, 2013;</ref><ref type="bibr" target="#b94">Lu et al., 2013;</ref><ref type="bibr" target="#b20">Boutsidis et al., 2016;</ref><ref type="bibr" target="#b37">Cohen, 2016;</ref><ref type="bibr" target="#b114">Razenshteyn et al., 2016;</ref><ref type="bibr" target="#b122">Song et al., 2017;</ref><ref type="bibr" target="#b0">2019)</ref>, linear programming<ref type="bibr" target="#b121">(Song &amp; Yu, 2021;</ref><ref type="bibr" target="#b47">Dong et al., 2021;</ref><ref type="bibr" target="#b76">Jiang et al., 2021;</ref><ref type="bibr" target="#b60">Gu &amp; Song, 2022)</ref>, semi-definite programming<ref type="bibr" target="#b60">(Gu &amp; Song, 2022;</ref><ref type="bibr" target="#b90">Song et al., 2023b)</ref>, empirical risk minimization<ref type="bibr" target="#b85">(Lee et al., 2019;</ref> Qin et al., 2023b), training over-parameterized neural network<ref type="bibr" target="#b22">(Brand et al., 2021;</ref> Song et al., 2021;<ref type="bibr" target="#b3">Alman et al., 2022;</ref><ref type="bibr" target="#b70">Hu et al., 2022;</ref><ref type="bibr" target="#b142">Zhang, 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_11"><p>In this case, we require logn o be an integer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_12"><p>For our purposes the signs need only be O(logd)-wise independent, and each column can be specified by a O(logd)-wise independent permutation, and the seeds specifying the permutations in different columns need only be O(logd)-wise independent.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_13"><p>This definition has the same behavior as sparse embedding matrix I for our purpose</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_14"><p>It is obvious to boost probability from constant to δ by repeating the data structure log(1/δ) times.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_15"><p>Recently, there a line of work that use fast MaxIP data structure to speedup the iterative-type optimization algorithms<ref type="bibr" target="#b117">(Shrivastava et al., 2021;</ref><ref type="bibr" target="#b120">Song &amp; Ye, 2023;</ref> Qin et al., 2023a; Song et al., 2023a).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Ryan Spring</rs>, <rs type="person">Laurel Orr</rs>, <rs type="person">Guangxuan Xiao</rs>, <rs type="person">Eric Han</rs>, <rs type="person">Xun Huang</rs>, <rs type="person">Daniel Y. Fu</rs>, <rs type="person">Benjamin Spector</rs>, <rs type="person">Ruan Silva</rs>, <rs type="person">Diana Liskovich</rs>, and the anonymous reviewers for helpful discussions and feedback. We acknowledge the generous support by <rs type="person">Together Computer</rs>, which enabled the necessary partial computations in this work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>X and F (X), which is close to 0.0. This happens because ∥X∥ is significantly greater than ∥F (X)∥, shown in the purple.</p><p>In the first layer, ∥F (X)∥ is larger, which explains the low cosine similarity. The magnitude of the L2 norm is different across models, however, we observe a similar trend with models of different sizes. There exists a normalization layer before F (X) and the layer normalization scale ∥X∥ to a consistent magnitude across layers (e.g. 85 for OPT-30B, 110 for OPT175B), but not necessarily scale down ∥X∥.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiment Detail C.1 Large Batch Size</head><p>To help understand where the speed-up comes from when batch size is greater than 1, we present the Union Contextual Sparsity (fraction of neurons/heads that are not used by any of the inputs in the batch) of different batches sizes for MLP and Attention blocks, respectively, in Figure <ref type="figure">11</ref>. Union Contextual Sparsity is calculated as 1.0 -the union of activated MLP neurons or Attention heads in the batch / total neurons or heads. The union operation is essential to realize a fast sparse GEMM.</p><p>Surprisingly the number of MLP neurons/Attention heads that DEJAVU activated does not grow linearly with the batch size. This suggests a power law distribution rather than a uniform distribution of parameter access from all input examples. Further, a larger batch size can easily lead to out-of-memory for long sequence settings due to the limited GPU memory, the giant large model size, and the stored KV cache. For example, the total GPU memory of 8 80GB A100 is 640GB. Model parameters are around 350GB for OPT175B. The KV cache for a batch size 32 with a sequence longer than 1920 tokens has already filled up the GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Near Neighbor classifier</head><p>In the DEJAVU framework, any near-neighbor search method under the inner product metric would be sufficient to predict a sparsity pattern. "Training predictor" is to reduce the cost of on-the-fly prediction, rather than training the model itself.</p><p>holds with probability 1-δ.</p><p>Rescaling the ϵ, we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Folded Gaussian Distribution</head><p>We state a standard tool from literature, Lemma G.6 (Lemma 1 on page 1325 of <ref type="bibr" target="#b81">Laurent and Massart (Laurent &amp; Massart, 2000)</ref>). Let X ∼ X 2 k be a chi-squared distributed random variable with k degrees of freedom. Each one has zero means and σ 2 variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then,</head><p>Pr[X -kσ 2 ≥ (2</p><p>We prove the following property, Fact G.7. Let h,q ∈ R p be fixed vectors and h ̸ = 0,W ∈ R m×p be random matrix with i.i.d. entries W i,j ∼ N (0, 2 m ), and vector v ∈ R m defined as</p><p>• |v i | follows i.i.d. from the following distribution: with half probability |v i | = 0, and with the other half probability |v i | follows from folded Gaussian distributions |N (0, 2∥h∥ 2 m )|.</p><p>• m∥v∥ 2 2∥h∥ 2 is in distribution identical to χ 2 ω (chi-square distribution of order ω ) where ω follows from binomial distribution B(m,1/2).</p><p>Proof. We assume each vector W i is generated by first generating a gaussian vector g ∼ N (0, 2I m ) and then setting W i = ±g where the sign is chosen with half-half probability. Now, |⟨W i ,h⟩| = |⟨g,h⟩| only depends on g, and is in distribution identical to |N (0, 2∥h∥ 2 m )|. Next, after the sign is determined, the indicator 1 ⟨Wi,h+q⟩≥0 is 1 with half probability and 0 with another half. Therefore, |v i | satisfies the aforementioned distribution. As for ∥v∥ 2 , letting ω ∈ {0,1,...,m} be the variable indicator how many indicators are 1 , then ω ∼ B(m,1/2) and m∥v∥ 2 2∥h∥ 2 ∼ χ 2 ω .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 ℓ 2 subspace embedding</head><p>We define a standard notion in sketching technique. 3   Definition G.8 (ℓ 2 subspace embedding <ref type="bibr" target="#b115">(Sarlos, 2006)</ref>). A (ϵ,δ,ℓ 2 )-subspace embedding for the column space of an n×d matrix A is a matrix S for which</p><p>where the U is the orthonormal basis of A.</p><p>For the reason of above conditions are equivalent, we refer the readers to the survey <ref type="bibr" target="#b135">(Woodruff, 2014)</ref>.</p><p>We state a standard tool in literature, Lemma G.9 (Lemma 5 in <ref type="bibr" target="#b135">(Woodruff, 2014)</ref>). Let X ⊂ R d be rank k. For any γ ∈ (0,1), there is a γ-net N of X for which</p><p>where the first step follows from Y ⊥ Y ⊤ ⊥ +Y Y ⊤ = I, the second step follows from simple algebra, and the last step follows from X is an orthogonal matrix, so X ⊤ = X -1 . This means that (Y ⊤ X) ⊥ = Y ⊤ ⊥ X. Lemma H.3 (Orthogonal and inverse share singular vectors). Let A ∈ R k×k be non-singular, then A ⊥ and A -1 have the same set of singular vectors. Consequently,</p><p>be the unit eigenvector of A that realizes the spectral norm, note that ∥A ⊥ x∥ 2 2 = 1-∥A∥ 2 , we argue that x corresponds to the smallest singular value of A ⊥ via contradiction. Suppose there exists some unit vector y with ∥A ⊥ y∥ 2 &lt; ∥A ⊥ x∥ 2 , by definition, we know that ∥A ⊥ y∥ 2 2 + ∥Ay∥ 2 2 = 1, this means that ∥Ay∥ 2 &gt; ∥Ax∥ 2 = ∥A∥, contradicts the definition of spectral norm. Similarly, if z is the unit vector that realizes the spectral norm of A ⊥ , then it is also singular vector corresponds to the smallest singular value of A, or equivalently, the spectral norm of A -1 . Our above argument essentially implies that A ⊥ and A -1 have the same set of singular vectors. The proof is then straightforward: suppose</p><p>= λµz, where the first step follows from our assumption, the second step follows from µ is a real number and a real number multiplying a matrix is commutative and follows from the associative property, and the third step follows from our assumption. Thus, we have ∥A ⊥ A -1 ∥ = ∥A ⊥ ∥∥A -1 ∥, and we have proved the assertion.</p><p>The proof then follows straightforwardly from Lemma H.3.</p><p>by Lemma H.3, we know that A ⊥ and A -1 have the same singular vectors, or equivalently, the singular vector realizing ∥A ⊥ ∥ corresponds to the smallest singular value of A. Let z ∈ R k be the unit singular vector with singular value ∥A ⊥ ∥, then</p><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Angle is close</head><p>Lemma H.6. Let ϵ ∈ (0,0.1) Let x denote a unit vector, i.e., ∥x∥ 2 = 1.</p><p>Proof. We have ∥x+y∥ 2 ≥ ∥x∥ 2 -∥y∥ 2 ≥ 1-ϵ where the first step follows from triangle inequality.</p><p>Here we write o(1) is equivalent to O(1/ √ logn). Note that we could reduce d to n o(1) with Johnson-Lindenstrauss Lemma <ref type="bibr" target="#b78">(Johnson &amp; Lindenstrauss, 1984)</ref>. Besides, we could achieve better ρ using LSH in <ref type="bibr">(Andoni &amp; Razenshteyn, 2015)</ref> if we allowed to have more proprocessing time.</p><p>In this work, we focus on a well-known problem in computational complexity: approximate MaxIP. In this work, we follow the standard notation in <ref type="bibr" target="#b33">(Chen, 2018</ref>) and define the approximate MaxIP problem as follows:</p><p>Definition J.4 (Approximate MaxIP). Let c ∈ (0,1) and τ ∈ (0,1) denote two parameters. Given an n-vector dataset Y ⊂ S d-1 on a unit sphere, the objective of the (c,τ )-MaxIP is to construct a data structure that, given a query x ∈ S d-1 such that max y∈Y ⟨x,y⟩ ≥ τ , it retrieves a vector z from Y that satisfies ⟨x,z⟩ ≥ c•max y∈Y ⟨x,y⟩.</p><p>In many applications, it is more convenient to doing inner product search in a transformed/projected space compared to doing inner product search in the original space. Thus, we propose the following definitions (Definition J.5 and Definition J.6)</p><p>, the goal of the (c,ϕ,ψ,τ )-MaxIP is to construct a data structure that, given a query x ∈ R d and ϕ(x) ∈ S k-1 such that max y∈Y ⟨ϕ(x),ψ(y)⟩ ≥ τ , it retrieves a vector z ∈ Y that satisfies ⟨ϕ(x),ψ(z)⟩ ≥ c•(ϕ,ψ)-MaxIP(x,Y ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Connections</head><p>Fact J.7. Let x denote the vector that ⟨ x,x⟩ ≥ 1-1 2 ϵ 2 , where both x and x are unit vectors. We have</p><p>= ϵ Now, we complete the proof.</p><p>Lemma J.8. Let x denote the vector that ⟨ x,x⟩ ≥ 1-1 2 ϵ 2 , where both x and x are unit vectors. Let 0.01c•τ &gt; ϵ. Suppose there is a z ∈ Y , where ∥z∥ 2 = 1, such that ⟨x,z⟩ ≥ c•max y∈Y ⟨x,y⟩</p><p>Note that max y∈Y ⟨x,y⟩ ≥ τ . Then, we can find a z ∈ Y such that</p><p>where the first step follows from simple algebra, the second step follows from the fact that ⟨x, y⟩ ≥ -|⟨x, y⟩|, the third step follows from the property of inner product, the fourth step follows from Fact J.7, the fifth step follows from ⟨x,z⟩ ≥ c•max y∈Y ⟨x,y⟩ and the final step follows from the fact that 0.01c•τ &gt; ϵ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.3 Efficient Transformations</head><p>We have learned from that (c,τ )-MaxIP on a unit sphere S d-1 using LSH for ANN. Therefore, the next step is to transform the direction search procedure in iterative optimization algorithm into a MaxIP on a unit sphere. To achieve this, we formulate the direction search as a projected approximate MaxIP (see Definition J.5). We start with presenting a pair of transformation ϕ 0 ,ψ 0 : R d → R d+1 such that, given a function g : R d → R, for any x,y in a convex set K, we have</p><p>In this way, we show that ⟨y-x,∇g(x)⟩ = -⟨ϕ 0 (x),ψ 0 (y)⟩, argmin Next, we present a standard transformations <ref type="bibr" target="#b105">(Neyshabur &amp; Srebro, 2015)</ref> that connects the MaxIP to ANN in unit sphere.</p><p>For any x,y ∈ R d , we propose transformation ϕ 1 ,ψ 1 : R d → R d+2 such that</p><p>Here D x , D y are some constant that make sure both x/D x and y/D y have norms less than 1. Under these transformations, both ϕ 1 (x) and ψ 1 (y) have norm 1 and argmax y∈Y ⟨ϕ 1 (x),ψ 1 (y)⟩ = argmax y∈Y ⟨x,y⟩.</p><p>Combining transformations in Eq. ( <ref type="formula">11</ref>) and Eq. ( <ref type="formula">13</ref>), we obtain query transform ϕ : R d → R d+3 with form ϕ(x) = ϕ 1 (ϕ 0 (x)) and data transform ϕ : R d → R d+3 with form ψ(y) = ψ 1 (ψ 0 (y)). Using ϕ and ψ, we transform the direction search problem in optimization into a MaxIP in unit sphere. Moreover, given a set Y ⊂ R d and a query x ∈ R d , the solution z of (c,ϕ,ψ,τ )-MaxIP over (x,Y ) has the propriety that ⟨z-x,∇g(x)⟩ ≤ c•min y∈Y ⟨y-x,∇g(x)⟩. Thus, we could approximate the direction search with LSH based MaxIP data-structure.</p><p>Note that only MaxIP problem with positive inner product values could be solved by LSH. We found the direction search problem naturally satisfies this condition. We show that if g is convex, given a set S ⊂ R d , we have min s∈S ⟨∇g(x),s-x⟩ ≤ 0 for any x ∈ B(S), where B is the convex hull of S. Thus, max y∈Y ⟨ϕ 0 (x),ψ 0 (y)⟩ is non-negative following Eq. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.4 Data Structures</head><p>In this section, we present a formal statement that solves (c,τ )-MaxIP problem on unit sphere using LSH for (c,r)-ANN.</p><p>Theorem J.9. Let c ∈ (0,1) and τ ∈ (0,1). Given a set of n-vector set Y ⊂ S d-1 on the unit sphere, there exists a data structure with O(dn 1+o(1) ) preprocessing time and O(n 1+o(1) +dn) space so that for any query x ∈ S d-1 , we take O(d•n ρ ) query time to retrieve the (c,τ )-MaxIP of x in Y with probability at least 0.9 7 , where ρ :</p><p>Proof. We know that ∥x-y∥ 2 2 = 2-2⟨x,y⟩ for all x,y ∈ S d-1 . In this way, if we have a LSH data-structure for (c,r)-ANN. It could be used to solve (c,τ )-MaxIP with τ = 1-0.5r 2 and c = 1-0.5c 2 r 2 1-0.5r 2 . Next, we write c 2 as</p><p>Next, we show that if the LSH is initialized following Theorem J.3, it takes query time O(d•n ρ ), space O(n 1+o(1) +dn) and preprocessing time O(dn 1+o(1) ) to solve (c,τ )-MaxIP through solving (c,r)-ANN, where</p><p>In practice, c is increasing as we set parameter τ close to MaxIP(x,Y ). There is also another LSH data structure <ref type="bibr">(Andoni &amp; Razenshteyn, 2015)</ref> with longer preprocessing time and larger space that could solve the (c,τ )-MaxIP with similar query</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.1 The capacity of embedding layer</head><p>First let us take a look at the following pairwise distance constraints between word embedding (e.g., some words should be close to each other, some should be far away from each other) as the following:</p><p>where D(i,j) is large for i and j that should be far apart and D(i,j) is small for i and j that are close to each other. In visualization, this is called Multidimensional Scaling (MDS) <ref type="bibr" target="#b39">(Cox &amp; Cox, 2008)</ref>.</p><p>Note that in neural network training, the constraint (Eqn. 16) is not directly enforced during training, but the clustering naturally happens. Since we talk about capacity, how we achieve Eqn. 16 doesn't matter for now.</p><p>In general we cannot find a fixed low-dimensional embedding (d ≪ m) to satisfy these constraints, since we only have md parameters (m vectors, each has d entries), but m 2 constraint. So two vectors that are supposed to be close may not be close enough (but hopefully they remain close to each other).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.2 The role of self-attention</head><p>For this, the self-attention mechanism comes to the rescue, trading model-size with additional computation. It fulfills what (static) embedding cannot achieve: to further group the embedding vectors together in a multi-layer structure.</p><p>Note that one sentence never covers all d vocabularies. Once the words in the sentence are picked, they are grouped together via self-attention layers to collectively represent a concept that can be useful for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.3 How the clustering happens through self-attention?</head><p>Now one fundamental questions arise: How the static clustering of embedding happens during end-to-end training? In practice, no one explicitly enforces the MDS constraint (Eqn. 16).</p><p>Let's start with a simple example. we have two unit embedding: x and y with the normalization condition that ∥x∥ 2 = 1 and ∥y∥ 2 = 1, and a simple self-attention layer (without projection) which output z:</p><p>Where the attention map is:</p><p>Note that here we attend to x so 0 &lt; p &lt; 1/2 always. The last two is due to normalization condition. Now we consider a loss function L = -1 2 ∥z∥ 2 2 . The intuition behind is that "for some reason, we found that z is a good representation for our task, and want to make sure its length is as long as possible".</p><p>Under this context, what would be the gradient rule for x and y? Will they cluster together?</p><p>The answer is yes! We could compute</p><p>Let t := 1-x ⊺ y and define the following function with respect to t:</p><p>Therefore, we can compute the gradient for x and gradient for y:</p><p>Note that since x and y are kept to be normalized, the term (1-p) 2 x in ∂L/∂x is gone (and similarly p 2 y for g y ). So how x and y move depends on the sign of 1-f (t).</p><p>With some computation, we could see 0 &lt; f (t) &lt; 1 when t &lt; 1.5424. In summary, if x ⊺ y &gt; -0.4576, then the (negative) gradient of x pushes it towards y and pushes x towards y, and the clustering of static embedding happens during training. Note that since both x and y are normalized, -1 ≤ x ⊺ y ≤ 1, so this is a quite loose condition and can be easily satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.4 Multiple embeddings</head><p>People might wonder what happen to multiple unit embeddings x,y 1 ,y 2 ,...,y K ? In this case, we can similarly define self-attention probability p i (note that here we consider the case that every embedding attends to x):</p><p>Define p S := K i=1 p i = 1-1 1+ j e x ⊺ y j &lt; 1 and we have:</p><p>Let pi := p i /p S be the (normalized) probability on y i and ȳ := 1 p S i p i y i = i pi y i be the weighted mean of {y i } other than x, then we have: </p><p>After some manipulation, we have:</p><p>where Q := j pj (y j -ȳ)(y j -ȳ) ⊺ is the weighted covariance matrix of data points {y j }.</p><p>Similar to the two unit case, we want to check -g x to see how the embedding x changes over time.</p><p>-g</p><p>= (1-p S ) 2 x+p S (1-2p S )x ⊺ ȳ-(1-p S )+p S ∥ ȳ∥ 2 ȳ+p S Qz If things are already quite clustered, then ∥ ȳ∥ ≈ 1 (usually ∥ ȳ∥ 2 &lt; 1 since sphere is a convex set), Qz ≈ 0 (since Q spans on the tangent space of z at the sphere and z is perpendicular to it), and we have:</p><p>-</p><p>which is high likely for large K, then -g x has positive component of ȳ and x will move towards ȳ.</p><p>On the other hand, we could also check</p><p>which gives an expression of -g y : • (35) With the same argument, it moves towards ȳ (so all y i will cluster together) and towards x.</p><p>When there is a W k and W q before the embedding, following the same logic, only the column subspace of W k (or W q ) will be clustered together. On the other hand, the value part will be different in order to enable encoding of more complicated concepts based on co-occurrence of multiple tokens.</p><p>M Link self-attention with generative models.</p><p>Consider the following self-attention structure. Consider an embedding matrix X ∈ R n×d and for embedding x i and x j , let</p><p>Here ϕ(x i ;x j ) := x i +β ij (x j -x i ) is the self-attention operation. More properties of this operator ϕ need to be explored. Then we want to maximize the following objective: max</p><p>or more formally, using a softmax to avoid trivial solution x i ≡ x, we have:</p><p>We can compute its gradient update. Here we assume the index k never appears in index i and j (encoding and decoding matrices are decoupled), then by gradient rule, we have:</p><p>where P ⊥ x k is the projection matrix that projects a vector to the orthogonal complement space of x k . The projection is due to the constraint ∥x k ∥ 2 = 1. If the training converges ( ẋk = 0), then we know that ij P(k|i,j)(1-δ ijk )y ij = γx k (41)</p><p>for some γ &gt; 0 (note that γ &lt; 0 will be an unstable stationary point).</p><p>Depending on different structure of the generative model specified by P (k|i,j), we might end up learning different embedding matrix X.</p><p>The first thing we want to check is independency. Assume that for some specific token k and i, we have P(k|i,j) = P(k|i) for any j, which means that the frequency of token k has nothing to do with the second entry j. Furthermore, token k is not connected with other token i ′ ̸ = i, i.e, P(k|i ′ ,j) ≡ 0. If we just let δ ijk = δ &gt; 0, then we have:</p><p>which yields</p><p>And we could possibly show that j β ij (x j -x i ) ≈ 0 since β ij = 1/(1+e 1-x ⊺ i xj ) applies equal weights for embeddings around x i and they cancel out. Therefore, x k is aligned with x i .</p><p>Another thing we might want to check is identification of two tokens. Assume that there exists two tokens j 1 and j 2 and specific k and i, so that P(k|i,j 1 ) = P(k|i,j 2 ). For other k,i,j combination P(k|i,j) ≡ 0, then we have: P(k|i,j 1 )y ij1 = γ 1 x k (44) (not sure how to continue).</p><p>If we have W q , W k and W v , then the formulation doesn't change that much. The only difference here is that now</p><p>and y ⊺ ij x k now becomes y ⊺ ij W v x k .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><surname>Winogrande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What can resnet learn efficiently, going beyond kernels?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Alman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13214</idno>
		<title level="m">Fast attention requires bounded entries</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bypass exponential time preprocessing: Fast neural network training via weight-data correlation preprocessing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14227</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The space complexity of approximating the frequency moments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-eighth annual ACM symposium on Theory of computing</title>
		<meeting>the twenty-eighth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="646" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal data-dependent hashing for approximate near neighbors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-seventh annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the forty-seventh annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond locality-sensitive hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1018" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical and optimal lsh for angular distance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laarhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1225" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal hashing-based time-space trade-offs for approximate near neighbors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laarhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Waingarten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<meeting>the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="47" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Approximate nearest neighbor search in high dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09823</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor queries in fixed dimensions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-D</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dingliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bodapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09095</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting resnets: Improved training and scaling strategies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22614" to="22627" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Piqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GPT-NeoX-20B: An open-source autoregressive language model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weinbach</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.06745" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models</title>
		<meeting>the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimal principal component analysis in distributed and streaming models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC&apos;16-Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Off the beaten path: Let&apos;s replace term-based retrieval with k-nn search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international on conference on information and knowledge management (CIKM)</title>
		<meeting>the 25th ACM international on conference on information and knowledge management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training (overparametrized) neural networks in near-linear time</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V D</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITCS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Algorithm and hardness for dynamic attention maintenance in large language models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V D</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02207</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Data distributional properties drive emergent in-context learning in transformers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pre-training tasks for embedding-based large-scale retrieval</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03932</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate stochastic gradient estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Farwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="291" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scatterbrain: Unifying sparse and low-rank attention</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Winsor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17413" to="17426" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mongoose: A learnable lsh framework for efficient neural network training</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">{SANNS}: Scaling up secure approximate k-nearest neighbors search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chillotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Poburinnaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Riazi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2111" to="2128" />
		</imprint>
	</monogr>
	<note>In 29th {USENIX} Security Symposium ({USENIX} Security 20</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the hardness of approximate and exact (bichromatic) maximum inner product</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd Computational Complexity Conference (CCC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Low-rank approximation and regression in input sparsity time</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nearly tight oblivious subspace embeddings by trace inequalities</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">CUDA Programming: A Developer&apos;s Guide to Parallel Computing with GPUs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Handbook of data visualization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="315" to="347" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>Multidimensional scaling</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flashattention: Fast and memory-efficient exact attention with io-awareness</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual symposium on Computational geometry (SoCG)</title>
		<meeting>the twentieth annual symposium on Computational geometry (SoCG)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tonhauser</surname></persName>
		</author>
		<title level="m">The commitmentbank: Investigating projection in naturally occurring discourse</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Attention scheme inspired softmax regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10411</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>arxiv preprint: arxiv 2304.03426, 2023b</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mean shift clustering</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Llm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
		<title level="m">-bit matrix multiplication for transformers at scale</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>int8 (</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A nearly-linear time algorithm for linear programs with small treewidth: A multiscale representation of robust central path</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 53rd Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1784" to="1797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning space partitions for nearest neighbor search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Turbotransformers: an efficient gpu serving system for transformer models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="389" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Massive language models can be accurately pruned in one-shot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gptq: Accurate post-training quantization for generative pretrained transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17323</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Algorithm-dependent generalization bounds for overparameterized deep residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5371628" />
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">An over-parameterized exponential regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16504</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Differentially private attention computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04701</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W07-1401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vldb</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roemmele</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/S12-1052" />
	</analytic>
	<monogr>
		<title level="m">SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montréal, Canada, 7-8</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06">June 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings of the main conference and the shared task Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.06033</idno>
		<title level="m">A faster small treewidth sdp solver</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Low rank matrix completion via robust alternating minimization in nearly linear time</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11068</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fast and accurate maximum inner product recommendations on map-reduce</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web (WWW)</title>
		<meeting>the 24th International Conference on World Wide Web (WWW)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1263" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">How to access global memory efficiently in CUDA C/C++ kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NVIDIA</title>
		<imprint>
			<date type="published" when="2013-01">Jan, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">241</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The hardware lottery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hooker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="58" to="65" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Training overparametrized neural networks in sublinear time</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04508</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC)</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing (STOC)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirtieth annual ACM symposium on Theory of computing</title>
		<meeting>the thirtieth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors in limited space</title>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference On Learning Theory</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2012" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Data movement is all you need: A case study on optimizing transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="711" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmeticonly inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A faster algorithm for solving general lps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 53rd Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="823" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Inducing and exploiting activation sparsity for fast inference on deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kopinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gelashvili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matveev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/kurtz20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Adaptive estimation of a quadratic functional by model selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Reasoning in vector space: An exploratory study of question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Snip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02340</idno>
		<title level="m">Single-shot network pruning based on connection sensitivity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Solving empirical risk minimization in the current matrix multiplication time</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2140" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Re-randomized densification for one permutation hashing and bin-wise consistent weighted sampling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">The closeness of in-context learning and weight shifting for softmax regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Improving minwise hashing with circulant permutation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>C-Minhash ; Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v162/li22m.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Large models are parsimonious learners: Activation sparsity in trained transformers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.06313" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Solving regularized exp, cosh and sinh regression problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2303.15725, 2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soylu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09110</idno>
		<title level="m">Holistic evaluation of language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05270</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Halos: Hashing large output space for cheap inference</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="110" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Faster ridge regression via the subsampled randomized hadamard transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="369" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor algorithm based on navigable small world graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Logvinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="61" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Yashunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="824" to="836" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fifth annual ACM symposium on Theory of computing</title>
		<meeting>the forty-fifth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Are sixteen heads really better than one? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12837</idno>
		<title level="m">Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient inference</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06440</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Baalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Nguyên</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 ieee 54th annual symposium on foundations of computer science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">On symmetric and asymmetric lshs for inner product search</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1926" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Fastertransformer</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/FasterTransformer" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html" />
		<title level="m">Gpu performance background user&apos;s guide</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09557</idno>
		<title level="m">Quantized matmul for efficient inference of large-scale generative language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05102</idno>
		<title level="m">Efficiently scaling transformer inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Fast submodular function maximization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2305.08367</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In AISTATS, 2023b</note>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Weighted low rank approximations with provable guarantees</title>
		<author>
			<persName><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-eighth annual ACM symposium on Theory of Computing</title>
		<meeting>the forty-eighth annual ACM symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="250" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Improved approximation algorithms for large matrices via random projections</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 47th annual IEEE symposium on foundations of computer science (FOCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Real-time open-domain question answering with dense-sparse phrase index</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4430" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Sublinear least-squares value iteration via locality sensitive hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08285</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A study of branch prediction strategies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25 years of the international symposia on Computer architecture</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="202" to="215" />
		</imprint>
	</monogr>
	<note>selected papers</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Subspace embeddings for the l1-norm with applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-third annual ACM symposium on Theory of computing</title>
		<meeting>the forty-third annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="755" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Efficient asynchronize stochastic gradient algorithm with structured data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<idno>CoRR, abs/2305.08001</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Oblivious sketching-based central path method for linear programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9835" to="9847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Low rank approximation with entrywise l1-norm error</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 49th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="688" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Relative error tensor low rank approximation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<meeting>the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2772" to="2789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Training multi-layer over-parametrized neural network in subquadratic time</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07628</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Fast and efficient matching algorithm with deadline instances</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<idno>CoRR, abs/2305.08353</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Sketching meets differential privacy: fast algorithm for dynamic kronecker projection maintenance</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Distilling task-specific knowledge from bert into simple neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Triton: an intermediate language and compiler for tiled neural network computations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Residual networks behave like ensembles of relatively shallow networks. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Tight bounds for lp oblivious subspace embeddings</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Lightseq: A high performance inference library for transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies: Industry Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Sketching as a tool for numerical linear algebra</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="157" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Accurate and efficient post-training quantization for large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Smoothquant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.10438</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=RdJVFCHjUMI" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zeroquant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01861</idno>
		<title level="m">Efficient and affordable post-training quantization for large-scale transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">A distributed serving system for {Transformer-Based} generative models</title>
		<author>
			<persName><forename type="first">G.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><surname>Orca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="521" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zandieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daliri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
		<author>
			<persName><surname>Kdeformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.02451</idno>
		<title level="m">Accelerating transformers via kernel density estimation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Speeding up optimizations via data structures: Faster search, sample and maintenance. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Navigating with graph representations for fast and scalable decoding of neural language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dotzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
