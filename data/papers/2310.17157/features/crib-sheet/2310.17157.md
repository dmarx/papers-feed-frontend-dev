- **Contextual Sparsity Definition**: Contextual sparsity refers to small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input.

- **DEJAVU System Overview**: DEJAVU is a system designed to exploit contextual sparsity to enhance LLM inference efficiency without compromising quality or in-context learning abilities.

- **Key Hypothesis**: Contextual sparsity exists in pre-trained LLMs, allowing for significant parameter reduction (up to 85% structured sparsity) while maintaining performance.

- **Challenges Addressed**:
  - **Existence**: Verification of contextual sparsity through empirical testing on models like OPT-175B.
  - **Prediction**: Development of a low-cost algorithm to predict sparsity on-the-fly.
  - **Efficiency**: Implementation of an asynchronous lookahead predictor to minimize latency overhead.

- **Sparsified MLP Computation**: 
  \[
  MLP_{S_M}(y) = \sigma(yW_1 S_M)(W_2 S_M)^\top
  \]
  where \( S_M \subset [4d] \) is the set of neurons used for computation.

- **Sparsified Attention Computation**:
  \[
  MHA_{S_A}(y) = \sum_{i \in S_A} H_i(y) \cdot W_O^i
  \]
  where \( H_i(y) \) is defined as:
  \[
  H_i(y) := D_i(y)^{-1} \exp(yW_Q^i(W_K^i)^\top X^\top)XW_V^i
  \]
  and \( D_i(y) := \exp(yW_Q^i(W_K^i)^\top X^\top)1_n \).

- **Asynchronous Lookahead Predictor**: Utilizes the similarity between layer parameters and outputs from previous layers to predict sparsity efficiently, reducing sequential overhead.

- **Empirical Results**: DEJAVU can reduce latency of models like OPT-175B by over 2× compared to existing implementations without quality degradation.

- **Contextual Sparsity Verification Method**: Two forward passes are used to identify and confirm the subset of parameters that yield large output norms, demonstrating the effectiveness of contextual sparsity.

- **Performance Metrics**: Achieved up to 80% sparsity in attention heads and 95% in MLP neurons, leading to potential 7× speedup in inference time.

- **Related Techniques**: Discusses the relationship and differences between quantization, pruning, and distillation in the context of LLM inference efficiency.

- **Latency Breakdown**: Highlights that the token generation phase dominates inference time, with attention and MLP being the primary bottlenecks.

- **Theoretical Guarantees**: Justifies the cross-layer design of the asynchronous predictor for accurate sparsity prediction.

- **Compatibility with Quantization**: DEJAVU's design is shown to be compatible with existing quantization techniques, enhancing its applicability in real-world scenarios.