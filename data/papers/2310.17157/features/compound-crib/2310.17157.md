The DEJAVU system and its underlying principles are designed to address the computational inefficiencies associated with large language models (LLMs) during inference. Below is a detailed technical explanation of the researchers' decisions regarding contextual sparsity, the DEJAVU system, and the challenges it addresses.

### Contextual Sparsity Definition
**Contextual sparsity** is defined as the phenomenon where, for a given input, only a small, input-dependent subset of attention heads and multi-layer perceptron (MLP) parameters is necessary to produce outputs that are approximately equivalent to those generated by the full model. This definition is crucial because it allows for significant reductions in the number of parameters used during inference without sacrificing the model's performance. The researchers hypothesize that this sparsity is inherent in pre-trained LLMs, which have been trained on diverse datasets and can generalize well to various inputs.

### DEJAVU System Overview
The **DEJAVU system** is built to leverage contextual sparsity to enhance the efficiency of LLM inference. By dynamically selecting which parameters to activate based on the input, DEJAVU aims to reduce computational overhead while maintaining the model's quality and in-context learning capabilities. The system employs a low-cost algorithm to predict which parameters are necessary for a given input, thus allowing for real-time adjustments during inference.

### Key Hypothesis
The researchers' **key hypothesis** posits that contextual sparsity exists in pre-trained LLMs, enabling a structured sparsity reduction of up to 85% while preserving performance. This hypothesis is grounded in empirical observations that suggest that not all parameters contribute equally to the output for every input, allowing for selective activation of only the most relevant parameters.

### Challenges Addressed
1. **Existence**: The researchers verified the existence of contextual sparsity through empirical testing on models like OPT-175B. They conducted two forward passes: the first to identify which parameters yield significant output norms, and the second to confirm that using only these parameters produces similar outputs.
  
2. **Prediction**: A low-cost algorithm was developed to predict the necessary sparsity on-the-fly. This algorithm analyzes the input and determines which parameters are likely to be relevant, thus enabling efficient computation without pre-training or extensive retraining.

3. **Efficiency**: The implementation of an **asynchronous lookahead predictor** minimizes latency overhead by predicting the required parameters for the next layer based on the current input. This approach takes advantage of the slow-changing nature of token embeddings across layers, allowing for efficient parameter selection without incurring significant computational costs.

### Sparsified MLP and Attention Computation
The mathematical formulations for the sparsified MLP and attention computations illustrate how the DEJAVU system operates:

- **Sparsified MLP Computation**:
  \[
  MLP_{S_M}(y) = \sigma(yW_1 S_M)(W_2 S_M)^\top
  \]
  Here, \( S_M \) is the subset of neurons selected based on the input \( y \). This formulation allows for the activation of only a fraction of the neurons, leading to reduced computational requirements.

- **Sparsified Attention Computation**:
  \[
  MHA_{S_A}(y) = \sum_{i \in S_A} H_i(y) \cdot W_O^i
  \]
  The attention heads \( H_i(y) \) are computed only for the selected heads in \( S_A \), further optimizing the computation by focusing only on the most relevant components.

### Asynchronous Lookahead Predictor
The **asynchronous lookahead predictor** is a key innovation that allows DEJAVU to efficiently predict which parameters to activate. By leveraging the similarity between layer parameters and outputs from previous layers, the predictor can anticipate the necessary sparsity without waiting for the completion of the current layer's computation. This design minimizes the sequential overhead typically associated with such predictions.

### Empirical Results
The empirical results demonstrate that DEJAVU can achieve over 2Ã— reduction in latency for models like OPT-175B compared to existing implementations, without degrading quality. This performance is attributed to the effective exploitation of contextual sparsity and the efficient prediction mechanisms employed.

### Contextual Sparsity Verification Method
The verification method involving two forward passes is a straightforward yet effective approach to confirm the existence of contextual sparsity. By comparing outputs from the full model and the sparsified model, the researchers can empirically validate their hypothesis and refine their understanding of which parameters are essential for different inputs.

### Performance Metrics
The reported performance metrics, including up to 80% sparsity in attention heads and 95% in MLP neurons, highlight the potential for significant speedup in inference time. The ability to reduce the number of active parameters while maintaining output quality is a critical advantage of the DEJAVU system.

### Related Techniques
The discussion of related techniques such as quantization, pruning, and distillation provides context for the DEJAVU approach. While