# Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time

## Abstract

## 

Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DEJAVU, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardwareaware implementation that speeds up LLM inference. We validate that DEJAVU can reduce the

## Introduction

Large language models (LLMs), such as GPT-3, PaLM, and OPT have demonstrated that an immense number of parameters unleashes impressive performance and emergent in-context-learning abilities-they can perform a task by conditioning on input-output examples, without updating their parameters [(Bommasani et al., 2021;](#b19)[Liang et al., 2022;](#b91)[Brown et al., 2020;](#b24)[Min et al., 2022;](#b101)[Chan et al., 2022)](#b25). However, they are very expensive at inference time, especially for latency-sensitive applications [(Pope et al., 2022](#b109)). An ideal inference-time model should use less computation and memory while maintaining the performance and special abilities of pre-trained LLMs. The simplest and most natural approach is sparsification or pruning, which has a long history before the LLM era [(LeCun et al., 1989)](#b82). Unfortunately, speeding up inference-time sparse LLMs in wall-clock time while maintaining quality and in-context learning abilities remains a challenging problem.

While sparsity and pruning have been well-studied, they have not seen wide adoption on LLMs due to the poor quality and efficiency trade-offs on modern hardware such as GPUs. First, it is infeasible to retrain or iteratively prune models at the scale of hundreds of billions of parameters. Thus, methods in iterative pruning and lottery ticket hypothesis [(Lee et al., 2018;](#b84)[Frankle & Carbin, 2018)](#b50) can only be applied to smaller-scale models. Second, it is challenging to find sparsity that preserves the in-context learning ability of LLMs. Many works have shown the effectiveness of task-dependent pruning [(Michel et al., 2019;](#b99)[Bansal et al., 2022)](#b13), but maintaining different models for each task conflicts with the task independence goal of LLMs. Lastly, it is hard to achieve wall-clock time speed-up with unstructured sparsity due to its well-known difficulty with modern hardware [(Hooker, 2021)](#b69). For example, recent development in zero-shot pruning like SparseGPT [(Frantar & Alistarh, 2023)](#b51) finds 60% unstructured sparsity but does not yet lead to any wall-clock time speedup.

An ideal sparsity for LLMs should (i) not require model retraining, (ii) preserve quality and in-context learning ability, and (iii) lead to speed-up in wall-clock time on modern hardware. To achieve such demanding requirements, we go beyond static sparsity in previous works (e.g., structured/unstructured weight pruning). We instead envision contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that lead to (approximately) the same output as the full model for an input. Inspired by the connections between LLMs, Hidden Markov Models [(Xie et al., 2022;](#b137)[Baum & Petrie, 1966)](#b14), and the classic Viterbi algorithm [(Viterbi, 1967)](#b131), we hypothesize that for pre-trained LLMs, contextual sparsity exists given any input.

The hypothesis, if true, would enable us to cut off specific attention heads and MLP parameters (structured sparsity) on the fly for inference-time, without modifying pre-trained models. However, there are three challenges.

Existence: It is nontrivial to verify if such contextual sparsity exists, and naive verification can be prohibitively expensive.

Prediction: Even if contextual sparsity exists, it is challenging to predict the sparsity for a given input in advance.

Efficiency: Even if the sparsity can be predicted, it might be difficult to achieve end-to-end wall-clock time speedup. Taking OPT-175B as an example, the latency of one MLP block is only 0.2 ms on an 8×A100 80GB machine. Without a fast prediction and optimized implementation, the overhead can easily increase the LLM latency rather than reduce it.

In this work, we address these challenges as follows:

Existence: Fortunately, we verify the existence of contextual sparsity with a surprisingly simple approach. To achieve essentially the same output, contextual sparsity is on average 85% structured sparse and thereby potentially leads to a 7× parameter reduction for each specific input while maintaining accuracy (Figure [1(a)](#fig_10)). During explorations of contextual sparsity, we make important empirical observations and build a theoretical understanding of major components in LLMs that help address the prediction and efficiency challenge. . DEJAVU uses lookahead predictors to side-step prediction costs: given the input to the attention layer at block k, they (asynchronously) predict the contextual sparsity for the MLP at block k, and given the input to the MLP at block k, they predict the sparsity for the attention head at the next layer.

## Deja Vu

Prediction: We discover that contextual sparsity depends not only on individual input tokens (i.e., non-contextual dynamic sparsity) but also on their interactions (contextual dynamic sparsity). Figure [1](#fig_10)[(b)](#) shows that with pure dynamic information, sparsity prediction is inaccurate. Only with token embeddings with sufficient contextual information can we predict sparsity accurately. Another finding is that contextual dynamic sparsity for every layer can be predicted based on the "similarity" between layer parameters (heads/MLP) and the output from the previous layer, which carries the immediate contextual mixture of token embeddings.

Efficiency: Because at inference time, model parameters are static, inspired by the classical nearest neighbor search (NNS) literature and its applications in efficient deep learning, it is possible to formulate the above similarity-based prediction as an NNS problem [(Indyk & Motwani, 1998b;](#)[Zhang et al., 2018;](#b143)[Chen et al., 2020a)](#). However, as mentioned, the overhead might be difficult to overcome as we would need to perform on-the-fly predictions before every layer. Luckily, we exploit a phenomenon of LLM where token embeddings change slowly across layers due to residual connections (wellknown in computer vision [(He et al., 2016)](#b65)). Since the inputs to a few consecutive layers are very similar, we can design an asynchronous lookahead predictor (Figure [2](#)).

Based on our findings, we present a system, DEJAVU, that exploits contextual sparsity and realizes efficient LLMs for latency-sensitive applications.

• In Section 4.1 and Section 4.2, we present a low-cost learning-based algorithm to predict sparsity on the fly.

Given the input to a specific layer, it predicts a relevant subset of attention (heads) or MLP parameters in the next layer and only loads them for the computation.

• In Section 4.3, we propose an asynchronous predictor (similar to classic branch predictor [(Smith, 1998)](#b118)) to avoid the sequential overhead. A theoretical guarantee justifies that the cross-layer design suffices for accurate sparsity prediction.

After integrating hardware-aware implementation of sparse matrix multiply (Section 4.4), DEJAVU (written mostly in Python) can reduce latency of open-source LLMs such as OPT-175B by over 2× end-to-end without quality degradation compared to the state-of-the-art library Faster-Transformer from Nvidia (written entirely in C++/CUDA), and over 2× compared to the widely used Hugging Face implementation at small batch sizes. Furthermore, we show several ablations on different components of DEJAVU and its compatibility with quantization techniques.

## Related Work and Problem Formulation

We first briefly discuss the rich literature on efficient inference. Then, we introduce the latency breakdown in our setting. Last, we provide a formal problem formulation.

## Quantization, Pruning, Distillation for Inference

Various relaxations have been studied for decades for model inference in machine learning. There are three main techniques: quantization [(Han et al., 2015;](#b63)[Jacob et al., 2018;](#b75)[Nagel et al., 2019;](#b103)[Zhao et al., 2019)](#b144), pruning or sparsity [(Molchanov et al., 2016;](#b102)[Liu et al., 2018;](#b92)[Hoefler et al., 2021)](#b68), and distillation [(Hinton et al., 2015;](#b67)[Tang et al., 2019;](#b127)[Touvron et al., 2021)](#b129). They are orthogonal areas and usually excel in different settings. Recently, there is active research attempting to apply one or a combination of such techniques in LLM inference [(Yao et al., 2022;](#b139)[Park et al., 2022;](#b108)[Dettmers et al., 2022;](#b46)[Frantar et al., 2022;](#b52)[Frantar & Alistarh, 2023;](#b51)[Bansal et al., 2022;](#b13)[Xiao et al., 2022)](#b136). More discussion is presented in Appendix A.

## LLM Inference Latency Breakdown

The generative procedure of LLMs consists of two phases: (i) the prompt phase takes an input sequence to generate the keys and values (KV cache) for each transformer block of LLMs, which is similar to the forwarding pass of LLMs training; and (ii) the token generation phase utilizes and updates the KV cache to generate tokens step by step, where the current token generation depends on previously generated tokens.

This paper studies the setting where the token generation phase easily dominates the end-to-end inference time. As shown in Table [1](#tab_0), generating a sequence of length 128 takes much longer time than processing a sequence of length 128 as prompt due to I/O latency of loading model parameters.

In addition, Table [2](#tab_1) shows that attention and MLP are both bottlenecks in LLMs, e.g., in 175B models, loading MLP parameters takes around 2 3 of the total I/O and attention heads take the other 1 3 . Further, in the tensor-parallel regime, there are two communications between GPUs, one after the attention block, and the other one after the MLP block. As shown in Table [3](#tab_2), communication between GPUs takes around 15 % token generation latency. This paper focuses on making attention and MLP more efficient. Communication cost implies that the upper bound of such speed-up is around 6× when skipping all transformer blocks.  

## Problem Formulation

The goal is to reduce the generation latency of LLMs by exploiting contextual sparsity. In the following, we formally define the sparsified attention and MLP blocks.

Sparsified MLP: There are two linear layers in one MLP block, W 1 , W 2 ∈ R d×4d . Denote y ∈ R 1×d as the input to the MLP block in the current generation step. Let each column (the weight of i-th neuron) of linear layers be W 1 i , W 2 i ∈ R d×1 . With contextual sparsity, only a small set of them are required for computation. Let S M ⊆ [4d] denote such set of neurons for input y. The sparsified MLP computation is

$MLP S M (y) = σ(yW 1 S M )(W 2 S M ) ⊤ , (1$$)$where σ is the activation function, e.g., ReLU, GeLU. Note that since the computation in the first linear results in sparse activations, the second linear layer is also sparsified.

Sparsified Attention: Let X ∈ R n×d denote the embeddings of all tokens (e.g., prompts and previously generated tokens). Let y ∈ R 1×d be the input to the Multi-Head-Attention (MHA) in the current generation step. Suppose there are h heads. For each i ∈ [h], we use

$W K i ,W Q i ,W V i ∈ R d×d h to$denote key, query, value projections for the i-th head, and

$W O i ∈ R d h ×d for output projections.$With contextual sparsity, we denote S A as a small set of attention heads leading to approximately the same output as the full attention for input y. Following the notation system in [(Alman & Song, 2023)](#b2), sparsified MHA computation can be formally written as

$MHA S A (y) = i∈S A H i (y) 1×d h W O i d h ×d$, where H i (y) : R d → R d h and D i (y) ∈ R can be written as

$H i (y) := D i (y) -1 exp(yW Q i (W K i ) ⊤ X ⊤ )XW V i , (2) D i (y) := exp(yW Q i (W K i ) ⊤ X ⊤ )1 n .$For both MLP and Attention, given a compute budget, the goal is to find S M and S A that minimize the error between the sparse approximation and full computation. We can zero out over 95% of MLP parameters for a given token.

## Pre-trained LLMs are Contextually Sparse

In this section, we present several key observations and theoretical understandings of sparsity in LLMs, upon which the DEJAVU design is based. We first test the contextual sparsity hypothesis and verify that contextual sparsity exists in pretrained LLMs in Section 3.1. Then, we build an understanding of why contextual sparsity happens naturally even when LLMs are densely trained in Section 3.2. Finally, we present an observation on residual connections and explain their relationship to contextual sparsity analytically in Section 3.3.

## Contextual Sparsity Hypothesis

Inspired by prior pruning literature [(Molchanov et al., 2016)](#b102), we find a surprisingly simple method is sufficient to study and verify our hypothesis. In this section, we describe the testing procedure, observation details, and insights of this study.

Verification: Our test is performed on OPT-175B, 66B, and 30B models and various downstream datasets such as Open-BookQA [(Mihaylov et al., 2018)](#b100) and Wiki-Text [(Merity et al., 2016)](#b98). We find the contextual sparsity for every input example with two forward passes of the model. In the first pass, we record a subset of parameters, specifically which attention heads and MLP neurons yield large output norms for the input. In the second pass, each input example only uses the recorded subset of parameters for the computation. Surprisingly, these two forward passes lead to similar prediction or performance on all in-context learning and language modeling tasks.

Observation: Figure [3](#) shows that on average, we can impose up to 80% sparsity on attention heads and 95% sparsity on MLP neurons. As mentioned in Section 2, OPT-175B model has 2× MLP parameters than those of attention blocks. Therefore total sparsity here is around 85%. Since these are all structured sparsity (heads and neurons), predicting them accurately could potentially lead to 7× speedup.

## Insight:

It is intuitive that we can find contextual sparsity in MLP blocks at inference time because of their activation functions, e.g., ReLU or GeLU [(Kurtz et al., 2020)](#b80). Similar observations were made by [(Li et al., 2022)](#). However, it is surprising that we can find contextual sparsity in attention layers. Note that, finding contextual sparsity in attention is not the same as head pruning. We cross-check that different examples have different contextual sparsity. Although 80% of the parameters are not included in the paths for a given example, they might be used by other examples. Next, we will try to understand why contextual sparsity exists in attention blocks.

## Token Clustering in Attention Layers

In the previous section, we have verified that there exists contextual sparsity for a given input in LLMs. In this section, we try to understand the reason for such phenomena, especially in attention layers. We first show an in-depth observation of attention. Then we present a hypothesis that self-attentions are conceptually clustering algorithms. Last we show analytical evidence to support this hypothesis.

Observation: Figure [4](#fig_3) shows the attention map of three different heads from the same layer for an example input. The next token it should predict is "Truck". Darker color represents higher attention scores. We observe that the middle head is a relatively uniform token-mixing head while the top and bottom ones are "heavy hitter" attention heads (with high attention to "like" and "shipping"). Unsurprisingly, only selecting heavy hitter heads but not uniform heads does not affect the prediction, since uniform heads do not model or encode important token interactions. In the next section, we will also explain in detail how the criteria for selecting uniform attention heads and heads with small output norms are highly correlated.

Hypothesis: We hypothesize that the attention head is performing mean-shift clustering [(Derpanis, 2005)](#b45).

Recall the notation defined in Section 2.3. For i-th head at current layer, X = [x 1 , ... , x n ] ⊤ ∈ R n×d are the token embeddings in the previous time steps. XW K i and XW V i are the projection of embedding. For an input embedding y, the output ỹi = H i (y), where H i (y) is defined in Eq. 2.

For each i ∈ [h], if we let K i (x j ,y) := exp(yW Q i (W K i ) ⊤ x j ) measure the similarity between x j and y, and define m i (y) := j Ki(xj ,y)xj j Ki(xj ,y) , then we have ỹi = m i (y)W V i . Further, if we set W V i = I and consider the residue connection followed by layer norm, then in the next layer, the embedding ŷi of the current token becomes ŷi = Normalize(y + ỹi ) = Normalize(y+m i (y)), which has a fixed point y = γm i (y) for any scalar γ. This iteration bears a resemblance to meanshift clustering, which simply performs iteration y ← m i (y) until convergence. This has an obvious fixed point y = m i (y).

Therefore, the self-attention head can be regarded as one mean-shift step to push input embeddings of different tokens together, if they are already neighbors in a projection space specified by W Q i (W K i ) ⊤ . Different heads learn different projection spaces to perform clustering. These dynamics explain the precise reason why token embeddings tend to cluster after going through more layers, resulting in high attention scores among cluster members, and low scores for non-members. Furthermore, the cluster patterns are different at different heads (More details in Appendix K).

The above analysis not only provides an understanding of why contextual sparsity exists naturally in pre-trained LLMs, but also inspires our design of "similarity"-based sparsity prediction for DEJAVU in Section 4.

## Slowly Changing Embeddings across Layers

We first present our observation that embeddings change slowly across consecutive layers. Then we provide a detailed analysis on the phenomenon. Finally, we show its close connection with contextual sparsity. Details are in Section B.

High similar embeddings in consecutive layers: In Figure [5](#fig_4)(a), we show that for the same given input, the cosine similarity between embeddings or activations in two consecutive layers is exceptionally high on 7 different sizes of OPT models. Specifically, we collect activations from each layer while performing OPT model inference on C4 validation set [(Raffel et al., 2019)](#b113). Taking OPT-175B as an example, starting from the second layer, the similarity between any two consecutive layers is around 0.99, which indicates that when an input is passed through the model, the direction of its embedding changes slowly. Interestingly, the most drastic change happens in the first layer. Furthermore, we increase the gap and investigate the similarity between the embedding at layer l and at layer l + n shown in Figure [5](#fig_4)(b). As we increase the gap, the similarity decreases as expected while the differences in cosine similarity between various choices ∥X∥ is significantly higher than ∥F (X)∥, which explains the slowly changing embedding.

of n are smaller at the shallower layer. We plot the mean similarity, and the standard deviation is indicated by the shading. Similar plots on more models are presented in Appendix B.

Connection to residuals: We verify that the high similarity in embeddings in LLM inference is due to the residual connection. We first dissect the computation graph inside each transformer layer to understand the cause behind this phenomenon. There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as X +F (X), where F is either the Multi-Head Attention or two MLP Layers. In Figure [5](#fig_4)(c) and Figure [5](#fig_4)(d), indeed we can see that ∥X∥ is significantly greater than ∥F (X)∥, confirming that embeddings are changing slowly because the residual norm is large.

Connection to Contextual Sparsity: We take a step deeper trying to understand the reason behind the large residual norm with mathematical modeling. We discover that one possible reason for small ∥F (X)∥ is due to high sparsity. For the MLP Block, high sparsity may contribute to the small norm of F (X) because a large portion of outputs have small norms. Similar reasoning applies to the Attention Block, and thus a large number of attention heads yield small norm outputs.

Residual Two Sides Bound: Besides empirical reasoning, we formally define the computation of LLMs mathematically. Under our computation model, we can show that a shrinking property which is observed by our practical experiments.

Proofs are in Appendix G, H, I.

Lemma 3.1 (Informal). Let 0 < ϵ 1 < ϵ 2 < 1 be the lower and upper bound of the shrinking factor. Let x be the y be the output. We have the residual connection y = x+F (x). For the MLP block F (x), we have ϵ 1 ≤ ∥y -x∥ 2 ≤ ϵ 2 . For the attention block F (x), we have ϵ 1 ≤ ∥y-x∥ 2 ≤ ϵ 2 .

## DEJAVU

In this section, we present our framework for inference-time contextual sparsity search for LLMs. We introduce the sparsity predictor for MLPs in Section 4.1 and for attention heads in Section 4.2. DEJAVU's workflow is shown in Figure [2](#). Section 4.3 discusses exploiting our observation on LLMs to avoid the sparse prediction overhead with theoretical guarantees. In Section 4.4, we present our optimized implementation that enables end-to-end latency reduction. More details are presented in Section D.

## Contextual Sparsity Prediction in MLP Blocks

As explained in Section 2, MLP blocks are one of the major bottlenecks for the LLM generation ( 2 3 of the FLOPs and IOs). In this section, we discuss how we achieve wall-clock time speed-up with contextual sparsity in the MLP blocks.

Challenge Figure [3](#)[(b)](#) shows that for a given token, the contextual sparsity of 95% is possible. The contextual sparsity in the MLP block can be identified after computing the activation. However, this only demonstrates the existence of contextual sparsity but brings no benefits in terms of efficiency. A fast and precise prediction is needed to exploit contextual sparsity for end-to-end efficiency. The naive way is to select a subset of neurons randomly. Unsurprisingly, random selection fails to identify the accurate contextual sparsity, resulting in drastic model degradation.

A Near-Neighbor Search Problem: Recall that we verify the existence of contextual sparsity by recording which neurons yield significant norms. Essentially, given the input, the goal is to search for the neurons that have high inner products with the input, because the activation function "filters" low activation. Thus, we formulate the contextual sparsity prediction of an MLP layer as the classical near-neighbor search problem under the inner product metric.

Definition 4.1 (Approximate MaxIP in MLP). Let c ∈ (0,1) and τ ∈ (0,1) denote two parameters. Given an n-vector dataset W 1 ⊂ S d-1 on a unit sphere, the objective of the (c, τ )-MaxIP is to construct a data structure that, given a query y ∈ S d-1 such that max w∈W 1 ⟨y,w⟩ ≥ τ , it retrieves a vector z from W 1 that satisfies ⟨y,z⟩ ≥ c•max w∈W 1 ⟨y,w⟩.

Remark 4.2. Our W 1 (first linear layer) and y (input embedding) in MLP blocks can be viewed as the dataset and query in Definition 4.1 respectively.

## Design

The standard state-of-the-art near-neighbor search methods and implementations slow down the computation. Take OPT-175B where d is 12288 as an example. HNSW [(Malkov & Yashunin, 2018)](#b96) requires more than 10ms, and FAISS [(Johnson et al., 2019)](#b77) requires more than 4ms, while the MLP computation is only 0.2ms. The high dimensionality and complications of data structure implementation on GPU make the search time longer than the MLP computation. Therefore, we choose a neural network classifier as our near-neighbor search method to exploit the fast matrix multiplication on GPU. For each MLP block, we train a small twolayer fully connected network to predict contextual sparsity.

Collecting training data is straightforward because we know the contextual sparsity using dense computation. The training algorithm is summarized in Algorithm 1. The sparsified computation in W 1 has two steps: (1) Given y, the sparsity predictor SP M predicts a set S M of important neurons in weights W 1 . (2) Compute the sparsified MLP defined in Eq. equation 1. Note here the sparsity in MLP is highly structured.

## Algorithm 1 Sparse Predictor Training

Input: A pre-trained LLM block with parameter set M , token embedding set at block

$M = {x i } i∈[N ] , threshold t Sparse Predictor SP P + ← ∅, P -← ∅ for i = 1 → N do P + ← P + ∪{(x i ,m r ) | m r ∈ M,m r (x i ) ≥ t} P -← P -∪{(x i ,m r ) | m r ∈ M,m r (x i ) < t} end for SP ← TRAIN(P + ,P -,L)$▷ L is a loss function

## Contextual Sparsity Prediction in Attention Blocks

Attention blocks take around 30% I/Os in the generation. In this section, we describe how DEJAVU exploits contextual sparsity to speed up the Attention blocks.

Challenge: As discussed in Section 3.1, only a few heads perform important computations for a given input token. Similar to the MLP blocks, a fast selection of attention heads without full computation is required to reduce end-to-end latency. Furthermore, one particular challenge of sparse prediction in attention blocks is attention's dependence on previous tokens. On the one hand, it is unclear whether the past token's key and value caches are needed for sparse prediction. On the other hand, it is unclear how to handle the missing KV cache of past tokens for the current token computation at the selected head.

A Near-Neighbor Search Problem: Head prediction can also be formulated as a near-neighbor search problem based on our understanding in Section 3.2. Since each head is performing mean-shift clustering, after the first few layers, the current token embedding alone is sufficient for the prediction thanks to the token-mixing nature of the transformer. Therefore, the prediction can be based on the similarity between y and head parameters.

Approach: We design our attention sparse predictor to be the same architecture as the MLP sparse predictor. Each head is regarded as one class and a similar training process is used (Algorithm 1). Then, similar to how MLP prediction is performed, the attention sparsity predictor SP A selects a set S A of heads H i (see Eq. equation 2). To address the problem of missing KV cache for a past token, we exploit the fact that the generation latency is I/O bounded while computation is essentially "free". Specifically, for the predicted attention head of input y, we compute the corresponding keys, and values and store them in the KV cache. But we also save a copy of y for all the other non-selected heads. Then during the future token generation, if there is missing KV cache in the selected heads, we could load stored token embeddings and compute the keys and values together. This requires almost minimal extra memory access (the main cost is loading the weight matrices).

## Reducing Overhead with Asynchronous Execution

Sparse prediction overhead may easily increase the end-toend latency rather than reduce it despite the reduction in FLOPs. Therefore, we introduce a look-ahead sparse prediction method, inspired by our observations in Section 3.3.

Challenge: Denote y l ∈ R d as the input to transformer layer l. We can write the computation at layer l as y l ← MHA l (y l ), y l ← MLP l ( y l ). With predictors SP l A and SP l M , the computation at the transformer layer l can be re-written as

$S l A ← SP l A (y l ), y l ← MHA l S l A (y l ), S l M ← SP l M ( y l ), y l ← MLP l S l M ( y l )$where set S l A is the contextual sparsity for the Attention block, and set S l M is the contextual sparsity for the MLP block at l-th layer. Note that the computation at Attention and MLP blocks have to wait for the sparse predictor decision. This overhead potentially outweighs the saving from Attention and MLP blocks in terms of latency.

Approach: In Section 3.3, we present the slowly evolving embedding phenomenon, which provides opportunities to relax the sequential computation to parallel computation. Along with the observation of low computation intensity during generation, we parallel the sparse prediction with the computation of each block ( See Figure [2](#)). The computation can be written as follows:

$y l ← MHA l S l A (y l ), y l ← MLP l S l M ( y l ), S l+1 A ← SP l A (y l ), S l+1 M ← SP l M (y l ), We remark S l+1$A and S l+1 M can be computed in parallel with y l or y l , while the previous 4 steps are sequential.

Theoretical guarantee: The sparse predictor can make further cross-layer decisions because of the residual connection. We present an informal lemma statement regarding crosslayer prediction. It is well-known that MaxIP is equivalent to ℓ 2 nearest neighbor search. For convenience, we use MaxIP here. We include more discussions and proofs in Section J.

Lemma 4.3 (Informal). Let ϵ ∈ (0,1). Let y l be input at l-th layer. Let y l-1 be the input at (l-1)-th layer. Suppose that ∥y l -y l-1 ∥ 2 ≤ ϵ. For any parameters c, τ such that ϵ < O(cτ ). Then we can show that, solving MaxIP(c,τ ) is sufficient to solve MaxIP(0.99c,τ ).

## Hardware-efficient Implementation

We describe how DEJAVU is implemented in a hardwareefficient manner to realize the theoretical speedup of contextual sparsity. Taking into account hardware characteristics leads to over 2× speedup compared to an optimized dense model, and 4× faster than a standard sparse implementation.

We highlight some hardware characteristics of GPUs:

• Small-batch generation is bottlenecked by GPU memory I/Os [(NVIDIA, 2022;](#b107)[Ivanov et al., 2021;](#b74)[Dao et al., 2022)](#b40). This is because of low arithmetic intensity. For each element loaded from GPU memory, only a small number of floating point operations are performed.

• GPUs are block-oriented devices: loading a single byte of memory takes the same time as loading a block of memory around that same address [(Harris, 2013)](#b64). The block size is usually 128 bytes for NVIDIA GPUs [(Cook, 2012)](#b38).

These characteristics present some challenges in implementing contextual sparsity. However, they can be addressed with classical techniques in GPU programming.

Kernel fusion: A standard implementation of sparse matrix-vector multiply (e.g., in PyTorch) that separately indexes a subset of the matrix W 1 S M before multiplying with input y would incur 3× the amount of memory I/Os. Therefore, to avoid such overhead, we fuse the indexing and the multiplication step. Specifically, we load a subset of W 1 S M to memory, along with y, perform the multiply, then write down the result. This fused implementation (in Triton [(Tillet et al., 2019)](#b128)) yields up to 4× speedup compared to a standard PyTorch implementation (Appendix E).

Memory coalescing: In the dense implementation, the weight matrices of two linear layers in MLP are stored as (W 1 ) ⊤ and W 2 so that no extra transpose operation is needed. They are conventionally stored in row-major format. In the sparse implementation, it allows us to load (W 1 S M ) ⊤ optimally (the second dimension is contiguous in memory). However, for cases where we need to load (W 2 S M ), this format significantly slows down memory loading, as indices in S M point to non-contiguous memory. We simply store these matrices in column-major format (i.e., store (W 2 ) ⊤ in row-major format), then use the same fused kernel above. Similarly, in attention blocks, we store attention output projection W O column-major format.

These two techniques (kernel fusion and memorycoalescing) make DEJAVU hardware-efficient, yielding up to 2× speedup end-to-end compared to the state-of-the-art FasterTransformer (Section 5.1). 

## Empirical Evaluation

In Section 5.1, we present the end-to-end results that show DEJAVU achieves over 2× reduction in token generation latency compared to the state-of-the-art FasterTransformer and over 6× compared to Hugging Face with no accuracy loss. In Section 5.2, we perform a list of ablation studies such as independent evaluation on the inference-time contextual sparsity of the MLP block and the Attention block (Details are presented in Section C). At last, we present the additional results to demonstrate the future possibility of sparsifying the entire LLMs via layer skipping in Section C.3.

## End-to-End Result

Experiment Setting: We compare the accuracy of DE-JAVU-OPT against the original OPT model on two language modeling datasets Wiki-Text [(Merity et al., 2016)](#b98) and C4 [(Raffel et al., 2019)](#b113) and seven few-shot downstream tasks: CB (de Marneffe et al., 2019), COPA [(Gordon et al., 2012)](#b59), Lambada [(Radford et al., 2019)](#b112), OpenBookQA [(Mihaylov et al., 2018)](#b100), PIQA [(Bisk et al., 2020)](#b17), RTE [(Giampiccolo et al., 2007)](#b57), [Winogrande (ai2, 2019)](#). We use lm-evalharness [(Gao et al., 2021)](#b54) for zero-shot and five-shot tasks. We collect training data for the sparsity predictor using 500 random data points from the C4 training dataset. Our experiments are conducted on NVIDIA A100 80GB GPU servers.

No accuracy drop until 75% sparsity: In Figure [6](#fig_11), we present DEJAVU-OPT-175B's accuracy trend. In a zero-shot setting, the average accuracy across tasks does not drop until 75% sparsity. A similar trend can be observed for the five-shot setting, which verifies the model's ability for in-context learning. This result is exceptionally encouraging given our observation in Figure [1(a)](#fig_10), where we could impose 85% sparsity when allowed full computation.

Over 2× latency reduction: Figure [7](#) presents the latency speed-up for the token generation with OPT-175B at batch size 1, where DEJAVU achieves the best performance. At around 75% sparsity, DEJAVU speeds up generation by 1.8-2× compared to the state-of-the-art FasterTransformers (FT)[foot_8](#foot_8) and by 4.8-6× to Hugging Face (HF) implementation[foot_9](#foot_9) .

## Ablation Results

Contextual Sparsity for Larger Batches: Although this paper focuses on latency-sensitive settings, we demonstrate that DEJAVU generalizes to larger batches. we present the Union contextual sparsity (fraction of neurons/heads that are not used by any of the inputs in the batch) of different batches sizes for MLP and Attention blocks, respectively, in Figure [8](#) and 11. The union operation is essential to realize a fast sparse GEMM. Surprisingly the number of MLP neurons and Attention heads that DEJAVU activated does not grow linearly with the batch size. This suggests a power law distribution rather than a uniform distribution of parameter access from all input examples. This provides an opportunity for potentially extending Dejavu to the high-throughout setting. For example, we can first pre-process the inputs and batch similar inputs to enjoy a higher level of union contextual sparsity.

## Contextual sparsity on MLP blocks:

We study the contextual sparsification of the MLP block in OPT-175B. We leave the Attention block as dense computation. Table [4](#tab_3) shows the model performance at 85% sparsity. The MLP sparse predictor introduces no accuracy loss on both zero-shot tasks and language modeling. In the training of the MLP sparse predictor, we observe that the sparse predictor achieves high validation accuracy. The shallow layer seems easier to model because the predictor has validation accuracy over 99% in the shallow layers and drops to around 93% in the ending layers.

## Contextual sparsity on attention blocks:

In this section, we study the sparse predictor for the Attention block on OPT-175B and leave the MLP block as dense computation. Table [4](#tab_3) displays the test accuracy on zero-shot tasks and perplexity on the language modeling datasets. In summary, the Attention sparse predictor introduces no accuracy loss at around 50% sparsity. During the training of the Attention sparse predictor, we observe different trends compared to the MLP sparse predictor. The validation accuracy is around 93% in the middle layers and near 99% in the shallow and deep layers.

Contextual Sparsity on Smaller Models: Our main experiments focus on OPT-175B. Here, we verify DEJAVU's effectiveness on a smaller model, specifically OPT-66B. In Table [5](#tab_4), we summarize the accuracy on zero-shot task at 50% sparsity. Similar to DEJAVU-OPT-175B, we notice no accuracy loss.

Contextual Sparsity on Other Models: We expand the evaluation to another model family. In Table [6](#tab_5), we summarize the accuracy at attention sparsity 50% and MLP sparsity 30%. Similar to OPT family, we notice no accuracy loss. The lower sparsity level in MLP is due to the difference in activation function.

Non-Contextual Sparsity: As we mentioned in Section 1, one could predict sparsity without contextual information.

For non-contextual sparsity, we rely on the original embedding at the input layer. At every block, we first pass the original embedding to record a subset of parameters yielding a large norm. In the second pass, the embedding at every layer only uses the recorded subset. As shown in Figure [1](#fig_10), non-contextual prediction is not sufficient and leads to accuracy losses even at 50% sparsity. This result verifies our design choices of relying on the activation at every layer as input to make contextual sparsity predictions.

Compatibility with Quantization: Quantization is another promising direction for efficient language models. We investigate the possibility of combining contextual sparsity with quantization techniques. For DEJAVU-OPT-175B, we set the entire model sparsity at 75%. For quantization, we apply 4-bit quantization on model weights (W4A16). As shown in Table [7](#tab_6), the combination of quantization and DEJAVU almost always achieves better accuracy than DEJAVU or quantization alone. This suggests that the approximation errors from these two directions do not get compounded.

## Conclusion

Our main goal is to make LLM inference efficient so that their powerful in-context learning abilities can be used in more application domains. We observe that contextual sparsity can be accurately predicted with lightweight learning-based algorithms. This motivated us to design DEJAVU that uses asynchronous lookahead predictors and hardware-efficient sparsity to speed up LLM inference in wall-clock time. Our encouraging empirical results validate that contextual sparsity can reduce inference latency by over 2× compared to the state-of-the-art FasterTransformer without model quality drops. Our method is a step towards making LLMs more accessible to the general community, which could unlock exciting new AI applications.

## Contents:

In Section A, we present an extended discussion on LLM inference and related works. In Section B, we provide more observation plots for slowly changing activation and further observation on the possibility of sparsifying LLMs via layer skipping. In Section C, we provide experiment details. In Section D, we demonstrate implementation details. In Section E, we provide detailed benchmarks regarding our implementation. In Section F, we define some basic notations and definitions.

In Section G, we define subspace embedding and show the norm preserving. In Section H, we introduce distances, angles, and inner product. In Section I, we provide the distance between different functions. In Section J, we provide the Near-neighbor Search data structure. In Section K, we discuss self-attention as a clustering algorithm in depth.

## A Related Work

Generative LLM inference. Taking OPT-175B as an example, assume 6 A100 80GB PCIe, based on the hardware specifications, we compare two main phases of inference time LLM, namely prompting and token generation in Table [1](#tab_0), and two major components, namely Multi-Head-Attention block and MLP block in Table [2](#tab_1). In practice, the token generation phase usually dominates the end-to-end test latency due to IO latency. Generating only two tokens is about the same latency as prompting. Further, during token generation, the MLP block is 2 × more expensive in both FLOPs and IO access. The hardware is often at low utilization because memory reads and writes are more limited on modern hardware than tensor core computation.

Given the rapid development of LLM, there is an emergence of systems that are specialized for LLM inference, such as Faster Transformer (NVIDIA), Orca [(Yu et al., 2022)](#b140), LightSeq [(Wang et al., 2021)](#), PaLM inference [(Pope et al., 2022)](#b109), TurboTransformers [(Fang et al., 2021)](#b49), and Deepspeed-Inference [(Aminabadi et al., 2022)](#b5). In practice, the token generation phase usually dominates the end-to-end inference time. Although the state-of-the-art systems introduce some helpful system optimizations for speedup, there is a lack of careful algorithm and system co-design to unleash the full potential of hardware efficiency during the LLM inference computation.

Near-neighbor Search for Efficient Deep Neural Networks. Near-neighbor Search is a well-studied problem with wide applications in recommendation system [(Xue et al., 2017;](#b138)[Hall & Attenberg, 2015)](#b62), question answering [(Boytsov et al., 2016;](#b21)[Seo et al., 2019;](#b116)[Chang et al., 2020)](#b26) and natural language processing [(Bengio et al., 2003;](#b16)[Lee et al., 2016)](#b83). There has been a line of work using Near-neighbor Search techniques such as Locality-sensitive hashing [(Gionis et al., 1999)](#b58) and Graph-based indexing [(Malkov et al., 2014)](#b95) for efficient deep neural network training or inference [(Zhang et al., 2018;](#b143)[Chen et al., 2019;](#b28)[2020a;](#)[Kitaev et al., 2020;](#b79)[Chen et al., 2021b;](#)[a;](#)[Liu et al., 2022)](#b93).

Quantization, pruning, distillation for LLM inference. Various system relaxations have been studied for decades for model inference in machine learning. For example, quantization [(Han et al., 2015;](#b63)[Jacob et al., 2018;](#b75)[Nagel et al., 2019;](#b103)[Zhao et al., 2019)](#b144), pruning [(Molchanov et al., 2016;](#b102)[Liu et al., 2018;](#b92)[He et al., 2019;](#b66)[Hoefler et al., 2021)](#b68), and distillation [(Hinton et al., 2015;](#b67)[Cho & Hariharan, 2019;](#b34)[Tang et al., 2019;](#b127)[Touvron et al., 2021)](#b129) have been applied to speed up the inference of the machine learning model. Active research has recently attempted to apply such techniques in LLM inference. For example, zeroQuant [(Yao et al., 2022)](#b139) and nuQmm [(Park et al., 2022)](#b108) implement customized CUDA kernels to support tenor-wise or group-wise quantization for LLM inference; LLM.int8 [(Dettmers et al., 2022)](#b46) adopts a mixed INT8/FP16 computation to diminish the influence of activation outliers; SmoothQuant [(Xiao et al., 2022)](#b136) enables efficient 8-bit weight and activation for LLM inference; GPTQ [(Frantar et al., 2022)](#b52) adopts a one-shot weight quantization method based on approximate second-order information for accuracy and efficiency; SparseGPT [(Frantar & Alistarh, 2023)](#b51) introduces an approximate sparse regression solver to enable the sparsity in LLM inference; [(Bansal et al., 2022)](#b13) has reported that a small set of attention heads can perform primitive induction operations associated with in-context learning, and use this property to prune LLM for acceleration.

Residual connections in neural networks. Residual connection shows great advantages for neural network generalization, it provides additional paths for activations to reach the latter parts of the neural network by skipping some layers [(He et al., 2016)](#b65). The advancement of residual connections can be viewed as ensembles of multiple shallow neural networks [(Veit et al., 2016)](#b130). Plenty of active research has discussed the effectiveness of residual connections [(Balduzzi et al., 2017;](#b12)[Bello et al., 2021;](#b15)[Allen-Zhu & Li, 2019;](#b1)[Frei et al., 2019)](#b53). However, as far as we know, there is no former work that leverages the property of residual connections to improve the efficiency of LLM inference.

## B Additional Observation on Slowly Changing Observation

First, we present more plots on the cosine similarity between representations. Figure [9](#) plots the cosine similarity between activation across layers on OPT family. It is evident that similarity is high for the larger models.

There are two residual connections inside a transformer layer, one around the attention block, and the other one around the MLP block. The residual connection can be written as X +F (X), where F is either the Multi-Head Attention or two MLP Layer. Figure [10](#fig_10) plots the cosine similarity between X and X +F (X), which is close to 1.0, and the cosine similarity between . Cosine similarity between X and F (X), and the cosine similarity between X and X ′ in orange color. L2 norm of X and F (X) and X after layer normalization in purple on the right. Except on the first layer, ∥X∥ is significantly higher than ∥F (X)∥. ∥F (X)∥ is higher at the first layer, which corresponds to the low cosine similarity at the first layer.

For example, in our exploration stage mentioned in Section 4.1, we adopt HNSW, a state-of-art near-neighbor search method, to predict MLP sparse pattern, and we can see from the following table there is no drop in the perplexity at 90 % sparsity ratio. However, due to the high dimensionality of embedding and HNSW's reliance on CPU, the time HNSW took to identify the sparsity pattern is 10ms, which is longer than the MLP computation.

In our paper, we choose a neural network classifier as our near neighbor search method to take advantage of the fast matrix multiplication on GPU. And training such classifiers to predict sparsity patterns is not only cheaper in terms of training cost but also inherently different from the method concept. Improving the inference efficiency of Transformer models is a challenging task due to their sequential execution of Transformer layers. Each sub-block depends on the output of the previous one, leading to low hardware efficiency, particularly during the token generation phase where each forward pass is computed for only one token. However, the sequential execution of blocks and sub-blocks yields computation bubbles, and the latter involves a large amount of communication overhead.

Here, we present an interesting observation that can potentially alleviate these challenges. We found that the activation of the model changes slowly across blocks. Specifically, the cosine similarity of activations between adjacent blocks is often above 0.99. This suggests that the blocks might take the previous activation as input -parallelize or reorder the blockswithout significantly affecting the output. Slowly changing activations suggest that it may be possible to parallelize, reorder, or even skip blocks while maintaining a similar output. Some existing models, such as GPT-J [(Wang & Komatsuzaki, 2021)](#b132), GPT-NeoX [(Black et al., 2022)](#b18), and PaLM [(Chowdhery et al., 2022)](#b35) already placed the Attention block and MLP block in parallel in training to facilitate parallel computation and reduce the communication overhead.

Here we investigate the possibility at inference time. And surprisingly, we found parallelizing those blocks for models that are trained in a sequence manner will not hurt the performance of downstream tasks significantly. And surprisingly, we found parallelizing those blocks for models that are trained in a sequence manner will not hurt the performance of downstream tasks significantly. TableC.3 presents some preliminary results of OPT-175B and Bloom

Given the activation y and Transformer layer l, we have:

$y l ← y l +MHA l (y l )$y l ← y l +MLP l ( y l ) Parallelizing two blocks refers to placing the Attention and MLP blocks in parallel, i.e.: y l ← y+MHA l (y l )+MLP l (y l ) Parallelizing four blocks then parallelize the blocks of two Transformer layers, defined as follows:

y l+1 ← y l +MHA l (y l )+MLP l (y l )+MHA l+1 (y l )+MLP l+1 (y l ) Skipping layers is straightforward, which drops an entire Transformer layer for every n layers.

We are surprised to find that parallel two layers preserve accuracy on a series of tasks across models. Besides, randomly skipping 25% layers doesn't lead to catastrophic quality. Our findings suggest from the downstream task perspective, the activation patterns within the model are relatively consistent across different blocks, providing a potential avenue for future research on model compression and optimization.

## D Implementation Details

Figure [12](#fig_7) presents a more detailed workflow of DEJAVU. The left diagram shows how an input y performs the sparse MHA with selected indices 0,3, predicted by the head predictor. Similarly, the right diagram shows how an input y performs the sparse MLP with selected indices 0,2, predicted by the neuron predictor of that layer.

## Selected Head Index

$! ! = {0,3} Attention with " " ! # , " " ! $ , " " ! % $ % & ($) " " ! ' Output Projection )%* " ! ($) % ( ($) " " " ) ! * = {0,2} $ σ($" " " + ) " " " + MLP " " ($)$Selected Neurons Index Sparsified Attention Sparsified MLP Next, we will present a general explanation of two optimizations we used in DEJAVU implementation. Kernel fusion: A standard implementation of sparse matrix-vector multiply (e.g., W x in PyTorch) that separately indexes a subset of the matrix W [idx,:] before multiplying with input x would incur 3× the amount of memory IOs: one to load a subset of W from GPU memory, one to write that subset to a different contiguous region in memory, and one to load that (now contiguous) subset in again to multiply with x. Similarly, to use sparse matrix multiply routines (e.g., cuSparse), we would first need to convert W [idx,:] to sparse format, again incurring more memory IOs. We instead fuse the indexing and the multiplication step: we load a subset of W [idx,:] to memory, along with x, perform the multiply, then write down the result. This fused implementation (in Triton [(Tillet et al., 2019)](#b128)) yields up to 4× speedup compared to a standard PyTorch implementation (Section E). Memory coalescing: the weight matrices are conventionally stored in row-major format. This allows us to load W [idx,:] optimally (as the second dimension is contiguous in memory). However, for cases where we need to load W [:,idx] (attention output projection and the 2nd weight matrix in the MLP) this format significantly slows down memory loading, as idx could contain indices pointing to non-contiguous memory. A simple solution is to store these matrices in column-major format (i.e., storing W ⊤ in contiguous row-major format), then use the same fused kernel above. This transposition is done once when loading the model, and incurs no added cost during generation. We validate that our hardware-aware implementation of sparse MLP and sparse attention (Section 4.4) yields wall-clock speed up compared to both dense MLP/attention and compared to the standard implementation in PyTorch.

## E Benchmarking

Recall that our implementation fuses the sparse indexing and the multiplication (W 1 S M ) ⊤ y for weight matrices (W 1 ) ⊤ and input vector y. In cases where we need to index W 2 S M , we store the transpose of W 2 to ensure memory coalescing. For the baseline implementation in PyTorch, we index (W 1 S M ) ⊤ as a separate operation before multiplying with y, which incurs more memory reads/writes.

## Similarly, we fuse the sparse indexing and the multiplication (W

$Q S A ) ⊤ y, (W K S A ) ⊤ y, (W V S A ) ⊤ y for weight matrices (W Q ) ⊤ , (W K ) ⊤ , (W V ) ⊤$and input vector y. Note we usually concatenate all three matrices in the standard implementation, but we separate them here for clarity. In cases where we need to index W O S A , we store the transpose of W O to ensure memory coalescing. In Figure [13](#fig_8) and Figure [14](#fig_9), our sparse MLP and attention implementations are 4-5× faster than the baseline implementation in Pytorch, and remains faster than the dense version for density up to 0.8.

## F Notations and Basic Definitions

For a positive integer n, let [n] := {1,2,•••,n}. For a matrix A ∈ R n×n , let A i,: and A :,j be two column vectors corresponding to the i-th row and the j-th column of A respectively, and A i,j be the entry at the i-th row and the j-th column. For a vector x ∈ R n , let √ x ∈ R n denote the vector with the i-th entry being √ x i and diag(x) ∈ R n×n denote the diagonal matrix with the i-th diagonal entry being x i . For two matrices A,W ∈ R n×n , let ∥A∥ W := (

$n i=1 n j=1 W i,j A 2 i,j ) 1/2 and W •A denote the matrix where (W •A) i,j = W i,j A i,j . For matrix W ∈ R n×n , let D Wi := diag(W i,: ) with i ∈ [n].$For two vectors x ∈ R n and w ∈ R n ≥0 , let ∥x∥ w := ( n i=1 w i x 2 i ) 1/2 . For a vector x, we denote ∥x∥ 2 := ( ℓ 2 norm. We denote ∥x∥ p := ( n i=1 |x i | p ) 1/p as its ℓ p norm. For a square matrix A, we denote tr[A] as the trace of matrix A. For a matrix A ∈ R n×k (suppose n ≥ k), we use ∥A∥ to denote its spectral norm, i.e., ∥A∥ = sup x ∥Ax∥ 2 /∥x∥ 2 . We use ∥A∥ F to denote its Frobenius norm ∥A∥ F := ( n i=1 k j=1 A 2 i,j ) 1/2 . Suppose matrix A ∈ R n×k has SVD decomposition U ΣV ⊤ where U ∈ R n×k (this matrix has orthonormal columns), Σ ∈ R k×k is a diagonal matrix, and V ∈ R k×k . We call columns of U are singular vectors. We use A † ∈ R k×n to denote the Moore-Penrose pseudoinverse, then A † = V Σ -1 U ⊤ . Suppose Σ ∈ R k×k is sorted diagonal matrix, let σ 1 ,•••,σ k denote the diagonal entries of Σ. Then we call σ i the i-th singular value of matrix, and we write it as σ i (A).

For any symmetric matrix B ∈ R k×k , we define its eigenvalue decomposition as U ΛU ⊤ , where Λ is a diagonal matrix. Let λ 1 ,•••,λ k denote the entries on diagonal of Λ ∈ R k×k . We say λ i is the i-th eigenvalue. Usually we write it as λ i (B).

## The connection between eigenvalues and singular values is

$σ 2 i (A) = λ i (A ⊤ A)$We use notation A ⪰ 0 to denote that matrix A is positive semidefinite (psd). Mathematically, A ⪰ 0 means for all vectors x, we have x ⊤ Ax ≥ 0.

Similarly, for two squarer matrices A and B, we use A ⪰ B to denote the case where for all vectors x, x ⊤ Ax ≥ x ⊤ Bx.

We use Pr[] and E[] for probability and expectation. We denote max{a,b} as the maximum between a and b. We denote min{a,b} (resp. max{a,b}) as the minimum (reps. maximum) between a and b.

Throughout, for non-negative real numbers a and b, we use the notation a

$= (1±ϵ)b if a ∈ [(1-ϵ)b,(1+ϵ)b].$
## G Subspace Embeddings and Norm Preserving

In Section G.1, we show the norm preserving of the soft-max functions. In Section G.2, we show the norm preserving of the ReLU function. In Section G.3, we introduce the folded Guassian distribution. In Section G.4, we introduce the ℓ 2 subspace embedding. In Section G.5, we introduce the ℓ 1 subspace embedding. In Section G.6, we introduce different sketching matrices for subspace embedding.

## G.1 Soft-Max Functions

Let K ∈ R s×d and V ∈ R d×s . Inspired by the softmax unit in the attention scheme of large language models. The softmax related regression has been studied in many settings [(Zandieh et al., 2023;](#b141)[Alman & Song, 2023;](#b2)[Brand et al., 2023;](#b23)[Li et al., 2023b;](#b90)[Deng et al., 2023b;](#)[a;](#)[Gao et al., 2023a;](#)[Li et al., 2023a;](#)[Gao et al., 2023b)](#). In this work, we follow the standard softmax definition. We define σ 1 : R s → R s to be a softmax function, i.e., for any vector y ∈ R s , the σ(y) can be written as

$σ 1 (y) i = exp(y i ) d j=1 exp(y j ) , ∀i ∈ [d]$The standard softmax is ℓ 1 version. In this work, we also consider the ℓ 2 generalization. We define σ 2 : R s → R s to be a softmax function (ℓ 2 version), i.e., for any vector y ∈ R s , the σ(y) can be written as

$σ 2 (y) i = exp(y i ) ( d j=1 exp(2y j )) 1/2 , ∀i ∈ [d]$We define function f :

$R d → R d f (x) = V •(σ(K •x))(3)$Definition G.1. We say X ⊂ R d is a rank-k subspace, if there is an orthonormal basis U ∈ R d×k , for any x ∈ X , there is y ∈ R k such that x = U y.

## We can have

Lemma G.2. Let τ ∈ (0,1). Let X ⊂ R d denote a subspace with rank k. Let f be defined based on

$σ 2 function. Let V is a random Gaussian matrices with d ≥ Ω(ϵ -2 (k+log(1/δ))) rows. Let V = τ V , then we have with probability 1-δ (1-ϵ)τ ∥x∥ 2 ≤ ∥f (x)∥ ≤ (1+ϵ)τ ∥x∥ 2 . for all unit vectors x ∈ X . Further, if d = O(k+log(1/δ)), then we have 0.5τ ∥x∥ 2 ≤ ∥f (x)∥ ≤ 2τ ∥x∥ 2 .$Remark G.3. The above condition implies that f is a shrinking operator but also not shrinking arbitrarily small.

Proof. Given d ≥ Ω(ϵ -2 (k+log(1/δ))), by using Lemma G.11 , we have (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 As the input of the function f here is the output of a softmax function (ℓ 2 version), we know that ∥y∥ 2 = 1.

## Thus, we have

(1-ϵ) ≤ ∥V y∥ 2 ≤ (1+ϵ) By rescaling V , we have

$(1-ϵ)∥x∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥x∥ 2 . Lemma G.4. Let τ ∈ (0,1). Let X ⊂ R d denote a subspace with rank k. Let f be defined based on σ 1 function. Suppose V is a random Gaussian matrix with d ≥ Ω((k+log(1/δ))) rows. Let V = 1 2 τ V . Then we have 1 4 √ s τ •∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ τ •∥x∥ 2$for all unit vectors x.

Proof. By property of subspace embedding, we know that if d ≥ Ω(ϵ -2 (s+log(1/δ))), (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 By property of function of f , we know we only need to care ∥y∥ 1 = 1, this implies that

$1 √ s ∥y∥ 1 ≤ ∥y∥ 2 ≤ ∥y∥ 1$On one hand, we have

$∥V y∥ 2 ≤ (1+ϵ)•∥y∥ 2 ≤ (1+ϵ)•∥y∥ 1 = (1+ϵ),(4)$where the first step follows from ∥V y∥ 2 ≤ (1+ϵ)∥y∥ 2 , the second step follows from ∥y∥ 2 ≤ ∥y∥ 1 and the last step follows from ∥y∥ 1 = 1.

On the other hand, we have

$∥V y∥ 2 ≥ (1-ϵ)∥y∥ 2 ≥ 1 √ s (1-ϵ)∥y∥ 1 = 1 √ s (1-ϵ),(5)$where the first step follows from (1-ϵ)∥y∥ 2 ≤ ∥V y∥ 2 , the second step follows from 1 √ s ∥y∥ 1 ≤ ∥y∥ 2 and the last step follows from ∥y∥ 1 = 1.

Combining Eq. ( [5](#formula_22))and Eq. ( [4](#formula_21)) together, we have

$(1-ϵ) 1 √ s ≤ ∥V y∥ 2 ≤ (1+ϵ) Choosing ϵ = 1/2, we have 1 2 √ s ≤ ∥V y∥ 2 ≤ 2. By V = 1 2 τ V and ∥x∥ 2 = 1, we have 1 4 √ s τ ∥x∥ 2 ≤ ∥V y∥ 2 ≤ τ ∥x∥ 2 .$
## G.2 ReLU Functions

We use ϕ : R → R to denote ReLU function, i.e., ϕ(z) = max{z,0}.

We define function g :

$R d → R d g(x) = V •(ϕ(K •x))(6)$Let K ∈ R s×d and V ∈ R d×s .

Lemma G.5. Let X ⊂ R d denote a rank-k subspace. Let K denote a random Gaussian matrix. Let V denote a random Gaussian matrix. Let s ≥ Ω(ϵ -2 klog(1/(δϵ))). Let d ≥ Ω(ϵ -2 (k + log(1/δ))). Then we know with high probability 1 -δ, for all unit vector x ∈ X

$(1-ϵ)∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ)∥x∥ 2 Proof. Suppose s ≥ Ω(ϵ -2 log(1/δ)).$Using Lemma G.6, Fact G.7, we can show that for each fixed

$(1-ϵ)∥x∥ 2 ≤ ∥ϕ(Kx)∥ 2 ≤ (1+ϵ)∥x∥ 2 holds with probability 1-δ.$By a standard ϵ-net argument (Lemma G.9), the net points in X is at most (10/ϵ) O(k) .

Taking a union bound over all the net points, we can show that for all x ∈ X

(1-ϵ)∥x∥ 2 ≤ ∥ϕ(Kx)∥ 2 ≤ (1+ϵ)∥x∥ 2 holds with probability 1-δ/2 and s ≥ Ω(ϵ -2 klog(1/(δϵ))).

Further, we using Lemma G.11, we can show that

(1-ϵ)∥ϕ(Kx)∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ)∥ϕ(Kx)∥ 2 holds with probability 1-δ/2.

## Combining together,

(1-ϵ) 2 ∥x∥ 2 ≤ ∥f (x)∥ 2 ≤ (1+ϵ) 2 ∥x∥ 2 G.5 ℓ 1 subspace embedding When p = 1, using Cauchy random variables, Sohler and Woodruff [(Sohler & Woodruff, 2011)](#b119) showed there exist ℓ 1 oblivious subspace embeddings with O(dlogd) rows and κ = O(dlogd). This approach was generalized by using p-stable random variables in work of [Meng and Mahoney (Meng & Mahoney, 2013)](#b97) to ℓ p -norms when 1 < p < 2, where they showed there exist ℓ p oblivious subspace embeddings with O(dlogd) rows and κ = O((dlogd) 1/p ). Unlike the case when p = 2, due to the large distortion

In [(Wang & Woodruff, 2018)](#b133), they show for every 1 ≤ p < 2, any oblivious subspace embedding with dimension r has distortion κ = Ω(

$1 ( 1 d ) 1/p •log 2/p r+( r n ) 1/p-1/2$). They also give sparse oblivious subspace embeddings for every 1 ≤ p < 2 which are optimal in dimension and distortion, up to poly (logd) factors. Importantly for p = 1, they achieve r = O(dlogd),κ = O(dlogd) and s = O(logd) non-zero entries per column.

Definition G.10 (ℓ 1 subspace embedding). Let 0 < α < β be parameters. We will say a matrix S is an ℓ 1 subspace embedding for an n×d matrix A if there are constants c 1 ,c 2 > 0 so that for all x ∈ R d , ∥Ax∥ ≤ ∥SAx∥ 1 ≤ d c1 ∥Ax∥ 1 , and S has at most d c2 rows.

$G.6 Random Matrices Matrices b Time for R•A Reference Random Gaussian ϵ -2 (d+log(1/δ)) T mat (b,n,d) Thm. 6 of (Woodruff, 2014) SRHT ϵ -2 ( √ d+ √ logn) 2 log(d/δ) ndlog(ϵ -1 d(logn)) Thm. 7 of (Woodruff, 2014) AMS ϵ -2 (d+log(1/δ)) T mat (b,n,d) Follow from JL guarantee Count-sketch ϵ -2 δ -1 d 2 nnz(A) Thm. 9 of (Woodruff, 2014) Sparse embedding ϵ -2 d• poly log(d/(ϵδ)) ϵ -1 nnz(A) poly log(d/(ϵδ)) Thm. 10 (2) of (Woodruff, 2014) Sparse embedding ϵ -2 d 1+γ$ϵ -1 nnz(A)poly(1/γ) Thm. 10 (1) of [(Woodruff, 2014)](#b135) Table [9](#). Summary for different sketching matrices for subspace embedding. The sketching matrix R has size b×n. The vectors are from the column subspace of matrix A with size n×d.ϵ ∈ (0,1) is the error parameter, and δ ∈ (0,1) is the probability parameter. Tmat (a,b,c) denotes the running time of fast matrix multiplication of two matrices with size a×b and b×c. In the first sparse embedding matrix, each column has s ≥ ϵ -1 poly log(d/(ϵδ)) non-zero entries; In the second sparse embedding matrix, each column has s ≥ ϵ -1 poly (1/γ) non-zero entries, γ > 0 is a tunable parameter that gives different trade-offs, and δ can be as small as 1/ poly (d). For count-sketch matrices, the subspace embedding guarantee is proved from JL moment property, instead of directly from JL guarantee.

Lemma G.11 (Theorem 6 of [(Woodruff, 2014)](#b135)). Let 0 < ϵ,δ < 1 and S = 1 √ k R ∈ R k×n where the entries R i,j of R are independent standard normal random variables. Then if k = Θ(ϵ -2 (d+log(1/δ))), then for any fixed n×d matrix A, with probability 1-δ,S is a (1±ϵ)ℓ 2 -subspace embedding for A, that is, simultaneously for all x ∈ R d ,∥SAx∥ 2 = (1±ϵ)∥Ax∥ 2 . Here C > 0 is an absolute constant.

We consider several standard sketching matrices:

1. Random Gaussian matrices.

2. Subsampled randomized Hadamard/Fourier transform (SRHT) matrices [(Lu et al., 2013)](#b94).

3. AMS sketch matrices [(Alon et al., 1996)](#b4), random {-1,+1} per entry. 4. Count-Sketch matrices [(Charikar et al., 2002)](#b27), each column only has one non-zero entry, and is -1,+1 half probability each.

5. Sparse embedding matrices [(Nelson & Nguyên, 2013)](#b104), each column only has s non-zero entries, and each entry is

$-1 √ s ,+ 1 √ s half probability each.$6. Uniform sampling matrices.

Definition G.12 (Random Gaussian matrix). We say R ∈ R b×n is a random Gaussian matrix if all entries are sampled from N (0,1/b) independently.

Definition G.13 (Subsampled randomized Hadamard/Fourier transform matrix [(Lu et al., 2013)](#b94)). We say R ∈ R b×n is a subsampled randomized Hadamard transform (SRHT) matrix[foot_11](#foot_11) if it is of the form R = n/bSHD, where S ∈ R b×n is a random matrix whose rows are b uniform samples (without replacement) from the standard basis of R n ,H ∈ R n×n is a normalized Walsh-Hadamard matrix, and D ∈ R n×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.

Definition G.14 (AMS sketch matrix [(Alon et al., 1996)](#b4)). Let h 1 ,h 2 ,•••,h b be b random hash functions picking from a 4-wise independent hash family H = {h :

$[n] → {-1 √ b ,+ 1 √ b }}.$Then R ∈ R b×n is a AMS sketch matrix if we set R i,j = h i (j) Definition G.15 (Count-sketch matrix [(Charikar et al., 2002)](#b27)). Let h : [n] → [b] be a random 2-wise independent hash function and σ : [n] → {-1,+1} be a random 4-wise independent hash function. Then R ∈ R b×n is a count-sketch matrix if we set R h(i),i = σ(i) for all i ∈ [n] and other entries to zero. Definition G.16 (Sparse embedding matrix I [(Nelson & Nguyên, 2013)](#b104)). We say R ∈ R b×n is a sparse embedding matrix with parameter s if each column has exactly s non-zero elements being ±1/ √ s uniformly at random, whose locations are picked uniformly at random without replacement (and independent across columns)[foot_12](#foot_12) . Definition G.17 (Sparse embedding matrix II [(Nelson & Nguyên, 2013)](#b104)). Let h : [n] × [s] → [b/s] be a random 2-wise independent hash function and σ : [n]×[s] → {-1,1} be a 4-wise independent. Then R ∈ R b×n is a sparse embedding matrix II with parameter s if we set R (j-1)b/s+h(i,j),i = σ(i,j)/ √ s for all (i,j) ∈ [n]×[s] and all other entries to zero[foot_13](#foot_13) .

Definition G.18 (Uniform sampling matrix). We say R ∈ R b×n is a uniform sampling matrix if it is of the form R = n/bSD, where S ∈ R b×n is a random matrix whose rows are b uniform samples (without replacement) from the standard basis of R n , and D ∈ R n×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables.

## H Distances, Angles, and Inner Product

Most of the properties in this section are very standard in literature, e.g., see [(Gu et al., 2023)](#b61).

Let U ∈ R n×k denote an orthonormal basis, we use

$U ⊥ ∈ R n×(n-k) denote the matrix such that U U ⊤ +U ⊥ U ⊤ ⊥ = I n . Definition H.1. Let X ∈ R n×k and Y ∈ R n×k .$For any matrix X, and for orthogonal matrix

$Y (Y ⊤ Y = I k ) we define • tanθ(Y,X) := ∥Y ⊤ ⊥ X(Y ⊤ X) -1 ∥$For orthogonal matrices Y and X (Y ⊤ Y = I k and X ⊤ X = I k ), we define

• cosθ(Y,X) := σ min (Y ⊤ X).

-It is obvious that cos(Y,X) = 1/∥(Y ⊤ X) -1 ∥ and cos(Y,X) ≤ 1.

• sinθ(Y,X) := ∥(I -Y Y ⊤ )X∥.

-

$It is obvious that sinθ(Y,X) = ∥Y ⊥ Y ⊤ ⊥ X∥ = ∥Y ⊤ ⊥ X∥ and sinθ(Y,X) ≤ 1. • dist(Y,X) := min Q∈O k ∥Y Q-X∥$where O k is the set of k×k orthogonal matrices.

Lemma H.2 (Structural lemma for orthogonal matrices). Let X,Y ∈ R n×k be orthogonal matrices. Then

$(Y ⊤ X) ⊥ = Y ⊤ ⊥ X. Proof. Let us first compute the Gram of Y ⊤ X, which is X ⊤ Y Y ⊤ X = X ⊤ (I -Y ⊥ Y ⊤ ⊥ )X = X ⊤ X -X ⊤ Y ⊥ Y ⊤ ⊥ X = I k -X ⊤ Y ⊥ Y ⊤ ⊥ X,$We also have

$∥x+y∥ 2 ≤ ∥x∥ 2 +∥y∥ 2 ≤ 1+ϵ(7)$We have

$(1-ϵ) 2 ≥ 1-2ϵ(8)$We also have

$1 (1+ϵ) 2 ≥ 1-3ϵ(9)$where ϵ ∈ (0,0.1).

Combining Eq. ( [8](#formula_36)) and Eq. ( [9](#formula_37)), we have

$1 (1+ϵ) 2 •(1-ϵ) 2 ≥ (1-2ϵ)•(1-3ϵ) = 1-5ϵ+6ϵ 2 ≥ 1-5ϵ+ϵ = 1-4ϵ$(10) where the first step follows from Eq. ( [8](#formula_36)) and Eq. ( [9](#formula_37)) and the rest of them follow from simple algebra.

Finally, we have

$1-⟨x,z⟩ 2 = 1-⟨x, x+y ∥x+y∥ 2 ⟩ 2 = 1- 1 ∥x+y∥ 2 2 ⟨x,x+y⟩ 2 = 1- 1 ∥x+y∥ 2 2 •(∥x∥ 2 2 +⟨x,y⟩) 2 = 1- 1 ∥x+y∥ 2 2 •(1+⟨x,y⟩) 2 ≤ 1- 1 (1+ϵ) 2 •(1+⟨x,y⟩) 2 ≤ 1- 1 (1+ϵ) 2 •(1-ϵ) 2 ≤ 1-(1-4ϵ) = 4ϵ,$where the first step follow the definition of z, the second step follows from the reorganization, the third step follows from the definition of inner product, the fourth step follows from ∥x∥ 2 = 1, the fifth step follows from Eq. ( [7](#formula_35)), the sixth step follows from 1+⟨x,y⟩ ≥ 1-|⟨x,y⟩| ≥ 1-∥x∥ 2 •∥y∥ 2 ≥ 1-ϵ, the seventh step follows from Eq. ( [10](#)) and the last step follows from simple algebra.

## I Function Approximations

We first we show the function approximation for two operators in Section I.1, which means that there are two functions. Then we show the function approximations for four operators in Section I.2. Assume the the following conditions

$• Condition 1a. f 1 is a linear function • Condition 1b. ∥f 1 (x)∥ 2 ≤ ϵ 1 ∥x∥ 2 (f 1 is shrinking) • Condition 1c. ∥f 1 (x)-f 1 (y)∥ 2 ≤ L 1 ∥x-y∥ 2 (f 1 is Lipschitz) • Condition 2a. f 2 is a linear function • Condition 2b. ∥f 2 (x)∥ 2 ≤ ϵ 2 ∥x∥ 2 (f 2 is shrinking) • Condition 2c. ∥f 2 (x)-f 2 (y)∥ 2 ≤ L 2 ∥x-y∥ 2 (f 2 is Lipschitz)$We define three functions

$• g 1 (x) =: (I +f 1 )•(I +f 2 )(x) = x+f 2 (x)+f 1 (x+f 2 (x)) • g 2 (x) =: (I +f 2 )•(I +f 1 )(x) = x+f 1 (x)+f 2 (x+f 1 (x)) • g 3 (x) =: (I +f 1 +f 2 )(x) = x+f 1 (x)+f 2 (x)$Then we can show that

$• Part 1. ∥g 1 (x)-g 2 (x)∥ 2 ≤ 2ϵ 1 ϵ 2 ∥x∥ 2 (if f 1 and f 2 are linear functions) • Part 2. ∥g 1 (x)-g 2 (x)∥ 2 ≤ (ϵ 2 •L 1 +ϵ 1 •L 2 )∥x∥ 2 (if f 1 and f 2 are Lipschitz functions) • Part 3. ∥g 1 (x)-g 3 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 (if f 1 is a linear function) • Part 4. ∥g 1 (x)-g 3 (x)∥ 2 ≤ ϵ 2 •L 1 ∥x∥ 2 (if f 1 is a Lipschitz function) • Part 5. ∥g 2 (x)-g 3 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 (if f 2 is a linear function) • Part 6. ∥g 2 (x)-g 3 (x)∥ 2 ≤ ϵ 1 •L 2 ∥x∥ 2 (if f 2 is a Lipschitz function)$Proof. Part 1.

We have

$∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ ϵ 1 ϵ 2 ∥x∥ 2 +ϵ 1 ϵ 2 ∥x∥ 2 = 2ϵ 1 ϵ 2 ∥x∥ 2$where the first step follows from triangular inequality, the second step follows from Part 3 and Part 5 and the last step follows from simple algebra.

Part 2.

We have

$∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ ϵ 2 •L 1 ∥x∥ 2 +ϵ 1 •L 2 ∥x∥ 2 = (ϵ 2 •L 1 +ϵ 1 •L 2 )∥x∥ 2$where the first step follows from triangular inequality, the second step follows from Part 4 and Part 6 and the last step follows from simple algebra.

Part 3.

## We have ∥g

$1 (x)-g 3 (x)∥ 2 = ∥f 1 (x+f 2 (x))-f 1 (x)∥ 2 = ∥f 1 (x+f 2 (x)-x)∥ 2 = ∥f 1 (f 2 (x))∥ 2 ≤ ϵ 1 •∥f 2 (x)∥ 2 ≤ ϵ 1 •ϵ 2 •∥x∥ 2$, where the first step follows from the definition of g 1 and g 3 , the second step follows from the fact that f 1 is a linear function, the third step follows from simple algebra, the fourth step follows from Condition 1b and the last step follows from Condition 2b.

Part 4.

$∥g 1 (x)-g 3 (x)∥ 2 = ∥f 1 (x+f 2 (x))-f 1 (x)∥ 2 ≤ L 1 •∥x+f 2 (x)-x∥ 2 = L 1 •∥f 2 (x)∥ 2 ≤ L 1 •ϵ 2 ∥x∥ 2$, where the first step follows from definition of g 1 and g 3 , the second step follows from Condition 1c, the third step follows from simple algebra and the last step follows from Condition 2b.

Part 5.

## We have ∥g

$2 (x)-g 3 (x)∥ 2 = ∥f 2 (x+f 1 (x))-f 2 (x)∥ 2 = ∥f 2 (x+f 1 (x)-x)∥ 2 = ∥f 2 (f 1 (x))∥ 2 ≤ ϵ 2 •∥f 1 (x)∥ 2 ≤ ϵ 2 •ϵ 1 •∥x∥ 2$, where the first step follows from the definition of g 2 and g 3 , the second step follows from the fact that f 2 is a linear function, the third step follows from simple algebra, the fourth step follows from Condition 2b and the last step follows from Condition 1b. Part 6.

$∥g 2 (x)-g 3 (x)∥ 2 = ∥f 2 (x+f 1 (x))-f 2 (x)∥ 2 ≤ L 2 •∥x+f 1 (x)-x∥ 2 = L 2 •∥f 1 (x)∥ 2 ≤ L 2 •ϵ 1 ∥x∥ 2 ,$where the first step follows from definition of g 1 and g 3 , the second step follows from Condition 2c, the third step follows from simple algebra and the last step follows from Condition 1b.

## I.2 Function Approximations for Four Operators

Lemma I.2. For each i ∈ [4], we assume the following conditions

$• i(a) f i is a linear function • i(b) ∥f i (x)∥ 2 ≤ ϵ i ∥x∥ 2 (f i is shriking) • i(c) ∥f i (x)-f i (y)∥ 2 ≤ L i ∥x-y∥ 2 (f i is Lipschitz)$We define three functions

$• g 1 (x) := (I +f 1 )•(I +f 2 )•(I +f 3 )•(I +f 4 )(x) • g 2 (x) := (I +f 1 )•(I +f 3 )•(I +f 2 )•(I +f 4 )(x) • g 3 (x) := (I +f 1 +f 2 +f 3 +f 4 )(x)$Then, we can show that

$• Part 1. ∥g 1 (x)-g 2 (x)∥ 2 ≤ 2(ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 2. ∥g 1 (x) -g 2 (x)∥ 2 ≤ (2L 1 ϵ 2 + 2L 1 ϵ 3 + 2L 1 ϵ 4 + L 2 ϵ 3 + 2L 2 ϵ 4 + 2L 3 ϵ 4 + 2L 1 ϵ 2 ϵ 3 + 2L 1 ϵ 2 ϵ 4 + 2L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 2 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) • Part 3. ∥g 1 (x)-g 3 (x)∥ 2 ≤ (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 4. ∥g 1 (x) -g 3 (x)∥ 2 ≤ (L 1 ϵ 2 + L 1 ϵ 3 + L 1 ϵ 4 + L 2 ϵ 3 + L 2 ϵ 4 + L 3 ϵ 4 + L 1 ϵ 2 ϵ 3 + L 1 ϵ 2 ϵ 4 + L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 + L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) • Part 5. ∥g 2 (x)-g 3 (x)∥ 2 ≤ (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are linear functions) • Part 6.∥g 2 (x) -g 3 (x)∥ 2 ≤ (L 1 ϵ 2 + L 1 ϵ 3 + L 1 ϵ 4 + L 2 ϵ 4 + L 3 ϵ 2 + L 3 ϵ 4 + L 1 ϵ 2 ϵ 3 + L 1 ϵ 2 ϵ 4 + L 1 ϵ 3 ϵ 4 + L 3 ϵ 2 ϵ 4 + L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 (if f i , ∀i ∈ [4] are Lipschitz functions) Proof. Part 1.$We have

$∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ 2(ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2$where the first step follows from triangular inequality and the last step follows from Part 3 and Part 5.

Part 2.

We have

$∥g 1 (x)-g 2 (x)∥ 2 ≤ ∥g 1 (x)-g 3 (x)∥ 2 +∥g 3 (x)-g 2 (x)∥ 2 ≤ (2L 1 ϵ 2 +2L 1 ϵ 3 +2L 1 ϵ 4 +L 2 ϵ 3 +2L 2 ϵ 4 +2L 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 +2L 1 ϵ 2 ϵ 4 +2L 1 ϵ 3 ϵ 4 + L 2 ϵ 3 ϵ 4 +2L 1 ϵ 2 ϵ 3 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 2 ϵ 4$)∥x∥ 2 where the first step follows from triangular inequality and the last step follows from Part 4 and Part 6.

Part 3. We have ∥g

$1 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 2 )•(I +f 3 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+ f 3 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 3 (f 4 (x))+f 2 (f 4 (x)+f 3 (x+f 4 (x)))+f 1 (f 4 (x)+ f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))))∥ 2 = ∥f 3 (f 4 (x))+f 2 (f 4 (x))+f 2 (f 3 (x))+f 2 (f 3 (f 4 (x)))+ f 1 (f 4 (x))+f 1 (f 3 (x))+f 1 (f 3 (f 4 (x)))+f 1 (f 2 (x))+f 1 (f 2 (f 4 (x))) +f 1 (f 2 (f 3 (x)))+f 1 (f 2 (f 3 (f 4 (x)))))∥ 2 ≤ ∥f 3 (f 4 (x))∥ 2 +∥f 2 (f 4 (x))∥ 2 +∥f 2 (f 3 (x))∥ 2 +∥f 2 (f 3 (f 4 (x)))∥ 2 + ∥f 1 (f 4 (x))∥ 2 +∥f 1 (f 3 (x))∥ 2 +∥f 1 (f 3 (f 4 (x)))∥ 2 +∥f 1 (f 2 (x))∥ 2 +∥f 1 (f 2 (f 4 (x)))∥ 2 + ∥f 1 (f 2 (f 3 (x)))∥ 2 +∥f 1 (f 2 (f 3 (f 4 (x))))∥ 2 ≤ (ϵ 3 ϵ 4 +ϵ 2 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 4 +ϵ 1 ϵ 3 +ϵ 1 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 = (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 ,$where the first step follows from the definition of g 1 and g 3 , the second step follows from simple algebra, the third step follows from reorganization, the fourth step follows from the fact that all f i ,∀i ∈ [4] are linear function, the fifth step follows from triangular inequality, the sixth step follows from i(b) and the last step follows from reorganization.

Part 4. We have

$∥g 1 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 2 )•(I +f 3 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+ f 3 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x))) +f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))) -f 1 (x)-f 2 (x)-f 3 (x))∥ 2 = ∥f 3 (x+f 4 (x))-f 3 (x)+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))-f 2 (x) +f 1 (x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x))))-f 1 (x)∥ ≤ L 3 ∥x+f 4 (x)-x∥ 2 +L 2 ∥x+f 4 (x)+f 3 (x+f 4 (x))-x∥ 2 +L 1 ∥x+f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))-x∥ 2 ≤ L 3 ∥f 4 (x)∥ 2 +L 2 ∥f 4 (x)+f 3 (x+f 4 (x))∥ 2 +L 1 ∥f 4 (x)+f 3 (x+f 4 (x))+f 2 (x+f 4 (x)+f 3 (x+f 4 (x)))∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x+f 4 (x)∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x+f 4 (x)∥ 2 +L 1 ϵ 2 ∥x+f 4 (x)+f 3 (x+f 4 (x))∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x∥+L 2 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥ 2 +L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 3 ∥x+f 4 (x)∥ 2 ≤ L 3 ϵ 4 ∥x∥ 2 +L 2 ϵ 4 ∥x∥ 2 +L 2 ϵ 3 ∥x∥+L 2 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥ 2 +L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ϵ 3 ϵ 4 ∥x∥ 2 = (L 3 ϵ 4 +L 2 ϵ 4 +L 2 ϵ 3 +L 2 ϵ 3 ϵ 4 +L 1 ϵ 4 +L 1 ϵ 3 +L 1 ϵ 3 ϵ 4 +L 1 ϵ 2 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 = (L 1 ϵ 2 +L 1 ϵ 3 +L 1 ϵ 4 +L 2 ϵ 3 +L 2 ϵ 4 +L 3 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 3 ϵ 4 +L 2 ϵ 3 ϵ 4 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2$where the first step follows from the definition of g 1 and g 3 , the second step follows from simple algebra, the third step follows from simple algebra, the fourth step follows from reorganization, the fifth step follows from the fact that all f i ,∀i ∈ [4] are Lipschitz functions, the sixth step follows from simple algebra, the seventh step follows from i(b), the eighth step follows from triangular inequality, the ninth step follows from i(b), the tenth step follows from i(b) and the last step follows from reorganization.

Part 5. We have

$∥g 2 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 3 )•(I +f 2 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+ f 2 (x+f 4 (x)))+f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 2 (f 4 (x))+f 3 (f 4 (x))+ f 3 (f 2 (x+f 4 (x)$))+f 1 (f 4 (x))+f 1 (f 2 (x+f 4 (x)))+f 1 (f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))))∥ 2 ≤ (ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 3 ϵ 2 +ϵ 3 ϵ 2 ϵ 4 +ϵ 1 ϵ 4 +ϵ 1 ϵ 2 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 +ϵ 1 ϵ 3 ϵ 4 +ϵ 1 ϵ 3 ϵ 2 +ϵ 1 ϵ 3 ϵ 2 ϵ 4 )∥x∥ 2 = (ϵ 1 ϵ 2 +ϵ 1 ϵ 3 +ϵ 1 ϵ 4 +ϵ 2 ϵ 3 +ϵ 2 ϵ 4 +ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 +ϵ 1 ϵ 2 ϵ 4 +ϵ 1 ϵ 3 ϵ 4 +ϵ 2 ϵ 3 ϵ 4 +ϵ 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 , where the first step follows from the definition of g 2 and g 3 , the second step follows from simple algebra, the third step follows from the fact that all f i ,∀i ∈ [4] are linear function, the fourth step follows from triangular inequality and i(b), and the last step follows from reorganization. ))-f 1 (x)∥ 2 ≤ L 2 ϵ 4 ∥x∥ 2 +L 3 ϵ 4 ∥x∥ 2 +L 3 ϵ 2 ∥x+f 4 (x)∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x+f 4 (x)∥ 2 +L 1 ϵ 3 ∥x+f 4 (x)+f 2 (x+f 4 (x))∥ 2 ≤ L 2 ϵ 4 ∥x∥ 2 +L 3 ϵ 4 ∥x∥ 2 +L 3 ϵ 2 ∥x∥ 2 +L 3 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 4 ∥x∥ 2 +L 1 ϵ 2 ∥x∥ 2 +L 1 ϵ 2 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ∥x∥+L 1 ϵ 3 ϵ 4 ∥x∥ 2 +L 1 ϵ 3 ϵ 2 ∥x∥ 2 +L 1 ϵ 3 ϵ 2 ϵ 4 ∥x∥ 2 = (L 1 ϵ 2 +L 1 ϵ 3 +L 1 ϵ 4 +L 2 ϵ 4 +L 3 ϵ 2 +L 3 ϵ 4 +L 1 ϵ 2 ϵ 3 +L 1 ϵ 2 ϵ 4 +L 1 ϵ 3 ϵ 4 +L 3 ϵ 2 ϵ 4 +L 1 ϵ 2 ϵ 3 ϵ 4 )∥x∥ 2 where the first step follows from the definition of g 2 and g 3 , the second step follows from simple algebra, the third step follows from reorganization, the fourth step follows from triangular inequality, the fifth step follows from the fact that all f i ,∀i ∈ [4] are Lipschitz functions and i(b), the sixth step follows from triangular inequality, and the last step follows from reorganization.

## J Nearest Neighbor Search Data Structure

We use the reduction-based approximate MaxIP method with LSH data-structure to achieve sublinear iteration cost. Note that we choose this method due to its clear theoretical guarantee on the retrieval results. It is well-known that an LSH data-structures is used for approximate nearest neighbor problem. The following definition of approximate nearest neighbor search is very standard in literature [(Arya & Mount, 1993;](#b11)[Indyk & Motwani, 1998a;](#)[Datar et al., 2004;](#b41)[Andoni et al., 2014;](#b7)[2015;](#)[Andoni & Razenshteyn, 2015;](#)[Indyk & Wagner, 2018;](#b73)[Andoni et al., 2017;](#b9)[2018;](#b33)[Dong et al., 2019;](#b48)[Chen et al., 2020b;](#)[Li & Li, 2022;](#)[Li et al., 2019)](#b86).

## J.1 LSH and MaxIP

We start with the defining the Approximate Nearest Neighbor (ANN) problem [(Arya & Mount, 1993;](#b11)[Indyk & Motwani, 1998a;](#)[Datar et al., 2004;](#b41)[Andoni et al., 2014;](#b7)[2015;](#)[Andoni & Razenshteyn, 2015;](#)[Indyk & Wagner, 2018;](#b73)[Andoni et al., 2017;](#b9)[2018;](#b33)[Dong et al., 2019;](#b48)[Chen et al., 2020b)](#) as: Definition J.1 (Approximate Nearest Neighbor (ANN)). Let c > 1 and r ∈ (0,2) denote two parameters. Given an n-vector set Y ⊂ S d-1 on a unit sphere, the objective of the (c,r)-Approximate Nearest Neighbor (ANN) is to construct a data structure that, for any query x ∈ S d-1 such that min y∈Y ∥y-x∥ 2 ≤ r, it returns a vector z from Y that satisfies ∥z-x∥ 2 ≤ c•r.

The ANN problem can be solved via locality sensitive hashing (LSH) [(Indyk & Motwani, 1998a;](#)[Datar et al., 2004;](#b41)[Indyk & Wagner, 2018)](#b73). In this paper, we use the standard definitions of LSH (see [Indyk and Motwani (Indyk & Motwani, 1998a)](#)).

Definition J.2 (Locality Sensitive Hashing). Let c > 1 denote a parameter. Let p 1 ,p 2 ∈ (0,1) denote two parameters and p 1 > p 2 . We say a function family H is (r,c • r,p 1 ,p 2 )-sensitive if and only if, for any vectors x,y ∈ R d , for any h chosen uniformly at random from H, we have:

• if ∥x-y∥ 2 ≤ r, then Pr h∼H [h(x) = h(y)] ≥ p 1 ,

• if ∥x-y∥ 2 ≥ c•r, then Pr h∼H [h(x) = h(y)] ≤ p 2 .

Next, we show that LSH solves ANN problem with sublinear query time complexity.

Theorem J.3 [(Andoni, Laarhoven, Razenshteyn and Waingarten (Andoni et al., 2017)](#b9)). Let c > 1 and r ∈ (0,2) denote two parameters. One can solve (c,r)-ANN on a unit sphere in query time O(d•n ρ ) using preprocessing time O(dn 1+o(1) ) and space O(n 1+o(1) +dn), where ρ = 2 c 2 -1 c 4 +o(1).

time complexity. We refer readers to Section 8.2 in [(Shrivastava et al., 2021)](#b117) for more details[foot_15](#foot_15) . Moreover, Corollary J.9 could be applied to projected MaxIP problem.

Theorem J.10. Let c ∈ (0,1) and τ ∈ (0,1). Let ϕ,ψ : R d → R k denote two transforms. Let T ϕ denote the time to compute ϕ(x) and T ψ denote the time to compute ψ(y). Given a set of n-points Y ∈ R d with ψ(Y ) ⊂ S k-1 on the sphere, one can construct a data structure with O(dn 1+o(1) + T ψ n) preprocessing time and O(n 1+o(1) + dn) space so that for any query x ∈ R d with ϕ(x) ∈ S k-1 , we take query time complexity O(d•n ρ +T ϕ ) to solve (c,ϕ,ψ,τ )-MaxIP with respect to (x,Y ) with probability at least 0.9, where ρ := 2(1-τ ) 2 (1-cτ ) 2 -(1-τ ) 4 (1-cτ ) 4 +o(1).

Proof. The preprocessing phase can be decomposed in two parts.

• It takes O(T ψ n) time to transform every y ∈ Y into ψ(y).

• It takes O(O(dn 1+o(1) ) time and O(dn 1+o(1) +dn) to index every ψ(y) into LSH using Theorem J.9.

The query phase can be decomposed in two parts.

• It takes O(T ϕ ) time to transform every x ∈ R d into ϕ(x).

• It takes O(d•n ρ ) time perform query for ϕ(x) in LSH using Theorem J.9.

## K Self-attention layer as a clustering algorithm

The self-attention layer in the Transformer looks like mean-shift clustering. Suppose {(x j ,v j )} are a bunch of key and value pairs and q is the query. Note that q = W q x, k = W k x and v = W v x are computed by three projection matrices W k , W q and W v from a common x. Then from self-attention we have:

$v = j p j v j = j exp(x ⊺ W ⊺ q W k x j )W v x j j exp(x ⊺ W ⊺ q W k x j ) = W v j exp(x ⊺ W ⊺ q W k x j )x j j exp(x ⊺ W ⊺ q W k x j )(14)$where ∼ (q,k j ) := exp(q ⊺ k j ) = exp(x ⊺ W ⊺ q W k x j ) and p j =∼ (q,k j )/ j ∼ (q,k j ). On the other hand, mean-shift clustering looks like the following: m(x) = j K(x j ,x)x j j K(x j ,x)

where K(x j ,x) is a kernel matrix that measure the similarity between x j and x. According to the mean-shift algorithm, in the next iteration, we will simply replace x with m(x).

So in some sense, self-attention is just to do some kind of clustering for the input embedding q and k, plus a transformation of the embedding to another place. The term "projection" is due to the fact that there is a projection matrix W v on x for the next level.

Residue connection and LayerNorm. Compared to mean-shift, Transformer layer has residue connection. Therefore, for single-headed attention, what you actually get is v+x, followed by a LayerNorm. For the residue connection, the mean-shift analog already shows the output m(x) contains x+ part. The reason why we need residue connection is that the self-attention part might only model the "change" of x in the mean-shift picture, rather than the full update of x.

## L The role of self-attention

Consider we have a vocabulary of size m and d dimensional embedding space. In practice, many papers in NLP have reported clustering behaviors of word embeddings: such a clustering of word embedding naturally occurs after training.

An explanation for the above phenomenon is that, by grouping these word embedding together, we might generalize better, since similarity in word now can transfer (e.g., A linked to B, B linked to C, then A might link to C as well) and generalization follows.

Let's treat it as a fact and focus on how this is achieved and how self-attention plays a role here.

![Accuracy-Efficiency Trade-offs Figure1. (1) LLMs have up to 85% contextual sparsity for a given input. (2) Contextual sparsity has much better efficiency-accuracy trade-offs (up to 7×) than non-contextual sparsity or static sparsity.]()

![Figure2. DEJAVU uses lookahead predictors to side-step prediction costs: given the input to the attention layer at block k, they (asynchronously) predict the contextual sparsity for the MLP at block k, and given the input to the MLP at block k, they predict the sparsity for the attention head at the next layer.]()

![Figure 3. In Figure (a), we plot the percentage of not-activated attention heads. By only keeping heads that yield large output norms, we can silence over 80% attention heads for a given token. In Figure (b), we plot the average sparsity we impose on MLP layers.We can zero out over 95% of MLP parameters for a given token.]()

![Figure 4. We visualize the attention scores of three different heads for an exemplary sentence. Head 42 and Head 44 give heavy attention scores on particular tokens while Head 43 is more uniform.]()

![Figure 5. Slowly Changing Embedding. Figure (a) shows the median cosine similarity between representations at two consecutive layers across all layers for different OPT models. All models show a similarity greater than 95%. Figure (b) shows cosine similarity stays high even a few layers apart. For the residual connection X ′ = X +F (X) inside each block, we plot the ℓ2 norm of X and F (X) in Figure (c) and Figure (d).∥X∥ is significantly higher than ∥F (X)∥, which explains the slowly changing embedding.]()

![Figure6. Accuracy Trend for DEJAVU-OPT-175B. This figure shows the accuracy of DEJAVU-OPT-175B on language modeling datasets and downstream tasks when we set different sparsity at test time. In general, DEJAVU-OPT-175B incurs no accuracy drop until 75% sparsity.]()

![Figure10. Cosine similarity between X and F (X), and the cosine similarity between X and X ′ in orange color. L2 norm of X and F (X) and X after layer normalization in purple on the right. Except on the first layer, ∥X∥ is significantly higher than ∥F (X)∥. ∥F (X)∥ is higher at the first layer, which corresponds to the low cosine similarity at the first layer.]()

![Figure 12. Detailed diagram on the sparsified computation process of MLP and Attention. Notation refers to Section 2.3]()

![Figure13. Speed benchmarking of the MLP layer of OPT-175B on 8xA100s. Our sparse implementation is up to 4.5× faster than the baseline implementation in PyTorch. Our sparse MLP implementation remains faster than dense MLP for density up to 0.8.]()

![Figure14. Speed benchmarking of the attention layer of OPT-175B on 8xA100s. Our sparse implementation is up to 5× faster than the baseline implementation in PyTorch. Our sparse attention implementation remains faster than dense MLP for density up to 0.8.]()

![Function Approximations for Two Operators Lemma I.1. Let f 1 : R d → R d and let f 2 : R d → R d .]()

![We have ∥g 2 (x)-g 3 (x)∥ 2 = ∥(I +f 1 )•(I +f 3 )•(I +f 2 )•(x+f 4 (x))-(I +f 1 +f 2 +f 3 +f 4 )(x))∥ 2 = ∥(x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))) +f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))) -(I +f 1 +f 2 +f 3 +f 4 )(x)))∥ 2 = ∥f 2 (x+f 4 (x))-f 2 (x)+f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))-f 3 (x) +f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))))-f 1 (x)∥ 2 ≤ ∥f 2 (x+f 4 (x))-f 2 (x)∥ 2 +∥f 3 (x+f 4 (x)+f 2 (x+f 4 (x)))-f 3 (x)∥ 2 +∥f 1 (x+f 4 (x)+f 2 (x+f 4 (x))+f 3 (x+f 4 (x)+f 2 (x+f 4 (x))]()

![Theoretical breakdown for prompting versus token generation (tensor model parallelism on 8 A100-80G GPUs).]()

![Theoretical breakdown for Attention block versus MLP block in one transformer layer when generating one token (tensor model parallelism on 8 A100-80G GPUs).]()

![Latency breakdown of generating 1 token under the setting of batch size 1 and prompt length 128 on 8 A100-80GB.]()

![Accuracy of zero-shot tasks and language modeling when sparsifying the MLP block and the Attention block separately. The sparsity is set at 85% for MLP-block and 50% for Attention-block. DEJAVU incurs no accuracy drop across the boards.]()

![DEJAVU-OPT66B on zero-shot downstream task.]()

![DEJAVU-BLOOM on zero-shot downstream task.]()

![DEJAVU-OPT-175B with 4-bit quantization.]()

![Sparsify from the Depth: Skipping or parallel entire transformer blocks may not lead to catastrophic drop in accuracy at test time.]()

Rice University

Zhe Jiang University

Stanford University

University of California, San Diego

ETH Zurich

Adobe Research

Meta AI (FAIR)

Carnegie Mellon University. Correspondence to: Zichang Liu <zl71@rice.edu>, Tri Dao <trid@stanford.edu>, Tianyi Zhou <t8zhou@ucsd.edu>, Zhao Song <zsong@adobe.com>, Beidi Chen <beidic@andrew.cmu.edu>.Proceedings of the40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).

http://github.com/NVIDIA/FasterTransformer

http://github.com/huggingface/transformers

We remark that sketching technique has widely applied to many applications such as linear regression, low-rank approximation[(Clarkson & Woodruff, 2013;](#b36)[Nelson & Nguyên, 2013;](#b104)[Lu et al., 2013;](#b94)[Boutsidis et al., 2016;](#b20)[Cohen, 2016;](#b37)[Razenshteyn et al., 2016;](#b114)[Song et al., 2017;](#b122)[2019)](#b0), linear programming[(Song & Yu, 2021;](#b121)[Dong et al., 2021;](#b47)[Jiang et al., 2021;](#b76)[Gu & Song, 2022)](#b60), semi-definite programming[(Gu & Song, 2022;](#b60)[Song et al., 2023b)](#b90), empirical risk minimization[(Lee et al., 2019;](#b85) Qin et al., 2023b), training over-parameterized neural network[(Brand et al., 2021;](#b22) Song et al., 2021;[Alman et al., 2022;](#b3)[Hu et al., 2022;](#b70)[Zhang, 2022)](#b142).

In this case, we require logn o be an integer.

For our purposes the signs need only be O(logd)-wise independent, and each column can be specified by a O(logd)-wise independent permutation, and the seeds specifying the permutations in different columns need only be O(logd)-wise independent.

This definition has the same behavior as sparse embedding matrix I for our purpose

It is obvious to boost probability from constant to δ by repeating the data structure log(1/δ) times.

Recently, there a line of work that use fast MaxIP data structure to speedup the iterative-type optimization algorithms[(Shrivastava et al., 2021;](#b117)[Song & Ye, 2023;](#b120) Qin et al., 2023a; Song et al., 2023a).

