<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maximize Your Data&apos;s Potential: Enhancing LLM Accuracy with Two-Phase Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-18">18 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Steven</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
							<email>sprabhumoye@nvidia.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Nvidia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stanford</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maximize Your Data&apos;s Potential: Enhancing LLM Accuracy with Two-Phase Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-18">18 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">E7AC0D9727D27BB0AAAD923F95343FF5</idno>
					<idno type="arXiv">arXiv:2412.15285v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLM) are typically pretrained on large amounts of data in the order of billions (B) or trillions (T) of tokens derived from multiple data sources such as web crawl, books, papers, patents, mathematical and legal documents, and so forth <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr">Parmar et al., 2024b;</ref><ref type="bibr">Team et al., 2024b;</ref><ref type="bibr">Dubey et al., 2024a;</ref><ref type="bibr" target="#b32">Nvidia et al., 2024)</ref>. To develop a state-of-the-art model, it is critical to understand the nature of these data sources and to make informed decisions about optimal data blending (how different data sources are weighed during pretraining) and training strategies. These decisions typically involve * equal contribution † Work done during internship at NVIDIA running multiple large-scale experiments to empirically investigate the optimal training data blend(s) and ordering of data.</p><p>Most advanced models <ref type="bibr" target="#b34">(OpenAI et al., 2024;</ref><ref type="bibr">Dubey et al., 2024b)</ref> do not divulge information on the data blends that are used, nor the ablation studies informing the data mixing and ordering decisions. Recent works <ref type="bibr" target="#b4">(Blakeney et al., 2024;</ref><ref type="bibr" target="#b17">Groeneveld et al., 2024;</ref><ref type="bibr">Dubey et al., 2024b;</ref><ref type="bibr" target="#b45">Snowflake, 2024)</ref> provide high-level data blend information about a small portion of pretraining by encouraging the upsampling of certain domains towards the end. In general, there exists a knowledge gap regarding how to craft and choose an optimal data blend(s) for the entire training process, and the generalizability of data blends and ordering strategies to larger token horizons and model sizes.</p><p>In this work, we address the above knowledge gap by understanding optimal data blends and ordering strategies for training LLM. We formalize and extensively explore a two-phase training approach (Figure <ref type="figure" target="#fig_0">1</ref>) that balances diversity and quality: phase-1 emphasizes diverse, high-quality web crawl data, while phase-2 focuses on high-quality data sources such as math, code, and wiki data. Specifically, in this work we propose to use downsampled data to prototype and explore multiple blends at a smaller scale of 1T tokens. We craft our blends based on quality of the data source and the number of epochs to be seen during pretraining. We then demonstrate the effectiveness of our approach at a 15T token scale using the full data.</p><p>We evaluated on a comprehensive set of downstream tasking covering knowledge, reasoning, coding and math benchmarks. Our experiments illustrate that a quality and epoch based blend is better than a blend based on natural distribution by 13.2% and the two-phase approach is better than random ordering of data (blend is based on quality and epochs) by an average of 3.4% across downstream tasks. Furthermore, our results on downsampled data generalize across longer 15T token horizons on full data and larger model sizes, demonstrating the scalability and robustness of the two-phase approach. We also provide a fine-grained quality analysis of web crawl data, revealing optimal blending strategies to balance diversity and quality.</p><p>We share and highlight a series of findings made to create blends and order in our two-phase approach. Our main contributions are:</p><p>• Formalization and large-scale evaluation of the two-phase training approach for LLMs, with actionable strategies that enable effective LLM pretraining. • Improving the understanding of data selection and blending with quality-based and epochbased analyses of data, including web crawl. • Demonstration of the scalability of blends using downsampled data at 1T to using full data at 15T tokens and larger model size of 25B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Two-Phase Approach to Pretraining</head><p>In this work, we explore a two-phased approach to pretraining: phase-1 (P 1 ) then phase-2 (P 2 ). Figure <ref type="figure" target="#fig_0">1</ref> demonstrates our two-phased approach. In each phase, we explore different data blends based on the quality and number of epochs to be seen of a data source. In phase-1 (P 1 ), we explore a general data distribution which consists of a mix of web crawl data, medium-quality data, and low amounts of high-quality data. In phase-2 (P 2 ), we explore a blend which includes task data and emphasizes high-quality datasets such as math, code, and high-quality web crawl ( §5.1). Estimate the quality of a data source ( §5.1), 3) Estimate the epochs to be seen in the whole pretraining ( §5.2) and finally 4) distribute the epochs appropriately in P 1 and P 2 ( §3.2). The downsampling factor f is based on the final total token budget which we assume to be 15T similar to <ref type="bibr">Dubey et al. (2024b)</ref>. Hence, for us f = 1/15 i.e for each data source, the number of tokens available for pretraining is 1/15 th of the total token in that dataset. Downsampling helps to observe the impact of epochs of datasets at a smaller scale of 1T tokens and then can be used to scale the blend to a longer token horizon of 15T tokens using the full data.</p><p>Baselines: Since our blends are based on quality and epoch based analyses of the data as well as the ordering of the data in the two phases, we consider the following two baselines: 1) Natural Distribution Blend (BASE-ND): This blend is based on ratio of the number of tokens available in each data source. The weight for each dataset is equal to the total number of tokens in that dataset divided by the sum of tokens available in all the datasets. This weighting is neither based on quality nor the epochs to be seen for the dataset. 2) Random Order Pretraining (BASE-RO): This blend is based on quality and epochs of each dataset but does not use two phases to train the model. The weight for each dataset here is the same as our two-phase approach but the order in which the the dataset is seen during pretraining is random.</p><p>3 Experimental Setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sources</head><p>Our pretraining corpus spans a vast range of text data sources that cover several domains, types of data, and languages. We broadly divide our datasets into the following categories and their token counts in billions is shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Category Domain Blend1 Blend2 Blend3 Blend4 Blend5 Web Crawl -65.0 65.0 58.0 59.0 70.0 High Quality Math 1.9 1.9 1.9 2.9 1.9 Wiki 0.1 0.1 0.1 0.1 0.1 Code 15.0 8.0 15.0 20.0 13.0 Medium Quality Books 5.5 9.0 9.0 5.5 4.5 Papers 3.5 5.0 5.0 3.5 1.9 CC dv 4.0 6.0 6.0 4.0 3.6 Multilingual -5.0 5.0 5.0 5.0 5.0</p><p>Table 2: Phase-1 Blends (in %)</p><p>• Web Crawl: Data derived from Common Crawl (CC). We discuss the quality of this data and how to blend it in detail in §5.1. • High-Quality: This includes datasets from more specialized and professional domains such as mathematics <ref type="bibr" target="#b37">(Paster et al., 2024;</ref><ref type="bibr">Stack Exchange, Accessed 2024)</ref>, code <ref type="bibr" target="#b25">(Li et al., 2023)</ref>, and Wikipedia (wiki) data. • Medium-Quality: Data derived from books &amp; patents, papers <ref type="bibr" target="#b14">(Gao et al., 2020)</ref>, and Common Crawl derivatives (CC dv ) such as OpenWebText <ref type="bibr" target="#b16">(Gokaslan and Cohen, 2019)</ref>, BigScience <ref type="bibr" target="#b22">(Laurençon et al., 2022)</ref>, Reddit <ref type="bibr" target="#b2">(Baumgartner et al., 2020)</ref>, and CC-News. This category was determined by comparing this data to medium-quality crawl (see §5.1).</p><p>• Multilingual: Multilingual data (9 languages) derived from Wikipedia and Common Crawl. • Task Data: This includes data used for supervised finetuning (SFT) during the alignment phase <ref type="bibr" target="#b53">(Toshniwal et al., 2024;</ref><ref type="bibr" target="#b32">Nvidia et al., 2024)</ref>. We also include the FLAN collection <ref type="bibr" target="#b27">(Longpre et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Blends for Each Phase</head><p>The final blends in P 1 and P 2 are based on quality and epoch based ablations shown in §5.1 and §5.2. The insights from these studies are incorporated in Table <ref type="table">2</ref> and <ref type="table">3</ref>.</p><p>In P 1 , we encourage diversity in data by including a high percentage of web crawl data which consists of high, medium, and low-quality crawl. We want to introduce a limited amount of high-quality data such as math, code, and wiki in P 1 . In P 2 , the emphasis is primarily on high-quality datasets and only includes a limited amount of medium-quality data. For example, in P 2 , we only use high-quality crawl instead of medium or low-quality (see §5.1).</p><p>Table 2 details the five blends explored in P 1 . These blends are designed to compare the proportion of high-level categories with each other. The Category Domain Blend1 Blend2 Blend3 Blend4 Blend5 Web Crawl -31.0 35.0 31.0 40.0 35.0 High Quality Math 24.0 24.0 24.0 24.0 29.0 Wiki 1.0 1.0 1.0 1.0 1.0 Code 20.0 25.0 29.0 20.0 20.0 Medium Quality Multilingual -3.7 3.7 3.7 3.7 3.7 Task Data -1.3 1.3 1.3 1.3 1.3 Table 3: Phase-2 Blends (in %) difference between Blend1 and Blend2 is that Blend2 has less code and more medium-quality datasets compared to Blend1. Blend3 has less web crawl and more medium-quality datasets compared to Blend1. Blend4 has less web crawl and more high-quality datasets compared to Blend1.</p><p>Blend5 is designed to have majority web crawl at the cost of code and medium-quality data. Table <ref type="table">3</ref> outlines the five blends explored in P 2 . In P 2 , we use more epochs and higher proportions of high-quality data such as high-quality web crawl, math, wiki, and code data. Blend3 has more code and less medium-quality datasets compared to Blend1, and Blend4 has more high-quality web crawl and less medium-quality datasets compared to Blend1. Blend2 has a more balanced distribution among the data categories, while Blend5 upsamples math data more heavily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Specifications</head><p>We experiment using the Megatron <ref type="bibr" target="#b44">(Shoeybi et al., 2020)</ref> model, an autoregressive causal left-to-right LLM, with the Tiktokenizer <ref type="bibr" target="#b33">(OpenAI, 2023)</ref>. We downsample all our data by factor f = 1/15. Hence, only 1/15 of the tokens shown in Table <ref type="table" target="#tab_0">1</ref> will be available for pretraining. We perform all our investigations using an 8 billion parameter model trained on 1 trillion total tokens. Furthermore, we test our two-phase approach by scaling along two dimensions: (1) we scale the token horizon to 1.7T tokens on a 8B model, and (2) we scale the parameters of the model to 25B and train on 1T tokens. Additionally, we train a 8B model on 15T tokens on full data (not downsampled) to observe if decisions made with downsampled data scales. Specifics on model architecture and hyperparameters are shared in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Suite</head><p>To comprehensively assess our models, we use various benchmarks that evaluate different capabilities. These can be broadly divided into the following 4 categories, of which we report the final averages.</p><p>We assess 5-shot accuracy for MMLU <ref type="bibr" target="#b19">(Hendrycks et al., 2021)</ref>, 0-shot accuracy 1 for reasoning tasks: CommonsenseQA <ref type="bibr" target="#b50">(Talmor et al., 2019)</ref>, ARC-Easy &amp; Challenge <ref type="bibr" target="#b7">(Clark et al., 2018)</ref>, PIQA <ref type="bibr" target="#b3">(Bisk et al., 2019)</ref>, WinoGrande <ref type="bibr" target="#b40">(Sakaguchi et al., 2019)</ref>, Hel-laSwag <ref type="bibr" target="#b59">(Zellers et al., 2019)</ref>, OpenBookQA <ref type="bibr" target="#b30">(Mihaylov et al., 2018)</ref>, RACE <ref type="bibr" target="#b21">(Lai et al., 2017)</ref>, 0shot accuracy for code benchmarks: HumanEval (+) <ref type="bibr" target="#b6">(Chen et al., 2021)</ref> and MBPP (+) <ref type="bibr" target="#b1">(Austin et al., 2021)</ref>, and 8-shot chain-of-thought (CoT) accuracy for GSM8K <ref type="bibr" target="#b8">(Cobbe et al., 2021)</ref>. We also report a final overall Avg. for most results, which is an average over all individual evaluation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results for Two-Phase Pretraining Findings</head><p>• A two-phase approach for pretraining is effective.</p><p>• Phase-1 should focus on data diversity and phase-2 on high-quality data.</p><p>We compare our best blends P 1 -Blend4-P 2 -Blend1 2 using two-phase training with two baselines: 1) BASE-ND: the weights are determined by the tokens available in each dataset and are not based on quality, and 2) BASE-RO: the weights for all the datasets are the same in this and P 1 -Blend4-P 2 -Blend1. The only difference is the order in which the data is presented during training (random or two-phased). Table <ref type="table" target="#tab_4">4</ref> illustrates that using a quality and epoch based blend is on average 13.2% better than natural distribution blend (compare BASE-RO vs BASE-ND) across downstream tasks. It also presents that using our two-phase training approach noticeably improves average accuracy by 3.4% compared to BASE-RO and 17%</p><p>1 We use normalized accuracy for ARC-Easy, ARC-Challenge, PIQA, HellaSwag, and OpenBookQA.</p><p>2 see §4.1 Tab. 7 on how we select this blend and §5.3 Tab. 14 for the duration of P2 for best results.  compared to BASE-ND. This empirically demonstrates that the strategy of two-phase training is useful and tasks such as code and math are sensitive to the ordering of high-quality data in the second phase. We scale our best blend P 1 -Blend4-P 2 -Blend1 to 15T tokens and use the full dataset to train a 8B model. All the previous experiments are performed on downsampled data and 1T scale. This means that the number of epochs is constant in both the runs. Table <ref type="table" target="#tab_5">5</ref> shows that blends crafted at smaller scale can generalize to longer token budgets if the quality and epochs of the datasets are maintained at scale. This shows the generalizability of our twophase approach to pretraining as well as qualityand epoch-based approach to designing blends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Determining Blends</head><p>As discussed in §3.2, we explore five different blends for phase-1. <ref type="foot" target="#foot_0">3</ref> We train an 8B model on downsampled data for 1T tokens for all five blends and eliminate blends based on a separately held-out validation split. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the validation loss for all five blends. As we can see, Blend5 and Blend2 had 2.8% and 2.1% higher validation loss, respectively, relative to Blend4 at approx. 250B tokens. Hence, we discontinue these two blends at Table 7: Evaluation results after P 2 of training.</p><p>that point. Since, the validation loss of the remaining three blends was within a margin of 1%, we periodically evaluate their accuracy on downstream tasks. Table <ref type="table" target="#tab_6">6</ref> shows the results of the remaining three phase-1 blends at various token counts.</p><p>At each token evaluation point -200B, 250B and 629B, we see that Blend3 is consistently worse than the other two blends. Hence, we eliminate this blend after 629B tokens of training. For this experiment, we switch from P 1 to P 2 after ≈70% of training, i.e the last 30% of training is P 2 . In §5.3, we explore varying the percentage of P 2 . Results in Table <ref type="table" target="#tab_6">6</ref> follow intuition since Blend4 has the highest amount of high-quality data and is hence better than Blend1 and Blend3. Blend3 has more medium-quality data at the cost of web crawl compared to Blend1. This result confirms that books, papers, and CC dv are of medium-quality compared to our high-quality datasets and our web crawl blend.</p><p>Finally, we explore five different blends of P 2 described in Table <ref type="table">3</ref> in combination with P 1 -Blend1 and P 1 -Blend4. Hence, we have ten different combinations of P 1 and P 2 blends. Table <ref type="table">7</ref> shows the results on all ten combinations of blends. We find that P 1 -Blend4-P 2 -Blend1 performs the best on average. Table <ref type="table">7</ref> also presents the final results of P 1 -Blend1 and P 1 -Blend4 if only the P 1 blend was continued for 1T tokens without ever switching to P 2 blends. It shows that switching to any of the P 2 blends for training is better than continuing the P 1 blends for all metrics. We observe the largest absolute gains in GSM8K and code of 14.6% and 9.4%, respectively, for P 1 -Blend4-P 2 -Blend1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scaling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Findings</head><p>• Two-phase approach is scalable and robust to token horizon and model scale.</p><p>• Data blends need adjusting at longer token horizons based on epoch count to avoid high-quality data overexposure.</p><p>We further explore scaling our best blend along two dimensions: (1) a longer token horizon of 1.7 trillion tokens and (2) larger model size of 25B parameters. For a longer token horizon, we aim to assess whether the blend can be used as is or if adjustments are necessary to prevent overfitting (observed in §5.2). Note that this is different from scaling to 15T token where we use the full data. Here we still use the downsampled data and scale to 1.7T tokens and hence the epochs seen of each dataset would be higher. Since high number of epochs of high-quality datasets are primarily seen in P 2 of pretraining, we create a new blend, P 2 -Blend6<ref type="foot" target="#foot_1">foot_1</ref> , which is an epoch-adjusted version of P 2 -Blend1 to ensure that we do not see more than 8 epochs of certain high-quality data sources like math and task data. Table <ref type="table" target="#tab_8">8</ref> shows the comparison of scaling from 1T to 1.7T total tokens. We see that P 1 -Blend4-P 2 -Blend6 is on average 2.2% better than P 1 -Blend4-P 2 -Blend1, illustrating that we need to adjust our blends according to the epoch counts of high-quality data for optimal results. Both the 1.7T models are better than 1T, demonstrating that we can still obtain higher downstream accuracies Validation Loss phase1 phase2 by training on more tokens, even if it means training on more than 8 epochs of high-quality data.</p><p>We also investigate if our best blend can scale to a larger model size. Given the high number of epochs of high-quality data in P 1 -Blend4-P 2 -Blend1, we also want to determine if a model with a larger capacity might memorize the data and overfit on it. Figure <ref type="figure" target="#fig_2">3</ref> shows that the validation loss is always decreasing for the 25B model, indicating that there is no overfitting with P 1 -Blend4-P 2 -Blend1. Table <ref type="table" target="#tab_9">9</ref> shows results for the 25B model compared to the 8B model on this data blend combination. Understandably, the 25B model is substantially better across the board, demonstrating that the two-phase training approach and data blend combination can also scale to larger model sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablations</head><p>This section details quality-based data blending, epoch study and percentage of phase-2 to be conducted in the pretraining. Additional fine-grained analyses of data blends and study on learning rate schedule to be used in phase-2 is shown in Appendix §D and §E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quality-Based Data Blending</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insights</head><p>• Upsampling high quality and not using low quality CC data is most effective.</p><p>• CC dv , papers and books are similar in quality to CC-Medium-High.</p><p>The data blends of our two-phase approach are mainly based on the assessment of each data source's quality. Hence, we carry out extensive experiments to find an optimal data blend for web crawl documents. While previous work <ref type="bibr">(Dubey et al., 2024a;</ref><ref type="bibr" target="#b58">Yang et al., 2024;</ref><ref type="bibr">Team et al., 2024a)</ref> mentions that web crawl documents like Common Crawl (CC) form a large majority of their pretraining data, none of them share a recipe on how to mix different slices of CC. Some recent work on constructing crawl-based pretraining datasets <ref type="bibr">(Penedo et al., 2024b;</ref><ref type="bibr">Li et al., 2024a)</ref> directly use the high quality crawl documents in pretraining but provide no specific data mixing strategy. In this section, we provide comprehensive details on how to create a data blend for CC documents and use it effectively in our phase-1 and phase-2 of pretraining. Additionally, we provide a quality assessment of other datasets like CC dv , papers, books and our high quality datasets. We compare them with medium, and high quality web crawl to position them optimally in our P 1 and P 2 blends.</p><p>Quality-Based Blending for Web Crawl: Each document in our web crawl data is classified into one of five quality categories: High, Medium-High, Medium, Medium-Low, and Low using the classifier from <ref type="bibr" target="#b48">Su et al. (2024)</ref>.</p><p>We investigate various blends using qualitybased weighted sampling approach<ref type="foot" target="#foot_2">foot_2</ref> for all of web Table 12: Results of different quality crawl and their comparison with other datasets. Since code data is not included in most of these experiments, we exclude code evaluation.</p><p>crawl data from 99 CC snapshots for our phase-1 of pretraining. The idea is to upsample high and medium-quality crawl documents while avoiding a high quantity of low-quality data. The overall idea for the four web crawl blends in Table <ref type="table" target="#tab_10">10</ref> is to iteratively decrease the percentage of tokens from High and Medium-High and increase the tokens in the lower categories. The results in Table <ref type="table" target="#tab_11">11</ref> 6 demonstrate that eliminating the tail-end of the web crawl data belonging to Medium-Low and Low quality categories is beneficial as opposed to to keeping them for diversity. Based on these results, we choose CC-Blend1 as the final data blend for web crawl documents to be used in all our final P 1 blends ( §4 and Table <ref type="table">2</ref>). For P 2 , we only use web crawl data that belongs to High-quality category.</p><p>Quality Estimation of Other Datasets: We assess how our CC dv , papers, books and high quality datasets such as math, code and Wiki compare to CC-Medium,CC-Medium-High and CC-High quality crawl data. We continue training the last checkpoint of P 1 -Blend4, for an additional 50B tokens using a data mix that consisted 66% of the data being tested, mixed with 34% of CC-High.</p><p>The results in Table <ref type="table" target="#tab_0">12</ref> show that CC dv , papers and books datasets have similar accuracies to CC-Medium-High on the majority of benchmarks, and lag behind CC-High. As such, we group them under the "medium-quality" data category for our exper- 6 The average in this table is primarily based on reasoning tasks and MMLU because these blends do not have math or code data.  iments (see §3.1). The high quality datasets have an average accuracy better than CC-High.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Epoch-Based Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insights</head><p>• We recommend 6 epochs of highquality crawl and 8 epochs of math and task data for data mixing.</p><p>We take the number of epochs of high quality datasets into account while creating our P 1 and P 2 blends. We experiment with different numbers of epochs for high-quality crawl, math, and task data.</p><p>Since, majority of web crawl is used in P 1 , we pretrained an 8B model with 1T tokens, using different epochs of high-quality crawl tokens in the data mix, and evaluate each model's MMLU score. Note that we keep the overall percentage of web crawl the same in all the experiments. As we can see in Figure <ref type="figure" target="#fig_3">4</ref>, increasing the number of highquality tokens increases the MMLU score until 6 epochs. We primarily present MMLU score because these experiments do not include high amount of math or code data.</p><p>Since, majority of math and task-data is seen in P 2 , Table <ref type="table" target="#tab_13">13</ref>  of epochs for them in P 2 . It shows that ≈ 8 epochs of math is a good balance while not sacrificing accuracy on MMLU and reasoning. For task data, all metrics generally improve with more epochs, although there appears to be diminishing returns on several past epoch 8. Note that 8 epochs of both math and task data corresponds to our best P 1 -Blend4-P 2 -Blend1 blend combo from §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Optimal Duration of Phase-2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insights</head><p>• Pretraining with the P 2 blend for the final 40% gives the best results.</p><p>We investigate the percentage of phase-2 to use in the whole pretraining regime. We experiment with 0 to 50% of P 2 in the whole of pretraining. The longer the duration of P 2 , the shorter the duration of P 1 . We use the P 1 -Blend4-P 2 -Blend1 blend combination that we found best in §4.</p><p>Table <ref type="table" target="#tab_14">14</ref> illustrates that a higher percentage of P 2 until 40% is better overall, especially in math and code. Going above this, e.g. to P 2 as 50% of training, downstream accuracies start to degrade across the board, potentially due to overfitting.  <ref type="bibr">(2024a)</ref> demonstrating that refined data selection impacts model accuracies more significantly than simply the quantity of data. But these studies are primarily aimed at CC data and they do not suggest any data mixing strategies for pretraining. <ref type="bibr">Parmar et al. (2024a)</ref> provide a systematic approach to building effective LLM pretraining datasets with ablations on data attributes, and existing curation, selection, and sampling methods. In our work, we provide a systematic approach to craft data blends and to order the data in pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Strategic weighting and timing of data usage can also noticeably impact model accuracies. Techniques like domain upsampling <ref type="bibr" target="#b4">(Blakeney et al., 2024;</ref><ref type="bibr">Dubey et al., 2024b)</ref> towards the end of training have been shown to be effective. Snowflake (2024); <ref type="bibr" target="#b17">Groeneveld et al. (2024)</ref> provide details about high level blends for their pretraining process. In contrast, our work provides fine grained details about the data blend creation process along with actionable steps that model developers can use to develop data blends and order. Prior work <ref type="bibr" target="#b43">(Shen et al., 2023;</ref><ref type="bibr" target="#b28">Longpre et al., 2024;</ref><ref type="bibr" target="#b31">Mindermann et al., 2022;</ref><ref type="bibr">Xie et al., 2023a,b;</ref><ref type="bibr" target="#b41">Shao et al., 2024)</ref> investigates optimizing data mixtures based on clustering methods, manually designed domain composition weights, proxy models or reference models to determine data composition weights and sample-level data selection. Our work primarily focuses on data ordering and scaling of data blends in pretraining and can be used in conjunction with other data sampling techniques.</p><p>Curriculum learning approaches inspired by human learning offer an ordered way to introduce data gradually to enhance model learning. <ref type="bibr" target="#b29">Martinez et al. (2023)</ref>; <ref type="bibr" target="#b54">Wang et al. (2022)</ref>; <ref type="bibr" target="#b13">Feng et al. (2024)</ref> investigate cognitively-motivated curriculum-based training including vocabulary, and objective curricula, and outline and the challenges and potential solutions for designing effective curricula. Our work shows that ordering of data based on quality in pretraining LLMs has a significant impact of downstream accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, through extensive experiments, we demonstrate the effectiveness of a two-phase pretraining approach for LLM. For the initial training phase, a more general data distribution consisting of mainly of web crawl proves most effective, while phase two benefits from a comprehensive data blend, with additional focus on math, code, and task data. Phase-two for the last ≈40% of training yields the best results, and over-extending it leads to diminishing returns. Increasing model size and token horizon further enhances accuracy, demonstrating the scalability of our approach. Importantly, we also show that considering both the quality of the data (including web crawl) and the number of epochs of each data source is crucial to attain optimal results and prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Some limitations of our work include our present suite of models and evaluation benchmarks. We can extend our work and show the effectiveness of two-phase pretraining approach on more LLM architectures such as Mamba <ref type="bibr" target="#b18">(Gu and Dao, 2023)</ref>, other hybrid SSM based architectures <ref type="bibr" target="#b15">(Glorioso et al., 2024;</ref><ref type="bibr" target="#b26">Lieber et al., 2024)</ref> and mixture of experts <ref type="bibr" target="#b42">(Shazeer et al., 2017)</ref>. While our evaluation benchmarks are quite comprehensive, we could potentially expand to an even broader range of evaluations, including nuanced domain-specific or interactive tasks, or more theory of mind and developmental psychology-inspired benchmarks. This includes assessing capabilities such as analogical reasoning <ref type="bibr" target="#b55">(Webb et al., 2023)</ref>. Further, our scaling experiments could be expanded. Scaling up to hundreds of billions of parameters or significantly longer training may yield additional insights. Lastly, while our work focuses on two-phase training and shows its efficacy, we can potentially investigate multi-phase training, and the impact of the order of the phases. However, we believe this is more suited for future work. Overall, these are directions to potentially improve and expand upon our work. Despite these potential limitations, we feel that our current work is an insightful and useful contribution to the research community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Our research uses publicly available and commonly used datasets in LLM development. These sources, including Common Crawl, Wikipedia, and code repositories, are widely adopted in the research community. We examined the quality and origins of our data, prioritizing high-quality, domainrelevant data sources to improve LLM capabilities in a responsible manner. However, web crawl data may inherently contain biases or inappropriate content despite filtering efforts. We used established data cleaning and quality assurance procedures but acknowledge that potential biases may persist and impact model behavior in certain circumstances.</p><p>We recognize that scaling models and exploring data blending strategies require significant computational resources, which may raise environmental concerns. To mitigate this, we focused on efficient training strategies, such as two-phase training, to improve accuracy without excessively increasing resource usage. Future studies could benefit from exploring energy-efficient training methods to further minimize the environmental impact.</p><p>Our models, data blends, and accompanying publication are intended solely for research purposes, with no intended real-world application without additional safety evaluations. We caution against deploying models based on our methods without thorough testing, as they may carry unknown risks, particularly when applied to tasks involving sensitive or personal information. Our work aims to advance the understanding of LLM training strategies, and we feel that it is an important contribution to the research community. We encourage researchers to expand upon our work while further investigating the ethical and societal implications of LLM.</p><p>Tables 18 to 25 contain detailed evaluation results of the major experiments reported in §4 for reasoning, MMLU, and code, broken down by individual categories and benchmarks. They correspond to the results found in Tables 7 to 9 in §4.</p><p>Table <ref type="table" target="#tab_5">15</ref> shows the comparison of Blend1 and Blend6 used in scaling experiments in Table <ref type="table" target="#tab_8">8</ref>. If we use the same Blend1 as is and train for more number of tokens (1.7T) then the number of epochs seen of each dataset would be higher compared to 1T training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Quality-Based Data Blend</head><p>We first compare a baseline blend (ND) which uses the natural distribution of tokens with a smartly constructed weighted sampling blend (WS). ND is based on the number of tokens that belong in each category as opposed to utilizing the quality label i.e. if 59% of the tokens belong to Low then 59% of tokens seen during pretraining would be- long to Low. We then create a data blend (WS) based on weighted sampling of high and mediumquality tokens. The idea is to upsample high and medium-quality crawl documents and not use the low-quality data at all. Table <ref type="table" target="#tab_15">16</ref> shows the token percentages that belong to each of the five quality labels for both ND and WS blends. Table <ref type="table" target="#tab_0">17</ref> illustrates the results of the two models trained on the ND and WS data blends of web crawl, respectively. We see that our data blend (WS) outperforms on most of the evaluation tasks by a large margin, and the improvement on MMLU is substantial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Finegrained P 2 Blend Experiments</head><p>We investigate fine-grained P 2 blends to determine the optimal blend. For these experiments, we use a model trained on a P 1 blend for 900B tokens (10% P 2 duration), with a linear LR decay to 0.</p><p>Crawl, Math, &amp; Code: We investigate different percentages of high-quality crawl, math, and code data as shown in Table <ref type="table" target="#tab_6">26</ref>. Table <ref type="table" target="#tab_8">28</ref> demonstrates that a higher amount of math data (i.e. 30%) helps across the board. However, code data results are mixed, as too much code without enough math (CMC-B1) seems to hurt all non-code metrics. Comparing (CMC-B2) vs. (CMC-B3), more than 15% code does not add as much value, as gains saturate. Trading off crawl data for more code data also slightly hurts MMLU. As such, we decide that a final blend consisting of a higher amount of crawl and math with a moderate amount of code seems best overall. This corresponds to CMC-B3 in Table <ref type="table" target="#tab_6">26</ref>, which consists of 30% crawl, 33% math, and 15% code.</p><p>Task Data: Second, we investigate the inclusion of task data. Specifically, adding FLAN and synthetically-generated GSM8K-train data (similar to data augmentation approaches <ref type="bibr" target="#b12">(Feng et al., 2021)</ref>) to the CMC-B3 blend. Our FLAN data consists of a mixture of normal FLAN and FLAN-CoT (chain-of-thought) data. We compare 10 and 20 epochs of FLAN. These blends can be found in Table <ref type="table">27</ref>, with the results in Table <ref type="table" target="#tab_8">28</ref>. We can see that including synthetic GSM8K-train and FLAN data noticeably improves GSM8K scores while not detrimenting the other benchmarks. In fact, FLAN data also helps further improve MMLU and reasoning. 20 epochs of FLAN seems better than 10 epochs overall. Hence, including task data for P 2 of training seems to be a good idea.</p><p>All Data Mixture: Lastly, we investigate a final P 2 data mixture which is a combination of all the data sources we tried, including FLAN, GSM8K, and relatively higher amounts of math and code data. For this experiment, we use 30% upsampling with LR cosine decay to 3e -6. This blend can be found in Table <ref type="table">27</ref>, with the results at the bottom of Table <ref type="table" target="#tab_8">28</ref>. <ref type="foot" target="#foot_3">7</ref> We find that mixing all data sources helps greatly with GSM8K, noticeably with coding and reasoning, while retaining accuracy on MMLU. Hence, the final P 2 blends we investigate in §4 (Table <ref type="table">3</ref>) are motivated by these ablations -they are blends of all data sources, including task data, with higher proportions of math and code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Annealing Learning Rate Schedule</head><p>We investigate different learning rate (LR) schedules for phase P 2 . Specifically, using the same P 2 blend for a 10% duration, we try different LR strategies. We compare cosine vs. linear LR decay functions, and also compare decaying to a final LR of 0 vs. 3e-6 (1% of the original P 1 starting LR of 3e-4). Not decaying LR entirely to 0 leaves room for post-training, which is likely preferable.</p><p>As seen in Table <ref type="table" target="#tab_9">29</ref>, there is a negligible difference between linear and cosine LR decay, so we choose cosine decay for consistency with P 1 . We also see that LR decay to 3e-6 is comparable to decaying all the way to 0, while leaving room for post-training. Hence, our final chosen annealing strategy is cosine LR decay to 3e-6, which we use for our final two-phase experiments in §4. Table 21: MMLU and code evaluation results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 vs. a randomized mixture of both blends across the entire 1T token training run, broken down by individual category/benchmark. Corresponds to Table 4 in §4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of our two phase training pipeline.Phase-1 blend encourages data diversity and phase-2 blend is focused on high quality datasets.</figDesc><graphic coords="1,306.14,219.91,218.26,105.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Phase-1 validation loss for different P 1 blends.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Validation loss for the 25B model using twophase training with P 1 -Blend4-P 2 -Blend1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MMLU accuracy (%) vs. number of epochs of high-quality crawl in the data mix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>Selecting and structuring pretraining datasets is important to improve model generalization and efficiency. Dubey et al. (2024b) emphasize openness and accessibility of models, Computer (2023); Soldaini et al. (2024) assemble an open corpus of trillions of tokens for large-scale training, and Groeneveld et al. (2024) release a truly Open Language Model, including its framework, training data, and code. Studies such as Li et al. (2024b); Penedo et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Tokens (billions) in each data domain.</figDesc><table><row><cell cols="2">Data Domain Tokens (B)</cell></row><row><cell>Web Crawl</cell><cell>6244.3</cell></row><row><cell>Math</cell><cell>161.5</cell></row><row><cell>Wiki</cell><cell>16.7</cell></row><row><cell>Code</cell><cell>760.3</cell></row><row><cell>Books</cell><cell>776.3</cell></row><row><cell>Papers</cell><cell>212.6</cell></row><row><cell>CC dv</cell><cell>348.3</cell></row><row><cell>Multilingual</cell><cell>1457.2</cell></row><row><cell>Task Data</cell><cell>6.6</cell></row></table><note><p>As seen in</p><p>Figure 1, our model sees the first general data blend during P 1 for the majority of training, then a different data blend focused on high quality data during the shorter P 2 of training.</p><p>The steps to create blends for P 1 and P 2 are: 1) Downsample a data source by a factor of f , 2)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our two-phase training approach with BASE-ND and BASE-RO.</figDesc><table><row><cell>BASE-ND</cell><cell>49.78</cell><cell>56.48</cell><cell>19.64 24.96 45.17</cell></row><row><cell>BASE-RO</cell><cell>56.49</cell><cell>59.69</cell><cell>30.86 35.55 51.12</cell></row><row><cell>Two-Phase</cell><cell>56.28</cell><cell>60.34</cell><cell>40.33 38.33 52.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Tok. MMLU Reason. GSM8K Code Avg.Results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 for downsampled data at 1T and then complete data at 15T.</figDesc><table><row><cell cols="2">1T</cell><cell>56.28</cell><cell>60.34</cell><cell>40.33 38.33 52.86</cell></row><row><cell cols="2">15T</cell><cell>70.30</cell><cell>64.11</cell><cell>64.82 46.38 59.84</cell></row><row><cell>Validation Loss</cell><cell>1.45 1.50 1.55 1.60</cell><cell></cell><cell></cell><cell>phase1-blend1 phase1-blend2 phase1-blend3 phase1-blend4 phase1-blend5</cell></row><row><cell></cell><cell>1.40</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.35</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>200</cell><cell cols="2">400 Tokens (billion) 600</cell><cell>800</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>P 1 results after various token counts (billions).</figDesc><table><row><cell>Exp.</cell><cell cols="4">Tokens MMLU Reason. GSM8K Code Avg.</cell></row><row><cell>P 1 -Blend1</cell><cell>200</cell><cell>34.72</cell><cell>52.83</cell><cell>6.14 16.43 38.81</cell></row><row><cell>P 1 -Blend3</cell><cell>200</cell><cell>36.78</cell><cell>51.97</cell><cell>6.22 15.18 38.10</cell></row><row><cell>P 1 -Blend4</cell><cell>200</cell><cell>38.70</cell><cell>53.81</cell><cell>11.30 18.21 40.48</cell></row><row><cell>P 1 -Blend1</cell><cell>250</cell><cell>42.51</cell><cell>54.52</cell><cell>7.51 16.14 40.35</cell></row><row><cell>P 1 -Blend3</cell><cell>250</cell><cell>40.41</cell><cell>53.87</cell><cell>8.11 15.62 39.72</cell></row><row><cell>P 1 -Blend4</cell><cell>250</cell><cell>42.76</cell><cell>54.99</cell><cell>10.16 19.29 41.66</cell></row><row><cell>P 1 -Blend1</cell><cell>629</cell><cell>51.93</cell><cell>57.83</cell><cell>14.94 22.97 45.28</cell></row><row><cell>P 1 -Blend3</cell><cell>629</cell><cell>52.44</cell><cell>57.74</cell><cell>15.39 22.43 45.15</cell></row><row><cell>P 1 -Blend4</cell><cell>629</cell><cell>52.78</cell><cell>58.11</cell><cell>18.27 24.24 46.07</cell></row><row><cell>Exp.</cell><cell></cell><cell cols="3">MMLU Reason. GSM8K Code Avg.</cell></row><row><cell>P 1 -Blend1</cell><cell></cell><cell>55.00</cell><cell>59.12</cell><cell>20.09 23.56 46.76</cell></row><row><cell>P 1 -Blend4</cell><cell></cell><cell>56.25</cell><cell>59.54</cell><cell>23.43 27.61 48.40</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend1</cell><cell>56.04</cell><cell>60.04</cell><cell>37.00 36.19 51.88</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend2</cell><cell>55.88</cell><cell>60.15</cell><cell>36.85 35.89 51.84</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend3</cell><cell>55.80</cell><cell>60.08</cell><cell>39.80 35.75 51.96</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend4</cell><cell>56.15</cell><cell>60.26</cell><cell>36.85 36.30 51.88</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend5</cell><cell>56.49</cell><cell>60.41</cell><cell>36.92 34.40 51.65</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend1</cell><cell>56.58</cell><cell>60.18</cell><cell>37.98 37.01 52.28</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend2</cell><cell>56.89</cell><cell>60.00</cell><cell>36.62 36.97 52.10</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend3</cell><cell>56.10</cell><cell>60.08</cell><cell>39.27 35.15 51.78</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend4</cell><cell>57.03</cell><cell>59.98</cell><cell>36.85 36.30 51.93</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend5</cell><cell>56.92</cell><cell>60.35</cell><cell>38.29 34.56 51.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Scaling results for 1.7T tokens vs. 1T tokens, with and without high-quality data epoch adjustment.</figDesc><table><row><cell>Exp.</cell><cell cols="4">Tok. MMLU Reason. GSM8K Code Avg.</cell></row><row><cell cols="2">P1-Blend4-P2-Blend1 1T</cell><cell>56.58</cell><cell>60.18</cell><cell>37.98 37.01 52.28</cell></row><row><cell cols="2">P1-Blend4-P2-Blend1 1.7T</cell><cell>56.61</cell><cell>60.88</cell><cell>42.15 37.62 53.28</cell></row><row><cell cols="2">P1-Blend4-P2-Blend6 1.7T</cell><cell>59.85</cell><cell>61.63</cell><cell>43.90 39.61 54.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Model Size MMLU Reason. GSM8K Code Avg.Evaluation results for 8B vs. 25B parameter models, using the same blend: P 1 -Blend4-P 2 -Blend1. Note that we use a maximum sequence length of 8192 (instead of 4096) for both models here.</figDesc><table><row><cell>8B</cell><cell>57.31</cell><cell>61.16</cell><cell>45.11 38.97 53.92</cell></row><row><cell>25B</cell><cell>65.97</cell><cell>63.29</cell><cell>59.14 45.57 58.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>CC blends (in %) by quality. For CC-Blend3 and CC-Blend4, we merged the Medium and Medium-Low categories. Token column refers to the the natural distribution of tokens, i.e. percentage of total CC data that belongs to each category.</figDesc><table><row><cell cols="6">Quality Label Token CC-Blend1 CC-Blend2 CC-Blend3 CC-Blend4</cell></row><row><cell>High</cell><cell>35.96%</cell><cell>57.0</cell><cell>57.0</cell><cell>51.5</cell><cell>45.0</cell></row><row><cell cols="2">Medium-High 8.56%</cell><cell>25.0</cell><cell>25.0</cell><cell>23.5</cell><cell>20.0</cell></row><row><cell cols="2">Medium Medium-Low 15.41% 34.25%</cell><cell>18.0 0.0</cell><cell>13.0 2.0</cell><cell>23.0</cell><cell>32.0</cell></row><row><cell>Low</cell><cell>5.82%</cell><cell>0.0</cell><cell>3.0</cell><cell>2.0</cell><cell>3.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>P 1 results using our various CC blends.</figDesc><table><row><cell>Exp.</cell><cell cols="4">MMLU Reason. GSM8K Code Avg.</cell></row><row><cell>CC-Blend1</cell><cell>57.09</cell><cell>61.16</cell><cell cols="2">13.42 19.78 46.01</cell></row><row><cell>CC-Blend2</cell><cell>56.69</cell><cell>61.77</cell><cell cols="2">14.18 19.56 45.11</cell></row><row><cell>CC-Blend3</cell><cell>56.29</cell><cell>60.74</cell><cell cols="2">14.25 18.44 44.17</cell></row><row><cell>CC-Blend4</cell><cell>55.73</cell><cell>60.57</cell><cell cols="2">14.31 18.50 44.06</cell></row><row><cell>Dataset</cell><cell cols="4">MMLU Reason. GSM8K Avg.</cell></row><row><cell>CC-Medium</cell><cell></cell><cell>52.86</cell><cell>56.00</cell><cell>18.04 42.30</cell></row><row><cell cols="2">CC-Medium-High</cell><cell>53.75</cell><cell>58.09</cell><cell>18.50 43.45</cell></row><row><cell>CC-High</cell><cell></cell><cell>55.82</cell><cell>59.65</cell><cell>20.85 45.44</cell></row><row><cell>High Quality</cell><cell></cell><cell>54.53</cell><cell>58.51</cell><cell>24.11 45.72</cell></row><row><cell>CC dv</cell><cell></cell><cell>54.47</cell><cell>58.20</cell><cell>20.14 44.27</cell></row><row><cell>Books</cell><cell></cell><cell>55.36</cell><cell>58.93</cell><cell>18.50 44.26</cell></row><row><cell>Papers</cell><cell></cell><cell>54.55</cell><cell>58.65</cell><cell>19.41 44.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Results of varying the number of epochs of math and task data during P 2 of training.</figDesc><table><row><cell>Domain</cell><cell cols="4">Epochs MMLU Reason. GSM8K Code Avg.</cell></row><row><cell>Math</cell><cell>1</cell><cell>57.06</cell><cell>60.51</cell><cell>36.47 33.55 51.49</cell></row><row><cell>Math</cell><cell>4</cell><cell>57.01</cell><cell>60.32</cell><cell>38.21 35.20 51.92</cell></row><row><cell>Math</cell><cell>8</cell><cell>56.58</cell><cell>60.18</cell><cell>37.98 37.01 52.28</cell></row><row><cell>Math</cell><cell>12</cell><cell>56.09</cell><cell>59.69</cell><cell>38.29 35.70 51.63</cell></row><row><cell>Task Data</cell><cell>1</cell><cell>56.37</cell><cell>59.39</cell><cell>34.50 30.57 49.84</cell></row><row><cell>Task Data</cell><cell>4</cell><cell>56.57</cell><cell>59.46</cell><cell>40.18 35.27 51.53</cell></row><row><cell>Task Data</cell><cell>8</cell><cell>56.58</cell><cell>60.18</cell><cell>37.98 37.01 52.28</cell></row><row><cell>Task Data</cell><cell>12</cell><cell>56.77</cell><cell>59.96</cell><cell>38.44 36.04 51.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>presents results for different numbersP 2 % MMLU Reason. GSM8K Code Avg.Results of different durations of P 2 using P 1 -Blend4-P 2 -Blend1.</figDesc><table><row><cell>0</cell><cell>56.10</cell><cell>61.81</cell><cell>16.60 16.22 37.68</cell></row><row><cell>10</cell><cell>56.52</cell><cell>59.70</cell><cell>33.13 32.55 50.48</cell></row><row><cell>20</cell><cell>56.54</cell><cell>59.93</cell><cell>40.16 34.29 51.58</cell></row><row><cell>30</cell><cell>56.58</cell><cell>60.18</cell><cell>37.98 37.01 52.28</cell></row><row><cell>40</cell><cell>56.28</cell><cell>60.34</cell><cell>40.33 38.33 52.86</cell></row><row><cell>50</cell><cell>55.94</cell><cell>59.82</cell><cell>37.68 36.86 51.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Data blends for CC Quality estimation experiment. The overall percentage of the Common Crawl Snapshots in our experiments is fixed at 73.3%.</figDesc><table><row><cell cols="2">Quality Label</cell><cell>ND</cell><cell>WS</cell><cell></cell></row><row><cell>High</cell><cell></cell><cell>0.01</cell><cell>0.04</cell><cell></cell></row><row><cell cols="2">Medium-High</cell><cell>1.08</cell><cell>6.42</cell><cell></cell></row><row><cell cols="2">Medium</cell><cell cols="2">7.01 41.83</cell><cell></cell></row><row><cell cols="2">Medium-Low</cell><cell cols="2">26.46 25.09</cell><cell></cell></row><row><cell>Low</cell><cell></cell><cell>64.44</cell><cell>0.00</cell><cell></cell></row><row><cell cols="5">Data Mixing MMLU Reason. GSM8K Code</cell></row><row><cell>ND</cell><cell>42.94</cell><cell>59.40</cell><cell>8.11</cell><cell>19.25</cell></row><row><cell>WS</cell><cell>56.10</cell><cell>61.60</cell><cell>11.98</cell><cell>17.41</cell></row><row><cell cols="5">Table 17: Our WS: weighted sampling data mix-</cell></row><row><cell cols="5">ing method outperforms the ND: natural distribution</cell></row><row><cell>method.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 18 :</head><label>18</label><figDesc>Exp. ARC-Easy ARC-Challenge RACE PIQA WinoGrande HellaSwag OpenBookQA CommonsenseQA Avg. Final reasoning evaluation results after P 2 of training, broken down by individual benchmark. Corresponds to Table 7 in §4.</figDesc><table><row><cell>P 1 -Blend1</cell><cell>75.97</cell><cell>51.19</cell><cell cols="2">36.36 80.96</cell><cell>67.64</cell><cell>76.23</cell><cell>44.00</cell><cell cols="2">53.07</cell><cell>59.12</cell></row><row><cell>P 1 -Blend4</cell><cell>77.23</cell><cell>53.24</cell><cell cols="2">36.46 80.47</cell><cell>68.35</cell><cell>76.48</cell><cell>44.40</cell><cell cols="2">53.15</cell><cell>59.54</cell></row><row><cell>P 1 -Blend1-P 2 -Blend1</cell><cell>78.32</cell><cell>51.54</cell><cell cols="2">36.75 79.76</cell><cell>66.54</cell><cell>76.44</cell><cell>43.80</cell><cell cols="2">61.67</cell><cell>60.04</cell></row><row><cell>P 1 -Blend1-P 2 -Blend2</cell><cell>78.79</cell><cell>53.07</cell><cell cols="2">35.69 80.79</cell><cell>67.09</cell><cell>76.52</cell><cell>43.40</cell><cell cols="2">61.18</cell><cell>60.15</cell></row><row><cell>P 1 -Blend1-P 2 -Blend3</cell><cell>79.29</cell><cell>53.16</cell><cell cols="2">36.27 79.76</cell><cell>66.77</cell><cell>76.43</cell><cell>42.80</cell><cell cols="2">61.43</cell><cell>60.08</cell></row><row><cell>P 1 -Blend1-P 2 -Blend4</cell><cell>78.37</cell><cell>52.99</cell><cell cols="2">36.65 80.30</cell><cell>66.93</cell><cell>76.67</cell><cell>43.80</cell><cell cols="2">61.92</cell><cell>60.26</cell></row><row><cell>P 1 -Blend1-P 2 -Blend5</cell><cell>79.21</cell><cell>52.56</cell><cell cols="2">36.75 80.63</cell><cell>67.25</cell><cell>76.64</cell><cell>44.00</cell><cell cols="2">61.83</cell><cell>60.41</cell></row><row><cell>P 1 -Blend4-P 2 -Blend1</cell><cell>80.30</cell><cell>54.95</cell><cell cols="2">35.50 79.98</cell><cell>68.35</cell><cell>76.55</cell><cell>43.80</cell><cell cols="2">56.59</cell><cell>60.18</cell></row><row><cell>P 1 -Blend4-P 2 -Blend2</cell><cell>79.92</cell><cell>54.10</cell><cell cols="2">35.50 80.20</cell><cell>67.96</cell><cell>76.75</cell><cell>43.80</cell><cell cols="2">56.35</cell><cell>60.00</cell></row><row><cell>P 1 -Blend4-P 2 -Blend3</cell><cell>79.92</cell><cell>54.27</cell><cell cols="2">35.12 80.20</cell><cell>67.56</cell><cell>76.39</cell><cell>44.20</cell><cell cols="2">57.08</cell><cell>60.08</cell></row><row><cell>P 1 -Blend4-P 2 -Blend4</cell><cell>79.92</cell><cell>54.27</cell><cell cols="2">35.89 80.47</cell><cell>67.25</cell><cell>76.79</cell><cell>43.80</cell><cell cols="2">56.18</cell><cell>59.98</cell></row><row><cell>P 1 -Blend4-P 2 -Blend5</cell><cell>79.29</cell><cell>54.44</cell><cell cols="2">37.03 79.98</cell><cell>67.80</cell><cell>76.92</cell><cell>44.80</cell><cell cols="2">57.41</cell><cell>60.35</cell></row><row><cell>Exp.</cell><cell></cell><cell></cell><cell>MMLU</cell><cell></cell><cell></cell><cell></cell><cell>Code</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="10">STEM Humanities Social Sciences Others Avg. HumanEval HumanEval+ MBPP MBPP+ Avg.</cell></row><row><cell>P 1 -Blend1</cell><cell>45.61</cell><cell>50.24</cell><cell>65.29</cell><cell cols="2">61.54 55.00</cell><cell>18.90</cell><cell>13.41</cell><cell>31.52</cell><cell>30.42</cell><cell>23.56</cell></row><row><cell>P 1 -Blend4</cell><cell>48.30</cell><cell>49.88</cell><cell>67.53</cell><cell cols="2">62.79 56.25</cell><cell>18.90</cell><cell>16.46</cell><cell>42.80</cell><cell>32.28</cell><cell>27.61</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend1 47.57</cell><cell>51.12</cell><cell>65.88</cell><cell cols="2">62.34 56.04</cell><cell>32.32</cell><cell>27.44</cell><cell>42.41</cell><cell>42.59</cell><cell>36.19</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend2 48.65</cell><cell>50.41</cell><cell>65.78</cell><cell cols="2">61.67 55.88</cell><cell>31.71</cell><cell>26.83</cell><cell>42.41</cell><cell>42.59</cell><cell>35.89</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend3 47.61</cell><cell>50.69</cell><cell>65.78</cell><cell cols="2">61.96 55.80</cell><cell>31.10</cell><cell>25.61</cell><cell>43.97</cell><cell>42.33</cell><cell>35.75</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend4 48.11</cell><cell>50.84</cell><cell>65.81</cell><cell cols="2">62.76 56.15</cell><cell>28.66</cell><cell>25.61</cell><cell>42.80</cell><cell>42.86</cell><cell>35.59</cell></row><row><cell cols="2">P 1 -Blend1-P 2 -Blend5 48.94</cell><cell>51.41</cell><cell>66.14</cell><cell cols="2">62.28 56.49</cell><cell>28.66</cell><cell>23.78</cell><cell>42.02</cell><cell>43.12</cell><cell>34.40</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend1 49.29</cell><cell>50.35</cell><cell>67.44</cell><cell cols="2">62.66 56.58</cell><cell>31.10</cell><cell>24.39</cell><cell>49.42</cell><cell>43.12</cell><cell>37.01</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend2 49.44</cell><cell>50.92</cell><cell>67.47</cell><cell cols="2">62.99 56.89</cell><cell>30.49</cell><cell>25.00</cell><cell>49.81</cell><cell>42.59</cell><cell>36.97</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend3 49.19</cell><cell>49.37</cell><cell>66.88</cell><cell cols="2">62.60 56.10</cell><cell>27.44</cell><cell>20.73</cell><cell>48.25</cell><cell>44.18</cell><cell>36.15</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend4 49.16</cell><cell>50.84</cell><cell>68.35</cell><cell cols="2">63.18 57.03</cell><cell>31.71</cell><cell>23.17</cell><cell>47.47</cell><cell>42.86</cell><cell>36.30</cell></row><row><cell cols="2">P 1 -Blend4-P 2 -Blend5 49.38</cell><cell>50.86</cell><cell>68.18</cell><cell cols="2">62.60 56.92</cell><cell>28.66</cell><cell>20.12</cell><cell>45.53</cell><cell>43.92</cell><cell>34.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 19 :</head><label>19</label><figDesc>Final MMLU and code evaluation results after P 2 of training, broken down by individual category/benchmark. Corresponds to Table7in §4.</figDesc><table><row><cell>Exp.</cell><cell cols="8">ARC-Easy ARC-Challenge RACE PIQA WinoGrande HellaSwag OpenBookQA CommonsenseQA Avg.</cell></row><row><cell>BASE</cell><cell>78.75</cell><cell>53.84</cell><cell>35.69 80.30</cell><cell>68.51</cell><cell>76.30</cell><cell>45.40</cell><cell>51.68</cell><cell>59.69</cell></row><row><cell>Two-Phase</cell><cell>80.30</cell><cell>54.95</cell><cell>35.50 79.98</cell><cell>68.35</cell><cell>76.55</cell><cell>43.80</cell><cell>56.59</cell><cell>60.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 20 :</head><label>20</label><figDesc>Reasoning evaluation results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 vs. a randomized mixture of both blends across the entire 1T token training run, broken down by individual benchmark. Corresponds to Table 4 in §4.</figDesc><table><row><cell>Exp.</cell><cell></cell><cell></cell><cell>MMLU</cell><cell></cell><cell></cell><cell>Code</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">STEM Humanities Social Sciences Others Avg. HumanEval HumanEval+ MBPP MBPP+ Avg.</cell></row><row><cell>BASE</cell><cell>50.40</cell><cell>49.67</cell><cell>66.49</cell><cell>63.08 56.49</cell><cell>28.66</cell><cell>25.00</cell><cell>44.36</cell><cell>44.18</cell><cell>35.55</cell></row><row><cell cols="2">Two-Phase 49.29</cell><cell>50.35</cell><cell>67.44</cell><cell>62.66 56.58</cell><cell>31.10</cell><cell>24.39</cell><cell>49.42</cell><cell>43.12</cell><cell>37.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>More detailed evaluation results of the major experiments in this section broken down by individual reasoning, MMLU, and code benchmarks and categories can be found in §B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We show comparison of Blend1 and Blend6 in Table15.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We show comparison of quality-based blending with natural token distribution-based blend in §C.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>The CMC-Blend3-30% result at the bottom of Table28is also using 30% upsampling with LR cosine decay to 3e -6.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Model Specifications</head><p>We use RoPE position embeddings <ref type="bibr" target="#b49">(Su et al., 2021)</ref>, RMSNorm layer normalization <ref type="bibr" target="#b60">(Zhang and Sennrich, 2019)</ref>, with Grouped Query Attention <ref type="bibr" target="#b0">(Ainslie et al., 2023)</ref>. The maximum sequence length is 4096. We use a global batch size of 1536, and the Adam optimizer (Kingma and Ba, 2017) with β = (0.9, 0.95) and ϵ = 1e-08. P 1 training uses cosine LR decay with an initial LR of 3e-4 and targeted to reach a min-LR of 3e-6 at the end of the full training run (both phases). We start P 2 with the intermediate LR reached at the end of P 1 , and anneal using cosine LR decay to 3e-6 ( §E). Our experiments are run using up to 1024 NVIDIA H100 GPUs.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GQA: Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Lebron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4895" to="4901" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Zannettou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Keegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Squire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08435</idno>
		<title level="m">The pushshift reddit dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11641</idno>
		<title level="m">Piqa: Reasoning about physical commonsense in natural language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Does your data spark joy? performance gains from domain upsampling at the end of training</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Blakeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansheej</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03476</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evaluating large language models trained on code</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Redpajama: an open dataset for training large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">Angela Fan, et al. 2024a. The llama 3 herd of models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The llama 3 herd of models</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of data augmentation approaches for NLP</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.84</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="968" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Is child-directed speech effective training data for language models?</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="22055" to="22071" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Tokpanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Whittington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.16712</idno>
		<title level="m">Zamba: A compact 7b ssm hybrid model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://Skylion007.github.io/OpenWebTextCorpus" />
		<title level="m">Openwebtext corpus</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OLMo: Accelerating the science of language models</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.841</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15789" to="15809" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.00752</idno>
		<title level="m">Mamba: Linear-time sequence modeling with selective state spaces</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The bigscience roots corpus: A 1.6tb composite multilingual dataset</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Laurençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31809" to="31826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Kushal Arora, et al. 2024a. Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2024b. Datacomp-LM: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Starcoder: may the source be with you! Transactions on Machine Learning Research</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Reproducibility Certification</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jhonathan</forename><surname>Osin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Safahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19887</idno>
		<title level="m">Jamba: A hybrid transformer-mamba language model</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The flan collection: Designing data and methods for effective instruction tuning</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22631" to="22648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A pretrainer&apos;s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference of the North American Chapter</title>
		<meeting>the 2024 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3245" to="3276" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CLIMB -curriculum learning for infant-inspired model building</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Diehl Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hope</forename><surname>Mcgovern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zebulon</forename><surname>Goriely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Caines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paula</forename><surname>Buttery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Beinborn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.conll-babylm.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning</title>
		<meeting>the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="112" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prioritized training on points that are learnable, worth learning, and not yet learnt</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Sören Mindermann</surname></persName>
		</author>
		<author>
			<persName><surname>Brauner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinank</forename><surname>Muhammed T Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Höltgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Morisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><surname>Gal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="15630" to="15649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11704</idno>
		<title level="m">Nemotron-4 340b technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://github.com/openai/tiktoken" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2024a. Data, data everywhere: A guide for pretraining dataset construction</title>
		<author>
			<persName><forename type="first">Jupinder</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aastha</forename><surname>Jhunjhunwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2024 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="10671" to="10695" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jupinder</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16819</idno>
		<title level="m">Nemotron-4 15b technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Openwebmath: An open dataset of high-quality mathematical web text</title>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangir</forename><surname>Azerbayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024a. The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Balanced data sampling for language model training with clustering</title>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoye</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.14526</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Vassilieva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.10818</idno>
		<title level="m">Slimpajama-dc: Understanding data combinations for llm training</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Snowflake arctic: The best llm for enterprise ai -efficiently intelligent, truly open</title>
		<author>
			<persName><surname>Snowflake</surname></persName>
		</author>
		<ptr target="https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-\models-snowflake/" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dolma: an open corpus of three trillion tokens for language model pretraining research</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.acl-long.840</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="15725" to="15788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><surname>Stack Exchange</surname></persName>
		</author>
		<title level="m">Stack exchange data dump</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Nemotron-cc: Transforming common crawl into a refined long-horizon pretraining dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.02595</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<title level="m">Roformer: Enhanced transformer with rotary position embedding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<title level="m">Alexandre Ramé, et al. 2024a. Gemma 2: Improving open language models at a practical size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08295</idno>
		<title level="m">Gemma: Open models based on gemini research and technology</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Narenthiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10176</idno>
		<title level="m">Openmathinstruct-1: A 1.8 million math instruction tuning dataset</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A survey on curriculum learning</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3069908</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4555" to="4576" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Emergent analogical reasoning in large language models</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">J</forename><surname>Holyoak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjing</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09196</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">2023a. Doremi: Optimizing data mixtures speeds up language model pretraining</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="69798" to="69818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">2023b. Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34201" to="34227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
