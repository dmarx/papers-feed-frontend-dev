# Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase Pretraining

## Abstract

## 

Pretraining large language models effectively requires strategic data selection, blending and ordering. However, key details about data mixtures especially their scalability to longer token horizons and larger model sizes remain underexplored due to limited disclosure by model developers. To address this, we formalize the concept of two-phase pretraining and conduct an extensive systematic study on how to select and mix data to maximize model accuracies for the two phases. Our findings illustrate that a two-phase approach for pretraining outperforms random data ordering and natural distribution of tokens by 3.4% and 17% on average accuracies. We provide in-depth guidance on crafting optimal blends based on quality of the data source and the number of epochs to be seen. We propose to design blends using downsampled data at a smaller scale of 1T tokens and then demonstrate effective scaling of our approach to larger token horizon of 15T tokens and larger model size of 25B model size. These insights provide a series of steps practitioners can follow to design and scale their data blends.

## Introduction

Large language models (LLM) are typically pretrained on large amounts of data in the order of billions (B) or trillions (T) of tokens derived from multiple data sources such as web crawl, books, papers, patents, mathematical and legal documents, and so forth [(Brown et al., 2020;](#b5)[Parmar et al., 2024b;](#)[Team et al., 2024b;](#)[Dubey et al., 2024a;](#)[Nvidia et al., 2024)](#b32). To develop a state-of-the-art model, it is critical to understand the nature of these data sources and to make informed decisions about optimal data blending (how different data sources are weighed during pretraining) and training strategies. These decisions typically involve * equal contribution † Work done during internship at NVIDIA running multiple large-scale experiments to empirically investigate the optimal training data blend(s) and ordering of data.

Most advanced models [(OpenAI et al., 2024;](#b34)[Dubey et al., 2024b)](#) do not divulge information on the data blends that are used, nor the ablation studies informing the data mixing and ordering decisions. Recent works [(Blakeney et al., 2024;](#b4)[Groeneveld et al., 2024;](#b17)[Dubey et al., 2024b;](#)[Snowflake, 2024)](#b45) provide high-level data blend information about a small portion of pretraining by encouraging the upsampling of certain domains towards the end. In general, there exists a knowledge gap regarding how to craft and choose an optimal data blend(s) for the entire training process, and the generalizability of data blends and ordering strategies to larger token horizons and model sizes.

In this work, we address the above knowledge gap by understanding optimal data blends and ordering strategies for training LLM. We formalize and extensively explore a two-phase training approach (Figure [1](#fig_0)) that balances diversity and quality: phase-1 emphasizes diverse, high-quality web crawl data, while phase-2 focuses on high-quality data sources such as math, code, and wiki data. Specifically, in this work we propose to use downsampled data to prototype and explore multiple blends at a smaller scale of 1T tokens. We craft our blends based on quality of the data source and the number of epochs to be seen during pretraining. We then demonstrate the effectiveness of our approach at a 15T token scale using the full data.

We evaluated on a comprehensive set of downstream tasking covering knowledge, reasoning, coding and math benchmarks. Our experiments illustrate that a quality and epoch based blend is better than a blend based on natural distribution by 13.2% and the two-phase approach is better than random ordering of data (blend is based on quality and epochs) by an average of 3.4% across downstream tasks. Furthermore, our results on downsampled data generalize across longer 15T token horizons on full data and larger model sizes, demonstrating the scalability and robustness of the two-phase approach. We also provide a fine-grained quality analysis of web crawl data, revealing optimal blending strategies to balance diversity and quality.

We share and highlight a series of findings made to create blends and order in our two-phase approach. Our main contributions are:

• Formalization and large-scale evaluation of the two-phase training approach for LLMs, with actionable strategies that enable effective LLM pretraining. • Improving the understanding of data selection and blending with quality-based and epochbased analyses of data, including web crawl. • Demonstration of the scalability of blends using downsampled data at 1T to using full data at 15T tokens and larger model size of 25B.

## A Two-Phase Approach to Pretraining

In this work, we explore a two-phased approach to pretraining: phase-1 (P 1 ) then phase-2 (P 2 ). Figure [1](#fig_0) demonstrates our two-phased approach. In each phase, we explore different data blends based on the quality and number of epochs to be seen of a data source. In phase-1 (P 1 ), we explore a general data distribution which consists of a mix of web crawl data, medium-quality data, and low amounts of high-quality data. In phase-2 (P 2 ), we explore a blend which includes task data and emphasizes high-quality datasets such as math, code, and high-quality web crawl ( §5.1). Estimate the quality of a data source ( §5.1), 3) Estimate the epochs to be seen in the whole pretraining ( §5.2) and finally 4) distribute the epochs appropriately in P 1 and P 2 ( §3.2). The downsampling factor f is based on the final total token budget which we assume to be 15T similar to [Dubey et al. (2024b)](#). Hence, for us f = 1/15 i.e for each data source, the number of tokens available for pretraining is 1/15 th of the total token in that dataset. Downsampling helps to observe the impact of epochs of datasets at a smaller scale of 1T tokens and then can be used to scale the blend to a longer token horizon of 15T tokens using the full data.

Baselines: Since our blends are based on quality and epoch based analyses of the data as well as the ordering of the data in the two phases, we consider the following two baselines: 1) Natural Distribution Blend (BASE-ND): This blend is based on ratio of the number of tokens available in each data source. The weight for each dataset is equal to the total number of tokens in that dataset divided by the sum of tokens available in all the datasets. This weighting is neither based on quality nor the epochs to be seen for the dataset. 2) Random Order Pretraining (BASE-RO): This blend is based on quality and epochs of each dataset but does not use two phases to train the model. The weight for each dataset here is the same as our two-phase approach but the order in which the the dataset is seen during pretraining is random.

3 Experimental Setup

## Data Sources

Our pretraining corpus spans a vast range of text data sources that cover several domains, types of data, and languages. We broadly divide our datasets into the following categories and their token counts in billions is shown in Table [1](#tab_0).

Category Domain Blend1 Blend2 Blend3 Blend4 Blend5 Web Crawl -65.0 65.0 58.0 59.0 70.0 High Quality Math 1.9 1.9 1.9 2.9 1.9 Wiki 0.1 0.1 0.1 0.1 0.1 Code 15.0 8.0 15.0 20.0 13.0 Medium Quality Books 5.5 9.0 9.0 5.5 4.5 Papers 3.5 5.0 5.0 3.5 1.9 CC dv 4.0 6.0 6.0 4.0 3.6 Multilingual -5.0 5.0 5.0 5.0 5.0

Table 2: Phase-1 Blends (in %)

• Web Crawl: Data derived from Common Crawl (CC). We discuss the quality of this data and how to blend it in detail in §5.1. • High-Quality: This includes datasets from more specialized and professional domains such as mathematics [(Paster et al., 2024;](#b37)[Stack Exchange, Accessed 2024)](#), code [(Li et al., 2023)](#b25), and Wikipedia (wiki) data. • Medium-Quality: Data derived from books & patents, papers [(Gao et al., 2020)](#b14), and Common Crawl derivatives (CC dv ) such as OpenWebText [(Gokaslan and Cohen, 2019)](#b16), BigScience [(Laurençon et al., 2022)](#b22), Reddit [(Baumgartner et al., 2020)](#b2), and CC-News. This category was determined by comparing this data to medium-quality crawl (see §5.1).

• Multilingual: Multilingual data (9 languages) derived from Wikipedia and Common Crawl. • Task Data: This includes data used for supervised finetuning (SFT) during the alignment phase [(Toshniwal et al., 2024;](#b53)[Nvidia et al., 2024)](#b32). We also include the FLAN collection [(Longpre et al., 2023)](#b27).

## Data Blends for Each Phase

The final blends in P 1 and P 2 are based on quality and epoch based ablations shown in §5.1 and §5.2. The insights from these studies are incorporated in Table [2](#) and [3](#).

In P 1 , we encourage diversity in data by including a high percentage of web crawl data which consists of high, medium, and low-quality crawl. We want to introduce a limited amount of high-quality data such as math, code, and wiki in P 1 . In P 2 , the emphasis is primarily on high-quality datasets and only includes a limited amount of medium-quality data. For example, in P 2 , we only use high-quality crawl instead of medium or low-quality (see §5.1).

Table 2 details the five blends explored in P 1 . These blends are designed to compare the proportion of high-level categories with each other. The Category Domain Blend1 Blend2 Blend3 Blend4 Blend5 Web Crawl -31.0 35.0 31.0 40.0 35.0 High Quality Math 24.0 24.0 24.0 24.0 29.0 Wiki 1.0 1.0 1.0 1.0 1.0 Code 20.0 25.0 29.0 20.0 20.0 Medium Quality Multilingual -3.7 3.7 3.7 3.7 3.7 Task Data -1.3 1.3 1.3 1.3 1.3 Table 3: Phase-2 Blends (in %) difference between Blend1 and Blend2 is that Blend2 has less code and more medium-quality datasets compared to Blend1. Blend3 has less web crawl and more medium-quality datasets compared to Blend1. Blend4 has less web crawl and more high-quality datasets compared to Blend1.

Blend5 is designed to have majority web crawl at the cost of code and medium-quality data. Table [3](#) outlines the five blends explored in P 2 . In P 2 , we use more epochs and higher proportions of high-quality data such as high-quality web crawl, math, wiki, and code data. Blend3 has more code and less medium-quality datasets compared to Blend1, and Blend4 has more high-quality web crawl and less medium-quality datasets compared to Blend1. Blend2 has a more balanced distribution among the data categories, while Blend5 upsamples math data more heavily.

## Model Specifications

We experiment using the Megatron [(Shoeybi et al., 2020)](#b44) model, an autoregressive causal left-to-right LLM, with the Tiktokenizer [(OpenAI, 2023)](#b33). We downsample all our data by factor f = 1/15. Hence, only 1/15 of the tokens shown in Table [1](#tab_0) will be available for pretraining. We perform all our investigations using an 8 billion parameter model trained on 1 trillion total tokens. Furthermore, we test our two-phase approach by scaling along two dimensions: (1) we scale the token horizon to 1.7T tokens on a 8B model, and (2) we scale the parameters of the model to 25B and train on 1T tokens. Additionally, we train a 8B model on 15T tokens on full data (not downsampled) to observe if decisions made with downsampled data scales. Specifics on model architecture and hyperparameters are shared in Appendix A.

## Evaluation Suite

To comprehensively assess our models, we use various benchmarks that evaluate different capabilities. These can be broadly divided into the following 4 categories, of which we report the final averages.

We assess 5-shot accuracy for MMLU [(Hendrycks et al., 2021)](#b19), 0-shot accuracy 1 for reasoning tasks: CommonsenseQA [(Talmor et al., 2019)](#b50), ARC-Easy & Challenge [(Clark et al., 2018)](#b7), PIQA [(Bisk et al., 2019)](#b3), WinoGrande [(Sakaguchi et al., 2019)](#b40), Hel-laSwag [(Zellers et al., 2019)](#b59), OpenBookQA [(Mihaylov et al., 2018)](#b30), RACE [(Lai et al., 2017)](#b21), 0shot accuracy for code benchmarks: HumanEval (+) [(Chen et al., 2021)](#b6) and MBPP (+) [(Austin et al., 2021)](#b1), and 8-shot chain-of-thought (CoT) accuracy for GSM8K [(Cobbe et al., 2021)](#b8). We also report a final overall Avg. for most results, which is an average over all individual evaluation tasks.

## Results for Two-Phase Pretraining Findings

• A two-phase approach for pretraining is effective.

• Phase-1 should focus on data diversity and phase-2 on high-quality data.

We compare our best blends P 1 -Blend4-P 2 -Blend1 2 using two-phase training with two baselines: 1) BASE-ND: the weights are determined by the tokens available in each dataset and are not based on quality, and 2) BASE-RO: the weights for all the datasets are the same in this and P 1 -Blend4-P 2 -Blend1. The only difference is the order in which the data is presented during training (random or two-phased). Table [4](#tab_4) illustrates that using a quality and epoch based blend is on average 13.2% better than natural distribution blend (compare BASE-RO vs BASE-ND) across downstream tasks. It also presents that using our two-phase training approach noticeably improves average accuracy by 3.4% compared to BASE-RO and 17%

1 We use normalized accuracy for ARC-Easy, ARC-Challenge, PIQA, HellaSwag, and OpenBookQA.

2 see §4.1 Tab. 7 on how we select this blend and §5.3 Tab. 14 for the duration of P2 for best results.  compared to BASE-ND. This empirically demonstrates that the strategy of two-phase training is useful and tasks such as code and math are sensitive to the ordering of high-quality data in the second phase. We scale our best blend P 1 -Blend4-P 2 -Blend1 to 15T tokens and use the full dataset to train a 8B model. All the previous experiments are performed on downsampled data and 1T scale. This means that the number of epochs is constant in both the runs. Table [5](#tab_5) shows that blends crafted at smaller scale can generalize to longer token budgets if the quality and epochs of the datasets are maintained at scale. This shows the generalizability of our twophase approach to pretraining as well as qualityand epoch-based approach to designing blends.

## Determining Blends

As discussed in §3.2, we explore five different blends for phase-1. [3](#foot_0) We train an 8B model on downsampled data for 1T tokens for all five blends and eliminate blends based on a separately held-out validation split. Fig. [2](#fig_1) illustrates the validation loss for all five blends. As we can see, Blend5 and Blend2 had 2.8% and 2.1% higher validation loss, respectively, relative to Blend4 at approx. 250B tokens. Hence, we discontinue these two blends at Table 7: Evaluation results after P 2 of training.

that point. Since, the validation loss of the remaining three blends was within a margin of 1%, we periodically evaluate their accuracy on downstream tasks. Table [6](#tab_6) shows the results of the remaining three phase-1 blends at various token counts.

At each token evaluation point -200B, 250B and 629B, we see that Blend3 is consistently worse than the other two blends. Hence, we eliminate this blend after 629B tokens of training. For this experiment, we switch from P 1 to P 2 after ≈70% of training, i.e the last 30% of training is P 2 . In §5.3, we explore varying the percentage of P 2 . Results in Table [6](#tab_6) follow intuition since Blend4 has the highest amount of high-quality data and is hence better than Blend1 and Blend3. Blend3 has more medium-quality data at the cost of web crawl compared to Blend1. This result confirms that books, papers, and CC dv are of medium-quality compared to our high-quality datasets and our web crawl blend.

Finally, we explore five different blends of P 2 described in Table [3](#) in combination with P 1 -Blend1 and P 1 -Blend4. Hence, we have ten different combinations of P 1 and P 2 blends. Table [7](#) shows the results on all ten combinations of blends. We find that P 1 -Blend4-P 2 -Blend1 performs the best on average. Table [7](#) also presents the final results of P 1 -Blend1 and P 1 -Blend4 if only the P 1 blend was continued for 1T tokens without ever switching to P 2 blends. It shows that switching to any of the P 2 blends for training is better than continuing the P 1 blends for all metrics. We observe the largest absolute gains in GSM8K and code of 14.6% and 9.4%, respectively, for P 1 -Blend4-P 2 -Blend1.

## Scaling

## Findings

• Two-phase approach is scalable and robust to token horizon and model scale.

• Data blends need adjusting at longer token horizons based on epoch count to avoid high-quality data overexposure.

We further explore scaling our best blend along two dimensions: (1) a longer token horizon of 1.7 trillion tokens and (2) larger model size of 25B parameters. For a longer token horizon, we aim to assess whether the blend can be used as is or if adjustments are necessary to prevent overfitting (observed in §5.2). Note that this is different from scaling to 15T token where we use the full data. Here we still use the downsampled data and scale to 1.7T tokens and hence the epochs seen of each dataset would be higher. Since high number of epochs of high-quality datasets are primarily seen in P 2 of pretraining, we create a new blend, P 2 -Blend6[foot_1](#foot_1) , which is an epoch-adjusted version of P 2 -Blend1 to ensure that we do not see more than 8 epochs of certain high-quality data sources like math and task data. Table [8](#tab_8) shows the comparison of scaling from 1T to 1.7T total tokens. We see that P 1 -Blend4-P 2 -Blend6 is on average 2.2% better than P 1 -Blend4-P 2 -Blend1, illustrating that we need to adjust our blends according to the epoch counts of high-quality data for optimal results. Both the 1.7T models are better than 1T, demonstrating that we can still obtain higher downstream accuracies Validation Loss phase1 phase2 by training on more tokens, even if it means training on more than 8 epochs of high-quality data.

We also investigate if our best blend can scale to a larger model size. Given the high number of epochs of high-quality data in P 1 -Blend4-P 2 -Blend1, we also want to determine if a model with a larger capacity might memorize the data and overfit on it. Figure [3](#fig_2) shows that the validation loss is always decreasing for the 25B model, indicating that there is no overfitting with P 1 -Blend4-P 2 -Blend1. Table [9](#tab_9) shows results for the 25B model compared to the 8B model on this data blend combination. Understandably, the 25B model is substantially better across the board, demonstrating that the two-phase training approach and data blend combination can also scale to larger model sizes.

## Ablations

This section details quality-based data blending, epoch study and percentage of phase-2 to be conducted in the pretraining. Additional fine-grained analyses of data blends and study on learning rate schedule to be used in phase-2 is shown in Appendix §D and §E. 

## Quality-Based Data Blending

## Insights

• Upsampling high quality and not using low quality CC data is most effective.

• CC dv , papers and books are similar in quality to CC-Medium-High.

The data blends of our two-phase approach are mainly based on the assessment of each data source's quality. Hence, we carry out extensive experiments to find an optimal data blend for web crawl documents. While previous work [(Dubey et al., 2024a;](#)[Yang et al., 2024;](#b58)[Team et al., 2024a)](#) mentions that web crawl documents like Common Crawl (CC) form a large majority of their pretraining data, none of them share a recipe on how to mix different slices of CC. Some recent work on constructing crawl-based pretraining datasets [(Penedo et al., 2024b;](#)[Li et al., 2024a)](#) directly use the high quality crawl documents in pretraining but provide no specific data mixing strategy. In this section, we provide comprehensive details on how to create a data blend for CC documents and use it effectively in our phase-1 and phase-2 of pretraining. Additionally, we provide a quality assessment of other datasets like CC dv , papers, books and our high quality datasets. We compare them with medium, and high quality web crawl to position them optimally in our P 1 and P 2 blends.

Quality-Based Blending for Web Crawl: Each document in our web crawl data is classified into one of five quality categories: High, Medium-High, Medium, Medium-Low, and Low using the classifier from [Su et al. (2024)](#b48).

We investigate various blends using qualitybased weighted sampling approach[foot_2](#foot_2) for all of web Table 12: Results of different quality crawl and their comparison with other datasets. Since code data is not included in most of these experiments, we exclude code evaluation.

crawl data from 99 CC snapshots for our phase-1 of pretraining. The idea is to upsample high and medium-quality crawl documents while avoiding a high quantity of low-quality data. The overall idea for the four web crawl blends in Table [10](#tab_10) is to iteratively decrease the percentage of tokens from High and Medium-High and increase the tokens in the lower categories. The results in Table [11](#tab_11) 6 demonstrate that eliminating the tail-end of the web crawl data belonging to Medium-Low and Low quality categories is beneficial as opposed to to keeping them for diversity. Based on these results, we choose CC-Blend1 as the final data blend for web crawl documents to be used in all our final P 1 blends ( §4 and Table [2](#)). For P 2 , we only use web crawl data that belongs to High-quality category.

Quality Estimation of Other Datasets: We assess how our CC dv , papers, books and high quality datasets such as math, code and Wiki compare to CC-Medium,CC-Medium-High and CC-High quality crawl data. We continue training the last checkpoint of P 1 -Blend4, for an additional 50B tokens using a data mix that consisted 66% of the data being tested, mixed with 34% of CC-High.

The results in Table [12](#tab_0) show that CC dv , papers and books datasets have similar accuracies to CC-Medium-High on the majority of benchmarks, and lag behind CC-High. As such, we group them under the "medium-quality" data category for our exper- 6 The average in this table is primarily based on reasoning tasks and MMLU because these blends do not have math or code data.  iments (see §3.1). The high quality datasets have an average accuracy better than CC-High.

## Epoch-Based Analysis

## Insights

• We recommend 6 epochs of highquality crawl and 8 epochs of math and task data for data mixing.

We take the number of epochs of high quality datasets into account while creating our P 1 and P 2 blends. We experiment with different numbers of epochs for high-quality crawl, math, and task data.

Since, majority of web crawl is used in P 1 , we pretrained an 8B model with 1T tokens, using different epochs of high-quality crawl tokens in the data mix, and evaluate each model's MMLU score. Note that we keep the overall percentage of web crawl the same in all the experiments. As we can see in Figure [4](#fig_3), increasing the number of highquality tokens increases the MMLU score until 6 epochs. We primarily present MMLU score because these experiments do not include high amount of math or code data.

Since, majority of math and task-data is seen in P 2 , Table [13](#tab_13)  of epochs for them in P 2 . It shows that ≈ 8 epochs of math is a good balance while not sacrificing accuracy on MMLU and reasoning. For task data, all metrics generally improve with more epochs, although there appears to be diminishing returns on several past epoch 8. Note that 8 epochs of both math and task data corresponds to our best P 1 -Blend4-P 2 -Blend1 blend combo from §4.

## Optimal Duration of Phase-2

## Insights

• Pretraining with the P 2 blend for the final 40% gives the best results.

We investigate the percentage of phase-2 to use in the whole pretraining regime. We experiment with 0 to 50% of P 2 in the whole of pretraining. The longer the duration of P 2 , the shorter the duration of P 1 . We use the P 1 -Blend4-P 2 -Blend1 blend combination that we found best in §4.

Table [14](#tab_14) illustrates that a higher percentage of P 2 until 40% is better overall, especially in math and code. Going above this, e.g. to P 2 as 50% of training, downstream accuracies start to degrade across the board, potentially due to overfitting.  [(2024a)](#) demonstrating that refined data selection impacts model accuracies more significantly than simply the quantity of data. But these studies are primarily aimed at CC data and they do not suggest any data mixing strategies for pretraining. [Parmar et al. (2024a)](#) provide a systematic approach to building effective LLM pretraining datasets with ablations on data attributes, and existing curation, selection, and sampling methods. In our work, we provide a systematic approach to craft data blends and to order the data in pretraining.

## Related Work

Strategic weighting and timing of data usage can also noticeably impact model accuracies. Techniques like domain upsampling [(Blakeney et al., 2024;](#b4)[Dubey et al., 2024b)](#) towards the end of training have been shown to be effective. Snowflake (2024); [Groeneveld et al. (2024)](#b17) provide details about high level blends for their pretraining process. In contrast, our work provides fine grained details about the data blend creation process along with actionable steps that model developers can use to develop data blends and order. Prior work [(Shen et al., 2023;](#b43)[Longpre et al., 2024;](#b28)[Mindermann et al., 2022;](#b31)[Xie et al., 2023a,b;](#)[Shao et al., 2024)](#b41) investigates optimizing data mixtures based on clustering methods, manually designed domain composition weights, proxy models or reference models to determine data composition weights and sample-level data selection. Our work primarily focuses on data ordering and scaling of data blends in pretraining and can be used in conjunction with other data sampling techniques.

Curriculum learning approaches inspired by human learning offer an ordered way to introduce data gradually to enhance model learning. [Martinez et al. (2023)](#b29); [Wang et al. (2022)](#b54); [Feng et al. (2024)](#b13) investigate cognitively-motivated curriculum-based training including vocabulary, and objective curricula, and outline and the challenges and potential solutions for designing effective curricula. Our work shows that ordering of data based on quality in pretraining LLMs has a significant impact of downstream accuracies.

## Conclusion

In conclusion, through extensive experiments, we demonstrate the effectiveness of a two-phase pretraining approach for LLM. For the initial training phase, a more general data distribution consisting of mainly of web crawl proves most effective, while phase two benefits from a comprehensive data blend, with additional focus on math, code, and task data. Phase-two for the last ≈40% of training yields the best results, and over-extending it leads to diminishing returns. Increasing model size and token horizon further enhances accuracy, demonstrating the scalability of our approach. Importantly, we also show that considering both the quality of the data (including web crawl) and the number of epochs of each data source is crucial to attain optimal results and prevent overfitting.

## Limitations

Some limitations of our work include our present suite of models and evaluation benchmarks. We can extend our work and show the effectiveness of two-phase pretraining approach on more LLM architectures such as Mamba [(Gu and Dao, 2023)](#b18), other hybrid SSM based architectures [(Glorioso et al., 2024;](#b15)[Lieber et al., 2024)](#b26) and mixture of experts [(Shazeer et al., 2017)](#b42). While our evaluation benchmarks are quite comprehensive, we could potentially expand to an even broader range of evaluations, including nuanced domain-specific or interactive tasks, or more theory of mind and developmental psychology-inspired benchmarks. This includes assessing capabilities such as analogical reasoning [(Webb et al., 2023)](#b55). Further, our scaling experiments could be expanded. Scaling up to hundreds of billions of parameters or significantly longer training may yield additional insights. Lastly, while our work focuses on two-phase training and shows its efficacy, we can potentially investigate multi-phase training, and the impact of the order of the phases. However, we believe this is more suited for future work. Overall, these are directions to potentially improve and expand upon our work. Despite these potential limitations, we feel that our current work is an insightful and useful contribution to the research community.

## Ethical Considerations

Our research uses publicly available and commonly used datasets in LLM development. These sources, including Common Crawl, Wikipedia, and code repositories, are widely adopted in the research community. We examined the quality and origins of our data, prioritizing high-quality, domainrelevant data sources to improve LLM capabilities in a responsible manner. However, web crawl data may inherently contain biases or inappropriate content despite filtering efforts. We used established data cleaning and quality assurance procedures but acknowledge that potential biases may persist and impact model behavior in certain circumstances.

We recognize that scaling models and exploring data blending strategies require significant computational resources, which may raise environmental concerns. To mitigate this, we focused on efficient training strategies, such as two-phase training, to improve accuracy without excessively increasing resource usage. Future studies could benefit from exploring energy-efficient training methods to further minimize the environmental impact.

Our models, data blends, and accompanying publication are intended solely for research purposes, with no intended real-world application without additional safety evaluations. We caution against deploying models based on our methods without thorough testing, as they may carry unknown risks, particularly when applied to tasks involving sensitive or personal information. Our work aims to advance the understanding of LLM training strategies, and we feel that it is an important contribution to the research community. We encourage researchers to expand upon our work while further investigating the ethical and societal implications of LLM.

Tables 18 to 25 contain detailed evaluation results of the major experiments reported in §4 for reasoning, MMLU, and code, broken down by individual categories and benchmarks. They correspond to the results found in Tables 7 to 9 in §4.

Table [15](#tab_5) shows the comparison of Blend1 and Blend6 used in scaling experiments in Table [8](#tab_8). If we use the same Blend1 as is and train for more number of tokens (1.7T) then the number of epochs seen of each dataset would be higher compared to 1T training.

## C Details of Quality-Based Data Blend

We first compare a baseline blend (ND) which uses the natural distribution of tokens with a smartly constructed weighted sampling blend (WS). ND is based on the number of tokens that belong in each category as opposed to utilizing the quality label i.e. if 59% of the tokens belong to Low then 59% of tokens seen during pretraining would be- long to Low. We then create a data blend (WS) based on weighted sampling of high and mediumquality tokens. The idea is to upsample high and medium-quality crawl documents and not use the low-quality data at all. Table [16](#tab_15) shows the token percentages that belong to each of the five quality labels for both ND and WS blends. Table [17](#tab_0) illustrates the results of the two models trained on the ND and WS data blends of web crawl, respectively. We see that our data blend (WS) outperforms on most of the evaluation tasks by a large margin, and the improvement on MMLU is substantial.

## D Finegrained P 2 Blend Experiments

We investigate fine-grained P 2 blends to determine the optimal blend. For these experiments, we use a model trained on a P 1 blend for 900B tokens (10% P 2 duration), with a linear LR decay to 0.

Crawl, Math, & Code: We investigate different percentages of high-quality crawl, math, and code data as shown in Table [26](#tab_6). Table [28](#tab_8) demonstrates that a higher amount of math data (i.e. 30%) helps across the board. However, code data results are mixed, as too much code without enough math (CMC-B1) seems to hurt all non-code metrics. Comparing (CMC-B2) vs. (CMC-B3), more than 15% code does not add as much value, as gains saturate. Trading off crawl data for more code data also slightly hurts MMLU. As such, we decide that a final blend consisting of a higher amount of crawl and math with a moderate amount of code seems best overall. This corresponds to CMC-B3 in Table [26](#tab_6), which consists of 30% crawl, 33% math, and 15% code.

Task Data: Second, we investigate the inclusion of task data. Specifically, adding FLAN and synthetically-generated GSM8K-train data (similar to data augmentation approaches [(Feng et al., 2021)](#b12)) to the CMC-B3 blend. Our FLAN data consists of a mixture of normal FLAN and FLAN-CoT (chain-of-thought) data. We compare 10 and 20 epochs of FLAN. These blends can be found in Table [27](#), with the results in Table [28](#tab_8). We can see that including synthetic GSM8K-train and FLAN data noticeably improves GSM8K scores while not detrimenting the other benchmarks. In fact, FLAN data also helps further improve MMLU and reasoning. 20 epochs of FLAN seems better than 10 epochs overall. Hence, including task data for P 2 of training seems to be a good idea.

All Data Mixture: Lastly, we investigate a final P 2 data mixture which is a combination of all the data sources we tried, including FLAN, GSM8K, and relatively higher amounts of math and code data. For this experiment, we use 30% upsampling with LR cosine decay to 3e -6. This blend can be found in Table [27](#), with the results at the bottom of Table [28](#tab_8). [7](#foot_3) We find that mixing all data sources helps greatly with GSM8K, noticeably with coding and reasoning, while retaining accuracy on MMLU. Hence, the final P 2 blends we investigate in §4 (Table [3](#)) are motivated by these ablations -they are blends of all data sources, including task data, with higher proportions of math and code.

## E Annealing Learning Rate Schedule

We investigate different learning rate (LR) schedules for phase P 2 . Specifically, using the same P 2 blend for a 10% duration, we try different LR strategies. We compare cosine vs. linear LR decay functions, and also compare decaying to a final LR of 0 vs. 3e-6 (1% of the original P 1 starting LR of 3e-4). Not decaying LR entirely to 0 leaves room for post-training, which is likely preferable.

As seen in Table [29](#tab_9), there is a negligible difference between linear and cosine LR decay, so we choose cosine decay for consistency with P 1 . We also see that LR decay to 3e-6 is comparable to decaying all the way to 0, while leaving room for post-training. Hence, our final chosen annealing strategy is cosine LR decay to 3e-6, which we use for our final two-phase experiments in §4. Table 21: MMLU and code evaluation results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 vs. a randomized mixture of both blends across the entire 1T token training run, broken down by individual category/benchmark. Corresponds to Table 4 in §4.

![Figure 1: Diagram of our two phase training pipeline.Phase-1 blend encourages data diversity and phase-2 blend is focused on high quality datasets.]()

![Figure 2: Phase-1 validation loss for different P 1 blends.]()

![Figure 3: Validation loss for the 25B model using twophase training with P 1 -Blend4-P 2 -Blend1.]()

![Figure 4: MMLU accuracy (%) vs. number of epochs of high-quality crawl in the data mix.]()

![Selecting and structuring pretraining datasets is important to improve model generalization and efficiency. Dubey et al. (2024b) emphasize openness and accessibility of models, Computer (2023); Soldaini et al. (2024) assemble an open corpus of trillions of tokens for large-scale training, and Groeneveld et al. (2024) release a truly Open Language Model, including its framework, training data, and code. Studies such as Li et al. (2024b); Penedo et al.]()

![Tokens (billions) in each data domain.]()

![Comparison of our two-phase training approach with BASE-ND and BASE-RO.]()

![Tok. MMLU Reason. GSM8K Code Avg.Results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 for downsampled data at 1T and then complete data at 15T.]()

![P 1 results after various token counts (billions).]()

![Scaling results for 1.7T tokens vs. 1T tokens, with and without high-quality data epoch adjustment.]()

![Model Size MMLU Reason. GSM8K Code Avg.Evaluation results for 8B vs. 25B parameter models, using the same blend: P 1 -Blend4-P 2 -Blend1. Note that we use a maximum sequence length of 8192 (instead of 4096) for both models here.]()

![CC blends (in %) by quality. For CC-Blend3 and CC-Blend4, we merged the Medium and Medium-Low categories. Token column refers to the the natural distribution of tokens, i.e. percentage of total CC data that belongs to each category.]()

![P 1 results using our various CC blends.]()

![Results of varying the number of epochs of math and task data during P 2 of training.]()

![presents results for different numbersP 2 % MMLU Reason. GSM8K Code Avg.Results of different durations of P 2 using P 1 -Blend4-P 2 -Blend1.]()

![Data blends for CC Quality estimation experiment. The overall percentage of the Common Crawl Snapshots in our experiments is fixed at 73.3%.]()

![Exp. ARC-Easy ARC-Challenge RACE PIQA WinoGrande HellaSwag OpenBookQA CommonsenseQA Avg. Final reasoning evaluation results after P 2 of training, broken down by individual benchmark. Corresponds to Table 7 in §4.]()

![Final MMLU and code evaluation results after P 2 of training, broken down by individual category/benchmark. Corresponds to Table7in §4.]()

![Reasoning evaluation results of our two-phase training approach with P 1 -Blend4-P 2 -Blend1 vs. a randomized mixture of both blends across the entire 1T token training run, broken down by individual benchmark. Corresponds to Table 4 in §4.]()

More detailed evaluation results of the major experiments in this section broken down by individual reasoning, MMLU, and code benchmarks and categories can be found in §B.

We show comparison of Blend1 and Blend6 in Table15.

We show comparison of quality-based blending with natural token distribution-based blend in §C.

The CMC-Blend3-30% result at the bottom of Table28is also using 30% upsampling with LR cosine decay to 3e -6.

