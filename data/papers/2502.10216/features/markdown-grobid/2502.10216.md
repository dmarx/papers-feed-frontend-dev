# Forget the Data and Fine-tuning! Just Fold the Network to Compress

## Abstract

## 

We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to datadriven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments. Our code is online. ¶

## Introduction

Deep neural networks (DNNs) have emerged as a fundamental technology, driving progress across a multitude of applications from natural language processing to computer vision. However, the deployment of these models in real-world settings is often constrained by the computational and memory resources available, particularly on edge devices like smartphones and embedded systems [(Chen et al., 2020;](#b10)[Kumar et al., 2017;](#b46)[Wan et al., 2020)](#b78). This limitation poses a significant challenge, as the growing complexity and size of SOTA models demand increasingly substantial resources [(Bommasani et al., 2021;](#b6)[Chang et al., 2024;](#b8)[Rombach et al., 2022)](#b68).

Conventional model compression techniques, such as pruning [(Han et al., 2015;](#b27)[Hassibi et al., 1993;](#b28)[LeCun et al., 1989;](#b48)[Li et al., 2016b)](#) and quantization [(Gupta et al., 2015;](#b26)[Li et al., 2016a;](#)[Zhou et al., 2017)](#b87), have been developed to mitigate this issue by reducing the model size and computational requirements. These methods usually remove redundant or less critical parameters from the model, thereby reducing the overall size and computational load. For example, pruning eliminates weights that contribute minimally to the model's output [(Entezari and Saukh, 2020;](#b16)[Han et al., 2015;](#b27)[Li et al., 2016b)](#). Quantization reduces the precision of the weights and activations [(Gupta et al., 2015)](#b26), which decreases memory usage and speeds up inference [(Zhou et al., 2017)](#b87). Despite their effectiveness, these approaches often introduce a degradation in model performance, necessitating a phase of fine-tuning to maintain the internal data statistics within the model [(Jordan et al., 2022)](#b39) and restore the original accuracy levels [(Frankle and Carbin, 2018;](#b20)[Frantar and](#b21)  Right: To maintain accuracy, the data variances of compressed and uncompressed models must align (i.e., the variance ratio must be close to 1), as variance collapse or explosion leads to suboptimal performance. Our data-free and fine-tuning-free model folding methods (Fold-AR and Fold-DIR) achieve performance comparable to data-driven statistics repair (Fold-R), while outperforming naive statistics repair (Fold-naive) and the recently proposed IFM [(Chen et al., 2023)](#b11). All methods were evaluated on a public ResNet18 checkpoint trained on CIFAR10. Lines connect the performance of different methods at the same weight sparsity level, applied uniformly across all layers. Variance ratio refers to the activation outputs in the last layer. A precise definition and analysis are in [Sec. 3. Alistarh, 2022;](#)[Hassibi et al., 1993)](#b28). This requirement can be a significant drawback in scenarios where access to the original training data is limited.

Recent methods have sought to circumvent the need for extensive retraining or fine-tuning by exploring alternatives to traditional approaches. Instead, several recent strategies build on model merging techniques [(Ainsworth et al., 2023;](#b0)[Entezari et al., 2022;](#b17)[Jordan et al., 2022)](#b39) and achieve (multi-)model compression by fusing similar computational units. For example, ZipIt! [(Stoica et al., 2024)](#b73) merges two models of the same architecture by combining similar features both within and across models. They provide both theoretical and empirical evidence suggesting that features within the same model are more similar than those between models trained on different tasks. This method avoids the need for retraining the compressed model but requires training data to match features based on the similarity of their activations. Similarly, [Yamada et al. (2023)](#b83) examine various model merging techniques and conclude that merged models require a dataset-such as a coreset-for effective merging and to achieve high accuracy. This data is essential for adjusting internal data statistics that are disrupted by weight fusion, such as updating the running mean and variance in BatchNorm layers [(Ioffe and Szegedy, 2015)](#b35). The process involves a simple forward pass through the model and is a well-established method to adapt models in low-resource environments [(Leitner et al., 2023)](#b49).

In contrast, IFM [(Chen et al., 2023)](#b11) offers a fully data-free and fine-tuning-free approach, utilizing weight matching [(Ainsworth et al., 2023)](#b0) to iteratively merge similar hidden units, similar to [Stoica et al. (2024)](#b73). However, despite a heuristic for preserving data statistics, we demonstrate that IFM fails to maintain performance across standard architectures and for high sparsity. Other data-free approaches, such as [(Yin et al., 2020)](#b84), generate synthetic images directly from the uncompressed model for fine-tuning to restore pruned model accuracy. More related work is covered in Appendix M.

This paper presents a model compression technique, model folding, that exploits weight similarity through three phases: neuron clustering, merging, and data statistics repair, summarized in [Fig. 1 (left)](#). We demonstrate that k-means clustering provides a theoretically optimal and data-free method for merging weights. Building on [Jordan et al. (2022)](#b39), which addresses variance collapse using REPAIR with training data, we introduce two data-free alternatives: Fold-AR (folding with approximate REPAIR) and Fold-DIR (folding with Deep Inversion-based REPAIR). Fold-AR estimates mean correlations within clusters assuming independent inputs, while Fold-DIR uses Deep Inversion [(Yin et al., 2020)](#b84) to synthesize a single batch of images for updating BatchNorm statistics via a forward pass. Both methods maintain data statistics and prevent variance collapse or explosion to avoid suboptimal compression performance, with Fold-AR standing out as a more resource-efficient option while still significantly surpassing existing methods. [Fig. 1 (right)](#) shows that the highest accuracy at any target sparsity is achieved when the mean variance ratio over the last layer between the compressed and uncompressed models stays close to one. Our contributions are:

• We introduce model folding, a novel model compression technique that merges structurally similar neurons within the same network to achieve compression. We provide both theoretical justification and empirical evidence demonstrating that k-means clustering is an optimal and effective method for fusing model weights in a data-free manner.

• To enable data-free model compression, we adapt the REPAIR framework proposed by [Jordan et al. (2022)](#b39) to address variance collapse of data statistics within a model after layer compression. We introduce data-free and fine-tuning-free versions of REPAIR, that effectively maintain model statistics and achieve high performance.

• We demonstrate that model folding surpasses the performance of SOTA model compression methods which do not use data or fine-tune the pruned model, including recently proposed IFM [(Chen et al., 2023)](#b11), and INN [(Solodskikh et al., 2023)](#b72), in particularly at higher levels of sparsity and when applied to more complex datasets.

• We use model folding on LLaMA-7B without utilizing data or post-tuning and achieve comparable results to methods that require data and fine-tuning.

## Preliminaries

Our work is inspired by recent advances in two key areas: neuron alignment algorithms for fusing model pairs in weight space, and data-driven methods for recovering from variance collapse in fused models. Below, we summarize the relevant results from the literature. Neuron alignment algorithms. Model merging involves combining the parameters of multiple trained models into a single model, with a key challenge being the alignment of neurons across these models, particularly when they are trained on different datasets or tasks. Neuron alignment methods can be classified based on their dependency on the input data. Methods like the Straight Through Estimator (STE) [(Ainsworth et al., 2023)](#b0), Optimal Transport (OT) [(Singh and Jaggi, 2020)](#b71) and correlation-based activation matching [(Li et al., 2015)](#b53) require data for effective merging. In contrast, weight matching [(Ainsworth et al., 2023;](#b0)[Yamada et al., 2023)](#b83) is a data-free method, making it efficient in scenarios when training data is not available. In weight matching, neurons are aligned by minimizing the L 2 distance between the weight vectors of neurons across models. Given two models with weight matrices W A and W B , the goal is to find a permutation P of the weights in W B that minimizes the distance:

$min P ∥W A -PW B ∥ 2 2 ,$where PW B denotes the weight matrix W B after applying the permutation P to align it with W A . Once the optimal permutation is found, the models are merged by averaging the aligned weights:

$W merged = 1 2 (W A + P * W B ) ,$where P * is the permutation that minimizes the L 2 distance. Weight matching solves an instance of the linear sum assignment problem (LSAP), usually solved by Hungarian algorithm [(Kuhn, 1955)](#b45) as done in [(Ainsworth et al., 2023;](#b0)[Jordan et al., 2022)](#b39), to layer-wise align weight vectors. Unlike merging different models, aligning neurons within a single model requires an acyclic matching graph, a challenge not addressed by LSAP, which For each layer, we use activation matching matching with L 2 distance measure to greedily pair similar neurons.

Each subplot shows the correlation within all matched pairs. assumes disjoint task and worker sets. To overcome the challenge Chen et al. ( [2023](#)) and [He et al. (2018)](#b30) apply iterative approach greedily merging a pair of the most similar neurons in each iteration. This work extends weight matching to align clusters of similar neurons within the same model, remaining data-free. We show that IFM is inferior to clustering utilized by model folding as described in the next section.

Variance collapse and REPAIR. When interpolating between independently trained, neuron-aligned networks, [(Jordan et al., 2022)](#b39) observed a phenomenon they termed variance collapse. This occurs when the variance of hidden unit activations in the interpolated network significantly diminishes compared to the original networks, leading to a steep drop in performance. To solve this issue, [Jordan et al. (2022)](#b39) introduce the REPAIR method (Renormalizing Permuted Activations for Interpolation Repair) which uses input data to recompute the internal data statistics. REPAIR works by rescaling the preactivations of the interpolated network to restore the statistical properties of the original networks. Specifically, it adjusts the mean and variance of the activations in each layer of the interpolated network to match those of the corresponding layers in the original networks. This is done by computing affine transformation parameters-rescaling and shifting coefficients-for each neuron, ensuring that the mean and standard deviation of activations in the interpolated network are consistent with those in the original models. REPAIR effectively mitigates the variance collapse, enabling the interpolated network to maintain performance closer to that of the original models. This technique has become essential in recent work to preserve model accuracy after merging [(Ainsworth et al., 2023;](#b0)[Jolicoeur-Martineau et al., 2024;](#b38)[Yamada et al., 2023)](#b83). While REPAIR relies on input data to preserve the network's statistical properties, this paper proposes a data-free alternative.

## Model Folding

In this section, we introduce model folding, a novel compression technique that reduces the computational complexity and size of neural networks by merging similar neurons in each layer without requiring training data. As illustrated in Fig. [1](#fig_0) (left), model folding processes the network layer by layer, involving filter clustering, merging, and correcting data statistics. Below, we present a theoretical analysis of our approach, supported by empirical results on ResNet18 using CIFAR10.

## Channel clustering

Channel similarity. Neural networks trained with stochastic gradient descent (SGD) tend to have many correlated hidden units, as illustrated in Fig. [2](#fig_1). Model folding exploits this observation, which is related to the implicit bias of SGD. As discussed in [(Gunasekar et al., 2017)](#b25), SGD exhibits a minimum norm bias, which can be viewed as a form of regularization when no explicit regularization is used. In contrast to L 1 regularization, which promotes sparsity, the minimum Euclidean norm solution (L 2 norm) penalizes large weights, encouraging smaller, more regular weights. This not only prevents overfitting but also results in smoother decision boundaries [(Bishop, 2006)](#b5). While the minimum norm solution does not directly enforce weight similarity, we empirically demonstrate in Appendix C that it leads to effective model compression when applying similarity-based methods. Recently published methods [(Chen et al., 2023;](#b11)[Stoica et al., 2024)](#b73) leverage the same observation. Folding as a clustering problem. This work extends weight matching [(Ainsworth et al., 2023)](#b0), which minimizes the L 2 distance between weight vectors and operates without requiring training data. Instead of finding pairs of similar neurons by solving the linear sum assignment problem (LSAP) with a Hungarian algorithm [(Kuhn, 1955)](#b45) as done in [(Ainsworth et al., 2023;](#b0)[Jordan et al., 2022)](#b39), we achieve channel matching using k-means clustering. In the following, we justify this approach as it provides an optimal weight matrix approximation.

Given a neural network layer l with a weight matrix W l ∈ R n×m , we define the output of this layer as y l = σ(W l x l ), where x l ∈ R m is the input vector to this layer, y l ∈ R n is the output vector, and σ(•) is a non-linear activation function applied element-wise.

To reduce the number of outputs of layer l we cluster (fold) rows of W l , i.e., k cluster centroids are determined which serve as a prototype of the respective cluster of rows. All rows of a cluster are replaced by their cluster centroid. This can be formulated as

$W l ≈ UM,$where M ∈ R k×m contains the k < n cluster centroids and the cluster matrix U ∈ {0, 1} n×k determines the membership of a row: u(i, j) = 1 if the i-th row of W l belongs to the j-th cluster, and u(i, j) = 0 otherwise.

As a measure of the approximation error when replacing the rows of W l by k < n prototypes, we use the Frobenius norm ∥ • ∥ 2 F of the difference between W l and the low-rank factorization UM:

$J = ∥W l -UM∥ 2 F = tr(W l W T l ) + tr(UMM T U T ) -2tr(UMW T l ).$We determine the optimal matrix of cluster centroids by setting the derivative of J with respect to M to zero:

$M = (U T U) -1 U T W l .$As a result, we can write

$W l ≈ UM = CW l with C = U(U T U) -1 U T .$As mentioned above, we use k-means clustering for folding as this minimizes J by determining the optimal clustering matrix U and the corresponding cluster centroids M , also see [(Bauckhage, 2015)](#b4). Interdependence between layers. We will expand the above result to successive layers l and l + 1. For simplicity of notation, we neglect the bias and get

$y l+1 = σ(W l+1 σ(W l x l )).$Following the above notation, we describe the folding of activations by some clustering matrix U and

$C = U(U T U) -1 U T . It is shown in Appendix B that the corresponding approximation satisfies ỹl+1 = σ(W l+1 σ((CW l )x l ) = σ((W l+1 C T )σ((CW l )x l ).$Adding up the individual folding costs J l+1 = ∥W T l+1 -CW T l+1 ∥ 2 F and J l = ∥W l -CW l ∥ 2 F yields the combined approximation error J l,l+1 = J l+1 + J l for folding layer l which can be rewritten as

$J l,l+1 = ∥W l,l+1 -CW l,l+1 ∥ 2 F with W l,l+1 = W l | W T l+1 .$If we perform k-means clustering on W l,l+1 and use the resulting clustering matrix U in C = U(U T U) -1 U T , then the combined approximation error J l,l+1 is minimized. This approach accounts for the impact of compressing one layer on the next, leading to more efficient compression that balances the process and preserves learned representations while reducing model size. Our folding methods outperforms other methods experimentally, see Fig. [3](#fig_2) for a comparison to other clustering methods and Iterative Greedy (greedy) adopted in SOTA. Batch Normalization. Now, let us consider batch normalization in layer l represented by two diagonal matrices Σ s (scaling) and Σ n (normalization), again neglecting the bias to reduce notation. In this case, we get

$y l+1 = σ(W l+1 σ(Σ s Σ n W l x l )).$The folding of layer l can be distributed to the matrices Σ s , Σ n , and W l in various ways, depending on the chosen correction of the variance, see Sec. 3.2. For example, one can cluster each matrix separately, leading to

$ỹl+1 = σ((W l+1 C T )σ((CΣ s )(CΣ n )(CW l )x l )).$Adding up the individual folding costs J l+1 , J s , J n , and J l for each of the matrices W l+1 , Σ s , Σ n and W l , respectively, yields the total approximation error J tot = J l+1 + J s + J n + J l for folding layer l

$J tot = ∥W tot -CW tot ∥ 2 F with W tot = W T l+1 | W l | diag(Σ s ) | diag(Σ n )$If we perform k-means clustering on W tot then the total approximation error J tot is minimized. This approach is used in the Deep Inversion (DI) REPAIR, see next section.

Instead, if we decompose the folding of layer l according to

$ỹl+1 = σ((W l+1 C T )σ((CΣ s )(CΣ n W l )x l )).$then the individual folding costs of W l+1 , Σ s and the normalized weight matrix Σ n W l add up to

$J tot = ∥W tot -CW tot ∥ 2 F with W tot = Σ n W l | diag(Σ s ) | W T l+1 .$Again, if we perform k-means clustering on this combined matrix W tot then the corresponding total approximation error J tot is minimized. This approach is used in the approximate REPAIR, see Sec. 3.2. For completeness, we present in Appendix E how we handle residual connections.

Merging similar channels in each cluster. To fuse similar channels, various approaches have been proposed in the literature, such as fusing weights for multitasking, which involves Hessian calculations [(He et al., 2018)](#b30), or by combining the matched weights into a single channel [(Chen et al., 2023)](#b11). [(Matena and Raffel, 2022)](#b59) introduces Fisher-weighted averaging based on the Laplace approximation for merging weights, while [(Jin et al., 2023)](#b37) suggests computing a regression mean, which is both computationally efficient and scalable for merging multiple models. In our approach, we use above formulation of the optimization problem as k-means clustering and use a simple mean to compute the cluster centroids.

## Maintaining data statistics in a compressed model

Variance collapse and variance overshooting. We use the conceptual framework in [(Jordan et al., 2022)](#b39) to analyze the performance of model compression methods. We use the following definition.

Definition 3.1 (Variance ratio). Consider a neural network f (x, Θ) with layer activations {x l } L 1 and its compressed version f (x, Θ) with activations {x l } L 1 . The variance ratio of the l-the layer is:

$µ Var(x l ) Var(x l ) = 1 |x l | |x l | k=1 Var(x l,k ) Var(x l,k ) .$We observe not only variance collapse but also variance overshooting phenomena. Specifically, when data statistics are not accurately corrected after channel merging, as in IFM, variance overshooting can occur, leading to network performance decline. Fig. [4](#fig_4) shows layerwise variance ratio between the compressed and uncompressed networks. Staying close to 1 is essential to mitigate both phenomena. This highlights the critical need for precise statistical corrections during model merging. Fold-AR: Folding with approximate REPAIR. In the context of model compression, particularly when using folding as a clustering method, it is crucial to ensure that the compressed model maintains accurate data statistics. This is especially important for layers involving operations like BatchNorm, where maintaining the correct statistical properties of activations is vital for model performance [(Jordan et al., 2022;](#b39)[Yamada et al., 2023)](#b83).

In the following explanation of the data-free approximate REPAIR, we neglect biases for ease of notation. Following the previous section, we consider folding of the normalized weight matrix with z l = CΣ n W l x l using the post-activation output x l of the previous layer and the input z l to the scaling matrix Σ s . A cluster c is defined by the column of the clustering matrix U , i.e., all values z l (i) with u(i, c) = 1 belong to cluster c. Moreover, by definition of C, all values z l (i) belonging to a single cluster c equal the centroid ẑl (c) of the cluster, i.e., the average of all values Σ n W l x l belonging to this cluster. More formally,

$∀u(i, c) == 1 : z l (i) = ẑl (c) ∀1 ≤ c ≤ k : ẑl (c) = 1 N c i∈Ic xl (i),$where The averaging over all xl (i) belonging to a single cluster destroys this property and leads to the observed variance collapse. We will describe various methods to compensate this loss in variance, at first the data-free approximate REPAIR (Fold-AR).

$I c = {i : u(i, c) = 1}$The variance of the cluster centroid ẑl (c) of cluster c is given by

$Var(ẑ l (c)) = 1 N 2 c   i∈Ic Var(x l (i)) + i,j∈Ic;i̸ =j Cov(x l (i), xl (j))   , which further simplifies to Var(ẑ l (c)) = 1 N 2 c N c + (N 2 c -N c )E[c]$, where E[c] is the mean correlation within the cluster. To prevent variance collapse, we aim for Var(ẑ l (c)) = 1, which would occur if E[c] = 1, meaning  all channels in the cluster are fully correlated. However, as E[c] < 1 typically, we multiply each cluster centroid by a scaling parameter assuming an average cluster correlation

$E[c] ẑl (c) ← ẑl (c) N c N c + (N 2 c -N c )E[c]$.

Suppose now that the covariance matrix Σ x l of the output x l of the previous layer is available and that we define the normalized weight matrix Wl = Σ n W l with rows wl (i). Then the correlation E[c] can be computed as:

$E[c] = 1 N 2 c -N c i,j∈Ic;i̸ =j wl (i)Σ x l wT l (j) ( wl (i)Σ x l wT l (i))( wl (j)Σ x l wT l (j))$.

In the absence of data, E[c] can be estimated by assuming that the output values x l of the previous layer are uncorrelated. As the individual variances of xl (i) equal 1 we obtain

$E[c] = 1 N 2 c -N c i,j∈Ic;i̸ =j wl (i) wT l (j) ( wl (i) wT l (i))( wl (j) wT l (j))$.

We term this approach to maintain the data statistics within the model folding with approximate REPAIR (Fold-AR). This approach helps to ensure that the statistical properties of the data are preserved even after model compression, maintaining the performance of the network while reducing its size. Fig. [5](#fig_5) shows how the performance of Fold-AR compares to the data-driven REPAIR (Fold-R) and surpasses the SOTA data-free methods.

Fold-DIR: Correcting data statistics with deep inversion. Deep Inversion (DI) [(Yin et al., 2020](#b84)) is a technique that synthesizes realistic images directly from a pre-trained neural network without requiring access to the original data. The process involves inverting the model by optimizing random noise to produce class-conditional images that match the statistics of the data the model was trained on [(Mordvintsev et al., 2015)](#b64). DI leverages the BatchNorm layers within the network, which store the running mean and variance of activations during training. By using these stored statistics as a regularization term in We leverage a single batch of DI-synthesized data within model folding to preserve data statistics after channel merging, eliminating the need for training data. By generating synthetic images aligned with the network's internal statistics, DI recalibrates the folded model's parameters, ensuring that activation variance and mean are maintained. This helps the model retain its performance post-folding, mitigating issues such as variance collapse or explosion without requiring the original dataset. Notably, updating BatchNorm statistics requires only a forward pass, with no backpropagation needed. Thus, Fold-DIR offers a data-free and fine-tuning-free solution for maintaining data statistics. Fig. [5](#fig_5) shows that Fold-DIR closely follows the performance of the data-driven REPAIR (Fold-R), effectively maintaining the data statistics within the model. Fold-DIR ourperforms Fold-AR as the cost of generating a batch of synthetic images and a forward pass through the network.

$R(x) = L class (x, t) + l ∥µ(x l ) -µ(x l )∥ 2 2 + l ∥Var(x l ) -Var(x l )∥ 2 2 + ∥x∥ 2 2 + ∥x∥ T V ,$
## Relationship Between Weight Matching and Model Folding

Weight Matching [(Ainsworth et al., 2023)](#b0) fuses two models into one, whereas Model Folding compresses the weight tensors/matrices of a single network. While inspired by Weight Matching, Model Folding addresses a distinct use case, leading to different optimization problems (K-Means vs. LAP). Notably, the Linear Sum Assignment Problem (LAP) can be framed as a constrained K-Means variant, where each cluster contains exactly two vectors: one from network A and one from network B.

As an example for this discussion, consider a simple feedforward network. The steps of our proposed compression algorithm involve iteratively solving the following:

$C l = arg min C l ∥W l -C l W l ∥ 2 F + ∥W T l+1 -C l W T l+1 ∥ 2 F , such that C l = U l (U T l U l )U T l$, where U T l is a clustering matrix. Weight Matching merges two feedforward networks by iteratively optimizing:

$P l = arg min P l ∥W A,l -P l W B,l ∥ 2 F + ∥W T A,l+1 -P l W T B,l+1 ∥ 2 F ,$where P l is a permutation matrix. To connect Weight Matching with our method, we frame our approach within the model merging domain. This begins by establishing a relationship between K-Means and the Linear Sum Assignment (LAP) problem. K-Means and LAP Connection. In the standard K-Means formulation, given a dataset represented as rows of a matrix X ∈ R n×d , the objective is to cluster these rows into k groups. This can be represented as:

$C = arg min C ∥X -CX∥ 2 F ,(1)$where C ∈ R n×n is a clustering matrix satisfying: (1) each row of C corresponds to a single cluster assignment; and (2) C has a block-diagonal structure that assigns each row of X to a single cluster centroid.

The clustering matrix C can be explicitly written in terms of a matrix U ∈ R n×k as:

$C = U(U T U) -1 U T ,$where U encodes the cluster assignments and centroids.

To connect this with LAP, let X be the concatenation of rows from two matrices W A and W B (e.g., weights from two neural networks):

$X = W A W B , such that C = P I ,$where (1) P is a permutation matrix representing a one-to-one mapping between rows of W A and W B ; and

(2) I is the identity matrix, allowing for exact cluster assignments during merging. Under this constraint, C enforces a specific structure, aligning rows of W A and W B pairwise. Substituting C into Equation 1, we get:

$P = arg min P ∥ W A W B -P W A W B ∥ 2 F .$This is an instance of the Linear Sum Assignment Problem. Minimizing the cost:

$J = ∥ W A W B -P W A W B ∥ 2 F ,$is equivalent to maximizing:

$J + = tr P W A W B W A W B T .$Model Folding. Building on these results, we define Model Folding for merging networks as follows:

$J l = W l,A W l,B -C l W l,A W l,B 2 F + W l+1,A W l+1,B -W l+1,A W l+1,B C T l 2 F .$Constraining C l to C l = P I , where P is a permutation matrix, yields the Weight Matching [(Ainsworth et al., 2023)](#b0) coordinate descent cost:

$J l = 1 2 ∥W l,A -P l W l,B ∥ 2 F + 1 2 W T l+1,A -P l W T l+1,B 2 F .$Model Folding for Connecting Models. We provide a small experimental setup comparing WM [(Ainsworth et al., 2023)](#b0), ZipIt! [(Stoica et al., 2024)](#b73), and our proposed method for merging networks trained on the same task and networks trained on separate tasks. For the experiments involving merging networks trained on disjoint tasks (see Table 2: Performance comparison for merging networks trained on the same task.  Comparison to structured pruning. We compare model folding with the structured magnitude pruning (SP) method used in [(Cai et al., 2020;](#b7)[Yin et al., 2022)](#b85), based on L 1 and L 2 norms, without fine-tuning. Fig. [6](#fig_6) demonstrates that model folding significantly outperforms magnitude pruning, with the performance gap widening as sparsity increases. At 70% sparsity, the folded ResNet18 on CIFAR10 maintains over 80% accuracy, while pruned networks barely surpass random chance. On ImageNet, the performance collapse is even more pronounced across all methods due to the dataset's higher complexity, yet model folding consistently performs well across both datasets. Following [(Chen et al., 2023)](#b11), Fig. [7](#fig_7) (right) compares model folding with the SOTA data-free pruning method INN [(Solodskikh et al., 2023)](#b72), which struggles to manage even moderate sparsity.

Folding LLMs. LLMs are built with a large number of parameters, achieving strong performance across various tasks. However, structurally compressing these deep and large models remains a challenge. LLM-Pruner [(Ma et al., 2023)](#b58) performs structured pruning using gradient calculations, while Wanda [(Sun et al., 2023)](#b74) leverages an importance score by multiplying weights with their corresponding input activations. FLAP [(An et al., 2023)](#b1) dynamically computes a fluctuation pruning metric using calibration data. In Tab. 3, we compare model folding with these methods on LLaMA-7B [(Touvron et al., 2023a)](#), focusing on perplexity on the WikiText2 [(Merity et al., 2016)](#b61) validation set and zero-shot performance across four tasks using the EleutherAI LM Harness [(Gao et al., 2024)](#). The folded model performs only very slightly worse than models compressed with data-driven methods. Following SOTA, the clustering phase of model folding was applied to LLaMA-7B, introducing 20% and 50% sparsity in the attention and feed-forward layers of decoder blocks 22-29, and 10% and 40% sparsity in the attention and feed-forward layers of decoder blocks 11-21, respectively. As there is no batchnorm layer in LLaMA-like LLMs, we just applied clustering in LLMs without

Prune ratio Method Data usage WikiText2↓ BoolQ WinoGrande ARC-e ARC-c Average↑ 0% LLaMA-7B (Touvron et al., 2023a) / 5.68 75.05 69.93 75.34 41.89 65.55 20% Magnitude Prune / 36136 43.21 49.40 27.23 21.59 35.36 20% LLM-Pruner (Ma et al., 2023) Gradients 10.53 59.39 61.33 59.18 37.18 54.27 20% FLAP (An et al., 2023) Calibration 6.87 69.63 68.35 69.91 39.25 61.79 20% Wanda_sp (Sun et al., 2023) Calibration 8.22 71.25 67.09 71.09 42.58 63.00 20% SliceGPT (Ashkboos et al., 2024) Calibration 7.00 57.80 67.96 62.67 36.01 56.11 20% ShortGPT (Men et al., 2024) Calibration 15.48 62.17 67.40 58.88 31.91 55.09 20% Model Folding / 13.33 62.29 62.19 49.83 26.37 50.17

Table 3: Performance of structured pruning methods on LLaMA-7B without post-tuning, showing perplexity on WikiText2 and zero-shot performance across tasks. The "Average" is computed over four tasks. "Wanda_sp" represents an adapted Wanda method for structured pruning. Despite not using data or fine-tuning, model folding achieves comparable performance to data-driven methods. REPAIR. Tab. 5 shows the generated examples of dense and folded LLaMA-7B processed by model folding without REPAIR in Appendix D. Results of folding LLaMA2-7B (Touvron et al., 2023b) are also provided in Appendix D. When folding with 20% sparsity, the pruned model continues to perform well.

Fine-Tuning-Free and Data-Free Folding for LLMs. While modern LLMs are trained on extensive datasets, access to such data or related domains is not always feasible in real-world scenarios. In regulated industries such as healthcare, finance, or defense, where data is often sensitive or proprietary, even general public datasets may not be suitable for fine-tuning or compression. Our work specifically addresses data-free settings, offering a robust solution for compressing LLMs without requiring any data or fine-tuning. To illustrate the importance of this setting, we demonstrate that using a suboptimally chosen, out-of-distribution (OOD) calibration dataset can result in worse performance compared to our data-free Model Folding approach.

For example, we generated a dataset of random Hungarian words in repeated sequences and applied the Wanda compression method to LLaMA-7B. Although LLaMA-7B was trained on some Hungarian text, the language is underrepresented in its training corpus. Using this OOD calibration dataset, the perplexity on the WikiText2 benchmark increased from 8.22 (with the original C4 dataset) to 13.98. A similar performance drop (perplexity = 13.94) was observed with a Ukrainian dataset, highlighting the sensitivity of data-driven methods like Wanda to the domain alignment of the calibration data. These results highlight the robustness of data-free approaches like Model Folding in scenarios where appropriate calibration data is unavailable. Note that further optimization of these experiments is possible (we explored only a limited set of options), yet they showcase the challenges faced by data-driven methods with OOD calibration data.

## Conclusion

In this paper, we introduce model folding, a novel compression technique that reduces model size by merging similar channels across layers, without requiring fine-tuning or training data. Model folding achieves high sparsity while preserving data statistics, outperforming traditional pruning and data-free compression methods.

Our experiments demonstrate that wider networks, such as VGG11 and ResNet50, offer greater opportunities for folding due to increased redundancy, further improving compression efficiency. In LLMs, model folding can prune models while maintaining performance comparable to data-driven methods, but without the need for data access or fine-tuning, which are typically required by most structured pruning techniques.

Limitations and future work. Model folding offers significant compression without data or fine-tuning, but its effectiveness may be limited in networks with low redundancy. Additionally, it does not optimize sparsity levels per layer, leaving this for future work.

Lemma B.2. Let x ∈ R k , let U ∈ {0, 1} n×k be a binary clustering matrix with j u ij = 1, let σ(•) be an element-wise nonlinear function, and define

$C = U(U T U) -1 U T . Then σ(Cx) = C T σ(Cx)$Proof of Lemma B.2. We can write

$σ(Cx) = σ(U(U T U) -1 U T x) = Uσ((U T U) -1 U T x) (Lemma B.1) = U(U T U) -1 (U T U)σ((U T U) -1 U T x) = U(U T U) -1 U T σ(U(U T U) -1 U T x) (Lemma B.1) = C T σ(Cx).$Lemma B.3. Let U T be a clustering matrix and let D be a diagonal matrix, then the following is true

$(U T U) -1 U T DU = Diag((U T U) -1 U T diag(D))$Proof of Theorem B.3. The clustering matrix U T can be expressed as:

$U T =      u T 1 u T 2 . . . u T k      =      u 11 u 12 . . . u 1n u 21 u 22 . . . u 2n . . . . . . . . . . . . u k1 u k2 . . . u kn     $, where u T i represents the rows of the clustering matrix. Each row corresponds to cluster i, and the entries u ij satisfy the binary clustering property: u ij = 1 if the j-th data point belongs to cluster i, and u ij = 0 otherwise.

The product DU is given by:

$DU =      d 1 0 . . .$0 0 d 2 . . . 0 . . . . . . . . . . . . 0 0 . . . d n           u 11 u 12 . . . u 1k u 21 u 22 . . . u 2k . . . . . . . . . . . . u n1 u n2 . . . u nk      . This simplifies to: DU =      d 1 u 11 d 1 u 12 . . . d 1 u 1k d 2 u 21 d 2 u 22 . . . d 2 u 2k . . . . . . . . . . . .

$d n u n1 d n u n2 . . . d n u nk      .$Using the clustering property of U, it follows that:

$u ij u i ′ j = 1, if i = i ′ , 0, otherwise.$From this, the product U T DU simplifies to:

$U T DU = Diag(U T diag(D)).$This result holds because only the diagonal entries remain due to the clustering matrix's orthogonality and binary properties.

Finally, using the above result, we compute:

$(U T U) -1 U T DU = (U T U) -1 Diag(U T diag(D)).$By the property diag(Diag(x)) = x for any x ∈ R n , we obtain:

$(U T U) -1 U T DU = Diag((U T U) -1 U T diag(D)).$The lemma demonstrates that projecting the diagonal matrix D through the clustering matrix U T preserves its diagonal structure. The diagonal entries are determined by the clustering matrix's mapping of the original diagonal values diag(D), ensuring efficient computation and alignment with clustering properties.

Lemma B.4. Let U T be a clustering matrix and let w ∈ R n and x ∈ R n , then the following is true UDiag(w)x = Diag(Uw)Ux Proof of Lemma B.4. The clustering matrix U can be expressed as:

$U =      v T 1 v T 2 . . . v T n     $, where each row v T m is defined by a mapping function f : {1, 2, . . . , n} → {1, 2, . . . , k}. For each row v T m , the entries are defined as:

$v m,j = 1, if j = f (m), 0, otherwise.$This representation indicates that the clustering matrix U assigns each element m to a specific cluster f (m).

Each row v T m has a single non-zero element corresponding to the cluster index f (m).

Calculation of the Left-Hand Side (LHS). The left-hand side of the equality is:

$UDiag(w)x.$First, compute Diag(w)x, which scales each element of x by the corresponding element of w:

$Diag(w)x =      w 1 x 1 w 2 x 2 . . . w n x n      .$Then, multiplying by U aggregates these scaled values according to the clusters defined by f . Specifically, the j-th element of UDiag(w)x is given by:

$(UDiag(w)x) j = m:f (m)=j w m x m .$Calculation of the Right-Hand Side (RHS). The right-hand side of the equality is:

$Diag(Uw)Ux.$First, compute Uw. The j-th element of Uw is:

$(Uw) j = m:f (m)=j w m ,$which sums the w m values for all elements assigned to cluster j.

Next, construct Diag(Uw), a diagonal matrix with entries (Uw) j along the diagonal:

$Diag(Uw) =      (Uw) 1 0 . . . 0 0 (Uw) 2 . . . 0 . . . . . . . . . . . . 0 0 . . . (Uw) k      .$Finally, compute Ux. The j-th element of Ux is:

$(Ux) j = m:f (m)=j$x m , which sums the x m values for all elements assigned to cluster j.

Multiplying Diag(Uw) by Ux gives:

$(Diag(Uw)Ux) j = (Uw) j (Ux) j =   m:f (m)=j w m     m:f (m)=j x m   .$Verification of Equality. Both the LHS and RHS compute the same aggregated sums m:f (m)=j w m x m for each cluster j. The LHS directly performs the aggregation of w m x m within clusters, while the RHS separates the computation into two steps: summing w m and x m for each cluster, followed by multiplying these sums. Since multiplication distributes over addition, the two expressions are equivalent:

$UDiag(w)x = Diag(Uw)Ux.$The lemma is proven, as both sides of the equation compute the same weighted aggregation of w m x m over the clusters defined by the clustering matrix U.

Lemma B.5. Let C T be a clustering matrix and let D be a diagonal matrix, then the following is true

$∥W -Diag(Cdiag(W))∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2$Proof of Lemma B.5. Let W = Diag(Cdiag(W)), where W represents the diagonal matrix obtained by clustering the diagonal entries of W using the clustering matrix C. Both W and W are diagonal matrices, so their difference W -W is also diagonal. The entries of this difference are:

$w i,j -wi,j = w i,i -wi,i , if i = j, 0, otherwise.$The Frobenius norm of the difference W -W is:

$∥W -W∥ 2 F = i,j (w i,j -wi,j ) 2 .$Since W and W are diagonal matrices, this simplifies to:

$∥W -W∥ 2 F = i (w i,i -wi,i ) 2 .$The diagonal entries of W can be represented as a vector diag(W), and the diagonal entries of W are given by Cdiag(W). Substituting these representations, we have:

$∥W -W∥ 2 F = i (diag(W) i -(Cdiag(W)) i ) 2 .$This is equivalent to the squared ℓ 2 -norm of the difference between the vectors diag(W) and Cdiag(W), giving: ∥W -W∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2 . Substituting back W = Diag(Cdiag(W)), we conclude that:

$∥W -Diag(Cdiag(W))∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2 .$Lemma B.6. Let A ∈ R n×n and B ∈ R n×n be diagonal matrices, then:

$AB = Diag(Adiag(B))$Proof of Lemma B.6. Since both A and B are diagonal matrices, their product AB is also a diagonal matrix.

The entries of the product AB are given by:

(AB) i,j = a i,j b i,j .

For diagonal matrices, all off-diagonal entries are zero, so:

$(AB) i,j = a i,i b i,i , if i = j, 0, otherwise.$Thus, the diagonal entries of AB are a i,i b i,i , and the matrix AB is:

$AB =      a 1 b 1 0 . . . 0 0 a 2 b 2 . . . 0 . . . . . . . . . . . . 0 0 . . . a n b n     $, where a i = a i,i and b i = b i,i represent the diagonal entries of A and B, respectively. Now, let diag(B) denote the vector of diagonal entries of B, i.e.,

$diag(B) =      b 1 b 2 . . . b n     $.

The operation Adiag(B) represents the element-wise multiplication of the diagonal entries of A and B:

$Adiag(B) =      a 1 b 1 a 2 b 2 . . . a n b n      .$Next, using the function Diag(•), we can construct a diagonal matrix from this vector:

$Diag(Adiag(B)) =      a 1 b 1 0 . . . 0 0 a 2 b 2 . . . 0 . . . . . . . . . . . . 0 0 . . . a n b n      .$Clearly, AB and Diag(Adiag(B)) are identical, as they both produce the same diagonal matrix with entries a i b i along the diagonal. Therefore:

$AB = Diag(Adiag(B)).$
## C Channel similarity

Models learned by SGD trend to have correlated patterns or similar parameters in the weight space. Fig. [9](#) shows 3 × 3 filter weights in conv1 of a pre-trained ResNet18. These filters across the first 3 input channels and first 16 output channels ordered by the entropy of filter weight. From the plot, most filters of a channel can find at least one another similar filter in other channels, which means filter similarity may lead to structured redundancy.

Figure [9](#): Similar patterns in weight map of conv1 layer in ResNet18 pre-trained on Ima-geNet [(Deng et al., 2009)](#b15). Each small square represents the weights of a single filter in cool-warm color map, where each color of grid corresponds to a weight value.

To investigate the filter redundancy within a layer, we apply weight matching activation matching from the literature [(Jordan et al., 2022)](#b39) to each layer of ResNet18 pretrained on CIFAR10 [(Krizhevsky et al., 2009a)](#) in Fig. [2](#fig_1) and on ImageNet [(Deng et al., 2009)](#b15) in Fig. [10](#fig_0). We observe two findings: (1) The correlation score distribution varies across layers. The earlier and narrower the lay ers are, the more scattered the correlation coefficients are, and only a few have high correlation coefficients. The wider and later the layers are, the more compact the correlation coefficients are, and most of the matching channels have high correlation coefficients.

(2) In the same layer, the distribution of correlation coefficients among matched channels differs across various pre-training datasets. This observation does not fully align with the claim by [Chen et al. (2023)](#b11) regarding the downward trend of similarity before a reversal. It appears that this characterization might not consistently hold across different models and pre-trained dataset.

Figure [10](#fig_0): Layer-wise correlation between matched channels in ResNet18 trained on ImageNet. We compute a layer-wise correlation matrix by matching activations between channels, then assign each channel its best match in the same layer using a greedy pairing based on the correlation matrix.

## C.1 The impact of regularization

In Fig. [6](#fig_6), the models on CIFAR10 were trained without regularization, while the pre-trained ImageNet models were sourced from torchvision. In Fig. [11](#fig_9), we extend the comparison of folding and pruning methods on CIFAR10, including ResNet18 (left column) and VGG11 (right column) models trained with explicit L 1 and L 2 regularization. L 1 regularization, in particular, promotes neuron sparsity, leading structured magnitude pruning methods to outperform model folding under these conditions. However, a comparison between Fig. [6](#fig_6) and Fig. [11](#fig_9) shows that model folding with L 2 regularization maintains the highest accuracy at higher sparsity levels, surpassing 80% accuracy. In contrast, the accuracy of the pruned network trained with L 1 drops significantly, reaching just 33% at 75% sparsity. 

## C.2 Folding wider models

Do wider networks present more opportunities for model folding? We first examine the layer-wise correlation among matched channels in VGG11 and its wider variants on CIFAR10, as shown in Fig. [8](#fig_8). This ablation study reveals that increasing the layer width strengthens the matched correlations, suggesting greater potential for folding. Building on this, Fig. [12](#fig_10) demonstrates the application of model folding also to 1x/2x/3x wider MLP and ResNet50 architectures, trained on CIFAR10 and CIFAR100, showing consistent performance gains as width increases.

## D Model Folding on LLMs

Table [5](#) presents example outputs from both the original and the pruned LLaMA-7B models, as processed by model folding. From the responses presented in Table [5](#), it is evident that when folding 20% of the parameters, the pruned model continues to perform well. In Tab. 4, we also compare model folding with these methods on LLaMA2-7B [(Touvron et al., 2023b)](#), focusing on perplexity on the WikiText2 [(Merity et al., 2016)](#b61) validation set and zero-shot performance across four tasks using the EleutherAI LM Harness [(Gao et al., 2024)](#). We take the same folding sparsity as shown in Tab. 3.  showing perplexity on WikiText2 and zero-shot performance across tasks. "Inf" represents an extremely great value. The "Average" is computed over four tasks. "Wanda_sp" represents an adapted Wanda method for structured pruning. Despite not using data or fine-tuning, model folding achieves comparable performance to data-driven methods.

## E Handling Residual Blocks

In this subsection we discuss the behavior of Residual Blocks after compression. In a similar manner to the analysis of Normalized Blocks, we investigate the possible dependencies between the clustering matrices for different parts of the residual block and the incoming layers.

## E.1 Simple Residual Blocks

Consider a Simple Residual Block, consisting of a shortcut represented by an identity transform W l,s = I, and a preceding layer decomposed using a clustering matrix U l-1 . The projection matrix is defined as:

$C l-1 = U l-1 U T l-1 U l-1 -1 U T l-1 .$This decomposition allows for approximating the residual block while reducing redundancy in the weights. The residual block approximation satisfies:

$y l ≈ σ W (2) l σ W (1) l C T l-1 x l-1 + C T l-1 x l-1 ,$where x l-1 is the input to the block, y l is the output, and σ(•) represents the activation function.

The shortcut W l,s = I ensures that the input x l-1 is directly added to the output of the main path, preserving information and facilitating gradient flow.

Total Approximation Error. The total approximation error for folding the residual block is defined as:

$J tot = ∥W tot -C (2) l W tot ∥ 2$F , where:

$W tot = W l-1 W (2) l .$Here, W tot combines the weights of both layers in the residual block into a single representation. This unified view allows the clustering process to be applied holistically, ensuring that redundancies across the entire block are captured and reduced. By asserting U l-1 = U

(2) l and summing the individual folding costs J

(2) l and J l-1 , we achieve a compact representation of the residual block with minimal approximation error. This approach ensures that the compressed residual block remains effective while reducing redundancy in the weights.

## E.2 Residual Blocks with Non-Identity Shortcuts

Consider a Residual Block with a shortcut represented by a weight matrix W l,s , and a preceding layer decomposed using a clustering matrix U l-1 . The projection matrix is defined as:

$C l-1 = U l-1 U T l-1 U l-1 -1 U T l-1$. This decomposition allows for approximating and clustering the preceding layer's weights while maintaining their representational capacity. The corresponding approximation for the residual block satisfies:

$y l ≈ σ W (2) l σ W (1) l C T l-1 x l-1 + W l,s C T l-1 x l-1 ,$where:

• W

(2) l

is the weight matrix of the second layer in the residual block,

• W

(1) l

is the weight matrix of the first layer in the residual block,

• W l,s is the shortcut connection weight matrix,

• σ(•) represents the activation function.

Decomposition of Weight Matrices. The weights W

(2) l and W l,s are decomposed using their respective clustering matrices. For W

(2) l , the decomposition is:

$C (2) l = U (2) l U (2)T l U (2) l -1 U (2)T l .$For W l,s , the decomposition is:

$C l,s = U l,s U T l,s U l,s -1 U T l,s$. Substituting these decompositions into the approximation yields:

$y l ≈ σ C (2) l U (2)T l W (2) l σ W (1) l C T l-1 x l-1 + C l,s W l,s C T l-1 x l-1 .$Consistency Constraint and Total Approximation Error. To simplify the folding process and ensure consistency across the layers, we introduce the constraint:

$U l,s = U (2)$l . This ensures that the same clustering matrix is used for both the shortcut weights W l,s and the second layer's weights W

(2) l . By adding the individual folding costs J

(2) l and J l,s , we ensure that Lemma B.1 holds, leading to the total approximation error for the residual block:

$J tot = J (2) l + J l,s .$Unified Approximation for Residual Blocks. The total approximation error can be expressed compactly as:

$J tot = ∥W tot -C (2) l W tot ∥ 2 F , where: W tot = W l,s | W (2) l .$Here, W tot combines the shortcut weights W l,s and the second-layer weights W

(2) l into a single matrix. This unified representation allows the folding process to be applied holistically, reducing redundancies across the entire residual block.

The decomposition of weights in residual blocks with non-identity shortcuts introduces a consistent clustering mechanism for both the shortcut and the second layer. By ensuring that U l,s = U

(2) l , we maintain alignment in the clustering process, leading to a compact and efficient representation with minimal approximation error.

## F Handling Batch Normalization Layers

Batch Normalization layers, when combined with linear layers, introduce additional scaling and normalization operations. One special case is a layer consisting of a linear block followed by a Batch Normalization block, formally defined as:

$z l+1 = W l+1 σ(Σ s Σ n W l x l-1$), where:

• W l : weight matrix of the linear block,

• Σ s : Batch Normalization scaling matrix,

• Σ n : Batch Normalization normalization matrix,

• W l+1 : weight matrix of the subsequent layer,

• σ(•): activation function applied element-wise.

A design choice in handling such layers is to decompose Σ s , Σ n , and W l separately while preserving the original structure of the layer. This ensures that the scaling, normalization, and linear blocks are treated as distinct functional units. The decomposed approximation for the layer can then be expressed as:

$z l+1 ≈ zl+1 = W l+1 C T s σ(C s Σ s C n Σ n C l W l x l-1$), where the projection matrices C s , C n , and C l are defined as:

$C s = U s (U T s U s ) -1 U T s = U s M s , C n = U n (U T n U n ) -1 U T n = U n M n , C l = U l (U T l U l ) -1 U T l = U l M l .$Here, U s , U n , and U l are clustering matrices, and M s , M n , and M l are normalization terms.

Clustering Assumptions. To simplify the decomposition and ensure alignment across the layer components, we impose the following consistency constraint:

$U s = U n = U l .$This assumption ensures that the same clustering structure is applied to the scaling, normalization, and linear blocks, leading to a unified decomposition. Under this assumption, the approximation becomes:

$zl+1 = W l+1 C T l σ(U l M l W b,l U l M l Σ n U l M l W l x l-1$), where W b,l represents the intermediate scaling factors.

Applying Diagonal Properties. Using Lemma B.3, we observe that the normalization and scaling matrices can be represented as diagonal matrices:

$zl+1 = W l+1 C T l σ(U l Diag(M l diag(W b,l ))Diag(M l diag(Σ n ))M l W l x l-1 ).$Furthermore, by applying Lemma B.4, we rewrite this expression as:

$zl+1 = W l+1 C T l σ(Diag(C l diag(W b,l ))Diag(C l diag(Σ n ))C l W l x l-1 ).$This shows that the diagonal structure of the scaling and alignment matrices is preserved through the decomposition, maintaining the original behavior of the Batch Normalization block.

Compression Cost. According to the definition of the Model Folding problem and using the properties stated in Lemma B.5, the compression cost for the layer can be expressed as:

$J tot = ∥W tot -C l W tot ∥ 2 F ,$where:

$W tot = W T l+1 W l diag(Σ s ) diag(Σ n$) . This cost quantifies the approximation error introduced by clustering the weights, scaling, and normalization matrices while preserving the layer's functional structure.

By decomposing the Batch Normalization and linear blocks separately and aligning their clustering structures (U s = U n = U l ), we ensure that the original diagonal properties of the scaling and normalization matrices are preserved. The resulting compression cost captures the overall error of folding the entire layer into a compact representation.

## F.1 Algorithmic Description of Fold-AR

The Fold-AR algorithm for a single layer combines the Batch Normalization components and layer weights into a compact representation, followed by clustering to reduce redundancy. The steps are described in Algorithm 1.

Algorithm 1 Fold-AR for a Single Layer

$Require: Σ s , Σ n , W l , W l+1$▷ Input components of the layer 1: Compute the normalized weight matrix: Ŵl ← Σ n W l 2: Construct the combined weight matrix: W tot ← W T l+1 Ŵl diag(Σ s ) 3: Solve the clustering problem: Compute intra-cluster correlation:

$U ← arg min U ∥W tot -U(U T U) -1 U T W tot ∥ 2$$E[c] ← 1 N 2 c -N c i,j$ŵl,i,: • ŵT l,j,:

∥ ŵl,i,: ∥ 2 ∥ ŵl,j,:

$∥ 2 I(U i,c = U j,c = 1)I(i ̸ = j) 10:$Update the scaling factor for cluster c:

$(Σ s ) c,c ← (Σ s ) c,c N c N c + (N 2 c -N c )E[c]$11: end for Explanation of Key Steps 1. Combining Normalization and Weights. The normalization matrix Σ n is diagonal, and multiplying it with the weight matrix W l produces the normalized weight matrix:

$Ŵl = Σ n W l .$This step integrates the normalization operation into the weights of the current layer, reducing the complexity of subsequent computations.

2. Construction of Combined Weight Matrix. The combined matrix W tot is defined as:

$W tot = W T l+1$Ŵl diag(Σ s ) .

This matrix aggregates the second-layer weights (W T l+1 ), the normalized current-layer weights ( Ŵl ), and the scaling factors (diag(Σ s )) into a single representation, preparing them for joint clustering.

## Clustering.

The projection matrix U is computed by solving the clustering problem:

$U = arg min U ∥W tot -U(U T U) -1 U T W tot ∥ 2 F ,$subject to U T ∈ {0, 1} m×n and m < n. The clustering minimizes the reconstruction error by projecting the combined weights into a lower-dimensional space defined by m clusters.

4. Scaling Adjustments. To ensure proper scaling within each cluster, the diagonal elements of Σ s are updated. For each cluster c, the adjustment considers the size of the cluster (N c ) and the intra-cluster correlation (E[c]):

$(Σ s ) c,c ← (Σ s ) c,c N c N c + (N 2 c -N c )E[c]$.

The intra-cluster correlation E[c] is computed as a normalized dot product, capturing the redundancy among the weights within the same cluster. This adjustment preserves the scaling properties of the original layer.

5. Final Updates. The current-layer weights Ŵl and second-layer weights W T l+1 are updated to align with the clustered representation:

$Ŵl ← (U T U) -1 U T Ŵl , W T l+1 ← U T W T l+1$. These updates ensure consistency between the clustered weights and the projection matrix U.

This algorithm combines clustering, scaling adjustments, and weight updates to compress the layer while preserving its functional properties. The clustering step minimizes redundancy, and the final updates align all components of the layer with the clustered structure.

## G Folding Similar Channels in MLPs

For fully connected networks, where two successive layers are defined as:

$x l = σ(W l x l-1 ) and x l+1 = σ(W l+1 x l ),$where x l represents the activations of layer l, W l and W l+1 are the weight matrices, and σ is the activation function. The channels of the layer are defined as the coordinates x l,i of the vector x l . Each channel corresponds to a specific dimension in the activations.

The folding cost J l for the l-th layer is defined as:

$J l = ∥W l -C l W l ∥ 2 F + W T l+1 -C l W T l+1 2 F$, where C l is a clustering matrix. This cost function represents the optimization objective to minimize the approximation error introduced by folding (clustering) the weights of the l-th layer. The first term measures the reconstruction error for the weights W l , while the second term measures the reconstruction error for the weights W l+1 under the transformation C l . Together, these terms ensure that the clustering transformation preserves the structure and relationships of the weights across layers.

From the perspective of K-Means as a matrix decomposition problem, the grouping of scalar weights into vectors is defined as follows:

$W l =      p T 1 p T 2 . . . p T n     $and W l+1 = q 1 q 2 . . . q n , where p T i are the rows of W l and q i are the columns of W l+1 . These groupings reflect the natural structure of the weight matrices in fully connected layers:

• Each row of W l represents the weights associated with a specific output channel of layer l.

• Each column of W l+1 represents the weights associated with a specific input channel of layer l + 1.

In this formulation, the rows p T i and columns q i are treated as vectors to be clustered by the matrix C l , which aligns with the K-Means decomposition perspective. The clustering matrix C l maps these weights into representative clusters, preserving the relationships between input and output channels across layers while enabling efficient compression.

## I Folding Similar Channels in LlamaMLP and LlamaAttention

## I.1 Folding Similar Channels in LlamaMLP

The LlamaMLP module is composed of three sub-layers: gate_proj, up_proj, and down_proj. These sub-layers define the structure and functionality of the MLP, with the main computation pipeline expressed as: down_proj(act_fn(gate_proj(x)) × up_proj(x)).

We cluster similar channels in both the output channel and input channel of each sub-layer.

Input Channel Folding. To fold the input channels of LlamaMLP, we simultaneously consider the input dimensions of both gate_proj and up_proj layers, as they collectively define the effective input to the gate_up sub-layer. The input channels of gate_proj and up_proj are clustered respectively using methods similar to those applied in standard MLP layers.

Output Channel Folding. To fold the output channels of LlamaMLP, we first consider the output channels of both gate_proj and up_proj by clustering and adjusting the input channel of the down_proj. Subsequently, we adjust the output channel of down_proj according to the residual connection used outside of LlamaMLP.

## I.2 Folding Similar Channels in LlamaAttention

The LlamaAttention module consists of four primary sub-layers: q_proj, k_proj, v_proj, and o_proj. These sub-layers define the query, key, value, and output projections, respectively. For clarity and simplicity, we conceptualize q_proj, k_proj, and v_proj as a unified sub-layer referred to as q_k_v, which computes the intermediate representations required for attention calculations. The o_proj sub-layer processes the final output of the attention mechanism. We treat the attention head as the structure to be folded in LlamaAttention. By reshaping the weights of each sub-layer into an MLP-like tensor, we can cluster similar heads, similar to how it is done for a standard MLP layer.

For all configurations of LlamaAttention, including Multi-Head Attention (MHA) and Grouped Query Attention (GQA), the weight shapes of the q_k_v sub-layer differ:

• In MHA, the weights for q, k, and v projections share the same shape: [num_heads×head_dim, hidden_size].

• In GQA, the weights for k and v projections have the shape: [num_kv_heads×head_dim, hidden_size].

Output Channel Folding. When performing output channel folding for the LlamaAttention layer, the clustering of the o_proj sub-layer's output channels is dictated by the residual connection outside of LlamaAttention, ensuring alignment with the clustering results from previous modules. Specifically:

• The o_proj weights, originally shaped as [num_heads × head_dim, hidden_size], are reshaped into [num_heads, head_dim, hidden_size], clustered along the first dimension (num_heads), and then reshaped back to their original form.

• For clustering within the q_k_v sub-layer, the weights for q, k, and v are reshaped into [num_heads, head_dim, hidden_size] (or [num_kv_heads, head_dim, hidden_size] for k and v in GQA) and clustered along the first dimension (num_heads or num_kv_heads). After clustering, the weights are reshaped back to their original dimensions.

Input Channel Folding. To perform input channel folding, the focus is on the input channels of q, k, and v weights. Since these weights share the same input hidden_states, each of their weights is clustered along the first dimension (hidden_size) of their respective matrices. This ensures that the clustering process respects the shared input representation across the q_k_v sub-layer while maintaining the integrity of the attention mechanism.

## J Comparison with Knowledge Distillation

We evaluated some data-free knowledge distillation (KD) methods [(Chen et al., 2019;](#b9)[Fang et al., 2020;](#b19)[Micaelli and Storkey, 2019;](#b62)[Yu et al., 2023)](#b86), on an NVIDIA A100 GPU, for all methods using the same pre-trained teacher model, data loader, and student model setup for consistency. The full model is a ResNet18 pre-defined by torchvision and trained on CIFAR10, while the student models for each KD method share the same architecture but differ in the number of channels across all layers to achieve the desired sparsity levels. Specifically, in ResNet18, the number of output channels for all blocks is a multiple of 64, which is also the number of output channels in the first convolutional layer.

To reduce the model's channel dimensions, we scale this base hyperparameter by a reduction factor, effectively reducing the width of all layers proportionally. The following table presents the test accuracy of compressed by KD methods and model folding on CIFAR10 test dataset.The time taken to achieve each accuracy is provided in parentheses next to the corresponding accuracy value. From the table, it is evident that the proposed model folding achieves model compression within seconds, even at high sparsity levels, compared to other KD methods that require tens of hours to complete. Sparsity Full model 10% 25% 50% 70% ABM (Micaelli and Storkey, 2019) 94.72 93.30 (17h19m) 91.99 (16h8m) 89.42 (15h30m) 85.43 (13h23m) DFAD (Chen et al., 2019) 94.72 93.79 (2h31m) 93.52 (2h3m) 92.04 (2h1m) 89.67 (1h54m) DAFL (Fang et al., 2020) 94.72 71.73 (16h48m) 77.80 (15h39m) 68.06 (15h19m) 53.86(76h34m) SpaceshipNet (Yu et al., 2023) 94.72 94.50 (42h33m) 93.95 (40h3m) 92.96 (37h57m) 91.53 (27h10m) Model Folding (ours) 94.72 94 (56.35s) 92 (53.55s) 88 (55.75s) 82 (54.95s) Table 7: Performance and resource usage at various sparsity levels across devices, with detailed breakdowns for runtime (ms), RAM usage (K), and Flash storage usage (M).

## L Deep Inversion Sample Images

Deep Inversion (DI) [(Yin et al., 2020)](#b84) generates synthetic images from the uncompressed network by optimizing noise to match the internal statistics stored in BatchNorm layers. These images, exemplified in Fig. [13](#fig_12), which reflect the original data's statistical properties, are used during model folding to restore data statistics in the compressed network, ensuring accuracy without requiring external data. 

## M Further Related Work

Model folding intersects with several established approaches in model compression, network architecture optimization and model merging. This section outlines key related works that inspired the development of model folding, highlighting both their contributions and limitations.

## M.1 Model compression

Model compression techniques reduce models' size and computational requirements while maintaining or minimally sacrificing performance. Various methods have been developed. Most can be classified as pruning, quantization, knowledge distillation, and low-rank factorization. Traditional pruning techniques [(Entezari and Saukh, 2020;](#b16)[Han et al., 2015;](#b27)[Hassibi et al., 1993;](#b28)[LeCun et al., 1989;](#b48)[Li et al., 2016b)](#), structured or unstructured, involve removing weights, neurons, or filters that are deemed less important, typically measured by the magnitude of their contributions (e.g.,, L 1 or L 2 norm) [(Cheng et al., 2023;](#b12)[Entezari and Saukh, 2020;](#b16)[Li et al., 2017)](#b52). While effective in reducing the size of the model, pruning often leads to a degradation of performance that requires fine-tuning or complete retraining of the network [(Cheng et al., 2023;](#b12)[Frankle and Carbin, 2018;](#b20)[Frantar and Alistarh, 2022;](#b21)[Han et al., 2015;](#b27)[He et al., 2018)](#b30). Quantization [(Gupta et al., 2015;](#b26)[Li et al., 2016a;](#)[Zhou et al., 2017)](#b87) reduces the precision of the numerical values in a model, from floating-point to lower-bit representations (e.g.,, 8-bit integers). This approach significantly reduces the model's memory footprint and speeds up computation, especially when combined with hardware accelerators designed for low-precision arithmetic [(Gholami et al., 2021)](#b23). Like pruning, post-training quantization may also require fine-tuning to restore model performance. Knowledge distillation [(Hinton et al., 2015)](#b32) trains a smaller model, called the student, to replicate a well-trained larger model, called the teacher, by mimicking the output of the teacher model, which transfers knowledge between the teacher model and the student model. While effective in transferring knowledge and reducing model size, the training process for knowledge distillation can be computationally expensive and time-consuming [(Gou et al., 2021;](#b24)[Hinton et al., 2015)](#b32). Moreover, knowledge distillation often assumes substantial differences between student and teacher model architectures [(Gou et al., 2021)](#b24). Low-rank factorization decomposes weight matrices into lower-rank matrices to reduce parameter size through such as singular value decomposition [(Horvath et al., 2024;](#b33)[Ren and Zhu, 2023)](#b67) or tensor decomposition [(Kim et al., 2016;](#b42)[Lebedev et al., 2015)](#b47). Approaches such as mixture of experts [(Jacobs et al., 1991;](#b36)[Shazeer et al., 2017)](#b69), subspace-configurable networks [(Papst et al., 2024;](#b66)[Wang et al., 2024)](#b79) and resource-efficient deep subnetworks [(Corti et al., 2024a,b)](#), explore dynamic model reconfiguration to minimize the number of active weights during inference. Structured pruning. Structured pruning is of particular interest because it removes entire structures (such as neurons, channels, or layers) [(Entezari and Saukh, 2020;](#b16)[Hu et al., 2016;](#b34)[Li et al., 2016b;](#)[Luo et al., 2017a;](#)[Wen et al., 2016)](#b81) rather than individual parameters, reducing model complexity while maintaining or even improving performance. This method is especially valuable for enhancing efficiency with easily implemented acceleration in resource-constrained environments [(Liu et al., 2024;](#b55)[Wang et al., 2020)](#b80). However, structured pruning typically requires additional retraining or fine-tuning [(He et al., 2017;](#b31)[Liu et al., 2024;](#b55)[Luo et al., 2017b)](#). Recent work by [Theus et al. (2024)](#b75) combines model pruning and fusion using Optimal Transport theory, demonstrating that a significant portion of pruning accuracy can be recovered without access to training data. However, the impact of pruning on the model's data statistics and how to recover them is not addressed.

## M.2 Model merging

Model merging combines multiple models to generate a single, unified model which leverages the strengths and diversity of each individual model. It particularly benefits ensemble learning and distributed training scenarios, where models are trained independently on different subsets of data or across different devices.

Merging can be achieved by averaging the parameters of model trained independently. Recently, multiple methods have been developed to enhance model performance and robustness. MTZ [(He et al., 2018)](#b30) and ZipIt! [(Stoica et al., 2024)](#b73) compress multiple models pre-trained for different tasks by merging them through neuron sharing. Model soup [(Wortsman et al., 2022)](#b82) averages the weights of multiple fine-tuned models from same initialization to improve accuracy and robustness without increasing inference time. Taking permutation invariance of neural networks into account, a finding [(Entezari et al., 2022)](#b17) shows the interpolation between models trained with SGD has no barrier. Git Re-Basin [(Ainsworth et al., 2023)](#b0) utilizes activation matching and weight matching to achieve permutated alignment between models trained from different initialization. REPAIR [(Jordan et al., 2022)](#b39) mitigate variance collapse problem while aligning neurons by rescaling the preactivations of fused models. PAPA leverages a population of diverse models trained on different data variations and slowly pushes the weights of the networks towards the population average [(Jolicoeur-Martineau et al., 2024)](#b38). A recent work [(Yamada et al., 2023)](#b83) shows that for model merging on different datasets, using original or condensed datasets during the model merging process can significantly improve accuracy. However, those methods do not consider model efficiency and internal parameter redundancy. Another recent work [(Theus et al., 2024)](#b75) achieves intra-layer model fusion by integrating optimal transport [(Kantorovich, 2006;](#b40)[Monge, 1781;](#b63)[Singh and Jaggi, 2020)](#b71) to fuse computational structures in the model without fine-tuning. We note that this approach is orthogonal to the problem solved in this paper, as we do not consider intra-layer dependencies.

Merging multiple computational units. Merging computational units has been extensively explored in ensemble methods. [Wortsman et al. (2022)](#b82) demonstrate that combining multiple models fine-tuned from the same pretrained initialization enhances both accuracy and robustness. [Ainsworth et al. (2023)](#b0) extend this approach to models trained on the same data with different initializations, albeit with some accuracy loss.

Jordan et al. (2022) improve upon Git Re-Basin by adjusting batch normalization layers where applicable. IFM Chen et al. (2023) and ZipIt! Stoica et al. (2024) focus on merging multiple computational units within a single model, pioneering this approach.

![Figure 1: Model compression and repair of data statistics. Left: Model folding pipeline is applied layer-wise, consisting of three phases: weight tensor clustering and merging, and data statistics repair.Right: To maintain accuracy, the data variances of compressed and uncompressed models must align (i.e., the variance ratio must be close to 1), as variance collapse or explosion leads to suboptimal performance. Our data-free and fine-tuning-free model folding methods (Fold-AR and Fold-DIR) achieve performance comparable to data-driven statistics repair (Fold-R), while outperforming naive statistics repair (Fold-naive) and the recently proposed IFM(Chen et al., 2023). All methods were evaluated on a public ResNet18 checkpoint trained on CIFAR10. Lines connect the performance of different methods at the same weight sparsity level, applied uniformly across all layers. Variance ratio refers to the activation outputs in the last layer. A precise definition and analysis are in Sec. 3.]()

![Figure 2: Layer-wise correlation between matched channels in ResNet18 trained on CIFAR10.For each layer, we use activation matching matching with L 2 distance measure to greedily pair similar neurons. Each subplot shows the correlation within all matched pairs.]()

![Figure 3: k-means (KM) outperforms other clustering methods: Spectral Clustering (SC), Agglomerative Clustering (AC) with different linkage criteria and Iterative Greedy (greedy) used to compress ResNet18 trained on CIFAR10. Data-based REPAIR was used to restore data statistics after clustering for all methods.]()

![denotes the indices of all values belonging to cluster c, N c = |I c | denotes the number of values in the cluster, and xl = Σ n W l x l . The batch normalization using Σ n ensures that the variances of all xl (i) equal 1.]()

![Figure 4: Variance collapse and overshooting on ResNet18 with CIFAR10. The goal is to align the layer-wise variance in the compressed network to that of the uncompressed model. Naive averaging of statistics (Fold-Naive) leads to variance collapse (Jordan et al., 2022), while IFM overshoots. Fold-AR and Fold-DIR closely match the performance of the data-driven REPAIR (Fold-R). Layer-wise sparsity is 0.5.]()

![Figure 5: Data-free folding methods with approximate REPAIR (Fold-AR) and Deep Inversion (Yin et al., 2020) (Fold-DIR) and on ResNet18 with CI-FAR10 at various weight sparsity levels, uniformly distributed across layers. Fold-DIR performs similarly to the data-based REPAIR (Fold-R). Both Fold-AR and Fold-DIR surpass IFM (Chen et al., 2023) by a significant margin.]()

![Figure 6: Comparison with IFM(Chen et al., 2023) and structured magnitude pruning(Cai et al., 2020;Yin et al., 2022). Model folding, when tested on ResNet18 (top row) and VGG11-BN (bottom row) trained on CIFAR10 (left column) and ImageNet (right column), outperforms IFM with higher sparsity and increasing dataset difficulty.]()

![Figure 7: Comparison of model folding with IFM (Chen et al., 2023), and INN(Solodskikh et al., 2023) using ResNet18 on CIFAR10. In the original experiment defined in the IFM and INN papers, where only the last two blocks of a ResNet18 are pruned, folding is significantly better than INN while it matches the performance of IFM for lower sparsities and becomes significantly better for higher sparsities. Note, the maximum sparsity achievable by INN is 54%(Solodskikh et al., 2023).]()

![Figure 8: Layer-wise correlation among matched channels in VGG11 and its wider variants on CIFAR10. This figure shows correlation matrices for each layer of VGG11 and its 1x and 3x wider variants, derived from activation matching. Opaque black represents the 1x wider model, while vibrant colors indicate the 3x wider model, highlighting differences in correlation strength.]()

![Figure 11: ResNet18 (left column) and VGG11 (right column) models trained with L 1 (top row) and L 2 (bottom row) regularization. Structured magnitude pruning outperforms model folding only if training explicitly regularizes for model sparsity (L 1 norm). REPAIR is hardly beneficial for all structural pruning methods.]()

![Figure 12: Model folding performance improves with increasing model width. The MLP model consists of three stacked mlp blocks (including a fully connected layer, a BN layer, and a ReLU layer), followed by a final classifier. Upscaled versions of MLP (left) and ResNet50 (right) architectures, trained on CIFAR10 and CIFAR100, demonstrate the consistent advantages of model folding.]()

![subject to U T ∈ {0, 1} m×n and m < n 4: Update the scaling matrix:Σ s ← (U T U) -1 U T Σ s U 5: Update the second-layer weights: W T l+1 ← U T W T l+1Update the current-layer weights: Ŵl ← (U T U) -1 U T Ŵl 7: for c = 1, . . . , m do ▷ Adjust scaling factors for each cluster 8:Compute cluster size:N c ← i I(U i,c = 1) ▷ I(•)is the indicator function 9:]()

![Figure 13: Sample images generated by Deep Inversion (Yin et al., 2020) using ResNet18 trained on CIFAR100. These images are generated from the uncompressed network and used in model folding to restore data statistics in the compressed network.]()

![]()

![, we used instances of VGG11 and ResNet18 trained on CIFAR10 with a 5+5 label split. All experiments were performed with REPAIR.]()

![Performance comparison for merging networks trained on separate tasks.For the experiments involving merging networks trained on the same task (see Table2), we used instances of VGG11 and ResNet18, both trained on CIFAR10. All experiments were performed with REPAIR.]()

![Performance of structured pruning methods on LLaMA2-7B without post-tuning,]()

![Performance comparison of knowledge distillation and model folding, showing accuracy (%) and runtime (in parentheses). The sparsity levels indicate the percentage of weights pruned.We apply model folding on a LeNet5 model pre-trained on FashionMNIST with different sparsity, and then evaluate the folded models on NVIDIA Jetson Nano, ESP-EYE, and Arduino Nano 33 BLE. All models are converted and executed as a float32 Tensorflow Lite model in all devices.]()

ExperimentsFollowing related works on model merging[(Ainsworth et al., 2023;](#b0)[Chen et al., 2023;](#b11)[Jordan et al., 2022)](#b39), we evaluate folding on convolutional architectures, including ResNets[(He et al., 2016)](#b29) and VGGs[(Simonyan and Zisserman, 2014)](#b70) of varying sizes on CIFAR10, CIFAR100(Krizhevsky et al., 2009b)  and ImageNet[(Deng et al., 2009)](#b15). For models trained on the CIFAR10 and CIFAR100 datasets, we used the hyperparameters available from online benchmarks 23 . For models trained on ImageNet, the pre-trained weights were taken from torchvision. For large language models (LLMs), we evaluate model folding on LLaMA-7B(Touvron  et al., 2023a)  with pre-trained weights from Hugging Face Hub. In all experiments, model sparsity denotes the proportion of weights that have been removed as a result of model compression. Experimental setup is detailed in Appendix A. Further evaluation results are in Appendix J and K. Model folding mitigates variance collapse. Fig.6compares model folding with IFM[(Chen et al., 2023)](#b11), a recently introduced data-free, fine-tuning-free method that combines aspects of folding and pruning. Unlike model folding, which accurately corrects the data statistics in the compressed model, IFM merges matched input channels by summing one and zeroing the other, followed by a weighted average of output channels. In contrast to the original paper, Fig.6applies the same sparsity ratio across all layers for every method. We find that model folding significantly outperforms IFM, particularly at higher sparsity levels and for larger networks. Additionally, Fig.7(left) replicates the experiment from[(Chen et al., 2023)](#b11) on ResNet18 with CIFAR10, using the same per-layer sparsity pattern where only the last two blocks are sparsified. In this scenario, IFM offers a slight performance edge over our method for low sparsity, but struggles with higher sparsity.2 https://github.com/huyvnphan/PyTorch_CIFAR10 3 https://github.com/weiaicunzai/pytorch-cifar100/

https://pytorch.org/hub/

https://huggingface.co/docs/hub/index

https://wandb.ai

