<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forget the Data and Fine-tuning! Just Fold the Network to Compress</title>
				<funder ref="#_wJb3yxK">
					<orgName type="full">Austrian Research Promotion Agency (FFG)</orgName>
				</funder>
				<funder ref="#_TqdcVWD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-14">14 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>dong.wang@</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haris</forename><surname>Šikić</surname></persName>
							<email>haris.sikic@student.</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lothar</forename><surname>Thiele</surname></persName>
							<email>thiele@tik.ee.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olga</forename><surname>Saukh</surname></persName>
							<email>saukh@tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Complexity Science Hub Vienna</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forget the Data and Fine-tuning! Just Fold the Network to Compress</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-14">14 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">2B14E55E32E416EDAA13B5A46BCE8008</idno>
					<idno type="arXiv">arXiv:2502.10216v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce model folding, a novel data-free model compression technique that merges structurally similar neurons across layers, significantly reducing the model size without the need for fine-tuning or access to training data. Unlike existing methods, model folding preserves data statistics during compression by leveraging k-means clustering, and using novel data-free techniques to prevent variance collapse or explosion. Our theoretical framework and experiments across standard benchmarks, including ResNet18 and LLaMA-7B, demonstrate that model folding achieves comparable performance to datadriven compression techniques and outperforms recently proposed data-free methods, especially at high sparsity levels. This approach is particularly effective for compressing large-scale models, making it suitable for deployment in resource-constrained environments. Our code is online. ¶</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have emerged as a fundamental technology, driving progress across a multitude of applications from natural language processing to computer vision. However, the deployment of these models in real-world settings is often constrained by the computational and memory resources available, particularly on edge devices like smartphones and embedded systems <ref type="bibr" target="#b10">(Chen et al., 2020;</ref><ref type="bibr" target="#b46">Kumar et al., 2017;</ref><ref type="bibr" target="#b78">Wan et al., 2020)</ref>. This limitation poses a significant challenge, as the growing complexity and size of SOTA models demand increasingly substantial resources <ref type="bibr" target="#b6">(Bommasani et al., 2021;</ref><ref type="bibr" target="#b8">Chang et al., 2024;</ref><ref type="bibr" target="#b68">Rombach et al., 2022)</ref>.</p><p>Conventional model compression techniques, such as pruning <ref type="bibr" target="#b27">(Han et al., 2015;</ref><ref type="bibr" target="#b28">Hassibi et al., 1993;</ref><ref type="bibr" target="#b48">LeCun et al., 1989;</ref><ref type="bibr">Li et al., 2016b)</ref> and quantization <ref type="bibr" target="#b26">(Gupta et al., 2015;</ref><ref type="bibr">Li et al., 2016a;</ref><ref type="bibr" target="#b87">Zhou et al., 2017)</ref>, have been developed to mitigate this issue by reducing the model size and computational requirements. These methods usually remove redundant or less critical parameters from the model, thereby reducing the overall size and computational load. For example, pruning eliminates weights that contribute minimally to the model's output <ref type="bibr" target="#b16">(Entezari and Saukh, 2020;</ref><ref type="bibr" target="#b27">Han et al., 2015;</ref><ref type="bibr">Li et al., 2016b)</ref>. Quantization reduces the precision of the weights and activations <ref type="bibr" target="#b26">(Gupta et al., 2015)</ref>, which decreases memory usage and speeds up inference <ref type="bibr" target="#b87">(Zhou et al., 2017)</ref>. Despite their effectiveness, these approaches often introduce a degradation in model performance, necessitating a phase of fine-tuning to maintain the internal data statistics within the model <ref type="bibr" target="#b39">(Jordan et al., 2022)</ref> and restore the original accuracy levels <ref type="bibr" target="#b20">(Frankle and Carbin, 2018;</ref><ref type="bibr" target="#b21">Frantar and</ref>  Right: To maintain accuracy, the data variances of compressed and uncompressed models must align (i.e., the variance ratio must be close to 1), as variance collapse or explosion leads to suboptimal performance. Our data-free and fine-tuning-free model folding methods (Fold-AR and Fold-DIR) achieve performance comparable to data-driven statistics repair (Fold-R), while outperforming naive statistics repair (Fold-naive) and the recently proposed IFM <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>. All methods were evaluated on a public ResNet18 checkpoint trained on CIFAR10. Lines connect the performance of different methods at the same weight sparsity level, applied uniformly across all layers. Variance ratio refers to the activation outputs in the last layer. A precise definition and analysis are in <ref type="bibr">Sec. 3. Alistarh, 2022;</ref><ref type="bibr" target="#b28">Hassibi et al., 1993)</ref>. This requirement can be a significant drawback in scenarios where access to the original training data is limited.</p><p>Recent methods have sought to circumvent the need for extensive retraining or fine-tuning by exploring alternatives to traditional approaches. Instead, several recent strategies build on model merging techniques <ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b17">Entezari et al., 2022;</ref><ref type="bibr" target="#b39">Jordan et al., 2022)</ref> and achieve (multi-)model compression by fusing similar computational units. For example, ZipIt! <ref type="bibr" target="#b73">(Stoica et al., 2024)</ref> merges two models of the same architecture by combining similar features both within and across models. They provide both theoretical and empirical evidence suggesting that features within the same model are more similar than those between models trained on different tasks. This method avoids the need for retraining the compressed model but requires training data to match features based on the similarity of their activations. Similarly, <ref type="bibr" target="#b83">Yamada et al. (2023)</ref> examine various model merging techniques and conclude that merged models require a dataset-such as a coreset-for effective merging and to achieve high accuracy. This data is essential for adjusting internal data statistics that are disrupted by weight fusion, such as updating the running mean and variance in BatchNorm layers <ref type="bibr" target="#b35">(Ioffe and Szegedy, 2015)</ref>. The process involves a simple forward pass through the model and is a well-established method to adapt models in low-resource environments <ref type="bibr" target="#b49">(Leitner et al., 2023)</ref>.</p><p>In contrast, IFM <ref type="bibr" target="#b11">(Chen et al., 2023)</ref> offers a fully data-free and fine-tuning-free approach, utilizing weight matching <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref> to iteratively merge similar hidden units, similar to <ref type="bibr" target="#b73">Stoica et al. (2024)</ref>. However, despite a heuristic for preserving data statistics, we demonstrate that IFM fails to maintain performance across standard architectures and for high sparsity. Other data-free approaches, such as <ref type="bibr" target="#b84">(Yin et al., 2020)</ref>, generate synthetic images directly from the uncompressed model for fine-tuning to restore pruned model accuracy. More related work is covered in Appendix M.</p><p>This paper presents a model compression technique, model folding, that exploits weight similarity through three phases: neuron clustering, merging, and data statistics repair, summarized in <ref type="bibr">Fig. 1 (left)</ref>. We demonstrate that k-means clustering provides a theoretically optimal and data-free method for merging weights. Building on <ref type="bibr" target="#b39">Jordan et al. (2022)</ref>, which addresses variance collapse using REPAIR with training data, we introduce two data-free alternatives: Fold-AR (folding with approximate REPAIR) and Fold-DIR (folding with Deep Inversion-based REPAIR). Fold-AR estimates mean correlations within clusters assuming independent inputs, while Fold-DIR uses Deep Inversion <ref type="bibr" target="#b84">(Yin et al., 2020)</ref> to synthesize a single batch of images for updating BatchNorm statistics via a forward pass. Both methods maintain data statistics and prevent variance collapse or explosion to avoid suboptimal compression performance, with Fold-AR standing out as a more resource-efficient option while still significantly surpassing existing methods. <ref type="bibr">Fig. 1 (right)</ref> shows that the highest accuracy at any target sparsity is achieved when the mean variance ratio over the last layer between the compressed and uncompressed models stays close to one. Our contributions are:</p><p>• We introduce model folding, a novel model compression technique that merges structurally similar neurons within the same network to achieve compression. We provide both theoretical justification and empirical evidence demonstrating that k-means clustering is an optimal and effective method for fusing model weights in a data-free manner.</p><p>• To enable data-free model compression, we adapt the REPAIR framework proposed by <ref type="bibr" target="#b39">Jordan et al. (2022)</ref> to address variance collapse of data statistics within a model after layer compression. We introduce data-free and fine-tuning-free versions of REPAIR, that effectively maintain model statistics and achieve high performance.</p><p>• We demonstrate that model folding surpasses the performance of SOTA model compression methods which do not use data or fine-tune the pruned model, including recently proposed IFM <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>, and INN <ref type="bibr" target="#b72">(Solodskikh et al., 2023)</ref>, in particularly at higher levels of sparsity and when applied to more complex datasets.</p><p>• We use model folding on LLaMA-7B without utilizing data or post-tuning and achieve comparable results to methods that require data and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Our work is inspired by recent advances in two key areas: neuron alignment algorithms for fusing model pairs in weight space, and data-driven methods for recovering from variance collapse in fused models. Below, we summarize the relevant results from the literature. Neuron alignment algorithms. Model merging involves combining the parameters of multiple trained models into a single model, with a key challenge being the alignment of neurons across these models, particularly when they are trained on different datasets or tasks. Neuron alignment methods can be classified based on their dependency on the input data. Methods like the Straight Through Estimator (STE) <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref>, Optimal Transport (OT) <ref type="bibr" target="#b71">(Singh and Jaggi, 2020)</ref> and correlation-based activation matching <ref type="bibr" target="#b53">(Li et al., 2015)</ref> require data for effective merging. In contrast, weight matching <ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b83">Yamada et al., 2023)</ref> is a data-free method, making it efficient in scenarios when training data is not available. In weight matching, neurons are aligned by minimizing the L 2 distance between the weight vectors of neurons across models. Given two models with weight matrices W A and W B , the goal is to find a permutation P of the weights in W B that minimizes the distance:</p><formula xml:id="formula_0">min P ∥W A -PW B ∥ 2 2 ,</formula><p>where PW B denotes the weight matrix W B after applying the permutation P to align it with W A . Once the optimal permutation is found, the models are merged by averaging the aligned weights:</p><formula xml:id="formula_1">W merged = 1 2 (W A + P * W B ) ,</formula><p>where P * is the permutation that minimizes the L 2 distance. Weight matching solves an instance of the linear sum assignment problem (LSAP), usually solved by Hungarian algorithm <ref type="bibr" target="#b45">(Kuhn, 1955)</ref> as done in <ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b39">Jordan et al., 2022)</ref>, to layer-wise align weight vectors. Unlike merging different models, aligning neurons within a single model requires an acyclic matching graph, a challenge not addressed by LSAP, which For each layer, we use activation matching matching with L 2 distance measure to greedily pair similar neurons.</p><p>Each subplot shows the correlation within all matched pairs. assumes disjoint task and worker sets. To overcome the challenge Chen et al. ( <ref type="formula">2023</ref>) and <ref type="bibr" target="#b30">He et al. (2018)</ref> apply iterative approach greedily merging a pair of the most similar neurons in each iteration. This work extends weight matching to align clusters of similar neurons within the same model, remaining data-free. We show that IFM is inferior to clustering utilized by model folding as described in the next section.</p><p>Variance collapse and REPAIR. When interpolating between independently trained, neuron-aligned networks, <ref type="bibr" target="#b39">(Jordan et al., 2022)</ref> observed a phenomenon they termed variance collapse. This occurs when the variance of hidden unit activations in the interpolated network significantly diminishes compared to the original networks, leading to a steep drop in performance. To solve this issue, <ref type="bibr" target="#b39">Jordan et al. (2022)</ref> introduce the REPAIR method (Renormalizing Permuted Activations for Interpolation Repair) which uses input data to recompute the internal data statistics. REPAIR works by rescaling the preactivations of the interpolated network to restore the statistical properties of the original networks. Specifically, it adjusts the mean and variance of the activations in each layer of the interpolated network to match those of the corresponding layers in the original networks. This is done by computing affine transformation parameters-rescaling and shifting coefficients-for each neuron, ensuring that the mean and standard deviation of activations in the interpolated network are consistent with those in the original models. REPAIR effectively mitigates the variance collapse, enabling the interpolated network to maintain performance closer to that of the original models. This technique has become essential in recent work to preserve model accuracy after merging <ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b38">Jolicoeur-Martineau et al., 2024;</ref><ref type="bibr" target="#b83">Yamada et al., 2023)</ref>. While REPAIR relies on input data to preserve the network's statistical properties, this paper proposes a data-free alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Folding</head><p>In this section, we introduce model folding, a novel compression technique that reduces the computational complexity and size of neural networks by merging similar neurons in each layer without requiring training data. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> (left), model folding processes the network layer by layer, involving filter clustering, merging, and correcting data statistics. Below, we present a theoretical analysis of our approach, supported by empirical results on ResNet18 using CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Channel clustering</head><p>Channel similarity. Neural networks trained with stochastic gradient descent (SGD) tend to have many correlated hidden units, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. Model folding exploits this observation, which is related to the implicit bias of SGD. As discussed in <ref type="bibr" target="#b25">(Gunasekar et al., 2017)</ref>, SGD exhibits a minimum norm bias, which can be viewed as a form of regularization when no explicit regularization is used. In contrast to L 1 regularization, which promotes sparsity, the minimum Euclidean norm solution (L 2 norm) penalizes large weights, encouraging smaller, more regular weights. This not only prevents overfitting but also results in smoother decision boundaries <ref type="bibr" target="#b5">(Bishop, 2006)</ref>. While the minimum norm solution does not directly enforce weight similarity, we empirically demonstrate in Appendix C that it leads to effective model compression when applying similarity-based methods. Recently published methods <ref type="bibr" target="#b11">(Chen et al., 2023;</ref><ref type="bibr" target="#b73">Stoica et al., 2024)</ref> leverage the same observation. Folding as a clustering problem. This work extends weight matching <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref>, which minimizes the L 2 distance between weight vectors and operates without requiring training data. Instead of finding pairs of similar neurons by solving the linear sum assignment problem (LSAP) with a Hungarian algorithm <ref type="bibr" target="#b45">(Kuhn, 1955)</ref> as done in <ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b39">Jordan et al., 2022)</ref>, we achieve channel matching using k-means clustering. In the following, we justify this approach as it provides an optimal weight matrix approximation.</p><p>Given a neural network layer l with a weight matrix W l ∈ R n×m , we define the output of this layer as y l = σ(W l x l ), where x l ∈ R m is the input vector to this layer, y l ∈ R n is the output vector, and σ(•) is a non-linear activation function applied element-wise.</p><p>To reduce the number of outputs of layer l we cluster (fold) rows of W l , i.e., k cluster centroids are determined which serve as a prototype of the respective cluster of rows. All rows of a cluster are replaced by their cluster centroid. This can be formulated as</p><formula xml:id="formula_2">W l ≈ UM,</formula><p>where M ∈ R k×m contains the k &lt; n cluster centroids and the cluster matrix U ∈ {0, 1} n×k determines the membership of a row: u(i, j) = 1 if the i-th row of W l belongs to the j-th cluster, and u(i, j) = 0 otherwise.</p><p>As a measure of the approximation error when replacing the rows of W l by k &lt; n prototypes, we use the Frobenius norm ∥ • ∥ 2 F of the difference between W l and the low-rank factorization UM:</p><formula xml:id="formula_3">J = ∥W l -UM∥ 2 F = tr(W l W T l ) + tr(UMM T U T ) -2tr(UMW T l ).</formula><p>We determine the optimal matrix of cluster centroids by setting the derivative of J with respect to M to zero:</p><formula xml:id="formula_4">M = (U T U) -1 U T W l .</formula><p>As a result, we can write</p><formula xml:id="formula_5">W l ≈ UM = CW l with C = U(U T U) -1 U T .</formula><p>As mentioned above, we use k-means clustering for folding as this minimizes J by determining the optimal clustering matrix U and the corresponding cluster centroids M , also see <ref type="bibr" target="#b4">(Bauckhage, 2015)</ref>. Interdependence between layers. We will expand the above result to successive layers l and l + 1. For simplicity of notation, we neglect the bias and get</p><formula xml:id="formula_6">y l+1 = σ(W l+1 σ(W l x l )).</formula><p>Following the above notation, we describe the folding of activations by some clustering matrix U and</p><formula xml:id="formula_7">C = U(U T U) -1 U T . It is shown in Appendix B that the corresponding approximation satisfies ỹl+1 = σ(W l+1 σ((CW l )x l ) = σ((W l+1 C T )σ((CW l )x l ).</formula><p>Adding up the individual folding costs J l+1 = ∥W T l+1 -CW T l+1 ∥ 2 F and J l = ∥W l -CW l ∥ 2 F yields the combined approximation error J l,l+1 = J l+1 + J l for folding layer l which can be rewritten as</p><formula xml:id="formula_8">J l,l+1 = ∥W l,l+1 -CW l,l+1 ∥ 2 F with W l,l+1 = W l | W T l+1 .</formula><p>If we perform k-means clustering on W l,l+1 and use the resulting clustering matrix U in C = U(U T U) -1 U T , then the combined approximation error J l,l+1 is minimized. This approach accounts for the impact of compressing one layer on the next, leading to more efficient compression that balances the process and preserves learned representations while reducing model size. Our folding methods outperforms other methods experimentally, see Fig. <ref type="figure" target="#fig_2">3</ref> for a comparison to other clustering methods and Iterative Greedy (greedy) adopted in SOTA. Batch Normalization. Now, let us consider batch normalization in layer l represented by two diagonal matrices Σ s (scaling) and Σ n (normalization), again neglecting the bias to reduce notation. In this case, we get</p><formula xml:id="formula_9">y l+1 = σ(W l+1 σ(Σ s Σ n W l x l )).</formula><p>The folding of layer l can be distributed to the matrices Σ s , Σ n , and W l in various ways, depending on the chosen correction of the variance, see Sec. 3.2. For example, one can cluster each matrix separately, leading to</p><formula xml:id="formula_10">ỹl+1 = σ((W l+1 C T )σ((CΣ s )(CΣ n )(CW l )x l )).</formula><p>Adding up the individual folding costs J l+1 , J s , J n , and J l for each of the matrices W l+1 , Σ s , Σ n and W l , respectively, yields the total approximation error J tot = J l+1 + J s + J n + J l for folding layer l</p><formula xml:id="formula_11">J tot = ∥W tot -CW tot ∥ 2 F with W tot = W T l+1 | W l | diag(Σ s ) | diag(Σ n )</formula><p>If we perform k-means clustering on W tot then the total approximation error J tot is minimized. This approach is used in the Deep Inversion (DI) REPAIR, see next section.</p><p>Instead, if we decompose the folding of layer l according to</p><formula xml:id="formula_12">ỹl+1 = σ((W l+1 C T )σ((CΣ s )(CΣ n W l )x l )).</formula><p>then the individual folding costs of W l+1 , Σ s and the normalized weight matrix Σ n W l add up to</p><formula xml:id="formula_13">J tot = ∥W tot -CW tot ∥ 2 F with W tot = Σ n W l | diag(Σ s ) | W T l+1 .</formula><p>Again, if we perform k-means clustering on this combined matrix W tot then the corresponding total approximation error J tot is minimized. This approach is used in the approximate REPAIR, see Sec. 3.2. For completeness, we present in Appendix E how we handle residual connections.</p><p>Merging similar channels in each cluster. To fuse similar channels, various approaches have been proposed in the literature, such as fusing weights for multitasking, which involves Hessian calculations <ref type="bibr" target="#b30">(He et al., 2018)</ref>, or by combining the matched weights into a single channel <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>. <ref type="bibr" target="#b59">(Matena and Raffel, 2022)</ref> introduces Fisher-weighted averaging based on the Laplace approximation for merging weights, while <ref type="bibr" target="#b37">(Jin et al., 2023)</ref> suggests computing a regression mean, which is both computationally efficient and scalable for merging multiple models. In our approach, we use above formulation of the optimization problem as k-means clustering and use a simple mean to compute the cluster centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maintaining data statistics in a compressed model</head><p>Variance collapse and variance overshooting. We use the conceptual framework in <ref type="bibr" target="#b39">(Jordan et al., 2022)</ref> to analyze the performance of model compression methods. We use the following definition.</p><p>Definition 3.1 (Variance ratio). Consider a neural network f (x, Θ) with layer activations {x l } L 1 and its compressed version f (x, Θ) with activations {x l } L 1 . The variance ratio of the l-the layer is:</p><formula xml:id="formula_14">µ Var(x l ) Var(x l ) = 1 |x l | |x l | k=1 Var(x l,k ) Var(x l,k ) .</formula><p>We observe not only variance collapse but also variance overshooting phenomena. Specifically, when data statistics are not accurately corrected after channel merging, as in IFM, variance overshooting can occur, leading to network performance decline. Fig. <ref type="figure" target="#fig_4">4</ref> shows layerwise variance ratio between the compressed and uncompressed networks. Staying close to 1 is essential to mitigate both phenomena. This highlights the critical need for precise statistical corrections during model merging. Fold-AR: Folding with approximate REPAIR. In the context of model compression, particularly when using folding as a clustering method, it is crucial to ensure that the compressed model maintains accurate data statistics. This is especially important for layers involving operations like BatchNorm, where maintaining the correct statistical properties of activations is vital for model performance <ref type="bibr" target="#b39">(Jordan et al., 2022;</ref><ref type="bibr" target="#b83">Yamada et al., 2023)</ref>.</p><p>In the following explanation of the data-free approximate REPAIR, we neglect biases for ease of notation. Following the previous section, we consider folding of the normalized weight matrix with z l = CΣ n W l x l using the post-activation output x l of the previous layer and the input z l to the scaling matrix Σ s . A cluster c is defined by the column of the clustering matrix U , i.e., all values z l (i) with u(i, c) = 1 belong to cluster c. Moreover, by definition of C, all values z l (i) belonging to a single cluster c equal the centroid ẑl (c) of the cluster, i.e., the average of all values Σ n W l x l belonging to this cluster. More formally,</p><formula xml:id="formula_15">∀u(i, c) == 1 : z l (i) = ẑl (c) ∀1 ≤ c ≤ k : ẑl (c) = 1 N c i∈Ic xl (i),</formula><p>where The averaging over all xl (i) belonging to a single cluster destroys this property and leads to the observed variance collapse. We will describe various methods to compensate this loss in variance, at first the data-free approximate REPAIR (Fold-AR).</p><formula xml:id="formula_16">I c = {i : u(i, c) = 1}</formula><p>The variance of the cluster centroid ẑl (c) of cluster c is given by</p><formula xml:id="formula_17">Var(ẑ l (c)) = 1 N 2 c   i∈Ic Var(x l (i)) + i,j∈Ic;i̸ =j Cov(x l (i), xl (j))   , which further simplifies to Var(ẑ l (c)) = 1 N 2 c N c + (N 2 c -N c )E[c]</formula><p>, where E[c] is the mean correlation within the cluster. To prevent variance collapse, we aim for Var(ẑ l (c)) = 1, which would occur if E[c] = 1, meaning  all channels in the cluster are fully correlated. However, as E[c] &lt; 1 typically, we multiply each cluster centroid by a scaling parameter assuming an average cluster correlation</p><formula xml:id="formula_18">E[c] ẑl (c) ← ẑl (c) N c N c + (N 2 c -N c )E[c]</formula><p>.</p><p>Suppose now that the covariance matrix Σ x l of the output x l of the previous layer is available and that we define the normalized weight matrix Wl = Σ n W l with rows wl (i). Then the correlation E[c] can be computed as:</p><formula xml:id="formula_19">E[c] = 1 N 2 c -N c i,j∈Ic;i̸ =j wl (i)Σ x l wT l (j) ( wl (i)Σ x l wT l (i))( wl (j)Σ x l wT l (j))</formula><p>.</p><p>In the absence of data, E[c] can be estimated by assuming that the output values x l of the previous layer are uncorrelated. As the individual variances of xl (i) equal 1 we obtain</p><formula xml:id="formula_20">E[c] = 1 N 2 c -N c i,j∈Ic;i̸ =j wl (i) wT l (j) ( wl (i) wT l (i))( wl (j) wT l (j))</formula><p>.</p><p>We term this approach to maintain the data statistics within the model folding with approximate REPAIR (Fold-AR). This approach helps to ensure that the statistical properties of the data are preserved even after model compression, maintaining the performance of the network while reducing its size. Fig. <ref type="figure" target="#fig_5">5</ref> shows how the performance of Fold-AR compares to the data-driven REPAIR (Fold-R) and surpasses the SOTA data-free methods.</p><p>Fold-DIR: Correcting data statistics with deep inversion. Deep Inversion (DI) <ref type="bibr" target="#b84">(Yin et al., 2020</ref>) is a technique that synthesizes realistic images directly from a pre-trained neural network without requiring access to the original data. The process involves inverting the model by optimizing random noise to produce class-conditional images that match the statistics of the data the model was trained on <ref type="bibr" target="#b64">(Mordvintsev et al., 2015)</ref>. DI leverages the BatchNorm layers within the network, which store the running mean and variance of activations during training. By using these stored statistics as a regularization term in We leverage a single batch of DI-synthesized data within model folding to preserve data statistics after channel merging, eliminating the need for training data. By generating synthetic images aligned with the network's internal statistics, DI recalibrates the folded model's parameters, ensuring that activation variance and mean are maintained. This helps the model retain its performance post-folding, mitigating issues such as variance collapse or explosion without requiring the original dataset. Notably, updating BatchNorm statistics requires only a forward pass, with no backpropagation needed. Thus, Fold-DIR offers a data-free and fine-tuning-free solution for maintaining data statistics. Fig. <ref type="figure" target="#fig_5">5</ref> shows that Fold-DIR closely follows the performance of the data-driven REPAIR (Fold-R), effectively maintaining the data statistics within the model. Fold-DIR ourperforms Fold-AR as the cost of generating a batch of synthetic images and a forward pass through the network.</p><formula xml:id="formula_21">R(x) = L class (x, t) + l ∥µ(x l ) -µ(x l )∥ 2 2 + l ∥Var(x l ) -Var(x l )∥ 2 2 + ∥x∥ 2 2 + ∥x∥ T V ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relationship Between Weight Matching and Model Folding</head><p>Weight Matching <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref> fuses two models into one, whereas Model Folding compresses the weight tensors/matrices of a single network. While inspired by Weight Matching, Model Folding addresses a distinct use case, leading to different optimization problems (K-Means vs. LAP). Notably, the Linear Sum Assignment Problem (LAP) can be framed as a constrained K-Means variant, where each cluster contains exactly two vectors: one from network A and one from network B.</p><p>As an example for this discussion, consider a simple feedforward network. The steps of our proposed compression algorithm involve iteratively solving the following:</p><formula xml:id="formula_22">C l = arg min C l ∥W l -C l W l ∥ 2 F + ∥W T l+1 -C l W T l+1 ∥ 2 F , such that C l = U l (U T l U l )U T l</formula><p>, where U T l is a clustering matrix. Weight Matching merges two feedforward networks by iteratively optimizing:</p><formula xml:id="formula_23">P l = arg min P l ∥W A,l -P l W B,l ∥ 2 F + ∥W T A,l+1 -P l W T B,l+1 ∥ 2 F ,</formula><p>where P l is a permutation matrix. To connect Weight Matching with our method, we frame our approach within the model merging domain. This begins by establishing a relationship between K-Means and the Linear Sum Assignment (LAP) problem. K-Means and LAP Connection. In the standard K-Means formulation, given a dataset represented as rows of a matrix X ∈ R n×d , the objective is to cluster these rows into k groups. This can be represented as:</p><formula xml:id="formula_24">C = arg min C ∥X -CX∥ 2 F ,<label>(1)</label></formula><p>where C ∈ R n×n is a clustering matrix satisfying: (1) each row of C corresponds to a single cluster assignment; and (2) C has a block-diagonal structure that assigns each row of X to a single cluster centroid.</p><p>The clustering matrix C can be explicitly written in terms of a matrix U ∈ R n×k as:</p><formula xml:id="formula_25">C = U(U T U) -1 U T ,</formula><p>where U encodes the cluster assignments and centroids.</p><p>To connect this with LAP, let X be the concatenation of rows from two matrices W A and W B (e.g., weights from two neural networks):</p><formula xml:id="formula_26">X = W A W B , such that C = P I ,</formula><p>where (1) P is a permutation matrix representing a one-to-one mapping between rows of W A and W B ; and</p><p>(2) I is the identity matrix, allowing for exact cluster assignments during merging. Under this constraint, C enforces a specific structure, aligning rows of W A and W B pairwise. Substituting C into Equation 1, we get:</p><formula xml:id="formula_27">P = arg min P ∥ W A W B -P W A W B ∥ 2 F .</formula><p>This is an instance of the Linear Sum Assignment Problem. Minimizing the cost:</p><formula xml:id="formula_28">J = ∥ W A W B -P W A W B ∥ 2 F ,</formula><p>is equivalent to maximizing:</p><formula xml:id="formula_29">J + = tr P W A W B W A W B T .</formula><p>Model Folding. Building on these results, we define Model Folding for merging networks as follows:</p><formula xml:id="formula_30">J l = W l,A W l,B -C l W l,A W l,B 2 F + W l+1,A W l+1,B -W l+1,A W l+1,B C T l 2 F .</formula><p>Constraining C l to C l = P I , where P is a permutation matrix, yields the Weight Matching <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref> coordinate descent cost:</p><formula xml:id="formula_31">J l = 1 2 ∥W l,A -P l W l,B ∥ 2 F + 1 2 W T l+1,A -P l W T l+1,B 2 F .</formula><p>Model Folding for Connecting Models. We provide a small experimental setup comparing WM <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref>, ZipIt! <ref type="bibr" target="#b73">(Stoica et al., 2024)</ref>, and our proposed method for merging networks trained on the same task and networks trained on separate tasks. For the experiments involving merging networks trained on disjoint tasks (see Table 2: Performance comparison for merging networks trained on the same task.  Comparison to structured pruning. We compare model folding with the structured magnitude pruning (SP) method used in <ref type="bibr" target="#b7">(Cai et al., 2020;</ref><ref type="bibr" target="#b85">Yin et al., 2022)</ref>, based on L 1 and L 2 norms, without fine-tuning. Fig. <ref type="figure" target="#fig_6">6</ref> demonstrates that model folding significantly outperforms magnitude pruning, with the performance gap widening as sparsity increases. At 70% sparsity, the folded ResNet18 on CIFAR10 maintains over 80% accuracy, while pruned networks barely surpass random chance. On ImageNet, the performance collapse is even more pronounced across all methods due to the dataset's higher complexity, yet model folding consistently performs well across both datasets. Following <ref type="bibr" target="#b11">(Chen et al., 2023)</ref>, Fig. <ref type="figure" target="#fig_7">7</ref> (right) compares model folding with the SOTA data-free pruning method INN <ref type="bibr" target="#b72">(Solodskikh et al., 2023)</ref>, which struggles to manage even moderate sparsity.</p><p>Folding LLMs. LLMs are built with a large number of parameters, achieving strong performance across various tasks. However, structurally compressing these deep and large models remains a challenge. LLM-Pruner <ref type="bibr" target="#b58">(Ma et al., 2023)</ref> performs structured pruning using gradient calculations, while Wanda <ref type="bibr" target="#b74">(Sun et al., 2023)</ref> leverages an importance score by multiplying weights with their corresponding input activations. FLAP <ref type="bibr" target="#b1">(An et al., 2023)</ref> dynamically computes a fluctuation pruning metric using calibration data. In Tab. 3, we compare model folding with these methods on LLaMA-7B <ref type="bibr">(Touvron et al., 2023a)</ref>, focusing on perplexity on the WikiText2 <ref type="bibr" target="#b61">(Merity et al., 2016)</ref> validation set and zero-shot performance across four tasks using the EleutherAI LM Harness <ref type="bibr">(Gao et al., 2024)</ref>. The folded model performs only very slightly worse than models compressed with data-driven methods. Following SOTA, the clustering phase of model folding was applied to LLaMA-7B, introducing 20% and 50% sparsity in the attention and feed-forward layers of decoder blocks 22-29, and 10% and 40% sparsity in the attention and feed-forward layers of decoder blocks 11-21, respectively. As there is no batchnorm layer in LLaMA-like LLMs, we just applied clustering in LLMs without</p><p>Prune ratio Method Data usage WikiText2↓ BoolQ WinoGrande ARC-e ARC-c Average↑ 0% LLaMA-7B (Touvron et al., 2023a) / 5.68 75.05 69.93 75.34 41.89 65.55 20% Magnitude Prune / 36136 43.21 49.40 27.23 21.59 35.36 20% LLM-Pruner (Ma et al., 2023) Gradients 10.53 59.39 61.33 59.18 37.18 54.27 20% FLAP (An et al., 2023) Calibration 6.87 69.63 68.35 69.91 39.25 61.79 20% Wanda_sp (Sun et al., 2023) Calibration 8.22 71.25 67.09 71.09 42.58 63.00 20% SliceGPT (Ashkboos et al., 2024) Calibration 7.00 57.80 67.96 62.67 36.01 56.11 20% ShortGPT (Men et al., 2024) Calibration 15.48 62.17 67.40 58.88 31.91 55.09 20% Model Folding / 13.33 62.29 62.19 49.83 26.37 50.17</p><p>Table 3: Performance of structured pruning methods on LLaMA-7B without post-tuning, showing perplexity on WikiText2 and zero-shot performance across tasks. The "Average" is computed over four tasks. "Wanda_sp" represents an adapted Wanda method for structured pruning. Despite not using data or fine-tuning, model folding achieves comparable performance to data-driven methods. REPAIR. Tab. 5 shows the generated examples of dense and folded LLaMA-7B processed by model folding without REPAIR in Appendix D. Results of folding LLaMA2-7B (Touvron et al., 2023b) are also provided in Appendix D. When folding with 20% sparsity, the pruned model continues to perform well.</p><p>Fine-Tuning-Free and Data-Free Folding for LLMs. While modern LLMs are trained on extensive datasets, access to such data or related domains is not always feasible in real-world scenarios. In regulated industries such as healthcare, finance, or defense, where data is often sensitive or proprietary, even general public datasets may not be suitable for fine-tuning or compression. Our work specifically addresses data-free settings, offering a robust solution for compressing LLMs without requiring any data or fine-tuning. To illustrate the importance of this setting, we demonstrate that using a suboptimally chosen, out-of-distribution (OOD) calibration dataset can result in worse performance compared to our data-free Model Folding approach.</p><p>For example, we generated a dataset of random Hungarian words in repeated sequences and applied the Wanda compression method to LLaMA-7B. Although LLaMA-7B was trained on some Hungarian text, the language is underrepresented in its training corpus. Using this OOD calibration dataset, the perplexity on the WikiText2 benchmark increased from 8.22 (with the original C4 dataset) to 13.98. A similar performance drop (perplexity = 13.94) was observed with a Ukrainian dataset, highlighting the sensitivity of data-driven methods like Wanda to the domain alignment of the calibration data. These results highlight the robustness of data-free approaches like Model Folding in scenarios where appropriate calibration data is unavailable. Note that further optimization of these experiments is possible (we explored only a limited set of options), yet they showcase the challenges faced by data-driven methods with OOD calibration data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce model folding, a novel compression technique that reduces model size by merging similar channels across layers, without requiring fine-tuning or training data. Model folding achieves high sparsity while preserving data statistics, outperforming traditional pruning and data-free compression methods.</p><p>Our experiments demonstrate that wider networks, such as VGG11 and ResNet50, offer greater opportunities for folding due to increased redundancy, further improving compression efficiency. In LLMs, model folding can prune models while maintaining performance comparable to data-driven methods, but without the need for data access or fine-tuning, which are typically required by most structured pruning techniques.</p><p>Limitations and future work. Model folding offers significant compression without data or fine-tuning, but its effectiveness may be limited in networks with low redundancy. Additionally, it does not optimize sparsity levels per layer, leaving this for future work.</p><p>Lemma B.2. Let x ∈ R k , let U ∈ {0, 1} n×k be a binary clustering matrix with j u ij = 1, let σ(•) be an element-wise nonlinear function, and define</p><formula xml:id="formula_32">C = U(U T U) -1 U T . Then σ(Cx) = C T σ(Cx)</formula><p>Proof of Lemma B.2. We can write</p><formula xml:id="formula_33">σ(Cx) = σ(U(U T U) -1 U T x) = Uσ((U T U) -1 U T x) (Lemma B.1) = U(U T U) -1 (U T U)σ((U T U) -1 U T x) = U(U T U) -1 U T σ(U(U T U) -1 U T x) (Lemma B.1) = C T σ(Cx).</formula><p>Lemma B.3. Let U T be a clustering matrix and let D be a diagonal matrix, then the following is true</p><formula xml:id="formula_34">(U T U) -1 U T DU = Diag((U T U) -1 U T diag(D))</formula><p>Proof of Theorem B.3. The clustering matrix U T can be expressed as:</p><formula xml:id="formula_35">U T =      u T 1 u T 2 . . . u T k      =      u 11 u 12 . . . u 1n u 21 u 22 . . . u 2n . . . . . . . . . . . . u k1 u k2 . . . u kn     </formula><p>, where u T i represents the rows of the clustering matrix. Each row corresponds to cluster i, and the entries u ij satisfy the binary clustering property: u ij = 1 if the j-th data point belongs to cluster i, and u ij = 0 otherwise.</p><p>The product DU is given by:</p><formula xml:id="formula_36">DU =      d 1 0 . . .</formula><p>0 0 d 2 . . . 0 . . . . . . . . . . . . 0 0 . . . d n           u 11 u 12 . . . u 1k u 21 u 22 . . . u 2k . . . . . . . . . . . . u n1 u n2 . . . u nk      . This simplifies to: DU =      d 1 u 11 d 1 u 12 . . . d 1 u 1k d 2 u 21 d 2 u 22 . . . d 2 u 2k . . . . . . . . . . . .</p><formula xml:id="formula_37">d n u n1 d n u n2 . . . d n u nk      .</formula><p>Using the clustering property of U, it follows that:</p><formula xml:id="formula_38">u ij u i ′ j = 1, if i = i ′ , 0, otherwise.</formula><p>From this, the product U T DU simplifies to:</p><formula xml:id="formula_39">U T DU = Diag(U T diag(D)).</formula><p>This result holds because only the diagonal entries remain due to the clustering matrix's orthogonality and binary properties.</p><p>Finally, using the above result, we compute:</p><formula xml:id="formula_40">(U T U) -1 U T DU = (U T U) -1 Diag(U T diag(D)).</formula><p>By the property diag(Diag(x)) = x for any x ∈ R n , we obtain:</p><formula xml:id="formula_41">(U T U) -1 U T DU = Diag((U T U) -1 U T diag(D)).</formula><p>The lemma demonstrates that projecting the diagonal matrix D through the clustering matrix U T preserves its diagonal structure. The diagonal entries are determined by the clustering matrix's mapping of the original diagonal values diag(D), ensuring efficient computation and alignment with clustering properties.</p><p>Lemma B.4. Let U T be a clustering matrix and let w ∈ R n and x ∈ R n , then the following is true UDiag(w)x = Diag(Uw)Ux Proof of Lemma B.4. The clustering matrix U can be expressed as:</p><formula xml:id="formula_42">U =      v T 1 v T 2 . . . v T n     </formula><p>, where each row v T m is defined by a mapping function f : {1, 2, . . . , n} → {1, 2, . . . , k}. For each row v T m , the entries are defined as:</p><formula xml:id="formula_43">v m,j = 1, if j = f (m), 0, otherwise.</formula><p>This representation indicates that the clustering matrix U assigns each element m to a specific cluster f (m).</p><p>Each row v T m has a single non-zero element corresponding to the cluster index f (m).</p><p>Calculation of the Left-Hand Side (LHS). The left-hand side of the equality is:</p><formula xml:id="formula_44">UDiag(w)x.</formula><p>First, compute Diag(w)x, which scales each element of x by the corresponding element of w:</p><formula xml:id="formula_45">Diag(w)x =      w 1 x 1 w 2 x 2 . . . w n x n      .</formula><p>Then, multiplying by U aggregates these scaled values according to the clusters defined by f . Specifically, the j-th element of UDiag(w)x is given by:</p><formula xml:id="formula_46">(UDiag(w)x) j = m:f (m)=j w m x m .</formula><p>Calculation of the Right-Hand Side (RHS). The right-hand side of the equality is:</p><formula xml:id="formula_47">Diag(Uw)Ux.</formula><p>First, compute Uw. The j-th element of Uw is:</p><formula xml:id="formula_48">(Uw) j = m:f (m)=j w m ,</formula><p>which sums the w m values for all elements assigned to cluster j.</p><p>Next, construct Diag(Uw), a diagonal matrix with entries (Uw) j along the diagonal:</p><formula xml:id="formula_49">Diag(Uw) =      (Uw) 1 0 . . . 0 0 (Uw) 2 . . . 0 . . . . . . . . . . . . 0 0 . . . (Uw) k      .</formula><p>Finally, compute Ux. The j-th element of Ux is:</p><formula xml:id="formula_50">(Ux) j = m:f (m)=j</formula><p>x m , which sums the x m values for all elements assigned to cluster j.</p><p>Multiplying Diag(Uw) by Ux gives:</p><formula xml:id="formula_51">(Diag(Uw)Ux) j = (Uw) j (Ux) j =   m:f (m)=j w m     m:f (m)=j x m   .</formula><p>Verification of Equality. Both the LHS and RHS compute the same aggregated sums m:f (m)=j w m x m for each cluster j. The LHS directly performs the aggregation of w m x m within clusters, while the RHS separates the computation into two steps: summing w m and x m for each cluster, followed by multiplying these sums. Since multiplication distributes over addition, the two expressions are equivalent:</p><formula xml:id="formula_52">UDiag(w)x = Diag(Uw)Ux.</formula><p>The lemma is proven, as both sides of the equation compute the same weighted aggregation of w m x m over the clusters defined by the clustering matrix U.</p><p>Lemma B.5. Let C T be a clustering matrix and let D be a diagonal matrix, then the following is true</p><formula xml:id="formula_53">∥W -Diag(Cdiag(W))∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2</formula><p>Proof of Lemma B.5. Let W = Diag(Cdiag(W)), where W represents the diagonal matrix obtained by clustering the diagonal entries of W using the clustering matrix C. Both W and W are diagonal matrices, so their difference W -W is also diagonal. The entries of this difference are:</p><formula xml:id="formula_54">w i,j -wi,j = w i,i -wi,i , if i = j, 0, otherwise.</formula><p>The Frobenius norm of the difference W -W is:</p><formula xml:id="formula_55">∥W -W∥ 2 F = i,j (w i,j -wi,j ) 2 .</formula><p>Since W and W are diagonal matrices, this simplifies to:</p><formula xml:id="formula_56">∥W -W∥ 2 F = i (w i,i -wi,i ) 2 .</formula><p>The diagonal entries of W can be represented as a vector diag(W), and the diagonal entries of W are given by Cdiag(W). Substituting these representations, we have:</p><formula xml:id="formula_57">∥W -W∥ 2 F = i (diag(W) i -(Cdiag(W)) i ) 2 .</formula><p>This is equivalent to the squared ℓ 2 -norm of the difference between the vectors diag(W) and Cdiag(W), giving: ∥W -W∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2 . Substituting back W = Diag(Cdiag(W)), we conclude that:</p><formula xml:id="formula_58">∥W -Diag(Cdiag(W))∥ 2 F = ∥diag(W) -Cdiag(W)∥ 2 2 .</formula><p>Lemma B.6. Let A ∈ R n×n and B ∈ R n×n be diagonal matrices, then:</p><formula xml:id="formula_59">AB = Diag(Adiag(B))</formula><p>Proof of Lemma B.6. Since both A and B are diagonal matrices, their product AB is also a diagonal matrix.</p><p>The entries of the product AB are given by:</p><p>(AB) i,j = a i,j b i,j .</p><p>For diagonal matrices, all off-diagonal entries are zero, so:</p><formula xml:id="formula_60">(AB) i,j = a i,i b i,i , if i = j, 0, otherwise.</formula><p>Thus, the diagonal entries of AB are a i,i b i,i , and the matrix AB is:</p><formula xml:id="formula_61">AB =      a 1 b 1 0 . . . 0 0 a 2 b 2 . . . 0 . . . . . . . . . . . . 0 0 . . . a n b n     </formula><p>, where a i = a i,i and b i = b i,i represent the diagonal entries of A and B, respectively. Now, let diag(B) denote the vector of diagonal entries of B, i.e.,</p><formula xml:id="formula_62">diag(B) =      b 1 b 2 . . . b n     </formula><p>.</p><p>The operation Adiag(B) represents the element-wise multiplication of the diagonal entries of A and B:</p><formula xml:id="formula_63">Adiag(B) =      a 1 b 1 a 2 b 2 . . . a n b n      .</formula><p>Next, using the function Diag(•), we can construct a diagonal matrix from this vector:</p><formula xml:id="formula_64">Diag(Adiag(B)) =      a 1 b 1 0 . . . 0 0 a 2 b 2 . . . 0 . . . . . . . . . . . . 0 0 . . . a n b n      .</formula><p>Clearly, AB and Diag(Adiag(B)) are identical, as they both produce the same diagonal matrix with entries a i b i along the diagonal. Therefore:</p><formula xml:id="formula_65">AB = Diag(Adiag(B)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Channel similarity</head><p>Models learned by SGD trend to have correlated patterns or similar parameters in the weight space. Fig. <ref type="figure">9</ref> shows 3 × 3 filter weights in conv1 of a pre-trained ResNet18. These filters across the first 3 input channels and first 16 output channels ordered by the entropy of filter weight. From the plot, most filters of a channel can find at least one another similar filter in other channels, which means filter similarity may lead to structured redundancy.</p><p>Figure <ref type="figure">9</ref>: Similar patterns in weight map of conv1 layer in ResNet18 pre-trained on Ima-geNet <ref type="bibr" target="#b15">(Deng et al., 2009)</ref>. Each small square represents the weights of a single filter in cool-warm color map, where each color of grid corresponds to a weight value.</p><p>To investigate the filter redundancy within a layer, we apply weight matching activation matching from the literature <ref type="bibr" target="#b39">(Jordan et al., 2022)</ref> to each layer of ResNet18 pretrained on CIFAR10 <ref type="bibr">(Krizhevsky et al., 2009a)</ref> in Fig. <ref type="figure" target="#fig_1">2</ref> and on ImageNet <ref type="bibr" target="#b15">(Deng et al., 2009)</ref> in Fig. <ref type="figure" target="#fig_0">10</ref>. We observe two findings: (1) The correlation score distribution varies across layers. The earlier and narrower the lay ers are, the more scattered the correlation coefficients are, and only a few have high correlation coefficients. The wider and later the layers are, the more compact the correlation coefficients are, and most of the matching channels have high correlation coefficients.</p><p>(2) In the same layer, the distribution of correlation coefficients among matched channels differs across various pre-training datasets. This observation does not fully align with the claim by <ref type="bibr" target="#b11">Chen et al. (2023)</ref> regarding the downward trend of similarity before a reversal. It appears that this characterization might not consistently hold across different models and pre-trained dataset.</p><p>Figure <ref type="figure" target="#fig_0">10</ref>: Layer-wise correlation between matched channels in ResNet18 trained on ImageNet. We compute a layer-wise correlation matrix by matching activations between channels, then assign each channel its best match in the same layer using a greedy pairing based on the correlation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 The impact of regularization</head><p>In Fig. <ref type="figure" target="#fig_6">6</ref>, the models on CIFAR10 were trained without regularization, while the pre-trained ImageNet models were sourced from torchvision. In Fig. <ref type="figure" target="#fig_9">11</ref>, we extend the comparison of folding and pruning methods on CIFAR10, including ResNet18 (left column) and VGG11 (right column) models trained with explicit L 1 and L 2 regularization. L 1 regularization, in particular, promotes neuron sparsity, leading structured magnitude pruning methods to outperform model folding under these conditions. However, a comparison between Fig. <ref type="figure" target="#fig_6">6</ref> and Fig. <ref type="figure" target="#fig_9">11</ref> shows that model folding with L 2 regularization maintains the highest accuracy at higher sparsity levels, surpassing 80% accuracy. In contrast, the accuracy of the pruned network trained with L 1 drops significantly, reaching just 33% at 75% sparsity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Folding wider models</head><p>Do wider networks present more opportunities for model folding? We first examine the layer-wise correlation among matched channels in VGG11 and its wider variants on CIFAR10, as shown in Fig. <ref type="figure" target="#fig_8">8</ref>. This ablation study reveals that increasing the layer width strengthens the matched correlations, suggesting greater potential for folding. Building on this, Fig. <ref type="figure" target="#fig_10">12</ref> demonstrates the application of model folding also to 1x/2x/3x wider MLP and ResNet50 architectures, trained on CIFAR10 and CIFAR100, showing consistent performance gains as width increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Model Folding on LLMs</head><p>Table <ref type="table">5</ref> presents example outputs from both the original and the pruned LLaMA-7B models, as processed by model folding. From the responses presented in Table <ref type="table">5</ref>, it is evident that when folding 20% of the parameters, the pruned model continues to perform well. In Tab. 4, we also compare model folding with these methods on LLaMA2-7B <ref type="bibr">(Touvron et al., 2023b)</ref>, focusing on perplexity on the WikiText2 <ref type="bibr" target="#b61">(Merity et al., 2016)</ref> validation set and zero-shot performance across four tasks using the EleutherAI LM Harness <ref type="bibr">(Gao et al., 2024)</ref>. We take the same folding sparsity as shown in Tab. 3.  showing perplexity on WikiText2 and zero-shot performance across tasks. "Inf" represents an extremely great value. The "Average" is computed over four tasks. "Wanda_sp" represents an adapted Wanda method for structured pruning. Despite not using data or fine-tuning, model folding achieves comparable performance to data-driven methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Handling Residual Blocks</head><p>In this subsection we discuss the behavior of Residual Blocks after compression. In a similar manner to the analysis of Normalized Blocks, we investigate the possible dependencies between the clustering matrices for different parts of the residual block and the incoming layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Simple Residual Blocks</head><p>Consider a Simple Residual Block, consisting of a shortcut represented by an identity transform W l,s = I, and a preceding layer decomposed using a clustering matrix U l-1 . The projection matrix is defined as:</p><formula xml:id="formula_66">C l-1 = U l-1 U T l-1 U l-1 -1 U T l-1 .</formula><p>This decomposition allows for approximating the residual block while reducing redundancy in the weights. The residual block approximation satisfies:</p><formula xml:id="formula_67">y l ≈ σ W (2) l σ W (1) l C T l-1 x l-1 + C T l-1 x l-1 ,</formula><p>where x l-1 is the input to the block, y l is the output, and σ(•) represents the activation function.</p><p>The shortcut W l,s = I ensures that the input x l-1 is directly added to the output of the main path, preserving information and facilitating gradient flow.</p><p>Total Approximation Error. The total approximation error for folding the residual block is defined as:</p><formula xml:id="formula_68">J tot = ∥W tot -C (2) l W tot ∥ 2</formula><p>F , where:</p><formula xml:id="formula_69">W tot = W l-1 W (2) l .</formula><p>Here, W tot combines the weights of both layers in the residual block into a single representation. This unified view allows the clustering process to be applied holistically, ensuring that redundancies across the entire block are captured and reduced. By asserting U l-1 = U</p><p>(2) l and summing the individual folding costs J</p><p>(2) l and J l-1 , we achieve a compact representation of the residual block with minimal approximation error. This approach ensures that the compressed residual block remains effective while reducing redundancy in the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Residual Blocks with Non-Identity Shortcuts</head><p>Consider a Residual Block with a shortcut represented by a weight matrix W l,s , and a preceding layer decomposed using a clustering matrix U l-1 . The projection matrix is defined as:</p><formula xml:id="formula_70">C l-1 = U l-1 U T l-1 U l-1 -1 U T l-1</formula><p>. This decomposition allows for approximating and clustering the preceding layer's weights while maintaining their representational capacity. The corresponding approximation for the residual block satisfies:</p><formula xml:id="formula_71">y l ≈ σ W (2) l σ W (1) l C T l-1 x l-1 + W l,s C T l-1 x l-1 ,</formula><p>where:</p><p>• W</p><p>(2) l</p><p>is the weight matrix of the second layer in the residual block,</p><p>• W</p><p>(1) l</p><p>is the weight matrix of the first layer in the residual block,</p><p>• W l,s is the shortcut connection weight matrix,</p><p>• σ(•) represents the activation function.</p><p>Decomposition of Weight Matrices. The weights W</p><p>(2) l and W l,s are decomposed using their respective clustering matrices. For W</p><p>(2) l , the decomposition is:</p><formula xml:id="formula_72">C (2) l = U (2) l U (2)T l U (2) l -1 U (2)T l .</formula><p>For W l,s , the decomposition is:</p><formula xml:id="formula_73">C l,s = U l,s U T l,s U l,s -1 U T l,s</formula><p>. Substituting these decompositions into the approximation yields:</p><formula xml:id="formula_74">y l ≈ σ C (2) l U (2)T l W (2) l σ W (1) l C T l-1 x l-1 + C l,s W l,s C T l-1 x l-1 .</formula><p>Consistency Constraint and Total Approximation Error. To simplify the folding process and ensure consistency across the layers, we introduce the constraint:</p><formula xml:id="formula_75">U l,s = U (2)</formula><p>l . This ensures that the same clustering matrix is used for both the shortcut weights W l,s and the second layer's weights W</p><p>(2) l . By adding the individual folding costs J</p><p>(2) l and J l,s , we ensure that Lemma B.1 holds, leading to the total approximation error for the residual block:</p><formula xml:id="formula_76">J tot = J (2) l + J l,s .</formula><p>Unified Approximation for Residual Blocks. The total approximation error can be expressed compactly as:</p><formula xml:id="formula_77">J tot = ∥W tot -C (2) l W tot ∥ 2 F , where: W tot = W l,s | W (2) l .</formula><p>Here, W tot combines the shortcut weights W l,s and the second-layer weights W</p><p>(2) l into a single matrix. This unified representation allows the folding process to be applied holistically, reducing redundancies across the entire residual block.</p><p>The decomposition of weights in residual blocks with non-identity shortcuts introduces a consistent clustering mechanism for both the shortcut and the second layer. By ensuring that U l,s = U</p><p>(2) l , we maintain alignment in the clustering process, leading to a compact and efficient representation with minimal approximation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Handling Batch Normalization Layers</head><p>Batch Normalization layers, when combined with linear layers, introduce additional scaling and normalization operations. One special case is a layer consisting of a linear block followed by a Batch Normalization block, formally defined as:</p><formula xml:id="formula_78">z l+1 = W l+1 σ(Σ s Σ n W l x l-1</formula><p>), where:</p><p>• W l : weight matrix of the linear block,</p><p>• Σ s : Batch Normalization scaling matrix,</p><p>• Σ n : Batch Normalization normalization matrix,</p><p>• W l+1 : weight matrix of the subsequent layer,</p><p>• σ(•): activation function applied element-wise.</p><p>A design choice in handling such layers is to decompose Σ s , Σ n , and W l separately while preserving the original structure of the layer. This ensures that the scaling, normalization, and linear blocks are treated as distinct functional units. The decomposed approximation for the layer can then be expressed as:</p><formula xml:id="formula_79">z l+1 ≈ zl+1 = W l+1 C T s σ(C s Σ s C n Σ n C l W l x l-1</formula><p>), where the projection matrices C s , C n , and C l are defined as:</p><formula xml:id="formula_80">C s = U s (U T s U s ) -1 U T s = U s M s , C n = U n (U T n U n ) -1 U T n = U n M n , C l = U l (U T l U l ) -1 U T l = U l M l .</formula><p>Here, U s , U n , and U l are clustering matrices, and M s , M n , and M l are normalization terms.</p><p>Clustering Assumptions. To simplify the decomposition and ensure alignment across the layer components, we impose the following consistency constraint:</p><formula xml:id="formula_81">U s = U n = U l .</formula><p>This assumption ensures that the same clustering structure is applied to the scaling, normalization, and linear blocks, leading to a unified decomposition. Under this assumption, the approximation becomes:</p><formula xml:id="formula_82">zl+1 = W l+1 C T l σ(U l M l W b,l U l M l Σ n U l M l W l x l-1</formula><p>), where W b,l represents the intermediate scaling factors.</p><p>Applying Diagonal Properties. Using Lemma B.3, we observe that the normalization and scaling matrices can be represented as diagonal matrices:</p><formula xml:id="formula_83">zl+1 = W l+1 C T l σ(U l Diag(M l diag(W b,l ))Diag(M l diag(Σ n ))M l W l x l-1 ).</formula><p>Furthermore, by applying Lemma B.4, we rewrite this expression as:</p><formula xml:id="formula_84">zl+1 = W l+1 C T l σ(Diag(C l diag(W b,l ))Diag(C l diag(Σ n ))C l W l x l-1 ).</formula><p>This shows that the diagonal structure of the scaling and alignment matrices is preserved through the decomposition, maintaining the original behavior of the Batch Normalization block.</p><p>Compression Cost. According to the definition of the Model Folding problem and using the properties stated in Lemma B.5, the compression cost for the layer can be expressed as:</p><formula xml:id="formula_85">J tot = ∥W tot -C l W tot ∥ 2 F ,</formula><p>where:</p><formula xml:id="formula_86">W tot = W T l+1 W l diag(Σ s ) diag(Σ n</formula><p>) . This cost quantifies the approximation error introduced by clustering the weights, scaling, and normalization matrices while preserving the layer's functional structure.</p><p>By decomposing the Batch Normalization and linear blocks separately and aligning their clustering structures (U s = U n = U l ), we ensure that the original diagonal properties of the scaling and normalization matrices are preserved. The resulting compression cost captures the overall error of folding the entire layer into a compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Algorithmic Description of Fold-AR</head><p>The Fold-AR algorithm for a single layer combines the Batch Normalization components and layer weights into a compact representation, followed by clustering to reduce redundancy. The steps are described in Algorithm 1.</p><p>Algorithm 1 Fold-AR for a Single Layer</p><formula xml:id="formula_87">Require: Σ s , Σ n , W l , W l+1</formula><p>▷ Input components of the layer 1: Compute the normalized weight matrix: Ŵl ← Σ n W l 2: Construct the combined weight matrix: W tot ← W T l+1 Ŵl diag(Σ s ) 3: Solve the clustering problem: Compute intra-cluster correlation:</p><formula xml:id="formula_88">U ← arg min U ∥W tot -U(U T U) -1 U T W tot ∥ 2</formula><formula xml:id="formula_89">E[c] ← 1 N 2 c -N c i,j</formula><p>ŵl,i,: • ŵT l,j,:</p><p>∥ ŵl,i,: ∥ 2 ∥ ŵl,j,:</p><formula xml:id="formula_90">∥ 2 I(U i,c = U j,c = 1)I(i ̸ = j) 10:</formula><p>Update the scaling factor for cluster c:</p><formula xml:id="formula_91">(Σ s ) c,c ← (Σ s ) c,c N c N c + (N 2 c -N c )E[c]</formula><p>11: end for Explanation of Key Steps 1. Combining Normalization and Weights. The normalization matrix Σ n is diagonal, and multiplying it with the weight matrix W l produces the normalized weight matrix:</p><formula xml:id="formula_92">Ŵl = Σ n W l .</formula><p>This step integrates the normalization operation into the weights of the current layer, reducing the complexity of subsequent computations.</p><p>2. Construction of Combined Weight Matrix. The combined matrix W tot is defined as:</p><formula xml:id="formula_93">W tot = W T l+1</formula><p>Ŵl diag(Σ s ) .</p><p>This matrix aggregates the second-layer weights (W T l+1 ), the normalized current-layer weights ( Ŵl ), and the scaling factors (diag(Σ s )) into a single representation, preparing them for joint clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Clustering.</head><p>The projection matrix U is computed by solving the clustering problem:</p><formula xml:id="formula_94">U = arg min U ∥W tot -U(U T U) -1 U T W tot ∥ 2 F ,</formula><p>subject to U T ∈ {0, 1} m×n and m &lt; n. The clustering minimizes the reconstruction error by projecting the combined weights into a lower-dimensional space defined by m clusters.</p><p>4. Scaling Adjustments. To ensure proper scaling within each cluster, the diagonal elements of Σ s are updated. For each cluster c, the adjustment considers the size of the cluster (N c ) and the intra-cluster correlation (E[c]):</p><formula xml:id="formula_95">(Σ s ) c,c ← (Σ s ) c,c N c N c + (N 2 c -N c )E[c]</formula><p>.</p><p>The intra-cluster correlation E[c] is computed as a normalized dot product, capturing the redundancy among the weights within the same cluster. This adjustment preserves the scaling properties of the original layer.</p><p>5. Final Updates. The current-layer weights Ŵl and second-layer weights W T l+1 are updated to align with the clustered representation:</p><formula xml:id="formula_96">Ŵl ← (U T U) -1 U T Ŵl , W T l+1 ← U T W T l+1</formula><p>. These updates ensure consistency between the clustered weights and the projection matrix U.</p><p>This algorithm combines clustering, scaling adjustments, and weight updates to compress the layer while preserving its functional properties. The clustering step minimizes redundancy, and the final updates align all components of the layer with the clustered structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Folding Similar Channels in MLPs</head><p>For fully connected networks, where two successive layers are defined as:</p><formula xml:id="formula_97">x l = σ(W l x l-1 ) and x l+1 = σ(W l+1 x l ),</formula><p>where x l represents the activations of layer l, W l and W l+1 are the weight matrices, and σ is the activation function. The channels of the layer are defined as the coordinates x l,i of the vector x l . Each channel corresponds to a specific dimension in the activations.</p><p>The folding cost J l for the l-th layer is defined as:</p><formula xml:id="formula_98">J l = ∥W l -C l W l ∥ 2 F + W T l+1 -C l W T l+1 2 F</formula><p>, where C l is a clustering matrix. This cost function represents the optimization objective to minimize the approximation error introduced by folding (clustering) the weights of the l-th layer. The first term measures the reconstruction error for the weights W l , while the second term measures the reconstruction error for the weights W l+1 under the transformation C l . Together, these terms ensure that the clustering transformation preserves the structure and relationships of the weights across layers.</p><p>From the perspective of K-Means as a matrix decomposition problem, the grouping of scalar weights into vectors is defined as follows:</p><formula xml:id="formula_99">W l =      p T 1 p T 2 . . . p T n     </formula><p>and W l+1 = q 1 q 2 . . . q n , where p T i are the rows of W l and q i are the columns of W l+1 . These groupings reflect the natural structure of the weight matrices in fully connected layers:</p><p>• Each row of W l represents the weights associated with a specific output channel of layer l.</p><p>• Each column of W l+1 represents the weights associated with a specific input channel of layer l + 1.</p><p>In this formulation, the rows p T i and columns q i are treated as vectors to be clustered by the matrix C l , which aligns with the K-Means decomposition perspective. The clustering matrix C l maps these weights into representative clusters, preserving the relationships between input and output channels across layers while enabling efficient compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Folding Similar Channels in LlamaMLP and LlamaAttention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Folding Similar Channels in LlamaMLP</head><p>The LlamaMLP module is composed of three sub-layers: gate_proj, up_proj, and down_proj. These sub-layers define the structure and functionality of the MLP, with the main computation pipeline expressed as: down_proj(act_fn(gate_proj(x)) × up_proj(x)).</p><p>We cluster similar channels in both the output channel and input channel of each sub-layer.</p><p>Input Channel Folding. To fold the input channels of LlamaMLP, we simultaneously consider the input dimensions of both gate_proj and up_proj layers, as they collectively define the effective input to the gate_up sub-layer. The input channels of gate_proj and up_proj are clustered respectively using methods similar to those applied in standard MLP layers.</p><p>Output Channel Folding. To fold the output channels of LlamaMLP, we first consider the output channels of both gate_proj and up_proj by clustering and adjusting the input channel of the down_proj. Subsequently, we adjust the output channel of down_proj according to the residual connection used outside of LlamaMLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Folding Similar Channels in LlamaAttention</head><p>The LlamaAttention module consists of four primary sub-layers: q_proj, k_proj, v_proj, and o_proj. These sub-layers define the query, key, value, and output projections, respectively. For clarity and simplicity, we conceptualize q_proj, k_proj, and v_proj as a unified sub-layer referred to as q_k_v, which computes the intermediate representations required for attention calculations. The o_proj sub-layer processes the final output of the attention mechanism. We treat the attention head as the structure to be folded in LlamaAttention. By reshaping the weights of each sub-layer into an MLP-like tensor, we can cluster similar heads, similar to how it is done for a standard MLP layer.</p><p>For all configurations of LlamaAttention, including Multi-Head Attention (MHA) and Grouped Query Attention (GQA), the weight shapes of the q_k_v sub-layer differ:</p><p>• In MHA, the weights for q, k, and v projections share the same shape: [num_heads×head_dim, hidden_size].</p><p>• In GQA, the weights for k and v projections have the shape: [num_kv_heads×head_dim, hidden_size].</p><p>Output Channel Folding. When performing output channel folding for the LlamaAttention layer, the clustering of the o_proj sub-layer's output channels is dictated by the residual connection outside of LlamaAttention, ensuring alignment with the clustering results from previous modules. Specifically:</p><p>• The o_proj weights, originally shaped as [num_heads × head_dim, hidden_size], are reshaped into [num_heads, head_dim, hidden_size], clustered along the first dimension (num_heads), and then reshaped back to their original form.</p><p>• For clustering within the q_k_v sub-layer, the weights for q, k, and v are reshaped into [num_heads, head_dim, hidden_size] (or [num_kv_heads, head_dim, hidden_size] for k and v in GQA) and clustered along the first dimension (num_heads or num_kv_heads). After clustering, the weights are reshaped back to their original dimensions.</p><p>Input Channel Folding. To perform input channel folding, the focus is on the input channels of q, k, and v weights. Since these weights share the same input hidden_states, each of their weights is clustered along the first dimension (hidden_size) of their respective matrices. This ensures that the clustering process respects the shared input representation across the q_k_v sub-layer while maintaining the integrity of the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Comparison with Knowledge Distillation</head><p>We evaluated some data-free knowledge distillation (KD) methods <ref type="bibr" target="#b9">(Chen et al., 2019;</ref><ref type="bibr" target="#b19">Fang et al., 2020;</ref><ref type="bibr" target="#b62">Micaelli and Storkey, 2019;</ref><ref type="bibr" target="#b86">Yu et al., 2023)</ref>, on an NVIDIA A100 GPU, for all methods using the same pre-trained teacher model, data loader, and student model setup for consistency. The full model is a ResNet18 pre-defined by torchvision and trained on CIFAR10, while the student models for each KD method share the same architecture but differ in the number of channels across all layers to achieve the desired sparsity levels. Specifically, in ResNet18, the number of output channels for all blocks is a multiple of 64, which is also the number of output channels in the first convolutional layer.</p><p>To reduce the model's channel dimensions, we scale this base hyperparameter by a reduction factor, effectively reducing the width of all layers proportionally. The following table presents the test accuracy of compressed by KD methods and model folding on CIFAR10 test dataset.The time taken to achieve each accuracy is provided in parentheses next to the corresponding accuracy value. From the table, it is evident that the proposed model folding achieves model compression within seconds, even at high sparsity levels, compared to other KD methods that require tens of hours to complete. Sparsity Full model 10% 25% 50% 70% ABM (Micaelli and Storkey, 2019) 94.72 93.30 (17h19m) 91.99 (16h8m) 89.42 (15h30m) 85.43 (13h23m) DFAD (Chen et al., 2019) 94.72 93.79 (2h31m) 93.52 (2h3m) 92.04 (2h1m) 89.67 (1h54m) DAFL (Fang et al., 2020) 94.72 71.73 (16h48m) 77.80 (15h39m) 68.06 (15h19m) 53.86(76h34m) SpaceshipNet (Yu et al., 2023) 94.72 94.50 (42h33m) 93.95 (40h3m) 92.96 (37h57m) 91.53 (27h10m) Model Folding (ours) 94.72 94 (56.35s) 92 (53.55s) 88 (55.75s) 82 (54.95s) Table 7: Performance and resource usage at various sparsity levels across devices, with detailed breakdowns for runtime (ms), RAM usage (K), and Flash storage usage (M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Deep Inversion Sample Images</head><p>Deep Inversion (DI) <ref type="bibr" target="#b84">(Yin et al., 2020)</ref> generates synthetic images from the uncompressed network by optimizing noise to match the internal statistics stored in BatchNorm layers. These images, exemplified in Fig. <ref type="figure" target="#fig_12">13</ref>, which reflect the original data's statistical properties, are used during model folding to restore data statistics in the compressed network, ensuring accuracy without requiring external data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M Further Related Work</head><p>Model folding intersects with several established approaches in model compression, network architecture optimization and model merging. This section outlines key related works that inspired the development of model folding, highlighting both their contributions and limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.1 Model compression</head><p>Model compression techniques reduce models' size and computational requirements while maintaining or minimally sacrificing performance. Various methods have been developed. Most can be classified as pruning, quantization, knowledge distillation, and low-rank factorization. Traditional pruning techniques <ref type="bibr" target="#b16">(Entezari and Saukh, 2020;</ref><ref type="bibr" target="#b27">Han et al., 2015;</ref><ref type="bibr" target="#b28">Hassibi et al., 1993;</ref><ref type="bibr" target="#b48">LeCun et al., 1989;</ref><ref type="bibr">Li et al., 2016b)</ref>, structured or unstructured, involve removing weights, neurons, or filters that are deemed less important, typically measured by the magnitude of their contributions (e.g.,, L 1 or L 2 norm) <ref type="bibr" target="#b12">(Cheng et al., 2023;</ref><ref type="bibr" target="#b16">Entezari and Saukh, 2020;</ref><ref type="bibr" target="#b52">Li et al., 2017)</ref>. While effective in reducing the size of the model, pruning often leads to a degradation of performance that requires fine-tuning or complete retraining of the network <ref type="bibr" target="#b12">(Cheng et al., 2023;</ref><ref type="bibr" target="#b20">Frankle and Carbin, 2018;</ref><ref type="bibr" target="#b21">Frantar and Alistarh, 2022;</ref><ref type="bibr" target="#b27">Han et al., 2015;</ref><ref type="bibr" target="#b30">He et al., 2018)</ref>. Quantization <ref type="bibr" target="#b26">(Gupta et al., 2015;</ref><ref type="bibr">Li et al., 2016a;</ref><ref type="bibr" target="#b87">Zhou et al., 2017)</ref> reduces the precision of the numerical values in a model, from floating-point to lower-bit representations (e.g.,, 8-bit integers). This approach significantly reduces the model's memory footprint and speeds up computation, especially when combined with hardware accelerators designed for low-precision arithmetic <ref type="bibr" target="#b23">(Gholami et al., 2021)</ref>. Like pruning, post-training quantization may also require fine-tuning to restore model performance. Knowledge distillation <ref type="bibr" target="#b32">(Hinton et al., 2015)</ref> trains a smaller model, called the student, to replicate a well-trained larger model, called the teacher, by mimicking the output of the teacher model, which transfers knowledge between the teacher model and the student model. While effective in transferring knowledge and reducing model size, the training process for knowledge distillation can be computationally expensive and time-consuming <ref type="bibr" target="#b24">(Gou et al., 2021;</ref><ref type="bibr" target="#b32">Hinton et al., 2015)</ref>. Moreover, knowledge distillation often assumes substantial differences between student and teacher model architectures <ref type="bibr" target="#b24">(Gou et al., 2021)</ref>. Low-rank factorization decomposes weight matrices into lower-rank matrices to reduce parameter size through such as singular value decomposition <ref type="bibr" target="#b33">(Horvath et al., 2024;</ref><ref type="bibr" target="#b67">Ren and Zhu, 2023)</ref> or tensor decomposition <ref type="bibr" target="#b42">(Kim et al., 2016;</ref><ref type="bibr" target="#b47">Lebedev et al., 2015)</ref>. Approaches such as mixture of experts <ref type="bibr" target="#b36">(Jacobs et al., 1991;</ref><ref type="bibr" target="#b69">Shazeer et al., 2017)</ref>, subspace-configurable networks <ref type="bibr" target="#b66">(Papst et al., 2024;</ref><ref type="bibr" target="#b79">Wang et al., 2024)</ref> and resource-efficient deep subnetworks <ref type="bibr">(Corti et al., 2024a,b)</ref>, explore dynamic model reconfiguration to minimize the number of active weights during inference. Structured pruning. Structured pruning is of particular interest because it removes entire structures (such as neurons, channels, or layers) <ref type="bibr" target="#b16">(Entezari and Saukh, 2020;</ref><ref type="bibr" target="#b34">Hu et al., 2016;</ref><ref type="bibr">Li et al., 2016b;</ref><ref type="bibr">Luo et al., 2017a;</ref><ref type="bibr" target="#b81">Wen et al., 2016)</ref> rather than individual parameters, reducing model complexity while maintaining or even improving performance. This method is especially valuable for enhancing efficiency with easily implemented acceleration in resource-constrained environments <ref type="bibr" target="#b55">(Liu et al., 2024;</ref><ref type="bibr" target="#b80">Wang et al., 2020)</ref>. However, structured pruning typically requires additional retraining or fine-tuning <ref type="bibr" target="#b31">(He et al., 2017;</ref><ref type="bibr" target="#b55">Liu et al., 2024;</ref><ref type="bibr">Luo et al., 2017b)</ref>. Recent work by <ref type="bibr" target="#b75">Theus et al. (2024)</ref> combines model pruning and fusion using Optimal Transport theory, demonstrating that a significant portion of pruning accuracy can be recovered without access to training data. However, the impact of pruning on the model's data statistics and how to recover them is not addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.2 Model merging</head><p>Model merging combines multiple models to generate a single, unified model which leverages the strengths and diversity of each individual model. It particularly benefits ensemble learning and distributed training scenarios, where models are trained independently on different subsets of data or across different devices.</p><p>Merging can be achieved by averaging the parameters of model trained independently. Recently, multiple methods have been developed to enhance model performance and robustness. MTZ <ref type="bibr" target="#b30">(He et al., 2018)</ref> and ZipIt! <ref type="bibr" target="#b73">(Stoica et al., 2024)</ref> compress multiple models pre-trained for different tasks by merging them through neuron sharing. Model soup <ref type="bibr" target="#b82">(Wortsman et al., 2022)</ref> averages the weights of multiple fine-tuned models from same initialization to improve accuracy and robustness without increasing inference time. Taking permutation invariance of neural networks into account, a finding <ref type="bibr" target="#b17">(Entezari et al., 2022)</ref> shows the interpolation between models trained with SGD has no barrier. Git Re-Basin <ref type="bibr" target="#b0">(Ainsworth et al., 2023)</ref> utilizes activation matching and weight matching to achieve permutated alignment between models trained from different initialization. REPAIR <ref type="bibr" target="#b39">(Jordan et al., 2022)</ref> mitigate variance collapse problem while aligning neurons by rescaling the preactivations of fused models. PAPA leverages a population of diverse models trained on different data variations and slowly pushes the weights of the networks towards the population average <ref type="bibr" target="#b38">(Jolicoeur-Martineau et al., 2024)</ref>. A recent work <ref type="bibr" target="#b83">(Yamada et al., 2023)</ref> shows that for model merging on different datasets, using original or condensed datasets during the model merging process can significantly improve accuracy. However, those methods do not consider model efficiency and internal parameter redundancy. Another recent work <ref type="bibr" target="#b75">(Theus et al., 2024)</ref> achieves intra-layer model fusion by integrating optimal transport <ref type="bibr" target="#b40">(Kantorovich, 2006;</ref><ref type="bibr" target="#b63">Monge, 1781;</ref><ref type="bibr" target="#b71">Singh and Jaggi, 2020)</ref> to fuse computational structures in the model without fine-tuning. We note that this approach is orthogonal to the problem solved in this paper, as we do not consider intra-layer dependencies.</p><p>Merging multiple computational units. Merging computational units has been extensively explored in ensemble methods. <ref type="bibr" target="#b82">Wortsman et al. (2022)</ref> demonstrate that combining multiple models fine-tuned from the same pretrained initialization enhances both accuracy and robustness. <ref type="bibr" target="#b0">Ainsworth et al. (2023)</ref> extend this approach to models trained on the same data with different initializations, albeit with some accuracy loss.</p><p>Jordan et al. (2022) improve upon Git Re-Basin by adjusting batch normalization layers where applicable. IFM Chen et al. (2023) and ZipIt! Stoica et al. (2024) focus on merging multiple computational units within a single model, pioneering this approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model compression and repair of data statistics. Left: Model folding pipeline is applied layer-wise, consisting of three phases: weight tensor clustering and merging, and data statistics repair.Right: To maintain accuracy, the data variances of compressed and uncompressed models must align (i.e., the variance ratio must be close to 1), as variance collapse or explosion leads to suboptimal performance. Our data-free and fine-tuning-free model folding methods (Fold-AR and Fold-DIR) achieve performance comparable to data-driven statistics repair (Fold-R), while outperforming naive statistics repair (Fold-naive) and the recently proposed IFM<ref type="bibr" target="#b11">(Chen et al., 2023)</ref>. All methods were evaluated on a public ResNet18 checkpoint trained on CIFAR10. Lines connect the performance of different methods at the same weight sparsity level, applied uniformly across all layers. Variance ratio refers to the activation outputs in the last layer. A precise definition and analysis are in Sec. 3.</figDesc><graphic coords="2,319.36,78.74,219.95,117.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Layer-wise correlation between matched channels in ResNet18 trained on CIFAR10.For each layer, we use activation matching matching with L 2 distance measure to greedily pair similar neurons. Each subplot shows the correlation within all matched pairs.</figDesc><graphic coords="4,72.00,72.00,468.00,126.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: k-means (KM) outperforms other clustering methods: Spectral Clustering (SC), Agglomerative Clustering (AC) with different linkage criteria and Iterative Greedy (greedy) used to compress ResNet18 trained on CIFAR10. Data-based REPAIR was used to restore data statistics after clustering for all methods.</figDesc><graphic coords="7,165.60,72.00,280.81,150.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>denotes the indices of all values belonging to cluster c, N c = |I c | denotes the number of values in the cluster, and xl = Σ n W l x l . The batch normalization using Σ n ensures that the variances of all xl (i) equal 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variance collapse and overshooting on ResNet18 with CIFAR10. The goal is to align the layer-wise variance in the compressed network to that of the uncompressed model. Naive averaging of statistics (Fold-Naive) leads to variance collapse (Jordan et al., 2022), while IFM overshoots. Fold-AR and Fold-DIR closely match the performance of the data-driven REPAIR (Fold-R). Layer-wise sparsity is 0.5.</figDesc><graphic coords="8,72.00,72.00,229.32,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Data-free folding methods with approximate REPAIR (Fold-AR) and Deep Inversion (Yin et al., 2020) (Fold-DIR) and on ResNet18 with CI-FAR10 at various weight sparsity levels, uniformly distributed across layers. Fold-DIR performs similarly to the data-based REPAIR (Fold-R). Both Fold-AR and Fold-DIR surpass IFM (Chen et al., 2023) by a significant margin.</figDesc><graphic coords="8,310.68,77.97,229.32,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison with IFM<ref type="bibr" target="#b11">(Chen et al., 2023)</ref> and structured magnitude pruning<ref type="bibr" target="#b7">(Cai et al., 2020;</ref><ref type="bibr" target="#b85">Yin et al., 2022)</ref>. Model folding, when tested on ResNet18 (top row) and VGG11-BN (bottom row) trained on CIFAR10 (left column) and ImageNet (right column), outperforms IFM with higher sparsity and increasing dataset difficulty.</figDesc><graphic coords="9,75.02,195.84,229.32,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of model folding with IFM (Chen et al., 2023), and INN<ref type="bibr" target="#b72">(Solodskikh et al., 2023)</ref> using ResNet18 on CIFAR10. In the original experiment defined in the IFM and INN papers, where only the last two blocks of a ResNet18 are pruned, folding is significantly better than INN while it matches the performance of IFM for lower sparsities and becomes significantly better for higher sparsities. Note, the maximum sparsity achievable by INN is 54%<ref type="bibr" target="#b72">(Solodskikh et al., 2023)</ref>.</figDesc><graphic coords="12,75.02,72.00,229.32,122.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Layer-wise correlation among matched channels in VGG11 and its wider variants on CIFAR10. This figure shows correlation matrices for each layer of VGG11 and its 1x and 3x wider variants, derived from activation matching. Opaque black represents the 1x wider model, while vibrant colors indicate the 3x wider model, highlighting differences in correlation strength.</figDesc><graphic coords="12,74.34,276.54,463.30,81.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: ResNet18 (left column) and VGG11 (right column) models trained with L 1 (top row) and L 2 (bottom row) regularization. Structured magnitude pruning outperforms model folding only if training explicitly regularizes for model sparsity (L 1 norm). REPAIR is hardly beneficial for all structural pruning methods.</figDesc><graphic coords="25,93.74,233.58,210.59,112.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Model folding performance improves with increasing model width. The MLP model consists of three stacked mlp blocks (including a fully connected layer, a BN layer, and a ReLU layer), followed by a final classifier. Upscaled versions of MLP (left) and ResNet50 (right) architectures, trained on CIFAR10 and CIFAR100, demonstrate the consistent advantages of model folding.</figDesc><graphic coords="26,75.02,72.13,229.32,122.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F 6 :</head><label>6</label><figDesc>subject to U T ∈ {0, 1} m×n and m &lt; n 4: Update the scaling matrix:Σ s ← (U T U) -1 U T Σ s U 5: Update the second-layer weights: W T l+1 ← U T W T l+1Update the current-layer weights: Ŵl ← (U T U) -1 U T Ŵl 7: for c = 1, . . . , m do ▷ Adjust scaling factors for each cluster 8:Compute cluster size:N c ← i I(U i,c = 1) ▷ I(•)is the indicator function 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Sample images generated by Deep Inversion (Yin et al., 2020) using ResNet18 trained on CIFAR100. These images are generated from the uncompressed network and used in model folding to restore data statistics in the compressed network.</figDesc><graphic coords="36,205.38,72.00,201.24,181.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="24,74.34,418.94,463.33,119.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 )</head><label>1</label><figDesc>, we used instances of VGG11 and ResNet18 trained on CIFAR10 with a 5+5 label split. All experiments were performed with REPAIR.</figDesc><table><row><cell>Model</cell><cell cols="3">WM ZipIt! Model Folding (Ours)</cell></row><row><cell>VGG11</cell><cell>0.57</cell><cell>0.69</cell><cell>0.71</cell></row><row><cell cols="2">ResNet18 0.48</cell><cell>0.74</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison for merging networks trained on separate tasks.For the experiments involving merging networks trained on the same task (see Table2), we used instances of VGG11 and ResNet18, both trained on CIFAR10. All experiments were performed with REPAIR.</figDesc><table><row><cell>Model</cell><cell cols="3">WM ZipIt! Model Folding (Ours)</cell></row><row><cell>VGG11</cell><cell>0.89</cell><cell>0.87</cell><cell>0.92</cell></row><row><cell cols="2">ResNet18 0.92</cell><cell>0.91</cell><cell>0.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance of structured pruning methods on LLaMA2-7B without post-tuning,</figDesc><table><row><cell cols="2">Prune ratio Method</cell><cell cols="7">Data usage WikiText2↓ BoolQ WinoGrande ARC-e ARC-c Average↑</cell></row><row><cell>0%</cell><cell>LLaMA2-7B (Touvron et al., 2023b)</cell><cell>/</cell><cell>5.12</cell><cell>77.7</cell><cell>68.98</cell><cell>76.34</cell><cell>43.26</cell><cell>66.57</cell></row><row><cell>20%</cell><cell>Magnitude Prune</cell><cell>/</cell><cell>Inf</cell><cell>44.8</cell><cell>49.8</cell><cell>26.22</cell><cell>21.93</cell><cell>44.52</cell></row><row><cell>20%</cell><cell>LLM-Pruner (Ma et al., 2023)</cell><cell>Gradients</cell><cell>10.58</cell><cell>64.62</cell><cell>63.54</cell><cell>68.39</cell><cell>36.52</cell><cell>51.78</cell></row><row><cell>20%</cell><cell>FLAP (An et al., 2023)</cell><cell>Calibration</cell><cell>6.87</cell><cell>71.5</cell><cell>68.74</cell><cell>70.20</cell><cell>36.95</cell><cell>61.85</cell></row><row><cell>20%</cell><cell>Wanda_sp (Sun et al., 2023)</cell><cell>Calibration</cell><cell>8.78</cell><cell>72.20</cell><cell>63.93</cell><cell>70.50</cell><cell>40.01</cell><cell>61.66</cell></row><row><cell>20%</cell><cell>Model Folding</cell><cell>/</cell><cell>17.46</cell><cell>62.72</cell><cell>61.80</cell><cell>49.00</cell><cell>26.62</cell><cell>50.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of knowledge distillation and model folding, showing accuracy (%) and runtime (in parentheses). The sparsity levels indicate the percentage of weights pruned.We apply model folding on a LeNet5 model pre-trained on FashionMNIST with different sparsity, and then evaluate the folded models on NVIDIA Jetson Nano, ESP-EYE, and Arduino Nano 33 BLE. All models are converted and executed as a float32 Tensorflow Lite model in all devices.</figDesc><table><row><cell>Sparsity</cell><cell></cell><cell>10%</cell><cell></cell><cell>25%</cell><cell></cell><cell>50%</cell><cell></cell><cell>70%</cell></row><row><cell></cell><cell cols="8">Runtime RAM Flash Runtime RAM Flash Runtime RAM Flash Runtime RAM Flash</cell></row><row><cell>NVIDIA Jetson Nano (NVIDIA, 2024)</cell><cell>2ms</cell><cell>59.5K 3.4M</cell><cell>2ms</cell><cell>55.7K 2.8M</cell><cell>1ms</cell><cell>48.0K 1.9M</cell><cell>1ms</cell><cell>36.5K 1.2M</cell></row><row><cell>ESP-EYE (Espressif Systems, 2024)</cell><cell>2591ms</cell><cell>59.5K 3.4M</cell><cell>1868ms</cell><cell>55.7K 2.8M</cell><cell>1532ms</cell><cell>48.0K 1.9M</cell><cell>1186ms</cell><cell>36.5K 1.2M</cell></row><row><cell>Arduino Nano 33 BLE Sense (Arduino, 2024)</cell><cell>6831ms</cell><cell>59.5K 3.4M</cell><cell>3726ms</cell><cell>55.7K 2.8M</cell><cell>4218ms</cell><cell>48.0K 1.9M</cell><cell>2969ms</cell><cell>36.5K 1.2M</cell></row></table><note><p>K Inference Speed of Folded Models on Edge Devices</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>ExperimentsFollowing related works on model merging<ref type="bibr" target="#b0">(Ainsworth et al., 2023;</ref><ref type="bibr" target="#b11">Chen et al., 2023;</ref><ref type="bibr" target="#b39">Jordan et al., 2022)</ref>, we evaluate folding on convolutional architectures, including ResNets<ref type="bibr" target="#b29">(He et al., 2016)</ref> and VGGs<ref type="bibr" target="#b70">(Simonyan and Zisserman, 2014)</ref> of varying sizes on CIFAR10, CIFAR100(Krizhevsky et al., 2009b)  and ImageNet<ref type="bibr" target="#b15">(Deng et al., 2009)</ref>. For models trained on the CIFAR10 and CIFAR100 datasets, we used the hyperparameters available from online benchmarks 23 . For models trained on ImageNet, the pre-trained weights were taken from torchvision. For large language models (LLMs), we evaluate model folding on LLaMA-7B(Touvron  et al., 2023a)  with pre-trained weights from Hugging Face Hub. In all experiments, model sparsity denotes the proportion of weights that have been removed as a result of model compression. Experimental setup is detailed in Appendix A. Further evaluation results are in Appendix J and K. Model folding mitigates variance collapse. Fig.6compares model folding with IFM<ref type="bibr" target="#b11">(Chen et al., 2023)</ref>, a recently introduced data-free, fine-tuning-free method that combines aspects of folding and pruning. Unlike model folding, which accurately corrects the data statistics in the compressed model, IFM merges matched input channels by summing one and zeroing the other, followed by a weighted average of output channels. In contrast to the original paper, Fig.6applies the same sparsity ratio across all layers for every method. We find that model folding significantly outperforms IFM, particularly at higher sparsity levels and for larger networks. Additionally, Fig.7(left) replicates the experiment from<ref type="bibr" target="#b11">(Chen et al., 2023)</ref> on ResNet18 with CIFAR10, using the same per-layer sparsity pattern where only the last two blocks are sparsified. In this scenario, IFM offers a slight performance edge over our method for low sparsity, but struggles with higher sparsity.2 https://github.com/huyvnphan/PyTorch_CIFAR10 3 https://github.com/weiaicunzai/pytorch-cifar100/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://pytorch.org/hub/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://huggingface.co/docs/hub/index</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://wandb.ai</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Franz Papst</rs> and <rs type="person">Francesco Corti</rs> for their insightful comments on the early draft of the manuscript. This work was partly funded by the <rs type="funder">Austrian Research Promotion Agency (FFG)</rs> and <rs type="grantNumber">Pro2Future</rs> (<rs type="grantNumber">STRATP II 4.1</rs>.4 E-MINDS strategic project). The results presented in this paper were computed using the computational resources of <rs type="affiliation">Zentralen Informatikdienstes of Graz University of Technology and Pro2Future GmbH</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wJb3yxK">
					<idno type="grant-number">Pro2Future</idno>
				</org>
				<org type="funding" xml:id="_TqdcVWD">
					<idno type="grant-number">STRATP II 4.1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The following sections provide supplementary information omitted from the main text:</p><p>• Section A: Implementation Details.</p><p>• Section B: Further Theoretical Results to Support Model Folding.</p><p>• Section C: Channel Similarity.</p><p>• Section D: Model Folding on LLMs.</p><p>• Section E: Handling Residual Blocks.</p><p>• Section F: Handling Batch Normalization Layers.</p><p>• Section G: Folding Similar Channels in MLPs.</p><p>• Section H: Folding Similar Channels in Convolutional Layers.</p><p>• Section I: Folding Similar Channels in LlamaMLP and LlamaAttention.</p><p>• Section J: Comparison with Knowledge Distillation.</p><p>• Section K: Inference Speed of Folded Models on Edge Devices.</p><p>• Section L: Deep Inversion Sample Images.</p><p>• Section M: Further Related Work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation details</head><p>We trained over 100 models on a NVIDIA DGX Station A100 featuring eight NVIDIA A100 GPUs (each equipped with 80GB memory) to evaluate the performance of model folding presented in this work. For a folding experiment, we apply the same compression ratio to all layers. Pytorch Hub 4 and Huggingface Hub 5 are used to load pre-trained checkpoints for complex model-dataset combinations, including ResNet18/ResNet50/VGG11 on ImageNet and LLaMA-7B <ref type="bibr">(Touvron et al., 2023a)</ref>. WandB 6 is used to log training history, folding result, and evaluation metrics. The source code of all experiments is available here: <ref type="url" target="https://github.com/nanguoyu/model-folding-universal">https://github.com/nanguoyu/ model-folding-universal</ref> B Further theoretical results to support model folding Lemma B.1. Let x ∈ R k and let U ∈ {0, 1} n×k be a binary clustering matrix with j u ij = 1. Then with any element-wise nonlinear function σ(•) we have</p><p>Proof of Lemma B.1. Define y = Ux, z = σ(Ux) and v = σ(x), w = Uσ(x). Note that in any row of U just one element satisfies u ij = 1. We define such an element by a function p with u ij = 1 ⇔ p(i) = j.</p><p>Therefore, y i = x p(i) and z i = σ(y i ) = σ(x p(i) ) for all 1 ≤ i ≤ n. Moreover, v i = σ(x i ) and w i = v p(i) = σ(x p(i) ). Therefore, z i = w i and z = w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Generated Sentences</head><p>LLaMA-7B (ratio=0%) Instruction: What are prime numbers? Response: Prime numbers are numbers that are only divisible by 1 and itself. <ref type="bibr">For example,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">11,</ref><ref type="bibr">13,</ref><ref type="bibr">17,</ref><ref type="bibr">19,</ref><ref type="bibr">23,</ref><ref type="bibr">29,</ref><ref type="bibr">31,</ref><ref type="bibr">37,</ref><ref type="bibr">41,</ref><ref type="bibr">43,</ref><ref type="bibr">47,</ref><ref type="bibr">53,</ref><ref type="bibr">59,</ref><ref type="bibr">61,</ref><ref type="bibr">67,</ref><ref type="bibr">71,</ref><ref type="bibr">73</ref> Instruction: Please introduce Newton's second law Response: Newton's second law of motion states that the net force acting on an object is equal to the rate of change of the object's momentum. The net force acting on an object is equal to the rate of change of the object's momentum. The net force acting on an object is equal to the rate of change of the object's momentum. Newton's second law of motion states that the net force acting on an object is equal to the rate Instruction: What is the difference between monkeys and gorillas? Response: Monkeys are primates that are smaller than gorillas. They are also more agile and have longer tails. Gorillas are larger than monkeys and have shorter tails. What is the difference between a gorilla and a chimpanzee? What is the difference between a gorilla and a chimpanzee? What is the difference between a gorilla and a chimpanzee? What is the difference between a gorilla Instruction: How many planets are there in the solar system? Response: There are eight planets in the solar system. The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune. What is the name of the planet that is closest to the sun? What is the name of the planet that is the closest to the sun? What is the name of the planet that is the closest to the sun What is the name of the planet that is the closest to the sun LLaMA-7B (ratio=20%) Instruction: What are prime numbers? Response: A prime number is a number that is divisible by only two numbers. For example, 2, <ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">7,</ref><ref type="bibr">11,</ref><ref type="bibr">13,</ref><ref type="bibr">15,</ref><ref type="bibr">17,</ref><ref type="bibr">23,</ref><ref type="bibr">25,</ref><ref type="bibr">27,</ref><ref type="bibr">31,</ref><ref type="bibr">33,</ref><ref type="bibr">35,</ref><ref type="bibr">37,</ref><ref type="bibr">41,</ref><ref type="bibr">43,</ref><ref type="bibr">45,</ref><ref type="bibr">47,</ref><ref type="bibr">51</ref> Instruction: Please introduce Newton's second law Response: The second law of motion is Newton's law of gravity. Newton's law of gravity is that every object is attracted by every other object. The more the two objects are attracted, the more strongly they are attracted. Newton's law of gravity is the law of gravity. What is the second law of motion? What is the second law of motion? What is the second law of motion? What is the second law of motion Instruction: What is the difference between monkeys and gorillas? Response: The answer is simple: monkeys are smaller animals while gorillas are larger animals. What is the difference between a monkey and a gorilla? The answer is simple: monkeys are smaller animals while gorillas are larger animals. The size of the body is not the only difference between monkeys and g What is the difference between a monkey and a gri What is the difference between a monkey and a griWhat is the difference between a Instruction: How many planets are there in the solar system? Response: There are eight planets in the solar system. The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Aster and Nept. The planets are arranged in order of size from smallest to largest. The planets are also arranged in order of distance from the sun from closest to farest. What is the difference between planets and stars? What is the difference between planets and stars? What is the difference between planets Table <ref type="table">5</ref>: Generated examples from the original LLaMA-7B and pruned by model folding. The maximal number of output tokens is set to 100 in both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decomposing W</head><p>(2) l . Let the weights W</p><p>(2) l be decomposed using a clustering matrix U</p><p>(2) l and its corresponding projection:</p><p>Substituting this decomposition into the residual block yields:</p><p>This approximation captures the effect of clustering and compressing the weights while maintaining the structure of the residual block.</p><p>Aligning Clustering Matrices. To simplify the folding process, we assert that U l-1 = U</p><p>(2)</p><p>l . This ensures consistency in the clustering across the residual block, reducing the need for additional transformations between layers. As a result, the folding costs for the preceding layer and the current layer can be summed directly:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Folding Similar Channels in Convolutional Layers</head><p>For convolutional layers, two successive layers can be defined as:</p><p>where X l is a 3-dimensional feature tensor with values X (l) co,i,j . The first dimension, c o , corresponds to the output channels, while i and j represent spatial pixel locations. The 4-dimensional weight tensor W l has values W (l) co,ci,i,j , where: • c o corresponds to the output channels of X l ,</p><p>• c i corresponds to the input channels of X l-1 .</p><p>To simplify and compress the network, we decompose the weight tensor W l such that output channels of X l (i.e., the values X (l) co,i,j for c o = 1, . . . , c out ), which are similar in some sense, are merged. This folding problem is defined as:</p><p>, where C l corresponds to a 1 × 1 convolution parameterized by the clustering matrix C l , with</p><p>From this definition, it follows that:</p><p>where the weight tensors W l and W l+1 are mapped to matrices W l and W l+1 as follows:</p><p>1,2,:,:</p><p>1,cin,:,:</p><p>This means that each convolutional filter contributing to an output channel c o is flattened and stacked into a vector, forming the c o -th row of the matrix W l . Similarly, for W l+1 , each filter associated with the c i -th input channel is flattened and stacked into a vector, forming a column of the matrix W l+1 : From the perspective of K-Means as a matrix decomposition problem, the grouping of scalar weights into vectors is defined as follows:</p><p>and W l+1 = q 1 q 2 • • • q n , where:</p><p>p T i = vec(W In this formulation, the rows p T i of W l and columns q j of W l+1 are grouped into clusters for the folding process, aligning with the K-Means decomposition perspective.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Git re-basin: Merging models modulo permutation symmetries</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2209.04836" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fluctuation-based adaptive structured pruning for large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.11983" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Arduino nano 33 ble documentation</title>
		<author>
			<persName><surname>Arduino</surname></persName>
		</author>
		<ptr target="https://docs.arduino.cc/hardware/nano-33-ble/" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Croci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Do Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><surname>Slicegpt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.15024" />
		<title level="m">Compress large language models by deleting rows and columns</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1512.07548" />
		<title level="m">k-means clustering is matrix factorization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<ptr target="https://arxiv.org/abs/2108.07258" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Data-free learning of student networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.01186" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning on mobile and embedded devices: State-of-the-art, challenges, and future directions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06756</idno>
		<title level="m">Going beyond neural network feature similarity: The network feature complexity and its interpretation using category theory</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.06767" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HADS: Hardware-aware deep subnetworks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Corti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pferschy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=oDacwa4yb2" />
	</analytic>
	<monogr>
		<title level="m">5th Workshop on practical ML for limited/low resource settings, 2024a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">REDS: Resource-efficient deep subnetworks for dynamic resource constraints</title>
		<author>
			<persName><forename type="first">F</forename><surname>Corti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Pferschy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2311.13349" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Class-dependent compression of deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.10364" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The role of permutation invariance in linear mode connectivity of neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.06296" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://www.espressif.com/en/products/devkits/esp-eye/overview" />
		<title level="m">Esp-eye development board -espressif systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
		<respStmt>
			<orgName>Espressif Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data-free adversarial distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.11006" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<ptr target="https://arxiv.org/abs/1803.03635" />
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal brain compression: A framework for accurate post-training quantization and pruning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frantar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4475" to="4488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/12608602" />
		<imprint>
			<biblScope unit="page" from="7" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey of quantization methods for efficient neural network inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.13630" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-021-01453-z</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-021-01453-z" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<idno type="ISSN">1573-1405</idno>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Implicit regularization in matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.09280" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal brain surgeon and general network pruning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on neural networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="293" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-task zipping via layer-wise neuron sharing</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1503.02531" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Maestro: Uncovering low-rank structures via trainable decomposition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laskaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.14929" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Network trimming: A data-driven neuron pruning approach towards efficient deep architectures</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1607.03250" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Dataless knowledge fusion by merging weights of language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2212.09849" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gervais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.03094" />
		<title level="m">Population parameter averaging (papa)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Repair: Renormalizing permuted activations for interpolation repair</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.08403</idno>
		<ptr target="https://arxiv.org/abs/2211.08403" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the translocation of masses</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical sciences</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1381" to="1382" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y.-D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Shin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.06530" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cifar-100 and cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~kriz/cifar.html.MITLicense" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Resource-efficient machine learning in 2 kb ram for the internet of things</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1935" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6553" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/1989/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>c7093bd25041881277658-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Micorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.18953" />
		<title level="m">Sit back and relax: Learning to drive incrementally in all weather conditions</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04711</idno>
		<ptr target="https://arxiv.org/abs/1605.04711" />
		<title level="m">Ternary weight networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<ptr target="https://arxiv.org/abs/1608.08710" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.08710" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopcroft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07543</idno>
		<ptr target="https://arxiv.org/abs/1511.07543" />
		<title level="m">Convergent learning: Do different neural networks learn the same representations?</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H.-I</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Lightweight deep learning for resource-constrained environments: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-K</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2404.07236" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Llm-pruner: On the structural pruning of large language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="21702" to="21720" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Merging models with fisher-weighted averaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2111.09832" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Shortgpt: Layers in large language models are more redundant than you expect</title>
		<author>
			<persName><forename type="first">X</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.03853" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<ptr target="https://arxiv.org/abs/1609.07843" />
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.09768" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mémoire sur la théorie des déblais et des remblais</title>
		<author>
			<persName><forename type="first">G</forename><surname>Monge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mem. Math. Phys. Acad. Royale Sci</title>
		<imprint>
			<biblScope unit="page" from="666" to="704" />
			<date type="published" when="1781">1781</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
		<ptr target="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Jetson nano -nvidia developer</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/product-development/" />
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sensor-guided adaptive machine learning on resourceconstrained devices</title>
		<author>
			<persName><forename type="first">F</forename><surname>Papst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rechberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on the Internet of Things</title>
		<meeting>the International Conference on the Internet of Things</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Low-rank prune-and-factorize for language model compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.14152" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Model fusion via optimal transport</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22045" to="22055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Integral neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Solodskikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurbanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aydarkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zhelavskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Parfenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
			<biblScope unit="page" from="16113" to="16122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Zipit! merging models from different tasks without training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bjorner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.03053" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">A simple and effective pruning approach for large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11695</idno>
		<ptr target="https://arxiv.org/abs/2306.11695" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Theus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Geimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07839</idno>
		<ptr target="https://arxiv.org/abs/2402.07839" />
		<title level="m">Towards meta-pruning via optimal transport</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2302.13971" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2307.09288" />
		<imprint/>
	</monogr>
	<note>Llama 2: Open foundation and fine-tuned chat models, 2023b</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Deep learning models for real-time human activity recognition with smartphones</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Networks and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="743" to="755" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thiele</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.13536" />
		<title level="m">Subspace-configurable networks</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Sparse-yolo: Hardware/software co-design of an fpga accelerator for yolov2</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="116569" to="116585" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.05482" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Revisiting permutation symmetry for merging models between different datasets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chijiwa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2306.05641" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.08795" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Exploring structural sparsity in neural image compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.04595" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Data-free knowledge distillation via feature exchange and activation region constraint</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="24266" to="24275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<ptr target="https://arxiv.org/abs/1702.03044" />
		<title level="m">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
