# From Discrete to Continuous Convolution Layers

## Abstract

## 

A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.

* equal contribution Preprint. Under review.

## Introduction

The basic building block of CNNs, the convolution layer (conv-layer), applies a discrete convolution (more precisely, cross-correlation) to an input feature map, with a learned discrete filter. CNNs commonly employ spatial resizing of their feature maps, either downscaling (e.g., in classification networks) or upscaling (e.g., in generative models and image processing tasks). These resizing operations are either limited to an integer scale factor (achieved by an integer stride), or via unprincipled feature map interpolation. These resizing approaches suffer from inherent problems (elaborated below), which limit the performance and capabilities of current CNNs.

In this paper, we propose a generalization of the standard conv-layer. Given a discrete feature map I of spatial dimensions H×W , we are interested in a learned conv-layer that outputs a feature map I of H ×W , where none of the ratios of the spatial dimensions ( H H , H H , W W , W W ) need to be integer numbers. We show that this can be achieved by modeling a learned continuous convolution, which is end-to-end trainable by gradient descent. Furthermore, we show that such continuous modeling allows for the desired output size H × W to be chosen dynamically, at inference time. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. Fig. [1](#fig_0) shows the underlying process of a CC-layer, which has two parts: (i) modulating the discrete input with a learned continuous filter to get a continuous intermediate representation; (ii) re-sampling the continuous representation according to a re-sampling grid to get the discrete output. The only learned part in a CC-layer is the continuous filter. Once trained, the CC-layer can be used to output any scale or shape chosen at inference time, simply by using a different re-sampling grid.

To date, the most common approaches for feature map resizing in CNNs are the strided-conv or pooling for downscaling, or the transposed-conv for upscaling. These suffer from inherent limitations: (i) The stride must be an integer, hence a learned transformation that includes spatial resizing can only be done by an integer (or inverse of an integer) scale-factor. (ii) Once trained, the filter and stride are fixed and cannot be modified at inference time. (iii) Strided-convolutions suffer from impaired shift-equivariance [[20,](#b18)[2]](#b1). Transposed-convolutions suffer from inherent checkerboard artifacts [[13]](#b11).

Interpolation-based resizing methods (e.g., linear, nearest neighbor, cubic [[7]](#b6)), which are mostly used for images, are usually differentiable and can be incorporated in a neural network to resize feature maps. Such resizing is principled; it follows a consistent rule across all scales and sub-pixel/subfeature positions. However, these are not learned resizing operations, and furthermore cannot express more general feature transforms, only scaling. We do however draw inspiration from the way these methods are applied to images [[12]](#b10), and show how they can be replaced by dynamic learnable models.

The most closely related work to ours is that of [[17]](#b15). They were the first to coin the term "Continuous Convolution", and identified its need for applying CNNs on 3D point-clouds (points scattered irregularly in 3D space; not on a regular grid). They used a method similar to ours for calculating convolution weights for points in the irregular 3D cloud. However, modeling continuous convolution for point-clouds is a clear necessity, as there is no notion of "stride" in such input data. Moreover, their continuous convolution is not meant to resize and is used for a different goal. Inspired by their work, we generalize the concept of Continuous Convolution of [[17]](#b15) in several ways: (i) We adapt it to the realm of regular grids, where stride/scale is well-defined, yet show that it is useful also here. (ii) Our continuous convolution can be consistently applied to any choice of scale and output shape, and (iii) the scale and shape of our CC-layer output can be determined dynamically at inference time.

It was shown by [[2,](#b1)[20]](#b18) that very small shifts to the input data of a CNN (e.g., shifting an input image by 1-2 pixels) can drastically change the output prediction. This lack of shift-invariance/equivariance is due to the aliasing induced by strided conv-layers with small kernel support. CC enables more "gradual-architectures" (still with small kernel support) that avoid aliasing, thus giving rise to true shift-equivariance.

The contributions of our paper are therefore several-folded:

• From Discrete to Continuous CNNs: We introduce a generalization of the standard discrete conv-layer, to a continuous CC-layer, in a principled way. • Resize by any scale: CC is the first learnable layer that can resize feature-maps by any scale (integer or non-integer; downscaling/upscaling; can also differ between dimensions). • Dynamic: The desired scale and shape of the CC output can be set at inference-time.

• True shift-equivariance: CC enables gradual-downscaling that induces less aliasing.

• Resolving input-output misalignments: We further show that conv-layers often induce small inherent misalignments, which are ameliorated by CC.

2 Overview of the approach Fig. [1](#fig_0) describes the underlying process we aim to model: given a discrete feature map I[i, j], we are interested in constructing a CC-layer that operates on I as an input, and produces a next layer I [i , j ], by modeling a learned continuous convolution between the input and output feature-maps. Note that the input and output feature-maps I and I need not be related by an integer scale factor (or its inverse). Such an action can be realized by the following steps: (i) Use a "Bed of Nails" representation of I: I cont (h, w) = i,j δ(h -i, w -j)I[i, j]. where i, j are discrete 'pixel' (feature) locations in I, and h, w are continuous coordinates in the continuous feature-map space. For simplicity, we will refer from here on to all discrete coordinates inside feature maps by the term 'pixels', and their in-between non-integer coordinates as sub-'pixel' (although we are referring to general network layers; usually not images).

(ii) Apply a convolution with a learned continuous kernel K θ (h, w).

(iii) Resample the continuous result of the convolution according to the desired shape and scale-factors of I . The resampling grid is purely a function of the shape and scales. This means that the same continuous kernel K θ is eligible for producing any desired scale-factor for I .

These phases are indicated in Fig. [1](#fig_0), and can be mathematically formulated as follows:

$CC{I}[n] = {I cont * K θ }(g n ) = m δ (g n -τ -m) I[m]K θ (τ )dτ = m I[m] δ (g n -τ -m) K θ (τ ) dτ = m I[m]K θ (g n -m)(1)$where m = (i, j), n = (i , j ) are discrete 'pixel' coordinates in I and I respectively, and g n denotes the continuous sampling locations of the grid in Fig. [1](#fig_0) (i.e., the sub-'pixel' position of each 'pixel' in I when projected onto the coordinates of I).

Eq. 1 shows that for K θ with finite support, the output of the continuous convolution at I [n] is a weighted sum of the discrete "Neighbors" of its projected location g n in the input I. The weights are a function of the distance between the discrete 'pixel' center m to the continuous sampling location g n . The desired sampling grid g n is calculated by projecting the output integer 'pixel' locations n = (i , j ) in I , to sub-'pixel' coordinates in the input I. We call it the "Projected Grid".

In order to execute our approach we need to overcome an inherent challenge: CNNs are built upon regular grids with discrete coordinates. We therefore need to model a continuous process with discrete components. Fig. [2](#fig_1) illustrates our CC-layer construction, which builds upon implementation ideas from standard image resizing techniques [[12]](#b10). Our CC-layer consists of 4 principled building blocks. These are marked by numbers in Fig. [2](#fig_1), and are described in detail in Sec. 3:

1. Calculate Projected Grid g n from the desired scale-factors and output-shape. 2. Extract Neighbors N : Map each sampling point g n to its discrete neighbors N [n] within the continuous kernel support. 3. Calculate Weights W by applying K θ to distances between sampling points to their neighbors:

$W[n, m] = K θ (g n -m$). This "Internal-Net" K θ is the only trainable part in the CC-layer. 4. Apply Weights: Multiply N W and sum over all neighbors and input channels, to obtain the output feature map I , with any desired scale or shape (which can be determined at test time).  Here N is the batch-size, C is the number of channels. H, W are spatial dimensions. The desired spatial size H , W may be chosen at inference time. Our CC-layer consists of 4 principled building blocks, marked by numbers in Fig. [2](#fig_1) (for simplicity, I and I are drawn as 2D vectors in Fig. [2](#fig_1), i.e., 4D tensors with N = 1 and C = 1). These are described next:

Block 1 -Projected-grid: The Projected grid g matches each output 'pixel' n=(i , j )∈I to a sub-'pixel' location g n in the continuous space of the input I. This grid is captured by a tensor of shape [2, H , W ] where the the first dimension are the 2 projected sub-'pixel' coordinates (vertical and horizontal) of each output 'pixel' n=(i , j )∈I , and H , W are for all the output 'pixels'.

Let's start with a simple example. Fig. [4](#fig_3).a depicts a standard 1D downscaling by 2. Note that even when the scale-factor s is an integer (or an inverse integer: s = 1 /2 in this case), it is wrong to map an output 'pixel' position n∈I to its input position in I by simply multiplying (or dividing) by the scalefactor s. It can be observed in Fig. [4](#fig_3).a that g n = 2n = n/s To obtain the correct mapping, let's define d out to be the distance of any output 'pixel' n from the leftmost boundary of I . d out is measured in units of output 'pixels'. The matching coordinates g n in the input I is defined to have a distance d in from the leftmost boundary of I, and is measured in units of input 'pixels'. The correct mapping rule across image scales [[12]](#b10) is d in =d out /s, since the total shape (from boundary-to-boundary) is resized, rather than discrete 'pixel' centers. Since the first 'pixel' center (in any feature map/image) is always half a 'pixel' away from the leftmost boundary, hence: d out =n+ 1 2 and d in =g n + 1 2 . This entails the well known relation used in image resizing methods [[12]](#b10):

$g n = n s + 1 2 ( 1 s -1)$. However, we need to handle also non-integer output shapes. The intrinsic output size s • in_size may not be an integer, yet images or feature-maps can only be represented by integer-sized vectors/tensors. We will usually (but not always) determine the size of the output I to be out_size = s • in_size . Fig. [4](#fig_3).b shows an example of a projected grid for resizing a 4 × 4 input feature map by scales 0.6 and 1.4, respectively. The final output size (3 × 6) is larger than the intrinsic size (2.4 × 5.6), due to the ceiling operation * . Therefore, the final grid mapping (which accurately covers all cases) is:

$g n = n s + 1 2 in_size -1 - 1 2s out_size -1(2)$Block 2 -Neighbors extraction: For each grid point g n , we now extract all its discrete nearestneighbors N [n] from I. These are all the input 'pixels' centers within the support of the continuous kernel K θ . This information is captured by a tensor of order 6 (blue tensor in Fig. [2](#fig_1)), with shape [N, 1, C in , K, H , W ] where K is the number of discrete neighbors in the kernel support. The second singleton dimension is for convenience in the next steps. We further need to extract the distances of each sub-'pixel' grid point g n (of an output 'pixel' n), to all its discrete neighbors m ∈ I. These distances D[n,m] are kept in a tensor D (shown in cyan in Fig. [2](#fig_1)), whose shape is [K, 2, H , W ].

Block 3 -Calculate weights: The weights are produced by a learnable model K θ , which is applied to the distances tensor: W = K θ D (similarly to [[17]](#b15)). A weight is assigned to connect between each output 'pixel' and all its discrete input neighbors. Therefore, the weight tensor W will have a shape of [1, C out , C in , K, H , W ] (red tensor in Fig. [2](#fig_1)). Its first singleton dimension is needed to match the size of the Neighbors tensor N . As in standard conv, each output channel C out is connected to all input channels C in through W. We use a small neural network for K θ called the "Internal-Net" (marked in green in Fig. [2](#fig_1)). It is a simple CNN sequence of 1×1 conv layers and ReLUs. This way every output 'pixel' n connects only to distances D[n] within its own set of discrete input neighbors.

Block 4 -Apply weights: The final stage executes Eq. 1 for the entire output tensor, by multiplying the Weights tensor (red) with the Neighbors tensor (blue) and summing over all neighbors and input channels:

$I = CC{I} = cin,k N ⊗ W(3)$with the following tensor shapes:

$I : [N, C out , H , W ], N : [N, 1, C in , K, H , W ], W : [1, C out , C in , K, H , W ]$
## Training and Generalization

CC is end-to-end trainable. The only trainable parameters of CC are θ, the parameters of K θ . The gradient is propagated from the CC output through the weights tensor W to them. Gradients need to also be propagated to previous layers through the CC input. This is done easily through the neighbors extraction since it is just slicing (each neighbour neuron is actually a copy of an input neuron).

The output shape is determined by the shape of the grid g. Since K θ is a fully convolutional network, it can be applied to any spatial input size both at training and inference. This means that regardless of what sizes and scales we train on, a single CC, with a single set of parameters θ is applicable to any size and scale determined at real-time.

The input to K θ is D, which through the grid g depends only on the desired output scale & shape.

Naturally, to generalize to many scales, we can sample various scales during training. However, being fully 1×1 convolutional, K θ actually maps every 2D distance vector D[n,m] to a single value (weight) W[n,m]. This means that D actually contains a huge batch of inputs. This produces an interesting advantage: in almost all cases CC generalizes from one scale to any other scale. This happens as long as the diversity of distances in a single grid is reasonable. Eq. 2 suggests that if the scale is a rational number with a small numerator, then there exists only a small set of grid coordinates, and consequently a small set of unique distances D[n,m]. For example, for s = 1 /2 and a kernel with support of 2×2, we get:

$D[n] = (-1 /2, -1 /2), (-1 /2, 1 /2), ( 1 /2, -1 /2), ( 1 /2, 1/$2) ∀n, which will not generalize to other scales/distances. In other words, generalization occurs over the distribution of distances D between the grid points and 'pixel' centers. If this distribution collapses to a small set of possibilities, then such generalization is damaged. However, training with a randomly selected float scale-factor will give a huge diversity of sub-'pixel' distances in D, hence will be able to generalize with very high probability to any other scale factor. Empirical evaluation of this property is described in the experiments section and in Fig. [7](#fig_6).

## Features & Properties

This section reviews several important and useful properties of CC. We show that introducing CClayers gives rise to new CNN capabilities, and furthermore -allows for new dynamic architectural designs determined at inference time.

A standard Conv-layer is a special case of the CC-Layer: Note that when the scale-factor s = 1 /k, k ∈ N, and k has the same parity as the kernel support size in both dimensions, g n in Eq. 2 reduces to simple grid locations with integer pixel spacing between them. Thus, all output grid points share the same set of local distances to their discrete input "Neighbors". Hence, the set of weights are shared by all output 'pixels', which is the case in standard convolution. This implies that for integer scale ratios CC reduces to standard convolution, i.e., CC is a generalization of the standard Conv-layer.

Dynamic scale-factor at inference: As mentioned, the projected-grid is purely a function of the scale-factors and output shape. It is also important to note that in case of training the CC-layer with a random float scale factor, we obtain a huge diversity of distances in D (see more details in Sec. 3.2). Hence the "Internal-Net" K θ which gets all the sub-'pixel' distances D as an input, and outputs a single kernel consistent with them all, must learn a true continuous weight function (the continuous kernel), independently of the scale factor or shape of the output. Once it has recovered the continuous kernel (continuous weights), CC can be applied at inference time to output any dynamically chosen scale or shape, simply by changing the sampling grid.

True Shift-Invariance/equivariance: It was shown [[20,](#b18)[2]](#b1) that strided conv-layers lack equivariance to shifts of the input. This mostly happens due to aliasing. The common filter size tends to be smaller than the low-pass filter size required to remove high-frequencies when downscaling by a factor of 2. CC allows more gradual downscaling, so that (with the same kernel support) the sampling frequency is higher and less aliasing occurs. For example, one can replace a single strided convolution layer with downscaling-factor s = 1/2, with a few (2-3) CC-layers, each with scale 1/2 < s < 1 (e.g., 2 CC-layers with s = 1/ √ 2, or 3 CC-layers with s = 1/ 3 √ 2).

Standard conv-layers often suffer from inherent misalignments, which are ameliorated by CClayers: Examining the accurate grid mapping in Eq. 2, one can see that there are cases in standard discrete convolutions which fail to satisfy it. Such cases occur when the size of the filter and the stride have different parities, in some dimension. The intuitive reason is that the filter center is defined by its parity: In odd-sized filters, the center of the output 'pixel' falls on an input 'pixel'-center, whereas in even-sized filter, it falls on the boundary between two input 'pixels'. As mentioned, for a scale factor 1/int, the set of weights is identical for any output position. Eq. 2 suggests that in such cases the grid locations are either all exactly on input 'pixel' centers, or all on the boundary between input 'pixels' (e.g., scale-factor of 1 2 produces locations 0.5, 2.5, 4.5 etc., whereas scale factor of 1  3 produces locations 1,4,7 etc.) This means that in case of an even scale with odd-sized filter, standard conv-layers introduce small misalignments between the input and output (I,I ). These small misalignments can accumulate to large misalignments over many layers.

Such an example is shown in Fig. [6](#fig_5). We used a simple gaussian kernel, and applied it repeatedly to an input image (white cross), using either a sequence of standard conv-layers or a sequence of CC-layers, all with stride=1 or scale=1, respectively. We chose an even-sized 4×4 kernel. It can be seen that standard conv-layer shifts the result by half a pixel per iteration, resulting in a large shift after many iterations. In contrast, CC-based convolutions remain centered, due to the accurate sub-pixel grid mapping of Eq. 2. Choosing a different padding method for the standard conv would only result in shifts to other directions; there is no way to apply a 4×4 discrete convolution that keeps the same input size and does not induce misalignments. The reason neural networks still get good results is simple: they learn a variety of shifted kernels. This misalignment phenomenon has been observed in super-resolution works [[19,](#b17)[16]](#b14). Still, the way misalignments are currently addressed is non-ideal and unprincipled, as some portion of the filter size is not used and may create artifacts.

Scale Ensembles: Here we present an entirely new capability in Deep-Learning -Dynamic Architectural Design of CNNs, at inference time, which is made possible by introducing multiple CC-layers into CNNs. As mentioned before, a trained CC-layer can generate any output scale-factor chosen at inference time. Therefore, a single trained CNN which consists of multiple CC-layers, can be applied many times at inference, each time with a different sequence of scales, as long as the final output scale & shape of the entire network remain fixed. This means that there are infinitely many ways to apply such a trained network, and still get the same final output size/scale. Moreover, each dimension (vertical or horizontal) can be scaled differently at intermediate CC-layers. This gives rise to a new type of "self-ensembles" of CNNs. We call it "Scale Ensembles". Fig. [5](#fig_4) schematically illustrates a few different scales-sequences for 3 consecutive CC-layers in the same pre-trained CNN. For example, the Green sequence applies 3 consecutive uniform scaling in both dimensions, same scaling each time; the Blue sequence scales the horizontal and vertical dimensions alternatingly; whereas the Red sequence offers yet another non-uniform sequence of scaling. At inference time we can apply the same pre-trained network several times, each with a different sequence of scales. We can then aggregate all the results of this ensemble, to get an improved result which is consistent with the entire ensemble. Aggregation can be achieved, e.g., by averaging the class logits in a classification task, or by computing the median image of the ensemble in an image-processing task.

## Efficient Implementation

This section provides major implementation details which give rise to efficient implementation of the CC-layer. Code will be made available.

No need to keep the Neighbors distances: The distances D of a grid point g n to all its discrete input Neighbors can actually be predicted by keeping only one Neighbor. The reason is simple: all  neighbors are at a sequence of positions 1 'pixel' apart from each other. It suffices to keep only the distance of a grid point to its closest 'pixel' center in order to predict all other distances. Moreover, the distance to each Neighbor is already implied by the sub-'pixel' grid coordinates. Hence, we can train the Internal-Net K θ to predict the weights directly from the grid g, thus saving on memory.

Efficient implementation using discrete convolutions: Eq. 2 suggests that special types of grids exist. We already noted that for 1/int scales, CC collapses to a standard convolution. However, Eq. 2 further suggests that for rational-numbers scale-factors (s = k / ) the grid is periodic, with a period equals the numerator k. This generalizes observations made by [[15,](#b13)[4]](#b3). We take advantage of this property by calculating the sets of weights for one period only, to avoid calculation of a huge weights tensor W. We do not need to explicitly extract neighbors and multiply them with the weights. Instead, we use a set of standard discrete convolution filters, each with a different starting shift matching a different grid position within one period. We apply them to the input feature-map and interleave their results. This allows us to use the existing, highly optimized, implementation of conv operation. This approach offers a trade-off between speed (small k) and better generalization (large k).

Saving memory by keeping only K θ : The most memory consuming tensor in CC is the weights tensor W. The second one is the Neighbors tensor N . We can save almost all the memory used by a CC-layer by simply removing these tensors at each iteration after the output is calculated. In the back-propagation step we simply recalculate them, at some extra cost of time. The memory footprint can be further reduced by chunking the input and computing the per-chunk results sequentially.

## Experiments

Visualizing the continuous learned kernels K θ : Filters of discrete conv-layers can be easily visualized. They are sets of discrete numbers on a regular grid, which can just be printed out or viewed as images. Filters of CC-layers, on the other hand, assign a unique set of numbers to any sub-'pixel' grid location. But once trained, we can easily sample K θ on a regular grid at any desired resolution for the purpose of visualization. Fig. [3](#fig_2) shows 2 such examples of CC kernel visualization. In both examples we trained a single CC-layer to imitate an image-resizing method (for which we know the ground-truth kernels): (i) Imitate bicubic resizing, and (ii) imitate resizing by a Gaussian kernel with different σ for each axis, rotated by 45 • . The CC-training was performed on a single image with a single random sampled scale s∈[0.3, .., [1.3]](#) in each dimension. The input to the CC-layer was the single image, and the "output label" was the resized image (generated by the external image-resizing method for that s). Once trained, we sampled K θ at dense 200×200 coordinate-pairs. Fig. [3](#fig_2) shows such visualizations of the continuous recovered kernel K θ , as a function of the training iteration.

Verifying scale generalization: To test generalization across scales, we sampled a single pair of random scale-factors s x ,s y ∈[0.3, ..,1.3], and applied an independent bicubic resizing to a single image for this scale. As before, we trained a CC-layer with the original image as the input, and the resized image as the "output label". To test generalization, we applied our CC-layer (which was trained for one scale), on 100 new random scales. We compared the resulting 100 output images to ground-truth bicubic resizing by those 100 scales. The left side of Fig. [7](#fig_6) shows MSE of the train-error and the generalization (test) error. The test-error for scales CC never trained for, is very close to the train-error for the single scale it trained on. This is due to the fact that CC generalizes well to other scales, as long as the training scale generates sufficient diversity in sub-'pixel' grid shifts. To further confirm this, we repeated the experiment, only this time we chose the training scale to be 1/int (1/2). Eq. 2 shows that in such case all grid locations have the same shift from 'pixel' centers (0.5). The right side of Fig. [7](#fig_6) shows that generalization was severely damaged; test-error for 100 other scales was 2-orders-of-magnitude higher. Fig. [7](#fig_6) further shows visualization of K θ for these 2 experiments (ground-truth can be found in Fig. [3](#fig_2)). In the first experiment the learned kernel is similar to the ground-truth, whereas in the second experiment it is not (since it was only required to produce correct results for a very small subset of coordinate pairs).

Verifying Shift-Equivariance: To check shift-equivariance, we used a variant of a very simple CNN-based classification net for CIFAR-10 [9] (similar to the net used in [[18,](#b16)[1]](#b0)). The network consisted of several conv-layers of strides 1,2,1,1,1,1,1,2, followed by 3 fully-connected layers, all with ReLU activations. We call this the "Baseline Net". We constructed a "CC-Net" with the same number of layers, but with CC-layers of gradually changing layer sizes (starting from the second layer). The overall scale factor from the net's input to the last conv-layer's output remained the same (1:4), and the scale-factors of all layers were approximately ( 1 /4) 1 /6 . When testing on the CIFAR-10 test-set, CC-Net had higher accuracy (87.81%) compared to the Baseline (87.14%), which is not surprising as CC-Net has more params than Baseline-Net, but was not the purpose of this experiment.

Following [[20]](#b18), we tested the shift-equivariance as follows: (i) Each input image was fed to the network twice: First the original input, and then, a shifted version of it with horizontal/vertical shifts up to a max allowed radius. (ii) We took the output of the last conv-layer. Since it is 4 times smaller, a 1-pixel shift in the input image should translate to 0.25-'pixel' shift in the output feature-map of the last layer. We therefore 'shift-back' the output feature-map by the expected shift (using cubic-interpolation), to bring it back to alignment with the original (unshifted) output. (iii) We compute the cosine similarity between the 2 output feature-maps after alignment. To avoid boundary effects, we cut off 2 boundary 'pixels' from each side. The results are shown in Table . 1. We tested it for 2 types of inputs: images & Gaussian noise. The table shows that CC-layers are much more resilient to shifts than regular conv-layers (both on natural images and on random noise).   [5](#fig_4)). This means that our trained CC-Net can be applied numerous times at inference, each time with a different sequence of scales (as long as their product yields 1/4), and average the logits of all forwarded passes. We tested this for CC-Net. Even small ensembles (2,3) already gave a significant boost in classification accuracy (+1.7%, +2.4%, respectively). While test-time augmentations for regular CNNs can be applied to the input image only, Scale-Ensembles apply test-time augmentation to all CC-layers in the network. Hence, deep nets with many CC-layers, are likely to benefit from larger self-ensembles.

## Max-shift

Potential use in image-processing tasks: CC-layers could potentially be very useful in imageprocessing tasks. For example, existing Super-Resolution (SR) networks are trained for a fixed scale-factor [[11,](#b9)[3]](#b2). To increase the resolution of an image by a factor of

3, one must use a SRx3 net which was trained for this specific scale-factor. SR by a factor of 4 will require a different pretrained SRx4 net. Constructing a SR network based on CC-layers is expected to give SR with any desired scale factor (or at least an impressive range of scale-factors), all with a single trained SR net. Moreover, it could potentially accommodate different SR scale factors for different axes (if so desired). Note that training a separate net for every potential SR scale (or any combination of vertical x horizontal SR scales) is combinatorially daunting. A CC-based SR network may alleviate this problem. Designing such a CC-based SR network is part of our future work.

![Figure 1: The underlying process of CC.]()

![Figure 2: Overview of CC-layer. (see Sections 2 and 3 for details)]()

![Figure 3: Visualizing the continuous learned kernels]()

![Figure 4: The projected grid.]()

![Figure 5: "Scale-Ensemble" (see Sec. 4 for details).]()

![Figure 6: Standard conv-layers induce misalignments. CC-layers do not.]()

![Figure 7: CC trained on one (float) scale, generalizes well to other scales.The power of dynamic Scale-Ensembles at inference: Since the above "CC-Net" was trained with non-integer (float) internal scale-factors, it can generalize to new internal scale-factors at inferencetime. This gives rise to ensemble-based image classification (Sec. 4 & Fig.5). This means that our trained CC-Net can be applied numerous times at inference, each time with a different sequence of scales (as long as their product yields 1/4), and average the logits of all forwarded passes. We tested this for CC-Net. Even small ensembles (2,3) already gave a significant boost in classification accuracy (+1.7%, +2.4%, respectively). While test-time augmentations for regular CNNs can be applied to the input image only, Scale-Ensembles apply test-time augmentation to all CC-layers in the network. Hence, deep nets with many CC-layers, are likely to benefit from larger self-ensembles.]()

![Baseline CC-netComparison of shift equivariance results, measured in cosine-similarity]()

