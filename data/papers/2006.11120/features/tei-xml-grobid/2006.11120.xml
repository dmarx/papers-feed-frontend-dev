<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Discrete to Continuous Convolution Layers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-06-19">19 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Feinstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niv</forename><surname>Haim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science and Applied Math</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Discrete to Continuous Convolution Layers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-19">19 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">88F99DFA2527D0280689A436B94F3BCA</idno>
					<idno type="arXiv">arXiv:2006.11120v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing of feature maps. This is done either by strided convolution (donwscaling) or transposed convolution (upscaling). Such operations are limited to a fixed filter moving at predetermined integer steps (strides). Spatial sizes of consecutive layers are related by integer scale factors, predetermined at architectural design, and remain fixed throughout training and inference time. We propose a generalization of the common Conv-layer, from a discrete layer to a Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by representing the filter as a learned continuous function over sub-pixel coordinates. This allows learnable and principled resizing of feature maps, to any size, dynamically and consistently across scales. Once trained, the CC layer can be used to output any scale/size chosen at inference time. The scale can be non-integer and differ between the axes. CC gives rise to new freedoms for architectural design, such as dynamic layer shapes at inference time, or gradual architectures where the size changes by a small factor at each layer. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. We further show that current Conv-layers suffer from inherent misalignments, which are ameliorated by CC layers.</p><p>* equal contribution Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The basic building block of CNNs, the convolution layer (conv-layer), applies a discrete convolution (more precisely, cross-correlation) to an input feature map, with a learned discrete filter. CNNs commonly employ spatial resizing of their feature maps, either downscaling (e.g., in classification networks) or upscaling (e.g., in generative models and image processing tasks). These resizing operations are either limited to an integer scale factor (achieved by an integer stride), or via unprincipled feature map interpolation. These resizing approaches suffer from inherent problems (elaborated below), which limit the performance and capabilities of current CNNs.</p><p>In this paper, we propose a generalization of the standard conv-layer. Given a discrete feature map I of spatial dimensions H×W , we are interested in a learned conv-layer that outputs a feature map I of H ×W , where none of the ratios of the spatial dimensions ( H H , H H , W W , W W ) need to be integer numbers. We show that this can be achieved by modeling a learned continuous convolution, which is end-to-end trainable by gradient descent. Furthermore, we show that such continuous modeling allows for the desired output size H × W to be chosen dynamically, at inference time. This gives rise to many desired CNN properties, new architectural design capabilities, and useful applications. Fig. <ref type="figure" target="#fig_0">1</ref> shows the underlying process of a CC-layer, which has two parts: (i) modulating the discrete input with a learned continuous filter to get a continuous intermediate representation; (ii) re-sampling the continuous representation according to a re-sampling grid to get the discrete output. The only learned part in a CC-layer is the continuous filter. Once trained, the CC-layer can be used to output any scale or shape chosen at inference time, simply by using a different re-sampling grid.</p><p>To date, the most common approaches for feature map resizing in CNNs are the strided-conv or pooling for downscaling, or the transposed-conv for upscaling. These suffer from inherent limitations: (i) The stride must be an integer, hence a learned transformation that includes spatial resizing can only be done by an integer (or inverse of an integer) scale-factor. (ii) Once trained, the filter and stride are fixed and cannot be modified at inference time. (iii) Strided-convolutions suffer from impaired shift-equivariance <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b1">2]</ref>. Transposed-convolutions suffer from inherent checkerboard artifacts <ref type="bibr" target="#b11">[13]</ref>.</p><p>Interpolation-based resizing methods (e.g., linear, nearest neighbor, cubic <ref type="bibr" target="#b6">[7]</ref>), which are mostly used for images, are usually differentiable and can be incorporated in a neural network to resize feature maps. Such resizing is principled; it follows a consistent rule across all scales and sub-pixel/subfeature positions. However, these are not learned resizing operations, and furthermore cannot express more general feature transforms, only scaling. We do however draw inspiration from the way these methods are applied to images <ref type="bibr" target="#b10">[12]</ref>, and show how they can be replaced by dynamic learnable models.</p><p>The most closely related work to ours is that of <ref type="bibr" target="#b15">[17]</ref>. They were the first to coin the term "Continuous Convolution", and identified its need for applying CNNs on 3D point-clouds (points scattered irregularly in 3D space; not on a regular grid). They used a method similar to ours for calculating convolution weights for points in the irregular 3D cloud. However, modeling continuous convolution for point-clouds is a clear necessity, as there is no notion of "stride" in such input data. Moreover, their continuous convolution is not meant to resize and is used for a different goal. Inspired by their work, we generalize the concept of Continuous Convolution of <ref type="bibr" target="#b15">[17]</ref> in several ways: (i) We adapt it to the realm of regular grids, where stride/scale is well-defined, yet show that it is useful also here. (ii) Our continuous convolution can be consistently applied to any choice of scale and output shape, and (iii) the scale and shape of our CC-layer output can be determined dynamically at inference time.</p><p>It was shown by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">20]</ref> that very small shifts to the input data of a CNN (e.g., shifting an input image by 1-2 pixels) can drastically change the output prediction. This lack of shift-invariance/equivariance is due to the aliasing induced by strided conv-layers with small kernel support. CC enables more "gradual-architectures" (still with small kernel support) that avoid aliasing, thus giving rise to true shift-equivariance.</p><p>The contributions of our paper are therefore several-folded:</p><p>• From Discrete to Continuous CNNs: We introduce a generalization of the standard discrete conv-layer, to a continuous CC-layer, in a principled way. • Resize by any scale: CC is the first learnable layer that can resize feature-maps by any scale (integer or non-integer; downscaling/upscaling; can also differ between dimensions). • Dynamic: The desired scale and shape of the CC output can be set at inference-time.</p><p>• True shift-equivariance: CC enables gradual-downscaling that induces less aliasing.</p><p>• Resolving input-output misalignments: We further show that conv-layers often induce small inherent misalignments, which are ameliorated by CC.</p><p>2 Overview of the approach Fig. <ref type="figure" target="#fig_0">1</ref> describes the underlying process we aim to model: given a discrete feature map I[i, j], we are interested in constructing a CC-layer that operates on I as an input, and produces a next layer I [i , j ], by modeling a learned continuous convolution between the input and output feature-maps. Note that the input and output feature-maps I and I need not be related by an integer scale factor (or its inverse). Such an action can be realized by the following steps: (i) Use a "Bed of Nails" representation of I: I cont (h, w) = i,j δ(h -i, w -j)I[i, j]. where i, j are discrete 'pixel' (feature) locations in I, and h, w are continuous coordinates in the continuous feature-map space. For simplicity, we will refer from here on to all discrete coordinates inside feature maps by the term 'pixels', and their in-between non-integer coordinates as sub-'pixel' (although we are referring to general network layers; usually not images).</p><p>(ii) Apply a convolution with a learned continuous kernel K θ (h, w).</p><p>(iii) Resample the continuous result of the convolution according to the desired shape and scale-factors of I . The resampling grid is purely a function of the shape and scales. This means that the same continuous kernel K θ is eligible for producing any desired scale-factor for I .</p><p>These phases are indicated in Fig. <ref type="figure" target="#fig_0">1</ref>, and can be mathematically formulated as follows:</p><formula xml:id="formula_0">CC{I}[n] = {I cont * K θ }(g n ) = m δ (g n -τ -m) I[m]K θ (τ )dτ = m I[m] δ (g n -τ -m) K θ (τ ) dτ = m I[m]K θ (g n -m)<label>(1)</label></formula><p>where m = (i, j), n = (i , j ) are discrete 'pixel' coordinates in I and I respectively, and g n denotes the continuous sampling locations of the grid in Fig. <ref type="figure" target="#fig_0">1</ref> (i.e., the sub-'pixel' position of each 'pixel' in I when projected onto the coordinates of I).</p><p>Eq. 1 shows that for K θ with finite support, the output of the continuous convolution at I [n] is a weighted sum of the discrete "Neighbors" of its projected location g n in the input I. The weights are a function of the distance between the discrete 'pixel' center m to the continuous sampling location g n . The desired sampling grid g n is calculated by projecting the output integer 'pixel' locations n = (i , j ) in I , to sub-'pixel' coordinates in the input I. We call it the "Projected Grid".</p><p>In order to execute our approach we need to overcome an inherent challenge: CNNs are built upon regular grids with discrete coordinates. We therefore need to model a continuous process with discrete components. Fig. <ref type="figure" target="#fig_1">2</ref> illustrates our CC-layer construction, which builds upon implementation ideas from standard image resizing techniques <ref type="bibr" target="#b10">[12]</ref>. Our CC-layer consists of 4 principled building blocks. These are marked by numbers in Fig. <ref type="figure" target="#fig_1">2</ref>, and are described in detail in Sec. 3:</p><p>1. Calculate Projected Grid g n from the desired scale-factors and output-shape. 2. Extract Neighbors N : Map each sampling point g n to its discrete neighbors N [n] within the continuous kernel support. 3. Calculate Weights W by applying K θ to distances between sampling points to their neighbors:</p><formula xml:id="formula_1">W[n, m] = K θ (g n -m</formula><p>). This "Internal-Net" K θ is the only trainable part in the CC-layer. 4. Apply Weights: Multiply N W and sum over all neighbors and input channels, to obtain the output feature map I , with any desired scale or shape (which can be determined at test time).  Here N is the batch-size, C is the number of channels. H, W are spatial dimensions. The desired spatial size H , W may be chosen at inference time. Our CC-layer consists of 4 principled building blocks, marked by numbers in Fig. <ref type="figure" target="#fig_1">2</ref> (for simplicity, I and I are drawn as 2D vectors in Fig. <ref type="figure" target="#fig_1">2</ref>, i.e., 4D tensors with N = 1 and C = 1). These are described next:</p><p>Block 1 -Projected-grid: The Projected grid g matches each output 'pixel' n=(i , j )∈I to a sub-'pixel' location g n in the continuous space of the input I. This grid is captured by a tensor of shape [2, H , W ] where the the first dimension are the 2 projected sub-'pixel' coordinates (vertical and horizontal) of each output 'pixel' n=(i , j )∈I , and H , W are for all the output 'pixels'.</p><p>Let's start with a simple example. Fig. <ref type="figure" target="#fig_3">4</ref>.a depicts a standard 1D downscaling by 2. Note that even when the scale-factor s is an integer (or an inverse integer: s = 1 /2 in this case), it is wrong to map an output 'pixel' position n∈I to its input position in I by simply multiplying (or dividing) by the scalefactor s. It can be observed in Fig. <ref type="figure" target="#fig_3">4</ref>.a that g n = 2n = n/s To obtain the correct mapping, let's define d out to be the distance of any output 'pixel' n from the leftmost boundary of I . d out is measured in units of output 'pixels'. The matching coordinates g n in the input I is defined to have a distance d in from the leftmost boundary of I, and is measured in units of input 'pixels'. The correct mapping rule across image scales <ref type="bibr" target="#b10">[12]</ref> is d in =d out /s, since the total shape (from boundary-to-boundary) is resized, rather than discrete 'pixel' centers. Since the first 'pixel' center (in any feature map/image) is always half a 'pixel' away from the leftmost boundary, hence: d out =n+ 1 2 and d in =g n + 1 2 . This entails the well known relation used in image resizing methods <ref type="bibr" target="#b10">[12]</ref>:</p><formula xml:id="formula_2">g n = n s + 1 2 ( 1 s -1)</formula><p>. However, we need to handle also non-integer output shapes. The intrinsic output size s • in_size may not be an integer, yet images or feature-maps can only be represented by integer-sized vectors/tensors. We will usually (but not always) determine the size of the output I to be out_size = s • in_size . Fig. <ref type="figure" target="#fig_3">4</ref>.b shows an example of a projected grid for resizing a 4 × 4 input feature map by scales 0.6 and 1.4, respectively. The final output size (3 × 6) is larger than the intrinsic size (2.4 × 5.6), due to the ceiling operation * . Therefore, the final grid mapping (which accurately covers all cases) is:</p><formula xml:id="formula_3">g n = n s + 1 2 in_size -1 - 1 2s out_size -1<label>(2)</label></formula><p>Block 2 -Neighbors extraction: For each grid point g n , we now extract all its discrete nearestneighbors N [n] from I. These are all the input 'pixels' centers within the support of the continuous kernel K θ . This information is captured by a tensor of order 6 (blue tensor in Fig. <ref type="figure" target="#fig_1">2</ref>), with shape [N, 1, C in , K, H , W ] where K is the number of discrete neighbors in the kernel support. The second singleton dimension is for convenience in the next steps. We further need to extract the distances of each sub-'pixel' grid point g n (of an output 'pixel' n), to all its discrete neighbors m ∈ I. These distances D[n,m] are kept in a tensor D (shown in cyan in Fig. <ref type="figure" target="#fig_1">2</ref>), whose shape is [K, 2, H , W ].</p><p>Block 3 -Calculate weights: The weights are produced by a learnable model K θ , which is applied to the distances tensor: W = K θ D (similarly to <ref type="bibr" target="#b15">[17]</ref>). A weight is assigned to connect between each output 'pixel' and all its discrete input neighbors. Therefore, the weight tensor W will have a shape of [1, C out , C in , K, H , W ] (red tensor in Fig. <ref type="figure" target="#fig_1">2</ref>). Its first singleton dimension is needed to match the size of the Neighbors tensor N . As in standard conv, each output channel C out is connected to all input channels C in through W. We use a small neural network for K θ called the "Internal-Net" (marked in green in Fig. <ref type="figure" target="#fig_1">2</ref>). It is a simple CNN sequence of 1×1 conv layers and ReLUs. This way every output 'pixel' n connects only to distances D[n] within its own set of discrete input neighbors.</p><p>Block 4 -Apply weights: The final stage executes Eq. 1 for the entire output tensor, by multiplying the Weights tensor (red) with the Neighbors tensor (blue) and summing over all neighbors and input channels:</p><formula xml:id="formula_4">I = CC{I} = cin,k N ⊗ W<label>(3)</label></formula><p>with the following tensor shapes:</p><formula xml:id="formula_5">I : [N, C out , H , W ], N : [N, 1, C in , K, H , W ], W : [1, C out , C in , K, H , W ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Generalization</head><p>CC is end-to-end trainable. The only trainable parameters of CC are θ, the parameters of K θ . The gradient is propagated from the CC output through the weights tensor W to them. Gradients need to also be propagated to previous layers through the CC input. This is done easily through the neighbors extraction since it is just slicing (each neighbour neuron is actually a copy of an input neuron).</p><p>The output shape is determined by the shape of the grid g. Since K θ is a fully convolutional network, it can be applied to any spatial input size both at training and inference. This means that regardless of what sizes and scales we train on, a single CC, with a single set of parameters θ is applicable to any size and scale determined at real-time.</p><p>The input to K θ is D, which through the grid g depends only on the desired output scale &amp; shape.</p><p>Naturally, to generalize to many scales, we can sample various scales during training. However, being fully 1×1 convolutional, K θ actually maps every 2D distance vector D[n,m] to a single value (weight) W[n,m]. This means that D actually contains a huge batch of inputs. This produces an interesting advantage: in almost all cases CC generalizes from one scale to any other scale. This happens as long as the diversity of distances in a single grid is reasonable. Eq. 2 suggests that if the scale is a rational number with a small numerator, then there exists only a small set of grid coordinates, and consequently a small set of unique distances D[n,m]. For example, for s = 1 /2 and a kernel with support of 2×2, we get:</p><formula xml:id="formula_6">D[n] = (-1 /2, -1 /2), (-1 /2, 1 /2), ( 1 /2, -1 /2), ( 1 /2, 1<label>/</label></formula><p>2) ∀n, which will not generalize to other scales/distances. In other words, generalization occurs over the distribution of distances D between the grid points and 'pixel' centers. If this distribution collapses to a small set of possibilities, then such generalization is damaged. However, training with a randomly selected float scale-factor will give a huge diversity of sub-'pixel' distances in D, hence will be able to generalize with very high probability to any other scale factor. Empirical evaluation of this property is described in the experiments section and in Fig. <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features &amp; Properties</head><p>This section reviews several important and useful properties of CC. We show that introducing CClayers gives rise to new CNN capabilities, and furthermore -allows for new dynamic architectural designs determined at inference time.</p><p>A standard Conv-layer is a special case of the CC-Layer: Note that when the scale-factor s = 1 /k, k ∈ N, and k has the same parity as the kernel support size in both dimensions, g n in Eq. 2 reduces to simple grid locations with integer pixel spacing between them. Thus, all output grid points share the same set of local distances to their discrete input "Neighbors". Hence, the set of weights are shared by all output 'pixels', which is the case in standard convolution. This implies that for integer scale ratios CC reduces to standard convolution, i.e., CC is a generalization of the standard Conv-layer.</p><p>Dynamic scale-factor at inference: As mentioned, the projected-grid is purely a function of the scale-factors and output shape. It is also important to note that in case of training the CC-layer with a random float scale factor, we obtain a huge diversity of distances in D (see more details in Sec. 3.2). Hence the "Internal-Net" K θ which gets all the sub-'pixel' distances D as an input, and outputs a single kernel consistent with them all, must learn a true continuous weight function (the continuous kernel), independently of the scale factor or shape of the output. Once it has recovered the continuous kernel (continuous weights), CC can be applied at inference time to output any dynamically chosen scale or shape, simply by changing the sampling grid.</p><p>True Shift-Invariance/equivariance: It was shown <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b1">2]</ref> that strided conv-layers lack equivariance to shifts of the input. This mostly happens due to aliasing. The common filter size tends to be smaller than the low-pass filter size required to remove high-frequencies when downscaling by a factor of 2. CC allows more gradual downscaling, so that (with the same kernel support) the sampling frequency is higher and less aliasing occurs. For example, one can replace a single strided convolution layer with downscaling-factor s = 1/2, with a few (2-3) CC-layers, each with scale 1/2 &lt; s &lt; 1 (e.g., 2 CC-layers with s = 1/ √ 2, or 3 CC-layers with s = 1/ 3 √ 2).</p><p>Standard conv-layers often suffer from inherent misalignments, which are ameliorated by CClayers: Examining the accurate grid mapping in Eq. 2, one can see that there are cases in standard discrete convolutions which fail to satisfy it. Such cases occur when the size of the filter and the stride have different parities, in some dimension. The intuitive reason is that the filter center is defined by its parity: In odd-sized filters, the center of the output 'pixel' falls on an input 'pixel'-center, whereas in even-sized filter, it falls on the boundary between two input 'pixels'. As mentioned, for a scale factor 1/int, the set of weights is identical for any output position. Eq. 2 suggests that in such cases the grid locations are either all exactly on input 'pixel' centers, or all on the boundary between input 'pixels' (e.g., scale-factor of 1 2 produces locations 0.5, 2.5, 4.5 etc., whereas scale factor of 1  3 produces locations 1,4,7 etc.) This means that in case of an even scale with odd-sized filter, standard conv-layers introduce small misalignments between the input and output (I,I ). These small misalignments can accumulate to large misalignments over many layers.</p><p>Such an example is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. We used a simple gaussian kernel, and applied it repeatedly to an input image (white cross), using either a sequence of standard conv-layers or a sequence of CC-layers, all with stride=1 or scale=1, respectively. We chose an even-sized 4×4 kernel. It can be seen that standard conv-layer shifts the result by half a pixel per iteration, resulting in a large shift after many iterations. In contrast, CC-based convolutions remain centered, due to the accurate sub-pixel grid mapping of Eq. 2. Choosing a different padding method for the standard conv would only result in shifts to other directions; there is no way to apply a 4×4 discrete convolution that keeps the same input size and does not induce misalignments. The reason neural networks still get good results is simple: they learn a variety of shifted kernels. This misalignment phenomenon has been observed in super-resolution works <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b14">16]</ref>. Still, the way misalignments are currently addressed is non-ideal and unprincipled, as some portion of the filter size is not used and may create artifacts.</p><p>Scale Ensembles: Here we present an entirely new capability in Deep-Learning -Dynamic Architectural Design of CNNs, at inference time, which is made possible by introducing multiple CC-layers into CNNs. As mentioned before, a trained CC-layer can generate any output scale-factor chosen at inference time. Therefore, a single trained CNN which consists of multiple CC-layers, can be applied many times at inference, each time with a different sequence of scales, as long as the final output scale &amp; shape of the entire network remain fixed. This means that there are infinitely many ways to apply such a trained network, and still get the same final output size/scale. Moreover, each dimension (vertical or horizontal) can be scaled differently at intermediate CC-layers. This gives rise to a new type of "self-ensembles" of CNNs. We call it "Scale Ensembles". Fig. <ref type="figure" target="#fig_4">5</ref> schematically illustrates a few different scales-sequences for 3 consecutive CC-layers in the same pre-trained CNN. For example, the Green sequence applies 3 consecutive uniform scaling in both dimensions, same scaling each time; the Blue sequence scales the horizontal and vertical dimensions alternatingly; whereas the Red sequence offers yet another non-uniform sequence of scaling. At inference time we can apply the same pre-trained network several times, each with a different sequence of scales. We can then aggregate all the results of this ensemble, to get an improved result which is consistent with the entire ensemble. Aggregation can be achieved, e.g., by averaging the class logits in a classification task, or by computing the median image of the ensemble in an image-processing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Efficient Implementation</head><p>This section provides major implementation details which give rise to efficient implementation of the CC-layer. Code will be made available.</p><p>No need to keep the Neighbors distances: The distances D of a grid point g n to all its discrete input Neighbors can actually be predicted by keeping only one Neighbor. The reason is simple: all  neighbors are at a sequence of positions 1 'pixel' apart from each other. It suffices to keep only the distance of a grid point to its closest 'pixel' center in order to predict all other distances. Moreover, the distance to each Neighbor is already implied by the sub-'pixel' grid coordinates. Hence, we can train the Internal-Net K θ to predict the weights directly from the grid g, thus saving on memory.</p><p>Efficient implementation using discrete convolutions: Eq. 2 suggests that special types of grids exist. We already noted that for 1/int scales, CC collapses to a standard convolution. However, Eq. 2 further suggests that for rational-numbers scale-factors (s = k / ) the grid is periodic, with a period equals the numerator k. This generalizes observations made by <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b3">4]</ref>. We take advantage of this property by calculating the sets of weights for one period only, to avoid calculation of a huge weights tensor W. We do not need to explicitly extract neighbors and multiply them with the weights. Instead, we use a set of standard discrete convolution filters, each with a different starting shift matching a different grid position within one period. We apply them to the input feature-map and interleave their results. This allows us to use the existing, highly optimized, implementation of conv operation. This approach offers a trade-off between speed (small k) and better generalization (large k).</p><p>Saving memory by keeping only K θ : The most memory consuming tensor in CC is the weights tensor W. The second one is the Neighbors tensor N . We can save almost all the memory used by a CC-layer by simply removing these tensors at each iteration after the output is calculated. In the back-propagation step we simply recalculate them, at some extra cost of time. The memory footprint can be further reduced by chunking the input and computing the per-chunk results sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Visualizing the continuous learned kernels K θ : Filters of discrete conv-layers can be easily visualized. They are sets of discrete numbers on a regular grid, which can just be printed out or viewed as images. Filters of CC-layers, on the other hand, assign a unique set of numbers to any sub-'pixel' grid location. But once trained, we can easily sample K θ on a regular grid at any desired resolution for the purpose of visualization. Fig. <ref type="figure" target="#fig_2">3</ref> shows 2 such examples of CC kernel visualization. In both examples we trained a single CC-layer to imitate an image-resizing method (for which we know the ground-truth kernels): (i) Imitate bicubic resizing, and (ii) imitate resizing by a Gaussian kernel with different σ for each axis, rotated by 45 • . The CC-training was performed on a single image with a single random sampled scale s∈[0.3, .., <ref type="bibr">1.3]</ref> in each dimension. The input to the CC-layer was the single image, and the "output label" was the resized image (generated by the external image-resizing method for that s). Once trained, we sampled K θ at dense 200×200 coordinate-pairs. Fig. <ref type="figure" target="#fig_2">3</ref> shows such visualizations of the continuous recovered kernel K θ , as a function of the training iteration.</p><p>Verifying scale generalization: To test generalization across scales, we sampled a single pair of random scale-factors s x ,s y ∈[0.3, ..,1.3], and applied an independent bicubic resizing to a single image for this scale. As before, we trained a CC-layer with the original image as the input, and the resized image as the "output label". To test generalization, we applied our CC-layer (which was trained for one scale), on 100 new random scales. We compared the resulting 100 output images to ground-truth bicubic resizing by those 100 scales. The left side of Fig. <ref type="figure" target="#fig_6">7</ref> shows MSE of the train-error and the generalization (test) error. The test-error for scales CC never trained for, is very close to the train-error for the single scale it trained on. This is due to the fact that CC generalizes well to other scales, as long as the training scale generates sufficient diversity in sub-'pixel' grid shifts. To further confirm this, we repeated the experiment, only this time we chose the training scale to be 1/int (1/2). Eq. 2 shows that in such case all grid locations have the same shift from 'pixel' centers (0.5). The right side of Fig. <ref type="figure" target="#fig_6">7</ref> shows that generalization was severely damaged; test-error for 100 other scales was 2-orders-of-magnitude higher. Fig. <ref type="figure" target="#fig_6">7</ref> further shows visualization of K θ for these 2 experiments (ground-truth can be found in Fig. <ref type="figure" target="#fig_2">3</ref>). In the first experiment the learned kernel is similar to the ground-truth, whereas in the second experiment it is not (since it was only required to produce correct results for a very small subset of coordinate pairs).</p><p>Verifying Shift-Equivariance: To check shift-equivariance, we used a variant of a very simple CNN-based classification net for CIFAR-10 [9] (similar to the net used in <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b0">1]</ref>). The network consisted of several conv-layers of strides 1,2,1,1,1,1,1,2, followed by 3 fully-connected layers, all with ReLU activations. We call this the "Baseline Net". We constructed a "CC-Net" with the same number of layers, but with CC-layers of gradually changing layer sizes (starting from the second layer). The overall scale factor from the net's input to the last conv-layer's output remained the same (1:4), and the scale-factors of all layers were approximately ( 1 /4) 1 /6 . When testing on the CIFAR-10 test-set, CC-Net had higher accuracy (87.81%) compared to the Baseline (87.14%), which is not surprising as CC-Net has more params than Baseline-Net, but was not the purpose of this experiment.</p><p>Following <ref type="bibr" target="#b18">[20]</ref>, we tested the shift-equivariance as follows: (i) Each input image was fed to the network twice: First the original input, and then, a shifted version of it with horizontal/vertical shifts up to a max allowed radius. (ii) We took the output of the last conv-layer. Since it is 4 times smaller, a 1-pixel shift in the input image should translate to 0.25-'pixel' shift in the output feature-map of the last layer. We therefore 'shift-back' the output feature-map by the expected shift (using cubic-interpolation), to bring it back to alignment with the original (unshifted) output. (iii) We compute the cosine similarity between the 2 output feature-maps after alignment. To avoid boundary effects, we cut off 2 boundary 'pixels' from each side. The results are shown in Table . 1. We tested it for 2 types of inputs: images &amp; Gaussian noise. The table shows that CC-layers are much more resilient to shifts than regular conv-layers (both on natural images and on random noise).   <ref type="figure" target="#fig_4">5</ref>). This means that our trained CC-Net can be applied numerous times at inference, each time with a different sequence of scales (as long as their product yields 1/4), and average the logits of all forwarded passes. We tested this for CC-Net. Even small ensembles (2,3) already gave a significant boost in classification accuracy (+1.7%, +2.4%, respectively). While test-time augmentations for regular CNNs can be applied to the input image only, Scale-Ensembles apply test-time augmentation to all CC-layers in the network. Hence, deep nets with many CC-layers, are likely to benefit from larger self-ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max-shift</head><p>Potential use in image-processing tasks: CC-layers could potentially be very useful in imageprocessing tasks. For example, existing Super-Resolution (SR) networks are trained for a fixed scale-factor <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b2">3]</ref>. To increase the resolution of an image by a factor of</p><p>3, one must use a SRx3 net which was trained for this specific scale-factor. SR by a factor of 4 will require a different pretrained SRx4 net. Constructing a SR network based on CC-layers is expected to give SR with any desired scale factor (or at least an impressive range of scale-factors), all with a single trained SR net. Moreover, it could potentially accommodate different SR scale factors for different axes (if so desired). Note that training a separate net for every potential SR scale (or any combination of vertical x horizontal SR scales) is combinatorially daunting. A CC-based SR network may alleviate this problem. Designing such a CC-based SR network is part of our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The underlying process of CC.</figDesc><graphic coords="2,95.72,72.00,415.81,112.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of CC-layer. (see Sections 2 and 3 for details)</figDesc><graphic coords="3,127.80,43.65,356.39,185.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualizing the continuous learned kernels</figDesc><graphic coords="4,285.77,72.00,237.60,103.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The projected grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: "Scale-Ensemble" (see Sec. 4 for details).</figDesc><graphic coords="7,126.15,75.42,178.19,76.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Standard conv-layers induce misalignments. CC-layers do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CC trained on one (float) scale, generalizes well to other scales.The power of dynamic Scale-Ensembles at inference: Since the above "CC-Net" was trained with non-integer (float) internal scale-factors, it can generalize to new internal scale-factors at inferencetime. This gives rise to ensemble-based image classification (Sec. 4 &amp; Fig.5). This means that our trained CC-Net can be applied numerous times at inference, each time with a different sequence of scales (as long as their product yields 1/4), and average the logits of all forwarded passes. We tested this for CC-Net. Even small ensembles (2,3) already gave a significant boost in classification accuracy (+1.7%, +2.4%, respectively). While test-time augmentations for regular CNNs can be applied to the input image only, Scale-Ensembles apply test-time augmentation to all CC-layers in the network. Hence, deep nets with many CC-layers, are likely to benefit from larger self-ensembles.</figDesc><graphic coords="8,353.93,320.83,152.46,94.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Baseline CC-netComparison of shift equivariance results, measured in cosine-similarity</figDesc><table><row><cell></cell><cell>±2</cell><cell>0.90</cell><cell>0.92</cell></row><row><cell>CIFAR-10</cell><cell>±4</cell><cell>0.78</cell><cell>0.85</cell></row><row><cell></cell><cell>±6</cell><cell>0.65</cell><cell>0.77</cell></row><row><cell></cell><cell>±2</cell><cell>0.80</cell><cell>0.91</cell></row><row><cell>Noise</cell><cell>±4</cell><cell>0.67</cell><cell>0.85</cell></row><row><cell></cell><cell>±6</cell><cell>0.56</cell><cell>0.85</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Details of Experiments and Architectures</head><p>A.1 Internal Net K θ Table . 2 below shows the architecture of the internal net K θ used for all experiments. The properties of each layer is denoted by CONV [Channels], [Kernel_height] × [Kernel_width] + <ref type="bibr">[Stride]</ref>. Each Conv layer of K θ except the last one, is followed by LeakyReLU activation. Each CC layer contains its own INTERNALNET: R 2 → R (the learned continuous kernel). The input to INTERNALNET are distances from a specific pixel position in the output, given as a 4D tensor, and computed using 3 consecutive 2D 1×1 convolutions, as detailed in the table .</p><p>Block 2 in Sec. 3.1 describes the input to K θ -distances tensor of size [K, 2, H , W ] where K is the number of neighbors within the support. When applying 2D conv layers to it, the first dimension of the tensor is regarded as the batch dimension. We do so intentionally, as we want to apply the same operation to all neighbor distances in parallel. The final calculated weight tensor is of size</p><p>where C in , C out are the numbers of input channels and output channels, respectively.</p><p>By default, the number of channels of the final internal conv layer, C f inal needs to cover both in and out external channels of the CC layer. C f inal = C in × C out , then the calculated weights tensor can be obtained by simple reshaping of the Internal Net's output:</p><p>In the equivariance experiment we used the efficient implementation mentioned in Sec. 5: "No need to keep the Neighbors distances". In this case, the input is just the grid [1,2,H',W'] so the batch size is 1 and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Shift Equivariance Experiments</head><p>All experiments were conducted on CIFAR10 dataset <ref type="bibr" target="#b8">[9]</ref> and implemented using PYTORCH <ref type="bibr" target="#b12">[14]</ref> deep learning framework.</p><p>The architectures we used for the Classification Networks in Sec. 5 are detailed in Table <ref type="table">3</ref>. CC corresponds to a continuous convolution layer with similar terminology (except that CC layers do not have a fixed stride). FC n corresponds to a fully connected layer with n outputs. Each CONV/FC layer is followed by a ReLU activation except for the last fully connected layer. As can be seen, CCNET and CONVNET share the same general architecture. The models are based on similar models from <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b0">1]</ref>, where we added a few more convolution layers to exemplify the gradual change in scale.</p><p>Initialization The outputs of the INTERNALNET are equivalent to the weights of an ordinary CONV layer. At initialization, we require those outputs to have a variance similar to an ordinary CONV layer's weights at initialization (either Xavier <ref type="bibr" target="#b4">[5]</ref>, Kaiming <ref type="bibr" target="#b5">[6]</ref> or the default PYTORCH <ref type="bibr" target="#b12">[14]</ref> initializations). This is done by initializing the biases of the last layer of the INTERNALNET using a normal distribution with the required variance.</p><p>Training Hyperparameters All models were trained using a batch size of 64 of images obtained by padding the original CIFAR10 images by 8 rows/cols (4 from each side) and taking a random crop of 32 × 32. We also use random horizontal-flip and normalize the inputs to have zero mean and unit variance (as done with common models for training on CIFAR10 [10]). We used ADAM optimizer <ref type="bibr" target="#b7">[8]</ref> with weight-decay of 0.0005. For CCNET we use learning rate of 0.0005 with a reduction by 10 on epochs <ref type="bibr">[200,</ref><ref type="bibr">300]</ref>. For the baseline CONVNET model we checked learning rates 0.0001, 0.0005, 0.001, 0.002 with several learning rate scheduling (reducing by 10) at: Training and Inference with scale-augmentations As mentioned, CC layers can be given the scale and output shape dynamically during forward pass. In order to train a CCNET model with scale-augmentations we use the following design: the first CC layer has a fixed scale of 1 and output shape equal to input shape. We are left with 7 CC layers to reduce spatial dimension by 1/4, that is, we want the product of all 7 CC layers' scales to equal to 1/4. The CC layers' scales are then sampled from a normal distribution of mean (1/4) (1/7) and standard deviation of 0.01. All scales are then projected to the nearest rational fraction with a maximal denominator of 10. At this point we have 7 scales {s i } 7 i=1 , however Πs i does not necessarily equal to our target scale 1/4. To this end we uniformly at random choose one layer j and set s j = (1/4)/Π i =j s i . At inference time we sample k scales using the same methodology described here. The output shapes are determined by the following:</p><p>A few examples for scale augmentations are shown in Table <ref type="table">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmentation Example 1 Augmentation Example 2 Layer Scale [H,W]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Runtime and Memory Analysis B.1 CC Layers</head><p>We measured the runtime and memory consumption of CC layers, with varying input size and number of consecutive layers. We show that our efficient implementation techniques (introduced in Sec. 5) significantly reduce the memory footprint of a CC-layer, and some techniques (implementation using discrete convolutions) further speed up the layer. For reference, we also compare to a Conv-layer, although it lacks the expressiveness of the CC-layer. In the column "CC (clear memory)" (see Tables <ref type="table">5</ref> and <ref type="table">6</ref>), we divided the computation into chunks of size 32 × 32, computed the results sequentially and discarded all intermediate results (see "Saving memory by keeping only K θ " in Sec. 5). This implementation supports non-rational scales, and allows us to use deeper networks than the non-optimized implementation ("CC (standard)" column). In the column "CC (by discrete conv)", the number of computed filters is reduced from the output size to the numerator of the scale factorin our case the scale factor is 2 /3 so we have 2 filters. This mechanism is further optimized by using the convolution operation as backend (see "Efficient implementation using discrete convolutions" in Sec. 5). This implementation significantly reduces memory consumption and runtime, at the cost of expressiveness -as it only supports rational scale factors (and is beneficial as long as the numerator is small).</p><p>In terms of runtime, we measured the time of a single forward and backward pass through the layer (or layers). To accurately measure the time, we run 11 repeats and excluded the first (warm-up), and report the average of the last 10 (the deviation was negligible). In terms of memory consumption, we report the peak allocated memory in GiB (2 30 bytes). Sometimes there is a difference between the peak reserved memory and the peak allocated memory, but that's more related to the internals of PYTORCH <ref type="bibr" target="#b12">[14]</ref>. All the tests were conducted on a single Tesla V100-PCIE-16GB GPU.</p><p>Table . 5 shows the runtime and memory consumption of a single layer with varying input size. In all the experiments we used a 3 × 3 kernel with 32 input channels and 32 output channels, and a batch size of 50. For the CC-layer, we used inner architecture as in Table . 2 with scale of 2 /3. For the Conv-layer, we used stride of 1. difference is mainly attributed to the fact that CC-layer has a constant overhead of computing the weights, which is independent of the spatial size (when using conv implementation) and thus more significant when the input is small. Also, the feature maps in the CCNET are always (spatially) larger than in BASELINE CONVNET due to the gradual downscaling. Finally, some scale factors (which were randomly chosen at each iteration) has large numerator which reduces the speed of the conv-based implementation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controlling neural level sets</title>
		<author>
			<persName><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2032" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Why do deep convolutional networks generalize so poorly to small image transformations</title>
		<author>
			<persName><forename type="first">Aharon</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image and video upscaling from local self-examples</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raanan</forename><surname>Fattal</surname></persName>
		</author>
		<idno>TOG) 30.2</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CIFAR-10 (Canadian Institute for Advanced Research)</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/~kriz/cifar.html" />
		<imprint/>
	</monogr>
	<note>In: (</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m">MATLAB. version 7.10.0 (R2010a)</title>
		<meeting><address><addrLine>Natick, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>The MathWorks Inc</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deconvolution and Checkerboard Artifacts</title>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RAISR: Rapid and accurate image super resolution</title>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Isidoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="125" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zero-Shot Super-Resolution using Deep Internal Learning</title>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling provable adversarial defenses</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8400" to="8409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep Unfolding Network for Image Super-Resolution</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10428</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Making Convolutional Networks Shift-Invariant Again</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
