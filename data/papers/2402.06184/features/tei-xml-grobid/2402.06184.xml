<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The boundary of neural network trainability is fractal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
							<email>jascha.sohldickstein@gmail.com</email>
						</author>
						<title level="a" type="main">The boundary of neural network trainability is fractal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D72434E198950D35BA69525A24A3DD8E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Some fractals -for instance those associated with the Mandelbrot and quadratic Julia sets -are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded <ref type="bibr" target="#b6">[7]</ref>. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A common way to generate fractals is to iterate a fixed function f (•) repeatedly, and to keep only the set of points for which small perturbations to hyperparameters <ref type="foot" target="#foot_0">1</ref> of that function lead to dramatic changes in the sequence of iterated values. These points can be thought of as defining a boundary in hyperparameter space along which dynamics bifurcate. For instance, on one side of the boundary function iterations may converge or remain bounded, while on the other side of the boundary they may diverge towards infinity. <ref type="foot" target="#foot_1">2</ref> If the hyperparameters are the initial conditions of the iterated function, this bifurcation boundary is known as the Julia set <ref type="bibr" target="#b4">[5]</ref>.</p><p>As an example, consider iterating the complex valued function f (z; c) = z 2 + c: the Mandelbrot fractal <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> is defined by the bifurcation boundary between values of the hyperparameter c for which this iterated function diverges or remains bounded (for an initial z value of 0); while quadratic Julia sets are defined by the bifurcation boundary between initial z values for which this iterated functions diverges or remains bounded (for fixed c).</p><p>When we train a neural network, we iterate a function (a gradient descent step) of many variables (the parameters of the neural network). For instance, if we perform full batch steepest descent with a fixed learning rate η, we update the parameters W by iterating the function f (W ; η) = W -η g(W ), where g(W ) is the gradient of the training loss. Iterated steps of gradient descent are known to exhibit bifurcation boundaries, between hyperparameters that lead to converging or diverging training runs. The final loss value achieved when training a neural network has also been shown to have a chaotic dependence on hyperparameters <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Motivated by these similarities between fractal generation and neural network training, in this paper I visualize the bifurcation boundary between hyperparameters which lead to successful and unsuccessful training of neural networks. I find that this boundary is fractal in all experimental conditions, including full batch training with tanh and ReLU nonlinearities, training a deep linear network, minibatch training, training on a dataset of size 1, and visualizing training success for different subsets of hyperparameters. II. EXPERIMENTS A. Network and data I train a one hidden layer network with inputs x ∈ R n and parameters W 0</p><formula xml:id="formula_0">∈ R n×n , W 1 ∈ R 1×n on an mse loss, ŷ(x; W 0 , W 1 ) = α 1 W 1 σ (α 0 W 0 x)<label>(1)</label></formula><formula xml:id="formula_1">ℓ (W 0 , W 1 ) = 1 |D| {x,y}∈D (y -ŷ(x; W 0 , W 1 )) 2 .<label>(2)</label></formula><p>Weights, datapoints, and labels are all randomly initialized from a standard normal distribution, N (0, 1). The function σ(•) : R → R is applied pointwise. The scaling factors α 0 and α 1 are chosen to correspond to the mean field neural network parameterization <ref type="bibr" target="#b8">[9]</ref>: </p><formula xml:id="formula_2">α 1 = 1 n ; α 0 = 2 n when σ(•) is tanh</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training</head><p>The input and output weights W 0 and W 1 are trained with learning rates η 0 and η 1 respectively. Training consists of 500 (sometimes 1000) iterations of full batch steepest gradient descent. Training is performed for a 2d grid of η 0 and η 1 hyperparameter values, with all other hyperparameters held fixed (including network initialization and training data). Training was performed in float64. Some of these design choices are modified for individual experiments, as stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization and analysis</head><p>Training runs that diverge are shown in shades of red. Training runs that converge are shown in shades of blue. For converging runs, color intensity is proportional to t ℓ t (•), where ℓ t is the loss at training step t. The more intense the blue color, the longer the training run spent with higher loss. For diverging runs, color intensity is proportional to t ℓ -1 t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2402.06184v1 [cs.LG] 9 Feb 2024</head><p>Deep linear full batch (fractal dim 1.17)</p><p>1.90010080e+2 1.90011840e+2 Input layer learning rate 4.77745244e+0 4.77749670e+0 Output layer learning rate ReLU full batch (fractal dim 1.20) 5.77622737e-1 5.77625590e-1 Input layer learning rate 8.14432439e+1 8.14436463e+1 Output layer learning rate tanh dataset set size 1 (fractal dim 1.41) 4.8298735e+1 4.8299146e+1 Input layer learning rate 1.82066141e+1 1.82067690e+1 Output layer learning rate tanh minibatch (fractal dim 1.55)</p><p>2.67398835471957e-1 2.67398835475071e-1 Input layer learning rate 1.74269509700711e+2 1.74269509702741e+2 Output layer learning rate tanh full batch (fractal dim 1.66) 1.329976231e+1 1.329978604e+1 Input layer learning rate The more intense the red, the longer the training run spent with lower loss. The color scale is adapted to each image.</p><p>For each zoom sequence, a series of roughly 50 images of size 4096 × 4096 pixels are generated, each of which increases the zoom factor by a factor of two. The zoom animation interpolates between these images.</p><p>The estimated fractal dimension in Figure <ref type="figure" target="#fig_1">1</ref> is the median of the estimated fractal dimension for all ∼50 images in the zoom sequence. Estimation is performed using the boxcount method of PoreSpy <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental conditions</head><p>To explore how general fractal behavior is, I performed experiments in six conditions:</p><p>1) Baseline condition: tanh nonlinearity, full batch gradient descent, grid search over η 0 and η 1 learning rate hyperparameters. 2) ReLU nonlinearity. 3) Identity nonlinearity (i.e., a deep linear network). 4) Minibatch gradient descent, with minibatch size 16. 5) Only a single training datapoint, |D| = 1. 6) A grid search over a different pair of hyperparameters, one of which specified the mean value used during parameter initialization, and the other of which specified the learning rate used for both parameters. Unspecified design choices are the same as in the baseline condition, and as described earlier in Section II. Representative images from all experimental conditions are shown in Figure <ref type="figure" target="#fig_1">1</ref>, sorted by fractal dimension, and with links to the corresponding animations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Elaborate functions in high dimensional spaces</head><p>Most popular fractals defined by bifurcation boundaries iterate only a simple one-dimensional function, consisting of a low degree polynomial or ratio of polynomials <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. The resulting fractals are typically perceived as possessing a lot of both repeated geometric structure and symmetry (e.g. consider the presence of 'mini-Mandelbrot' sets deep within the Mandelbrot set).</p><p>In contrast, neural network training involves iterating a complicated function, with many random terms stemming from weight initialization and training data, acting in a high dimensional space (i.e. the function acts on the parameter space of the neural network being trained). The resulting fractals seem visually more organic, with less repeated structure and symmetry. It will be a fascinating to further explore how properties of fractals depend on properties of the generating function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Non-homogeneity of boundary</head><p>It will similarly be fascinating to explore how properties of the bifurcation boundary vary for a single generating function, in different regions of hyperparameter space. I decided which regions of the hyperparameter landscape to explore by hand in an ad hoc way, and the resulting images are inevitably biased. A limiting example to consider is when the learning rate η 0 for the input layer is made very small, so that only the readout layer trains. Training only the readout layer corresponds to linear regression on an mse loss, with dynamics that are known in closed form <ref type="bibr" target="#b0">[1]</ref>, and are not fractal. So some regions of the bifurcation boundary for the experimental conditions in Section II will not be fractal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stochastic training</head><p>For minibatch training, the iterated function is stochastic rather than deterministic due to minibatch sampling. I was surprised that this stochastic function also generated fractals, without the fine multiscale structure being corrupted by minibatch noise. This is suggestive of Lyapunov fractals <ref type="bibr" target="#b7">[8]</ref>, for which the function being iterated changes at every time step in a sequence, though in a more restricted way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Higher dimensional fractals</head><p>This paper explored fractals in the two dimensional space defined by pairs of hyperparameters. Neural network training involves countless hyperparameters (e.g. we could specify an initialization, learning rate schedule, and regularization schedule for every weight in the network, in addition to data augmentation and loss function hyperparameters). It has been a challenge to extend Mandelbrot and Julia sets to higher dimensions in a satisfying way. That challenge should not exist for fractals stemming from neural network hyperparameters; they are naturally defined in three or more dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Meta-loss landscapes are difficult to navigate</head><p>Many types of meta-learning optimize hyperparameters associated with neural network training (e.g. this is done in learned optimizers <ref type="bibr" target="#b10">[11]</ref>). The meta-loss landscapes associated with neural network hyperparameters are often pathological and chaotic, and descending this badly behaved landscape is a central challenge in meta-learning <ref type="bibr" target="#b9">[10]</ref>.</p><p>The loss functions visualized in Figure <ref type="figure" target="#fig_1">1</ref> can be interpreted as meta-loss landscapes. These experiments therefore suggest a more nuanced explanation for chaotic meta-loss landscapes; meta-loss landscapes have extreme sensitivity to small changes in hyperparameters, because they are fractal in those hyperparameters.</p><p>Although we only observe fractal structure at the boundary between hyperparameters that result in successful or failed neural network training, this is nonetheless relevant for metalearning. The best performing hyperparameters are typically near the edge of instability, and meta-training seeks out this region in order to minimize the meta-loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Fractals are beautiful and relaxing</head><p>This has been a particularly fun project to work on (and the first project where my daughter is as excited about the results as I am). I hope the reader found these experiments unusually enjoyable, as I did.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>or ReLU, and α 0 = 1 n when σ(•) is the identity function. The training dataset D has the same number of examples |D| as the number of free parameters of f (•). |D| = n 2 + n datapoints for nonlinear networks, and |D| = n datapoints for deep linear networks. Input and hidden layer widths are fixed to n = 16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The boundary between trainable and untrainable neural network hyperparameters is fractal, for all experimental conditions. Images show a 2d grid search over neural network hyperparameters. For points shaded red, training diverged. For points shaded blue, training converged. Paler points correspond to faster convergence or divergence. Experimental conditions include different network nonlinearities, both minibatch and full batch training, and grid searching over either training or initialization hyperparameters. See Section II-D for details. Each image is a hyperlink to an animation zooming into the corresponding fractal landscape (to the depth at which float64 discretization artifacts appear). Experimental code, images, and videos are available at https://github.com/Sohl-Dickstein/fractal.</figDesc><graphic coords="2,83.51,496.87,203.99,172.10" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For consistency with machine learning terminology, I use the term hyperparameter for parameters governing the dynamics or initial conditions of function iteration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The bifurcation boundary may also be between sequences that converge to different finite solutions (or limit cycles), as in Newton fractals<ref type="bibr" target="#b13">[14]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>Thank you to <rs type="person">Maika Mars Miyakawa Sohl-Dickstein</rs> for inspiring the original idea, and for detailed feedback on the generated fractals.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The dynamics of 2generator subgroups of psl (2, c)</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matelski</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Riemann surfaces and related topics: Proceedings of the 1978 Stony Brook Conference</title>
		<meeting><address><addrLine>Princeton, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Xuxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnakumar</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01687</idno>
		<title level="m">Promit Ghosal, and Bhavya Agrawalla. From stability to chaos: Analyzing gradient descent dynamics in quadratic regression</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Porespy: A python toolkit for quantitative analysis of porous media images</title>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">T</forename><surname>Gostick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohaib</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Tranter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew Dr</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrez</forename><surname>Agnaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadamin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rhodri</forename><surname>Jervis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page">1296</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mémoire sur l&apos;itération des fonctions rationnelles</title>
		<author>
			<persName><forename type="first">Gaston</forename><surname>Julia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de mathématiques pures et appliquées</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="245" />
			<date type="published" when="1918">1918</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function</title>
		<author>
			<persName><forename type="first">Lingkai</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molei</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2625" to="2638" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><surname>Mandelbrot</surname></persName>
		</author>
		<title level="m">The fractal geometry of nature</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>WH freeman</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lyapunov exponents of the logistic map with periodic forcing</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chaos and Fractals</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layer neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phan-Minh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="7665" to="E7671" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding and correcting pathologies in the training of learned optimizers</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4556" to="4565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amil</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<title level="m">Training versatile learned optimizers by scaling up</title>
		<meeting><address><addrLine>Velo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The &quot;burning ship&quot; and its quasi-julia sets</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Michelitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otto</forename><forename type="middle">E</forename><surname>Rössler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="435" to="438" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dynamics in One Complex Variable</title>
		<author>
			<persName><forename type="first">John</forename><surname>Milnor</surname></persName>
		</author>
		<idno>AM- 160):(AM-160</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">160</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tatham</surname></persName>
		</author>
		<ptr target="http://www.chiark.greenend.org.uk/˜sgtatham/newton/" />
		<title level="m">Fractals derived from newtonraphson</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
