# The boundary of neural network trainability is fractal

## Abstract

## 

Some fractals -for instance those associated with the Mandelbrot and quadratic Julia sets -are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded [[7]](#b6). Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.

## I. INTRODUCTION

A common way to generate fractals is to iterate a fixed function f (•) repeatedly, and to keep only the set of points for which small perturbations to hyperparameters [1](#foot_0) of that function lead to dramatic changes in the sequence of iterated values. These points can be thought of as defining a boundary in hyperparameter space along which dynamics bifurcate. For instance, on one side of the boundary function iterations may converge or remain bounded, while on the other side of the boundary they may diverge towards infinity. [2](#foot_1) If the hyperparameters are the initial conditions of the iterated function, this bifurcation boundary is known as the Julia set [[5]](#b4).

As an example, consider iterating the complex valued function f (z; c) = z 2 + c: the Mandelbrot fractal [[2,](#b1)[7]](#b6) is defined by the bifurcation boundary between values of the hyperparameter c for which this iterated function diverges or remains bounded (for an initial z value of 0); while quadratic Julia sets are defined by the bifurcation boundary between initial z values for which this iterated functions diverges or remains bounded (for fixed c).

When we train a neural network, we iterate a function (a gradient descent step) of many variables (the parameters of the neural network). For instance, if we perform full batch steepest descent with a fixed learning rate η, we update the parameters W by iterating the function f (W ; η) = W -η g(W ), where g(W ) is the gradient of the training loss. Iterated steps of gradient descent are known to exhibit bifurcation boundaries, between hyperparameters that lead to converging or diverging training runs. The final loss value achieved when training a neural network has also been shown to have a chaotic dependence on hyperparameters [[10,](#b9)[6,](#b5)[3]](#b2).

Motivated by these similarities between fractal generation and neural network training, in this paper I visualize the bifurcation boundary between hyperparameters which lead to successful and unsuccessful training of neural networks. I find that this boundary is fractal in all experimental conditions, including full batch training with tanh and ReLU nonlinearities, training a deep linear network, minibatch training, training on a dataset of size 1, and visualizing training success for different subsets of hyperparameters. II. EXPERIMENTS A. Network and data I train a one hidden layer network with inputs x ∈ R n and parameters W 0

$∈ R n×n , W 1 ∈ R 1×n on an mse loss, ŷ(x; W 0 , W 1 ) = α 1 W 1 σ (α 0 W 0 x)(1)$$ℓ (W 0 , W 1 ) = 1 |D| {x,y}∈D (y -ŷ(x; W 0 , W 1 )) 2 .(2)$Weights, datapoints, and labels are all randomly initialized from a standard normal distribution, N (0, 1). The function σ(•) : R → R is applied pointwise. The scaling factors α 0 and α 1 are chosen to correspond to the mean field neural network parameterization [[9]](#b8): 

$α 1 = 1 n ; α 0 = 2 n when σ(•) is tanh$
## B. Training

The input and output weights W 0 and W 1 are trained with learning rates η 0 and η 1 respectively. Training consists of 500 (sometimes 1000) iterations of full batch steepest gradient descent. Training is performed for a 2d grid of η 0 and η 1 hyperparameter values, with all other hyperparameters held fixed (including network initialization and training data). Training was performed in float64. Some of these design choices are modified for individual experiments, as stated.

## C. Visualization and analysis

Training runs that diverge are shown in shades of red. Training runs that converge are shown in shades of blue. For converging runs, color intensity is proportional to t ℓ t (•), where ℓ t is the loss at training step t. The more intense the blue color, the longer the training run spent with higher loss. For diverging runs, color intensity is proportional to t ℓ -1 t .

## arXiv:2402.06184v1 [cs.LG] 9 Feb 2024

Deep linear full batch (fractal dim 1.17)

1.90010080e+2 1.90011840e+2 Input layer learning rate 4.77745244e+0 4.77749670e+0 Output layer learning rate ReLU full batch (fractal dim 1.20) 5.77622737e-1 5.77625590e-1 Input layer learning rate 8.14432439e+1 8.14436463e+1 Output layer learning rate tanh dataset set size 1 (fractal dim 1.41) 4.8298735e+1 4.8299146e+1 Input layer learning rate 1.82066141e+1 1.82067690e+1 Output layer learning rate tanh minibatch (fractal dim 1.55)

2.67398835471957e-1 2.67398835475071e-1 Input layer learning rate 1.74269509700711e+2 1.74269509702741e+2 Output layer learning rate tanh full batch (fractal dim 1.66) 1.329976231e+1 1.329978604e+1 Input layer learning rate The more intense the red, the longer the training run spent with lower loss. The color scale is adapted to each image.

For each zoom sequence, a series of roughly 50 images of size 4096 × 4096 pixels are generated, each of which increases the zoom factor by a factor of two. The zoom animation interpolates between these images.

The estimated fractal dimension in Figure [1](#fig_1) is the median of the estimated fractal dimension for all ∼50 images in the zoom sequence. Estimation is performed using the boxcount method of PoreSpy [[4]](#b3).

## D. Experimental conditions

To explore how general fractal behavior is, I performed experiments in six conditions:

1) Baseline condition: tanh nonlinearity, full batch gradient descent, grid search over η 0 and η 1 learning rate hyperparameters. 2) ReLU nonlinearity. 3) Identity nonlinearity (i.e., a deep linear network). 4) Minibatch gradient descent, with minibatch size 16. 5) Only a single training datapoint, |D| = 1. 6) A grid search over a different pair of hyperparameters, one of which specified the mean value used during parameter initialization, and the other of which specified the learning rate used for both parameters. Unspecified design choices are the same as in the baseline condition, and as described earlier in Section II. Representative images from all experimental conditions are shown in Figure [1](#fig_1), sorted by fractal dimension, and with links to the corresponding animations.

## III. DISCUSSION

## A. Elaborate functions in high dimensional spaces

Most popular fractals defined by bifurcation boundaries iterate only a simple one-dimensional function, consisting of a low degree polynomial or ratio of polynomials [[2,](#b1)[7,](#b6)[12,](#b11)[8,](#b7)[13,](#b12)[14]](#b13). The resulting fractals are typically perceived as possessing a lot of both repeated geometric structure and symmetry (e.g. consider the presence of 'mini-Mandelbrot' sets deep within the Mandelbrot set).

In contrast, neural network training involves iterating a complicated function, with many random terms stemming from weight initialization and training data, acting in a high dimensional space (i.e. the function acts on the parameter space of the neural network being trained). The resulting fractals seem visually more organic, with less repeated structure and symmetry. It will be a fascinating to further explore how properties of fractals depend on properties of the generating function.

## B. Non-homogeneity of boundary

It will similarly be fascinating to explore how properties of the bifurcation boundary vary for a single generating function, in different regions of hyperparameter space. I decided which regions of the hyperparameter landscape to explore by hand in an ad hoc way, and the resulting images are inevitably biased. A limiting example to consider is when the learning rate η 0 for the input layer is made very small, so that only the readout layer trains. Training only the readout layer corresponds to linear regression on an mse loss, with dynamics that are known in closed form [[1]](#b0), and are not fractal. So some regions of the bifurcation boundary for the experimental conditions in Section II will not be fractal.

## C. Stochastic training

For minibatch training, the iterated function is stochastic rather than deterministic due to minibatch sampling. I was surprised that this stochastic function also generated fractals, without the fine multiscale structure being corrupted by minibatch noise. This is suggestive of Lyapunov fractals [[8]](#b7), for which the function being iterated changes at every time step in a sequence, though in a more restricted way.

## D. Higher dimensional fractals

This paper explored fractals in the two dimensional space defined by pairs of hyperparameters. Neural network training involves countless hyperparameters (e.g. we could specify an initialization, learning rate schedule, and regularization schedule for every weight in the network, in addition to data augmentation and loss function hyperparameters). It has been a challenge to extend Mandelbrot and Julia sets to higher dimensions in a satisfying way. That challenge should not exist for fractals stemming from neural network hyperparameters; they are naturally defined in three or more dimensions.

## E. Meta-loss landscapes are difficult to navigate

Many types of meta-learning optimize hyperparameters associated with neural network training (e.g. this is done in learned optimizers [[11]](#b10)). The meta-loss landscapes associated with neural network hyperparameters are often pathological and chaotic, and descending this badly behaved landscape is a central challenge in meta-learning [[10]](#b9).

The loss functions visualized in Figure [1](#fig_1) can be interpreted as meta-loss landscapes. These experiments therefore suggest a more nuanced explanation for chaotic meta-loss landscapes; meta-loss landscapes have extreme sensitivity to small changes in hyperparameters, because they are fractal in those hyperparameters.

Although we only observe fractal structure at the boundary between hyperparameters that result in successful or failed neural network training, this is nonetheless relevant for metalearning. The best performing hyperparameters are typically near the edge of instability, and meta-training seeks out this region in order to minimize the meta-loss.

## F. Fractals are beautiful and relaxing

This has been a particularly fun project to work on (and the first project where my daughter is as excited about the results as I am). I hope the reader found these experiments unusually enjoyable, as I did.

![or ReLU, and α 0 = 1 n when σ(•) is the identity function. The training dataset D has the same number of examples |D| as the number of free parameters of f (•). |D| = n 2 + n datapoints for nonlinear networks, and |D| = n datapoints for deep linear networks. Input and hidden layer widths are fixed to n = 16.]()

![Fig. 1. The boundary between trainable and untrainable neural network hyperparameters is fractal, for all experimental conditions. Images show a 2d grid search over neural network hyperparameters. For points shaded red, training diverged. For points shaded blue, training converged. Paler points correspond to faster convergence or divergence. Experimental conditions include different network nonlinearities, both minibatch and full batch training, and grid searching over either training or initialization hyperparameters. See Section II-D for details. Each image is a hyperlink to an animation zooming into the corresponding fractal landscape (to the depth at which float64 discretization artifacts appear). Experimental code, images, and videos are available at https://github.com/Sohl-Dickstein/fractal.]()

For consistency with machine learning terminology, I use the term hyperparameter for parameters governing the dynamics or initial conditions of function iteration.

The bifurcation boundary may also be between sequences that converge to different finite solutions (or limit cycles), as in Newton fractals[[14]](#b13).

