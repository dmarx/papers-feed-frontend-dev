- Decision to analyze fine-tuning through the lens of intrinsic dimensionality
- Choice of pre-trained models for empirical analysis
- Selection of datasets for evaluating intrinsic dimensionality
- Methodology for calculating intrinsic dimension (e.g., random projections)
- Use of Fastfood transform for subspace optimization
- Definition and implementation of Structure-Aware Intrinsic Dimension (SAID)
- Comparison of SAID and Direct Intrinsic Dimension (DID) methods
- Criteria for defining a satisfactory solution in intrinsic dimension calculations
- Empirical evaluation of intrinsic dimension across various NLP tasks
- Interpretation of intrinsic dimension as minimal description length
- Connection between intrinsic dimensionality and model size
- Application of compression-based generalization bounds
- Framework for analyzing the relationship between pre-training and intrinsic dimension
- Decision to use Huggingface Transformers library for implementation
- Approach to hyperparameter tuning during experiments
- Analysis of the impact of model architecture on intrinsic dimensionality
- Consideration of layer-wise structure in parameter optimization
- Insights on the effectiveness of fine-tuning with low-dimensional representations
- Exploration of the relationship between intrinsic dimension and generalization performance
- Documentation of empirical results and their implications for future research