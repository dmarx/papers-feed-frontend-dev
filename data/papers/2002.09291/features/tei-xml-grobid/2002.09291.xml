<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformer Hawkes Process</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-02-23">February 23, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zichong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
						</author>
						<title level="a" type="main">Transformer Hawkes Process</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-23">February 23, 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">C6FC605E4705C407A212A9D47538E214</idno>
					<idno type="arXiv">arXiv:2002.09291v5[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture longterm dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event sequence data are naturally observed in our daily life. Through social media such as Twitter and Facebook, we share our experiences and respond to other users' information <ref type="bibr" target="#b33">(Yang et al., 2011)</ref>. In these websites, each user has a sequence of events such as tweets and interactions. Hundreds of millions of users generate large amounts of tweets, which are essentially sequences of events at different time stamps. Besides social media, event data also exist in domains like financial transactions <ref type="bibr" target="#b1">(Bacry et al., 2015)</ref> and personalized healthcare <ref type="bibr" target="#b30">(Wang et al., 2018)</ref>. For example, in electronic medical records, tests and diagnoses of each patient can be treated as a sequence of events. Unlike other sequential data such as time series, event sequences tend to be asynchronous <ref type="bibr" target="#b25">(Ross et al., 1996)</ref>, which means time intervals between events are just as important as the order of them to describe their dynamics. Also, depending on specific application requirements, event data show sophisticated dependencies on their history.</p><p>Published as a conference paper in ICML 2020. † Zuo, Jiang and Zhao are affiliated with Georgia Tech, Li is affiliated with University of Science and Technology of China, and Zha is affiliated with Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong (currently on leave from Georgia Tech). Correspondence to simiaozuo@gatech.edu, tourzhao@gatech.edu, zhahy@ cuhk.edu.cn.</p><p>Point process is a powerful tool for modeling sequences of discrete events in continuous time, and the technique has been widely applied. Hawkes process <ref type="bibr" target="#b10">(Hawkes, 1971;</ref><ref type="bibr" target="#b14">Isham and Westcott, 1979)</ref> and Poisson point process are traditionally used as examples of point processes. However, the simplified assumptions of the complicated dynamics of point processes limit the models' practicality. As an example, Hawkes process states that all past events should have positive influences on the occurrence of current events. However, a user on Twitter may initiate tweets on different topics, and these events should be considered as unrelated instead of mutually-excitepd.</p><p>To alleviate the over-simplifications, likelihood-free methods <ref type="bibr">(Xiao et al., 2017a;</ref><ref type="bibr" target="#b18">Li et al., 2018</ref>) and non-parametric models like kernel methods and splines <ref type="bibr" target="#b29">(Vere-Jones et al., 1990)</ref> have been proposed, but the increasing complexity and quantity of collected data crave for more powerful models. With the development of neural networks, in particular deep neural networks, focuses have been placed on incorporating these flexible models into classical point processes. Because of the sequential nature of event steams, existing methods rely heavily on Recurrent Neural Networks (RNNs). Neural networks are known for their ability to capture complicated high-level features, in particular, RNNs have the representation power to model the dynamics of event sequence data. In previous works, either vanilla RNN <ref type="bibr" target="#b7">(Du et al., 2016)</ref> or its variants <ref type="bibr" target="#b20">(Mei and Eisner, 2017;</ref><ref type="bibr">Xiao et al., 2017b)</ref> have been used and significant progress in terms of likelihood and event prediction have been achieved.</p><p>However, there are two significant drawbacks with RNN-based models. First, recurrent neural networks, even those equipped with forget gates, such as Long Short-Term Memory <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997)</ref> and Gated Recurrent Units <ref type="bibr" target="#b5">(Chung et al., 2014)</ref>, are unlikely to capture long-term dependencies. In financial transactions, short-term effects such as policy changes are important for modeling buy-sell behaviors of stocks. On the other hand, because of the delays in asset returns, stock transactions and prices often exhibit long-term dependencies on their history. As another example, in medical domains, at times we are interested in examining short-term dependencies on symptoms such as fever and cough for acute diseases like pneumonia. But for certain types of chronic diseases such as diabetes, long-term dependencies on disease diagnoses and medications are more critical. Desirable models should be able to capture these long-term dependencies. Yet with recurrent structures, interactions between two events located far in the temporal domain are always weak <ref type="bibr" target="#b12">(Hochreiter et al., 2001)</ref>, even though in reality they may be highly correlated. The reason is that the probability of keeping information in a state that is far away from the current state decreases exponentially with distance.</p><p>The second drawback is trainability of recurrent neural networks. Training deep RNNs (including LSTMs) is notoriously difficult because of gradient explosion and gradient vanishing <ref type="bibr" target="#b22">(Pascanu et al., 2013)</ref>. In practice, single-layer and two-layer RNNs are mostly used, and they may not successfully model sophisticated dependencies among data <ref type="bibr" target="#b3">(Bengio et al., 1994)</ref>. Additionally, inputs are fed into the recurrent models sequentially, which means future states must be processed after the current state, rendering it impossible to process all the events in parallel. This limits RNNs' ability to scale to large problems.</p><p>Recently, convolutional neural network variants that are tailored for analyzing sequential data <ref type="bibr" target="#b21">(Oord et al., 2016;</ref><ref type="bibr" target="#b9">Gehring et al., 2017;</ref><ref type="bibr" target="#b34">Yin et al., 2017)</ref> have been proposed to better capture long-Figure <ref type="figure">1</ref>: Illustration of dependency computation between the last event (the red triangle) and its history (the blue circles). RNN-based NHP models dependencies through recursion. THP directly and adaptively models the event's dependencies on its history. Convolution-based models enforce static dependency patterns. term effects. However, these models enforce many unnecessary dependencies. This particular downside plus the increased computational burdens deem these models insufficient.</p><p>To address the above concerns, we propose the Transformer Hawkes Process (THP) model that is able to capture both short-term and long-term dependencies whilst enjoying computational efficiency. Even though the Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> is widely adopted in natural language processing, it has rarely been used in other applications. We remark that such an architecture is not readily applicable to event sequences that are defined in a continuous-time domain. To the best of our knowledge, our proposed THP is the first of this type in point process literature.</p><p>Building blocks of THP are the self-attention modules <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref>. These modules directly model dependencies among events by assigning attention scores. A large score between two events implies a strong dependency, and a small score implies a weak one. In this way, the modules are able to adaptively select events that are at any temporal distance from the current event. Therefore, THP has the ability to capture both short-term and long-term dependencies. Figure <ref type="figure">1</ref> demonstrates dependency computation of different models.</p><p>The non-recurrent structure of THP facilitates efficient training of multi-layer models. Transformerbased architectures can be as deep as dozens of layers <ref type="bibr" target="#b6">(Devlin et al., 2018;</ref><ref type="bibr" target="#b23">Radford et al., 2019)</ref>, where deeper layers capture higher order dependencies. The ability to capture such dependencies creates models that are more powerful than RNNs, which are often shallow. Also, THP allows full parallelism when calculating dependencies across all events, i.e., the computation between any two event pairs is independent with each other. This yields a model presenting strong efficiency.</p><p>Our proposed model is quite general, and can incorporate additional structural knowledge to learn more complicated event sequence data, such as multiple point processes over a graph.</p><p>In social networks, each user has her own sequence of events, like tweets and comments. Sequences among users can be related, for example, a tweet from a user may trigger retweets from her followers. We can use graphs to model these follower-followee relationships <ref type="bibr" target="#b37">(Zhou et al., 2013;</ref><ref type="bibr" target="#b8">Farajtabar et al., 2017)</ref>, where each vertex corresponds to a specific user and each edge represents connections between the two associated users. We propose an extension to THP that integrates these relational graphs <ref type="bibr" target="#b4">(Borgatti et al., 2009;</ref><ref type="bibr" target="#b19">Linderman and Adams, 2014)</ref> into the self-attention module via a similarity metric among users. Such a metric can be learned by our proposed graph regularization.</p><p>We experiment THP on five datasets to evaluate both validation likelihood and event prediction accuracy. Our THP model exhibits superior performance to RNN-based models in all these experiments. We further test our structured-THP on two additional datasets, where the model achieves improved prediction performance for learning multiple point processes when incorporating their relational information. Our code is available at <ref type="url" target="https://github.com/SimiaoZuo/Transformer-Hawkes-Process">https://github.com/SimiaoZuo/ Transformer-Hawkes-Process</ref>.</p><p>The rest of this paper is organized as follows: Section 2 introduces the background; Section 3 introduces our proposed transformer Hawkes process model; Section 4 demonstrates an extension of our model to multiple event sequences on graphs; Section 5 presents numerical experiments on various real datasets; Section 6 draws a brief conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We briefly review Hawkes Process <ref type="bibr" target="#b10">(Hawkes, 1971)</ref>, <ref type="bibr">Neural Hawkes Process (Mei and Eisner, 2017)</ref>, and Transformer <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> in this section.</p><p>• Hawkes Process is a doubly stochastic point process, whose intensity function is defined as</p><formula xml:id="formula_0">λ(t) = µ + j:t j &lt;t ψ(t -t j ). (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>Here µ is the base intensity and ψ(•) is a pre-specified decaying function, i.e., exponential function and power-law function. Intuitively, Eq. 1 means that each of the past events has a positive contribution to occurrence of the current event, and this influence decreases through time. However, a major limitation of this formulation is the simplification that history events can never inhibit occurrence of future events, which is unrealistic in complex real-life scenarios.</p><p>• Neural Hawkes Process generalizes the classical Hawkes process by parameterizing its intensity function with recurrent neural networks. Specifically,</p><formula xml:id="formula_2">λ(t) = K k=1 λ k (t) = K k=1 f k w k h(t) , t ∈ (0, T ], where f k (x) = β k log 1 + exp x β k . Prediction is then P[k t = k] = λ k (t)/λ(t).</formula><p>Here, λ(t) is the intensity function, K is the number of event types, and h(t)s are the hidden states of the event sequence, obtained by a continuous-time LSTM (CLSTM) module. CLSTM is an interpolated version of the standard LSTM, and it allows us to generate outputs in a continuous-time domain. Also, f k (•) is the softplus function with parameter β k that guarantees a positive intensity. One downside of the neural Hawkes process is that intrinsic weaknesses of RNNs are inherited, namely the model is unable to capture long-term dependencies and is difficult to train.</p><p>• Transformer is an attention-based model that has been broadly applied in tasks such as machine translation <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> and language modeling <ref type="bibr" target="#b23">(Radford et al., 2019)</ref>. Despite its success in natural language processing, it has rarely been used in other areas. We remark that the Transformer architecture is not directly applicable to model point processes. In particular, time intervals between any two events can be arbitrary in event streams, while in natural languages, words are observed on regularly spaced time intervals. Therefore, we need to generalize the architecture to a continuous-time domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We introduce our proposed Transformer Hawkes Process. Suppose we are given an event sequence S = {(t j , k j )} L j=1 of L events, where each event has type k j ∈ {1, 2, . . . , K}, with a total number of K types. Then each pair (t j , k j ) corresponds to an event of type k j occurs at time t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformer Hawkes Process</head><p>The key ingredient of our proposed THP model is the self-attention module. Different from RNNs, the attention mechanism discards recurrent structures. However, our model still needs to be aware of the temporal information of inputs, i.e., time stamps. Therefore, analogous to the original positional encoding method <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, we propose to use a temporal encoding procedure, defined by</p><formula xml:id="formula_3">[z(t j )] i =          cos t j /10000 i-1 M , if i is odd, sin t j /10000 i M , if i is even. (2)</formula><p>Eq. 2 uses trigonometric functions to define a temporal encoding for each time stamp, i.e., for each t j , we deterministically computes z(t j ) ∈ R M , where M is the dimension of encoding. Other temporal encoding methods can also be applied, such as the relative position representation model <ref type="bibr" target="#b26">(Shaw et al., 2018)</ref>, where two temporal encoding matrices are learned instead of predefined.</p><p>Besides temporal encoding, we train an embedding matrix U ∈ R M×K for the event types, where the k-th column of U is a M-dimensional embedding for event type k. For any event of type k j , let k j be its one-hot encoding (a K-dimensional vector with all 0s except for the k j -th index, which has value 1), then its embedding is Uk j . Notice that for any event and its corresponding time stamp (t j , k j ), the temporal encoding z(t j ) and the event embedding Uk j both reside in R M . Embedding of the event sequence S = {(t j , k j )} L j=1 is then specified by</p><formula xml:id="formula_4">X = UY + Z , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">Y = [k 1 , k 2 , . . . , k L ] ∈ R K×L</formula><p>is the collection of event type one-hot encodings, and Z = [z(t 1 ), z(t 2 ), . . . , z(t L )] ∈ R M×L is the concatenation of event time encodings. Notice that X ∈ R L×M and each row of X corresponds to the embedding of a specific event in the sequence.</p><p>After the initial encoding and embedding layers, we pass X through the self-attention module. Specifically, we compute the attention output S by</p><formula xml:id="formula_7">S = Softmax QK √ M K V, where Q = XW Q , K = XW K , V = XW V . (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>Here Q, K, and V are the query, key, and value matrices obtained by different transformations of X, and</p><formula xml:id="formula_9">W Q , W K ∈ R M×M K , W V ∈ R M×M V</formula><p>are weights for the linear transformations, respectively. In practice using multi-head self-attention to increase model flexibility is more beneficial for data fitting. To facilitate this, different attention outputs S 1 , S 2 , . . . , S H are computed using different sets of weights</p><formula xml:id="formula_10">{W Q h , W K h , W V h } H h=1 .</formula><p>The final attention output for the event sequence is then</p><formula xml:id="formula_11">S = S 1 , S 2 , . . . , S H W O ,</formula><p>where W O ∈ R HM V ×M is an aggregation matrix.</p><p>We highlight that the self-attention module is able to directly select events whose occurrence time is at any distance from the current time. The j-th column of the attention weights Softmax(QK / √ M K ) signifies event t j 's extent of dependency on its history. In contrast, RNNbased models encode history information sequentially via hidden representations of the events, i.e., the state of t j depends on that of t j-1 , which in turn depends on t j-2 , etc. Should any of these encodings be weak, i.e., the RNN fails to learn sufficient relevant information for event t k , hidden representations of any event t j where j ≥ k will be inferior.</p><p>The attention output S is then fed through a position-wise feed-forward neural network, generating hidden representations h(t) of the input event sequence:</p><formula xml:id="formula_12">H = ReLU SW FC 1 + b 1 W FC 2 + b 2 , h(t j ) = H(j, :). (<label>5</label></formula><formula xml:id="formula_13">)</formula><p>Here</p><formula xml:id="formula_14">W FC 1 ∈ R M×M H , W FC 2 ∈ R M H ×M , b 1 ∈ R M H</formula><p>, and b 2 ∈ R M are parameters of the neural network, and W FC 2 has identical columns. The resulting matrix H ∈ R L×M contains hidden representations of all the events in the input sequence, where each row corresponds to a particular event.</p><p>To avoid "peeking into the future", our attention algorithm is equipped with masks. That is, when computing the attention output S(j, :) (the j-th row of S), we mask all the future positions, i.e., we set Q(j, j+1), Q(j, j+1), . . . , Q(j, L) to inf. This will avoid the softmax function from assigning dependency to events in the future.</p><p>In practice we stack multiple self-attention modules together, and inputs are passed through each of these modules sequentially. In this way our model is able to capture high level dependencies. We remark that stacking RNN/LSTM is not plausible because gradient explosion and gradient vanishing will render the stacked model difficult to train. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the architecture of THP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Continuous Time Conditional Intensity</head><p>Dynamics of temporal point processes are described by a continuous conditional intensity function. Eq. 5 only generates hidden representations for discrete time stamps, and the associated intensity is also discrete. Therefore an interpolated continuous time intensity function is in need.</p><p>Let λ(t|H t ) be the conditional intensity function for our model, where H t = {(t j , k j ) : t j &lt; t} is the history up to time t. We define different intensity functions for different event types, i.e., for every k ∈ {1, 2, . . . , K}, define λ k (t|H t ) as the conditional intensity function for events of type k. The conditional intensity function for the entire event sequence is defined by</p><formula xml:id="formula_15">λ(t|H t ) = K k=1 λ k (t|H t ),</formula><p>where each of the type-specific intensity takes the form</p><formula xml:id="formula_16">λ k (t|H t ) = f k α k t -t j t j current + w k h(t j ) history + b k base . (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>In Eq. 6, time is defined on interval t ∈ [t j , t j+1 ), and f k (x) = β k log 1 + exp(x/β k ) is the softplus function with "softness" parameter β k . The reason for choosing this particular function is twofold: first, the softplus function ensures that the intensity is positive; second, "softness" of the softplus function guarantees stable computation and avoids dramatic changes in the intensity. Now we explain each term in Eq. 6 in detail: • The "current" influence is an interpolation between two observed time stamps t j and t j+1 , and α k modulates importance of the interpolation. When t = t j , i.e., a new observation comes in, this influence is 0. When t → t j+1 , the conditional intensity function is no longer continuous. As a matter of fact, Eq. 6 is continuous everywhere except for the observed events {(t j , k j )}. However, these "jumps" in intensity is a non-factor when computing likelihood.</p><p>• The "history" term contains two parts: a vector w k that transforms the hidden states of the THP model into a scalar, and the hidden states h(t) (Sec. 3.1) themselves that encode past events up to time t.</p><p>• The "base" intensity represents probability of occurrence of events without considering history information.</p><p>With our proposed conditional intensity function, next time stamp prediction and next event type prediction is given by<ref type="foot" target="#foot_0">foot_0</ref> </p><formula xml:id="formula_18">p(t|H t ) = λ(t|H t ) exp - t t j λ(τ|H τ )dτ , t j+1 = ∞ t j t • p(t|H t )dt, and k j+1 = argmax k λ k (t j+1 |H j+1 ) λ(t j+1 |H j+1</formula><p>) .</p><p>(7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>For any sequence S over an observation interval [t 1 , t L ], given its conditional intensity function λ(t|H t ), the log-likelihood is</p><formula xml:id="formula_19">(S) = L j=1 log λ(t j |H j ) event log-likelihood - t L t 1 λ(t|H t )dt non-event log-likelihood . (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>Model parameters are learned by maximizing the log-likelihood across all sequences. Concretely, suppose we have N sequences S 1 , S 2 , . . . , S N , then the goal is to find parameters that solve max N i=1</p><formula xml:id="formula_21">(S i ),</formula><p>where (S i ) is the log-likelihood of event sequence S i . This optimization problem can be efficiently solved by stochastic gradient type algorithms like ADAM <ref type="bibr" target="#b16">(Kingma and Ba, 2014)</ref>. Additionally, techniques that help stabilizing training such as layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref> and residual connection <ref type="bibr" target="#b11">(He et al., 2016)</ref> are also applied.</p><p>In Eq. 8, one challenge is to compute</p><formula xml:id="formula_22">Λ = t L t 1</formula><p>λ(t|H t )dt, the non-event log-likelihood. Because of the softplus function, there is no closed-form computation for this integral, and a proper approximation is needed.</p><p>The first approach to approximate the non-event log-likelihood is by using Monte Carlo integration <ref type="bibr" target="#b24">(Robert and Casella, 2013)</ref>: Here u i ∼ Unif(t j-1 , t j ) is sampled from a uniform distribution with support [t j-1 , t j ]. Notice that λ(u i ) and ∇λ(u i ) can be calculated by feed-forward and back-propagation through the model, respectively. Moreover, Eq. 9 yields an unbiased estimation to the integral, i.e., E[ Λ MC ] = Λ.</p><formula xml:id="formula_23">Λ MC = L j=2 (t j -t j-1 ) 1 N N i=1 λ(u i ) , ∇ Λ MC = L j=2 (t j -t j-1 ) 1 N N i=1 ∇λ(u i ) . (9)</formula><p>The second approach is to apply numerical integration methods, which are faster because of the elimination of sampling. For example, the trapezoidal rule <ref type="bibr" target="#b27">(Stoer and Bulirsch, 2013)</ref> states that</p><formula xml:id="formula_24">Λ NU = L j=2 t j -t j-1 2 λ(t j |H j ) + λ(t j-1 |H j-1 )<label>(10)</label></formula><p>qualifies as an approximation to Λ. Other higher order methods such as the Simpson's rule <ref type="bibr" target="#b27">(Stoer and Bulirsch, 2013)</ref> can also be applied. Even though approximations build upon numerical integration algorithms are biased, in practice they are affordable. This is because the conditional intensity (Eq. 6) uses softplus as its activation function, which is highly smooth and ensures bias introduced by linear interpolations (Eq. 10) between consecutive events are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Structured Transformer Hawkes Process</head><p>THP is quite general and can incorporate additional structural knowledge. We consider multiple point processes, where any two of them can be related. Such relationships are often described by a graph G = (V , E), where V is the vertex set, and each vertex is associated with a point process. Also, E is the edge set, where each edge signifies relational information between the corresponding two vertices. Figure <ref type="figure" target="#fig_1">3</ref> illustrates event sequences on a graph. The graph encodes relationships among vertices, and further indicates potential interactions. We propose to model all the point processes with a single THP, and the heterogeneity of the vertices' point processes is handled by a vertex embedding approach.</p><p>Suppose we have an event sequence S = {(t j , k j , v j )} L j=1 , where t j and k j are time stamps and event types as before. Further, v j ∈ {1, 2, . . . , |V |} is an indicator to which vertex the event belongs. In addition to the event embedding and the temporal encoding (Eq. 3), we introduce a vertex embedding matrix E ∈ R M×|V | , where the j-th column of E denotes the M-dimensional embedding for vertex j. Let v j be the one-hot encoding of v j , then embedding of S is specified by</p><formula xml:id="formula_25">X = UY + EV + Z , where V = [v 1 , v 2 , . . . , v L ] ∈ R |V |×L</formula><p>is the concatenation of vertices, and other terms are defined in Eq. 3.</p><p>The graph attention output is defined by</p><formula xml:id="formula_26">S = Softmax QK √ M K + A V value , where A = (EV) Ω(EV),<label>(11)</label></formula><p>where Q, K, and V value are the same<ref type="foot" target="#foot_1">foot_1</ref> as in Eq. 4. Matrix A ∈ R L×L is the vertex similarity matrix, where each entry A ij signifies the similarity between two vertices v i and v j , and Ω ∈ R M×M is a metric to be learned. To extend the graph self-attention module to a multi-head setting, we use different metric matrices {Ω j } H j=1 for different heads. We remark that unlike RNN-based shallow models, in structured-THP, multiple multi-head self-attention modules can be stacked (Figure <ref type="figure" target="#fig_0">2</ref>) to learn high level representations, a feature that enables learning of complicated similarities among vertices. Moreover, the vertex similarity matrix enables modeling of even more complicated structured data, such as sequences on dynamically evolving graphs.</p><p>With the incorporation of relational information, we need to modify the conditional intensity function accordingly. As an extension to Eq. 6, where each type of events has its own intensity, we define a different intensity function for each event type and each vertex. Specifically,</p><formula xml:id="formula_27">λ(t|H t ) = K k=1 |V | v=1 λ k,v (t|H t ), t ∈ [t j , t j+1 ), where λ k,v (t|H t ) = f k,v α k,v t -t j t j + w k,v h(t) + b k,v .</formula><p>Model parameters are learned by maximizing the log-likelihood (Eq. 8) across all sequences. Concretely, suppose we have N sequences S 1 , S 2 , . . . , S N , then parameters are obtained by solving max N i=1</p><formula xml:id="formula_28">(S i ) + µL graph (V, Ω),</formula><p>where µ is a hyper-parameter and</p><formula xml:id="formula_29">L graph (V, Ω) = |V | k=1 k j=1 -log 1 + exp(V j ΩV k ) + 1{(v j , v k ) ∈ E} V j ΩV k .</formula><p>Here L graph (V, Ω) is a regularization term that encourages V j ΩV k to be large when there exists an edge between v j and v k . Which means if two vertices are connected in graph G, then the regularizer will promote attention between them, and vice versa.</p><p>Notice that in the simplest case, A in Eq. 11 can be some transformation of the adjacency matrix, i.e., A ij = 1 if (v i , v j ) ∈ E, and 0 otherwise. However, we believe that this constraint is too strict, i.e., some connected vertices may not behave similarly. Therefore, we treat the graph as a guide and introduce a regularization term that encourages A to be similar to the adjacency matrix, but not enforce it. In this way, our model is more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare THP against existing models: Recurrent Marked Temporal Point Process (RMTPP, <ref type="bibr" target="#b7">Du et al. (2016)</ref>), Neural Hawkes Process (NHP, <ref type="bibr" target="#b20">Mei and Eisner (2017)</ref>), Time Series Event Sequence (TSES, <ref type="bibr">Xiao et al. (2017b)</ref>), and Self-attentive Hawkes Processes (SAHP, Zhang et al. ( <ref type="formula">2019</ref>))<ref type="foot" target="#foot_2">foot_2</ref> . We evaluate the models by per-event log-likelihood (in nats) and event prediction accuracy on heldout test sets. Details about training are deferred to the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We adopt several datasets to evaluate the models. Table <ref type="table" target="#tab_0">1</ref> summarizes statistics of the datasets.</p><p>• Retweets <ref type="bibr" target="#b36">(Zhao et al., 2015)</ref>: The Retweets dataset contains sequences of tweets, where each sequence contains an origin tweet (i.e., some user initiates a tweet), and some follow-up tweets. We record the time and the user tag of each tweet. Further, users are grouped into three categories based on the number of their followers: "small", "medium", and "large".</p><p>• MemeTrack <ref type="bibr" target="#b17">(Leskovec and Krevl, 2014)</ref>: This dataset contains mentions of 42 thousand different memes spanning ten months. We collect data on over 1.5 million documents (blogs, web articles, etc.) from over 5000 websites. Each sequence in this dataset is the life-cycle of a particular meme, where each event (usage of meme) is associated with a time stamp and a website id.</p><p>• Financial Transactions <ref type="bibr" target="#b7">(Du et al., 2016)</ref>: This financial dataset contains transaction records of a stock in one day. We record the time (in milliseconds) and the action that was taken in each transaction. The dataset is a single long sequence with only two types of events: "buy" and "sell". The event sequence is further partitioned by time stamps.</p><p>• Electrical Medical Records <ref type="bibr" target="#b15">(Johnson et al., 2016)</ref>: MIMIC-II medical dataset collects patients' visit to a hospital's ICU in a seven-year period. We treat the visits of each patient as a separate sequence, where each event in the sequence contains a time stamp and a diagnosis.</p><p>• StackOverflow <ref type="bibr" target="#b17">(Leskovec and Krevl, 2014)</ref>: StackOverflow is a question-answering website. The website rewards users with badges to promote engagement in the community, and the same badge can be rewarded multiple times to the same user. We collect data in a two-year period, and we treat each user's reward history as a sequence. Each event in the sequence signifies receipt of a particular medal.</p><p>• 911-Calls<ref type="foot" target="#foot_3">foot_3</ref> : The 911-Calls dataset contains emergency phone call records. Calling time, location of the caller, and nature of the emergency are logged for each record. We consider three types of emergencies: EMS, fire, and traffic. We treat location of callers (given by zipcodes) as vertices on a relational information graph. Zipcodes are ranked based on the number of recorded calls, and only the top 75 zipcodes are kept. An undirected edge exists between two vertices if their zipcodes are within 10 of each other. • Earthquake<ref type="foot" target="#foot_4">foot_4</ref> : This dataset contains time and location of earthquakes in China in an eight-year period. We partition the records into two categories: "small" and "large". A relational information graph is built based on geographical locations of the earthquakes, i.e., each province is a vertex and earthquakes are sequences on the vertices. Two vertices are connected if their associated provinces are neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Details</head><p>To facilitate comparison with previous works, all the datasets are used by <ref type="bibr" target="#b7">Du et al. (2016)</ref> and <ref type="bibr" target="#b20">Mei and Eisner (2017)</ref>, except for 911-Calls and Earthquake. Details about data pre-processing and train-dev-test split, as well as downloadable links, can be found in the aforementioned papers.</p><p>For the 911-Calls dataset, we exclude zipcodes (and the associated events) whose occurrences are scarce, i.e., we only keep zipcodes that have the top 75 frequent occurrences. The dataset contains 141 types of events, and we cluster them into three categories, namely EMS, fire, and traffic. We do not exclude any events in the Earthquake dataset. Earthquakes are partitioned into two categories, "small" and "large", where small earthquakes are the ones whose Richter scale is equal to or lower than 1.0. We perform this partition because of the imbalance in data, i.e., most of the recorded earthquakes are on small magnitude. Models are trained on 911-Calls and Earthquake with different number of training events. In each experiment, we equally divide the events that are not in the training set in half to construct the development set and the test set.</p><p>There are three sets of hyper-parameters that we use, and they are summarized in Table <ref type="table">2</ref>. Besides layer normalization and residual connection, we also employ the dropout technique to avoid overfitting. Table <ref type="table">3</ref> contains the specific parameters that are applied for the training of each dataset. In the table, from left to right columns specify: name of the dataset, the set of applied hyper-parameters, batch size, learning rate, and solver for the integral approximation (MC stands for Monte Carlo integration, and NU stands for numerical integration with the trapezoidal rule), respectively. In the 911-Calls and the Earthquakes datasets, we also employ the graph regularization method, and the corresponding regularization parameter is set to be 0.01 in all the experiments. We use a single NVIDIA RTX graphics card to run all the experiments.  model fits the data well and outperforms all the baselines in all the experiments. Figure <ref type="figure" target="#fig_2">5</ref> visualizes attention patterns of THP. We can see that each attention head employs a different pattern to capture dependencies. Moreover, while attention heads in the first layer tend to focus on individual events, the attention patterns in the last layer are more uniformly distributed. This is because features in deeper layers are already transformed by attention heads in shallow layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Event Prediction Comparison</head><p>For point processes, event prediction is just as important as data fitting. Eq. 7 enables us to predict future events. In practice, however, adding additional prediction layers on top of the THP model yields better performance. Specifically, given the hidden representation h(t j ) for event (t j , k j ), the next event type and time predictions are as follows.</p><p>• The next event type prediction is</p><formula xml:id="formula_30">k j+1 = argmax k p j+1 (k), where p j+1 = Softmax W type h(t j ) ,</formula><p>where W type ∈ R K×M is the predictor parameter, and p j (k) is the k-th element of p j ∈ R K .</p><p>• The next event time prediction is</p><formula xml:id="formula_31">t j+1 = W time h(t j ),</formula><p>where W time ∈ R 1×M is the predictor parameter.</p><p>To learn the predictor parameters, the loss function is equipped with a cross-entropy term for event type prediction and a mean square error term for event time prediction. Concretely, for  an event sequence S = {(t j , k j )} L j=1 , let k 1 , k 2 , . . . , k L be the ground-truth one-hot encodings for the event types, we define</p><formula xml:id="formula_32">L type (S) = L j=2 -k j log( p j ), and L time (S) = L j=2 (t j -t j ) 2 .</formula><p>Notice that we do not predict the first event. Then, given event sequences {S i } N i=1 , we seek to solve min</p><formula xml:id="formula_33">N i=1 -(S i ) + L type (S i ) + L time (S i ),</formula><p>where (S i ) is the log-likelihood (Eq. 8) of S i .</p><p>To evaluate model performance, we predict every held-out event (t j , k j ) given its history H j , i.e., for a test sequence of length L, we make L -1 predictions. We evaluate event type prediction by accuracy and event time prediction by Root Mean Square Error (RMSE). Table <ref type="table" target="#tab_2">5</ref> and <ref type="table" target="#tab_3">Table 6</ref> summarize experiment results. We can see that THP outperforms the baselines in all these tasks. The datasets we adopted vary significantly in average sequence length, i.e., the average length in Financial Transactions is 2074 while it is only 4 in MIMIC-II. In all the three datasets, THP improves upon RNN-based models by a notable margin. The results demonstrate that THP is able to capture both short-term and long-term dependencies better than existing methods.</p><p>Figure <ref type="figure" target="#fig_3">6</ref> illustrates run-to-run variance of THP, NHP, and RMTPP. The error bars are wide because of how the data are split. Held-out test sets are constructed by randomly sampling some events from the entire dataset. That is, at times "important" events are sampled out, which will yield unsatisfactory model performance. Our results are better than all the baselines in all the individual experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">THP vs. Structured-THP</head><p>Now we demonstrate by incorporating relational information, THP achieves improved performance.</p><p>Baseline models are constructed as following: for each vertex on a relational graph G, there exists a point process that consists of time and type of events. These event sequences are learned separately by both THP and NHP, i.e., we do not allow information sharing among vertices in these models.</p><p>To integrate G into THP, we consider two approaches. The first approach is by allowing full attention, i.e., information from one vertex can be shared with all the other vertices. The second approach is by using the neighborhood graph, which is constructed based on spatial proximity. In this approach, a specific vertex can only share information with its neighbors. We fit a structured-THP to both of the cases.</p><p>Figure <ref type="figure" target="#fig_4">7</ref> summarizes experimental results. We can see that THP is comparable or better than NHP in both validation likelihood and event prediction, which further demonstrates that THP can model complicated dynamics better than RNN-based models. Notice that THP-F, the structured-THP with full attention, yields a much better likelihood than the baseline models, which means relational information sharing can help the models in capturing latent dynamics. However, unlike likelihood, THP-F does not show consistent improvements in event prediction. This is because when the number of training events is small, the model cannot build a sufficient informationsharing heuristic. Also, the performance drop when the number of training events is large is due to the inhomogeneity of data. This demonstrates that the full attention scheme results in undesirable dependencies on which the attention heads focus. THP-S successfully resolves this issue by eliminating such dependencies from the attention heads' span based on spatial closeness of vertices. In this way, THP-S further improves upon THP-F, especially in event prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>We perform ablation study on Retweets and MemeTrack, and we evaluate models by validation log-likelihood. We inspect variants of THP by removing the self-attention and the temporal encoding mechanisms. Moreover, we test the effect of temporal encoding on NHP. Table <ref type="table">7</ref> summarizes experimental results. As shown, both the self-attention module and the temporal encoding contribute to model performance.</p><p>We examine the models' sensitivity to the number of parameters on the Retweets dataset. As shown in Table <ref type="table" target="#tab_4">8</ref>, our model is not sensitive to its number of parameters. Without the recurrent structure, Transformer-based models often have large number of parameters, but our THP model can outperform RNN-based models with fewer parameters. In all the experiments, using a small model (about 100-200k parameters) will suffice. In comparison, NHP has about 1000k and TSES has about 2000k parameters to achieve the best performance, which are much larger than THP. We also include run-time comparison in Table <ref type="table" target="#tab_4">8</ref>. We conclude that THP is efficient in both model size and training speed.</p><p>Table <ref type="table">7</ref>: Log-likelihood of variants of NHP and THP fitted on Retweets and Meme-Track. TE stands for temporal encoding (Eq. 2), and PE stands for positional encoding <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we present Transformer Hawkes Process, a framework for analyzing event streams. Event sequence data are common in our daily life, and they exhibit sophisticated short-term and long-term dependencies. Our proposed model utilizes the self-attention mechanism to capture both of these dependencies, and meanwhile enjoys computational efficiency. Moreover, THP is quite general and can integrate structural knowledge into the model. This facilitates analyzing more complicated data, such as event sequences on graphs. Experiments on various real-world datasets demonstrate that THP achieves state-of-the-art performance in terms of both likelihood and event prediction accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the Transformer Hawkes Process. Each event sequence S is fed through embedding layers and N multi-head self-attention modules. Outputs of the THP are hidden representations of events in S, with history information encoded.</figDesc><graphic coords="5,189.00,446.07,233.99,201.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of event sequences on a graph. Sequences on vertices are aligned temporally to form a long sequence, and relational information among events are shown in arrows. Notice that only the structural information of the last event (the blue circle) and the third to the last event (the purple diamond) are shown. Like before, events cannot attend to future.</figDesc><graphic coords="9,164.21,72.00,280.82,113.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of attention patterns of different attention heads in different layers. Pixel (i, j) in each figure signifies the attention weight of event (t j , k j ) attending to event (t i , k i ). Attention heads in the upper two figures are from the first layer, while they are from the last layer in the lower two figures.</figDesc><graphic coords="14,72.00,184.17,468.00,112.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Prediction error rates of THP, NHP, and RMTPP. Based on a same train-dev-test splitting ratio, each dataset is sampled five times to produce different train, development and test sets. Error bars are generated according to these experiments.</figDesc><graphic coords="15,142.20,229.71,327.60,182.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Log-likelihood and prediction accuracy of NHP, THP, THP with full attention (THP-F), and structured-THP (THP-S) fitted on the 911-Calls (upper two figures) and the Earthquake (lower two figures) datasets. Models are trained using different number of events.</figDesc><graphic coords="16,130.50,72.00,350.99,280.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="13,118.80,281.87,374.40,143.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics. From left to right columns: name of the dataset, number of event types, number of events in the dataset, and average length per sequence.</figDesc><table><row><cell>Dataset</cell><cell>K</cell><cell># events</cell><cell>Avg. length</cell></row><row><cell>Retweets</cell><cell>3</cell><cell>2, 173, 533</cell><cell>109</cell></row><row><cell>MemeTrack</cell><cell>5000</cell><cell>123, 639</cell><cell>3</cell></row><row><cell>Financial</cell><cell>2</cell><cell>414, 800</cell><cell>2074</cell></row><row><cell>MIMIC-II</cell><cell>75</cell><cell>2, 419</cell><cell>4</cell></row><row><cell>StackOverflow</cell><cell>22</cell><cell>480, 413</cell><cell>72</cell></row><row><cell>911-Calls</cell><cell>3</cell><cell>290, 293</cell><cell>403</cell></row><row><cell>Earthquake</cell><cell>2</cell><cell>256, 932</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Log-likelihood comparison.</figDesc><table><row><cell cols="6">Model Retweets MemeTrack Financial MIMIC-II StackOverflow</cell></row><row><cell>RMTPP</cell><cell>-5.99</cell><cell>-6.04</cell><cell>-3.89</cell><cell>-1.35</cell><cell>-2.60</cell></row><row><cell>NHP</cell><cell>-5.60</cell><cell>-6.23</cell><cell>-3.60</cell><cell>-1.38</cell><cell>-2.55</cell></row><row><cell>SAHP</cell><cell>-4.56</cell><cell>-</cell><cell>-</cell><cell>-0.52</cell><cell>-1.86</cell></row><row><cell>THP</cell><cell>-2.04</cell><cell>0.68</cell><cell>-1.11</cell><cell>0.820</cell><cell>0.042</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Event type prediction accuracy comparison. Here FIN is the Financial Transactions dataset, and SO is the StackOverflow dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">FIN MIMIC-II SO</cell></row><row><cell cols="2">RMTPP 61.95</cell><cell>81.2</cell><cell>45.9</cell></row><row><cell>NHP</cell><cell>62.20</cell><cell>83.2</cell><cell>46.3</cell></row><row><cell>TSES</cell><cell>62.17</cell><cell>83.0</cell><cell>46.2</cell></row><row><cell>THP</cell><cell>62.64</cell><cell>85.3</cell><cell>47.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 :</head><label>6</label><figDesc>Event time prediction RMSE comparison. Here FIN is the Financial Transactions dataset, and SO is the StackOverflow dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">FIN MIMIC-II SO</cell></row><row><cell cols="2">RMTPP 1.56</cell><cell>6.12</cell><cell>9.78</cell></row><row><cell>NHP</cell><cell>1.56</cell><cell>6.13</cell><cell>9.83</cell></row><row><cell>TSES</cell><cell>1.50</cell><cell>4.70</cell><cell>8.00</cell></row><row><cell>SAHP</cell><cell>-</cell><cell>3.89</cell><cell>5.57</cell></row><row><cell>THP</cell><cell>0.93</cell><cell>0.82</cell><cell>4.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity to the number of parameters and run-time comparison. Speedup is the speed of THP against NHP.</figDesc><table><row><cell></cell><cell>.</cell><cell></cell><cell># parameters</cell><cell>Log-likelihood THP NHP</cell><cell>Speedup</cell></row><row><cell>Model</cell><cell cols="2">Retweets MemeTrack</cell><cell>100k</cell><cell>-2.090 -6.019</cell><cell>×1.985</cell></row><row><cell>NHP</cell><cell>-5.60</cell><cell>-6.23</cell><cell>200k</cell><cell>-2.072 -5.595</cell><cell>×2.564</cell></row><row><cell>NHP + TE</cell><cell>-2.50</cell><cell>-1.64</cell><cell>500k</cell><cell>-2.058 -5.590</cell><cell>×2.224</cell></row><row><cell>Atten</cell><cell>-5.29</cell><cell>-5.09</cell><cell>1000k</cell><cell>-2.060 -5.614</cell><cell>×1.778</cell></row><row><cell>Atten + PE</cell><cell>-5.25</cell><cell>-4.70</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Atten + TE</cell><cell>-2.03</cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Without causing any confusion, denote H t j as H j .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We use V value to denote the value matrix instead of V, which denotes the vertices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This is a concurrent work that also employs the Transformer architecture, and we only include results reported in their paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The dataset is available on www.kaggle.com/mchirico/montcoalert.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The dataset is provided by China Earthquake Data Center. (http://data.earthquake.cn)</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Likelihood Comparison</head><p>We fit THP and NHP on Retweets and MemeTrack. From Figure <ref type="figure">4</ref>, we can see that THP outperforms NHP during the entire training process by large margins on both of the datasets. The reason is because of the complicated nature of social media data, and RNN-based models such as NHP are not powerful enough to model the dynamics.</p><p>In the Retweets dataset, we often observe time gaps between two consecutive retweets become larger, and this dynamic can be successfully modeled by temporal encoding. Also, unlike RNNbased models, our model is able to capture long-term dependencies that exist in long sequences. In the MemeTrack dataset, we have extremely short sequences, i.e., average sequence length is 3. Even though the data only exhibit short-term dependencies, we still need to model latent properties of memes such as topics and targeted users. We build deep THP models to capture these high-level features, and we remark that constructing deep NHP is not plausible because of the difficulty in training.</p><p>Table <ref type="table">4</ref> summarizes results on other datasets. Note that TSES is likelihood-free. Our THP</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hawkes processes in finance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bacry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mastromatteo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Muzy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Market Microstructure and Liquidity</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1550005</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Borgatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Brass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Labianca</surname></persName>
		</author>
		<title level="m">Network analysis in the social sciences. science</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="892" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent marked temporal point processes: Embedding event history to vector</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coevolve: A joint point process model for information diffusion and network evolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1305" to="1353" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectra of some self-exciting and mutually exciting point processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A self-correcting point process</title>
		<author>
			<persName><forename type="first">V</forename><surname>Isham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Westcott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and Their Applications</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="335" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<title level="m">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<title level="m">Snap datasets: Stanford large network dataset collection</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning temporal point processes via reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering latent network structure in point process data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally self-modulating multivariate point process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Monte Carlo statistical methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Washburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Boyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Bristow</surname></persName>
		</author>
		<title level="m">Stochastic processes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introduction to numerical analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stoer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bulirsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Statistical Methods for the Description and Display of Earthquake Catalogues</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vere-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">V U</forename><surname>Of Wellington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Research</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Victoria University of Wellington. Institute of Statistics and Operations Research), Victoria University of Wellington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Institute of Statistics,</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wasserstein learning of deep generative point process models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling the intensity function of point process via recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Like like alike: joint friendship and interest propagation in social networks</title>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadagopan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Comparative study of cnn and rnn for natural language processing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sch Ütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01923</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kirnap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07561</idno>
		<title level="m">Self-attentive hawkes processes</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Seismic: A selfexciting point process model for predicting tweet popularity</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
