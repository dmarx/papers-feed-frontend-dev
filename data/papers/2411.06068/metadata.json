{
  "arxivId": "2411.06068",
  "title": "Zyda-2: a 5 Trillion Token High-Quality Dataset",
  "authors": "Yury Tokpanov, Paolo Glorioso, Quentin Anthony, Beren Millidge",
  "abstract": "In this technical report, we present Zyda-2: a five trillion token dataset\nfor language model pretraining. Zyda-2 was used to train our Zamba2 series of\nmodels which are state-of-the-art for their weight class. We build Zyda-2 by\ncollating high-quality open-source tokens such as FineWeb and DCLM, then\ndistilling them to the highest-quality subset via cross-deduplication and\nmodel-based quality filtering. Zyda-2 is released under a permissive open\nlicense, and is available at https://huggingface.co/datasets/Zyphra/Zyda-2",
  "url": "https://arxiv.org/abs/2411.06068",
  "issue_number": 0,
  "issue_url": "",
  "created_at": "2025-01-04T06:52:36.611803",
  "state": "open",
  "labels": [
    "paper"
  ],
  "total_reading_time_seconds": 6,
  "last_read": "2025-01-04T06:52:36.612646",
  "last_visited": "2024-12-30T22:18:39.627000+00:00",
  "main_tex_file": null,
  "published_date": "2024-11-09T04:57:41Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.AI"
  ]
}