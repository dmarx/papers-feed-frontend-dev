# Online Locality Meets Distributed Quantum Computing

## Abstract

## 

We connect three distinct lines of research that have recently explored extensions of the classical LOCAL model of distributed computing: A. distributed quantum computing and non-signaling distributions [e.g. STOC 2024], B. finitely-dependent processes [e.g. Forum Math. Pi 2016], and C. locality in online graph algorithms and dynamic graph algorithms [e.g. ICALP 2023].

We prove new results on the capabilities and limitations of all of these models of computing, for locally checkable labeling problems (LCLs). We show that all these settings can be sandwiched between the classical LOCAL model and what we call the randomized online-LOCAL model. Our work implies limitations on the quantum advantage in the distributed setting, and we also exhibit a new barrier for proving tighter bounds. Our main technical results are these:

## Introduction

In this work, we connect three distinct lines of research that have recently explored extensions of the classical LOCAL model of distributed computing:

A. Distributed quantum computing and non-signaling distributions [[3,](#b4)[27,](#b28)[36]](#b37). B. Finitely-dependent processes [[44,](#b45)[45,](#b46)[47]](#b48). C. Locality in online graph algorithms and dynamic graph algorithms [[2,](#)[22]](#b23).

We prove new results on the capabilities and limitations of all of these models of computing, for locally checkable labeling problems (LCLs), with the help of a unifying model that we call randomized online-LOCAL. Our work implies limitations on the quantum advantage in the distributed setting, and we also exhibit a new barrier for proving tighter bounds.

## Highlights

Our main technical results are these: This answers an open question by Holroyd [[44]](#b45). This also presents a new barrier for proving bounds on distributed quantum advantage: all current quantum-LOCAL lower bounds are, in essence, lower bounds for non-signaling distributions, and our result shows that fundamentally different techniques will be needed to solve some of the biggest open questions in this area (e.g., showing that there is no constant-round quantum-LOCAL algorithm for coloring cycles). One of many implications is that in rooted trees, O(log * n) locality in quantum-LOCAL is not stronger than O(log * n) locality in classical LOCAL, and also finitely-dependent distributions are not stronger than O(log * n) locality in classical LOCAL.

We will now proceed to explain what all of these terms and models mean, and how they are connected with each others.

## Roadmap

As our main goal is to unify and relate several distinct models studied in prior work, we will need to introduce a fair number of models of computing. We recommend that the reader keep the roadmap that we have in Fig. [10](#fig_13) (final page) at hand while reading the introduction, in order to maintain a clear view of things, as well as to consult this overview again when needed. We start our adventure in Section 1.3 by introducing the classical models that we have at the very top of Fig. [10](#fig_13) and then relate these to the current landscape of LCLs in Section 1.4. Next, we gradually work our way through the quantum as well as bounded-dependence and non-signaling models in Section 1.5, after which we take our first break. At this point, we are familiar with the top half of Fig. [10](#fig_13), and we are ready to state our first main contributions related to symmetry breaking with finitely-dependent processes in Section 1. [6](#b7).

In Section 1.7, we then turn to models that at first may seem completely unrelated. They deal with locality in sequential, dynamic, and online settings. As we will see in Section 1.8, however, we can connect all of these models into a single hierarchy, with seemingly orthogonal models sandwiched between deterministic LOCAL and randomized online-LOCAL, and we can prove various strong results that connect the complexity landscape between these two extremes.

## Classical models

Let us first recall the definitions of the classical models of distributed graph algorithms [[54,](#b55)[61]](#b62) that form the foundation for our work; we keep it brief here and postpone formal definitions to Section 4.

• Deterministic LOCAL: Our input graph G = (V, E) represents a computer network; each node v ∈ V is a computer and each edge {u, v} ∈ E is a communication link between two computers. Each node is labeled with a unique identifier from {1, 2, . . . , poly(|V |)}. All nodes follow the same distributed algorithm. Initially a node is only aware of its own identifier and its own degree. Computation proceeds in synchronous rounds, and in each round a node can send and receive a message to and from each neighbor and update its state. Eventually each node must stop and announce its local output (its part of the solution, e.g. in graph coloring its own color). The running time, round complexity, or locality of the algorithm is the (worst-case) number of rounds T (n) until the algorithm stops in any n-node graph. • Randomized LOCAL: As above, but each node also has a private source of random bits.

We also define the following variants (see e.g. [[50]](#b51) for more on the impact of shared global information):

• Deterministic LOCAL (shared): Deterministic LOCAL with shared global information. The set of nodes and their unique identifiers is globally known, and hence we also know n = |V |. • Randomized LOCAL (shared): Randomized LOCAL with shared global information and shared randomness. The set of nodes and their unique identifiers is globally known, and in addition to the private sources of random bits, there is also a shared source of random bits that all nodes can access.

We can interpret the shared versions of the models so that we get to see the set of nodes V and their unique identifiers in advance, and we can also initialize the nodes as we want based on this information (and hence in the randomized model, we can also initialize all nodes with the same shared random string), but the set of edges E is only revealed later. This interpretation will be useful especially with the quantum models.

## Landscape of LCL problems

There has been more than three decades of work on understanding the capabilities and limitations of the classical deterministic and randomized LOCAL models, but for our purposes most interesting is the recent line of work that has studied distributed algorithms for locally checkable labeling problems, or LCLs. This is a family of problems first introduced by Naor and Stockmeyer [[57]](#b58). LCL problems are graph problems that can be defined by specifying a finite set of valid neighborhoods. Many natural problems belong to this family: coloring graphs of maximum degree ∆ with ∆ + 1 colors, computing a maximal independent set, finding a maximal matching, etc.

Since 2016, we have seen a large body of work dedicated to understanding the computational complexity of LCL problems in the deterministic and randomized LOCAL models [[4-9, 11, 16, 17, 19, 20, 34, 37, 39, 40, 62]](#), and nowadays there are even algorithms and computer tools available for exploring such questions [[9,](#b10)[23,](#b24)[59]](#b60). As a result of this large international research effort, a landscape of the localities of LCL problems emerges [[64]](#b65). We can classify LCL problems in discrete classes based on their locality, and we also understand how much randomness helps in comparison with deterministic algorithms. Our main goal in this work is to extend this understanding of LCL problems far beyond the classical models, and especially explore what can be computed very fast in models that are much stronger than deterministic or randomized LOCAL.

## Quantum-LOCAL and finitely-dependent processes

We start our exploration of stronger models with distributed quantum computation. The key question is understanding the distributed quantum advantage: what can we solve faster if our nodes are quantum computers and our edges are quantum communication channels? There is a long line of prior work exploring this theme in different models of distributed computing [[3,](#b4)[18,](#b19)[27,](#b28)[33,](#b34)[35,](#b36)[36,](#b37)[43,](#b44)[48,](#b49)[49,](#b50)[51,](#b52)[52,](#b53)[55,](#b56)[[66]](#b67)[[67]](#b68)[[68]](#b69), but for our purposes these are the models of interest:

• Quantum-LOCAL: This model of computing is similar to the deterministic LOCAL model above, but now with quantum computers and quantum communication links. More precisely, the quantum computers manipulate local states consisting of an unbounded number of qubits with arbitrary unitary transformations, the communication links are quantum communication channels (adjacent nodes can exchange any number of qubits), and the local output can be the result of any quantum measurement. • Quantum-LOCAL (shared): Quantum-LOCAL with shared global information and a shared quantum state. As above, but now the algorithm may inspect and manipulate the set of nodes (before any edges are revealed). In particular, it may initialize the nodes with a globally shared entangled state.

As quantum theory intrinsically involves randomness, quantum-LOCAL is at least as strong as randomized LOCAL. There are some (artificial) problems that are known to be solvable much faster in quantum-LOCAL than deterministic or randomized LOCAL [[52]](#b53); however, whether any LCL admits such a quantum advantage is a major open question in the field. Directly analyzing quantum-LOCAL is beyond the scope of current techniques. In essence, the only known technique for proving limitations of quantum-LOCAL is sandwiching it between the classical randomized-LOCAL model and more powerful models than quantum-LOCAL that do not explicitly refer to quantum information. These more powerful models are based on the physical causality principle (a.k.a. non-signaling principle). The idea is perhaps easiest to understand with the help of the following thought experiment: = Y (G ′ )↾ U , it would be possible to use A to transmit information in T time steps between two parties, Alice and Bob, that are within distance T + 1 from each other: Bob holds all nodes of U , and he can, therefore, observe Y (G)↾ U , while Alice controls the graph outside G[U, T ] = G ′ [U, T ], and she can, therefore, instantiate either G or G ′ . This would enable Alice to send a signal to Bob even if no physical communication occurred from Alice to Bob (as they are at distance T + 1 from each other and only T communication steps occurred), and thus violate the non-signaling principle.

This thought experiment suggests the following definition, also known as the φ-LOCAL model and the causal model [[3,](#b4)[36]](#b37):

• Non-signaling model: We can produce an arbitrary output distribution as long as it does not violate the non-signaling principle: for any set of nodes U , modifying the structure of the input graph at more than a distance T (n) from U does not affect the output distribution of U .

We also need to introduce the following definition to better connect our work with the study of finitely-dependent processes and in particular finitely-dependent colorings [[44,](#b45)[45,](#b46)[47]](#b48):

• Bounded-dependence model: We can produce an arbitrary output distribution as long as it does not violate the non-signaling principle, and, furthermore, distant parts are independent: if we fix any sets of nodes U 1 and U 2 such that their radius-T (n) neighborhoods are disjoint, then the output labels of U 1 are independent of the output labels of U 2 . Now if we set T (n) = O(1), algorithms in the bounded-dependence model are in essence what is usually called finitely-dependent processes. Now we can connect all the above models with each others as follows, sandwiching the two versions of quantum-LOCAL between other models (see Appendix A for details; the connection with the non-signaling model is known [[3,](#b4)[36]](#b37) but the connection with the bounded-dependence model is to our knowledge new): 

Here an arrow M 1 → M 2 indicates that an algorithm with locality (or round complexity) T (n) in model M 1 implies an algorithm with the same locality in M 2 . For some problems it is possible to prove near-tight bounds for quantum-LOCAL by using diagram [(1)](#). For example, a very recent work [[27]](#b28) used these connections to prove limits for the distributed quantum advantage in approximate graph coloring: they prove an upper bound for the deterministic LOCAL model and a near-matching lower bound for the non-signaling model.

1.6 Contribution 1: symmetry breaking with finitely-dependent processes Now we are ready to state our first contribution. Recall the following gap result by Chang et al. [21]: all LCL problems that can be solved with locality o(log n) in deterministic LOCAL or with locality o(log log n) in randomized LOCAL can also be solved with locality O(log * n) in deterministic LOCAL. The class of problems with locality Θ(log * n) contains in essence all symmetry-breaking problems: these are problems that could be solved with constant locality if only we had some means of breaking symmetry (e.g. distance-k coloring for some constant k would suffice). In Section 6 we show the following result: Theorem 1.2. Let Π be any LCL problem with locality O(log * n) in the deterministic LOCAL model. Then Π can be also solved with locality O(1) in the bounded-dependence model. Furthermore, the resulting finitely-dependent processes are invariant under subgraph isomorphism. Put otherwise, there is a finitely-dependent distribution over valid solutions of Π. Here the invariance under subgraph isomorphisms implies that, for any two graphs G, H that share some isomorphic subgraphs G ′ and H ′ such that their radius-O(1) neighborhoods are still isomorphic, the finitelydependent processes solving Π restricted to G ′ and H ′ are equal. For any constant d, the task of coloring d-regular trees with d + 1 colors is a problem with locality O(log * n) in the deterministic LOCAL model. Hence, we can answer the open question by Holroyd [44]: Corollary 1.3. For each d ≥ 2, there is a finitely-dependent coloring with d + 1 colors in d-regular trees. Furthermore, the resulting process is invariant over subgraph isomorphisms.

More specifically, there exists a finitely-dependent 4-coloring distribution of the infinite 3-regular tree that is invariant under automorphisms.

Theorem 1.2 also introduces a formal barrier for proving limitations on distributed quantum advantage. Recall that all current quantum-LOCAL lower bounds are, in essence, lower bounds in the non-signaling model. Before our work, there was a hope that we could discover a symmetry-breaking problem Π with the following properties: (1) its locality is O(log * n) in deterministic LOCAL, and (2) we can show that its locality is Ω(log * n) in the non-signaling model, and therefore (3) Π cannot admit any distributed quantum advantage. However, our work shows that no such problem Π can exist. In particular, arguments related to non-signal distributions are not sufficient to exclude distributed quantum advantage in this region.

## Locality in online and dynamic settings

Let us now switch gears and consider a very different line of work. Ghaffari et al. [[38]](#b39) introduced a sequential counterpart of the classical LOCAL model:

• Deterministic SLOCAL model: The nodes are processed in an adversarial order. When a node v is processed, the algorithm gets to see all information in its radius-T (n) neighborhood (including states and outputs of previously processed nodes). The algorithm has to label v with its local output, and the algorithm can also record other information in v, which it can exploit when other nodes near v are later processed. • Randomized SLOCAL model: As above, with access to a source of random bits.

Clearly SLOCAL is stronger than LOCAL. One key feature is that the processing order naturally breaks symmetry, and all symmetry-breaking LCLs can be solved with O(1) locality in SLOCAL.

One interpretation of our first contribution is that we also establish a new, unexpected similarity between SLOCAL and the bounded-dependence model: both are able to solve any symmetry-breaking LCL with constant locality.

A recent work [[2]](#) introduced the following models that capture the notion of locality also in the context of centralized dynamic graph algorithms and centralized online graph algorithms:

• Deterministic dynamic-LOCAL model: The adversary constructs the graph one edge at a time. The algorithm has a global view of the graph (including states and outputs of previously processed nodes), but it has to maintain a feasible solution after each update. The algorithm is restricted so that after a modification at node v, it can only update the solution within distance T (n) from v. • Deterministic online-LOCAL model: The adversary presents the input graph one node at a time. When a node v is presented, the adversary also reveals the radius-T (n) neighborhood of v. The algorithm has to then choose the output label of v. Crucially, the algorithm has access to a global view of the graph (including states and outputs of previously processed nodes). This model can be seen as a stronger version of the deterministic SLOCAL model where all nodes have access to some global shared memory.

Both SLOCAL and dynamic-LOCAL can be sandwiched between LOCAL and online-LOCAL [[2]](#):

$deterministic LOCAL deterministic SLOCAL deterministic dynamic-LOCAL deterministic online-LOCAL(2)$There are also some problems in which deterministic online-LOCAL is much stronger than deterministic LOCAL: 3-coloring in bipartite graphs has locality Θ( √ n) in deterministic LOCAL [[17,](#b18)[27]](#b28) but O(log n) in online-LOCAL [[2]](#); very recently Chang et al. [[22]](#b23) also showed that this is tight for online-LOCAL.

## Contribution 2: connecting all models for LCLs in rooted trees

At first sight, the models discussed in Sections 1.5 and 1.7 seem to have very little in common; they seem to be orthogonal extensions of the classical deterministic LOCAL model. Furthermore, we have already seen evidence that online-LOCAL can be much stronger than deterministic LOCAL. Nevertheless, we can connect all these models in a unified manner, and prove strong limits on their expressive power. To this end, we introduce yet another model:

• Randomized online-LOCAL model: Like deterministic online-LOCAL, but the algorithm has access to a source of random bits, and we play against an oblivious adversary (the adversary fixes the graph and the order of presenting nodes before the algorithm starts to flip coins).

Trivially, this is at least as strong as all models in diagram (2). However, the big surprise is that it is also at least as strong as all models in diagram (1). In Section 5 we prove:

Theorem 1.4. Any LCL that can be solved in the non-signaling model with locality T (n) can also be solved in the randomized online-LOCAL model with the same locality.

Then we zoom into the case of rooted trees in Section 7 and prove:

Theorem 1.5. Any LCL on rooted trees that can be solved in the randomized online-LOCAL model with locality o(log log log n) can also be solved in the deterministic LOCAL model with locality O(log * n).

Together with Theorem 1.2 and previously-known results, we also obtain the following corollary:

Corollary 1.6. In rooted trees, the following families of LCLs are the same:

• locality O(log * n) in deterministic or randomized LOCAL or quantum-LOCAL,

• locality O(1) in bounded-dependence model, non-signaling model, deterministic or randomized SLOCAL, dynamic-LOCAL, or deterministic or randomized online-LOCAL.

In rooted trees, there is no LCL problem with locality between ω(log * n) and o(log log log n) in any of these models: deterministic and randomized LOCAL, quantum-LOCAL, boundeddependence model, non-signaling model, deterministic and randomized SLOCAL, dynamic-LOCAL, and deterministic and randomized online-LOCAL.

In particular, when we look at LCLs in rooted trees, O(log * n)-round quantum algorithms are not any stronger than O(log * n)-round classical algorithms. (However, it is still possible that there are some LCLs in trees that can be solved in O(1) rounds in quantum-LOCAL and that require Θ(log * n) rounds in deterministic LOCAL; recall the discussion in Section 1.6.) Theorem 1.5 can be seen as an extension of the result of Akbari et al. [[2]](#) that connects deterministic online-LOCAL with LOCAL for LCLs on rooted regular trees without inputs. Our result is applicable to the randomized online-LOCAL (and hence we can connect it with the non-signaling and quantum models), and it holds for any LCLs on rooted trees (possibly with irregularities and inputs). Recall that the presence of inputs makes a huge difference already in the case of directed paths [[5,](#b6)[23]](#b24), and we need fundamentally new ideas, as we can no longer build on the classification from [[9,](#b10)[11]](#b12).

## Big picture

By putting together all our contributions (including some auxiliary results that we discuss later in Section 3), the landscape shown in Fig. [10](#fig_13) emerges. We can sandwich all models between deterministic LOCAL and randomized online-LOCAL. Going downwards, we get symmetry breaking for free (as indicated by the blue arrows). And in the case of rooted trees, for the low-locality region o(log log log n), we can also navigate upwards (as indicated by the orange arrows). Table [1](#) gives some concrete examples of localities for LCL problems across the landscape of models.

## Model

Symmetry-breaking problems 3-coloring in bipartite graphs

$Deterministic LOCAL Θ(log * n) by definition Θ( √ n) [27] Randomized LOCAL Θ(log * n) [19] Θ( √ n) [27] Quantum LOCAL O(log * n) trivial Θ( √ n) [27] Bounded-dependence O(1) Theorem 6.16 Θ( √ n) [27] Deterministic SLOCAL O(1) [38] Õ( √ n), n Ω(1) [2, 27] Randomized SLOCAL O(1) [38] Õ( √ n), n Ω(1) [2, 27] Deterministic dynamic-LOCAL O(1) Theorem 9.2 Õ( √ n), Ω(log n) [22, 27] Deterministic online-LOCAL O(1) [2] Θ(log n) [2, 22] Randomized online-LOCAL O(1) [2] Θ(log n) Theorem 8.1$Table 1: Examples of localities across the models. Here symmetry-breaking problems refer to LCL problems with locality Θ(log * n) in the deterministic LOCAL model; this includes many classical problems such as maximal independent set, maximal matching, and (∆ + 1)-vertex coloring.

## Overview of techniques and key ideas

In this section, we give an overview of the techniques and key ideas that we use to prove our main results, and we also provide a roadmap to the rest of this paper. We note that our first contribution is presented in Section 6, while the second contribution comes before it in Section 5-the proofs are ordered this way since Section 5 also develops definitions that will be useful in Section 6.

## Bounded-dependence model can break symmetry (Section 6)

Let us first present an overview of the proof of Theorem 1.2 from Section 1.6. We show that the bounded-dependence model can break symmetry with constant locality; that is, there is a finitely-dependent process for any symmetry-breaking LCL.

It is well known that any LCL problem Π that has complexity O(log * n) in the LOCAL model has the following property: there exists a constant k ∈ N + (that depends only on the hidden constant in O(log * n)) such that, if the graph is given a distance-k coloring (with sufficiently small number of colors) as an input, then Π is solvable in time O(1) in the LOCAL model (using the distance-k coloring as a local assignment of identifiers) [[20]](#b21).

We prove that for each bounded-degree graph, there is a finitely-dependent process providing a distance-k coloring for constant k. Then, we can combine such a process with the LOCAL algorithm that solves the problem with locality O(1) if a distance-k coloring is given, and we prove that the resulting process is still a finitely-dependent distribution. Furthermore, we also prove that all these processes are invariant under subgraph isomorphisms (even those that do not preserve node identifiers), meaning that, for any two graphs G and H sharing two isomorphic subgraphs with isomorphic radius-O(1) neighborhoods, the restrictions of the finitely-dependent processes solving Π over G and H restricted to G ′ and H ′ are equal in law.

One of the key observations that we use is that LOCAL algorithms that do not exploit the specific assignment of node identifiers and do not depend on the size of the graph provide finitely-dependent distributions that are invariant under subgraph isomorphisms whenever the input labeling for the graphs is invariant under subgraph isomorphisms.

Overview. The cornerstone of our proof is a surprising result by Holroyd and Liggett [[45]](#b46) and its follow-up in [[47]](#b48), that state that there exist k-dependent distributions giving a q-coloring of the infinite path and of cycles for (k, q) ∈ {(1, 4), [(2,](#)[3)](#b4)} that are invariant under subgraph isomorphisms.

Recently, Holroyd has combined the finitely-dependent distributions of infinite paths to provide a finitely-dependent 4-coloring of the d-dimensional lattice [[44]](#b45). Getting a translation invariant distribution is quite easy: First, use the distributions for the paths on each horizontal and vertical path obtaining a distance-k coloring (with k being a large enough constant) of the lattice with constantly many colors as shown in [[45,](#b46)[Corollary 20]](#). Second, apply some LOCAL algorithm that starts from a distance-k coloring and reduces the number of colors to 4 while keeping the resulting distribution symmetric (e.g., the algorithms from [[10,](#b11)[17]](#b18)). The major contribution of [[44]](#b45) is transforming such a distribution into a process that is invariant under subgraph isomorphisms. However, this symmetrization phase is quite specific to the considered topology.

We come up with a new approach that obtains similar results in all bounded-degree graphs through the following steps:

1. We show that the finitely-dependent coloring of paths and cycles can be combined to obtain finitely-dependent 3-coloring distributions of rooted pseudoforests of bounded-degree that are invariant under subgraph isomorphisms.

2. We observe that all graphs of bounded-degree admit a random decomposition in rooted pseudoforests that satisfies the required symmetry properties. 3. We prove that such a random decomposition can be combined with the finitely-dependent 3-coloring of rooted pseudoforests to obtain finitely-dependent distributions that give a (∆ + 1)coloring of graphs of maximum degree ∆ that are invariant under subgraph isomorphism. 4. We show that we can use this finitely-dependent (∆ + 1)-coloring distributions to provide a distance-k coloring for bounded-degree graphs, which is enough to simulate any O(log * n)round LOCAL algorithms that solves an LCL Π. Such combinations result in finitely-dependent processes that are invariant under subgraph-isomorphisms and solve Π.

Notice that, in spirit, steps 1 to 3 are similar to the steps needed to produce a (∆ + 1)-coloring in time O(log * n) in the LOCAL model [[42,](#b43)[60]](#b61): however, the detailed way these steps are obtained in the bounded-dependence model is quite different and requires a careful analysis.

Step 1: Finitely-dependent 3-coloring distributions of rooted pseudoforests. A rooted pseudoforest is a directed graph in which each node has outdegree at most 1. Let us now fix any rooted pseudoforest of maximum degree ∆. Consider the following process: each node v colors its in-neighbors with a uniformly randomly permutation of {1, . . . , indeg(v)}. The graph G i induced by nodes colored with color i is a union of directed paths and cycles (see Fig. [1](#fig_0)) and, hence, admits a finitely-dependent 4-coloring given by [[45]](#b46) that is invariant under subgraph isomorphisms; if a node is isolated, it can deterministically join any of the G i s, say G 1 . The sequence of graphs (G 1 , . . . , G ∆ in ) is said to be a random ∆ in -decomposition of the rooted pseudoforest. Furthermore, if two graphs G, H have isomorphic subgraphs G ′ , H ′ (together with some constant-radius neighborhoods), the decompositions in directed paths and cycles induced in G ′ and H ′ have the same distribution (because node colors are locally chosen uniformly). We prove that the combination of the random decomposition and the finitely-dependent coloring yields a finitely-dependent 4∆-coloring which is invariant under subgraph isomorphisms: by further combining such distribution with the Cole-Vishkin color reduction algorithm [[29,](#b30)[42,](#b43)[60]](#b61) (that has complexity O(log * k) with k being the size of the input coloring), we can obtain a finitely-dependent 3-coloring distribution for rooted pseudoforests of maximum degree ∆ that is invariant under subgraph isomorphisms.

Steps 2-3: Finitely-dependent (∆ + 1)-coloring distribution of bounded-degree graphs. First, if the input graph is undirected, make it a directed graph by duplicating all edges and assigning both orientations to duplicated edges. Since a coloring of the nodes can be given in both cases equivalently, we focus on the directed case for simplicity. Second, consider the following process: each node v labels its out-edges with a uniformly sampled permutation of the elements of {1, . . . , outdeg(v)}; this way we obtain a random decomposition of the edges of the graph into rooted pseudoforests, as each node has at most one out-edge with label i. Furthermore, if two graphs G, H have isomorphic subgraphs G ′ , H ′ (together with some constant-radius neighborhoods), the decompositions induced in G ′ and H ′ have the same distribution (because edge labelings are locally chosen uniformly). We prove that if we apply the finitely-dependent 3-coloring from step 1 to each pseudoforest, we obtain a finitely-dependent 3 ∆ -coloring of the input graph which is invariant under subgraph isomorphisms. By further combining such a distribution with a variant of the Cole-Vishkin color reduction algorithm, we obtain a finitely-dependent (∆ + 1)-coloring distribution for bounded-degree graphs of maximum degree ∆ that is invariant under subgraph isomorphisms.

Step 4: Finitely-dependent distribution solving Π. Consider any graph G of maximum degree ∆, and its k-th power graph defined as follows: simply add edges to G between each pair of nodes at distance at most k, where k is some large enough constant. Observe that G k is a graph of maximum degree ∆ k . Now, step 3 implies that there is a finitely-dependent (∆ k + 1)-coloring of G k that is invariant under subgraph isomorphisms: such distribution yields a distance-k coloring of G. For any LCL Π that has complexity O(log * n) in LOCAL, we know that there exists a constant k such that, if given a distance-k coloring in input (with a constant number of colors), then there is an O(1)-round port-numbering algorithm solving Π [[20]](#b21): the combination of the input distance-k coloring of G given by step 2-3 with such an algorithm yields a finitely-dependent distribution solving Π that is invariant under subgraph isomorphisms.

Random decomposition of a graph. In steps 1 and 3 we proceed in an analogous way: First, we construct a process that induces a random decomposition of a graph. Second, we consider finitely-dependent distributions of output labelings over the outputs of the random decomposition. The combination of the random decomposition and the finitely-dependent distributions gives rise to a process over the whole graph. In Section 6, we derive a general result (Lemma 6.8) which gives sufficient conditions on the random decomposition and the finitely-dependent distributions in order to ensure the final process is still finitely-dependent (possibly with symmetry properties). Lemma 6.8 is then the tool used in practice in steps 1 and 3.

Independent related work. Very recently, an independent and parallel work provided a finitelydependent coloring of bounded-degree graphs with exponentially many colors (in the degree of the graph) [[65]](#b66). The technique employed in [[65]](#b66) is very similar to ours: it exploits the decomposition of graphs in rooted pseudoforests, and then colors rooted pseudoforests using the finitely-dependent coloring of paths and cycles [[45,](#b46)[47]](#b48). However, [[65]](#b66) stops at the mere coloring problem and does not make use of color reduction algorithms, which are the key ingredient for extending results to all symmetry-breaking LCLs.

## Simulating non-signaling in randomized online-LOCAL (Section 5)

Let us now give the intuition behind the proof of Theorem 1.4 from Section 1.8: we show that the non-signaling model can be simulated in randomized online-LOCAL without any loss in the locality.

A randomized online-LOCAL algorithm is given in input the size of the input graph, and a distribution that is non-signaling beyond distance T and that solves some problem Π over some graph family F. When the adversary picks any node v 1 and shows to the randomized online-LOCAL algorithm its radius-T neighborhood, the randomized online-LOCAL algorithm simply goes over all graphs of n nodes in F until it finds one, say H 1 , that includes the radius-T neighborhood of v 1 : then, it samples an output according to the restriction of the non-signaling distribution in H 1 to v 1 . Notice that such distribution does not change if the topology of the graph changes outside the radius-T neighborhood of v 1 . Recursively, when the adversary picks the i-th node v i , the randomized online-LOCAL algorithm goes over all graphs of n nodes in F until it finds one, say H i , that includes the union of radius-T neighborhoods of v 1 , . . . , v i (it must necessarily exist as the graph chosen by the adversary is a valid input): hence, it samples an output according to the restriction of the non-signaling distributions in H i to v i conditional on the outputs of v 1 , . . . , v i-1 . We prove that the non-signaling property ensures that the algorithm described above fails with at most the same probability of failure of the non-signaling distribution.

## Online-LOCAL can be simulated in SLOCAL for rooted trees (Section 7)

Next we give an overview of the proof of Theorem 1.5 from Section 1.8: we show that a randomized online-LOCAL algorithm that solves an LCL problem in rooted trees with locality o(log log log n) can be simulated in the deterministic SLOCAL model with locality O(1), and therefore also in the deterministic LOCAL model with locality O(log * n).

Online-LOCAL algorithms can be seen simply as SLOCAL algorithms with global memory. Notice that SLOCAL algorithms instead only have a form of "incremental" memory, i.e., they keep track of an incremental sequence of intersecting neighborhoods N 1 , N 2 . . . ., where N i intersects N i+1 .

The new ingredient we use in this section are component-wise online-LOCAL algorithms. Roughly speaking, a component-wise algorithm is a deterministic online-LOCAL algorithm that, when processing a node v, uses information only coming from the connected component of the input graph that has been revealed so far to which v belongs, and nothing else. (If two or more components are merged, then the algorithm may use information it knows from any component.)

We prove Theorem 1.5 in three steps:

1. We first show that any randomized online-LOCAL algorithm solving an LCL with locality T (n) can be turned into a deterministic component-wise online-LOCAL algorithm with locality

$T (2 O(2 n 2 )$). 2. We then prove that, for LCLs on rooted trees, we can simulate the component-wise algorithms in SLOCAL. 3. Finally, we show that SLOCAL algorithms solving any LCL Π with locality o(log n) over rooted trees can be turned into an O(log * n)-round LOCAL algorithm solving Π over rooted trees.

Step 1: Constructing component-wise algorithms from deterministic online-LOCAL algorithms. To give some intuition, consider an LCL problem Π on a family F of graphs that is closed under disjoint graph union and node and edge removals. Suppose there is a deterministic online-LOCAL algorithm A solving Π with locality T (n) on n-node graphs. Note that the output label A chooses for a node may depend arbitrarily on everything the algorithm has seen so far.

We show how to turn algorithm A into an algorithm whose output for isolated nodes depends only on the local topology and inputs and is oblivious to any previously-processed nodes. We call such algorithms 1-amnesiac. Here with isolated we mean the node v is such that all nodes belong to the radius T -neighborhood around v are new to algorithm A, that is, A has no knowledge of how v is connected (if at all) to the parts of the graph it has seen thus far.

Let

$N 1 = |Σ out | |Σ in | n • 2 n 2 • n 2$, and let G 1 be the set of all possible subgraphs of any n-node graphs (from F) with inputs (up to isomorphisms) that are the radius-T (N 1 ) neighborhood of some node, which we call the center of the neighborhood. Let

$g 1 = |G 1 | and notice that g 1 ≤ 2 n 2 |Σ in | n .$Consider now the following experiment:

$1. Construct a simulation graph H 1 that consists of |Σ out | • n copies of all graphs in G 1 . The size of H 1 is at most N 1 = n 2 |Σ out | • g 1 ≤ |Σ out | |Σ in | n • 2 n 2 • n 2 . 2.$Reveal the center node of each of those neighborhood graphs to A in an arbitrary order with locality T (N 1 ). For each type of radius-T (N 1 ) neighborhood T 1 (i.e., any element of G 1 ), there exists some output label σ T 1 that occurs at least n times. We call such neighborhoods good and such a label a canonical labeling of T 1 . 3. Continue labelling nodes of H 1 using A under an arbitrary ordering of the nodes.

We describe a new online-LOCAL algorithm B that, using this experiment, produces a correct labeling. Let G be an input graph n nodes, and let node v be revealed to B along with its radius-T ′ (n) neighborhood, where T ′ (n) = T (N 1 ). Whenever v is an isolated node, algorithm B finds a "fresh" (i.e., not previously chosen) good neighborhood in the experiment graph matching the radius-T (N 1 ) neighborhood of v in G. It then takes that unused good neighborhood, identifies all nodes with the revealed input neighborhood, and labels v accordingly. An unused good neighborhood always exists since there are at least n good neighborhoods in the experiment graph matching the radius-T (N 1 ) neighborhood of v in G. Algorithm B effectively cuts and pastes the neighborhood from the experiment graph to the actual input graph without algorithm A noticing. For non-isolated nodes, B just simulates what A would do, following the adversarial order of the nodes presented to B. This is always possible because the labels of isolated nodes come from valid online-LOCAL runs of A.

The correctness of algorithm B follows from that of A. Moreover, when labeling an isolated node, B always labels it in the same way that depends only on the local structure and inputs of the graph; hence B is 1-amnesiac. Clearly, there is an exponential overhead in the locality: the locality of B is

$T ′ (n) = T (N 1 ) = T (2 O(n 2 ) ).$Using the above, we now describe how to obtain a 2-amnesiac algorithm, that is, an algorithm that always produces the same labels for the same types of connected components formed by the union of intersecting neighborhoods of two distinct nodes. We modify the previous experiment as follows:

1. First we must increase the size of the experiment graph. Let

$N 2 = |Σ out | 2 |Σ in | n • 2 n • n 2 and redefine G 1 with the radius-T (N 2 ) neighborhoods. 2. Instead of considering |Σ out | • n many disjoint copies of elements of G 1 , we now take |Σ out | 2 • n copies.$By the same argument as before, there are at least n • |Σ out | good neighborhoods. 3. Let G 2 be the set of all possible unions of two non-disjoint radius-T (N 2 ) neighborhoods of two different nodes of any n-node graph (in F), with all possible input labelings and orderings of the center nodes. Notice that the size of

$G 2 is g 2 ≤ 2 n 2 |Σ in | n .$We take |Σ out | • n many disjoint copies of all graphs in G 2 , with the catch that the neighborhood of the center node that comes first in the processing order is chosen arbitrarily among the good neighborhoods. The resulting graph H 2 is our new experiment graph, whose size is now

$N 2 ≤ |Σ out | |Σ in | n • 2 n 2 • n 3 . 4.$Use A to label all nodes that come first in the input order in each graph, then all nodes that come second in the same respective order. 5. Use A to label the second center node of each connected component. By the pigeonhole principle, for all types of such connected components T 2 there are at least n identical labelings. These make out the canonical labelings σ T 2 .

The 2-amnesiac algorithm B starts by running the above experiment. When given in input an n-node graphs, it outputs the canonical labeling σ T 1 for isolated nodes whose radius-T ′ (n) = T (N 2 ) neighborhood matches type T 1 . Similarly, B outputs the canonical labeling σ T 2 for nodes seeing a connected component of type T 2 when we look at their radius-T ′ (n) = T (N 2 ) neighborhood. The correctness argument is the same as that for 1-amnesiac algorithms. For the remaining nodes, B just simulates A using global memory as usual.

We can continue this process all the way up to n-amnesiac algorithms, which are simply component-wise online-LOCAL algorithms. See Lemma 7.5 for the formal details. As a remark, notice the restriction to LCLs is necessary to prove correctness: if the amnesiac algorithm fails, it must fail locally; hence also the original online-LOCAL algorithm fails locally, contradicting its correctness.

Dealing with randomness. Let us now turn to the setting where A is randomized. For deterministic online-LOCAL algorithms, we adaptively picked the good neighborhoods before processing further; however, since in randomized online-LOCAL the adversary is oblivious, we must adapt our strategy.

Our experiment graph H n is now random. It is constructed exactly as before, though now the good connected components have to be "guessed" uniformly at random each time. Clearly, the probability that our guesses are good is incredibly small. Hence we would like to amplify the success probability so that the probability that the guesses are good enough and the randomized online-LOCAL algorithm works correctly is bounded away from zero. (Once we have this, we may apply a standard derandomization argument.) The amplification is by replicating k times the experiment graph H n , guessing the good components for each copy of H n independently. Since the randomness of the algorithm and the randomness of these guesses are independent for each copy of H n , a simple union bound together with the inclusion-exclusion principle give a positive probability that, in at least one copy of H n , the guesses are correct and the algorithm works properly.

We now apply a standard derandomization argument: Since there exists an assignment of a random bit string to the randomized online-LOCAL algorithm as well as for "guessing" the good components in the k copies of the experiment graph, there is a fixed, deterministic realization of H n that yields a (correct) deterministic online-LOCAL algorithm. Hence our n-amnesiac algorithm B goes over all possible definitions of H n and of deterministic online-LOCAL algorithms (according to an arbitrary order) until it finds this good combination. It must eventually succeed because, when the size of the input graph is fixed, there are only finitely many combinations. The proof then reduces to the previous case. See [Lemma 7.6](#) for the formal details.

Step 2: From component-wise algorithms to SLOCAL algorithms on rooted trees. We heavily exploit the fact that, in rooted trees, there is a consistent orientation of the edges towards the root. We adapt results from [[23,](#b24)[Section 7]](#) to the SLOCAL model to show how to "cluster" a rooted tree in connected components (which are also rooted trees) in time O(α) and so that the following properties are met:

1. All connected components have depth Θ(α). 2. All leaves of a connected component that are not at distance Θ(α) from the root of the component are real leaves of the original rooted tree. 3. All other leaves are either real leaves or roots of other connected components.

The formal details can be found in Lemma 7.8.

Suppose now we are given an n-amnesiac (equivalently, component-wise) algorithm A solving an LCL Π on rooted trees with locality T (n). We briefly describe how to construct an SLOCAL algorithm B that solves Π with locality O(T (n)) on rooted trees. Recall that in SLOCAL we may compose two algorithms with localities T 1 and T 2 and obtain an SLOCAL algorithm with locality O(T 1 + T 2 ) [[38]](#b39). Algorithm B is the composition of the following four algorithms with locality O(T ):

1. B 1 constructs a clustering with properties 1-3 above with α = O(T ), where the hidden constant is large enough. 2. B 2 ensures each root of a connected component of the cluster precommits a solution in the neighborhood of the leaves of the component using A. This can be done independently between different components if the localities are appropriately chosen. The root of the original tree also outputs a solution for itself. 3. B 3 outputs the precommitments on the nodes designated by B 2 . 4. The fact that we used A independently on disjoint components of the graph (and A being correct) ensures that a solution to the LCL exists and the "inner part" of all connected components can be completed with a correct solution. Hence B 4 simply brute-forces a solution inside the clusters.

Since Π is an LCL, different components will have compatible solutions as B 2 precommitted a solution around leaf nodes, which are roots of other connected components.

Step 3: From SLOCAL algorithms to LOCAL algorithms on rooted trees. Any o(log n)time SLOCAL algorithm A solving an LCL over rooted trees can be turned into an O(1)-time SLOCAL algorithm B achieving the same. This is obtained by exploiting the same tree decomposition described above and providing fake, repeating identifiers to each cluster so that two equal identifiers are "far enough". Then, we use A as a black-box and lie to A by providing the size of a cluster as the input graph size. Once we have an O(1)-time SLOCAL algorithm B, how to turn it into an O(log * n)-time LOCAL algorithm is folklore.

## Additional results

We will now discuss some additional results that help to establish the missing parts of the big picture in Fig. [10](#fig_13), and provide further intuition and examples on these models and their key properties.

## Lower bound on 3-coloring in randomized online-LOCAL (Section 8)

So far we have connected randomized online-LOCAL with other models through simulation arguments that only work in rooted trees. Let us now put limitations on randomized online-LOCAL in a broader setting. Recall that in deterministic online-LOCAL we can 3-color bipartite graphs with locality O(log n) [[2]](#), and this is tight [[22]](#b23). In Section 8 we show that randomness does not help:

Theorem 3.1. 3-coloring in bipartite graphs is not possible with locality o(log n) in the randomized online-LOCAL model.

This demonstrates that even though randomized online-LOCAL is a very strong model, strong enough to simulate e.g. any non-signaling distribution, it is nevertheless possible to prove strong lower bounds in this model (which then imply lower bounds across the entire landscape of models).

Here it is important to assume that the adversary is oblivious, i.e., it cannot see the random decisions of the randomized algorithm. This lower bound complements a recent result of Chang et al. [[22]](#b23) showing the same bound for the deterministic online-LOCAL model.

In this proof, we use the notion of a b-value defined in [[22]](#b23) as a measure of the number of incompatible boundaries present in a region of a grid. We start with the assumption that a grid can be 3-colored with locality o(log n) and derive a contradiction. The high level idea is to construct two path segments below each other, where one path segment has a large count of incompatible boundaries (a high b-value) and the other segment has a low boundary count (incompatible b-value to the upper path). This forces an algorithm to make boundaries escape on the side between the two segments. We show that the boundary count is, however, too large compared to the distance between the two segments and thus the boundaries "cannot escape".

Two difficulties arise in the randomized case compared to the deterministic lower bound: (i) In order to create a path with a large b-value we have to use a probabilistic construction that produces a segment with a large b-value with high probability; (ii) Since the first construction is probabilistic and the adversary oblivious, we cannot "see" the large b-value segment constructed in (i), that is, we can neither predict its position nor its size. We therefore need to use another probabilistic construction that positions the segment in a position that forces a contradiction (and which succeeds with constant probability).

## Deterministic dynamic-LOCAL can derandomize LOCAL (Section 9)

It is known that deterministic SLOCAL is strong enough to simulate randomized LOCAL. In Section 9 we show that the same holds also for dynamic-LOCAL, using the method of conditional expectation in a manner similar to how the result is proved for SLOCAL: Theorem 3.2. Deterministic dynamic-LOCAL can simulate randomized LOCAL.

We also show that dynamic-LOCAL can break symmetry for free, similar to SLOCAL and the bounded-dependence model: if we can solve an LCL problem with O(log * n) locality in LOCAL, we can solve it with O(1) locality in dynamic-LOCAL.

## Randomized online-LOCAL with adaptive adversary (Section 10)

The new model that we introduced, randomized online-LOCAL, is defined using an oblivious adversary. In Section 10 we show that this is also necessary: if we defined randomized online-LOCAL with an adaptive adversary, it would be as weak as the deterministic online-LOCAL model.

We show that an adaptive adversary in randomized online-LOCAL is so strong that a succeeding randomized online-LOCAL algorithm of locality T would admit a single random-bit string that outputs a good solution for all possible graphs of a given size: hence, a correct deterministic online-LOCAL algorithm exists. Since deterministic online-LOCAL algorithms of locality T for graphs of n nodes are only finitely many, the deterministic online-LOCAL algorithm in its initialization phase can go over all of them until it finds the one working for all graphs of n nodes: it then uses that one.

## Randomized online-LOCAL in paths and cycles (Section 11)

Our final technical part shows that LCL problems in paths and cycles have complexity either O(1) or Θ(n) in the randomized online-LOCAL model; moreover the locality is O(1) in randomized online-LOCAL if and only if it is O(log * n) in the deterministic LOCAL model. Together with prior work, this also shows that locality of an LCL problem in paths and cycles is decidable across all models [[5]](#b6) (with the caveat that we cannot distinguish between O(1) and Θ(log * n) for quantum-LOCAL). The proof is a reworking of its deterministic variant from [[2]](#). The main take-home message of this result is the following: cycles are not a fundamental obstacle for simulating randomized online-LOCAL in weaker models. Hence, there is hope for generalizing the simulation result of Section 7 from trees to a broader class of graphs.

## Quantum, bounded-dependence, and non-signaling (Appendix A)

Appendix A aims at serving a dual purpose. First, it aims at formally introducing the non-signaling model based on the non-signaling principle, and at explaining why it is more powerful than the quantum-LOCAL model. This is not a new result, but included for completeness and to clarify the early works of Gavoille et al. [[36]](#b37) and Arfaoui and Fraigniaud [[3]](#b4). Second, it formally introduces the bounded-dependence model based on finitely-dependent processes and argues why the relations in diagram (1) hold, and in particular why quantum-LOCAL without shared quantum state is contained not only in the non-signaling model but also in the bounded-dependence model. While all the ingredients are well-known, to our knowledge this relation between the quantum-LOCAL model and the bounded-dependence model is not made explicit in the literature before.

## Open questions

Our work suggests a number of open questions; here are the most prominent ones: Question 1. Is quantum-LOCAL stronger than randomized LOCAL for any LCL problem? In particular, is there any LCL problem with constant locality in quantum-LOCAL but super-constant locality in LOCAL? We conjecture that such a problem does not exist. Our work proves that to show this conjecture, new proof techniques are needed. Question 2. Does shared global information or shared quantum state ever help with any LCL problem (beyond the fact that shared quantum state can be used to generate shared randomness, which is known to help [[13]](#b14))? Question 3. Is it possible to simulate deterministic or randomized online-LOCAL in SLOCAL and LOCAL also in a broader graph class than rooted trees? If we could extend the result to unrooted trees, it would also imply new lower bounds for the widely-studied sinkless orientation problem [[12,](#b13)[16]](#b17) across all models.

## Follow-up work

Since the first versions of this work originally appeared in arXiv earlier this year, this work has already influenced at least two follow-up papers:

1. Dhar et al. [[32]](#b33) study the relation between online-LOCAL and deterministic LOCAL in much greater depth, especially outside the o(log log log n) region. It turns out that e.g. in regular rooted trees the complexity classes of LCL problems in randomized online-LOCAL and deterministic LOCAL exactly coincide, and as a corollary all models in Fig. [10](#fig_13) are exactly as strong for this graph family in the Ω(log * n) region. 2. Balliu et al. [[13]](#b14) show that there are LCL problems in which shared randomness strictly helps.

This implies a number of new separations between the models in Fig. [10](#fig_13); for example, quantum-LOCAL with shared quantum state is strictly stronger than quantum-LOCAL without shared quantum state, the non-signaling model is strictly stronger than the bounded-dependence model, and randomized online-LOCAL is strictly stronger than randomized SLOCAL.

Put together, these papers demonstrate that especially in restricted graph classes such as trees, one can indeed often cover the entire hierarchy of models with a single theorem about randomized online-LOCAL, while in general we are dealing with a large number of genuinely distinct models.

## Preliminaries

We use the notation N = {0, 1, 2, . . . } and N + = N \ {0}. For any positive integer n ∈ N + , we denote the set {1, . . . , n} by [n].

Graphs. In this work, a graph G = (V, E) can be either directed (E ⊆ V 2 ) or undirected (E ⊆ V 2 ). If the set of nodes and the set of edges are not specified, we refer to them by V (G) and E(G), respectively. For any edge e = (u, v) ∈ E(G) we say that e is directed from u to v and is incident to u and to v (the latter holds also for undirected edges). All graphs in this paper are simple graphs without self-loops unless differently specified. The degree of a node v is the number of edges that are incident to v and is denoted by deg G (v), or simply by deg(v) when G is clear from the context. The indegree of a node v is the number of directed edges that are directed towards v and is denoted by indeg G (v), while the outdegree of v is the number of directed edges that are incident to v but directed to some other vertex and is denoted by outdeg G (v). Again, we omit the suffix G if the graph is clear from the context.

If G is a subgraph of H, we write G ⊆ H. For any subset of nodes A ⊆ V , we denote by G[A] the subgraph induced by the nodes in A. For any nodes u, v ∈ V , dist G (u, v) denotes the distance between u and v in G (i.e., the number of edges of any shortest path between u and v in G-it doesn't need to be a directed path); if u and v are disconnected, then dist G (u, v) = +∞. If G is clear from the context, we may also simply write dist(u, v) = dist G (u, v). Also, for subset of nodes A, B ⊆ V and any node v ∈ V , we can define dist

$G (v, A) = min u∈A dist G (u, v) and, by extension, dist G (A, B) = min u∈A dist G (u, B). We assume that dist G (A, ∅) = +∞. Similarly, for subgraphs G 1 , G 2 ⊆ G, we define dist G (G 1 , G 2 ) = dist G (V (G 1 ), V (G 2 )): here, we also assume dist G (G 1 , ∅) = +∞ where ∅ is now the empty graph. For T ∈ [n], the T -neighborhood of a node u ∈ V of a graph G is the set N T (u, G) = {v ∈ V | dist G (u, v) ≤ T }. The T -neighborhood of a subset A ⊆ V is the set N T (A, G) = {v ∈ V | ∃u ∈ A : dist G (u, v) ≤ T }. Similarly, the T -neighborhood of a subgraph H ⊆ G is the set N T (H, G) = {v ∈ V | ∃u ∈ V (H) : dist G (u, v) ≤ T }. If G is clear from the context, we just write N T (u), N T (A), and N T (H). If u / ∈ V , A∩V = ∅, or V (H)∩V (G) = ∅, then the neighborhood N T (u) = ∅, N T (A) = ∅, or N T (H) = ∅,$respectively. We make use of some graph operations: For any two graphs G, H, we denote by G ∩ H the intersection graph defined by

$G ∩ H = (V (G) ∩ V (H), E(G) ∩ E(H)). The graph union is defined by G ∪ H = (V (G) ∪ V (H), E(G) ∪ E(H)). Moreover, the graph difference is the graph G \ H = (V (G) \ V (H), E(G) \ E(H)).$Finally, for any two graphs G and H, we write G ∼ f H to denote that G and H are isomorphic and f :

$V (G) → V (H) is an isomorphism.$Labeling problems. We start with the notion of labeling problem. Definition 4.1 (Labeling problem). Let Σ in and Σ out two sets of input and output labels, respectively. A labeling problem Π is a mapping (G, λ in ) → {λ (out,i) } i∈I , with I being a discrete set of indexes, that assigns to every graph G with any input labeling λ in : V (G) → Σ in a set of permissible output vectors λ (out,i) : V (G) → Σ out that might depend on (G, λ in ). The mapping must be closed under graph isomorphism, i.e., if φ : V (G) → V (G ′ ) is an isomorphism between G and G ′ , and

$λ (out,i) ∈ Π((G ′ , λ in )), then λ (out,i) • φ ∈ Π((G, λ in • φ)).$A labeling problem can be thought as defined for any input graph of any number of nodes. If the set of permissible output vectors is empty for some input (G, λ in ), we say that the problem is not solvable on the input (G, λ in ): accordingly, the problem is solvable on the input

$(G, λ in ) if Π(G, λ in ) ̸ = ∅.$One observation on the generality of definition of labeling problem follows: one can actually consider problems that require to output labels on edges.

We actually focus on labeling problems where, for any input graph, an output vector λ out is permissible if and only if the restrictions of the problem on any local neighborhoods can be solved and there exist compatible local permissible output vectors whose combination provides λ out . This concept is grasped by the notion of locally checkable labeling (LCL) problems, first introduced by Naor and Stockmeyer [[57]](#b58). For any function f : A → B and any subset A ′ ⊆ A, let us denote the restriction of f to A ′ by f ↾ A ′ . Furthermore, we define a centered graph to be a pair (H, v H ) where H is a graph and v H ∈ V (H) is a vertex of H that we name the center of H. The radius of a centered graph is the maximum distance from v H to any other node in H. Definition 4.2 (Locally checkable labeling problem). Let r, ∆ ∈ N. Let Σ in and Σ out two finite sets of input and output labels, respectively, and Π a labeling problem. Π is locally checkable with checking radius r if there exists a family S = {((H, v H ), λin , λout ) i } i∈I of tuples, where (H, v H ) is a centered graph of radius at most r and maximum degree at most ∆, λin : V (H) → Σ in is an input labeling for H, λout : V (H) → Σ out is an output labeling for H (which can depend on λin ) with the following property

$• for any input (G, λ in ) to Π with deg(G) ≤ ∆, an output vector λ out : V (G) → Σ out is permissible (i.e., λ out ∈ Π((G, λ in ))) if and only if, for each node v ∈ V (G), the tuple ((G[N r (v)]), λ in ↾ Nr(v) , λ out ↾ Nr(v)$) belongs to S (up to graph isomorphisms).

Notice that the family S can be always thought to be finite up to graph isomorphisms, as r and ∆ are fixed and the set of input/output labels are finite. We now define the computational models we work in.

The port-numbering model. A port-numbered network is a triple N = (V, P, p) where V is the set of nodes, P is the set of ports, and p : P → P is a function specifying connections between ports. Each element x ∈ P is a pair (v, i) where v ∈ V , i ∈ N + . The connection function p between ports is an involution, that is,

$p(p(x)) = x for all x ∈ P . If (v, i) ∈ P , we say that (v, i) is port number i in node v. The degree of a node v in the network N is deg N (v) is the number of ports in v, that is, deg N (v) = |{i ∈ N : (v, i) ∈ P }|.$Unless otherwise mentioned, we assume that port numbers are consecutive, i.e., the ports of any node v ∈ V are (v, 1), . . . , (v, deg N (v)). Clearly, a port-numbered network identifies an underlying graph G = (V, E) where, for any two nodes u, v ∈ V , {u, v} ∈ E if and only if there exists ports x u , x v ∈ P such that p(x u ) = x v . Clearly, the degree of a node deg

$N (v) corresponds to deg G (v).$In the port-numbering model we are given distributed system consisting of a port-numbered network of |V | = n processors (or nodes) that operates in a sequence of synchronous rounds. In each round the processors may perform unbounded computations on their respective local state variables and subsequently exchange of messages of arbitrary size along the links given by the underlying input graph. Nodes identify their neighbors by using ports as defined before, where port assignment may be done adversarially. Barring their degree, all nodes are identical and operate according to the same local computation procedures. Initially all local state variables have the same value for all processors; the sole exception is a distinguished local variable x(v) of each processor v that encodes input data.

Let Σ in be a set of input labels. The input of a problem is defined in the form of a labeled graph (G, x) where G = (V, E) is the system graph, V is the set of processors (hence it is specified as part of the input), and x : V → Σ in is an assignment of an input label λ in (v) ∈ Σ in to each processor v. The output of the algorithm is given in the form of a vector of local output labels λ out : V → Σ out , and the algorithm is assumed to terminate once all labels λ out (v) are definitely fixed. We assume that nodes and their links are fault-free. The local computation procedures may be randomized by giving each processor access to its own set of random variables; in this case, we are in the randomized port-numbering model as opposed to the deterministic port-numbering model.

The running time of an algorithm is the number of synchronous rounds required by all nodes to produce output labels. If an algorithm running time is T , we also say that the algorithm has locality T . Notice that T can be a function of the size of the input graph. We say that a problem Π over some graph family F has complexity T in the port-numbering model if there is a port-numbering algorithm running in time T that solves Π over F, and T = T (n) is the minimum running time (among all possible algorithms that solve Π over F) in the worst case instance of size n. If the algorithm is randomized, we also require that the failure probability is at most 1/n, where n is the size of the input graph.

We remark that the notion of an (LCL) problem is a graph problem, and does not depend on the specific model of computation we consider (hence, the problem cannot depend on, e.g., port numbers).

The LOCAL model. The LOCAL model was first introduced by Linial [[53]](#b54): it is just the portnumbering model augmented with an assignment of unique identifiers to nodes. Let c ≥ 1 be a constant, and let Σ in be a set of input labels. The input of a problem is defined in the form of a labeled graph (G, x) where G = (V, E) is the system graph, V is the set of processors (hence it is specified as part of the input), and x : V → [n c ] × Σ in is an assignment of a unique identifier id(v) ∈ [n c ] and of an input label λ in (v) ∈ Σ in to each processor v. The output of the algorithm is given in the form of a vector of local output labels λ out : V → Σ out , and the algorithm is assumed to terminate once all labels λ out (v) are definitely fixed. We assume that nodes and their links are fault-free. The local computation procedures may be randomized by giving each processor access to its own set of random variables; in this case, we are in the randomized LOCAL (randomized LOCAL) model as opposed to deterministic LOCAL (deterministic LOCAL). Notice that the knowledge of n makes the randomized port-numbering model roughly equivalent to the randomized LOCAL model, as unique identifiers can be produced with high probability. We say that a problem Π over some graph family F has complexity T in the LOCAL model if there is a LOCAL algorithm running in time T that solves Π over F, and T = T (n) is the minimum running time (among all possible algorithms that solve Π over F) in the worst case instance of size n. If the algorithm is randomized, we also require that the failure probability is at most 1/n, where n is the size of the input graph.

The sequential LOCAL model. The sequential LOCAL model was first introduced by [[38]](#b39): it is a sequential version of the LOCAL model. Nodes are processed according to an adversarial order σ = v 1 , . . . , v n . When processing a node v i , a T -round algorithm collects all inputs in the radius-T neighborhood of v i (including the states and the outputs of previously processed nodes in N T (V i ), i.e., v j ∈ N T (V i ) for j < i): we say that such an algorithm has complexity T . Note that the algorithm might store all inputs in N T (v i ) in the state of v i : hence, when processing v i , it can see the input of v j , j < i, if and only if there is a subsequence of nodes {v

$h k } k∈[m] with j = h k < h k+1 < • • • < h km = i such that v h k ∈ N T (v h k+1 ) for all k ∈ [m].$If the algorithm is given an infinite random bit string, we talk about the randomized SLOCAL model, as opposed to the deterministic SLOCAL model. We assume that the adversarial order according to which nodes are processed is oblivious to the random bit string, as in the original definition of the model. We say that a problem Π over some graph family F has complexity T in the SLOCAL model if there is an SLOCAL algorithm running in time T that solves Π over F, and T = T (n) is the minimum running time (among all possible algorithms that solve Π over F) in the worst case instance of size n. If the algorithm is randomized, we also require that the failure probability is at most 1/n, where n is the size of the input graph.

The dynamic-LOCAL model. The deterministic dynamic-LOCAL was introduced in [[2]](#). It is a centralized model of computing where the adversary constructs the graph one edge at a time. An adversary construct the input graph by adding one edge at a time (with an ordering of the nodes). The algorithm has a global view of the current state of the graph and has to commit for the newly added nodes (according to the ordering), but it has to maintain a feasible solution after each update. The algorithm is restricted so that after a modification at node v, it can only update the solution within distance T (n) from v. We say that a problem Π over some graph family F has complexity T (n) in the dynamic-LOCAL model if T (n) is the minimum function such that there is a dynamic-LOCAL algorithm running in time T (n) that solves Π over F, and T = T (n) is the minimum running time (among all possible algorithms that solve Π over F) in the worst case instance of size n.

The online-LOCAL model. The (deterministic) online-LOCAL model was introduced in [[2]](#). It is basically equivalent to the SLOCAL model with global memory. More specifically, the online-LOCAL model is a centralized model of computing where the algorithm initially knows only the set of nodes of the input graph G. The nodes are processed with respect to an adversarial input sequence

$σ = v 1 , v 2 , . . . , v n . The output of v i depends on G i = G[ i j=1 N T (v j )],$i.e., the subgraph induced by the radius-T neighborhoods of v 1 , v 2 , . . . , v i (including all input data), plus all the outputs of previously processed nodes (in order).

We define the randomized online-LOCAL model as a randomized variant of the online-LOCAL model where the label assigned by the algorithm to v i is a random outcome. Note that this model is oblivious to the randomness used by the algorithm. In particular this means that the graph G \ G i cannot be changed depending on the label assigned to v i . One could also define the randomized online-LOCAL model in an adaptive manner, but it turns out that this is equivalent to the deterministic online-LOCAL model as we show in Section 10. We say that a problem Π over some graph family F has complexity T in the online-LOCAL (randomized online-LOCAL) model if there is an online-LOCAL (randomized online-LOCAL) algorithm running in time T that solves Π over F, and T = T (n) is the minimum running time (among all possible algorithms that solve Π over F) in the worst case instance of size n. If the algorithm is randomized, we also require that the failure probability is at most 1/n, where n is the size of the input graph.

5 Simulating non-signaling in randomized online-LOCAL

## Framework

In this section we give the necessary framework to define the non-signaling model and is largely inspired by the definitions given in [[27,](#b28)[36]](#b37). Next definition introduces the concept of outcome. Definition 5.1 (Outcome). Let Σ in and Σ out be two sets of input and output labels, respectively, and let F be a family of graphs. An outcome O over F is a mapping (G, x) → {(λ (out,i) , p i )} i∈I , with I being a discrete set of indexes, assigning to every input graph G ∈ F with any input data x = (id :

$V (G) → [|V (G)| c ], λ in : V (G) → Σ in ), a discrete probability distribution {p i } i∈I over output vectors λ (out,i) : V (G) → Σ out such that:$1. for all i ∈ I, p i > 0; 2.

i∈I p i = 1; 3. p i represents the probability of obtaining λ (out,i) as the output vector of the distributed system.

We say that an outcome O over some graph family F solves problem Π over F with probability p if, for every G ∈ F and any input data x = (id, λ in ), it holds that

$(λ (out,i) ,p i )∈O((G,x)) : λ (out,i) ∈Π((G,λ in )) p i ≥ p.$When p = 1, we will just say that O solves problem Π over the graph family F.

The next computational model tries to capture the fundamental properties of any physical computational model (in which one can run either deterministic, random, or quantum algorithms) that respects causality. The defining property of such a model is that, for any two (labeled) graphs (G 1 , x 1 ) and (G 2 , x 2 ) that share some identical subgraph (H, y), every node u in H must exhibit identical behavior in G 1 and G 2 as long as its local view, that is, the set of nodes up to distance T away from u together with input data and port numbering, is fully contained in H. As the port numbering can be computed with one round of communication through a fixed procedure (e.g., assigning port numbers 1, 2, . . . , deg(v) based on neighbor identifiers in ascending order) and we care about asymptotic bounds, we will omit port numbering from the definition of local view.

The model we consider has been introduced by [[36]](#b37). In order to proceed, we first define the non-signaling property of an outcome. Let T ≥ 0 be an integer, and I a set of indices. For any set of nodes V , subset S ⊆ V , and for any input (G = (V, E), x), we define its T -local view as the set

$v T (G, x, S) = {(u, x(u)) | ∃ u ∈ V, v ∈ S such that dist G (u, v) ≤ T } ,$where dist G (u, v) is the distance in G. Furthermore, for any subset of nodes S ⊆ V and any output distribution {λ (out,i) , p i } i∈I , we define the marginal distribution of {λ (out,i) , p i } i∈I on set S as the unique output distribution defined as follows: for any λ out : V (G) → Σ out , the probability of λ out ↾ S on S is given by p(λ out , S)

$= i∈I : λout↾ S =λ (out,i) ↾ S p i ,$where λ out ↾ S and λ (out,i) ↾ S are the restrictions of λ out and λ (out,i) on S, respectively.

Definition 5.2 (Non-signaling outcome). Let F be a family of graphs. An outcome

$O : (G, x) → {(λ (out,i) , p i )} i∈I over F is non-signaling beyond distance T = T (G, x) if for any pair of inputs (G 1 = (V 1 , E 1 ), x 1 ), (G 2 = (V 2 , E 2 ), x 2 ), with the same number of nodes, such that v T (G 1 ,x 1 ) (G 1 , x 1 , S) is isomorphic to v T (G 1 ,x 1 ) (G 2 , x 2 , S)$and G 1 , G 2 ∈ F, the output distributions corresponding to these inputs have identical marginal distributions on the set S.

Definition 5.2 is also the more general definition for the locality of an outcome: an outcome O has locality T if it is non-signaling beyond distance T .

The φ-LOCAL model. The φ-LOCAL model is a computational model that produces non-signaling outcomes over some family of graphs F. Let p ∈ [0, 1]. A problem Π over some graph family F has complexity T (and success probability p) if there exists an outcome O that is non-signaling beyond distance T which solves Π over F (with probability at least p), and T = T (n) is the minimum "non-signaling distance" (among all possible outcomes that solve Π over F) in the worst case instance of size n.

As every (deterministic or randomized) algorithm running in time at most T in the LOCAL model produces an outcome which has locality T , we can provide lower bounds for the LOCAL model by proving them in the φ-LOCAL model.

Notice that algorithms in the LOCAL model can be always thought as producing outputs for any input graph: when the computation at any round is not defined for some node, we can make the node output some garbage label, say ⊥. If we require that also outcomes are defined for every possible graph, then we are restricting the power of φ-LOCAL because outcomes must be defined accordingly. This gives rise to a slightly weaker model, the non-signaling model, which was considered in other works such as [[27]](#b28) and is still stronger than any classical or quantum variation of the LOCAL model. Theorem 5.3. Let Π be any LCL problem over any family of graphs F. Let O : (G, λ in ) → {(λ (out,h) , p i )} h∈H be an outcome over F which solves Π with failure probability at most ε and is non-signaling at distance greater than T . There exists a randomized online-LOCAL algorithm A with complexity T that solves Π over F and has failure probability at most ε.

We want to design a randomized online-LOCAL algorithm A with complexity T that solves Π over F and has failure probability at most ε that somehow simulates O. We need to assume that the number of nodes of the input graph is known by A.

Fix any graph G ∈ F of n nodes and any input labeling λ in , and fix any sequence of nodes v 1 , . . . , v n the adversary might choose to reveal to the algorithm.

The adversary initially shows G[N T (v 1 )] to the algorithm (including input labels and identifiers) and asks it to label v 1 . In order for the algorithm to choose an appropriate output, it can arbitrarily choose a graph H 1 ∈ F with n nodes that contains a subgraph isomorphic to G[N T (v 1 )] (again, including input labels and identifiers). Notice that H 1 necessarily exists since G itself is such a graph. We stress that the arbitrariness in the choice of H 1 is thanks to the non-signaling property, which ensures that the restriction of the output distribution O(H 1 , λ in ) over N T (v 1 ) does not change if the topology of the graph outside G[N T (v 1 )] differs (i.e., no matter how the adversary chooses it). Indeed, if O did not have the non-signaling property, then it would be impossible to sample from it correctly since the marginal output distribution on G[N T (v 1 )] would depend on the topology of the graph beyond it, which the algorithm still has no knowledge of.

For any λ out : V (G) → Σ out , the probability that A labels v 1 with λ out (v 1 ) is

$p(λ out , v 1 ) = k: λout↾ {v 1 } =λ (out,k) ↾ {v 1 } p k , where {(λ (out,k) , p k )} k∈K 1 = O(H 1 , λ in ).$Let Λ i be the random variable yielding the label assigned to v i by A. In general, for any

$λ out : V (G) → Σ out let p(λ out , v i ) denote the probability that Λ i = λ out (v i ). Assume now that G[N T (v i+1 )] is shown to the algorithm. Conditional on Λ 1 , . . . , Λ i , for any λ out : V (G) → Σ out such that λ out (v j ) = Λ j for all j = 1, . . . , i, the probability that Λ i+1 = λ out (v i+1 ) is p(λ out , v i+1 ) = k: λout↾ {v j } =λ (out,k) ↾ {v j } ∀ 1≤j≤i+1 p k 1≤l≤i p(λ out , v l )$, where {(λ (out,k) , p k )} k∈K i+1 = O(H i+1 , λ in ) for any graph H i+1 ∈ F of n nodes that contains a subgraph isomorphic to G[∪ i+1 i=1 N T (v j )] (including input labels and identifiers). As before, H i+1 may be chosen arbitrarily due to the non-signaling property. Now consider O(G, λ in ) = {(λ (out,h) , p h )} h∈H for the right input graph G, and take any

$(λ (out,h ⋆ ) , p h ⋆ ) ∈ O(G, λ in ).$The probability that the outcomes of Λ 1 , . . . , Λ n give exactly λ (out,h ⋆ ) is

$Pr Λ 1 = λ (out,h ⋆ ) (v 1 ), . . . , Λ n = λ (out,h ⋆ ) (v n ) = Pr Λ n = λ (out,h ⋆ ) (v n ) Λ 1 = λ (out,h ⋆ ) (v 1 ), . . . , Λ n-1 = λ (out,h ⋆ ) (v n-1 ) • Pr Λ n-1 = λ (out,h ⋆ ) (v n-1 ) Λ 1 = λ (out,h ⋆ ) (v 1 ), . . . , Λ n-2 = λ (out,h ⋆ ) (v n-2 )$. . .

$• Pr Λ 1 = λ (out,h ⋆ ) (v 1 ) = p(λ (out,h ⋆ ) , v 1 ) • . . . • p(λ (out,h ⋆ ) , v n ) = 1≤l≤n-1 p(λ out , v l ) • kn: λ (out,h ⋆ ) ↾ {v j } =λ (out,k j ) ↾ {v j } ∀ 1≤j≤n p kn 1≤l≤n-1 p(λ out , v l ) = kn: λ (out,h ⋆ ) ↾ {v j } =λ (out,k j ) ↾ {v j } ∀ 1≤j≤n p kn .$Notice that

$kn: λ (out,h ⋆ ) ↾ {v j } =λ (out,k j ) ↾ {v j } ∀ 1≤j≤n p kn = p h ⋆$as H n is necessarily isomorphic to G. By the hypothesis,

$h: λ (out,h) is valid for (G,λ in ) p h ≥ 1 -ε,$implying that A succeeds with probability at least 1 -ε.

## Bounded-dependence model can break symmetry

For any graph G = (V, E), a random process (or distribution) on the vertices of G is a family of random variables {X v } v∈V , indexed by V , while a random process on the edges of G is a family of random variables {X e } e∈E indexed by E. More generally, a random process on G is a family of random variables {X y } y∈V ∪E indexed by V ∪ E. The variables of a random process live in the same probability space and take values in some label set Σ. In general, we will consider random processes over vertices of graphs unless otherwise specified. We now introduce the notion of T -dependent distribution. To do so, we extend the definition of distance to edges as follows: For any two edges e = {v 1 , v 2 }, e ′ = {u 1 , u 2 } ∈ E, dist G (e, e ′ ) = min i,j∈ [[2]](#) dist G (v i , u j ). Similarly, the distance between any edge e = (v 1 , v 2 ) and a vertex v is dist G (e, v) = min i∈ [[2]](#) dist G (v i , v). The definition extends easily to subsets containing vertices and edges. Definition 6.1 (T -dependent distribution). Let T ∈ N be a natural number and G = (V, E) be any graph. A random process {X v } v∈V on the vertices G is said to be a T -dependent distribution if, for all subsets S, S ′ ⊆ V such that dist G (S, S ′ ) > T , the two processes {X v } v∈S and {X v } v∈S ′ are independent. Analogous definitions hold for random processes on the edges of a graph and for random processes on the whole graph.

A way to define T -dependent distributions that uses the same notation of Section 5.1 is by describing the output probability of global labelings: Let I be a discrete set of indices, and G = (V, E) some graph. A distribution {(λ i , p i )} i∈I over output labelings, where λ i : V → Σ is an output labeling, is T -dependent if the following holds: for all output labelings λ in {(λ i , p i )}, for every two subsets of nodes

$S 1 , S 2 ⊆ V (G) such that dist G (S 1 , S 2 ) > T , we have that p(λ, S 1 ∪ S 2 ) = p(λ, S 1 ) • p(λ, S 2 ).$Notice that outcomes, as defined in Definition 5.1, output a random process for every input. Often, we have a family of graphs of arbitrarily large size n on which a T (n)-dependent distribution is defined. [1](#foot_0)Now we define the hypothetical computational model that outputs T -dependent distributions on some input graph.

The bounded-dependence model. The bounded-dependence model is a computational model that, for a given family of graphs F, produces an outcome (as defined in Definition 5.1) over F that is non-signaling beyond distance T = T (G, x) (as defined in Definition 5.2), where G ∈ F and x represents the input, which, in turn, produces T (G, x)-dependent distributions. The random processes produced by an outcome are said to be finitely-dependent if T = O(1) for all graphs in F and all input data. If an outcome with the aforementioned properties solves a problem Π over a graph family F with probability at least p, we say that the pair (Π, F) has complexity T (with success probability p), and T = T (n) is the minimum dependence (among all possible distributions solving Π over F) in the worst case instance of size n. We remark that T is also called the locality or the complexity of the corresponding output distributions.

We are particularly interested in T -dependent distributions that satisfy invariance properties: We say that a random process {X v } v∈V over vertices of a graph G = (V, E) is invariant under automorphisms if, for all automorphisms f : V → V of G = (V, E), the two processes {X v } v∈V and {Y v = X f (v) } v∈V are equal in law. The definition is easily extendable to random processes on edges and random processes on the whole graph.

A stronger requirement is the invariance under subgraph isomorphism: Suppose we have an outcome O : (G, x) → {X v } v∈V (G) that maps each input graph G from family of graphs F and any input data x to a T = T (G, x)-dependent distribution over G from a family of random process R. We say that the random process over vertices in R are invariant under subgraph isomorphisms if, given any two graphs G 1 , G 2 ∈ F of size n 1 , n 2 with associated process {X

$(1) v } v∈V (G 1 ) and {X (2) v } v∈V (G 2 )$, and any two subgraphs

$H 1 ⊆ G 1 , H 2 ⊆ G 2 such that G 1 [N T (n 1 ) (H 1 )] and G 2 [N T (n 2 ) (H 2 )] are isomorphic (with the isomorphism that brings H 1 into H 2 ), then {X (1) v } v∈V (H 1 ) and {X (2) v } v∈V (H 2 )$are equal in law. [2](#foot_1) Trivially, invariance under subgraph isomorphisms implies invariance under automorphisms and the non-signaling property. This definition is again easily extendable to the case of families of random processes over edges or over graphs in general.

The baseline of our result is a finitely-dependent distribution provided by [[45,](#b46)[47]](#b48) on paths and cycles. Theorem 6.2 (Finitely-dependent coloring of the integers and of cycles [[45,](#b46)[47]](#b48)). Let G = (V, E) be a graph that is either a cycle with at least 2 nodes or has V = Z and E = {{i, i + 1} : i ∈ Z}. For (k, q) ∈ {(1, 4), (2, 3)}, there exists a k-dependent distribution {X (G) v } v∈V (G) that gives a q-coloring of G. Furthermore, such distributions can be chosen to meet the following properties: If {X i } i∈[n] is the k-dependent q-coloring of the n-cycle and {Y i } i∈Z is the k-dependent q-coloring of the integers, then

${X i } i∈[n-k] is equal in law to {Y i } i∈[n-k] . If n = 2,$then the finitely-dependent colorings on the 2-cycle and on a path of 2 nodes are identical in distribution.

It is immediate that the disjoint union of any number of paths and cycles (even countably many) admits such distributions as well. Corollary 6.3. Let F be the family of graphs formed by the disjoint union (possibly uncountably many) paths with countably many nodes and cycles of any finite length. For all G ∈ F and for (k, q) ∈ {(1, 4), (2, 3)}, there exists a k-dependent distribution {X (G) v } v∈V (G) that gives a q-coloring of G. Furthermore, such distributions can be chosen to meet all the following properties:

$1. On each connected component H ⊆ G, {X (G) v } v∈V (H) is given by Theorem 6.2. 2. {{X (G) v } v∈V (G) } G∈F is invariant under subgraph isomorphisms.$We will use this result to provide "fake local identifiers" to the nodes of the graph to simulate an O(log * n)-round LOCAL algorithm through a random process with constant dependency. In order to do that, we introduce some results on the composition of T -dependent distributions.

Trivially, every T -round (deterministic or randomized) port-numbering algorithm defines a 2Tdependent distribution over the input graph. Furthermore, if the underlying port-numbering model has access to random bits, the distribution can be made invariant under subgraph isomorphisms (provided that, whenever a distribution over input labelings is given together with the input graph, such distribution is also invariant under subgraph isomorphisms). The composition of the T 2dependent distribution obtained by a T 2 -round port-numbering algorithm and any T 1 -dependent distribution yields a (2T 2 + T 1 )-dependent distribution. Lemma 6.4. Let Σ (1) and Σ (2) be two label sets with countably many labels. Let F be any family of graphs, and let R = {{X v } v∈V (G) : G ∈ F} be a family of T 1 -dependent distributions taking values in Σ (1) , where T 1 might depend on parameters of G. Consider any T 2 -round port-numbering algorithm A that takes as an input G ∈ F labelled by {X v } v∈V (G) : it defines another distribution {Y v } v∈V (G) taking values in some label set Σ (2) . Then, the following properties hold:

$1. {Y v } v∈V (G) is a (2T 2 + T 1 )-dependent distribution on G.$2. If the processes in R are invariant under subgraph isomorphisms, T 2 is constant, and A does not depend on the size of the input graph and permutes port numbers locally u.a.r. at round 0, then the processes in {{Y v } v∈V (G) : G ∈ F} are invariant under subgraph isomorphisms.

Proof. We prove the claim 1 first. Fix any G = (V, E) ∈ F, and the corresponding

$T 2 -dependent distribution {X v } v∈V ∈ R. Fix any two subsets S, S ′ ⊆ V such that dist G (S, S ′ ) > 2T 2 + T 1 .$Consider any output labeling λ (2) : V → Σ (2) . Then,

$Pr ∩ v∈S∪S ′ {Y v = λ (2) (v)} = λ (1) : V →Σ (1) Pr ∩ v∈S∪S ′ {Y v = λ (2) (v)} ∩ v∈V {X v = λ (1) (v)} • Pr ∩ v∈V {X v = λ (1) (v)} = λ (1) : V →Σ (1) Pr ∩ v∈S∪S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)}(3)$$• Pr ∩ v∈V {X v = λ (1) (v)} = λ (1) : N T 2 (S∪S ′ )→Σ (1) Pr ∩ v∈S∪S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} ,$where Eq. ( [3](#formula_40)) holds because the output of

${Y v } v∈S∪S ′ is independent of {X v } v / ∈N T 2 (S∪S ′ ) . Since dist G (S, S ′ ) > 2T 2 ,$λ (1) : N T 2 (S∪S ′ )→Σ (1)   Pr

$∩ v∈S∪S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} = λ (1) : N T 2 (S∪S ′ )→Σ (1) Pr ∩ v∈S {Y v = λ (2) (v)} ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} = λ (1) : N T 2 (S∪S ′ )→Σ (1) Pr ∩ v∈S {Y v = λ (2) (v)} ∩ v∈N T 2 (S) {X v = λ (1) (v)} • Pr ∩ v∈S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} . Now observe that dist G (N T 2 (S), N T 2 (S ′ )) > T 1 . Since {X v } v∈V is a T 1 -dependent distribution, it holds that λ (1) : N T 2 (S∪S ′ )→Σ (1) Pr ∩ v∈S {Y v = λ (2) (v)} ∩ v∈N T 2 (S) {X v = λ (1) (v)} • Pr ∩ v∈S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S∪S ′ ) {X v = λ (1) (v)} = λ (1) : N T 2 (S∪S ′ )→Σ (1) Pr ∩ v∈S {Y v = λ (2) (v)} ∩ v∈N T 2 (S) {X v = λ (1) (v)} • Pr ∩ v∈S ′ {Y v = λ (2) (v)} ∩ v∈N T 2 (S ′ ) {X v = λ (1) (v)} • Pr ∩ v∈N T 2 (S) {X v = λ (1) (v)} Pr ∩ v∈N T 2 (S ′ ) {X v = λ (1) (v)} = Pr ∩ v∈S {Y v = λ (2) (v)} • Pr ∩ v∈S ′ {Y v = λ (2) (v)} .$We now prove claim 2. Given any G, H ∈ F of size n G and n H , respectively, consider any subgraph isomorphism f :

$N 2T 2 +T 1 (n G ) (G ′ , G) → N 2T 2 +T 1 (n H ) (H ′ , H) for any two G ′ ⊆ G, H ′ ⊆ H such that f restricted to G ′ is an isomorphism to H ′ . Let {X (G) v } v∈V (G) and {X (H) v } v∈V (H) , and {Y (G) v } v∈V (G) and {Y (H) v$} v∈V (H) be the corresponding distributions of interest before and after the combination with the port-numbering algorithm, respectively. Fix any subset of nodes U ⊆ V (G ′ ) and consider any family of labels {λ u } u∈U indexed by U . In order to prove property 1, it is sufficient to show that Pr

$∩ u∈U {Y (G) u = λ u } = Pr ∩ u∈U {Y (H) f (u) = λ u } . Notice that Pr [∩ u∈U {Y u = λ u }] depends solely on the graph G[N T 2 (U, G)] = ∪ u∈U G[N T 2 (u, G)],$the distribution of the port numbers in G[N T 2 (U, G)], and the random process 

${X u } u∈N T 2 (U ) . By hypothesis, {X (G) u } u∈N T 2 (U,G) is equal in law to {X (H) f (u) } u∈N T 2 (U,G) . Furthermore, the restriction of f to G[N T 2 (U, G)] defines an isomorphism from G[N T 2 (U, G)] to H[N T 2 (f (U ), H)],$$f (u) = λ u } .$T -dependent distributions over different graphs can be combined to obtain a T -dependent distribution over the graph union. Lemma 6.5.

$Let {X v } v∈V 1 and {Y v } v∈V 2 be a T 1 -dependent distribution over a graph G 1 = (V 1 , E 1 )$taking values in Σ (1) and a T 2 -dependent distribution over a graph G 2 = (V 2 , E 2 ) taking values in Σ (2) , respectively. Assume {X v } v∈V 1 and {Y v } v∈V 2 to be independent processes. Consider the graph

$H = (V = V 1 ∪ V 2 , E = E 1 ∪ E 2 ) and a distribution {Z v } v∈V over H taking values in Σ = (Σ (1) ∪ {0}) × (Σ (2) ∪ {0}) defined by Z v =      (X v , 0) if v ∈ V 1 \ V 2 , (X v , Y v ) if v ∈ V 1 ∩ V 2 , (0, Y v ) if v ∈ V 2 \ V 1 . Then, {Z v } v∈V is max(T 1 , T 2 )-dependent. Proof. For any vector v ∈ Σ 1 × • • • × Σ n , we write v[i]$to denote its i-th entry. Fix any output labeling λ :

$V → Σ such that λ(v)[2] = 0 for all v ∈ V 1 \ V 2 and λ(v)[1] = 0 for all v ∈ V 2 \ V 1 .$Observe that for any subset S ⊆ V it holds that

$Pr [∩ v∈S {Z v = λ(v)}] = Pr [(∩ v∈S∩V 1 {X v = λ(v)[1]}) ∩ (∩ v∈S∩V 2 {Y v = λ(v)[2]})] = Pr [∩ v∈S∩V 1 {X v = λ(v)[1]}] • Pr [∩ v∈S∩V 2 {Y v = λ(v)[2]}] ,(4)$where the latter equality follows by independence between

${X v } v inV 1 and {Y v } v inV 2 . W.l.o.g., suppose T 1 ≥ T 2 . Consider two subsets S, S ′ ⊆ V such that dist G (S, S ′ ) > T 1 . Fix any output labeling λ : V → Σ such that λ(v)[2] = 0 for all v ∈ V 1 \ V 2 and λ(v)[1] = 0 for all v ∈ V 2 \ V 1 .$Using Eq. ( [4](#formula_51)), we have that

$Pr [∩ v∈S∪S ′ {Z v = λ(v)}] = Pr ∩ v∈(S∪S ′ )∩V 1 {X v = λ(v)[1]} • Pr ∩ v∈(S∪S ′ )∩V 2 {Y v = λ(v)[2]} = Pr ∩ v∈(S∩V 1 )∪(S ′ ∩V 1 ) {X v = λ(v)[1]} • Pr ∩ v∈(S∩V 2 )∪(S ′ ∩V 2 ) {Y v = λ(v)[2]} = Pr [∩ S∩V 1 {X v = λ(v)[1]}] • Pr [∩ S ′ ∩V 1 {X v = λ(v)[1]}](5)$•

$Pr [∩ S∩V 2 {Y v = λ(v)[2]}] • Pr [∩ S ′ ∩V 2 {Y v = λ(v)[2]}] = Pr [(∩ v∈S∩V 1 {X v = λ(v)[1]}) ∩ (∩ v∈S∩V 2 {Y v = λ(v)[2]})](6)$•

$Pr [(∩ v∈S ′ ∩V 1 {X v = λ(v)[1]}) ∩ (∩ v∈S ′ ∩V 2 {Y v = λ(v)[2]})] = Pr [∩ v∈S {Z v = λ(v)}] • Pr [∩ v∈S ′ {Z v = λ(v)}] ,$where Eq. ( [5](#formula_53)) holds because {X v } v∈V 1 is T 1 -dependent and {Y v } v∈V 2 is T 2 -dependent, while Eq. ( [6](#formula_54)) holds because {X v } v∈V 1 and {Y v } v∈V 2 are independent. Now we present a final lemma on the composition of random processes. To do so, we first introduce the notation of random decomposition of a graph. Definition 6.6 (Random decomposition). Let G = (V, E) be any graph and P a family of subgraphs of G. For any k ∈ N, let Γ(G) be a random variable taking values in P k that is sampled according to any probability distribution. We say that Γ(G) is a random k-decomposition of G in P.

Given a random k-decomposition Γ(G) of G in P, for any y ∈ V ∪ E, we define the random variable Γ(G) y ∈ {0, 1} k as follows: Γ(G) y [i] = 1 if y belongs to Γ(G)[i] and 0 otherwise. Notice that {Γ(G) y } y∈V ∪E is a random process on G. If {Γ(G) y } y∈V ∪E is invariant under automorphisms, then we say that the random k-decomposition Γ(G) is invariant under automorphisms. If, for a family of graphs F and any graph G ∈ F, {Γ(G) y } y∈V (G)∪E(G) is T -dependent (with T being a function of the size of G) and the processes in {{Γ(G) y } y∈V (G)∪E(G) : G ∈ F} are invariant under subgraph isomorphisms, then we say that the random k-decompositions in {Γ(G) : G ∈ F} are T -dependent and invariant under subgraph isomorphisms.

For a random decomposition, we define the notion of induced random process.

Definition 6.7 (Induced process). Let G = (V, E) be a graph that admits a family of subgraphs P. Suppose there exists a random process {X v } v∈V (H(P)) with H(P) being the graph obtained by the disjoint union of all elements of P. Let Γ(G) be a random k-decomposition of G in P. For all v ∈ V and G ′ ∈ P, define the random process {X

$(G ′ ) v } v∈V by setting X (G ′ ) v = X f G ′ (v) for all v ∈ V (G ′ ),$where

$f G ′ : V (G ′ ) → V (H(P)) is the natural immersion of G ′ into H(P) otherwise set X (G ′ ) v = 0. Let {Y (Γ(G)) v$} v∈V be a random process that we define conditional on the output of Γ(G): for all

$G ∈ P k , conditional on Γ(G) = G, Y (Γ(G)) v = Y (G) v = (X (G[1]) v , . . . , X (G[k]) v ).$
## The random process {Y (Γ(G)) v

} v∈V on G is said to be induced by the action of {X v } v∈V (H(P)) over the random k-decomposition Γ(G). Now we present a result on the induced random process whenever the random decomposition and the family of random processes that acts on the random decomposition meet some invariance properties. Lemma 6.8. Let F be a family of graphs. For any G = (V, E) ∈ F, let P G be any family of subgraphs of G that is closed under node removal and disjoint graph union, that is, if } v∈V (G) be the random process induced by the action of

$G 1 , G 2 ∈ P G and V (G 1 ) ∩ V (G 2 ) = ∅, then G 1 ∪ G 2 ∈ P G . Furthermore, suppose that, for each pair of isomorphic subgraphs G 1 , G 2 ⊆, G 1 ∈ P G =⇒ G 2 ∈ P G . Moreover, let Γ(G) be a random k-decomposition of G in P G that is T 1 -dependent,$${X v } v∈V (H(P G )) over Γ(G). Then, {Y (Γ(G)) v } v∈V (G) is a (T 1 + 2T 2 )-dependent distribution. Furthermore, the processes in {{X v } v∈V (H(P G )) : G ∈ F} are invariant under subgraph isomorphism.$Proof. Fix G ∈ F and let Γ = Γ(G), P = P G . Since P k might be uncountable, we consider the density function f Γ and the probability measure P associated to Γ. Consider two subsets of nodes S, S ′ ⊆ V at distance at least max(T 1 , T 2 ) + 1. Fix any labeling λ :

$V → Σ k . It holds that Pr ∩ v∈S∪S ′ {Y (Γ) v = λ(v)} = P k Pr ∩ v∈S∪S ′ {Y (Γ) v = λ(v)} Γ = G f Γ (G) dP (7) = P k Pr ∩ v∈S∪S ′ {Y (G) v = λ(v)} f Γ (G) dP = P k Pr ∩ v∈S {Y G v = λ(v)} Pr ∩ v∈S ′ {Y G = λ(v)} f Γ (G) dP (8) = P k i∈[k] Pr ∩ v∈S {X G[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X G[i] v = λ(v)[i]} f Γ (G) dP ,(9)$where Eq. ( [7](#)) holds by the law of total probability, Eq. ( [8](#)) holds since {Y

$(G) v$} v∈V is T 2 -dependent for all G ∈ P k by Lemma 6.5, and Eq. ( [9](#formula_61)) holds since {X

$G[i] v } v∈V and {X G[j] v } v∈V are independent if i ̸ = j. Notice that, for any set S ⊆ V , G ′ = G[i][N T 2 (S, G[i])] ∈ P and the event ∩ v∈S {X G[i] v = λ(v)[i]} has the same probability as ∩ v∈S {X G ′ v = λ(v)[i]} because {X v } v∈V (H(P)) is invariant under subgraph isomorphisms.$For any subset U ⊆ V and any integer T ≥ 0, let

$H(T, U ) = (G[1][N T (U, G[1])], . . . , G[k][N T (U, G[k])]) G ∈ P k ⊆ P k ,$and let P[H(T, U )] be the restriction of P to H(T, U ) defined as follows: for any event E,

$P[H(T, U )](E) = P(E ∩ H(T, U ))/P(H(T, U )).$Finally, define the random variable Γ (T,U ) to be a k-dimensional vector, taking values in H(T, U ), whose i-th entry is

$Γ[i][N T (U, Γ[i])],$and let f Γ (T,U ) be its density function. Define U = S ∪ S ′ : Eq. ( [9](#formula_61)) becomes

$P k i∈[k] Pr ∩ v∈S {X G[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X G[i] v = λ(v)[i]} f Γ (G) dP(10)$=

$H(T 2 ,U ) i∈[k] Pr ∩ v∈S {X H[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X H[i] v = λ(v)[i]} f Γ (T 2 ,U ) (H) dP[H(T 2 , U )] .$Now notice that, since the random decomposition Γ is T 1 -dependent and dist G (S, S ′ ) > T 1 + 2T

2 , the probability space H(T 2 , U ) (with measure P[H(T 2 , U )]) is isomorphic to the product space H(T 2 , S) × H(T 2 , S ′ ) (with product measure P[H(T 2 , S)] × P[H(T 2 , S ′ )]). The isomorphism brings any element

$(G[1][N T 2 (U, G[1])], . . . , G[k][N T 2 (U, G[k])]) of H(T 2 , U ) into (G[1][N T 2 (S, G[1])], . . . , G[k][N T 2 (S, G[k])]), (G[1][N T 2 (S ′ , G[1])], . . . , G[k][N T 2 (S ′ , G[k])])$which belongs to H(T 2 , S) × H(T 2 , S ′ ). Hence, we get

$H(T 2 ,U ) i∈[k] Pr ∩ v∈S {X H[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X H[i] v = λ(v)[i]} f Γ (T 2 ,U ) (H) dP[H(T 2 , S)] = H(T 2 ,S) H(T 2 ,S ′ ) i∈[k] Pr ∩ v∈S {X H[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X H[i] v = λ(v)[i]} • f Γ (T 2 ,S) (H 1 )f Γ (T 2 ,S ′ ) (H 2 ) dP[H(T 2 , S)] dP[H(T 2 , S ′ )] .$The latter becomes

$H(T 2 ,S) H(T 2 ,S ′ ) i∈[k] Pr ∩ v∈S {X H[i] v = λ(v)[i]} Pr ∩ v∈S ′ {X H[i] v = λ(v)[i]} • f Γ (T 2 ,S) (H 1 )f Γ (T 2 ,S ′ ) (H 2 ) dP[H(T 2 , S)] dP[H(T 2 , S ′ )] = H(T 2 ,S) i∈[k] Pr ∩ v∈S {X H[i] v = λ(v)[i]} f Γ (T 2 ,S) (H 1 ) dP[H(T 2 , S)] • H(T 2 ,S ′ ) i∈[k] Pr ∩ v∈S ′ {X H[i] v = λ(v)[i]} f Γ (T 2 ,S ′ ) (H 2 ) dP[H(T 2 , S ′ )] = P k i∈[k] Pr ∩ v∈S {X G[i] v = λ(v)[i]} f Γ (G) dP(11)$•

$P k i∈[k] Pr ∩ v∈S ′ {X G[i] v = λ(v)[i]} f Γ (G) dP = Pr ∩ v∈S {Y (Γ(G)) v = λ(v)} Pr ∩ v∈S ′ {Y (Γ(G)) v = λ(v)} ,$where Eq. ( [11](#formula_71)) follows by the same reasoning as for Eq. ( [10](#formula_67)) but in reverse. Now we want to prove that {{Y

$(Γ(G)) v$} v∈V : G ∈ F} is invariant under subgraph isomorphism. Fix any two graphs G, H ∈ F of sizes n G and n H , respectively. Consider any isomorphism α between the radius-(T 1 (n G ) + 2T 2 (n G )) neighborhoods of any subgraph G ′ ⊆ G and the radius-

$(T 1 (n H ) + 2T 2 (n H )) neighborhoods of any subgraph H ′ ⊆ H, such that the restriction of α to G ′ is an isomorphism to H ′ . With an abuse of notation, for any subgraph K ⊆ G ′ , let us denote α(K) ⊆ H ′ its isomorphic image in H ′ through α. Let T G = T 1 (n G ) + 2T 2 (n G ) and T H = T 1 (n H ) + 2T 2 (n H ). Fix any labeling λ : V → Σ k . We have that Pr ∩ v∈V (G ′ ) {Y (Γ(G)) v = λ(v)} = P k G Pr ∩ v∈V (G ′ ) {Y (Γ(G)) v = λ(v)} Γ(G) = G f Γ(G) (G) dP G . = P k G Pr ∩ v∈V (G ′ ) {Y (G) v = λ(v)} f Γ(G) (G) dP G = P k G i∈[k] Pr ∩ v∈V (G ′ ) {X (G[i]) v = λ(v)[i]} f Γ(G) (G) dP G = H G (T G ,V (G ′ )) i∈[k] Pr ∩ v∈V (G ′ ) {X (H[i]) v = λ(v)[i]} f Γ(G) (T G ,V (G ′ )) (H) dP G (H G (T G , V (G ′ ))) = H G (T G ,V (G ′ )) i∈[k] Pr ∩ v∈V (G ′ ) {X (α(H[i])) α(v) = λ(v)[i]} f Γ(G) (T G ,V (G ′ )) (H) dP G (H G (T G , V (G ′ ))) ,$where the latter holds because

$Pr ∩ v∈V (G ′ ) {X (H[i]) v = λ(v)[i]} = Pr ∩ v∈V (G ′ ) {X (α(H[i])) α(v) = λ(v)[i]} as {{X v } v∈V (H(P G )) : G ∈ F} is T 2 -dependent$and invariant under subgraph isomorphism. Furthermore, since the processes in {Γ(G) : G ∈ F} are T 1 -dependent and invariant under subgraph isomorphism, we have that f

$Γ(G) (T G ,V (G ′ )) (H) = f Γ(H) (T H ,V (H ′ )) ((α(H[1]), . . . , α(H[k]))) almost everywhere in H G (T G , V (G ′ ))$, and that the probability space

$H G (T G , V (G ′ )) with measure dP G (H G (T G , V (G ′ ))) is isomorphic to H H (T H , V (H ′ )) with measure dP H (H H (T H , V (H ′ )))$, where the isomorphism brings

$H into (α(H[1]), . . . , α(H[k])). Hence, H G (T G ,V (G ′ )) i∈[k] Pr ∩ v∈V (G ′ ) {X (α(H[i])) α(v) = λ(v)[i]} f Γ(G) (T G ,V (G ′ )) (H) dP G (H G (T G , V (G ′ ))) = H H (T H ,V (H ′ )) i∈[k] Pr ∩ v∈V (H ′ ) {X (H[i]) v = λ(v)[i]} f Γ(H) (T H ,V (H ′ )) (H) dP H (H H (T H , V (H ′ ))) = Pr ∩ v∈V (H ′ ) {Y (Γ(H)) v = λ(v)} ,$concluding the proof.

Remark 6.9. When the underlying graph is a directed graph, Lemma 6.8 guarantees invariance under subgraph isomorphisms that keep edge orientation.

We are going to work on rooted pseudotrees and pseudoforests. A pseudotree is a graph that is connected and contains at most one cycle. A pseudoforest is a graph obtained by the disjoint union of pseudotrees; an equivalent definition of pseudoforest is a graph in which each connected component has no more edges than vertices. Note that a pseudotree might contain multiple edges: however, we assume it does not contain self-loops as self-loops are useless communication links in the LOCAL model. A rooted tree is a tree where each edge is oriented and all nodes have outdegree at most 1: it follows that all but one node have outdegree exactly 1 and one node (the root) has outdegree 0. Trivially, a tree can be rooted by selecting one node and orienting all edges towards it. A rooted pseudotree is a pseudotree where each edge is oriented and each node has outdegree at most 1: if the pseudotree contains a cycle, then all nodes necessarily have outdegree exactly 1. Any pseudotree can be oriented so that it becomes rooted: just orient the cycle first (if it exists) in a consistent way, then remove it, and make the remaining trees rooted at nodes that belonged to the cycle. A rooted pseudoforest is the union of rooted pseudotrees.

We will show that pseudoforests of maximum degree ∆ admit a O(log * ∆)-dependent 3-coloring distributions. In order to do so, we use a color reduction technique that follows by known revisions of the Cole-Vishkin technique [[29,](#b30)[42]](#b43). Lemma 6.10 (port-numbering algorithm for color reduction in pseudoforests [[29,](#b30)[42]](#b43)). Let G be a pseudoforest with countably many nodes. Assume G is given as an input a k-coloring for some k ≥ 3. There exists a deterministic port-numbering algorithm that does not depend on the size of G and outputs a 3-coloring of G in time O(log * k). Now we are ready to prove our result on pseudoforests. Lemma 6.11 (Finitely-dependent coloring of rooted pseudoforests). Let F be a family of rooted pseudoforests of countably many nodes of maximum degree ∆. Then, there exists an outcome that associates to each graph G ∈ F a O(log * ∆)-dependent distribution on the vertices of G that gives a 3-coloring of G. Furthermore, the family of distributions outputted by the outcome are invariant under subgraph isomorphisms.

Proof. Let us fix the rooted pseudoforest G ∈ F. Let P G be the family of all subgraphs of G formed by the disjoint union of directed paths and cycles. Notice that P G is closed under node removal and disjoint graph union. Furthermore, for any two pair of isomorphic subgraphs

$G 1 , G 2 ⊆ G, G 1 ∈ P G =⇒ G 2 ∈ P G .$Let H(P G ) be the graph formed by the disjoint union of a copy of each element of P G . By Corollary 6.3, H(P G ) admits a 1-dependent 4-coloring distribution {X v } v∈H(P G ) (with colors in [[4]](#b5)), such that {{X v } v∈H(P G ) : G ∈ F} is invariant under subgraph isomorphism. Consider now a (non-proper) coloring of the pseudoforest in which each node u colors its indegree neighbors with a permutation of {1, . . . , indeg(u)} sampled uniformly at random: if a node has outdegree zero, then it is deterministically colored with color 1. Such a coloring is described by a 2-dependent distribution {Z v } v∈V that is (trivially) invariant under subgraph isomorphisms that keep edge orientation. Also, {Z v } v∈V identifies ∆ in disjoint random subset of nodes V 1 , . . . , V ∆ in , where nodes in V i are colored with the color i, and ∆ in is the maximum indegree of the graph.

$Let Γ(G) = (G[V 1 ], . . . , G[V ∆ in ])$, where Γ(G)[i] is the random graph induced by V i . Furthermore, observe that, the output of Γ(G)[i] is the disjoint union of oriented paths and/or oriented cycles, with Γ(G)[i] being the i-th entry of the ∆ in -tuple Γ(G) (see Fig. [1](#fig_0)). Notice that process Γ(G) is 2-dependent and is a random ∆ in -decomposition of G in P G (according to Definition 6.6), such that the random decompositions in {Γ(G) : G ∈ F} are invariant under subgraph isomorphism. By Lemma 6.8, the random process {Y

$(Γ(G)) v } v∈V , that is induced by the action of {X v } v∈H(P G ) over the random ∆ in -decomposition Γ(G) (according to Definition 6.7), is a 4-dependent distribution that gives a 4∆ in -coloring of G: in fact, Γ(G)[i] and Γ(G)[j] are disjoint if i ̸ = j, hence only one entry of Y (Γ(G)) v is non-zero, for all v ∈ V . We then combine {Y (Γ(G)) v$} v∈V and a modified version of the port-numbering algorithm from Lemma 6.10 where at round 0 each node permutes port numbers locally u.a.r.: by Lemma 6.4, we obtain an O(log * ∆)-dependent 3-coloring distribution {Q

$(G) v } v∈V (G) of G such that processes in {{Q (G) v } v∈V (G) : G ∈ F} are invariant under subgraph isomorphisms.$Our finitely-dependent coloring of pseudoforests can be used as a baseline to provide a (∆ + 1)coloring of graphs with maximum degree ∆. The tool we use is again an application of the Cole-Vishkin color reduction technique [[56]](#b57). Lemma 6.12 (port-numbering algorithm for color reduction of general graphs [[56]](#b57)). Let G = (V, E) be a graph with maximum degree ∆ and countably many nodes. Suppose G is given in input a k-coloring  Figure [2](#fig_1): A decomposition of a graph of maximum degree ∆ = 5 in rooted pseudoforests: for the sake of image clarity, we focus on the undirected case. In Fig. [2a](#fig_11), each node v rearranges its port-numbers with a uniformly sampled permutation of the elements in [deg(v)]. As shown in Fig. [2b](#fig_11), edges hosting port number i at some endpoint are oriented away from that port (in case both endpoints host port number i, the edge is duplicated) and form a rooted pseudoforest.

for some k ≥ ∆ + 1. There exists a deterministic port-numbering algorithm that does not depend on the size of the input graph and outputs a

$(∆ + 1)-coloring of G in time O log * k + √ ∆ log ∆ .$We first present a corollary of Lemma 6.12 where we characterize the combination of an input finitely-dependent coloring distribution and the port-numbering color-reduction algorithm. Corollary 6.13. Let G = (V, E) be a graph with maximum degree ∆ and countably many nodes. Let {X v } v∈V be a T -dependent distribution that gives a k-coloring of G, with k ≥ ∆+1. Then there exists a distribution Y vv∈V that gives a (∆ + 1)-coloring of G and is

$O log * k + √ ∆ log ∆ + T -dependent. Furthermore, if {X v } v∈V is invariant under subgraph isomorphism, then so it is {Y v } v∈V .$Proof. We combine the distribution {X v } v∈V with a modified version of the port-numbering algorithm from Lemma 6.12 where at round 0 each node permutes port numbers locally u.a.r.: Lemma 6.4 implies the existence of an [Lemma 6.4](#) implies that {Y v } v∈V has the same property. Lemma 6.14 (Finitely-dependent coloring of bounded-degree graphs). Let F be a family of graphs of countably many nodes and maximum degree ∆. Then, there exists an outcome that associates to each graph G ∈ F an O( √ ∆ log ∆)-dependent distribution on the vertices of G that gives a (∆ + 1)-coloring of G. Furthermore, the family of distributions outputted by the outcome are invariant under subgraph isomorphisms.

$O log * k + √ ∆ log ∆ + T -dependent (∆ + 1)-coloring distribution of G. If {X v } v∈V is invariant under subgraph isomorphism,$Proof. Let us fix G = (V, E) ∈ F First, if G is not directed, then duplicate each edge and give to each pair of duplicates different orientations. Since a coloring of the original graph is a proper coloring if and only if the same coloring is proper in the directed version, w.l.o.g., we can assume G to be directed.

Let P G be a family of all subgraphs of G formed by rooted pseudotrees and disjoint union of rooted pseudotrees. Notice that P G is closed under node removal and disjoint graph union. Furthermore, for any two pair of isomorphic subgraphs

$G 1 , G 2 ⊆ G, G 1 ∈ P G =⇒ G 2 ∈ P G .$The graph H(P G ) that is the disjoint union of all elements of copies of each P G is a rooted pseudoforest of maximum degree ∆ and, by Lemma 6.11, admits a 3-coloring O(log * ∆)-dependent distribution {X v } v∈H(P G ) such that processes in {{X v } v∈H(P G ) : G ∈ F} are invariant under subgraph isomorphism. Now, consider a process in which each node v samples uniformly at random a permutation of a port numbering from {1, . . . , outdeg(v)} for its outgoing edges. For each i ∈ [∆ out ], consider the graph G i induced by edges that host port i: notice that G i is a rooted pseudoforest as each node has at most one out-edge with port i. If a node has degree 0, it deterministically joins G 1 , which remains a pseudoforest. The random choice of port numbering defines a random variable Γ ∈ P k G , where Γ[i] is the graph induced by port number i: according to Definition 6.6, we obtain a random ∆ out -decomposition Γ(G) of G in P G which is 2-dependent (by construction): also, the random decompositions in {Γ(G) : G ∈ F} are invariant under subgraph isomorphisms. For an example of a possible output of the random decomposition, see Fig. [2](#fig_1).

Hence, the random process {Y

$(Γ(G)) v$} v∈V from Definition 6.7 is well defined, and provides a proper 3 ∆ -coloring of G. By Lemma 6.8, the random process {Y

$(Γ(G)) v } v∈V is O(log * ∆)-dependent and, when G ∈ F varies, the processes {Y (Γ(G)) v$} v∈V are invariant under subgraph isomorphisms. By Corollary 6.13 we obtain an O( √ ∆ log ∆)-dependent (∆ + 1)-coloring distribution on G that is invariant under subgraph isomorphism (as G ∈ F varies). Lemma 6.14 answers an open question posed by Holroyd [[44]](#b45). (See also Corollary 1.3 and the discussion around it.) Corollary 6.15. Let G = (V, E) be the infinite d-regular tree. There exists a finitely-dependent distribution giving a (d + 1)-coloring of G that is invariant under automorphisms.

Proof. Finding a (d + 1)-coloring of any d-regular tree has complexity O(1) in SLOCAL; the coloring can just be performed greedily. Lemma 6.14 yields the desired result.

## Consider any graph

$G = (V, E). For any k ∈ N + , a distance-k coloring of G is an assignment of colors c : V → Σ such that, for each node v ∈ V , all nodes in N k (v) \ {v} = {u ∈ V : dist G (u, v) ≤ k} \ {v} have colors that are different from c(v).$It is well known that any LCL problem Π that has complexity O(log * n) in the LOCAL model has the following property: there exists a constant k ∈ N + (that depends only on the hidden constant in O(log * n)) such that, if the input graph is given a distance-k coloring, then Π is solvable in time O(1) in the port-numbering model [[20]](#b21). Furthermore, no knowledge of the size of the input graph is required. Theorem 6.16. Consider any LCL problem Π with checking radius r that has complexity T ≥ r in the LOCAL model over a family F of graphs with maximum degree ∆, where T = O(log * n) for input graphs of size n. For each G ∈ F, there exists an

$O(f (∆))-dependent distribution {Y (G) v } v∈V (G)$that solves Π over G. Furthermore, the processes in {{Y

$(G) v } v∈V (G) : G ∈ F} are invariant under subgraph isomorphisms.$Proof. Fix G ∈ F of size n. As said before, there exists a constant k ∈ N + (that depends only on the description of the Π) such that, if the input graph is given a distance-k coloring, then Π is solvable in time O(1) (hiding some dependence on ∆) in the port-numbering model [[20]](#b21). Furthermore, no knowledge of the size of the input graph is required. Consider the power graph G k , where k ≥ T , defined by

$G k = {V, E k } with E k = {{u, v} : u, v ∈ V, dist G (u, v) ≤ k}. The maximum degree of G k is ∆ k . By Lemma 6.14, G k admits an O k∆ k log ∆ -dependent (∆ k + 1)-coloring distribution {X v } v∈V$that is invariant under subgraph isomorphisms (as G ∈ F varies). Notice that such a coloring provides a distance-k coloring for G.

Let A be the algorithm in port-numbering model that solves Π in time O(1) while given the distance-k coloring in input. Consider a port-numbering algorithm A ′ that simulates A and is defined as follows: At round 0, A ′ permutes ports locally u.a.r. Then, A ′ simply simulates A using identifiers given by the distance-k coloring and properly solves Π by the hypotheses. Notice that {Y v } v∈V , that is, the random process induced by combining {X v } v∈V and A ′ is a O(f (∆))-dependent distribution on G by Lemma 6.4 with the required invariance properties, and we get the thesis by choosing k = T .

## Simulation of online-LOCAL in SLOCAL for rooted trees

In this section, we show how to turn an online-LOCAL algorithm solving an LCL problem in rooted forests into a deterministic SLOCAL algorithm solving the same problem. More concretely, we prove the following theorems: Theorem 7.1. Let Π be an LCL problem with degree constraint ∆, input label set Σ in , output label set Σ out , and checking radius r > 0. In addition, let A be an online-LOCAL algorithm solving Π with locality T (n) over rooted forests. Then the following holds:

1. If A is deterministic, then there exists a deterministic SLOCAL algorithm solving Π with locality O(r) + T (2 O(n 3 ) ).

## If

A is randomized and has success probability p(n) > 0, then there exists a deterministic

$SLOCAL algorithm solving Π with locality O(r) + T (2 O(n 3 ) + 2 O(2 n 2 ) • log 1 p(n)$). Theorem 7.1 implies that any randomized online-LOCAL algorithm with locality o(log log log n) solving an LCL Π over rooted trees can be turned into an SLOCAL algorithm solving Π with locality o(log n). Next theorem shows that, over rooted trees, the class of LCL problems with complexity o(log n) is the same as that of LCL problems with complexity O(1). Theorem 7.2. Let A be an SLOCAL algorithm solving an LCL problem Π that runs with locality o(log ∆ n) over rooted forests of maximum degree ∆ and n nodes. Then, there exists an SLOCAL algorithm B solving Π with locality O(1).

It is folklore that any O(1)-round SLOCAL algorithm solving an LCL can be turned into an O(log * n)-round LOCAL algorithm solving the same LCL, implying the thesis of Theorem 1.5.

## Amnesiac online-LOCAL algorithms

We start by formalizing what an online-LOCAL algorithm sees when run for a fixed number of steps on a graph. We then define formally what we mean by amnesiac algorithms: Definition 7.3 (Partial online-LOCAL run of length ℓ). Let G be a graph with an ordering of nodes v 1 , v 2 , . . . , v n . Consider the subgraph G ℓ ⊆ G induced by the radius-T neighborhoods of the first ℓ nodes v 1 , . . . , v ℓ . We call (G ℓ , (v 1 , . . . , v ℓ )) the partial online-LOCAL run of length ℓ of G as this is exactly the information that an online-LOCAL algorithm would know about G when deciding the output for node v ℓ .

We denote by the pair ( Ḡℓ , (w 1 , . . . , w k )) where Ḡℓ ⊆ G ℓ is the connected component containing v ℓ , and (w 1 , . . . , w k ) is the maximal subsequence of (v 1 , . . . , v ℓ ) of nodes that belong to Ḡℓ . In such case, k is the length of (G ℓ , (v 1 , . .

$. , v ℓ ))[v ℓ ]. Trivially, w k = v ℓ .$If (G ℓ , (v 1 , . . . , v ℓ ))[v ℓ ] = (G ℓ , (v 1 , . . . , v ℓ )), then we say that (G ℓ , (v 1 , . . . , v ℓ )) is a partial onecomponent online-LOCAL run of length ℓ of G. In such case, we call v ℓ the center of the neighborhood.

Finally, we denote by G - ℓ the disjoint set of partial one-component online-LOCAL runs formed by v 1 , . . . , v ℓ-1 , that is, one step shorter than G ℓ ; this is exactly the information that an online-LOCAL algorithm knows before labeling node v ℓ when (G ℓ , (v 1 , . . . , v ℓ )) is a partial one-component online-LOCAL run. Definition 7.4 (Amnesiac algorithm). Let A be a deterministic online-LOCAL algorithm, and let us fix the number of nodes to be n. We say that A is amnesiac if the following condition is met:

Consider any two graphs G, H of n nodes and any adversarial orderings of the nodes (v 1 , . . . , v n ) (for G) and (u 1 , . . . , u n ) (for H). Fix any pair (i, j) ∈ [n] 2 of indices such that

$(G i , (v 1 , . . . , v i ))[v i ] = ( Ḡi , (v ′ 1 , . . . , v ′ k )) and (H j , (u 1 , . . . , u j ))[u j ] = ( Hi , (u ′ 1 , . . . , u ′ k ))$and are isomorphic (with the isomorphism being order-preserving, i.e., bringing v ′ h into u ′ h , for all h ∈ [k]). Then the outputs of u i and v j coincide. Intuitively, A is amnesiac if the output of A for some node v depends only on the local connected component of the partial online local run and not anything else the algorithm has seen.

In general, an online-LOCAL algorithm uses global memory and hence it is not amnesiac. and the whole construction needs to work for all up to g = 2 n 2 |Σ in | n different partial one-component online-LOCAL runs, the total probability of finding a good label in all N ℓ , ℓ ∈ [n], is at least q gn 2 . This probability is minuscule, but we can boost it by increasing the size of the construction by considering

$k = 1 + q -2 n 2 |Σ in | n n 2 log 1 p(n)$disjoint copies of the simulation graph independently. Letting N be the size of a single simulation graph, kN is then the size of the whole experiment. Since N = 2 O(n 3 ) in Lemma 7.5, we have

$kN = 2 O(n 3 ) + 2 O(2 n 2 ) log 1 p(n) .$The probability that each simulation graph does not contain all the necessary good labels in all layers is at most

$1 -q gn 2 k ≤ e -kq gn 2 = e -kq 2 n 2 |Σ in | n n 2 < p(n).$Since the success probability of A is p(n), by the inclusion-exclusion principle, we get positive probability that least one simulation graph in the experiment contains all the necessary good labels and, in addition, A does not fail on it. We show that the probability of sampling a good component is indeed at least q = 1/(2|Σ out | ). Consider the components in phase ℓ. For each component type, there are at least N ℓ /|Σ out | that are good. By the calculations in the proof of Lemma 7.5, future layers will use at most N ℓ /(2|Σ out | ) of them. Hence, after each step, at least 1/(2|Σ out | ) = q fraction of the components are still good and unused, as desired.

We finish the proof by showing that there indeed exists an amnesiac algorithm A ′ as in the claim. As argued above, our experiment succeeds with positive probability. After fixing the randomness f : V (H) → {0, 1} N of the algorithm A, we obtain a uniquely defined deterministic online-LOCAL algorithm A[f ] that behaves exactly as A when given the random bit string f as input. In particular, for such f , the following holds:

1. There exists a simulation graph with the necessary number of good labels in all layers.

## A[f ]

does not fail on the aforementioned simulation graph. Now the amnesiac algorithm B works as follows: Since there are finitely many possible simulation graphs and finitely many behaviors of A[f ] (for all possible f ), B executes a preprocessing phase in which it goes over all possible simulation graphs and tries all possible deterministic online-LOCAL algorithms (according to some ordering, e.g., lexicographical in the description) until it finds the pair with the properties 1 and 2 above. We take such pair and use the labeling for that component as the base for our amnesiac algorithm. Proceeding as in the proof of Theorem 7.1, we then obtain an amnesiac algorithm that has locality

$T 2 O(n 3 ) + 2 O(2 n 2 ) log 1 p(n) .$
## From online-LOCAL to SLOCAL

We first show how the SLOCAL model can nicely "partition" a rooted forest. The idea is not new and comes from [[23,](#b24)[Section 7]](#) where it is directly applied to the LOCAL model; we present here an adaptation to SLOCAL. The definition of rooted forest is given in Section 6. For a rooted forest G = (V, E) and each node v ∈ V , we denote by G v the subtree of G that is rooted at v. A root-to-leaf path v 0 v 1 . . . v k , where v i ∈ V for all i ∈ {0, 1, . . . , k}, is a path of a rooted tree such that (v i , v i-1 ) is an edge for all i ∈ [k]: we say that the path starts at v 0 and ends at v k .

Definition 7.7 ((α, β)-clustering of rooted trees). Let G = (V, E) be a rooted forest. An (α, β)clustering of G is a subset L ⊆ V of leader nodes that contains the root and is such that, for each v ∈ L, the following properties are met:

1. G v does not contain elements of L at levels 1, . . . , α -1.

2. Each maximal oriented path in G v contains exactly one element u ∈ L such that dist Gv (u, v) ∈ [α, β], unless the length of the maximal oriented path is at most β -1, in which case there is at most one element of L in the path (and possibly none).

A cluster is a maximally connected component of non-leader nodes. A closed cluster is a cluster together with its adjacent leader nodes. See Fig. [3](#fig_9) for an example.

We will combine many SLOCAL algorithms: it is easy to prove that the combination of two SLOCAL algorithms with localities T 1 , T 2 gives an SLOCAL algorithm with locality O(T 1 + T 2 ) [38, Lemma 2.3]. Lemma 7.8. Let α ∈ N + , and let G = (V, E) be a rooted forest. There is an SLOCAL algorithm with locality O(α) that produces an (α -1, 2α + 1)-clustering of G.

Proof. We combine some SLOCAL algorithms. Consider first the following algorithm A 1 : Suppose a node v ∈ V is picked by the adversary and asked to commit to something. For each root-to-leaf path v 0 v 1 . . . v 2α-1 (with v 0 = v) that v can distinguish using locality 2α -1, in parallel, v precommits that the node v α-1 is the leader of the path v α-1 v α . . . v 2α-1 unless there is another precommitment on the path within distance α from v.

Next we define an algorithm A 2 that takes as input a rooted forest labelled by A 1 : Each node v looks at its radius-α neighborhood and checks if there is some other (unique) node that precommitted for v in the neighborhood. If so, it becomes a leader and stores the path that is under its leadership, otherwise it does nothing.

The final algorithm A 3 takes as input a rooted forest labelled by A 2 and is defined as follows: Each node v becomes a leader if and only if it belongs to an oriented root-to-leaf path v 0 v 1 . . . v α-1 , with v α-1 = v lead by v 0 . All other nodes do not output anything, except the root, that becomes a leader.

We now prove that the SLOCAL algorithm A providing the clustering is the composition of A 1 , A 2 , and A 3 , and has locality O(α).

Notice that the root is a leader thanks to A 3 . Property 1 in Definition 7.7 is satisfied due to A 3 as well. Furthermore, for each maximal root-to-leaf path, consecutive leader nodes must be within distance at most 2α + 1 between each other, unless the path ends with a leaf at distance at most 2α from the last leader. In fact, if the distance between consecutive leaders is at least 2α + 2, or there is only one leader and the path is longer than 2α from the last leader, there would be at least one non-leader node that does not see any leader within distance α in the path, which is impossible due to how precommitments are done in A 1 .

We can now prove Theorem 7.1, which we restate here for the reader's convenience: Theorem 7.1. Let Π be an LCL problem with degree constraint ∆, input label set Σ in , output label set Σ out , and checking radius r > 0. In addition, let A be an online-LOCAL algorithm solving Π with locality T (n) over rooted forests. Then the following holds:

1. If A is deterministic, then there exists a deterministic SLOCAL algorithm solving Π with locality O(r) + T (2 O(n 3 ) ).

## If

A is randomized and has success probability p(n) > 0, then there exists a deterministic SLOCAL algorithm solving Π with locality O(r) + T (2 O(n 3 ) + 2 O(2 n 2 ) • log 1 p(n) ). Proof. Let G denote the input graph, which is a rooted forest, and assume A is a deterministic online-LOCAL algorithm. We can apply Lemma 7.5 to get an amnesiac algorithm A ′ that solves Π with locality T ′ (n) = T (2 O(n 3 ) ). We now show how to get an SLOCAL algorithm B solving the problem with roughly the same locality. B is the composition of four different SLOCAL algorithms B 1 , B 3 , B 3 , B 4 , each of them having locality O(T ′ ):

1. We first use Lemma 7.8 with α = 10T ′ + 4r, where r is the checking radius of Π, and obtain an algorithm B 1 that outputs an (α -1, 2α + 1)-clustering of G with locality O(α).

2. Now consider an algorithm B 2 with locality 2α + 4r that takes as input G as labelled by B 1 and works as follows: When processing a leader node v, it collects the topology of the radius-2r neighborhood of the set composed of the closest leaders v sees in each root-to-leaf path. Then, v runs A ′ in this neighborhood and precommits a solution to the LCL for the nodes in such neighborhood. If v is the root, it also presents its own radius-2r neighborhood to A ′ and precommits a solution for the whole neighborhood. Observe that all such neighborhoods are disjoint by construction of the (α -1, 2α + 1)-clustering.

3. B 3 also has locality 2α + 4r and just makes all nodes whose label has been precommitted by some other node actually output the precommitted label. (The output of B 2 guarantees there are no conflicts to be resolved.)

4. Finally, B 4 has again locality 2α + 4r and just brute-forces a solution in each cluster. The solution is guaranteed to exist because A ′ was run in all disjoint neighborhoods and works correctly. In fact, B 4 can just continue calling A ′ on each cluster it sees. The combination of B 1 , B 2 , B 3 , and B 4 yields a deterministic SLOCAL algorithm B that has locality O

$(α) = O(r) + O(T ′ (n)).$The same argument applies to a randomized online-LOCAL algorithm, the only change is that now

$T ′ is T ′ (n) = T (2 O(n 3 ) + 2 O(2 n 2 ) • log 1 p(n) ).$7.4 From SLOCAL to LOCAL It is folklore that all LCL problems that have complexity O(1) in SLOCAL belong to the complexity class O(log * n) in LOCAL. Then, to obtain Theorem 1.5, it suffices to show that all LCLs with complexity o(log n) in SLOCAL in rooted trees actually belong to the complexity class O(1). We restate Theorem 7.2. Theorem 7.2. Let A be an SLOCAL algorithm solving an LCL problem Π that runs with locality o(log ∆ n) over rooted forests of maximum degree ∆ and n nodes. Then, there exists an SLOCAL algorithm B solving Π with locality O(1).

Proof. Assume Π has checking radius r ≥ 0, and T (n) = o(log ∆ n) to be the locality of A. Furthermore, w.l.o.g., suppose T (n) ≥ r for all n large enough. Let N be a large enough integer such that T (N ) = k ≥ r. Let G be a rooted tree of maximum degree ∆ with n nodes. We now construct a new SLOCAL algorithm B which is the composition of many SLOCAL algorithms.

For the first SLOCAL algorithm B 1 , we make use of Lemma 7.8 where α = ⌊(log ∆ N -2)/4⌋. Hence, B 1 yields an (α -1, 2α + 1)-clustering of G in time O(α), with α -1 ≥ (log ∆ N -10)/4 and 2α + 1 ≤ (log ∆ N )/2. Hence, any closed cluster will have at most ∆ 2α+1 ≤ √ N nodes (including the adjacent leader nodes).

Then, we consider a second SLOCAL algorithm B 2 that takes G and the (α -1, 2α + 1)-clustering of G in input and reassigns identifiers from the set [N ] "locally". To better describe how B 2 works, let us define some notation. For each leader node v, let C v denote the closed cluster where v is the leader node of minimum level, and let L v be the set of leader nodes in C v except for v. Consider a partition of the nodes in (C v \ N k (v)) ∪ N k (L v ) according to their distance from v. More specifically, C

v contains all nodes that have distance between k + 1 and ⌊(α -1)/4⌋ from v, while, for i ∈ {2, 3}, C (i) v contains nodes that have distance between ⌊(α -1)/(6 -i)⌋ + 1 and ⌊(α -1)/(5 -i)⌋ from v. Finally, C

v contains all the other nodes in C v , which have distance at least ⌊(α -

$1)/2⌋ + 1 from v. Notice that |(C v \ N k (v)) ∪ N k (L v )| ≤ ∆ 2α+1+k ≤ ∆ (log ∆ N )/2+o(log ∆ N ) ≤ N 2/3 for N large enough.$Then, B 2 works as follows: When a non-leader node is picked by the adversary, nothing happens. When a leader node v is selected, it precommits identifiers for the nodes in

$(C v \ N k (v)) ∪ N k (L v ). In details, it assigns identifiers from 1 to ⌊N/4⌋ to nodes in C (1) v , from ⌊N/4⌋ + 1 to ⌊N/2⌋ to C (3) v , from ⌊N/2⌋ + 1 to ⌊3N/4⌋ to C (2) v , from ⌊3N/4⌋ + 1 to N to C (4) v . Notice that, since C (i) v ≤ N 2/4$, then even ⌊N/5⌋ distinct identifiers are enough to cover the whole region C (i) v when N is large enough. Furthermore, if v is the root of the whole graph, it precommits distinct identifiers from the set {⌊3N/4⌋ + 1, . . . , N } for the nodes that have distance at most k from v (v itself included).

The third and last SLOCAL algorithm B 3 takes as input the whole rooted tree with the clustering given by B 1 and the output of leader nodes given by B 2 , and computes the solution to the problem as follows: Every node u (that does not belong to C v where v is the root of the graph) looks at its two closest leader nodes v   is crucial to observe that the boundaries of color 3 are not always compatible and that they have parities of their own (see Fig. [4](#fig_12)). Indeed, the core idea of the proof is to constrict many incompatible boundaries of color 3 to a small space, thus requiring a view of Ω(log n) to resolve them. Using the same terminology as [[22]](#b23), we count incompatible boundaries between two points by using so-called a-and b-values. The a-value is defined as an edge weight between any two nodes and captures the change of colors 1 and 2. Definition 8.2 (a-value [[22]](#b23)). Given a directed edge (u, v) and a 3-coloring of the nodes c : V → {1, 2, 3}, we define

$a(u, v) = c(u) -c(v), if c(u) ̸ = 3 and c(v) ̸ = 3 0, otherwise.$Observe that the a-value of any directed 4-cycle in a grid is equal to 0. Using the a-value, we define the b-value of a path. Definition 8.3 (b-value [[22]](#b23)). For a directed path P , its b-value is defined as

$b(P ) = (u,v)∈H a(u, v).$On a high level, the b-value of a path describes the cumulative total of incompatible boundaries along this path. Note we say "cumulative" because in this count boundaries of the same parity cancel each other out. Indeed, if we consider the b-value of a simple directed cycle in a grid, then it must be equal to 0: Lemma 8.4 (b-value of a cycle is zero [[22]](#b23)). Let C be a directed cycle in G. Then b(C) = 0.

As an example, observe that a path that starts and ends with color 3 and otherwise is colored with colors 1 and 2 always has a b-value of 0 or 1. In particular, the b-value is 1 if the distance between the nodes of color 3 is even (and thus the boundaries are incompatible). Meanwhile, a path that goes through two incompatible regions has a total b-value of 2. Nevertheless, a path going through two boundaries that are compatible has a total b-value of 0. Lemma 8.5 (Parity of the b-value [[22]](#b23)). Let P denote any directed path of length ℓ that starts in node u and ends in node v in a grid. Then, the parity of b(P ) is b

$(P ) ≡ β(u) + β(v) + ℓ (mod 2)$where β is an indicator variable stating whether a node is of color 3 or not:

$β(u) = 1, if c(u) = 3 0, otherwise.$Observe that the parity of the b-value of a path is determined by the colors of the endpoints of this path.

## From deterministic to randomized online-LOCAL

Suppose that we are given a randomized online-LOCAL algorithm with visibility radius T = o(log n). Our goal is to prove the algorithm fails to solve the 3-coloring problem. Using Yao's minimax principle, we show how to construct an (oblivious) adversarial distribution of inputs so that any deterministic online-LOCAL algorithm fails with noticeable probability.

As in [[22]](#b23), the lower bound consists of two main steps:

1. First we show how to force paths to have an arbitrarily large b-value. Generally we cannot force a large b-value between the path's endpoints (or in fact between any two fixed nodes), but we do get the guarantee that it contains two nodes v 1 and v 2 where b(v 1 , v 2 ) is large. Note the location of v 1 and v 2 is completely unknown to us since we are working with an oblivious adversary. The construction is inductive: Given a procedure that generates a path with b-value ≥ k -1 with probability ≥ 1/2, we show how to generate a path with b-value ≥ k also with probability ≥ 1/2. (Cf. the construction in [[22]](#b23) with an adaptive adversary, which succeeds every time.) Since we invoke the construction for k -1 a constant number of times, we are able to obtain any desired b-value of k = o(log n) with high probability. Note it is logical that we cannot do much better than this as it would contradict the existing O(log n) deterministic online-LOCAL algorithm.

2. The second step is to actually obtain a contradiction. (See Fig. [5](#fig_14).) First we construct a path P 1 with large b-value, say ≫ 4T , between nodes u s and v t . To get the contradiction, we wish to place two nodes w s and w t next to u s and v t , respectively, so that the four node cycle has positive b-value (which is impossible due to Lemma 8.4). If we had an adaptive adversary as in [[22]](#b23), this would be relatively simple: Since the adaptive adversary knows the location of u s and v t , it just constructs an arbitrary path P 2 with the same size as P 1 , picks any two w s and w t that are at the same distance from each other as u s and v t , mirrors P 2 if needed to obtain a non-negative b-value, and then places this at minimal distance to P 1 . We show that, allowing for some failure probability, we do not need any information about u s and v t (i.e., nor their location nor the distance between them) in order to obtain the same contradiction.

We now proceed with the proof as outlined above. Accordingly, the first step is the following: Lemma 8.6. Given any k = o(log n), there is an adversarial strategy to construct a directed path of length n o (1) with a b-value of at least k against any online-LOCAL algorithm with locality T = o(log n). Moreover, this strategy succeeds with high probability.

Proof. Consider the following recursive construction:

• If k = 0, create a single node with previously unrevealed nodes all around it (inside the visibility radius T ).

• Otherwise, repeat the following steps four times in total:

-Create four distinct paths P 1 , P 2 , P 3 , P 4 by following the procedure for k -1.

-Toss independent fair coins c 1 , c 2 , c 3 ∈ {0, 1}.

-Connect the P i 's horizontally aligned and in order while placing c i + 1 nodes between paths P i and P i+1 .

Let Q 1 , Q 2 , Q 3 , Q 4 be the four paths created by this procedure (each having their own four P i 's coming from the procedure in the previous iteration k -1). Next, we connect the Q i 's horizontally aligned with each other and in arbitrary order. Note that we need to place an additional node between each pair of Q i 's. Otherwise, we would have to have revealed the edge between the two endpoints too early to the algorithm.

Thus, for k ≥ 1 we have 16 invocations of the procedure for k -1 in total. We argue that, with probability at least 1/2, the path yielded by this procedure contains a segment with b-value at least k. By repeating this procedure O(log n) times independently, we obtain at least one path with the desired property with high probability.

To prove that the recursive construction works, we proceed by induction. Fix k ≥ 1 and suppose that the procedure for k -1 succeeds with probability p ≥ 1/2 at yielding a segment with b-value at least k -1. Let us first consider Q 1 . Let X i be a random variable that is 1 if this occurs for path P i and 0 otherwise. Then we have

$Pr 4 i=1 X i < 2 = Pr 4 i=1 X i = 0 + Pr 4 i=1 X i = 1 = (1 -p) 4 + 4p(1 -p) 3 = (1 -p) 3 (1 + 3p) ≤ 1 2 .$Hence, with probability at least 1/2 there are at least two paths P i and P j , i < j, for which the construction succeeds. Since we are dealing with paths, we may simplify the notation and write b(u, v) for the b-value of the (unique) segment that starts at u and ends at v. Let thus u i , v i , u j , v j appear in this order in

$Q 1 and |b(u i , v i )|, |b(u j , v j )| ≥ k -1.$Next we will show that, conditioned on this assumption, the probability that Q 1 contains a segment with b-value at least k is at least 1/2. Hence, a priori, Q 1 contains such a segment with probability at least 1/4. Since all Q i 's are constructed the in the same way and independently of one another, the probability that at least one of the Q i 's contains such a segment is at least 1

$-(1 -1/4) 4 > 1 -1/e > 1/2.$Let us write σ(x) for the sign function of x (i.e., σ(x) = 1 if x > 0, σ(x) = -1 if x < 0, and σ(0) = 0). To see why the above holds for Q 1 , consider the two following cases:

$Case 1: σ(b(u i , v i )) = σ(b(u j , v j )). Using Lemma 8.5, we have that b(v i , u j ) ≡ β(v i ) + β(u j ) + c i + • • • + c j-1 (mod 2$). Since β(v i ) and β(u j ) are fixed, and the coin tosses are independent, we have |b(v i , u j )| ̸ ≡ k -1 (mod 2) with probability 1/2. Assuming this holds, we have either |b(v i , u j )| ≥ k, in which case we are done, or |b(v i , u j )| ≤ k -2. Since b(u i , v i ) and b(u j , v j ) have the same sign, we have |b

$(u i , v j )| = |b(u i , v i ) + b(v i , u j ) + b(u j , v j )| ≥ |b(u i , v i ) + b(u j , v j )| -|b(v i , u j )| ≥ 2(k -1) -(k -2) = k. Case 2: σ(b(u i , v i )) ̸ = σ(b(u j , v j )).$Arguing by using Lemma 8.5 as before, we obtain that |b(v i , u j )| ̸ ≡ 0 (mod 2) holds with probability 1/2. Assuming this is the case, we have thus

$|b(v i , u j )| ≥ 1. Without restriction, let σ(b(u i , v i )) = σ(b(v i , u j )). Then |b(u i , u j )| = |b(u i , v i ) + b(v i , u j )| ≥ k -1 + 1 = k.$Finally, let us confirm that the construction fits into the ( √ n × √ n)-grid. We only reveal at most 2T + 1 = o( √ n) in a column, so we need to only consider nodes along a row. The initial path contains m 0 = 2T + 1 visible nodes and in the i-th recursive step we have m i ≤ 16p i-1 + 27 visible nodes in total for i ≥ 1. (Inside each Q i we need at most 2(4 -1) = 6 additional nodes to join the four P i 's, and to join the four Q i 's we need an additional 3 nodes.) Solving the recursion, in the k-th step we have thus

$m k = 1 5 2 4k+1 (5T + 7) -9 = n o(1)$visible nodes along the path since k, T = o(log n).

We now illustrate the idea for getting the contradiction previously described. (See Fig. [5](#fig_14).) Invoking Lemma 8.6, we obtain a path P 1 with a b-value of 4T + 4 between two nodes u s and v t (whose positions are unknown to us). Letting L be the length of P 1 , we (arbitrarily) create a second path P 2 of length 10L and then randomly choose how to align P 1 and P 2 . More specifically, letting u be the first node in P 1 , we choose some node w of P 2 uniformly at random and align u and w; then we mirror P 2 with probability 1/2 and reveal it at distance 2T + 2 to P 1 (which is consistent with all previously revealed nodes).

The reason why this works is the following: Since P 1 has length L and P 2 length 10L, with at least 4/5 we align the paths so that each node in P 1 has a matching node underneath it in P 2 . Let w s and w t be the nodes matching u s and v t , respectively. Then because we mirror P 2 with probability 1/2, we get that b(w s , . . . , w t ) is at least zero in expectation (conditioned on having properly aligned the two paths). Hence, with probability at least 2/5 we obtain a cycle (u s , . . . , v t , . . . , w t , . . . , w s , . . . , u s ) where b(u s , . . . , v t ) > 4T + 4, b(w t , . . . , w s ) ≥ 0, and b(v t , . . . , w t ), b(w s , . . . , u s ) ≥ -2T -2 (due to the distance between the two paths).

Proof of Theorem 8.1. Observe that, using Lemma 8.6, we can construct a path P 1 = (u 0 , . . . , u L ) of length L = n o (1) where some segment (u s , . . . , u t ) of P 1 has a b-value of k > 4T + 4. Next we construct a path P 2 = (v 0 , . . . , v 10L ) of length 10L that will be placed below P 1 . We assume that the points of the path are revealed to the algorithm in some predefined order.

The position and orientation of P 2 are chosen as follows:  Here the blue area includes the nodes revealed so far around the path segments (u 0 , . . . , u L ) and the corresponding part of P 2 underneath it. The green segments are the two segments we consider in the proof. A cycle going through both paths leads to a contradiction.

The nodes of P 2 are revealed to the algorithm in the same predefined and possibly mirrored order. See Fig. [5](#fig_14) for an example.

We next prove that we obtain the desired lower bound. First notice that, since we pick v r ∈ [L, 9L], every node in P 1 has a counterpart in P 2 whether we mirror P 2 or not. Let (w s , . . . , w t ) denote the segment matched to (u s , . . . , u t ). Consider the case where either of the following is true: (In fact, the probability is exactly 1/2 if b(w t , . . . , w s ) > 0 and 1 if b(w t , . . . , w s ) = 0.) Thus, the probability that b(w t , . . . , w s ) ≥ 0 is > (2/3) • (1/2) = 1/3. From here on, we proceed as in [[22]](#b23). By Lemma 8.4, the cycle (u s , . . . , u t , . . . , w t , . . . , w s , . . . , u s ) must have a b-value of 0. However, recall that the b-value of a path is bounded by its length by definition. In our case, -2T -2 < b(u t , . . . , w t ) < 2T + 2, -2T -2 < b(w s , . . . , u s ) < 2T + 2, b(u s , . . . , u t ) ≥ k, and b(w t , . . . , w s ) ≥ 0. In order for the b-value of the cycle to be 0, we would need to have 2(2T + 2) ≥ k, which is a contradiction. Since this occurs with noticeable probability (i.e., > 1/3), the claim follows. Proof of [Lemma 11.3](#). This proof closely follows the argument presented in [2, [Lemma 5.6]](#). Let Π denote an LCL problem with a constant checking-radius r, and suppose A is a randomized online-LOCAL algorithm solving Π with constant locality T . Define β = T + r + 1. As in [2, Lemma 5.6], we consider an input-labeled graph P formed by many copies of all feasible input neighborhoods with a radius of β. We can depict P as a collection of disjoint path fragments that we later connect to each other and create a long path with. Each of these path fragments has size 2β + 1, so the node in the center of each segment (i.e., the (β + 1)-th node in the segment) has the same view (up to radius β) as in the final path. Let V be the set of central nodes within these fragments.

We apply A to each node within the radius-r neighborhood of the nodes in V following an arbitrary order and then terminate. These nodes have the same view as in the final path because their distance to the endpoints is at most β -r = T + 1. We iterate the process multiple times, yielding a distribution of output labels around the central nodes. Given that the size of P is constant, there exists an output labeling L of the nodes occurring with probability p = Ω(1). If needed, we can augment P with (constantly many) additional nodes such that |P | > 1/p.

We set this labeling L as the deterministic output for graph P and proceed similarly to the deterministic case by constructing the canonical labeling f from L. Then, we use the function f to construct a LOCAL algorithm with O(log * n) locality following the same steps as in the proof of [[2,](#)[Lemma 5.6]](#). It is important to highlight that it is feasible to fill any gap of sufficient length between parts labeled with the canonical labeling: If the algorithm A would never produce a valid labeling for this gap, then the algorithm would fail to label P with probability at least p and, since p ≥ 1/|P |, the algorithm A would not succeed with high probability.  LOCAL model within T = 2 steps of synchronous communication. The difficulty is that a given node ignore which G ∈ G connects the set of labelled nodes: it can only discover a local part of the structure of G by communicating to its neighbors. The most general (T = 2)-round strategy for the node 1 consists of the following procedure, which alternates between randomized processing steps and communication steps (the computational power and size of exchanged messages are unbounded):

Processing 0: Sample a random real number and store it locally.

Communication 1: Send all stored information, including the sampled random number, to all neighbors. Receive information from all neighbors and store it for subsequent rounds (in G 0 , the neighbors are 2, 3, 5).

Processing 1: Process all stored information (possibly in a randomized way) and store the result. [4](#foot_5)Communication 2: Send all stored information, including all received messages and the outputs of processing steps, to all neighbors. Receive information from neighbors and store it.

Processing 2: Process all stored information (possibly in a randomized way) to output a color.

Such T -round strategy on the graph G can be represented formally as a circuit C G,T , such as in Fig. [8a](#fig_23) where semicircles, line wires, and squares respectively represent the sampling of a random number, the transfer (or storage) of information, and the processing of information. Once the concrete operations performed by the nodes (i.e. randomness sampling and processing) are made explicit, it is possible to compute (using classical information theory) the exact output distribution of the strategy on graph G, that is, the probability distribution Pr [c 1 , . . . , c n | C G,T ] of observing that the set of nodes {1, . . . , n} outputs the colors {c 1 , . . . , c n } when connected as per one of the lifebuoy-shaped graphs G ∈ G. Importantly, in our model, the operations performed by the nodes cannot depend on the connection graph G.   The above circuits represent non-signaling strategies (it includes as special cases the classical strategies and quantum strategies) executed by the nodes 1 and 2 in various scenarios. The semicircles represent private (Fig. [8a](#fig_23)) or shared (Fig. [8b](#fig_23) and Fig. [8c](#fig_23)) arbitrary-but-non-signaling resources; the wires depict communication (or storage), and the squares are local operations (using possibly private resources). The last layer of gates (or measurements) outputs the individual colors of the nodes (i.e. classical variables). Since the nodes start with no knowledge about the identity of their neighbors, the operations of the gates are a priori independent of the graph structure (as long as the nodes have the right degree). For the special cases of classical and quantum strategies, the output distribution can be computed directly from those circuits, which define it uniquely. Definition A.7 (Device-replication principle). Identical and independent copies of non-signaling gates and non-signaling resources can be prepared [10](#foot_7) .

Then, we obtain the following definition: Note a subtlety related to subgraph isomorphisms in the bounded-dependence model. There is the variant where the nodes have identifiers in G, and the one where they do not. When the nodes do not have any identifiers, the class of subgraph isomorphisms of all alternative graphs H created out of the nodes in G and their replicates is obviously larger than with identifiers, because the subgraph isomorphisms must respect the identifiers. However, even if the nodes of G do have distinct identifiers, as H is created out of possibly many copies of the original nodes of G, H might contain several nodes with the same identifiers. Hence, the group of subgraph isomorphism of H might be nontrivial even if all nodes in G have distinct identifiers. For instance, in lifebuoy-shaped graphs, one could consider the case represented in Fig. [9](#fig_22), which starts from the graph G 0 in Fig. [7b](#fig_18), duplicates all nodes, and constructs a new graph H of 24 nodes with identifiers ranging from 1 to 12 with one non-trivial graph isomorphism cyclically permuting the nodes.

![All LCL problems solvable with locality O(log * n) in the classical deterministic LOCAL model admit a finitely-dependent distribution, i.e., a bounded-dependence distribution with constant locality: classical LOCAL algorithm, locality O(log * n) bounded-dependence distribution, locality O(1) non-signaling distribution, locality O(1)]()

![In rooted trees, if we can solve an LCL problem with locality o(log log log n) in the randomized online-LOCAL model (or any of the weaker models, such as quantum-LOCAL), we can solve it with locality O(log * n) in the classical deterministic LOCAL model: quantum-LOCAL algorithm, locality o(log log log n) bounded-dependence distribution, locality o(log log log n) randomized online-LOCAL algorithm, locality o(log log log n) classical LOCAL algorithm, locality O(log * n)]()

![Fix a distributed algorithm A in the quantum-LOCAL model (with shared global information and quantum state) that runs in T rounds on graphs with n nodes. Let G = (V, E) be some n-node input graph. Apply A repeatedly to G to obtain some probability distribution Y (G) of outputs. Now fix some subset of nodes U ⊆ V , and consider the restriction ofY (G) to U , in notation Y (G)↾ U . Let G[U, T ] be the radius-T neighborhood of set U in G. Now modify G outside G[U, T ] to obtain a different n-node graph G ′ with G[U, T ] = G ′ [U, T ]. Apply A to G ′repeatedly, and we obtain another probability distribution Y (G ′ ) of outputs. If Y (G)↾ U ̸]()

![hence the distribution of the port numbers in G[N T 2 (U, G)] is the same as that in H[N T 2 (f (U ), H)] because each node permutes the port numbers locally u.a.r. Thus, Pr ∩ u∈U {Y (G) u = λ u } must be the same as Pr ∩ u∈U {Y (H)]()

![and suppose that the random decompositions in {Γ(G) : G ∈ F} are invariant under subgraph isomorphism. Suppose there is T 2 -dependent distribution {X v } v∈V (H(P G )) , taking values in a finite set Σ, such that {{X v } v∈V (H(P G )) : G ∈ F} is invariant under subgraph isomorphism, where H(P G ) is obtained by the disjoint union of a copy of each element of P G . Let {Y (Γ(G)) v]()

![Figure 1: A decomposition of rooted pseudoforests in directed paths and cycles: each node v colors its indegree neighbors with a uniformly sampled permutation of the elements in [indeg(v)]. The graph induced by nodes colored with color i is a disjoint union of directed paths and cycles.]()

![Each node v rearranges its port numbers uniformly at random.]()

![The ∆ = 5 rooted pseudoforests that we take by considering port numbers i ∈ [∆].]()

![Figure 3: A (3, 4)-clustering of a rooted tree. The leader nodes are colored and their closed clusters marked with their respective color.]()

![u (the closest one) and v]()

![u (the second closest one) that are ancestors]()

![Figure 4: Example of a 3-colored grid with two boundaries B 1 and B 2 that have different parities. As one can see, b(u, v) = 2: No matter if we take the blue or the red path from u to v, we always cross B 1 and then B 2 .]()

![(u s , . . . , u t ) ≥ k b(w t , . . . , w s ) ≥ 0]()

![Figure 5: How to turn paths with large b-value into a contradiction.Here the blue area includes the nodes revealed so far around the path segments (u 0 , . . . , u L ) and the corresponding part of P 2 underneath it. The green segments are the two segments we consider in the proof. A cycle going through both paths leads to a contradiction.]()

![and r ∈ [L, 9L -s -t]• m = 1 and r ∈ [L + s + t, 9L]Denoting by E the event in which this occurs, note we havePr[E] = 1 2 Pr[r ∈ [L, 9L -s -t]] + 1 2 Pr[r ∈ [L + s + t, 9L]] = 8L -s -Now conditionedon E, notice that every segment (v x , . . . , v y ) where x ∈ [L + s, 9L -s] and y ∈ [L + t, 9L -t] has equal probability of being matched with (u s , . . . , u t ) either in the same direction (i.e., w s = v x and u t = v y ) or reversed (i.e., w s = v y and w t = v x ). Hence, Pr[b(w t , . . . , w s ) ≥ 0 | E] ≥ 1 2 .]()

![Dynamic-LOCAL derandomizes LOCAL and breaks symmetryThis section presents a derandomization result for the dynamic-LOCAL model: we show that not only dynamic-LOCAL can simulate randomized LOCAL with no overhead in the locality, but actually it brings the complexity class O(log * n) in randomized LOCAL down to O(1).]()

![(a) H has chromatic number 3. (b) A distributed algorithm that finds a 2coloring of the lifebuoy-shaped graphs G ∈ G would by definition 2-color the particular instance G = G 0 .]()

![Figure 7: The joint view of the couple of nodes u, v = 1, 2 after T = 2 rounds of communication is limited to the gray area. In this region the graphs G 0 and H are identical. The non-signaling principle implies that the outputs (c 1 , c 2 ) must therefore be identically distributed in both G 0 and H.]()

![Circuit representation of a non-signaling strategy on graph G 0 of Fig.7b, without a shared resource. Note the cyclicity of the circuit. Highlighted in red is the past-light-cone of the joint output (c 1 , c 2 ), that is the set of gates which connects to the output gates producing (c 1 , c 2 ).]()

![Circuit representation of a non-signaling strategy on graph G 0 of Fig.7b, with an arbitrary shared resource. The joint output (c 1 , c 2 ) remain, after T = 2 rounds of communication, independent of the graph structure around nodes 11 and 12, because the difference lies outside their joint past light-cones.Circuit representation of a non-signaling strategy on graph H of Fig.7a, with an arbitrary shared resource. Note the difference between this circuit and the one of Fig.8b(namely, the different connections between the top layer and bottom layer of the circuit) is only manifested outside the joint past light-cone of the outputs (c 1 , c 2 ).]()

![Figure8: The above circuits represent non-signaling strategies (it includes as special cases the classical strategies and quantum strategies) executed by the nodes 1 and 2 in various scenarios. The semicircles represent private (Fig.8a) or shared (Fig.8band Fig.8c) arbitrary-but-non-signaling resources; the wires depict communication (or storage), and the squares are local operations (using possibly private resources). The last layer of gates (or measurements) outputs the individual colors of the nodes (i.e. classical variables). Since the nodes start with no knowledge about the identity of their neighbors, the operations of the gates are a priori independent of the graph structure (as long as the nodes have the right degree). For the special cases of classical and quantum strategies, the output distribution can be computed directly from those circuits, which define it uniquely.]()

![Figure 9: In the bounded-dependence model, one can find non-trivial subgraph isomorphisms even when the nodes are provided with unique identifiers.]()

![Bounded-dependence model, with unique identifiers). The distributionPr [c 1 , . . . , c n | C G,T ]with unique identifiers has bounded dependence with locality T on graph G (without pre-shared non-signaling resources) if and only if for all possible alternative connecting graph H of the nodes and their replicates, there exists a distribution Pr [c 1 , . . . , c m | C H,T ] such as the non-signaling and independence principles are respected, and such that the distribution is invariant under subgraph isomorphisms.]()

Note that the dependency T might depend on other graph parameters such as the maximum degree ∆, the chromatic number χ, etc.

We remark that, as opposed to Definition 5.2, the isomorphism must preserve the input but not the node identifiers.

Choose a node v r ∈ [L, 9L] of P 2 uniformly at random. This node is placed below the node u 0 of P 1 at 2T + 2 distance from it.

Throw a fair coin m ∈ {0, 1}. If m = 1, mirror P 2 along the vertical axis that goes through u 0 and v r .

A reader used to standard quantum nonlocality should see the graph -H or G ∈ G -as an input of the problem, split and distributed among the local parties.

In classical information theory, it is known that intermediate processing gates can be taken as identity gates, that is, any strategy can be simulated by a two-layer circuit where the parties send their first random number to all parties up to a distance T , and then make a unique processing step after all the communication has taken place. In quantum and non-signaling theories, this is not the case anymore[[28]](#b29).

While we are deceiving the nodes by promising a lifebuoy-shaped connecting graph but imposing instead H, the nodes cannot locally detect the fraud in T = 2 communication steps or less. (More formally, the past light-cone in H of any node is then compatible with a lifebuoy-shaped graph -an individual node cannot detect the difference and must therefore, according to the non-signaling principle, output a color as if it were in a lifebuoy-shaped graph.)

Note that this principle does not imply that one can duplicate unknown non-signaling resources: device-replication compatible with the quantum no-cloning theorem.

