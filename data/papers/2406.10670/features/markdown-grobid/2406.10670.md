# CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training

## Abstract

## 

Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models. In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.

## Introduction

The content of the data that a language model is trained on can have profound effects on its performance and the efficiency of the training process [[Rae et al., 2021](#b42)[, Longpre et al., 2023](#b31)[, Penedo et al., 2023](#b39)[, Soboleva et al., 2023](#b49)[, Li et al., 2024]](#). But it remains an open research question how to decide which data to include in the training set. In this paper, we analyze a family of loss-based approaches for targeted selection of pre-training data, propose a simple approach that outperforms existing methods, and provide some preliminary evidence of favorable scaling properties.

To formulate the data selection problem, we first need to specify an objective that quantifies whether the selected data is good. Defining this objective requires evaluating a pre-trained language model, which is an area of active research [[Gao et al., 2023](#)[, Magnusson et al., 2023](#)[, Engstrom et al., 2024](#b14)[, Chang et al., 2024]](#b8). For this paper, we will take the goal to be to maximize performance on a set of downstream tasks. Since the preferred metrics on a given set of tasks are not necessarily the same nor amenable to direct optimization, we consider the likelihood of sequences sampled from the downstream tasks as a proxy objective. With this objective, we now have a straightforward goal: given a very large corpus of sequences and a small amount of high-quality data from a set of downstream tasks, we want to select a subset from the corpus so that training on the selected data  CoLoR-Filter (τ = 64) Random

Figure [1](#fig_2): Learning curves for 1.2 billion parameter language models trained on data selected by CoLoR-Filter using smaller 150 million parameter auxiliary models for two different target distributions. (Left) We target and evaluate loss on Books, lower is better. (Right) We target and evaluate accuracy on a suite of 8 downstream tasks from [[Groeneveld et al., 2024]](#b20), higher is better.

In both cases, test data is held out from the data used by CoLoR-Filter to guide selection. τ is the subset size multiplier denoting the number of examples considered for each selected data point. The CoLoR-Filter line terminates when we run out of data in C4 (≈175b possible tokens).

maximizes likelihood on the downstream tasks. Then we can also test performance on the tasks under their preferred metrics.

From this objective, we derive an algorithm dubbed CoLoR-Filter (Conditional Loss Reduction Filtering). In Section 2 we derive this method by applying Bayes' rule and approximate empirical Bayes to the downstream likelihood objective. The resulting method is simple and intuitive: each sequence is scored by the difference in likelihood between a "prior" model and a "conditional" model that results from fine-tuning the prior model on the downstream data. Sequences that are more likely under the fine-tuned model are good. We also compare this algorithm to prior work (e.g., [[Mindermann et al., 2022]](#b35)) and discuss computational costs.

To evaluate our method, we consider two tasks. First, in Section 5, we consider a semi-synthetic task where the downstream task is language modeling on Books. Given access to C4 [[Raffel et al., 2020]](#b43) as potential pre-training data and a small (25 million tokens) sample of data from Books, we use CoLoR-Filter and a variety of baselines to select 3 billion tokens. We find that data selected by CoLoR-Filter can substantially outperform models trained on 8x as much randomly chosen data. Second, in Section 6, we consider a suite of 8 downstream multiple-choice tasks from [Groeneveld et al. [2024]](#b20). As downstream data we take the training sets of the tasks, but we evaluate accuracy on the held-out test sets. We again find that selecting with CoLoR-Filter outperforms training on 8x as much randomly selected data. Moreover, in both tasks, performance scales smoothly with the hyperparameter τ that governs how aggressively we select the data, suggesting that further scaling would yield further improvements.

In addition to finding that CoLoR-Filter can select good subsets of data, we also consider the computational cost of the selection procedure itself. CoLoR-Filter only requires running inference of the two auxiliary models to select data. This is computationally beneficial compared to online methods like RHOLoss [[Mindermann et al., 2022]](#b35) since inference is cheaper than training and is entirely parallelizable. To maximize the computational benefits we also show that data selected with a small (150 million parameter) model can be transferred to a larger (1.2 billion parameter) model. Results are shown in Figure [1](#fig_2), showing substantial efficiency improvements.

## Setting and Derivations

Assume that we are given a large pre-training dataset D train , a small downstream dataset D down from the downstream task(s) of interest, and a "prior" dataset D prior we can use as prior knowledge (in practice we often just sample from D train ). We will assume for all practical purposes that D train is infinite and training proceeds in the "online" or "single pass" setting where we do not repeat data points. Our goal is to choose a subset S ⊂ D train of a fixed size |S| = n that minimizes the downstream loss (maximizes the downstream likelihood).

This section introduces our CoLoR-Filter algorithm, inspired by and building upon the RHOLoss approach from prior work [[Mindermann et al., 2022](#b35)[, Evans et al., 2023]](#b15). We also discuss related algorithms applicable to this setting such as DSIR [[Xie et al., 2023]](#b62) and DSDM [[Engstrom et al., 2024]](#b14). Additional related work is discussed further in Section 3.

## Bayesian Data Selection

Our objective can be formulated as a Bayesian optimization problem, where the goal is to select a set S so as to maximize the posterior probability of D down , i.e.

min S⊂Dtrain,|S|=n log Pr(D down |S), (1) where Pr(D down |S) is the posterior probability. Applying Bayes rule we get: min S⊂Dtrain,|S|=n log Pr(S|D down ) + log Pr(S)log Pr(D down ) (2) Note that the last term does not depend on S, so it can be ignored when optimizing over S. Introducing a prior over model parameters θ, we get: min S⊂Dtrain,|S|=n log θ Pr(S|θ) Pr(θ|D down ) "conditional" + log θ Pr(S|θ) Pr(θ) "marginal"

(3)

We will refer to the two terms as the conditional and marginal terms, respectively.[foot_0](#foot_0) Note that the conditional and marginal terms together make up the negative pointwise mutual information between the selected and downstream data, which has deep connections to prior work on active learning and active sampling [[Lindley, 1956](#b30)[, Moore and Lewis, 2010](#b36)[, Houlsby et al., 2011](#b23)[, Bickford Smith et al., 2023](#b4)[, Kirsch, 2023](#b26)[, Rainforth et al., 2024]](#b44).

## CoLoR-Filter

Given that we have access to prior knowledge from the dataset D prior , we can replace the uninformed prior over θ with an empirical Bayes prior that conditions on D prior to obtain:

min S⊂Dtrain,|S|=n log θ Pr(S|θ) Pr(θ|D down , D prior ) + log θ Pr(S|θ) Pr(θ|D prior )

As this integration is still intractable, we now make our main simplifying assumption which is to replace this integration over parameters by a point estimate: ≈ min

## S⊂Dtrain,|S|=n

log Pr(S|θ prior+down ) + log Pr(S|θ prior ),

where θ prior is a model trained on D prior and θ prior+down is a model trained on both D prior and D down (in practice, we use a model that is pre-trained on D prior fine-tuned on D down ).

Moreover, this approximation leads to computational benefits by avoiding the full combinatorial optimization of subset selection. In particular, once we condition on a single model θ, and assuming the distribution over points x ∈ S is independent, i.

e. Pr(S|θ) = x∈S Pr(x|θ), we have: min {x1,...,xn}⊂Dtrain log n i=1 Pr(x i |θ prior+down ) + log n i=1 Pr(x i |θ prior ) (6) which simplifies to: min {x1,...,xn}⊂Dtrain n i=1 log Pr(x i |θ prior+down ) -(-log Pr(x i |θ prior ))

This gives our CoLoR-Filter criteria that we use to select data. This optimization selects the points with the largest conditional loss reduction (CoLoR), i.e. the points where the negative log-likelihood loss of the conditional model θ prior+down is lower than the marginal model θ prior . Intuitively, this selects data points that are more likely under the conditional model than the marginal model.

A note on data diversity. While the factorization that results from our point estimate of the parameters is computationally convenient, it makes an important simplifying assumption. In particular, the CoLoR-Filter objective no longer encourages the selection of a diverse dataset, as scores are applied independently to each point. In practice, this is remedied by a few considerations: (1) we can run CoLoR-Filter on a corpus that has already been deduplicated to prevent degenerate duplications, (2) for large n, we must select many different data points, and (3) each datapoint is itself a sequence that may contain diverse signal across tokens. We should also note this is not a unique property of CoLoR-Filter and also happens in other methods that do offline scoring like DSDM and DSIR. We defer a detailed discussion of the nuances of this issue to Appendix C.

## Related Algorithms

Connection to importance sampling. Since the CoLoR-Filter objective is written as a difference of logs, it can also be written as a log of the ratio between probabilities under θ prior+down and θ prior .

If data were actually sampled from θ prior , then this ratio would be the importance weight needed to reweight samples so that they are from the model defined by θ prior+down . Note that DSIR [[Xie et al., 2023]](#b62) directly attempts to perform importance sampling from D train to D down instead of optimizing performance on the downstream data. Thus, DSIR ends up with a somewhat related algorithm except in DSIR: (1) there is no language model, just features of a full data point (hashed n-grams), and (2) the algorithm samples rather than optimizes.

Connections to DSDM. Another closely related approach is DSDM [[Engstrom et al., 2024]](#b14) which uses a TRAK Datamodel estimator [[Ilyas et al., 2022](#b24)[, Park et al., 2023]](#b38) to score datapoints and then selects the top-n points. The motivation and setting of DSDM are similar to CoLoR-Filter, but DSDM relies on TRAK which constructs a linear approximation of the influence that data points have on each other. Instead, CoLoR-Filter operates directly in function space by comparing the loss between models directly rather than relying on linear approximations or Datamodels [[Ilyas et al., 2022]](#b24).

Connections to RHO-down. CoLoR-Filter is inspired by and builds on the RHOLoss approach introduced in prior work [[Mindermann et al., 2022]](#b35) with subtle but significant differences in the setting: the original RHO paper focuses on cases where the hold-out data is sampled from the same distribution as D train over multiple epochs of training. In contrast, we focus on selecting data to target downstream distributions that are different from D train and where we only take a single pass over the data. Here, we derive a straightforward adaptation of RHOLoss to our setting, which we call RHO-down.

We now derive RHO-down in our setting, aiming to illustrate the connections between RHO-down and CoLoR-Filter. First, RHO-down approximates the full subset selection problem from Equation (3) by a greedy (sequential) approximation where samples are added to S one (batch) at a time. Using a batch size of 1, the ith-sample would be ideally added according to the following criterion:

$≈ min xi∈Dtrain -log θ Pr(x i |θ) Pr(θ|D down , x <i ) + log θ Pr(x i |θ) Pr(θ|x <i ),(8)$where i ranges from 1 to n sequentially. RHO-down then uses a point estimate of the parameters (as we do in CoLoR-Filter):

$≈ min xi∈Dtrain -log Pr(x i |θ down+x<i ) + log Pr(x i |θ x<i )(9)$Finally, the RHO-down authors found that updating the conditional term to depend on x <i was unstable, so they instead approximate this by a fixed model θ down :

$≈ min xi∈Dtrain -log Pr(x i |θ down ) + log Pr(x i |θ x<i ).(10)$Note that while both CoLoR-Filter and RHO-down approximate the posterior over parameters with a point estimate, RHO-down makes a few additional approximations. This is largely a result of RHO-down attempting to increase data diversity by using a sequential approach to selection that conditions on the previously selected data x <i . This is an understandable goal, but it introduces more approximations, can cause instability by creating a non-stationary data distribution, and is computationally expensive since the data selection is no longer parallelizable. A continued discussion of the pros and cons of online selection is in Appendix C.

RHO-down + prior. We also consider a version of the algorithm that we call "RHO-down + prior" that replaces D down , θ down in the RHO-down algorithm with D prior ∪ D down , θ prior+down to incorporate the prior information. This corresponds to conditioning on both D prior and D down instead of only D down . Intuitively, this method can better leverage stronger features learned on the larger D prior to integrate the information from the small D down .

## Further Related Work

We now discuss some related work, more broadly, with regards to active learning and data curation.

Active & Curriculum learning. Our formulation of data selection has connections to classic and deep active learning [[Houlsby et al., 2011](#b23)[, Bickford Smith et al., 2023](#b4)[, Kirsch, 2023]](#b26), which are deeply rooted in optimal Bayesian experimental design [[Lindley, 1956](#b30)[, Rainforth et al., 2024]](#b44), whose goal is to select a set of experiments to optimize certain information criteria [[Pukelsheim, 2006]](#b41) such as maximally reducing the uncertainty about model parameters. Various acquisition functions are proposed in deep learning regimes [[Sener and Savarese, 2018](#b48)[, Ash et al., 2019](#b3)[, 2021]](#) and most of them focus on label-efficient image classification. Another line of recent techniques share deep methodological connections but emphasize the sub-selection of available data during training (rather than the collection of additional examples typically considered in active learning) and could thus be classified as curriculum learning [e.g. Graves et al., 2017]. Among them, RHOLoss [Mindermann [et al., 2022]](#) seeks to select data based on the hold-out reference dataset from the same distribution as the training data. It has been later implemented in continual pre-training [[Lin et al., 2024]](#b29) and vision domains [[Evans et al., 2023](#b15)[, Tack et al., 2024]](#b53).

Data curation practices in pre-training. Though large-scale public web-crawled data are common data sources for pre-training models, low-quality, toxic, and uninformative content that can prevent successful pre-training is prevalent [[Wenzek et al., 2020](#b60)[, Elazar et al., 2023](#)[, Sorscher et al., 2022](#b52)[, Allen-Zhu and Li, 2024]](#b1). Therefore, practitioners design sophisticated data pre-processing pipelines such as filtering [[Brown et al., 2020]](#b6), deduplication [[Lee et al., 2022]](#b27), and mixing [[Touvron et al., 2023a,b]](#) to improve the data quality. Due to the immense scale, state-of-the-art pre-training datasets usually depend on simple heuristic filters [[Raffel et al., 2020](#b43)[, Rae et al., 2021](#b42)[, Computer, 2023](#)] (e.g., URL, length, n-gram perplexity, fastest classifiers) that can be parallelized across CPU nodes.

Besides the above rule-based filtering, model-based filtering concerns using machine learning models to score and filter data, which has been proven to be effective in vision and vision-text domains [[Schuhmann et al., 2022](#b47)[, Abbas et al., 2023](#b0)[, Fang et al., 2023]](#b16). Such approaches usually leverage a given trustworthy data source like Wikipedia or Books as the reference and contrast the raw data with it. Due to computational cost, models are often designed to be small such as n-gram [[Xie et al., 2023]](#b62), single-layer neural networks [[Joulin et al., 2017](#b25)[, Brown et al., 2020]](#b6), k-means clustering [[Tirumala et al., 2024]](#b55). There is also a growing line of work illustrating that data quality is important in shaping model training from a variety of perspectives, such as increasing data scale [[Hoffmann et al., 2022](#b22)[, Meta, 2024]](#) and using synthetic data [[Gunasekar et al., 2023]](#b21).

## Algorithms

## From Derivations to Practical Algorithms

In our experiments, we will consider four algorithms based on the above derivations. In this section we go through each of these in turn.

CoLoR-Filter. Our proposed algorithm is presented formally in Algorithm 1. Compared to the derivation, the main difference is the introduction of τ , a hyperparameter that acts as a computeperformance trade-off controlling how expensive and aggressive the data selection is. Rather than selecting data from all of D train , we take a random subset D τ of size τ n. Thus, larger τ subselect more aggressively, but at the cost of more computation. A full discussion of this cost is in Section 4.2.

Conditional only. As an ablation of CoLoR-Filter, we follow prior work [[Evans et al., 2023]](#b15) and include a baseline that only uses the conditional model to select data. Essentially, this is CoLoR-Filter if we always assume that log Pr(x|θ marg ) = 0 in Line 4 of Algorithm 1. RHO-down. We present a practical variant of RHO-down in Algorithm 2 based on the derivation presented in Section 2. The main changes to make a practical algorithm are (1) the introduction of τ as in CoLoR-Filter, and (2) performing the algorithm batch-wise instead of using single data points.

RHO-down + Prior. We can also incorporate the prior data D prior into Algorithm 2 by simply replacing Line 1 where θ cond is trained on D down with a procedure where we first pre-train θ cond on D prior and then fine-tune it on D down .

## Computational Cost

To evaluate the computational cost of the various algorithms, we use units of "model forwards" per token where we assume that a backward pass is twice as expensive as a forward pass [[Fleuret, 2023]](#b17). Note that our 150m models take about 5e8 FLOPs per model forward of a single token [[Hoffmann et al., 2022](#b22)[, Casson, 2023]](#b7). The cost of running the selection algorithms depends on m, n, τ and L defined as follows: m is the size of the prior data D prior , n is the size of the selected dataset S, τ is the hyperparameter controlling how aggressively we subselect data. Note that we assume that |D down | is so small that the cost of training a model on D down is negligible towards the total cost (and all the methods we consider just fine-tune a model once on D down ). We will also be careful to note when computation can be done in parallel before training versus computation that must happen serially during a training run. Offline algorithms like CoLoR-Filter can take advantage of parallelism to improve efficiency. In this section, we go through each method in turn and aggregate the computational costs in table 1.

Scale transfer. We also include another parameter L to cover the case where we select data using small models and use it to train a larger model [[Evans et al., 2023]](#b15). Specifically, L is the ratio of cost of one model forward of the large target model compared to the small auxiliary models used for data RHO-down. The cost of selection is still 2τ n forward passes. Then we need an additional 2n to backward the output model (since the forward is already handled during scoring). Note that we need to evaluate the marginal model online, so it is not parallelizable, but the conditional model is fixed and can be computed offline. So, the cost is 2τ n + 2n + 3nL, and the τ n conditional model computation can be done in parallel.

RHO-down + Prior. For the version with an added prior, we just add 3m cost for training the prior. Thus, the cost is 2τ n + 2n + 3nL with τ n parallelizable.

Overall, the methods all have comparable costs, with Conditional Only being the cheapest and RHO-down + Prior the most expensive. The main difference is that CoLoR-Filter and Conditional Only are easily parallelized while RHO-down and RHO-down + Prior are not. It should also be noted that when doing experimentation, offline methods like CoLoR-Filter also benefit from being able to re-use likelihoods multiple times, while RHO-based methods need to recompute the serial cost any time that some hyperparameter of the algorithm.

5 Domain Transfer: a Simple Testbed

## Setup

Training. We train language models with 150 million non-embedding parameters using the OLMo codebase [[Groeneveld et al., 2024]](#b20) and following hyper-parameter choices from [[Wortsman et al., 2024]](#b61). Unless otherwise noted, we use 150m models as the auxiliary models (θ cond , θ marg ) as well as the target model θ. Full hyperparameters are described in detail in Appendix H.

We take D down to be a small dataset of 25 million tokens sampled from the Project Gutenberg Books data subset of Dolma [[Soldaini et al., 2024]](#b51), D prior to be a dataset of 3.1 billion tokens from C4 [[Raffel et al., 2020]](#b43), and D train to be all of C4. We select a dataset S of 3.1 billion tokens (which is approximately the "chinchilla optimal" amount for models of this size). To get θ prior+down or θ down , we fine-tune or train for one epoch on D down .

Evaluation. To evaluate the efficacy of our data selection, we report cross-entropy loss of next token prediction on a held-out dataset D down from the same distribution as D down (Books).

Baselines. The simplest baseline we consider is Random sampling, which has been shown to be a strong baseline for C4 pre-training [[Engstrom et al., 2024]](#b14). We consider all four algorithms described in Section 4: CoLoR-Filter, Conditional Only, RHO-down, and RHO-down + prior. And as one extra baseline, we also include DSIR [[Xie et al., 2023]](#b62) which estimates n-gram importance weights between D train and D down , and similarly has a parameter like τ that controls how aggressively to subselect.

Note that while it is in a similar setting to ours, we do not include DSDM [[Engstrom et al., 2024]](#b14) as a baseline since there is no publicly available code and based on the appendix of that paper, it it much more computationally expensive than the methods we consider.

## Results

2 1 2 2 2 3 2 4 τ 4.2 4.4 4.6 4.8 Final Books Cross Entropy (↓) CoLoR-Filter Conditional only RHO-down RHO-down + prior DSIR Random 1x Random 8x We first run the domain transfer experiments on 150m models, sweeping across τ that controls the selected subset size. In Figure [2](#fig_3) we plot how the final performance scales with τ across methods. We see that CoLoR-Filter has the best scaling performance with increased τ , with no sign of saturation for τ = 16. We hypothesize that by using strong models to select the data, CoLoR-Filter is able to more effectively scale to larger τ than the other methods. In Figure [7](#) in Appendix A, we plot the learning curves (evaluated on the held-out validation set) for the four methods introduced in Section 4. There, we see especially clean scaling for CoLoR-Filter across the entire learning curve, substantially outperforming random selection with much less data, similar to Figure [1](#fig_2). Scale generalization. Finally, we also conduct an experiment in scale generalization (partially shown in Figure [1](#fig_2)) using the data selected by our 150m auxiliary models to train a 1.2b target model. In Figure [3](#fig_5) we show learning curves for a sweep over τ . We still see consistent gains as we scale τ for a fixed number of training tokens. Interestingly, if we fix the total number of tokens we are selecting from (i.e. where the lines end when we run out of C4), then the final performance with τ = 32 is better than all other values of τ . This shows how a strict subset of tokens can outperform a superset (e.g. τ = 16).

We should also point out here the computational savings when using CoLoR-Filter. As an example, consider τ = 16 where we match the performance of 25 billion randomly selected tokens with about 1.5 billion filtered tokens. Considering the computational costs discussed above with L = 5.  Test task CoLoR-Filter Conditional Only RHO-down RHO-down + prior DSIR Random 8x Method 5.5 4.1 2.9 7.4 5.2 5.9 -4.6 -3.9 2.8 -0.1 1.1 0.6 -2.2 0.4 -2.3 2.5 -2.1 -0.3 2.4 2.8 2.9 2.5 2.4 0.6 -10.2 -4.6 -0.2 2.4 2.1 2.9 4.9 2.6 2.1 2.7 -2.4 2.2 4.4 4.3 2.0 2.2 1.0 1.5 1.1 -0.7 2.0 5.1 3.3 1.2 -0.2 2.0 -1.6 -0.7 -2.8 0.8 Improvement over Random 1x (τ = 16) (↑) -4 -2 0 2 4 Figure 5: Performance improvement over training on an equivalent amount of random data broken down by task (except for Random 8x, which uses 8x more data). A table of results is in Appendix B.

6 Downstream Tasks

## Setup

Training. We target the 8 tasks from the OLMo paper [[Groeneveld et al., 2024]](#b20): Hellaswag [[Zellers et al., 2019]](#b63), PIQA [[Bisk et al., 2020]](#b5), ARC-challenge and ARC-easy [[Clark et al., 2018]](#b10), Openbook QA [[Mihaylov et al., 2018]](#b34), SciQ [[Welbl et al., 2017]](#b59), BoolQ [[Clark et al., 2019]](#b9), and Winogrande [[Sakaguchi et al., 2021]](#b45). Each of these datasets has a separate train split. We use these train splits to construct D down as follows: for each question we concatenate the question and the correct answer formatted as a grammatical continuation. Overall, this results in a small D down dataset of 7.4 million tokens. D prior and D train are the same as before. And we again get θ prior+down by fine-tuning θ prior for one epoch on D down . Evaluation. We evaluate on held-out data from each downstream task test or validation sets (using val if test is not publicly available).

We use the evaluation procedure from OLMo [[Groeneveld et al., 2024]](#b20) which follows [[Gao et al., 2023]](#) for evaluating these multiple-choice tasks using the rank classification approach of [Brown et al. [2020]](#b6). We report aggregat perfromance across tasks as well as the task-specific performance.

Baselines. Same as in Section 5.

## Results

While the curves themselves are noisier now due to the noisier nature of accuracy evaluation on small datasets compared to cross entropy on a large one, the same trends hold as we saw for domain transfer to Books. CoLoR-Filter in particular is scaling the best as we increase τ . Other methods do not illustrate the same clean scaling as we increase τ , which is nearly linear on a log scale for CoLoR-Filter, as seen in Figure [4](#fig_8). Full learning curves are in Appendix A.

We can also look at the performance broken down by task and illustrated relative to training on an equivalent amount (3.1 billion tokens) of randomly selected data for τ = 16 illustrated in Figure [5](#).

We see especially large gains on Hellaswag, ARC easy, Openbook QA and SciQ and actually see performance decreases on BoolQ and Winogrande. However, we should note that at this scale and with all data selected from C4, we actually found BoolQ and Winogrande to be quite noisy and not even correlated with training on 8x as much random data, so it is not clear how much weight to place on those results. Across the other tasks, the gains of CoLoR-Filter over the baselines are clear. It is an interesting direction for future work to probe more deeply into how task-dependent the gains from targeted data selection can be. Figure [6](#): Scaling CoLoR-Filter with τ when training 1.2b models with data selected using smaller 150m models. Curves end when we exhaust the data in C4.

Scale generalization. We also consider scale generalization to a 1.2b target model and illustrate the full results of a sweep over τ in Figure [6](#). Again we find significant benefits of CoLoR-Filter across scales. A full table of per-task results is in Appendix B. Again we notice that training on a strict subset of data can outperform a larger dataset.

We can again do out the calculation of computational savings for τ = 16. It now takes about 3 billion tokens for CoLoR-Filter to match the performance of training on 25 billion random tokens. This amounts to a total cost of 3m+2τ n+3nL = 3 * 3.1+2 * 16 * 3+3 * 3 * 5.5 = 154.8, which is still an upwards of 2.5x reduction in compute to achieve the same average performance across the suite of tasks. A full plot visualizing the cost in FLOPs for all τ is in Appendix D.

Task generalization. We can also test task generalization beyond the 8 tasks that were used to select the data on a few more tasks that test common sense reasoning [[Wang et al., 2019](#b58)[, Socher et al., 2013](#b50)[, Talmor et al., 2018](#b54)[, Sap et al., 2019]](#b46). Results are presented in Table [2](#tab_7) compared to a random model trained on 10x as much data. The performance indicates that the data selected by CoLoR-Filter are not overfit to the particular evaluation tasks, but captures some general notion of good data for a range of tasks. 

## Discussion

While fairly simple to derive and implement, we show that CoLoR-Filter is an effective method for data selection on C4, with promising scaling behavior up to 1.2 billion models. In our experiments, CoLoR-Filter continues to improve when only using 1 out of 64 data points considered for selection and generalizes from small auxiliary models to larger target models. This opens many potential lines of research. First, while we have considered targeted pre-training, it is possible that CoLoR-Filter could be extended to fine-tuning, continual pre-training, and more general open-domain pre-training.

In particular, it is an interesting open question whether the lack of an explicit consideration of data diversity hinders CoLoR-Filter in any of these settings. Second, CoLoR-Filter could be applied to more challenging domains in language like code generation or even applied beyond the language domain to other modalities. Finally, there is plenty of work to be done to make the algorithm more efficient and to test the limits of scale generalization.

A Learning curves for 150m models 

## B Tables of downstream results

Table [3](#): Performance for all tasks for 150m models for data selection with τ = 16. 

## Method

## C Data diversity and online vs. offline selection

Much work on active learning focuses on ensuring that we select a diverse set of data points that cover the test distribution of interest. As explained in the main text, by making a point estimate of the parameters, CoLoR-Filter is simplifying the problem and sacrificing an explicit term for diversity in the objective. In practice, this seems to be saved by the facts that (1) C4 has already been deduplicated, (2) we still select a fairly large subset without replacement, and (3) an individual sequence contains diversity across tokens.

However, the fact that CoLoR-Filter sacrifices a notion of diversity in the objective is important to consider more deeply. Here, we derive what a loss-based algorithm for data selection that prioritizes diversity would look like and why it is computationally infeasible. Then we derive an approximation (that looks somewhat like RHOLoss [[Mindermann et al., 2022]](#b35)) and show how it is empirically unstable, as was also observed previously by [[Mindermann et al., 2022]](#b35).

To derive a CoLoR-Filter-like algorithm that values diversity, we can start from Equation (3) by a greedy approximation where samples are added to S one (batch) at a time, like in RHO:

$≈ min x1,...,xn⊂Dtrain n i=1 -log θ Pr(x i |θ) Pr(θ|D down , x <i ) + log θ Pr(x i |θ) Pr(θ|x <i )(11)$Note that this sort of greedy algorithm for subset selection has a long history in active learning [[Das and Kempe, 2018]](#b12), is actually theoretically sound in some cases [[Nemhauser et al., 1978]](#b37), and is used in prior work [[Ash et al., 2021](#b2)[, Mindermann et al., 2022]](#b35). Importantly, this algorithm still prioritizes selecting a diverse dataset. By conditioning on past data at step i, the objective encourages the algorithm to select data that is different from data that has already been selected.

We can also make an empirical bayes version by adding D prior :

min x1,...,xn⊂Dtrain n i=1 log θ Pr(x i |θ) Pr(θ|D prior , D down , x <i ) (12) + log θ Pr(x i |θ) Pr(θ|D prior , x <i ) (13) This is, of course, still intractable since it requires integrating the parameters. But, since we have already introduced the greedy algorithm that encourages diversity, if we now make the point estimate 5000 10000 15000 20000 Step 4.6 4.8 5.0 5.2 Books Cross Entropy (↓) Transfer from C4 to Books (τ = 2) Online Selection Random Final: Random 1x Final: Random 8x 0 5000 10000 15000 20000 25000

Step approximation, the incentive for data diversity remains. This results in:

$≈ min x1,...,xn⊂Dtrain n i=1 -log Pr(x i |θ prior+down+x<i ) + log Pr(x i |θ prior+x<i )(14)$The thorny issue here is how to define θ prior+down+x<i and θ prior+x<i in practice. In theory, these parameters should be trained on an iid sample from the union of the datasets. If we add the datapoints one at a time, the dynamics of the distribution shift over time can change how well the model corresponds to conditioning on the union of the dataset. But, this would require re-training the models every time we add a new x i which is clearly impractical.

In practice, this encourages using a fine-tuning approach (as in RHO) where we continually fine-tune on the x i as they are added. But when D down is small and the data distribution changes over time, we can get catastrophic forgetting and unstable training dynamics. For these reasons, RHO avoids training the conditional model entirely [(Appendix D of Mindermann et al. [2022]](#)). We also conduct an experiment on the Books task where we use this online fine-tuning algorithm that updates both the marginal and conditional models as we add data to S. Results in Figure [9](#fig_11) show how the training is unstable and in fact performs worse than random.

Moreover, Note that the computational cost of even the cheapest fine-tuning algorithm is substantial compared to the algorithms in the paper. In particular, the serial cost is now 2τ n + 4n (as compared to τ n + 2n for RHO) since we need to pass the full τ n samples through both the conditional and marginal models. So this variant is clearly inferior in practice to the other approaches we consider. tokens (i.e. cost until we reach the dotted line in Figure [1](#fig_2)). We split cost into the scoring cost for filtering the data using the small auxiliary models and then training cost for the large model.

## D Compute cost for scale generalization

In the main text we computed the cost for τ = 16 in terms of model forwards of 1 billion tokens.

Here we can convert this to FLOPs and compute the cost for all values of τ . Results are in Figure [10](#fig_12) showing the breakdown of costs into scoring FLOPs for running the small auxiliary models over the data and training FLOPs for training the large model. We measure the cost it takes to reach the final performance of the random model, i.e. until the CoLoR-filter learning curve crosses the dotted line in Figure [1](#fig_2). The main tradeoff is that lower τ values require more scoring cost and less training cost because they are able to select better data.

We should also note that if multiple models are being trained with the same dataset, then this scoring cost can be amortized over those runs and the larger τ values will look even better.

E Can we do data selection in distribution? RHO provides marginal gains here, while CoLoR-Filter does not provide gains at all. Conditional Only is worse than random. Scaling τ does not change results as much as when we target downstream tasks.

One obvious question raised by these data selection techniques is whether they can work in distribution, i.e. can we select data to make the iid loss on C4 go down faster? In Figure [11](#fig_13) we present results for running this experiment with CoLoR-Filter as well as RHO and Conditional Only. Note that there is no difference between RHO and RHO + prior now (and we drop the "down" from the name) since the prior distribution and the downstream distribution are the same. To implement CoLoR-Filter in this setting, we just take two checkpoints from pre-training the prior model and call the earlier one (at 2.5b tokens) the marginal model and the later one (at 3.1b tokens) the conditional model.

We find that in distribution selection does not work effectively with these methods. There are small gains to RHO loss, but here they are massively outweighed by the computational cost of the selection. CoLoR-Filter sees no gain at all over random and Conditional Only is worse than random. These preliminary results suggest why it is important to recognize that data selection (especially with these methods) will be most effective when we genuinely want to target a different distribution from D train . 

## F Global vs. batchwise selection

One more minor implementation aspect about CoLoR-Filter is that as presented in Algorithm 1, we do global selection where we take the best n data points across the entire train set, while in RHO-down in Algorithm 2 selection is done batchwise. Here we ablate whether the ability to do global selection is actually helpful for CoLoR-Filter. Results in Figure [12](#fig_14) suggest that there is not much difference between the two and at small τ , batchwise selection maybe even beat global selection. We provide this result to illustrate that CoLoR-Filter is fairly robust to how the selection is performed.

## G Finetuning after targeted pre-training

One possible question about the targeted pre-training setting we consider is: what happens if we finetune on D down after the targeted pre-training?

This is interesting since while the pre-trained models presented in the main text never have direct access to D down , the selection algorithm does. In this section, we also allow access to D down after pre-training and then compare the final performance of the finetuned models that are pre-trained on random data vs. selected data.

First, in Table [5](#tab_10) and Table [6](#tab_11) we present finetuning results for the 150m models. We find that CoLoR-Filter data outperforms 8x as much random data after finetuning. Note that the conditional model that we use to guide the selection of CoLoR-Filter is equivalent to a model that has been pre-trained on 3B random tokens and then finetuned on the task. Thus, these results show that we are substantially outperforming the conditional model when both models are finetuned on the downstream data. Next, we present results for the 1.2b models in Table [7](#) and [Table 8](#). We find that the CoLoR-Filter model outperforms or is competitive with training on about 10x as much data randomly selected data. We should also note that the CoLoR-Filter models are now dramatically outperforming the 150m conditional models that were used to filter the data, showing positive scale transfer of data selection. 

## I Inspecting the selected data

In this section, we conduct some basic analysis of the data that is selected by CoLoR-Filter. We leave a full analysis to future work, but here we provide some high level statistics about the distributions of the scores of the conditional vs. marginal models and some representative examples from the datasets.

## I.1 Distribution of scores

First, we simply plot the CDFs of the conditional loss reduction (CoLoR) score function used to select the data. We find that there are relatively few outliers and the CoLoR scores are fairly concentrated and normally distributed. Moreover, we note that the mean CoLoR in both experiments is positive, meaning that the conditional model actually has higher losses on the datapoints in C4 than the marginal model. This makes sense because the conditional model has been finetuned on D down which is out of distribution relative to C4. 

## I.2 Representative examples

Now we just list a few representative examples to give a flavor for the types of outliers that exist under our ranking of sequences and the sorts of typical sequences that are selected versus excluded. The sequences are sampled randomly from different quantiles of the distribution and we shorten all the sequences so that they fit more easily on the page.

Figure [14](#fig_8) shows outliers when targeting Books and Figure [15](#fig_2) shows more typical examples when targeting Books. Generally, we found that the documents with very high scores contain things like old English, poetry, and tables of contents that are particularly unusual in books compared to the rest of the internet. Other things like fiction and dialogue are also highly scored. Negative outliers typically have things like poorly encoded text or advertisements.

Figure [16](#fig_2) shows outliers when targeting downstream tasks and Figure [17](#fig_2) shows more typical examples when targeting downstream tasks. Here the patterns are less clear since the target tasks are more diverse, but we did observe many scientific and wiki-style documents with high scores as well as some descriptions of physical interactions that may be useful for common sense tasks. Again, the negative outliers tend to have things like poorly encoded text or advertisements.

AS now shall ye wyt, what tyme of the day ye shall angle. From the begynning of Maye vntill it be September: the byting tyme is early in the morow from four of the clocke vnto eyght of the clocke, at after none from foure to eyght also, but not so good as in the mornyng, and if it be a colde wynde and a lowryng day, it is muche better than a cleere daye.

Also many poole fysshes will byte best in the morne tyde. And if ye se in any tyme of the day the Troute or greylyng lepe angle to him with a dub according to the same moneth. And where the water ebbeth and floweth: the fish wyll byte in some place at the ebbe and in some place at the flud after they haue restyng (a) Good outlier, CoLoR = -0.35 ??????????????????????????????? ???????????????????????????????? ????????????????????????????????? ?????????????????????????????????? ?????????????????????????????????? ???????????????????????????????? ??????????????????????????????????? ???????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ?????????????????????????????????????

???????????? ????????????????????????? m88 ???????????????????????? ???? m88 ??????????????????????????????????????

??????????????????????????? ???? ?????????????????????????????????? ??????????????????????????????? ? ??????????????????? (b) Bad outlier, CoLoR = 5.45 Figure [14](#fig_8): Examples of outliers when targeting Books. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is written in an older dialect of English which may be related to some documents in the Project Gutenberg corpus, while the negative outlier appears to be poorly encoded.

C: Mrs Mackenzie, was there ever a time when you felt like you could just hop on a plane and make that flight down to the next State to be with your boys? B: Oh my dear, yes. I feel sometimes as if I'm twenty and so fit and active and I can do whatever I want to do and then I remember, good grief, I'm 86, you old fool, you can't do that. I wish I could just fly down there and live with them all together just how it was when they were little and I was their Mum and they followed me because I was so bright and cheery and smart and active and all the things that I'm not now. Oh, I'm so sorry, listen to me

. Maybe I'm just losing my marbles, what do you think, dear? C: Smiling -Imagine if I waved a magic wand and miraculously you were twenty again. What would you see yourself doing Beryl. Is it ok if I call you Beryl? (a) Sequence from best 3%, CoLoR = 0.40 Chamber of Commerce and other business venues, such as the Gwinnett Civic & Convention Centers and is an ideal working environment for commercial businesses and corporations in Northeast Atlanta.

The prominent location is on a heavily wooded, landscaped 6.5 acre site fronting on I-85. The exterior features green-tinted thermal glass and the entrance features a curtain wall glass leading into a granite-floored lobby with vaulted ceilings. Gwinnett County is home to leading Fortune 500 companies, drawn by its reputation as a commerce and technology hub, providing businesses with a regional market of five million people. SERVPRO of Gurnee can simplify the restoration process by handling both the initial water damage mitigation and rebuilding the affected areas. Having one qualified company for the entire process can save time and keep costs low.

(b) Sequence from median 3%, CoLoR = 0.73

Figure [15](#fig_2): Examples of more typical documents when targeting Books. First a document from the top 3% that would be selected with τ = 32, and then a document that scores near the median of all documents. The selected document is fictional dialogue while the median document is an advertisement.

among the pinacoderm are the ostia that allow entry of water into the body of the sponge. These pores have given the sponges their phylum name Porifera-pore-bearers. In some sponges, ostia are formed by porocytes, single tube-shaped cells that act as valves to regulate the flow of water into the spongocoel. In other sponges, ostia are formed by folds in the body wall of the sponge.

Between the outer layer and the feeding chambers of the sponge is a jelly-like substance called the mesohyl, which contains collagenous fibers. Various cell types reside within the mesohyl, including amoebocytes, the "stem cells" of sponges, and sclerocytes, which produce skeletal materials. The gel-like consistency of mesohyl acts like an endoskeleton and maintains the tubular morphology of sponges.

The feeding chambers inside the sponge are lined by choanocytes ("collar cells"). Figure [16](#fig_2): Examples of outliers when targeting downstream tasks. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is a scientific document that could be relevant for tasks like SciQ, while the negative outlier appears to be poorly encoded.

summer plans. After thinking for a while I decided to spend my summer in Squamish, where I would work for the Admissions Team. However, due to a very large number of students interested to work on campus and a limited number of work positions, I ended up not getting a job on campus. I was very upset indeed and I began to think that there were not any job openings elsewhere, which would then result in me travelling back home. Surprisingly, there were many job opportunities in the Squamish community. Since Quest University Canada hosted a job fair on campus I, along with all the students, had the chance to meet local businesses that were looking for summer employees. It was a great opportunity to network and give my resume to the ones that interested me. (a) Sequence from best 3%, CoLoR = 0.33 Can I install PDF Stacks on more than one computer? The license key is valid for only one device and is non-transferable. You can obtain additional license key(s) by placing an order. How do I use PDF Stacks? Click "File" and then "Import Folder"

Once you import the PDF files, your files will be copied into PDF Stacks for easier ability to read, search, organize, take notes, print and share.

Any questions, ask us! How do I create collections (virtual binders) and match/tag my documents for better organization? It's easy. Watch the video for creating collections and tagging documents. Can multiple users access the same documents or can I access and sync my documents through multiple devices?

(b) Sequence from median 3%, CoLoR = 0.55

Figure [17](#fig_2): Examples of more typical documents when targeting downstream tasks. First a document from the top 3% that would be selected with τ = 32, and then a document that scores near the median of all documents. The selected document appears to be a journal entry while the median document is software documentation

![Conference on Neural Information Processing Systems (NeurIPS 2024).]()

![CoLoR-FilterRequire: Prior data D prior , downstream data D down , training data D train , budget n, subset size multiplier τ 1: Pre-train θ marg on D prior 2: fine-tune to get θ cond on D down initialized from θ marg 3: Select a random subset D τ of size τ n from D train 4: Select data: S = bottom-n x∈Dτlog Pr(x|θ cond ) + log Pr(x|θ marg ) 5: return Selected dataset S to train θ on.]()

![RHO-downRequire: Downstream data D down , train data D train , budget n, subset size multiplier τ , batch size b 1: Train θ cond on D down 2: Initialize a random θ marg 1 and S = ∅ 3: for t ∈ [1, . . . , n/b] do 4: Randomly select a batch B t ⊂ D train of size τ b 5: Select data: Bt = bottom-b x∈Btlog Pr(x|θ cond ) + log Pr(x|θ marg t ) t+1 by training on Bt 8: end for 9: return Selected dataset S to train θ on.]()

![Figure 2: Scaling of final performance with τ when targeting Books with 150m parameter models.]()

![Figure3: Scaling CoLoR-Filter with τ when training 1.2b models with data selected by 150m models. Curves end when we exhaust the data in C4.]()

![5 and measuring n in billions of tokens, the total cost for training the CoLoR-Filter model is 3m + 2τ n + 3nL = 3 * 3.1 + 2 * 16 * 1.5 + 3 * 1.5 * 5.5 = 82 while the cost for training on 25 billion random tokens is 3N L = 3 * 25 * 5.5 = 412.5, illustrating a more than 5x total compute savings to achieve the same performance on Books. A full plot visualizing the cost in FLOPs for all τ is in Appendix D.]()

![Figure 4: Final performance versus τ on the suite of downstream tasks for 150m models. CoLoR-Filter scales the best with τ .]()

![Figure 7: Sweeping over τ when targeting Books from C4 for 150m models.]()

![Figure 9: (Left) Performance of online selection with fine-tuning as outlined in Equation (14). Online selection is worse than random. (Right) Training curves for the conditional and marginal models on the selected data S. The conditional model faces training instability early on (associated with forgetting), and then eventually becomes better than the marginal on the selected data.]()

![Figure10: Costs in FLOPs to reach equivalent performance to the final random model trained on 25b tokens (i.e. cost until we reach the dotted line in Figure1). We split cost into the scoring cost for filtering the data using the small auxiliary models and then training cost for the large model.]()

![Figure11: Using a sample of C4 as D down . RHO provides marginal gains here, while CoLoR-Filter does not provide gains at all. Conditional Only is worse than random. Scaling τ does not change results as much as when we target downstream tasks.]()

![Figure 12: Comparison between global and batchwise variants of CoLoR-Filter on Books. The two perform nearly identically here.]()

![Figure 13: CDFs for the conditional loss reduction (CoLoR), i.e.log Pr(x|θ prior+down ) -(-log Pr(x|θ prior )). The dashed line highlights the cutoff point for τ = 64. We select the points with the lowest CoLoR.]()

![(a) Good outlier, CoLoR = -0.46 *** **********. ****** *** ***, *** ******* **** **** ** ******** ******* plates ** ****** ** ** **-** *** (*** ******* ** tested), ***** ******* ********. *** ** *** ***, *** ********* *.* ********* ******* ***** capture ****** ******** ******** ****** ** **** ** **** ******, >10 ***, *** ******, **+ ***, **** ** ****** ***** or ****, *** ** ***** ****** *** *** **** **** field ** ****, ***** **'. ***** ******* ********, *** ******** ** ***** ****** ****** ****** to ******* ****** ** ****** **** ** **** ****** ** night, ****** ******* ******* *** ********. ******** ******** ** */****, *** ******** ****** ******** ******** **** front *** **** ****** ****** ** *** *** **** ******. However, **** ******* ******* *** ********** ** *** ***** ** night, ****** ** **** ****** *** ******* ************ ** *** scene. (b) Bad outlier, CoLoR = 5.36]()

![Compute cost of the various algorithms measured in "model forwards". The total cost of selection and training on the selected data is the sum of all costs across a row. The variables are m = |D prior |, n = |S|, τ is a hyperparameter that controls how aggressively we subselect, and L is a multiplier of the cost of model forwards between the selection model(s) and the target model (approximately the ratio of parameter counts between the models).For example, in our experiments, when we use 150 million parameter models to select data and then train a 1.2 billion parameter model on the resulting data, then L ≈ 5.5 2 . Training thus costs 3nL across all methods since we run a forward and backward for the large model on all n sequences.]()

![Task generalization for the 1.2b models with τ = 64.]()

![Final performance for all tasks for 1.2b models. Note that the CoLoR-Filter models do not train on as many tokens since we exhaust all of the tokens in C4 with these settings of τ .]()

![Performance after finetuning on Books for different pre-trained 150m models. Note that the Random (3.1b tokens) model is equivalent to the conditional model used to select data with CoLoR-Filter (τ = 16).]()

![Held out performance after finetuning on downstream data for different pre-trained 150m models. Note that the Random (3.1b tokens) model is equivalent to the conditional model used to select data with CoLoR-Filter (τ = 16).]()

![Training parameters, based onWortsman et al. [2024],Groeneveld et al. [2024]]()

Prior work[Mindermann et al.,  

2022,[Evans et al., 2023]](#b15) has referred to the models that estimate these two terms as the "reference" and "learner" or "actor", respectively. We opt for the names conditional and marginal for clarity in connections to the Bayesian viewpoint.

Even though there are 8x as many parameters in the large model, the FLOP multiplier is less since the attention computations take the same number of FLOPs regardless of parameters.

