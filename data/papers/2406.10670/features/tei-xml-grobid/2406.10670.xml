<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training</title>
				<funder ref="#_Tw8tRwc">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
				<funder ref="#_kyCgqDC">
					<orgName type="full">Eric</orgName>
				</funder>
				<funder ref="#_j29EJnz">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_uyWmWFs">
					<orgName type="full">Chan Zuckerberg Initiative Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanlin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Schwarz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">University of Oxford</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1297926AA9480EE64DC6FB5825842B31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models. In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The content of the data that a language model is trained on can have profound effects on its performance and the efficiency of the training process <ref type="bibr" target="#b42">[Rae et al., 2021</ref><ref type="bibr" target="#b31">, Longpre et al., 2023</ref><ref type="bibr" target="#b39">, Penedo et al., 2023</ref><ref type="bibr" target="#b49">, Soboleva et al., 2023</ref><ref type="bibr">, Li et al., 2024]</ref>. But it remains an open research question how to decide which data to include in the training set. In this paper, we analyze a family of loss-based approaches for targeted selection of pre-training data, propose a simple approach that outperforms existing methods, and provide some preliminary evidence of favorable scaling properties.</p><p>To formulate the data selection problem, we first need to specify an objective that quantifies whether the selected data is good. Defining this objective requires evaluating a pre-trained language model, which is an area of active research <ref type="bibr">[Gao et al., 2023</ref><ref type="bibr">, Magnusson et al., 2023</ref><ref type="bibr" target="#b14">, Engstrom et al., 2024</ref><ref type="bibr" target="#b8">, Chang et al., 2024]</ref>. For this paper, we will take the goal to be to maximize performance on a set of downstream tasks. Since the preferred metrics on a given set of tasks are not necessarily the same nor amenable to direct optimization, we consider the likelihood of sequences sampled from the downstream tasks as a proxy objective. With this objective, we now have a straightforward goal: given a very large corpus of sequences and a small amount of high-quality data from a set of downstream tasks, we want to select a subset from the corpus so that training on the selected data  CoLoR-Filter (τ = 64) Random</p><p>Figure <ref type="figure" target="#fig_2">1</ref>: Learning curves for 1.2 billion parameter language models trained on data selected by CoLoR-Filter using smaller 150 million parameter auxiliary models for two different target distributions. (Left) We target and evaluate loss on Books, lower is better. (Right) We target and evaluate accuracy on a suite of 8 downstream tasks from <ref type="bibr" target="#b20">[Groeneveld et al., 2024]</ref>, higher is better.</p><p>In both cases, test data is held out from the data used by CoLoR-Filter to guide selection. τ is the subset size multiplier denoting the number of examples considered for each selected data point. The CoLoR-Filter line terminates when we run out of data in C4 (≈175b possible tokens).</p><p>maximizes likelihood on the downstream tasks. Then we can also test performance on the tasks under their preferred metrics.</p><p>From this objective, we derive an algorithm dubbed CoLoR-Filter (Conditional Loss Reduction Filtering). In Section 2 we derive this method by applying Bayes' rule and approximate empirical Bayes to the downstream likelihood objective. The resulting method is simple and intuitive: each sequence is scored by the difference in likelihood between a "prior" model and a "conditional" model that results from fine-tuning the prior model on the downstream data. Sequences that are more likely under the fine-tuned model are good. We also compare this algorithm to prior work (e.g., <ref type="bibr" target="#b35">[Mindermann et al., 2022]</ref>) and discuss computational costs.</p><p>To evaluate our method, we consider two tasks. First, in Section 5, we consider a semi-synthetic task where the downstream task is language modeling on Books. Given access to C4 <ref type="bibr" target="#b43">[Raffel et al., 2020]</ref> as potential pre-training data and a small (25 million tokens) sample of data from Books, we use CoLoR-Filter and a variety of baselines to select 3 billion tokens. We find that data selected by CoLoR-Filter can substantially outperform models trained on 8x as much randomly chosen data. Second, in Section 6, we consider a suite of 8 downstream multiple-choice tasks from <ref type="bibr" target="#b20">Groeneveld et al. [2024]</ref>. As downstream data we take the training sets of the tasks, but we evaluate accuracy on the held-out test sets. We again find that selecting with CoLoR-Filter outperforms training on 8x as much randomly selected data. Moreover, in both tasks, performance scales smoothly with the hyperparameter τ that governs how aggressively we select the data, suggesting that further scaling would yield further improvements.</p><p>In addition to finding that CoLoR-Filter can select good subsets of data, we also consider the computational cost of the selection procedure itself. CoLoR-Filter only requires running inference of the two auxiliary models to select data. This is computationally beneficial compared to online methods like RHOLoss <ref type="bibr" target="#b35">[Mindermann et al., 2022]</ref> since inference is cheaper than training and is entirely parallelizable. To maximize the computational benefits we also show that data selected with a small (150 million parameter) model can be transferred to a larger (1.2 billion parameter) model. Results are shown in Figure <ref type="figure" target="#fig_2">1</ref>, showing substantial efficiency improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setting and Derivations</head><p>Assume that we are given a large pre-training dataset D train , a small downstream dataset D down from the downstream task(s) of interest, and a "prior" dataset D prior we can use as prior knowledge (in practice we often just sample from D train ). We will assume for all practical purposes that D train is infinite and training proceeds in the "online" or "single pass" setting where we do not repeat data points. Our goal is to choose a subset S ⊂ D train of a fixed size |S| = n that minimizes the downstream loss (maximizes the downstream likelihood).</p><p>This section introduces our CoLoR-Filter algorithm, inspired by and building upon the RHOLoss approach from prior work <ref type="bibr" target="#b35">[Mindermann et al., 2022</ref><ref type="bibr" target="#b15">, Evans et al., 2023]</ref>. We also discuss related algorithms applicable to this setting such as DSIR <ref type="bibr" target="#b62">[Xie et al., 2023]</ref> and DSDM <ref type="bibr" target="#b14">[Engstrom et al., 2024]</ref>. Additional related work is discussed further in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian Data Selection</head><p>Our objective can be formulated as a Bayesian optimization problem, where the goal is to select a set S so as to maximize the posterior probability of D down , i.e.</p><p>min S⊂Dtrain,|S|=n log Pr(D down |S), (1) where Pr(D down |S) is the posterior probability. Applying Bayes rule we get: min S⊂Dtrain,|S|=n log Pr(S|D down ) + log Pr(S)log Pr(D down ) (2) Note that the last term does not depend on S, so it can be ignored when optimizing over S. Introducing a prior over model parameters θ, we get: min S⊂Dtrain,|S|=n log θ Pr(S|θ) Pr(θ|D down ) "conditional" + log θ Pr(S|θ) Pr(θ) "marginal"</p><p>(3)</p><p>We will refer to the two terms as the conditional and marginal terms, respectively.<ref type="foot" target="#foot_0">foot_0</ref> Note that the conditional and marginal terms together make up the negative pointwise mutual information between the selected and downstream data, which has deep connections to prior work on active learning and active sampling <ref type="bibr" target="#b30">[Lindley, 1956</ref><ref type="bibr" target="#b36">, Moore and Lewis, 2010</ref><ref type="bibr" target="#b23">, Houlsby et al., 2011</ref><ref type="bibr" target="#b4">, Bickford Smith et al., 2023</ref><ref type="bibr" target="#b26">, Kirsch, 2023</ref><ref type="bibr" target="#b44">, Rainforth et al., 2024]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CoLoR-Filter</head><p>Given that we have access to prior knowledge from the dataset D prior , we can replace the uninformed prior over θ with an empirical Bayes prior that conditions on D prior to obtain:</p><p>min S⊂Dtrain,|S|=n log θ Pr(S|θ) Pr(θ|D down , D prior ) + log θ Pr(S|θ) Pr(θ|D prior )</p><p>As this integration is still intractable, we now make our main simplifying assumption which is to replace this integration over parameters by a point estimate: ≈ min</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S⊂Dtrain,|S|=n</head><p>log Pr(S|θ prior+down ) + log Pr(S|θ prior ),</p><p>where θ prior is a model trained on D prior and θ prior+down is a model trained on both D prior and D down (in practice, we use a model that is pre-trained on D prior fine-tuned on D down ).</p><p>Moreover, this approximation leads to computational benefits by avoiding the full combinatorial optimization of subset selection. In particular, once we condition on a single model θ, and assuming the distribution over points x ∈ S is independent, i.</p><p>e. Pr(S|θ) = x∈S Pr(x|θ), we have: min {x1,...,xn}⊂Dtrain log n i=1 Pr(x i |θ prior+down ) + log n i=1 Pr(x i |θ prior ) (6) which simplifies to: min {x1,...,xn}⊂Dtrain n i=1 log Pr(x i |θ prior+down ) -(-log Pr(x i |θ prior ))</p><p>This gives our CoLoR-Filter criteria that we use to select data. This optimization selects the points with the largest conditional loss reduction (CoLoR), i.e. the points where the negative log-likelihood loss of the conditional model θ prior+down is lower than the marginal model θ prior . Intuitively, this selects data points that are more likely under the conditional model than the marginal model.</p><p>A note on data diversity. While the factorization that results from our point estimate of the parameters is computationally convenient, it makes an important simplifying assumption. In particular, the CoLoR-Filter objective no longer encourages the selection of a diverse dataset, as scores are applied independently to each point. In practice, this is remedied by a few considerations: (1) we can run CoLoR-Filter on a corpus that has already been deduplicated to prevent degenerate duplications, (2) for large n, we must select many different data points, and (3) each datapoint is itself a sequence that may contain diverse signal across tokens. We should also note this is not a unique property of CoLoR-Filter and also happens in other methods that do offline scoring like DSDM and DSIR. We defer a detailed discussion of the nuances of this issue to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Related Algorithms</head><p>Connection to importance sampling. Since the CoLoR-Filter objective is written as a difference of logs, it can also be written as a log of the ratio between probabilities under θ prior+down and θ prior .</p><p>If data were actually sampled from θ prior , then this ratio would be the importance weight needed to reweight samples so that they are from the model defined by θ prior+down . Note that DSIR <ref type="bibr" target="#b62">[Xie et al., 2023]</ref> directly attempts to perform importance sampling from D train to D down instead of optimizing performance on the downstream data. Thus, DSIR ends up with a somewhat related algorithm except in DSIR: (1) there is no language model, just features of a full data point (hashed n-grams), and (2) the algorithm samples rather than optimizes.</p><p>Connections to DSDM. Another closely related approach is DSDM <ref type="bibr" target="#b14">[Engstrom et al., 2024]</ref> which uses a TRAK Datamodel estimator <ref type="bibr" target="#b24">[Ilyas et al., 2022</ref><ref type="bibr" target="#b38">, Park et al., 2023]</ref> to score datapoints and then selects the top-n points. The motivation and setting of DSDM are similar to CoLoR-Filter, but DSDM relies on TRAK which constructs a linear approximation of the influence that data points have on each other. Instead, CoLoR-Filter operates directly in function space by comparing the loss between models directly rather than relying on linear approximations or Datamodels <ref type="bibr" target="#b24">[Ilyas et al., 2022]</ref>.</p><p>Connections to RHO-down. CoLoR-Filter is inspired by and builds on the RHOLoss approach introduced in prior work <ref type="bibr" target="#b35">[Mindermann et al., 2022]</ref> with subtle but significant differences in the setting: the original RHO paper focuses on cases where the hold-out data is sampled from the same distribution as D train over multiple epochs of training. In contrast, we focus on selecting data to target downstream distributions that are different from D train and where we only take a single pass over the data. Here, we derive a straightforward adaptation of RHOLoss to our setting, which we call RHO-down.</p><p>We now derive RHO-down in our setting, aiming to illustrate the connections between RHO-down and CoLoR-Filter. First, RHO-down approximates the full subset selection problem from Equation (3) by a greedy (sequential) approximation where samples are added to S one (batch) at a time. Using a batch size of 1, the ith-sample would be ideally added according to the following criterion:</p><formula xml:id="formula_3">≈ min xi∈Dtrain -log θ Pr(x i |θ) Pr(θ|D down , x &lt;i ) + log θ Pr(x i |θ) Pr(θ|x &lt;i ),<label>(8)</label></formula><p>where i ranges from 1 to n sequentially. RHO-down then uses a point estimate of the parameters (as we do in CoLoR-Filter):</p><formula xml:id="formula_4">≈ min xi∈Dtrain -log Pr(x i |θ down+x&lt;i ) + log Pr(x i |θ x&lt;i )<label>(9)</label></formula><p>Finally, the RHO-down authors found that updating the conditional term to depend on x &lt;i was unstable, so they instead approximate this by a fixed model θ down :</p><formula xml:id="formula_5">≈ min xi∈Dtrain -log Pr(x i |θ down ) + log Pr(x i |θ x&lt;i ).<label>(10)</label></formula><p>Note that while both CoLoR-Filter and RHO-down approximate the posterior over parameters with a point estimate, RHO-down makes a few additional approximations. This is largely a result of RHO-down attempting to increase data diversity by using a sequential approach to selection that conditions on the previously selected data x &lt;i . This is an understandable goal, but it introduces more approximations, can cause instability by creating a non-stationary data distribution, and is computationally expensive since the data selection is no longer parallelizable. A continued discussion of the pros and cons of online selection is in Appendix C.</p><p>RHO-down + prior. We also consider a version of the algorithm that we call "RHO-down + prior" that replaces D down , θ down in the RHO-down algorithm with D prior ∪ D down , θ prior+down to incorporate the prior information. This corresponds to conditioning on both D prior and D down instead of only D down . Intuitively, this method can better leverage stronger features learned on the larger D prior to integrate the information from the small D down .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Further Related Work</head><p>We now discuss some related work, more broadly, with regards to active learning and data curation.</p><p>Active &amp; Curriculum learning. Our formulation of data selection has connections to classic and deep active learning <ref type="bibr" target="#b23">[Houlsby et al., 2011</ref><ref type="bibr" target="#b4">, Bickford Smith et al., 2023</ref><ref type="bibr" target="#b26">, Kirsch, 2023]</ref>, which are deeply rooted in optimal Bayesian experimental design <ref type="bibr" target="#b30">[Lindley, 1956</ref><ref type="bibr" target="#b44">, Rainforth et al., 2024]</ref>, whose goal is to select a set of experiments to optimize certain information criteria <ref type="bibr" target="#b41">[Pukelsheim, 2006]</ref> such as maximally reducing the uncertainty about model parameters. Various acquisition functions are proposed in deep learning regimes <ref type="bibr" target="#b48">[Sener and Savarese, 2018</ref><ref type="bibr" target="#b3">, Ash et al., 2019</ref><ref type="bibr">, 2021]</ref> and most of them focus on label-efficient image classification. Another line of recent techniques share deep methodological connections but emphasize the sub-selection of available data during training (rather than the collection of additional examples typically considered in active learning) and could thus be classified as curriculum learning [e.g. Graves et al., 2017]. Among them, RHOLoss [Mindermann <ref type="bibr">et al., 2022]</ref> seeks to select data based on the hold-out reference dataset from the same distribution as the training data. It has been later implemented in continual pre-training <ref type="bibr" target="#b29">[Lin et al., 2024]</ref> and vision domains <ref type="bibr" target="#b15">[Evans et al., 2023</ref><ref type="bibr" target="#b53">, Tack et al., 2024]</ref>.</p><p>Data curation practices in pre-training. Though large-scale public web-crawled data are common data sources for pre-training models, low-quality, toxic, and uninformative content that can prevent successful pre-training is prevalent <ref type="bibr" target="#b60">[Wenzek et al., 2020</ref><ref type="bibr">, Elazar et al., 2023</ref><ref type="bibr" target="#b52">, Sorscher et al., 2022</ref><ref type="bibr" target="#b1">, Allen-Zhu and Li, 2024]</ref>. Therefore, practitioners design sophisticated data pre-processing pipelines such as filtering <ref type="bibr" target="#b6">[Brown et al., 2020]</ref>, deduplication <ref type="bibr" target="#b27">[Lee et al., 2022]</ref>, and mixing <ref type="bibr">[Touvron et al., 2023a,b]</ref> to improve the data quality. Due to the immense scale, state-of-the-art pre-training datasets usually depend on simple heuristic filters <ref type="bibr" target="#b43">[Raffel et al., 2020</ref><ref type="bibr" target="#b42">, Rae et al., 2021</ref><ref type="bibr">, Computer, 2023</ref>] (e.g., URL, length, n-gram perplexity, fastest classifiers) that can be parallelized across CPU nodes.</p><p>Besides the above rule-based filtering, model-based filtering concerns using machine learning models to score and filter data, which has been proven to be effective in vision and vision-text domains <ref type="bibr" target="#b47">[Schuhmann et al., 2022</ref><ref type="bibr" target="#b0">, Abbas et al., 2023</ref><ref type="bibr" target="#b16">, Fang et al., 2023]</ref>. Such approaches usually leverage a given trustworthy data source like Wikipedia or Books as the reference and contrast the raw data with it. Due to computational cost, models are often designed to be small such as n-gram <ref type="bibr" target="#b62">[Xie et al., 2023]</ref>, single-layer neural networks <ref type="bibr" target="#b25">[Joulin et al., 2017</ref><ref type="bibr" target="#b6">, Brown et al., 2020]</ref>, k-means clustering <ref type="bibr" target="#b55">[Tirumala et al., 2024]</ref>. There is also a growing line of work illustrating that data quality is important in shaping model training from a variety of perspectives, such as increasing data scale <ref type="bibr" target="#b22">[Hoffmann et al., 2022</ref><ref type="bibr">, Meta, 2024]</ref> and using synthetic data <ref type="bibr" target="#b21">[Gunasekar et al., 2023]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">From Derivations to Practical Algorithms</head><p>In our experiments, we will consider four algorithms based on the above derivations. In this section we go through each of these in turn.</p><p>CoLoR-Filter. Our proposed algorithm is presented formally in Algorithm 1. Compared to the derivation, the main difference is the introduction of τ , a hyperparameter that acts as a computeperformance trade-off controlling how expensive and aggressive the data selection is. Rather than selecting data from all of D train , we take a random subset D τ of size τ n. Thus, larger τ subselect more aggressively, but at the cost of more computation. A full discussion of this cost is in Section 4.2.</p><p>Conditional only. As an ablation of CoLoR-Filter, we follow prior work <ref type="bibr" target="#b15">[Evans et al., 2023]</ref> and include a baseline that only uses the conditional model to select data. Essentially, this is CoLoR-Filter if we always assume that log Pr(x|θ marg ) = 0 in Line 4 of Algorithm 1. RHO-down. We present a practical variant of RHO-down in Algorithm 2 based on the derivation presented in Section 2. The main changes to make a practical algorithm are (1) the introduction of τ as in CoLoR-Filter, and (2) performing the algorithm batch-wise instead of using single data points.</p><p>RHO-down + Prior. We can also incorporate the prior data D prior into Algorithm 2 by simply replacing Line 1 where θ cond is trained on D down with a procedure where we first pre-train θ cond on D prior and then fine-tune it on D down .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computational Cost</head><p>To evaluate the computational cost of the various algorithms, we use units of "model forwards" per token where we assume that a backward pass is twice as expensive as a forward pass <ref type="bibr" target="#b17">[Fleuret, 2023]</ref>. Note that our 150m models take about 5e8 FLOPs per model forward of a single token <ref type="bibr" target="#b22">[Hoffmann et al., 2022</ref><ref type="bibr" target="#b7">, Casson, 2023]</ref>. The cost of running the selection algorithms depends on m, n, τ and L defined as follows: m is the size of the prior data D prior , n is the size of the selected dataset S, τ is the hyperparameter controlling how aggressively we subselect data. Note that we assume that |D down | is so small that the cost of training a model on D down is negligible towards the total cost (and all the methods we consider just fine-tune a model once on D down ). We will also be careful to note when computation can be done in parallel before training versus computation that must happen serially during a training run. Offline algorithms like CoLoR-Filter can take advantage of parallelism to improve efficiency. In this section, we go through each method in turn and aggregate the computational costs in table 1.</p><p>Scale transfer. We also include another parameter L to cover the case where we select data using small models and use it to train a larger model <ref type="bibr" target="#b15">[Evans et al., 2023]</ref>. Specifically, L is the ratio of cost of one model forward of the large target model compared to the small auxiliary models used for data RHO-down. The cost of selection is still 2τ n forward passes. Then we need an additional 2n to backward the output model (since the forward is already handled during scoring). Note that we need to evaluate the marginal model online, so it is not parallelizable, but the conditional model is fixed and can be computed offline. So, the cost is 2τ n + 2n + 3nL, and the τ n conditional model computation can be done in parallel.</p><p>RHO-down + Prior. For the version with an added prior, we just add 3m cost for training the prior. Thus, the cost is 2τ n + 2n + 3nL with τ n parallelizable.</p><p>Overall, the methods all have comparable costs, with Conditional Only being the cheapest and RHO-down + Prior the most expensive. The main difference is that CoLoR-Filter and Conditional Only are easily parallelized while RHO-down and RHO-down + Prior are not. It should also be noted that when doing experimentation, offline methods like CoLoR-Filter also benefit from being able to re-use likelihoods multiple times, while RHO-based methods need to recompute the serial cost any time that some hyperparameter of the algorithm.</p><p>5 Domain Transfer: a Simple Testbed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Training. We train language models with 150 million non-embedding parameters using the OLMo codebase <ref type="bibr" target="#b20">[Groeneveld et al., 2024]</ref> and following hyper-parameter choices from <ref type="bibr" target="#b61">[Wortsman et al., 2024]</ref>. Unless otherwise noted, we use 150m models as the auxiliary models (θ cond , θ marg ) as well as the target model θ. Full hyperparameters are described in detail in Appendix H.</p><p>We take D down to be a small dataset of 25 million tokens sampled from the Project Gutenberg Books data subset of Dolma <ref type="bibr" target="#b51">[Soldaini et al., 2024]</ref>, D prior to be a dataset of 3.1 billion tokens from C4 <ref type="bibr" target="#b43">[Raffel et al., 2020]</ref>, and D train to be all of C4. We select a dataset S of 3.1 billion tokens (which is approximately the "chinchilla optimal" amount for models of this size). To get θ prior+down or θ down , we fine-tune or train for one epoch on D down .</p><p>Evaluation. To evaluate the efficacy of our data selection, we report cross-entropy loss of next token prediction on a held-out dataset D down from the same distribution as D down (Books).</p><p>Baselines. The simplest baseline we consider is Random sampling, which has been shown to be a strong baseline for C4 pre-training <ref type="bibr" target="#b14">[Engstrom et al., 2024]</ref>. We consider all four algorithms described in Section 4: CoLoR-Filter, Conditional Only, RHO-down, and RHO-down + prior. And as one extra baseline, we also include DSIR <ref type="bibr" target="#b62">[Xie et al., 2023]</ref> which estimates n-gram importance weights between D train and D down , and similarly has a parameter like τ that controls how aggressively to subselect.</p><p>Note that while it is in a similar setting to ours, we do not include DSDM <ref type="bibr" target="#b14">[Engstrom et al., 2024]</ref> as a baseline since there is no publicly available code and based on the appendix of that paper, it it much more computationally expensive than the methods we consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>2 1 2 2 2 3 2 4 τ 4.2 4.4 4.6 4.8 Final Books Cross Entropy (↓) CoLoR-Filter Conditional only RHO-down RHO-down + prior DSIR Random 1x Random 8x We first run the domain transfer experiments on 150m models, sweeping across τ that controls the selected subset size. In Figure <ref type="figure" target="#fig_3">2</ref> we plot how the final performance scales with τ across methods. We see that CoLoR-Filter has the best scaling performance with increased τ , with no sign of saturation for τ = 16. We hypothesize that by using strong models to select the data, CoLoR-Filter is able to more effectively scale to larger τ than the other methods. In Figure <ref type="figure">7</ref> in Appendix A, we plot the learning curves (evaluated on the held-out validation set) for the four methods introduced in Section 4. There, we see especially clean scaling for CoLoR-Filter across the entire learning curve, substantially outperforming random selection with much less data, similar to Figure <ref type="figure" target="#fig_2">1</ref>. Scale generalization. Finally, we also conduct an experiment in scale generalization (partially shown in Figure <ref type="figure" target="#fig_2">1</ref>) using the data selected by our 150m auxiliary models to train a 1.2b target model. In Figure <ref type="figure" target="#fig_5">3</ref> we show learning curves for a sweep over τ . We still see consistent gains as we scale τ for a fixed number of training tokens. Interestingly, if we fix the total number of tokens we are selecting from (i.e. where the lines end when we run out of C4), then the final performance with τ = 32 is better than all other values of τ . This shows how a strict subset of tokens can outperform a superset (e.g. τ = 16).</p><p>We should also point out here the computational savings when using CoLoR-Filter. As an example, consider τ = 16 where we match the performance of 25 billion randomly selected tokens with about 1.5 billion filtered tokens. Considering the computational costs discussed above with L = 5.  Test task CoLoR-Filter Conditional Only RHO-down RHO-down + prior DSIR Random 8x Method 5.5 4.1 2.9 7.4 5.2 5.9 -4.6 -3.9 2.8 -0.1 1.1 0.6 -2.2 0.4 -2.3 2.5 -2.1 -0.3 2.4 2.8 2.9 2.5 2.4 0.6 -10.2 -4.6 -0.2 2.4 2.1 2.9 4.9 2.6 2.1 2.7 -2.4 2.2 4.4 4.3 2.0 2.2 1.0 1.5 1.1 -0.7 2.0 5.1 3.3 1.2 -0.2 2.0 -1.6 -0.7 -2.8 0.8 Improvement over Random 1x (τ = 16) (↑) -4 -2 0 2 4 Figure 5: Performance improvement over training on an equivalent amount of random data broken down by task (except for Random 8x, which uses 8x more data). A table of results is in Appendix B.</p><p>6 Downstream Tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Training. We target the 8 tasks from the OLMo paper <ref type="bibr" target="#b20">[Groeneveld et al., 2024]</ref>: Hellaswag <ref type="bibr" target="#b63">[Zellers et al., 2019]</ref>, PIQA <ref type="bibr" target="#b5">[Bisk et al., 2020]</ref>, ARC-challenge and ARC-easy <ref type="bibr" target="#b10">[Clark et al., 2018]</ref>, Openbook QA <ref type="bibr" target="#b34">[Mihaylov et al., 2018]</ref>, SciQ <ref type="bibr" target="#b59">[Welbl et al., 2017]</ref>, BoolQ <ref type="bibr" target="#b9">[Clark et al., 2019]</ref>, and Winogrande <ref type="bibr" target="#b45">[Sakaguchi et al., 2021]</ref>. Each of these datasets has a separate train split. We use these train splits to construct D down as follows: for each question we concatenate the question and the correct answer formatted as a grammatical continuation. Overall, this results in a small D down dataset of 7.4 million tokens. D prior and D train are the same as before. And we again get θ prior+down by fine-tuning θ prior for one epoch on D down . Evaluation. We evaluate on held-out data from each downstream task test or validation sets (using val if test is not publicly available).</p><p>We use the evaluation procedure from OLMo <ref type="bibr" target="#b20">[Groeneveld et al., 2024]</ref> which follows <ref type="bibr">[Gao et al., 2023]</ref> for evaluating these multiple-choice tasks using the rank classification approach of <ref type="bibr" target="#b6">Brown et al. [2020]</ref>. We report aggregat perfromance across tasks as well as the task-specific performance.</p><p>Baselines. Same as in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>While the curves themselves are noisier now due to the noisier nature of accuracy evaluation on small datasets compared to cross entropy on a large one, the same trends hold as we saw for domain transfer to Books. CoLoR-Filter in particular is scaling the best as we increase τ . Other methods do not illustrate the same clean scaling as we increase τ , which is nearly linear on a log scale for CoLoR-Filter, as seen in Figure <ref type="figure" target="#fig_8">4</ref>. Full learning curves are in Appendix A.</p><p>We can also look at the performance broken down by task and illustrated relative to training on an equivalent amount (3.1 billion tokens) of randomly selected data for τ = 16 illustrated in Figure <ref type="figure">5</ref>.</p><p>We see especially large gains on Hellaswag, ARC easy, Openbook QA and SciQ and actually see performance decreases on BoolQ and Winogrande. However, we should note that at this scale and with all data selected from C4, we actually found BoolQ and Winogrande to be quite noisy and not even correlated with training on 8x as much random data, so it is not clear how much weight to place on those results. Across the other tasks, the gains of CoLoR-Filter over the baselines are clear. It is an interesting direction for future work to probe more deeply into how task-dependent the gains from targeted data selection can be. Figure <ref type="figure">6</ref>: Scaling CoLoR-Filter with τ when training 1.2b models with data selected using smaller 150m models. Curves end when we exhaust the data in C4.</p><p>Scale generalization. We also consider scale generalization to a 1.2b target model and illustrate the full results of a sweep over τ in Figure <ref type="figure">6</ref>. Again we find significant benefits of CoLoR-Filter across scales. A full table of per-task results is in Appendix B. Again we notice that training on a strict subset of data can outperform a larger dataset.</p><p>We can again do out the calculation of computational savings for τ = 16. It now takes about 3 billion tokens for CoLoR-Filter to match the performance of training on 25 billion random tokens. This amounts to a total cost of 3m+2τ n+3nL = 3 * 3.1+2 * 16 * 3+3 * 3 * 5.5 = 154.8, which is still an upwards of 2.5x reduction in compute to achieve the same average performance across the suite of tasks. A full plot visualizing the cost in FLOPs for all τ is in Appendix D.</p><p>Task generalization. We can also test task generalization beyond the 8 tasks that were used to select the data on a few more tasks that test common sense reasoning <ref type="bibr" target="#b58">[Wang et al., 2019</ref><ref type="bibr" target="#b50">, Socher et al., 2013</ref><ref type="bibr" target="#b54">, Talmor et al., 2018</ref><ref type="bibr" target="#b46">, Sap et al., 2019]</ref>. Results are presented in Table <ref type="table" target="#tab_7">2</ref> compared to a random model trained on 10x as much data. The performance indicates that the data selected by CoLoR-Filter are not overfit to the particular evaluation tasks, but captures some general notion of good data for a range of tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>While fairly simple to derive and implement, we show that CoLoR-Filter is an effective method for data selection on C4, with promising scaling behavior up to 1.2 billion models. In our experiments, CoLoR-Filter continues to improve when only using 1 out of 64 data points considered for selection and generalizes from small auxiliary models to larger target models. This opens many potential lines of research. First, while we have considered targeted pre-training, it is possible that CoLoR-Filter could be extended to fine-tuning, continual pre-training, and more general open-domain pre-training.</p><p>In particular, it is an interesting open question whether the lack of an explicit consideration of data diversity hinders CoLoR-Filter in any of these settings. Second, CoLoR-Filter could be applied to more challenging domains in language like code generation or even applied beyond the language domain to other modalities. Finally, there is plenty of work to be done to make the algorithm more efficient and to test the limits of scale generalization.</p><p>A Learning curves for 150m models </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Tables of downstream results</head><p>Table <ref type="table">3</ref>: Performance for all tasks for 150m models for data selection with τ = 16. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Data diversity and online vs. offline selection</head><p>Much work on active learning focuses on ensuring that we select a diverse set of data points that cover the test distribution of interest. As explained in the main text, by making a point estimate of the parameters, CoLoR-Filter is simplifying the problem and sacrificing an explicit term for diversity in the objective. In practice, this seems to be saved by the facts that (1) C4 has already been deduplicated, (2) we still select a fairly large subset without replacement, and (3) an individual sequence contains diversity across tokens.</p><p>However, the fact that CoLoR-Filter sacrifices a notion of diversity in the objective is important to consider more deeply. Here, we derive what a loss-based algorithm for data selection that prioritizes diversity would look like and why it is computationally infeasible. Then we derive an approximation (that looks somewhat like RHOLoss <ref type="bibr" target="#b35">[Mindermann et al., 2022]</ref>) and show how it is empirically unstable, as was also observed previously by <ref type="bibr" target="#b35">[Mindermann et al., 2022]</ref>.</p><p>To derive a CoLoR-Filter-like algorithm that values diversity, we can start from Equation (3) by a greedy approximation where samples are added to S one (batch) at a time, like in RHO:</p><formula xml:id="formula_6">≈ min x1,...,xn⊂Dtrain n i=1 -log θ Pr(x i |θ) Pr(θ|D down , x &lt;i ) + log θ Pr(x i |θ) Pr(θ|x &lt;i )<label>(11)</label></formula><p>Note that this sort of greedy algorithm for subset selection has a long history in active learning <ref type="bibr" target="#b12">[Das and Kempe, 2018]</ref>, is actually theoretically sound in some cases <ref type="bibr" target="#b37">[Nemhauser et al., 1978]</ref>, and is used in prior work <ref type="bibr" target="#b2">[Ash et al., 2021</ref><ref type="bibr" target="#b35">, Mindermann et al., 2022]</ref>. Importantly, this algorithm still prioritizes selecting a diverse dataset. By conditioning on past data at step i, the objective encourages the algorithm to select data that is different from data that has already been selected.</p><p>We can also make an empirical bayes version by adding D prior :</p><p>min x1,...,xn⊂Dtrain n i=1 log θ Pr(x i |θ) Pr(θ|D prior , D down , x &lt;i ) (12) + log θ Pr(x i |θ) Pr(θ|D prior , x &lt;i ) (13) This is, of course, still intractable since it requires integrating the parameters. But, since we have already introduced the greedy algorithm that encourages diversity, if we now make the point estimate 5000 10000 15000 20000 Step 4.6 4.8 5.0 5.2 Books Cross Entropy (↓) Transfer from C4 to Books (τ = 2) Online Selection Random Final: Random 1x Final: Random 8x 0 5000 10000 15000 20000 25000</p><p>Step approximation, the incentive for data diversity remains. This results in:</p><formula xml:id="formula_7">≈ min x1,...,xn⊂Dtrain n i=1 -log Pr(x i |θ prior+down+x&lt;i ) + log Pr(x i |θ prior+x&lt;i )<label>(14)</label></formula><p>The thorny issue here is how to define θ prior+down+x&lt;i and θ prior+x&lt;i in practice. In theory, these parameters should be trained on an iid sample from the union of the datasets. If we add the datapoints one at a time, the dynamics of the distribution shift over time can change how well the model corresponds to conditioning on the union of the dataset. But, this would require re-training the models every time we add a new x i which is clearly impractical.</p><p>In practice, this encourages using a fine-tuning approach (as in RHO) where we continually fine-tune on the x i as they are added. But when D down is small and the data distribution changes over time, we can get catastrophic forgetting and unstable training dynamics. For these reasons, RHO avoids training the conditional model entirely <ref type="bibr">(Appendix D of Mindermann et al. [2022]</ref>). We also conduct an experiment on the Books task where we use this online fine-tuning algorithm that updates both the marginal and conditional models as we add data to S. Results in Figure <ref type="figure" target="#fig_11">9</ref> show how the training is unstable and in fact performs worse than random.</p><p>Moreover, Note that the computational cost of even the cheapest fine-tuning algorithm is substantial compared to the algorithms in the paper. In particular, the serial cost is now 2τ n + 4n (as compared to τ n + 2n for RHO) since we need to pass the full τ n samples through both the conditional and marginal models. So this variant is clearly inferior in practice to the other approaches we consider. tokens (i.e. cost until we reach the dotted line in Figure <ref type="figure" target="#fig_2">1</ref>). We split cost into the scoring cost for filtering the data using the small auxiliary models and then training cost for the large model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Compute cost for scale generalization</head><p>In the main text we computed the cost for τ = 16 in terms of model forwards of 1 billion tokens.</p><p>Here we can convert this to FLOPs and compute the cost for all values of τ . Results are in Figure <ref type="figure" target="#fig_12">10</ref> showing the breakdown of costs into scoring FLOPs for running the small auxiliary models over the data and training FLOPs for training the large model. We measure the cost it takes to reach the final performance of the random model, i.e. until the CoLoR-filter learning curve crosses the dotted line in Figure <ref type="figure" target="#fig_2">1</ref>. The main tradeoff is that lower τ values require more scoring cost and less training cost because they are able to select better data.</p><p>We should also note that if multiple models are being trained with the same dataset, then this scoring cost can be amortized over those runs and the larger τ values will look even better.</p><p>E Can we do data selection in distribution? RHO provides marginal gains here, while CoLoR-Filter does not provide gains at all. Conditional Only is worse than random. Scaling τ does not change results as much as when we target downstream tasks.</p><p>One obvious question raised by these data selection techniques is whether they can work in distribution, i.e. can we select data to make the iid loss on C4 go down faster? In Figure <ref type="figure" target="#fig_13">11</ref> we present results for running this experiment with CoLoR-Filter as well as RHO and Conditional Only. Note that there is no difference between RHO and RHO + prior now (and we drop the "down" from the name) since the prior distribution and the downstream distribution are the same. To implement CoLoR-Filter in this setting, we just take two checkpoints from pre-training the prior model and call the earlier one (at 2.5b tokens) the marginal model and the later one (at 3.1b tokens) the conditional model.</p><p>We find that in distribution selection does not work effectively with these methods. There are small gains to RHO loss, but here they are massively outweighed by the computational cost of the selection. CoLoR-Filter sees no gain at all over random and Conditional Only is worse than random. These preliminary results suggest why it is important to recognize that data selection (especially with these methods) will be most effective when we genuinely want to target a different distribution from D train . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Global vs. batchwise selection</head><p>One more minor implementation aspect about CoLoR-Filter is that as presented in Algorithm 1, we do global selection where we take the best n data points across the entire train set, while in RHO-down in Algorithm 2 selection is done batchwise. Here we ablate whether the ability to do global selection is actually helpful for CoLoR-Filter. Results in Figure <ref type="figure" target="#fig_14">12</ref> suggest that there is not much difference between the two and at small τ , batchwise selection maybe even beat global selection. We provide this result to illustrate that CoLoR-Filter is fairly robust to how the selection is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Finetuning after targeted pre-training</head><p>One possible question about the targeted pre-training setting we consider is: what happens if we finetune on D down after the targeted pre-training?</p><p>This is interesting since while the pre-trained models presented in the main text never have direct access to D down , the selection algorithm does. In this section, we also allow access to D down after pre-training and then compare the final performance of the finetuned models that are pre-trained on random data vs. selected data.</p><p>First, in Table <ref type="table" target="#tab_10">5</ref> and Table <ref type="table" target="#tab_11">6</ref> we present finetuning results for the 150m models. We find that CoLoR-Filter data outperforms 8x as much random data after finetuning. Note that the conditional model that we use to guide the selection of CoLoR-Filter is equivalent to a model that has been pre-trained on 3B random tokens and then finetuned on the task. Thus, these results show that we are substantially outperforming the conditional model when both models are finetuned on the downstream data. Next, we present results for the 1.2b models in Table <ref type="table">7</ref> and <ref type="table">Table 8</ref>. We find that the CoLoR-Filter model outperforms or is competitive with training on about 10x as much data randomly selected data. We should also note that the CoLoR-Filter models are now dramatically outperforming the 150m conditional models that were used to filter the data, showing positive scale transfer of data selection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Inspecting the selected data</head><p>In this section, we conduct some basic analysis of the data that is selected by CoLoR-Filter. We leave a full analysis to future work, but here we provide some high level statistics about the distributions of the scores of the conditional vs. marginal models and some representative examples from the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1 Distribution of scores</head><p>First, we simply plot the CDFs of the conditional loss reduction (CoLoR) score function used to select the data. We find that there are relatively few outliers and the CoLoR scores are fairly concentrated and normally distributed. Moreover, we note that the mean CoLoR in both experiments is positive, meaning that the conditional model actually has higher losses on the datapoints in C4 than the marginal model. This makes sense because the conditional model has been finetuned on D down which is out of distribution relative to C4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Representative examples</head><p>Now we just list a few representative examples to give a flavor for the types of outliers that exist under our ranking of sequences and the sorts of typical sequences that are selected versus excluded. The sequences are sampled randomly from different quantiles of the distribution and we shorten all the sequences so that they fit more easily on the page.</p><p>Figure <ref type="figure" target="#fig_8">14</ref> shows outliers when targeting Books and Figure <ref type="figure" target="#fig_2">15</ref> shows more typical examples when targeting Books. Generally, we found that the documents with very high scores contain things like old English, poetry, and tables of contents that are particularly unusual in books compared to the rest of the internet. Other things like fiction and dialogue are also highly scored. Negative outliers typically have things like poorly encoded text or advertisements.</p><p>Figure <ref type="figure" target="#fig_2">16</ref> shows outliers when targeting downstream tasks and Figure <ref type="figure" target="#fig_2">17</ref> shows more typical examples when targeting downstream tasks. Here the patterns are less clear since the target tasks are more diverse, but we did observe many scientific and wiki-style documents with high scores as well as some descriptions of physical interactions that may be useful for common sense tasks. Again, the negative outliers tend to have things like poorly encoded text or advertisements.</p><p>AS now shall ye wyt, what tyme of the day ye shall angle. From the begynning of Maye vntill it be September: the byting tyme is early in the morow from four of the clocke vnto eyght of the clocke, at after none from foure to eyght also, but not so good as in the mornyng, and if it be a colde wynde and a lowryng day, it is muche better than a cleere daye.</p><p>Also many poole fysshes will byte best in the morne tyde. And if ye se in any tyme of the day the Troute or greylyng lepe angle to him with a dub according to the same moneth. And where the water ebbeth and floweth: the fish wyll byte in some place at the ebbe and in some place at the flud after they haue restyng (a) Good outlier, CoLoR = -0.35 ??????????????????????????????? ???????????????????????????????? ????????????????????????????????? ?????????????????????????????????? ?????????????????????????????????? ???????????????????????????????? ??????????????????????????????????? ???????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ????????????????????????????????????? ?????????????????????????????????????</p><p>???????????? ????????????????????????? m88 ???????????????????????? ???? m88 ??????????????????????????????????????</p><p>??????????????????????????? ???? ?????????????????????????????????? ??????????????????????????????? ? ??????????????????? (b) Bad outlier, CoLoR = 5.45 Figure <ref type="figure" target="#fig_8">14</ref>: Examples of outliers when targeting Books. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is written in an older dialect of English which may be related to some documents in the Project Gutenberg corpus, while the negative outlier appears to be poorly encoded.</p><p>C: Mrs Mackenzie, was there ever a time when you felt like you could just hop on a plane and make that flight down to the next State to be with your boys? B: Oh my dear, yes. I feel sometimes as if I'm twenty and so fit and active and I can do whatever I want to do and then I remember, good grief, I'm 86, you old fool, you can't do that. I wish I could just fly down there and live with them all together just how it was when they were little and I was their Mum and they followed me because I was so bright and cheery and smart and active and all the things that I'm not now. Oh, I'm so sorry, listen to me</p><p>. Maybe I'm just losing my marbles, what do you think, dear? C: Smiling -Imagine if I waved a magic wand and miraculously you were twenty again. What would you see yourself doing Beryl. Is it ok if I call you Beryl? (a) Sequence from best 3%, CoLoR = 0.40 Chamber of Commerce and other business venues, such as the Gwinnett Civic &amp; Convention Centers and is an ideal working environment for commercial businesses and corporations in Northeast Atlanta.</p><p>The prominent location is on a heavily wooded, landscaped 6.5 acre site fronting on I-85. The exterior features green-tinted thermal glass and the entrance features a curtain wall glass leading into a granite-floored lobby with vaulted ceilings. Gwinnett County is home to leading Fortune 500 companies, drawn by its reputation as a commerce and technology hub, providing businesses with a regional market of five million people. SERVPRO of Gurnee can simplify the restoration process by handling both the initial water damage mitigation and rebuilding the affected areas. Having one qualified company for the entire process can save time and keep costs low.</p><p>(b) Sequence from median 3%, CoLoR = 0.73</p><p>Figure <ref type="figure" target="#fig_2">15</ref>: Examples of more typical documents when targeting Books. First a document from the top 3% that would be selected with τ = 32, and then a document that scores near the median of all documents. The selected document is fictional dialogue while the median document is an advertisement.</p><p>among the pinacoderm are the ostia that allow entry of water into the body of the sponge. These pores have given the sponges their phylum name Porifera-pore-bearers. In some sponges, ostia are formed by porocytes, single tube-shaped cells that act as valves to regulate the flow of water into the spongocoel. In other sponges, ostia are formed by folds in the body wall of the sponge.</p><p>Between the outer layer and the feeding chambers of the sponge is a jelly-like substance called the mesohyl, which contains collagenous fibers. Various cell types reside within the mesohyl, including amoebocytes, the "stem cells" of sponges, and sclerocytes, which produce skeletal materials. The gel-like consistency of mesohyl acts like an endoskeleton and maintains the tubular morphology of sponges.</p><p>The feeding chambers inside the sponge are lined by choanocytes ("collar cells"). Figure <ref type="figure" target="#fig_2">16</ref>: Examples of outliers when targeting downstream tasks. Examples are sampled randomly from the top or bottom 1000 sequences. The positive outlier is a scientific document that could be relevant for tasks like SciQ, while the negative outlier appears to be poorly encoded.</p><p>summer plans. After thinking for a while I decided to spend my summer in Squamish, where I would work for the Admissions Team. However, due to a very large number of students interested to work on campus and a limited number of work positions, I ended up not getting a job on campus. I was very upset indeed and I began to think that there were not any job openings elsewhere, which would then result in me travelling back home. Surprisingly, there were many job opportunities in the Squamish community. Since Quest University Canada hosted a job fair on campus I, along with all the students, had the chance to meet local businesses that were looking for summer employees. It was a great opportunity to network and give my resume to the ones that interested me. (a) Sequence from best 3%, CoLoR = 0.33 Can I install PDF Stacks on more than one computer? The license key is valid for only one device and is non-transferable. You can obtain additional license key(s) by placing an order. How do I use PDF Stacks? Click "File" and then "Import Folder"</p><p>Once you import the PDF files, your files will be copied into PDF Stacks for easier ability to read, search, organize, take notes, print and share.</p><p>Any questions, ask us! How do I create collections (virtual binders) and match/tag my documents for better organization? It's easy. Watch the video for creating collections and tagging documents. Can multiple users access the same documents or can I access and sync my documents through multiple devices?</p><p>(b) Sequence from median 3%, CoLoR = 0.55</p><p>Figure <ref type="figure" target="#fig_2">17</ref>: Examples of more typical documents when targeting downstream tasks. First a document from the top 3% that would be selected with τ = 32, and then a document that scores near the median of all documents. The selected document appears to be a journal entry while the median document is software documentation</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>38th</head><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2024).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>CoLoR-FilterRequire: Prior data D prior , downstream data D down , training data D train , budget n, subset size multiplier τ 1: Pre-train θ marg on D prior 2: fine-tune to get θ cond on D down initialized from θ marg 3: Select a random subset D τ of size τ n from D train 4: Select data: S = bottom-n x∈Dτlog Pr(x|θ cond ) + log Pr(x|θ marg ) 5: return Selected dataset S to train θ on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2</head><label>2</label><figDesc>RHO-downRequire: Downstream data D down , train data D train , budget n, subset size multiplier τ , batch size b 1: Train θ cond on D down 2: Initialize a random θ marg 1 and S = ∅ 3: for t ∈ [1, . . . , n/b] do 4: Randomly select a batch B t ⊂ D train of size τ b 5: Select data: Bt = bottom-b x∈Btlog Pr(x|θ cond ) + log Pr(x|θ marg t ) t+1 by training on Bt 8: end for 9: return Selected dataset S to train θ on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Scaling of final performance with τ when targeting Books with 150m parameter models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Scaling CoLoR-Filter with τ when training 1.2b models with data selected by 150m models. Curves end when we exhaust the data in C4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>5 and measuring n in billions of tokens, the total cost for training the CoLoR-Filter model is 3m + 2τ n + 3nL = 3 * 3.1 + 2 * 16 * 1.5 + 3 * 1.5 * 5.5 = 82 while the cost for training on 25 billion random tokens is 3N L = 3 * 25 * 5.5 = 412.5, illustrating a more than 5x total compute savings to achieve the same performance on Books. A full plot visualizing the cost in FLOPs for all τ is in Appendix D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Final performance versus τ on the suite of downstream tasks for 150m models. CoLoR-Filter scales the best with τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Sweeping over τ when targeting Books from C4 for 150m models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (Left) Performance of online selection with fine-tuning as outlined in Equation (14). Online selection is worse than random. (Right) Training curves for the conditional and marginal models on the selected data S. The conditional model faces training instability early on (associated with forgetting), and then eventually becomes better than the marginal on the selected data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure10: Costs in FLOPs to reach equivalent performance to the final random model trained on 25b tokens (i.e. cost until we reach the dotted line in Figure1). We split cost into the scoring cost for filtering the data using the small auxiliary models and then training cost for the large model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Using a sample of C4 as D down . RHO provides marginal gains here, while CoLoR-Filter does not provide gains at all. Conditional Only is worse than random. Scaling τ does not change results as much as when we target downstream tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison between global and batchwise variants of CoLoR-Filter on Books. The two perform nearly identically here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: CDFs for the conditional loss reduction (CoLoR), i.e.log Pr(x|θ prior+down ) -(-log Pr(x|θ prior )). The dashed line highlights the cutoff point for τ = 64. We select the points with the lowest CoLoR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>(a) Good outlier, CoLoR = -0.46 *** **********. ****** *** ***, *** ******* **** **** ** ******** ******* plates ** ****** ** ** **-** *** (*** ******* ** tested), ***** ******* ********. *** ** *** ***, *** ********* *.* ********* ******* ***** capture ****** ******** ******** ****** ** **** ** **** ******, &gt;10 ***, *** ******, **+ ***, **** ** ****** ***** or ****, *** ** ***** ****** *** *** **** **** field ** ****, ***** **'. ***** ******* ********, *** ******** ** ***** ****** ****** ****** to ******* ****** ** ****** **** ** **** ****** ** night, ****** ******* ******* *** ********. ******** ******** ** */****, *** ******** ****** ******** ******** **** front *** **** ****** ****** ** *** *** **** ******. However, **** ******* ******* *** ********** ** *** ***** ** night, ****** ** **** ****** *** ******* ************ ** *** scene. (b) Bad outlier, CoLoR = 5.36</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Compute cost of the various algorithms measured in "model forwards". The total cost of selection and training on the selected data is the sum of all costs across a row. The variables are m = |D prior |, n = |S|, τ is a hyperparameter that controls how aggressively we subselect, and L is a multiplier of the cost of model forwards between the selection model(s) and the target model (approximately the ratio of parameter counts between the models).For example, in our experiments, when we use 150 million parameter models to select data and then train a 1.2 billion parameter model on the resulting data, then L ≈ 5.5 2 . Training thus costs 3nL across all methods since we run a forward and backward for the large model on all n sequences.</figDesc><table><row><cell>Method</cell><cell cols="4">Prior cost Serial cost Parallel cost Training cost</cell></row><row><cell>CoLoR-Filter</cell><cell>3m</cell><cell>0</cell><cell>2τ n</cell><cell>3nL</cell></row><row><cell>Conditional Only</cell><cell>3m</cell><cell>0</cell><cell>τ n</cell><cell>3nL</cell></row><row><cell>RHO-down</cell><cell>0</cell><cell>τ n + 2n</cell><cell>τ n</cell><cell>3nL</cell></row><row><cell cols="2">RHO-down + Prior 3m</cell><cell>τ n + 2n</cell><cell>τ n</cell><cell>3nL</cell></row><row><cell>Random</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>3nL</cell></row><row><cell>selection.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>CoLoR-Filter. The cost of selection is 2τ n forward passes. But, this selection process is entirely parallelizable. Training the prior model costs 3m forwards since |D prior | = m. And training a model on the selected data costs 3nL forward passes. So the total cost is 3m + 2τ n + 3nL, but the 2τ n scoring computation can be done in parallel.</p><p>Conditional Only. The conditional-only method is almost the same as CoLoR-Filter, except we only need τ n forward passes for selection since we only run one model over the data. The cost is thus 3m + τ n + 3nL, with τ n being parallelizable.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Task generalization for the 1.2b models with τ = 64.</figDesc><table><row><cell>Method</cell><cell>copa rte</cell><cell>cb</cell><cell cols="2">sst2 commonsense qa social iqa</cell></row><row><cell>Random (25b tokens)</cell><cell cols="3">69.2 48.9 42.8 46.8 33.7</cell><cell>42.9</cell></row><row><cell cols="4">CoLoR-Filter (τ = 64, 2.5b tokens) 65.8 52.6 46.0 55.8 32.6</cell><cell>42.7</cell></row><row><cell cols="5">Note, we also conduct a few more experiments and ablations in the appendix: Appendix E considers</cell></row><row><cell cols="5">using CoLoR-Filter in-distribution to target C4 loss, Appendix F considers applying CoLoR-Filter</cell></row><row><cell cols="5">batchwise rather than globally, Appendix G considers finetuning on D down after targeted pre-training,</cell></row><row><cell cols="5">Appendix I inspects some of the selected and excluded examples, and Appendix J compared to</cell></row><row><cell>FineWeb-edu [Penedo et al., 2024].</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Final performance for all tasks for 1.2b models. Note that the CoLoR-Filter models do not train on as many tokens since we exhaust all of the tokens in C4 with these settings of τ .</figDesc><table><row><cell></cell><cell>hella-swag</cell><cell cols="2">piqa arc-c arc-e</cell><cell cols="2">open-book qa</cell><cell cols="2">sciq boolq</cell><cell>wino-grande</cell><cell>Avg</cell></row><row><cell>Random 1x</cell><cell>33.2</cell><cell cols="3">64.5 22.4 44.4 26.8</cell><cell></cell><cell cols="2">66.9 58.8</cell><cell>53.3</cell><cell>46.3</cell></row><row><cell>CoLoR-Filter</cell><cell>38.6</cell><cell cols="3">68.7 25.3 51.8 32.0</cell><cell></cell><cell cols="2">72.8 54.3</cell><cell>49.4</cell><cell>49.1</cell></row><row><cell>Conditional Only</cell><cell>33.0</cell><cell cols="3">65.6 23.0 42.2 27.2</cell><cell></cell><cell cols="2">64.6 61.4</cell><cell>51.1</cell><cell>46.0</cell></row><row><cell>RHO-down</cell><cell>35.5</cell><cell cols="3">67.3 25.3 46.9 29.2</cell><cell></cell><cell cols="2">67.5 48.6</cell><cell>48.7</cell><cell>46.1</cell></row><row><cell cols="2">RHO-down + prior 35.6</cell><cell cols="3">66.6 25.3 49.3 29.4</cell><cell></cell><cell cols="2">69.0 61.6</cell><cell>50.9</cell><cell>48.5</cell></row><row><cell>DSIR</cell><cell>37.6</cell><cell cols="3">68.8 24.4 46.6 27.8</cell><cell></cell><cell cols="2">68.4 59.9</cell><cell>52.6</cell><cell>48.3</cell></row><row><cell>Random 8x</cell><cell>38.2</cell><cell cols="3">67.8 23.5 44.2 28.8</cell><cell></cell><cell cols="2">65.3 58.1</cell><cell>50.5</cell><cell>47.1</cell></row><row><cell>Method</cell><cell></cell><cell>hella-swag</cell><cell cols="2">piqa arc-c arc-e</cell><cell cols="2">open-book qa</cell><cell>sciq boolq</cell><cell>wino-grande</cell><cell>Avg</cell></row><row><cell>Random (25b tokens)</cell><cell></cell><cell>52.9</cell><cell cols="3">73.0 26.1 53.7 32.8</cell><cell></cell><cell>75.5 56.7</cell><cell>54.3</cell><cell>53.1</cell></row><row><cell cols="2">CoLoR-Filter (τ = 7, 25b tokens)</cell><cell>62.3</cell><cell cols="3">75.6 29.7 60.3 38.0</cell><cell></cell><cell>79.7 48.3</cell><cell>58.0</cell><cell>56.5</cell></row><row><cell cols="3">CoLoR-Filter (τ = 16, 10b tokens) 59.3</cell><cell cols="3">75.4 31.7 62.7 36.2</cell><cell></cell><cell>81.0 57.7</cell><cell>56.4</cell><cell>57.6</cell></row><row><cell cols="2">CoLoR-Filter (τ = 32, 5b tokens)</cell><cell>54.8</cell><cell cols="3">74.3 29.4 60.9 35.4</cell><cell></cell><cell>78.4 59.1</cell><cell>54.1</cell><cell>55.8</cell></row><row><cell cols="3">CoLoR-Filter (τ = 64, 2.5b tokens) 49.3</cell><cell cols="3">73.2 28.9 59.7 35.6</cell><cell></cell><cell>77.1 59.8</cell><cell>53.0</cell><cell>54.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Performance after finetuning on Books for different pre-trained 150m models. Note that the Random (3.1b tokens) model is equivalent to the conditional model used to select data with CoLoR-Filter (τ = 16).</figDesc><table><row><cell>Pre-training data</cell><cell>Finetuned Books Val Cross Entropy</cell></row><row><cell>Random (3.1b tokens)</cell><cell>3.441</cell></row><row><cell>Random (25b tokens)</cell><cell>3.357</cell></row><row><cell>CoLoR-Filter (3.1b tokens)</cell><cell>3.258</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Held out performance after finetuning on downstream data for different pre-trained 150m models. Note that the Random (3.1b tokens) model is equivalent to the conditional model used to select data with CoLoR-Filter (τ = 16).</figDesc><table><row><cell>Pre-training data</cell><cell>hella-swag</cell><cell>piqa arc-c arc-e</cell><cell>open-book qa</cell><cell>sciq boolq</cell><cell>wino-grande</cell><cell>Avg</cell></row><row><cell>Random (3.1b tokens)</cell><cell>34.4</cell><cell cols="2">66.6 24.8 51.7 28.0</cell><cell>89.9 65.6</cell><cell>53.1</cell><cell>51.8</cell></row><row><cell>Random (25b tokens)</cell><cell>39.5</cell><cell cols="2">69.8 29.2 53.9 30.2</cell><cell>91.4 64.2</cell><cell>52.9</cell><cell>53.9</cell></row><row><cell cols="2">CoLoR-Filter (3.1b tokens) 39.2</cell><cell cols="2">71.1 29.1 55.3 33.2</cell><cell>90.0 65.1</cell><cell>51.6</cell><cell>54.3</cell></row><row><cell cols="7">Table 7: Performance after finetuning on Books for different pre-trained 1.2b models. Note that the</cell></row><row><cell cols="4">conditional model that selects data is only 150m parameters.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pre-training data</cell><cell cols="3">Finetuned Books Val Cross Entropy</cell><cell></cell><cell></cell></row><row><cell cols="3">Random (25b tokens)</cell><cell>3.074</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">CoLoR-Filter (2.6b tokens)</cell><cell>2.964</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Training parameters, based on<ref type="bibr" target="#b61">Wortsman et al. [2024]</ref>,<ref type="bibr" target="#b20">Groeneveld et al. [2024]</ref> </figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Batch size</cell><cell>256</cell></row><row><cell>Learning rate</cell><cell>1e-3</cell></row><row><cell>Schedule</cell><cell>Linear warmup, cosine decay</cell></row><row><cell>Warmup steps</cell><cell>5% of total steps</cell></row><row><cell cols="2">z-loss coefficient 1e-4</cell></row><row><cell>Weight decay</cell><cell>0.0</cell></row><row><cell>β 1</cell><cell>0.9</cell></row><row><cell>β 2</cell><cell>0.95</cell></row><row><cell>ϵ</cell><cell>1e-15</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Prior work[Mindermann et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2022,<ref type="bibr" target="#b15">Evans et al., 2023]</ref> has referred to the models that estimate these two terms as the "reference" and "learner" or "actor", respectively. We opt for the names conditional and marginal for clarity in connections to the Bayesian viewpoint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Even though there are 8x as many parameters in the large model, the FLOP multiplier is less since the attention computations take the same number of FLOPs regardless of parameters.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>HZ is supported by an <rs type="funder">Eric</rs> and <rs type="grantName">Susan Dunn Graduate Fellowship</rs>. SK acknowledges support from the <rs type="funder">Office of Naval Research</rs> under award <rs type="grantNumber">N00014-22-1-2377</rs> and the <rs type="funder">National Science Foundation</rs> Grant under award #<rs type="grantNumber">IIS 2229881</rs>. This work has been made possible in part by a gift from the <rs type="funder">Chan Zuckerberg Initiative Foundation</rs> to establish the <rs type="projectName">Kempner Institute for the Study of Natural and Artificial Intelligence</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_kyCgqDC">
					<orgName type="grant-name">Susan Dunn Graduate Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_Tw8tRwc">
					<idno type="grant-number">N00014-22-1-2377</idno>
				</org>
				<org type="funding" xml:id="_j29EJnz">
					<idno type="grant-number">IIS 2229881</idno>
				</org>
				<org type="funded-project" xml:id="_uyWmWFs">
					<orgName type="project" subtype="full">Kempner Institute for the Study of Natural and Artificial Intelligence</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://github.com/davidbrandfonbrener/color-filter-olmo">https://github.com/davidbrandfonbrener/color-filter-olmo</ref> Filtered data: <ref type="url" target="https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4">https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Comparison to Fineweb-Edu</head><p>Concurrent to our initial work, <ref type="bibr" target="#b40">Penedo et al. [2024]</ref> released FineWeb-edu, a classifier for educational content that can filter the FineWeb dataset. Here we provide a comparison between CoLoR-Filter and this classifier-based approach. Specifically, we re-implement the CoLoR-Filter pipeline on top of the Fineweb dataset and with slightly smaller auxiliary models (125m) to make a more fair comparison to FineWeb-edu. Then we compare on the same suite of 8 downstream tasks over various settings of τ using the two scores: CoLoR-Filter or the FineWeb-edu classifier. We then train larger models (680M parameters) for 10B tokens of selected data. Results are shown in fig. <ref type="figure">18</ref>. We find that CoLoR-Filter consistently outperforms FineWeb-edu, which is not so surprising since we are doing more targeted data selection by specifically targeting the downstream NLP tasks rather than a general notion of "educational content".</p><p>Figure <ref type="figure">18</ref>: A comparison of the performance of 680m models trained on 10B tokens selected with various τ between CoLoR-Filter and FineWeb-edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Broader Impact</head><p>The development of the CoLoR-Filter for data selection has notable broader impacts on both machine learning and society. It enhances efficiency in language model training, leading to reduced computational resources and environmental footprint, while its scalability democratizes access to high-performing models. The method's success in diverse downstream tasks promises advancements in fields like medical text processing and legal analysis. However, it also raises concerns about dataset bias, necessitating continuous evaluation and updates. Future research should focus on ensuring models do not inherit biases from the selected training data, extending applications, improving efficiency, and implementing safeguards to maximize societal benefits while minimizing risks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Compute resources</head><p>All training is conducted on an internal cluster using H100 GPUs. On one GPU, each 150m training run for 3.1b tokens takes about 4 hours, running the auxiliary models offline and in parallel can be faster. Training the 1.2b model to completion takes about 2 days on 4 GPUs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semdedup: Dataefficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Physics of language models: Part 3.3, knowledge capacity scaling laws</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gone fishing: Neural active learning with fisher embeddings</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8927" to="8939" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep batch active learning by diverse, uncertain gradient lower bounds</title>
		<author>
			<persName><forename type="first">Chicheng</forename><surname>Jordan T Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><surname>Agarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03671</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prediction-oriented Bayesian active learning</title>
		<author>
			<persName><forename type="first">Freddie</forename><forename type="middle">Bickford</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Casson</surname></persName>
		</author>
		<ptr target="https://adamcasson.com/posts/transformer-flops" />
		<title level="m">Transformer flops</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on evaluation of large language models</title>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaijie</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<title level="m">Exploring the surprising difficulty of natural yes/no questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Redpajama: an open dataset for training large language models</title>
		<ptr target="https://github.com/togethercomputer/RedPajama-Data" />
	</analytic>
	<monogr>
		<title level="m">Together Computer</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate submodularity and its applications: Subset selection, sparse approximation and dictionary selection</title>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v19/16-534.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What&apos;s in my big data?</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Helgi Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dsdm: Model-aware dataset selection with datamodels</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Talfan</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Merzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryutaro</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.05328</idno>
		<title level="m">Bad students make great teachers: Active learning accelerates large-scale visual understanding</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin Madappally</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Data filtering networks</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The little book of deep learning. A lovely concise introduction</title>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">297</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zou</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10256836" />
		<imprint>
			<biblScope unit="page" from="12" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Marc G Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In international conference on machine learning</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1311" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuling</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crystal</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilasha</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Luca Soldaini</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<pubPlace>Noah A. Smith</pubPlace>
		</imprint>
	</monogr>
	<note>and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<editor>Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Textbooks are all you need</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Training compute-optimal large language models</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Máté</forename><surname>Lengyel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.00622</idno>
		<title level="m">Datamodels: Predicting predictions from training data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Advanced deep active learning and data subset selection: unifying principles with information-theory intuitions</title>
		<author>
			<persName><surname>Kirsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deduplicating training data makes language models better</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8424" to="8445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rho-1: Not all tokens are what you need</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On a measure of the information provided by an experiment</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><surname>Lindley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="986" to="1005" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A pretrainer&apos;s guide to training data: Measuring the effects of data age, domain coverage, quality, &amp; toxicity</title>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13169</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<title level="m">A benchmark for evaluating language model fit</title>
		<meeting><address><addrLine>Paloma</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Meta. Llama</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02789</idno>
		<title level="m">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prioritized training on points that are learnable, worth learning, and not yet learnt</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">M</forename><surname>Sören Mindermann</surname></persName>
		</author>
		<author>
			<persName><surname>Brauner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinank</forename><surname>Muhammed T Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Höltgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Morisot</surname></persName>
		</author>
		<author>
			<persName><surname>Farquhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15630" to="15649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 conference short papers</title>
		<meeting>the ACL 2010 conference short papers</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functions-i</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><forename type="middle">L</forename><surname>Fisher</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:206800425" />
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Min</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Leclerc</surname></persName>
		</author>
		<author>
			<persName><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14186</idno>
		<title level="m">Trak: Attributing model behavior at scale</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimal design of experiments</title>
		<author>
			<persName><forename type="first">Friedrich</forename><surname>Pukelsheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modern bayesian experimental design</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freddie</forename><forename type="middle">Bickford</forename><surname>Desi R Ivanova</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="114" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Socialiqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09728</idno>
		<title level="m">Commonsense reasoning about social interactions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laion-5b: An open large-scale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarush</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SlimPajama: A 627B token cleaned and deduplicated version of RedPajama</title>
		<author>
			<persName><forename type="first">Daria</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Al-Khateeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">R</forename><surname>Steeves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nolan</forename><surname>Dey</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/cerebras/SlimPajama-627B" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Beyond neural scaling laws: beating power law scaling via data pruning</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Sorscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="19523" to="19536" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning large-scale neural fields via context pruned meta-learning</title>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sihyun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Richard</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00937</idno>
		<title level="m">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improving llm pretraining via document de-duplication and diversification</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<editor>Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov</editor>
		<imprint/>
	</monogr>
	<note>and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">E</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><surname>Kornblith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34201" to="34227" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
