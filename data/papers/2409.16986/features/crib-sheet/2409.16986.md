- **Importance of Data Selection**: Data selection is crucial in pretraining large language models (LLMs) due to varying quality in training corpora.
  
- **Influence Score**: A high influence score indicates that a data instance is likely to enhance model performance. The influence function is a key method for scoring data instances.

- **Limitations of Current Methods**:
  - **Computational Cost**: Calculating influence scores for all data instances is time-consuming due to the need for Hessian matrix computation.
  - **Lack of Diversity**: Top-k selection based on influence scores often results in a lack of diversity, which can hinder generalization.

- **Quad Approach**: 
  - **Objective**: To balance quality and diversity in data selection.
  - **Clustering**: Organizes data into clusters to ensure similarity within clusters and diversity across clusters.
  - **Sampling Strategy**: Uses a Multi-Arm Bandit (MAB) approach to sample from clusters based on influence scores and sampling frequency.

- **Influence Calculation**:
  - **Attention Layers**: Incorporates attention layers in the influence calculation to capture more semantic details.
  - **Kronecker Product**: Utilizes the Kronecker product to approximate the Hessian matrix, improving computational efficiency.

- **Cluster Score (CS)**: 
  - **Formula**: 
    \[
    CS_i = \bar{I}_i + \alpha \frac{2 \ln j T(C_j)}{T(C_i)}
    \]
  - **Components**: 
    - \(\bar{I}_i\): Average influence score of cluster \(C_i\).
    - \(T(C_i)\): Frequency of instances sampled from cluster \(C_i\).

- **Data Selection Process**:
  - Sample top-k clusters based on CS.
  - Calculate influence scores for samples in each cluster.
  - Select instances with influence scores above a threshold \(\tau\) for training.

- **Experimental Results**: Quad outperforms existing data selection methods on the Slimpajama dataset, achieving a 1.39% improvement in zero-shot accuracy with lower computational resource consumption.

- **Key Contributions**:
  - Balancing quality and diversity through iterative MAB sampling.
  - Novel influence computation method leveraging attention layers.
  - Demonstrated effectiveness through empirical results on popular downstream tasks.