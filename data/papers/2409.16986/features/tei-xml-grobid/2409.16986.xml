<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HARNESSING DIVERSITY FOR IMPORTANT DATA SE-LECTION IN PRETRAINING LARGE LANGUAGE MOD-ELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-05">5 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huaping</forename><surname>Zhong</surname></persName>
							<email>zhonghuaping@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kuan</forename><surname>Zhang</surname></persName>
							<email>zhangkuan@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chengliang</forename><surname>Chai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<email>wanggrbit@bit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinlin</forename><surname>Zhuang</surname></persName>
							<email>zhuangxinlin@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Bai</surname></persName>
							<email>baitianyi@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiantao</forename><surname>Qiu</surname></persName>
							<email>qiujiantao@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Cao</surname></persName>
							<email>lcao@csail.mit.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ju</forename><surname>Fan</surname></persName>
							<email>fanj@ruc.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
							<email>yuan-ye@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoren</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
							<email>heconghui@pjlab.org.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Artificial Intelligence Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HARNESSING DIVERSITY FOR IMPORTANT DATA SE-LECTION IN PRETRAINING LARGE LANGUAGE MOD-ELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-05">5 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2F3B5D40190CFA75B1ACC509A6317A5E</idno>
					<idno type="arXiv">arXiv:2409.16986v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data selection is of great significance in pretraining large language models, given the variation in quality within the large-scale available training corpora. To achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, i.e., a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-k instances with the highest scores. However, this approach has several limitations. (1) Calculating the accurate influence of all available data is time-consuming. (2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks. In this paper, we introduce Quad, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results. To compute the influence (i.e., the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. For the diversity, Quad clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity. Experiments on Slimpajama demonstrate that Quad significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation. Our code and data are available at (<ref type="url" target="https://anonymous.4open.science/r/Quad/">https://anonymous.4open.science/r/Quad/</ref>).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recently, large language models (LLMs) have significantly advanced the field of artificial intelligence <ref type="bibr" target="#b50">(Zhao et al., 2023;</ref><ref type="bibr" target="#b16">Hadi et al., 2023;</ref><ref type="bibr" target="#b24">Minaee et al., 2024)</ref>. Due to the unprecedented number of parameters (model size) and the pre-training on huge amount of training data, LLMs are generalizable a broad spectrum of downstream tasks. However, in practice, the computation resources limit both the model size and the volume of data used in pre-training. In this situation, judiciously selecting train datasets is critical for producing highly performance LLMs <ref type="bibr" target="#b3">(Brown, 2020;</ref><ref type="bibr" target="#b8">Du et al., 2022;</ref><ref type="bibr" target="#b15">Gururangan et al., 2020;</ref><ref type="bibr" target="#b17">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b31">Raffel et al., 2020)</ref>. In particular, the quality of the training datasets vary dramatically, while the LLaMA-3.1 report <ref type="bibr" target="#b9">(Dubey et al., 2024)</ref> shows that the use of high quality data in later training stages can greatly improve model performance.</p><p>Typical straightforward data selection approaches include rule-based data filtering <ref type="bibr" target="#b31">(Raffel et al., 2020;</ref><ref type="bibr" target="#b30">Rae et al., 2021)</ref>, querying high-performance models (e.g., GPT-4) <ref type="bibr" target="#b44">(Wettig et al., 2024;</ref><ref type="bibr" target="#b32">Sachdeva et al., 2024)</ref>, surrogate models <ref type="bibr" target="#b19">(Lin et al., 2024;</ref><ref type="bibr" target="#b35">Shao et al., 2024)</ref>, etc. Although these methods have achieved success on some datasets and models, they rely on simple heuristics to select training data. Without explicitly measuring the impact of the selected data on the model, these methods tend to produce sub-optimal pretraining results. To address this issue, some researchers <ref type="bibr" target="#b45">(Xia et al., 2024;</ref><ref type="bibr" target="#b47">Yu et al., 2024)</ref> start evaluating each data instance by assigning it a score that reflects its impact on the model. Frequently used scoring methods include the influence function <ref type="bibr" target="#b45">(Xia et al., 2024)</ref>, early loss <ref type="bibr" target="#b1">(Albalak et al., 2023)</ref>, and perplexity <ref type="bibr" target="#b4">(Chen et al., 2024)</ref>. Among these methods, the influence function consistently delivers state-of-the-art results by effectively approximating the impact of adding each instance to the training set. A higher score signifies a higher priority for selecting a data instance, and hence the top-k (or gumble top-k) instances with the highest scores are chosen <ref type="bibr" target="#b46">(Xie et al., 2023;</ref><ref type="bibr" target="#b44">Wettig et al., 2024;</ref><ref type="bibr" target="#b47">Yu et al., 2024)</ref>. However, the above methodologies have the following limitations.</p><p>Prohibitive Computation Cost. First, accurately calculating the influence score of one data instance is expensive, because it involves the computation of the Hessian matrix. However, in the LLM pre-training, the number of the candidate data instances is extremely large. It is thus prohibitively expensive to compute the scores for all of the candidates.</p><p>Lack of Diversity. Second, assume that all influence scores have been calculated, as shown in Figure <ref type="figure" target="#fig_2">1a</ref>. We can see that the top-k instances (e.g., some high-score instances in C 1 ) tend to be closely distributed in the feature space because the influence computation is closely related to the data features. That is, the training instances selected in this way are lack of diversity (e.g., other instances in C 3 with high influence are also worth selecting), while as confirmed by some studies <ref type="bibr" target="#b0">(Abbas et al., 2023;</ref><ref type="bibr" target="#b38">Tirumala et al., 2023)</ref>, diversifying training samples mitigates overfitting, thereby enhancing the generalizability of the model. Therefore, an effective data training selection method should take both the influence scores and the diversity into consideration.</p><p>We thus propose Quad, a scalabe and effective data selection approach, which successfully addressing above challenges, achieves state-of-the-art pretraining results. Initially, Quad organizes the given dataset into clusters where the data instances within each cluster are similar, and those in different clusters exhibit diversity. Hence, we can sample a data subset from a cluster to estimate the accurate average influence of the cluster, so as to represent the cluster quality w.r.t the model performance.</p><p>Next, leveraging the property of the attention-based Transformer architecture which is widely adopted by the LLMs, we design a novel method to accurately compute the influence of an instance on LLM pre-training. More specifically, rather than solely relying on the MLP layers to compute the influence <ref type="bibr" target="#b18">(Koh &amp; Liang, 2017;</ref><ref type="bibr" target="#b47">Yu et al., 2024;</ref><ref type="bibr" target="#b13">Grosse et al., 2023;</ref><ref type="bibr" target="#b10">Engstrom et al., 2024)</ref>, we incorporate the attention layers such that the influence computation considers more semantic information. In addition, given that calculating the Hessian matrix is time-consuming, particularly for attention layers with complex interactions, we incorporate the Kronecker product to approximate the Hessian matrix, thereby greatly expediting the computation. This successfully addresses the computation cost challenge.</p><p>To improve diversity, we apply the Multi-Arm Bandit (MAB) technique, where each cluster is regarded as an arm of the MAB. Upon selecting an arm, we draw samples from the cluster to calculate influence scores. Subsequently, Quad iteratively samples from clusters, taking into account both the influence score and data diversity, e.g., whether the cluster has already been sampled. Moreover, because this sampling strategy effectively avoids calculating the influence of all instances, it further speeds up the data selection process.</p><p>We summarize our main contributions as follows:</p><p>• To balance the quality and diversity, we incorporate an iterative MAB solution to first cluster the data instances and select data instances from these clusters.</p><p>• We propose a novel method to compute the influence function in attention-based Transformer architecture, so as to precisely measure the data quality in LLM pre-training.</p><p>• Experiments on the widely-used dataset Slimpajama and 9 popular downstream tasks demonstrate that Quad significantly outperforms state-of-art data selection methods by 1.39% in zero-shot accuracy, also with low computation resources consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Rule-based Methods. Initially, researchers often relied on intuition to design hand-crafted heuristics <ref type="bibr" target="#b36">(Soldaini et al., 2024)</ref> and <ref type="bibr" target="#b27">(Penedo et al., 2023)</ref>, aiming to improve data quality. Deduplication is another typical approach for selecting pretraining data, such as <ref type="bibr" target="#b27">(Penedo et al., 2023)</ref> and SemDedup <ref type="bibr" target="#b0">(Abbas et al., 2023)</ref> which use keyword-based and semantic deduplication, respectively. Additionally, certain approaches employ n-gram similarity <ref type="bibr" target="#b12">(Gao et al., 2020;</ref><ref type="bibr" target="#b46">Xie et al., 2023)</ref> to assist in choosing corpora that is semantically aligned with the validation set data. Although these methods effectively filter out noise and redundant data from web sources, they rely on simple heuristics and cannot be well generalized.</p><p>LLM As a Selector. Although large models such as GPT-4 can effectively assess data quality due to their semantic comprehension capacity, the metrics utilized to rate data (e.g., writing style, educational value etc.) heavily rely on human intuition <ref type="bibr" target="#b44">(Wettig et al., 2024;</ref><ref type="bibr" target="#b28">Penedo et al., 2024;</ref><ref type="bibr" target="#b49">Zhang et al., 2024;</ref><ref type="bibr" target="#b14">Gunasekar et al., 2023)</ref>. This often leads to a mismatch between the selected data and the data desired by the model.</p><p>Surrogate Models. DeepSeekMath <ref type="bibr" target="#b35">(Shao et al., 2024)</ref> proposes an active learning strategy to train a web data classifier. Similarly, in MATES <ref type="bibr" target="#b47">(Yu et al., 2024)</ref>, a surrogate model was developed to estimate the influence scores of the data instances. RHO-1 <ref type="bibr" target="#b19">(Lin et al., 2024</ref>) used a surrogate model trained with high-quality data to perform token-level data filtering. However, these surrogate models are not trained over large-scale data, and thus their generalizatio ability is limited.</p><p>Perplexity serves as a metric for selecting high-probability data in a language model. In <ref type="bibr" target="#b4">(Chen et al., 2024;</ref><ref type="bibr" target="#b21">Marion et al., 2023;</ref><ref type="bibr" target="#b25">Muennighoff et al., 2024;</ref><ref type="bibr" target="#b43">Wenzek et al., 2019)</ref>, perplexity (PPL) is utilized to filter data. As also discussed in Qurating <ref type="bibr" target="#b44">(Wettig et al., 2024)</ref>, we observe that this method often incorporates a significant amount of simple and redundant data, because they are easy for the model to predict.</p><p>Influence Function <ref type="bibr" target="#b13">(Grosse et al., 2023;</ref><ref type="bibr" target="#b5">Choe et al., 2024)</ref> demonstrates that influence function can reveal the impact of training data on the performance of large models. Consequently, LESS <ref type="bibr" target="#b45">(Xia et al., 2024)</ref> and MATES <ref type="bibr" target="#b47">(Yu et al., 2024)</ref> utilize influence functions for selecting data during the SFT and pretraining phases, respectively. For large models, computing influence functions is computationally expensive. <ref type="bibr" target="#b13">(Grosse et al., 2023)</ref>. Hence, given the large amount of data handled during pretraining, directly using LESS <ref type="bibr" target="#b45">(Xia et al., 2024)</ref> for data selection at this stage poses considerable difficulties. To overcome this, MATES <ref type="bibr" target="#b47">(Yu et al., 2024</ref>) employs a proxy model to approximate the influence score across the full dataset. However, the limited capacity of this small proxy model hinders its ability to provide accurate influence scores. Furthermore, relying on the influence to select data solely often leads to a lack of diversity in the chosen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>First, we present our problem statement in §3.1. Next, in §3.2, we explain how our method achieves the balance between quality and diversity in selecting pretraining data. Finally, in §3.3, we introduce how we compute the influence with attention layers more accurately and efficiently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BALANCE BETWEEN QUALITY AND DIVERSITY</head><p>As shown in Figure <ref type="figure" target="#fig_2">1b</ref>, there are significant variations in the distribution of influence scores among different clusters. To achieve the quality-diversity balance, it is necessary to know the precise average influence score for instances in each cluster. However, Figure <ref type="figure" target="#fig_2">1b</ref> shows that the influence scores for each cluster also fluctuate around the average, indicating a certain level of uncertainty.</p><p>Estimating the average with a small sample size will not be accurate enough, while taking a large number of samples to compute the average influence is costly.</p><p>Hence, we propose to use the MAB <ref type="bibr" target="#b41">(Vermorel &amp; Mohri, 2005)</ref> technique that is capable of making decisions iteratively under uncertainty. At a high level, each cluster represents an arm of the MAB, and during each iteration, a cluster with a high average influence score tends to be selected and sampled. We will then compute the influence of data instances to update the average. Moreover, clusters that are not visited often present significant opportunities for sampling to balance the diversity.</p><p>The overall process of this approach is illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Specifically, our method can be divided into the following four steps: First, we sample the top-k clusters with the highest cluster scores (denoted by CS) computed by MAB. Here, the cluster score is determined by both the influence score and the sample frequency. Then we calculate the influence scores for the samples in each cluster (Section 3.3). At this point, we select high scoring samples to be added for training and use their scores to update the cluster score for each cluster. Throughout the iterative process, the MAB algorithm focuses on frequently sampling high-quality clusters that have high influence scores, which also enhances the accuracy of their quality estimation (i.e., updating the average influence I i ). Simultaneously, it ensures diversity by also sampling less-visited clusters. Next, we discuss how to compute and update the cluster score in details.</p><p>Cluster Score (CS). The Upper Confidence Bound can effectively balance exploration (i.e., data diversity) and exploitation (i.e., data quality), so we use it as the cluster score to evaluate each cluster, as shown in Equation (1). Specifically, the cluster score is determined by the average influence score Īi and the exploration score</p><formula xml:id="formula_0">2 ln j T (Cj ) T (Ci)</formula><p>, where T (C i ) denotes the frequency of instances sampled from cluster C i , and j T (C j ) denotes the total times of samples taken from all clusters.</p><formula xml:id="formula_1">CS i = Īi + α 2 ln j T (C j ) T (C i ) (1)</formula><p>Update the cluster score. During each iteration, a subset of data B i is sampled from each cluster with a high cluster score (CS). The sum of their influence score I i can be used to denote the impact of the samples from the cluster C i on the model.</p><formula xml:id="formula_2">R(C i )+ = z∈Bi I θ (D r , z), T (C i )+ = 1 (2)</formula><p>where R(C j ) denotes the total reward accumulated by cluster C i over several iterations.Then the average influence score Īi for cluster C i can be represented as Īi = R(Ci) T (Ci) . As the sample size grows, Īi for each cluster C i steadily approaches the exact average influence of the cluster, which can be used to update the cluster scores for all clusters.</p><p>Data selection. During each iteration, we pick a small proportion(γ) of data instances from selected clusters. We also require that these instances have influence scores higher than the threshold τ , otherwise we will not select them, which are then added into the training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INFLUENCE CALCULATION WITH ATTENTION LAYERS</head><p>Instead of retraining the large model with each data sample z, the impact of z on the model M can be estimated by calculating the influence function for each instance. In this section, we extend the influence calculation to multi-head attention layers and provide acceleration techniques.</p><formula xml:id="formula_3">I θ (D r , z) = -∇L(θ, D r )(H + λI) -1 ∇L(θ, z)<label>(3)</label></formula><p>In the above equation, I θ (D r , z) denote the influence function of data z on model θ. ∇L(θ, D r ) and ∇L(θ, z) denote the gradient of reference dataset D r and data z, respectively. Since the training of the large model does not often fully converge, resulting in a non-invertible Hessian matrix H, a regularization term λI is introduced <ref type="bibr" target="#b2">(Bae et al., 2022)</ref>. Equation ( <ref type="formula" target="#formula_3">3</ref>) is typically divided into the following two stages to speed up the computation:</p><p>1. Approximate the multiplication of the gradient of the validation set ∇L(θ, D r ) and the inverse Hessian matrix H -1 using the inverse Hessian vector product (iHVP).</p><p>2. Compute the dot product between the iHVP and the gradient of each training data point ∇L(θ, z).</p><p>While this framework can accelerate the computation of the influence function, scaling it up to large language models (LLMs) with massive parameters is still expensive. Hence, K-FAC <ref type="bibr" target="#b22">(Martens &amp; Grosse, 2015;</ref><ref type="bibr" target="#b40">Ueno et al., 2020)</ref> can be used to accelerate the iHVP computation by using the Kronecker product to decompose the Hessian matrix.</p><p>The K-FAC approximate the parameters of different MLP layer θ 1 , θ 2 and θ 3 as independent. That's because, during the gradient computation and update process, there are usually only minimal direct dependencies between the gradients of different MLP layers. This is particularly evident during back propagation, where the weight updates for each MLP layer are primarily influenced by the parameters of that specific layer. Therefore, the influence function I θ1,θ2,θ3 (D r , z) in K-FAC method can be expressed as:</p><formula xml:id="formula_4">I θ1,θ2,θ3 (D r , z) = I θ1 (D r , z) + I θ2 (D r , z) + I θ3 (D r , z)<label>(4)</label></formula><p>In attention mechanisms, there exist complex connections between the Query, Key, and Value layers.</p><p>As the right-upper corner of Figure <ref type="figure">3</ref> shows, separately calculate the hessian matrix of Query, Key and Value layers, will miss massive information of Consequently, it is essential to consider the QKV layers as a unified layer θ qkv when computing the influence function. Therefore, the influence function I θatt (D r , z) can be expressed as:</p><formula xml:id="formula_5">I θatt (D r , z) = I θ qkv (D r , z) + I θo (D r , z)<label>(5)</label></formula><p>Figure <ref type="figure">3</ref>: Kronecker Product in calculating iHVP</p><p>Then, as the right-lower corner of Figure <ref type="figure">3</ref> shows, by decomposing the Hessian matrix into a kronecker product of smaller matrices and computing the inverse of each smaller matrix, we can avoid directly inverting the entire Hessian matrix, significantly reducing computational cost, and accelerate this process:</p><p>Forward propagation:</p><formula xml:id="formula_6">Attention(Q, K, V ) = sof tmax( QK T √ d k )V (6)</formula><p>Backward propagation: Dθ = vec(DW ) = δ ⊗ x (7) Here, ⊗ denotes the Kronecker product, and vec() represents the vectorization operation. Thus, the gradient of θ qkv can be written as:</p><formula xml:id="formula_7">Dθ qkv = vec(DW Q ) vec(DW K ) vec(DW V ) = δ q δ k δ v ⊗ x (8) Let δ qkv = δ q δ k δ v</formula><p>. Then, the Hessian matrix H qkv can be estimates by:</p><formula xml:id="formula_8">H qkv = E(Dθ qkv Dθ qkv T ) = E(δ qkv δ T qkv ⊗ x qkv x T qkv ) ≈ E(δ qkv δ T qkv ) ⊗ E(x qkv x T qkv ) = ∆ qkv ⊗ X qkv (9) Also, H o = ∆ o ⊗ X o .</formula><p>Thus, the iHVP of the attention layer can be estimated as follows:</p><formula xml:id="formula_9">H -1 att v att = H -1 qkv v qkv H -1 o v o = (∆ qkv ⊗ X qkv ) -1 v qkv (∆ o ⊗ X o ) -1 v o = (∆ -1 qkv ⊗ X -1 qkv )v qkv (∆ -1 o ⊗ X -1 o )v o = vec(∆ -1 qkv V qkv X -1 qkv ) vec(∆ -1 o V o X -1 o )<label>(10)</label></formula><p>where v att , v qkv , v o represent the gradient of reference dataset D r on parameters θ att , θ qkv , θ o , respectively. Thus, the influence score of attention layers can be written as: I θatt = -∇L(θ att , z)H -1 att v att . To avoid the excessive memory usage of validation set gradients, we apply the Johnson-Lindenstrauss Lemma to reduce the dimensionality of both the iHVP computation results and the training data gradients ∇L(θ, z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENT SETUP</head><p>Dataset Preparation. We use the entire 627B-token SlimPajama dataset as the candidate pool D c . In the clustering process, the BAAI/bge-large-en-v1.5 model is employed to generate embeddings for the input data, and approximately 600 million data points from the candidate pool D c are clustered into 10,000 groups using the k-means algorithm. We use LAMBADA <ref type="bibr" target="#b26">(Paperno et al., 2016)</ref> as our reference set D r , which is a widely used language modeling task and often serves as a validation benchmark for language model pretraining. <ref type="bibr" target="#b47">(Yu et al., 2024;</ref><ref type="bibr" target="#b46">Xie et al., 2023;</ref><ref type="bibr" target="#b17">Hoffmann et al., 2022)</ref>.</p><p>Experimental settings. We train a transformer-based decoder-only language model that contains 1.3B parameters, uses RoPE embeddings <ref type="bibr" target="#b37">(Su et al., 2023)</ref>, and has a maximum context window of 1024 tokens <ref type="bibr" target="#b39">(Touvron et al., 2023)</ref>. Following the setting of MATES <ref type="bibr" target="#b37">(Su et al., 2023)</ref>, 30B tokens out of the 627B are selected for training using Quad and compare with baselines. The learning rate is set to 5 × 10 -5 , the batch size is set to 4096, and the Adam optimizer is employed with hyperparameters β 1 = 0.9, β 2 = 0.95, ϵ = 10 -8 . As for Multi-Armed Badit, we set the α = 0.002 , sample proporation γ = 0.05 and the sample threshold τ as 0.0025.</p><p>Baselines. We compare our methods with several baselines. (1) Random samples data from the entire candidate dataset randomly. (2) Qurating uses the large language model to select data. (3) DSIR selects data instances that are similar to the LAMBADA dataset. ( <ref type="formula" target="#formula_4">4</ref>) PPL uses perplexitybased data selection, i.e., selecting data instances with the lowest perplexity scores. ( <ref type="formula" target="#formula_5">5</ref>) MATES trains a surrogate model to evaluate the influence of each data instance on the target model.</p><p>Evaluation datasets. To comprehensively evaluate the capabilities of pretrained models, we conduct experiments on various downstream tasks covering three significant categories:</p><p>General Knowledge: ARC-C, ARC-E <ref type="bibr" target="#b7">(Clark et al., 2018)</ref>, and SciQ <ref type="bibr" target="#b42">(Welbl et al., 2017)</ref>.</p><p>Commonsense Reasoning: HellaSwag <ref type="bibr" target="#b48">(Zellers et al., 2019)</ref>, SIQA <ref type="bibr" target="#b34">(Sap et al., 2019)</ref>, Wino-Grande <ref type="bibr" target="#b33">(Sakaguchi et al., 2021)</ref>, Logiqa <ref type="bibr" target="#b20">(Liu et al., 2020)</ref>.</p><p>Reading Comprehension: OpenbookQA <ref type="bibr" target="#b23">(Mihaylov et al., 2018)</ref>, and BoolQ <ref type="bibr" target="#b6">(Clark et al., 2019)</ref>.</p><p>Evaluations are conducted using the lm-evaluation-harness <ref type="bibr">(Gao et al., 2023)</ref> framework and the average accuracy (i.e., Overall Score) is reported for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS</head><p>Overall Performance. As demonstrated in Table <ref type="table" target="#tab_0">1</ref>, our method surpasses all the baseline methods in downstream tasks with zero-shot evaluation. To be specific, we can observe that on General Knowledge and Reading Comprehension tasks, Quad has the improvement of 1.75% and 1.98% respectively compared with Random. Quad outperforms DSIR and Semdedup because they use rule-based heuristics to select data without considering the model. Although PPL and MATES consider the model, they do not perform well because the former one always selects some simple and duplicated instances, and the surrogate model of the latter one is small and lacking of enough training data. Qurating generally performs the best among other baselines, but still worse than our approach, and it incorporates the highest FLOPS(1e19) because of the usage of LLMs for data selection. In terms of the FLOPs, we can observe that except the methods (i.e., DSIR, SemDeDup) that use simple heuristics, we consume minimal computation resources because our MAB solution samples from clusters without considering the entire candidate dataset like PPL, Qurating and MATES.</p><p>Effectiveness of MAB. This section evaluates the effectiveness of the MAB approach for data selection in contrast to the straightforward method of choosing the top-k clusters with the highest influence scores for model training. To be specific, we randomly select an equivalent number of data points from the top 150, 500, and 1000 clusters. Figure <ref type="figure">4a</ref> illustrates the trade-off between data quality and diversity: clusters with higher influence scores do not necessarily enhance model performance on downstream evaluation sets because of their lack of diversity. Hence, the multiarmed bandit method can more effectively capture the trade-off between quality and diversity across clusters, resulting in superior performance on downstream evaluation sets, as opposed to merely choosing the top-k clusters. Effectiveness of Influence Calculation. This experiment studies the effectiveness of our influence calculation method. In this section, we select the top 500 clusters with the highest scores using three methods: (1) no-Hessian (i.e., computing the gradient similarity between training data and reference data <ref type="bibr" target="#b29">(Pruthi et al., 2020)</ref>) without considering the Hessian matrix; (2) MLP(i.e., calculating influence function on MLP layers) and ( <ref type="formula" target="#formula_3">3</ref>) Ours (i.e., calculating influence function on both MLP and attention layers). From each cluster, we uniformly sample data to train the large model. As shown in Figure <ref type="figure">4b</ref>, our solution (MLP+Attention) performs better than MLP because the attention layer considers more semantics. no-Hessian performs the worst because it does not precisely capture the impact of training data instances on the model without the Hessian matrix.</p><p>Also, we conduct experiments to verify the relationship between the Query, Key, Value matrices, which is shown in Figure <ref type="figure">4c</ref>. In this experiment, we compare the Pearson correlation coefficients between the following three methods and the baseline approach, which computes the influence score for the attention layer without any acceleration. (1) No-Hessian(i.e., computing the gradient similarity between training data and reference data) without considering the Hessian matrix; (2) Independent (i.e., calculating the Hessian matrices of the query, key, and value layers independently) and ( <ref type="formula" target="#formula_3">3</ref>) Ours (i.e., calculating the Hessian matrices of the query, key, and value layers as a whole).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ABLATION STUDY</head><p>This group of experiments performs ablation studies on the hyperparameters of Quad. Figure <ref type="figure">4d</ref> and Figure <ref type="figure" target="#fig_4">5</ref> show the impact of sample threshold and α respectively.</p><p>Sampling Threshold of Influence (τ ). Setting the threshold too high or low will both degrade the model performance. This is because the selected data instances tend to exist in few clusters with high influence scores, resulting in poor diversity. In contrast, when the threshold is set too low, the sampled instances will be from many clusters with low influence scores, which also degrades the model performance.</p><p>α for Quality-Diversity Balance. Our approach employs α to balance the diversity and quality in the MAB framework. When α is small, we tend to focus on the several clusters with high influence scores without considering diversity much, so the MAB framework is likely to get stuck in a local optimum. For example, this results in the model enhancing its performance in specific areas (such as Common Sense Reasoning in Figure <ref type="figure" target="#fig_4">5</ref> when α = 1.5e -3), while the performance in other areas (i.e., General Knowledge and Reading Comprehension) is not good enough. Thus the overall score is not the optimal. However, when α is large, the MAB framework focuses too much on diversity without selecting enough high-quality data, which ultimately results in a limited improvement of model performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper presents Quad, a method designed to balance both the diversity and quality of data in pretraining data selection. Quad employs the influence function to identify data that benefits the model. First, we group the data into clusters and use a subset from each to represent the influence of the entire cluster. Given that influence scores within a cluster display some uncertainty, we view each cluster as an arm in an MAB framework. This method conducts samplings from high-quality clusters, allowing for more precise estimation of their influence scores and meanwhile maintaining the diversity. Moreover, we extend the influence function to attention layers and enhance the calculation efficiency to better measure the impact of data within each cluster on the model.    <ref type="bibr">1,345,423,360(1.3B)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>Figure 1: Distribution of influence scores of some sampled data instances.</figDesc><graphic coords="2,335.43,81.87,139.14,175.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Quad</figDesc><graphic coords="4,108.00,258.61,395.93,142.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :if</head><label>1</label><figDesc>Quad Algorithm Input: Candidate data pool D c , reference set D r , the model θ Output: Selected data D b 1 C = Cluster(D c ); 2 while do 3 C top k = top-k clusters with the highest Cluster Score(CS) ; 4 B top k = mini-batchs sampled from C top k 5 for C i in C top k do 6 R(C j ) += z∈Bi I θ (D r , z), T (C j ) += 1 ; Īi &gt; threshold then D b + = γC i ; 11 end 12 CS i = Īi + α 2 ln j T (Cj ) T (Ci) ; 13 end 14 return D b ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>Figure 4: (a) shows the effectiveness of the MAB method; (b) shows the accuracy of calculating the influence function on MLP and attention layers; (c) shows the correlation between Query, Key, Value layers impact a lot on the accuracy of influence calculation; (d) shows the model performance of varying sample threshold τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: α for Quality-Diversity Balance.</figDesc><graphic coords="10,108.00,81.86,396.00,118.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="6,108.00,517.03,395.96,135.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overall Performance</figDesc><table><row><cell></cell><cell cols="2">General</cell><cell cols="2">Commonsense</cell><cell cols="2">Reading</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Selection Method</cell><cell cols="2">Knowledge</cell><cell cols="2">Reasoning</cell><cell cols="2">Comprehension</cell><cell cols="2">Overall</cell><cell>FLOPs</cell></row><row><cell></cell><cell cols="2">(3 tasks)</cell><cell cols="2">(4 tasks)</cell><cell cols="2">(2 tasks)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Random</cell><cell cols="2">50.33</cell><cell cols="2">36.19</cell><cell cols="2">39.09</cell><cell cols="2">41.55</cell><cell>7.66</cell></row><row><cell>DSIR</cell><cell>50.37</cell><cell>↑0.04</cell><cell>34.01</cell><cell>↓2.18</cell><cell>38.80</cell><cell>↓1.29</cell><cell>40.53</cell><cell>↓1.02</cell><cell>7.66</cell></row><row><cell>PPL</cell><cell>48.71</cell><cell>↓1.62</cell><cell>37.72</cell><cell>↑1.53</cell><cell>38.57</cell><cell>↓0.52</cell><cell>41.57</cell><cell>↑0.02</cell><cell>9.51</cell></row><row><cell>Semdedup</cell><cell>50.99</cell><cell>↑0.66</cell><cell>36.11</cell><cell>↓0.08</cell><cell>39.44</cell><cell>↑0.35</cell><cell>41.81</cell><cell>↑0.26</cell><cell>8.11</cell></row><row><cell>Qurating</cell><cell>51.56</cell><cell>↑1.23</cell><cell>35.93</cell><cell>↓0.26</cell><cell>39.70</cell><cell>↑0.61</cell><cell>42.01</cell><cell>↑0.46</cell><cell>13.66</cell></row><row><cell>MATES</cell><cell>50.45</cell><cell>↑0.12</cell><cell>36.06</cell><cell>↓0.13</cell><cell>39.83</cell><cell>↑0.74</cell><cell>41.93</cell><cell>↑0.38</cell><cell>9.81</cell></row><row><cell>Quad(ours)</cell><cell>52.08</cell><cell>↑1.75</cell><cell>37.03</cell><cell>↑0.84</cell><cell>41.07</cell><cell>↑1.98</cell><cell>42.94</cell><cell>↑1.39</cell><cell>9.15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance Comparison</figDesc><table><row><cell>Selection Method</cell><cell cols="2">General Knowledge</cell><cell></cell><cell></cell><cell cols="3">Commonsense Reasoning</cell><cell></cell><cell>Reading Comprehension</cell><cell>Overall</cell></row><row><cell></cell><cell>arc-e arc-c</cell><cell>sciq</cell><cell>avg</cell><cell cols="2">logiqa hellaswag</cell><cell>siqa</cell><cell>winogrande</cell><cell>avg</cell><cell>openbookqa boolq</cell><cell>avg</cell></row><row><cell>Random</cell><cell cols="4">50.27 20.31 80.40 50.33 21.20</cell><cell>34.11</cell><cell>38.49</cell><cell>50.99</cell><cell>36.20</cell><cell>17.60</cell><cell>60.58 39.09</cell><cell>41.55</cell></row><row><cell>Semdedup</cell><cell cols="4">51.35 20.73 80.90 50.99 19.05</cell><cell>34.56</cell><cell>39.30</cell><cell>51.54</cell><cell>36.11</cell><cell>18.80</cell><cell>60.09 39.45</cell><cell>41.81</cell></row><row><cell>MATES</cell><cell cols="4">50.00 21.25 80.10 50.45 21.66</cell><cell>33.90</cell><cell>38.69</cell><cell>52.17</cell><cell>36.61</cell><cell>19.00</cell><cell>60.67 39.84</cell><cell>41.94</cell></row><row><cell>PPL</cell><cell cols="4">45.41 20.82 79.90 48.71 20.43</cell><cell>35.92</cell><cell>39.92</cell><cell>54.62</cell><cell>37.72</cell><cell>18.80</cell><cell>58.35 38.58</cell><cell>41.57</cell></row><row><cell>DSIR</cell><cell cols="4">49.28 20.14 81.70 50.37 21.20</cell><cell>30.89</cell><cell>35.98</cell><cell>47.99</cell><cell>34.02</cell><cell>16.20</cell><cell>61.41 38.81</cell><cell>40.53</cell></row><row><cell>Qurating</cell><cell cols="4">52.10 23.29 79.80 51.56 20.43</cell><cell>33.57</cell><cell>39.05</cell><cell>50.67</cell><cell>35.93</cell><cell>18.00</cell><cell>61.41 39.71</cell><cell>42.04</cell></row><row><cell>Quad(ours)</cell><cell cols="4">52.27 21.76 82.20 52.08 22.89</cell><cell>34.41</cell><cell>38.74</cell><cell>52.09</cell><cell>37.03</cell><cell>20.00</cell><cell>62.14 41.07</cell><cell>42.94</cell></row><row><cell>topk-cluster</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>top-150</cell><cell cols="4">48.61 20.90 79.00 49.50 23.66</cell><cell>34.51</cell><cell>39.00</cell><cell>51.78</cell><cell>37.24</cell><cell>19.20</cell><cell>61.74 40.47</cell><cell>42.04</cell></row><row><cell>top-500</cell><cell cols="4">51.05 21.25 79.70 50.67 22.73</cell><cell>34.40</cell><cell>39.20</cell><cell>52.41</cell><cell>37.19</cell><cell>18.80</cell><cell>62.76 40.78</cell><cell>42.48</cell></row><row><cell>top-1000</cell><cell cols="4">49.96 20.99 80.40 50.45 21.97</cell><cell>34.00</cell><cell>38.74</cell><cell>50.2</cell><cell>36.23</cell><cell>18.20</cell><cell>60.61 39.41</cell><cell>41.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study of Threshold τ</figDesc><table><row><cell>Threshold</cell><cell cols="2">General Knowledge</cell><cell></cell><cell></cell><cell cols="3">Commonsense Reasoning</cell><cell></cell><cell>Reading Comprehension</cell><cell>Overall</cell></row><row><cell></cell><cell>arc-e arc-c</cell><cell>sciq</cell><cell>avg</cell><cell cols="2">logiqa hellaswag</cell><cell>siqa</cell><cell>winogrande</cell><cell>avg</cell><cell>openbookqa boolq</cell><cell>avg</cell></row><row><cell>0.0015</cell><cell cols="4">51.26 21.16 80.20 50.87 21.51</cell><cell>33.92</cell><cell>39.00</cell><cell>51.07</cell><cell>36.38</cell><cell>19.60</cell><cell>61.74 40.67</cell><cell>42.16</cell></row><row><cell>0.0020</cell><cell cols="4">52.23 22.27 80.70 51.73 22.89</cell><cell>34.77</cell><cell>38.33</cell><cell>50.20</cell><cell>36.55</cell><cell>19.20</cell><cell>61.50 40.35</cell><cell>42.45</cell></row><row><cell>0.0025</cell><cell cols="4">52.27 21.76 82.20 52.08 22.89</cell><cell>34.41</cell><cell>38.74</cell><cell>52.09</cell><cell>37.03</cell><cell>20.00</cell><cell>62.14 41.07</cell><cell>42.94</cell></row><row><cell>0.0030</cell><cell cols="4">50.25 19.62 80.80 50.22 22.27</cell><cell>33.96</cell><cell>38.96</cell><cell>53.28</cell><cell>37.12</cell><cell>20.60</cell><cell>59.20 39.90</cell><cell>42.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study of α</figDesc><table><row><cell>Alpha</cell><cell cols="2">General Knowledge</cell><cell></cell><cell></cell><cell cols="3">Commonsense Reasoning</cell><cell></cell><cell>Reading Comprehension</cell><cell>Overall</cell></row><row><cell></cell><cell>arc-e arc-c</cell><cell>sciq</cell><cell>avg</cell><cell cols="2">logiqa hellaswag</cell><cell>siqa</cell><cell>winogrande</cell><cell>avg</cell><cell>openbookqa boolq</cell><cell>avg</cell></row><row><cell cols="5">0.0015 50.55 20.73 79.70 50.33 23.35</cell><cell>34.60</cell><cell>40.58</cell><cell>52.25</cell><cell>37.70</cell><cell>19.60</cell><cell>60.06 39.83</cell><cell>42.38</cell></row><row><cell cols="5">0.0020 52.27 21.76 82.20 52.08 22.89</cell><cell>34.41</cell><cell>38.74</cell><cell>52.09</cell><cell>37.03</cell><cell>20.00</cell><cell>62.14 41.07</cell><cell>42.94</cell></row><row><cell cols="5">0.0025 51.64 22.10 78.30 50.68 21.81</cell><cell>34.76</cell><cell>38.02</cell><cell>52.57</cell><cell>36.79</cell><cell>19.80</cell><cell>62.05 40.93</cell><cell>42.34</cell></row><row><cell cols="5">0.0030 50.63 21.93 78.70 50.42 22.12</cell><cell>34.64</cell><cell>39.15</cell><cell>51.14</cell><cell>36.76</cell><cell>18.60</cell><cell>61.71 40.16</cell><cell>42.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Effectiveness of Influence Calculation</figDesc><table><row><cell>Method</cell><cell cols="2">General Knowledge</cell><cell></cell><cell></cell><cell cols="3">Commonsense Reasoning</cell><cell></cell><cell>Reading Comprehension</cell><cell>Overall</cell></row><row><cell></cell><cell>arc-e arc-c</cell><cell>sciq</cell><cell>avg</cell><cell cols="2">logiqa hellaswag</cell><cell>siqa</cell><cell>winogrande</cell><cell>avg</cell><cell>openbookqa boolq</cell><cell>avg</cell></row><row><cell>Random</cell><cell cols="4">50.27 20.31 80.40 50.33 21.20</cell><cell>34.11</cell><cell>38.49</cell><cell>50.99</cell><cell>36.20</cell><cell>17.60</cell><cell>60.58 39.09</cell><cell>41.55</cell></row><row><cell cols="5">No-Hessian 49.03 20.99 80.50 50.20 22.58</cell><cell>33.40</cell><cell>38.89</cell><cell>52.41</cell><cell>36.82</cell><cell>19.20</cell><cell>61.50 40.35</cell><cell>42.06</cell></row><row><cell>MLP</cell><cell cols="4">50.63 21.50 78.90 50.34 22.89</cell><cell>33.32</cell><cell>38.74</cell><cell>52.57</cell><cell>36.88</cell><cell>19.60</cell><cell>61.77 40.69</cell><cell>42.21</cell></row><row><cell>Ours</cell><cell cols="4">51.05 21.25 79.70 50.67 22.73</cell><cell>34.40</cell><cell>39.20</cell><cell>52.41</cell><cell>37.19</cell><cell>18.80</cell><cell>62.76 40.78</cell><cell>42.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Table 6: Model Architecture</cell><cell></cell></row><row><cell></cell><cell cols="4">Hyperparameter</cell><cell></cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell></cell><cell cols="4">Vocabulary Size</cell><cell></cell><cell></cell><cell>32,000</cell><cell></cell></row><row><cell></cell><cell cols="3">MLP Ratio</cell><cell></cell><cell></cell><cell></cell><cell>8/3</cell><cell></cell></row><row><cell></cell><cell cols="4">Hidden Dimension Size</cell><cell></cell><cell></cell><cell>2048</cell><cell></cell></row><row><cell></cell><cell cols="4">Number of Layers</cell><cell></cell><cell></cell><cell>24</cell><cell></cell></row><row><cell></cell><cell cols="5">Number of Attention Heads</cell><cell></cell><cell>16</cell><cell></cell></row><row><cell></cell><cell cols="6">Number of KV Attention Heads</cell><cell>16</cell><cell></cell></row><row><cell></cell><cell cols="3">RoPE Base</cell><cell></cell><cell></cell><cell></cell><cell>10,000</cell><cell></cell></row><row><cell></cell><cell cols="7">Maximum Context Window Length 1024</cell><cell></cell></row><row><cell></cell><cell cols="4">Number of Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semdedup: Dataefficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">Amro</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dániel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient online data mixing for language model pre-training</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">If influence functions are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alston</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17953" to="17967" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><surname>Tom B Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards effective and efficient continual pre-training of large language models</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.18743</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">What is your data worth to gpt? llm-scale data valuation with influence functions</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Keun Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwijeen</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngseog</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithya</forename><surname>Pratapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.13954</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Boolq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10044</idno>
		<title level="m">Exploring the surprising difficulty of natural yes/no questions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glam: Efficient scaling of language models with mixture-of-experts</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5547" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12926</idno>
		<title level="m">Dsdm: Model-aware dataset selection with datamodels</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><surname>Le Noac'h</surname></persName>
		</author>
		<ptr target="https://zenodo.org/records/10256836" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Studying large language model generalization with influence functions</title>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Tajdini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03296</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11644</idno>
		<title level="m">Textbooks are all you need</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A survey on large language models: Applications, challenges, limitations, and practical usage</title>
		<author>
			<persName><forename type="first">Rizwan</forename><surname>Muhammad Usman Hadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anas</forename><surname>Irfan</surname></persName>
		</author>
		<author>
			<persName><surname>Zafar ; Naveed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyedali</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Mirjalili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Authorea Preprints</note>
	<note>Muhammad Bilal Shaikh</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical analysis of compute-optimal large language model training</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="30016" to="30030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rho-1: Not all tokens are what you need</title>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.07965</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08124</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">When less is more: Investigating data pruning for pretraining llms at scale</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><surname>Üstün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiza</forename><surname>Pozzobon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.04564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2408" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narjes</forename><surname>Nikzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meysam</forename><surname>Chenaghlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Amatriain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.06196</idno>
		<title level="m">Large language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06031</idno>
		<title level="m">Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad discourse context</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.01116</idno>
		<title level="m">The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating training data influence by tracing gradient descent</title>
		<author>
			<persName><forename type="first">Garima</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">19920-19930, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Noveen</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09668</idno>
		<title level="m">How to train data-efficient llms</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Social iqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03300</idno>
		<title level="m">Deepseekmath: Pushing the limits of mathematical reasoning in open language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roformer</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom</idno>
		<ptr target="https://doi.org/10.1016/j.neucom" />
		<imprint>
			<date type="published" when="2021">2021. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving llm pretraining via document de-duplication and diversification</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="53983" to="53995" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rich information is affordable: A systematic performance analysis of second-order optimization using k-fac</title>
		<author>
			<persName><forename type="first">Yuichiro</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rio</forename><surname>Yokota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2145" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-armed bandit algorithms and empirical evaluation</title>
		<author>
			<persName><forename type="first">Joannes</forename><surname>Vermorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="437" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><surname>Ccnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Extracting high quality monolingual datasets from web crawl data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Qurating: Selecting high-quality data for training language models</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wettig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aatmik</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumya</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=GLGYYqPwjy" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Less: Selecting influential data for targeted instruction tuning</title>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadhika</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data selection for language models via importance resampling</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="34201" to="34227" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mates: Model-aware data selection for efficient pretraining with data influence models</title>
		<author>
			<persName><forename type="first">Zichun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spandan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.06046</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Autonomous data selection with language models for mathematical texts</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
