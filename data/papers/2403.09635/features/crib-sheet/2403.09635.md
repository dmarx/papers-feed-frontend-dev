- **Unified Signal Propagation Theory**: Develops a framework to analyze signal propagation in transformers, addressing issues like vanishing/exploding gradients and rank collapse.

- **Key Formulae**: Derives closed-form expressions for first and second-order moments (mean and variance) of outputs and gradients across transformer components (Embeddings, FFN, LayerNorm, etc.).

- **DeepScaleLM (DSLM)**: A novel initialization and scaling scheme that conserves output/gradient moments, enabling the training of very deep models (up to 1000 layers) without instability.

- **Gradient Behavior**:
  - **Pre-LN Transformers**: Forward output increases linearly with depth; backward gradient increases hyperbolically.
  - **Post-LN Transformers**: Forward output remains stable, but backward gradient can grow/decay exponentially with depth.

- **Numerical Validation**: Theoretical predictions verified with real/synthetic data, showing mean and median relative errors of 6.8% and 5.2%, respectively, across various model depths and dimensions.

- **Impact of QK Values**: Large QK values can destabilize training; correct initialization of Q/K is critical to prevent gradient explosion.

- **Rank Collapse Mitigation**: Dropout and scaling techniques can prevent rank collapse in token representations during training.

- **Applications**: Improvements in performance for Language Modeling, Speech Translation, and Image Classification tasks across various transformer architectures.

- **Theoretical Insights**: Highlights the importance of initialization and scaling in maintaining stability and performance in deep transformer models.

- **Figures and Tables**: Refer to Figures 1-3 for signal propagation illustrations and Tables 1-3 for moment transformation equations and theoretical comparisons.