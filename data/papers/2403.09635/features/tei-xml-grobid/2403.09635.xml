<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-18">18 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Akhil</forename><surname>Kedia</surname></persName>
							<email>&lt;akhil.kedia@samsung.com&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Intelligence Lab</orgName>
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abbas</forename><surname>Mohd</surname></persName>
						</author>
						<author>
							<persName><surname>Zaidi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Intelligence Lab</orgName>
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sushil</forename><surname>Khyalia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Sci- ence</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungho</forename><surname>Jung</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Intelligence Lab</orgName>
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harshith</forename><surname>Goka</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Intelligence Lab</orgName>
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haejun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Language Intelligence Lab</orgName>
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-18">18 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">2651A5E684FA61B3386CF7A370EA35FD</idno>
					<idno type="arXiv">arXiv:2403.09635v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose Deep-ScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper -our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer models are extremely popular across different domains of machine learning, however, deep transformers are plagued with issues of gradient explosion/vanishing <ref type="bibr">(Rae et al., 2021;</ref><ref type="bibr" target="#b98">Shleifer et al., 2021;</ref><ref type="bibr" target="#b100">Smith et al., 2022;</ref><ref type="bibr" target="#b102">Takase et al., 2022;</ref><ref type="bibr" target="#b100">Smith et al., 2022;</ref><ref type="bibr">Zhang et al., 2022c;</ref><ref type="bibr" target="#b104">Dehghani et al., 2023;</ref><ref type="bibr" target="#b14">Chowdhery et al., 2023;</ref><ref type="bibr" target="#b75">Molybog et al., 2023;</ref><ref type="bibr" target="#b115">Wortsman et al., 2024)</ref> and rank collapse <ref type="bibr" target="#b132">(Zhou et al., 2021;</ref><ref type="bibr" target="#b81">Noci et al., 2022)</ref> that adversely affect training stability. Proposed remedies include residual scaling, changing initialization or extra/modified layernorms <ref type="bibr">(Zhang et al., 2019a;</ref><ref type="bibr" target="#b116">Xiong et al., 2020;</ref><ref type="bibr" target="#b5">Bachlechner et al., 2021;</ref><ref type="bibr" target="#b112">Wang et al., 2024;</ref><ref type="bibr" target="#b104">Dehghani et al., 2023)</ref>.</p><p>Theoretical analysis via signal propagation and kernel methods has led to an improved understanding of these issues. Several works in the signal propagation domain <ref type="bibr" target="#b33">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b3">Arpit et al., 2016;</ref><ref type="bibr" target="#b117">Xu et al., 2019;</ref><ref type="bibr" target="#b28">Dong et al., 2021;</ref><ref type="bibr" target="#b18">Davis et al., 2021;</ref><ref type="bibr" target="#b113">Wang et al., 2022)</ref> have analysed the propagation of moments for some components of deep transformers, but often make simplifying assumptions of IID inputs, uncorrelated outputs, ignoring effect of query/key initialization, simplifying non-linearity, etc. We observed break down of each of these assumptions with real world data, adversely affecting model stability.</p><p>These issues highlight the need for a holistic theoretical framework that can fully explain signal propagation through transformer models with real data. In this work, we provide a framework to fully explain signal propagation through transformer models, by deriving closed-form expressions for the first and second-order moments (mean and variance) of the outputs and gradients of each of the components of the transformer model (Embeddings, FFN, ReLU/GeLU, LayerNorm, Dropout, Softmax, Single-Head Attention), Attention and FFN blocks, and through the entire model. Our derived equations are empirically verified within strict error bounds with real world data<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>We apply this framework to understand and mitigate instability issues with deep transformers -vanishing/exploding gradients, rank collapse, and instability caused by high QK values. To harness the improved complexity of deeper models <ref type="bibr" target="#b76">(Montúfar et al., 2014;</ref><ref type="bibr" target="#b85">Poole et al., 2016;</ref><ref type="bibr" target="#b88">Raghu et al., 2017)</ref>, we propose DeepScaleLM, a novel initialization scheme that augments residual/output scaling, and ensures the moments of outputs and gradients remain fully conserved throughout the model. DSLM enables us to break the depth barrier and train models with 100s of layers which outperform shallow models for BERT, GPT, Encoder-Decoder models across text, vision and speech modalities.  </p><formula xml:id="formula_0">)σ 2 x in -1 L 2 e (1-r d x in )σ 2 x in L 2 σ 2 gout - -</formula><formula xml:id="formula_1">(1 -p) 1 -p d 2 σ 2 o σ 2 v * σ 2 gout (1 -p) r l gout 1 -p FFN Block 2d 2 σ 2 w1 σ 2 w2 σ 2 xin (1 -p) (1 -p)( 1 π + r l xin 2 + ( 1 2 - 1 π )r l xin 2 ) σ 2 xout * σ 2 gout (1 -p)( 1 2 + sin -1 (r l x in ) π</formula><p>)r l gout 2 Moments of Transformer Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Moments of Transformer Components</head><p>Following an analysis similar to that of Xavier initialization <ref type="bibr" target="#b33">(Glorot &amp; Bengio, 2010)</ref>, we derive closed-form expressions for the mean and variance of the output and of the backpropagated gradient for all the components of the transformer model in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Here µ xin , σ 2 xin , µ xout , σ 2 xout are the mean and variance of the input/outputs, σ 2 gout , σ 2 gin are the variance of the gradient back-propagated to/from the component, and r l , r d are the correlations across sequence length and hidden dimension. p is the dropout probability, L sequence length, d in , d out input/output dimensions of Linear layer, σ 2 w , σ 2 wembd variances of the weights of the Linear layer and the Embeddings table. At the input side, r l xin originates from repeated tokens. For text, we estimate input correlation theoretically by assuming that input tokens follow Zipf <ref type="bibr" target="#b52">(Kingsley, 1935)</ref> distribution. Detailed proofs are provided in Appendix A, and all assumptions are summarized in Appendix L.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Moments of Transformer Blocks</head><p>Combining the expressions reported in Table <ref type="table" target="#tab_0">1</ref>, we derive closed-form expressions for the moment transformation during the forward and backward pass of the transformer Attention and FFN blocks. The Attention block refers to the Q, K, V projection, followed by Multi-Head Attention and Output-Projection Layer. The FFN block refers to the Linear layer followed by non-linearity (ReLU) and output Linear layer. Table <ref type="table" target="#tab_1">2</ref> provides our derived equations for these, where σ 2 v , σ 2 o , σ 2 w1 , σ 2 w2 are the variances for V weights, Output-Projection weights, and weights of FFN block Linear layers, and d is model the hidden size. These results show that considering correlation r l , dropout p and effects  of non-linearity are crucial for correctly modelling signal propagation through Transformer blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Moments of Entire Transformer Model</head><p>By repeatedly applying the expressions in Table <ref type="table" target="#tab_1">2</ref> for each layer, we calculate the propagation of moments of outputs and gradients through the entire transformer model. We do this for both Pre-LN style transformers, in which the skip connection bypasses the LayerNorm, and for Post-LN style transformers, in which the Layernorm is applied before the skip-connection. The method is fully detailed in Appendices E.1 and E.2. Figures 1, 2 and 3 provide the forward (left to right) and backward (right to left) signal propagation at initialization through the layers of a very deep 192-layer model with Xavier initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Numerical Validation of Theoretical Results</head><p>We verify the theoretical formulae of transformer components and blocks by running simulations with real/synthetic data, (detailed in Appendix D, code released). Even at 99 percentile, no error (other than SHA gradient σ 2 ) is larger than 10%, verifying our assumptions.</p><p>All our derivations are modality-agnostic. We verify our formulae for the entire transformer model using real textual MLM data, as shown in Figures 1, 2 and 3 (Reproducible using our released code), and using ImageNet data (as shown in Appendix H). Our formulae predict the observed gradient and forward/backward norms with remarkable accuracy, with mean and median relative errors of 6.8% and 5.2% respectively, and an R 2 of 0.998. We further verify that for model depths in range [1 -768], and model dimensions <ref type="bibr">[128 -6096]</ref>, the reported formulae are within 10% error, even across 768 layers of the transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Validity of Theoretical Predictions even after Training</head><p>Interestingly, our theoretical estimates hold approximately even after the models have been trained for a large number of steps. The model stays in the regime it is initialized with (as has also been shown in <ref type="bibr" target="#b62">Li &amp; Liang (2018)</ref>; <ref type="bibr">Arora et al. (2019a)</ref>; <ref type="bibr" target="#b58">Lee et al. (2019)</ref>; <ref type="bibr" target="#b49">Jesus et al. (2021)</ref>; <ref type="bibr">Arora et al. (2019b)</ref>; <ref type="bibr">Dettmers et al. (2023)</ref>), highlighting the importance of correct initialization. We analyze gradient explosion in a 30B parameter 64-layer PreLN model (after 150k training steps) and use our theory to predict the moments. Our hyperbolic estimation for the gradient explosion match closely with the observed moments as shown in Figure <ref type="figure">5</ref>. Similarly, forward growth in a 48-layer 1024-d PreLN model (after 100k training steps) matches our linear estimations (Figure <ref type="figure" target="#fig_2">6</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Explaining Variance Explosion in Transformer</head><p>Our approach theoretically proves the gradient vanishing/explosion (Table <ref type="table">3</ref>) for both Pre-LN and Post-LN transformers.</p><p>Exploding Output and Gradient in Pre-LN The forward output for Pre-LN transformer increases linearly with increasing depth N (Appendix E.1) since each layer's output is directly added to the skip connection, as seen in Figure <ref type="figure">1</ref>. For the backward pass, the gradient increases hyperbolically with increasing N , as seen in Figure <ref type="figure">2</ref>. Intuitively, this is because the gradient increases in every layer when a block's gradient is added to the skip connection, and the fractional increase in gradient is inversely proportional to the forward variance (which increases by N ) because of LayerNorm.</p><p>Vanishing/Exploding Gradient in Post-LN While layernorm solves the explosion in the forward pass of networks with residual connections <ref type="bibr" target="#b19">(De &amp; Smith, 2020)</ref>, it has the opposite impact on the gradient. As proved in Appendix E.2, the gradient in a Post-LN transformer grows/decays exponentially with the number of layers (Figure <ref type="figure" target="#fig_0">3</ref>).</p><p>Intuitively, the gradient is first transformed within the layer and then at the LayerNorm placed before the layer. The multiplicative factor is applied repeatedly, and causes gradient to vanish or explode exponentially, as was also observed in <ref type="bibr" target="#b94">Schoenholz et al. (2017)</ref>. This explains why Post-LN models are more challenging to train than Pre-LN for deeper networks <ref type="bibr" target="#b112">(Wang et al., 2024;</ref><ref type="bibr" target="#b98">Shleifer et al., 2021;</ref><ref type="bibr" target="#b102">Takase et al., 2022)</ref>.</p><p>Table <ref type="table">3</ref>. Comparison of maximum theoretical forward pass and backward pass growth in variance for the entire transformer model across methods (See Appendix E for proofs). Here β is the initial value of residual scaling for LayerScale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Post-LN Pre-LN Backward Sensitivity Forward Backward Sensitivity</p><formula xml:id="formula_2">Vanilla O(c ±N ) O(N ) O(N ) O(N ) O(logN ) DSInit O(1) O(N -1 ) O(1) O(1) O(N -1 ) LayerScale O(1) O(βN ) O(1) O(1) O(βN ) DeepNet O(1) O(N -0.5 ) - - - DSLM (Ours) O(1) O(1) 1 O(1) O(1)</formula><p>3.2 Explaining Higher Pruning of Deeper Layers <ref type="bibr" target="#b34">Gromov et al. (2024)</ref> found that LLMs such as Llama-2-70B <ref type="bibr" target="#b107">(Touvron et al., 2023)</ref> have minimal degradation in performance on Question Answering tasks until almost half the deeper layers are removed -suggesting that parameters in deeper layers are less effective in current LLMs. As we prove in Appendix E.1, the output of a Pre-LN transformer grows proportionally with depth (Figure <ref type="figure">1</ref>). For an 80-layer model like Llama-2, this implies the deeper layers will have a significantly reduced impact on changing the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Explaining Impact of Large QK Values</head><p>In <ref type="bibr" target="#b104">Dehghani et al. (2023)</ref>, the authors observed large QK values destabilized the training, and solved this empirically by adding a layernorm after attention scores. Unlike prior works <ref type="bibr" target="#b112">(Wang et al., 2024;</ref><ref type="bibr" target="#b81">Noci et al., 2022)</ref>, note from our derivations of softmax(Appendix A.7) that the backwards gradients from Q/K are exponentially related to their variance, highlighting the critical significance of correct initialization of Q/K. For example, by initializing them to only 2x the xavier values (all other initializations the same), backwards gradients exploded 10000x through a 192 layer model. Our theory explains these empirical observations, and sug-gests a simple initialization strategy to fix this problem, achieving the same variance on QK without the overhead of LayerNorm (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explaining and Mitigating Rank Collapse</head><p>Similar to our work, <ref type="bibr" target="#b81">Noci et al. (2022)</ref> also analyze moment propagation through the transformer, and observed the rank collapse of the token's representations at initialization after just a few layers, i.e., all the token representations became the same (r l x ≈ 1 after just 12 layers) at initialization. This has also been reported in <ref type="bibr" target="#b97">Shi et al. (2022)</ref> Our theory suggests a very simple solution -Dropout. As our closed form expressions show, both FFN block (because of ReLU) and dropout reduce the correlation (Figure <ref type="figure">7</ref>). With dropout, our method shows that such a rank collapse will not occur, and r l</p><p>x will quickly reach a stable value &lt; 1 (Appendix F), as verified empirically in Figure <ref type="figure" target="#fig_3">8</ref>.</p><p>Alternatively, scaling the block output by β = 1 √ N , or equivalently initializing the weights very small in Post-LN will also prevent rank collapse, even without Dropout. For Pre-LN, λ = 1 slows down increase in r l compared to λ 2 = 1 -1 N (but the same slowdown can be achieved by decreasing β). This highlights the criticality of correct initialization, dropout and scaling for deep transformer models, as well as the explainability power of our theoretical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">DeepScaleLM: Enabling Deep Transformers</head><p>We propose DeepScaleLM (DSLM), a new initialization / scaling scheme that alleviates the issues discussed above.</p><p>Residual/Skip-Connection Scaling Let σ 2 skip , σ 2 block , σ 2 model be the variances of the skip connection, the block, and the output of the final layer of the model, respectively. Let σ 2 skip = σ 2 block , and we scale them by scalars λ and β respectively. Then, as has been proven in numerous works (Appendix K.3), if λ 2 + β 2 = 1, this scaling will maintain the variance after addition of the residual.</p><p>Initialization However while ensuring σ 2 skip = σ 2 block (and equal to the variance of model input) has been done for ResNets (Appendix K.1), it is difficult to achieve theoretically for transformers. By leveraging the equations in Table 2, our theory provides us the tools to achieve this. We modify the initialization of the components of the transformer FFN and Attention blocks such that the variance of their output is 1, as further detailed in Appendix M -1. We set the variance of embedding weights as σ 2 e = 1-p numembd , where num embd is the number of embeddings types. As embeddings are followed by a dropout, this ensures the input variance to the model is 1.</p><p>2. We set σ 2 w2 = σ 2 w1 = 1 d * 1-p 2 , to make the output of the FFN block 1.</p><p>3. We iteratively calculate layer-by-layer r l xin , r l xout using expressions from Table <ref type="table" target="#tab_1">2</ref>, and calculate the initial variance of the attention block weights to make the output variance 1.</p><p>This initialization of transformer blocks, combined with the scaling of the skip connection and residual, and correct initialization of the embeddings, results σ 2 model = 1, irrespective of the number of layer N . This initialization also preserves the backward gradient, as proved for Pre-LN and Post-LN, in Appendices E.3 and E.4. Empirically, we show the backward gradient being preserved for both Pre-LN and Post-LN even across 192 layers at initialization (Figure <ref type="figure" target="#fig_1">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Choice of Scaling Parameters</head><p>While any choice of β will work at initialization, higher values of β, for example β 2 = 0.5 causes gradients to vanish (Figure <ref type="figure">9</ref>, Table <ref type="table" target="#tab_4">4</ref>). This is because covariance between residual and skip connection increases the forward variance, which causes normalization to decrease backward gradient <ref type="bibr" target="#b19">(De &amp; Smith, 2020)</ref>.</p><p>Similar to other prior works (Appendix K.3), we use</p><formula xml:id="formula_3">β 2 = k</formula><p>N in all our experiments, where k is some small constant. This enables us to bound the fall in gradient (Appendix E.3) for Pre-LN. For Post-LN, β 2 ≤ k N 2 is theoretically required to bound the gradient (Appendix E.6). In practice, with β 2 = 2 N , even with 768 layers, we empirically observed the final output variance from the model does not exceed 30, and all our models converge. We hence use β 2 = k N (Figure <ref type="figure">10</ref>), but a practitioner may choose β 2 = k N α , with α &gt; 1 if more stability is required at the expense of performance/"sensitivity" (Refer to discussion of relative strength in Section 4.6 and comparison to prior works in Section 4.5). While the above analysis assumes positive covariance (which we always observed experimentally), negative covariance follows a similar reasoning, and will cause gradient explosion instead. Preventing Rank Collapse For DSLM, applying block equations iteratively shows that r l x &lt; 1 -1 e 2 after N layers.</p><p>Simpler Initialization Another avenue to handle the covariance between residual and skip connection could be to set λ 2 + β 2 &lt; 1. We therefore also consider a simpler initialization method(Appendix M), in which we modify the initialization of attention value and output matrices to be the same as those of FFN block. This decreases the "effective" β of the attention block, but as the attention block has 2x fewer params than FFN, this change in weightage seems reasonable. As we show in Appendices E.5 and E.6 while variances are no longer unit at initialization, they are still bounded. This change does not impact performance significantly, as we show in Table <ref type="table" target="#tab_14">14</ref>. All further experiments in Section 4 used this simpler initialization.</p><p>Folding Scaling into Weights for Inference The scaling parameters introduced here can be fully absorbed into the model checkpoint weights by recursively scaling layernorm gain and output linear weights, hence and do not require any changes to vanilla transformers inference code.</p><p>DeepScaleLM enables training deeper-narrower models with 100s of layers, outperforming standard models across transformer variants, tasks and modalities.</p><p>4 DeepScaleLM Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Improvements on Encoder-only Models (BERT)</head><p>Implementation Details We test our method on the Masked Language Modelling task with the BERT <ref type="bibr" target="#b25">(Devlin et al., 2019)</ref> model. Pile-CC dataset <ref type="bibr" target="#b31">(Gao et al., 2021)</ref> was used to pre-train our model. We use k = 2 for β while keep-ing all the original hyper-parameters of BERT the same, except for learning rate (LR). We find that higher LR is needed for our deeper-narrower models (similar to <ref type="bibr" target="#b120">Yang et al. (2021)</ref>). Hence, we search for LR for all the models. The training steps were decided based on Chinchilla <ref type="bibr" target="#b44">(Hoffmann et al., 2022)</ref>, at 6.6B tokens.</p><p>Table 25 provides all hyper-parameter details. For DSLM, model output was down-scaled by √ d before being passed to the LM-head. We train different language models with the same number of parameters and compute -while increasing the depth (N ), we reduce the hidden dimension d keeping number of transformer parameters (N d<ref type="foot" target="#foot_1">foot_1</ref> ) constant. When changing from 12-layer 1024-d model to 192-layer 256-d model, compute negligibly increases by only 6.6% when keeping N d 2 constant (Table 23), while the number of parameters decreases by 5 -15% due to decreased embedding parameters. Evaluation Metrics Pre-training Perplexity (exponential of pre-training test-set loss) is often used to measure MLM pre-training performance (RoBERTa (Liu et al., 2019b), Megatron-LM (Shoeybi et al., 2019), Tay et al. (2023), or similar variants in Salazar et al. (2020); Lu et al. ( <ref type="formula">2023</ref>)), and is well-correlated with downstream performance <ref type="bibr" target="#b32">(Geiping &amp; Goldstein, 2023)</ref>. We use the perplexity as reported by Megatron-LM here. Calling this measure "perplexity" is a slight abuse of notation (as previous words which are masked are not available, and future words are). For downstream fine-tuning, we use accuracy while for Speech-to-Text translation, we use BLEU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-Training Improvements In</head><p>Table 4, we provide the results obtained on scaling model depth after applying DSLM to Post-LN. Post-LN models often diverge while scaling model depth. DSLM stabilizes the training of Post-LN models, and even a 768 layer Post-LN model (with 2300 Linear and 768 attention layers) converges.  Sustained Improvements after Longer Pre-training Due to compute limitations, our models were trained for Chinchilla optimal steps. To ensure reproducibility of our work (scripts provided in released code), and demonstrate sustained improvements for standard models, we trained the BERT-base model using public Wikipedia data for 64B tokens (30x chinchilla tokens). We train a 4x deeper, 10% smaller model using DSLM (N /d = 48 / 384). We finetune these models on the public RACE-M, RACE-H <ref type="bibr" target="#b56">(Lai et al., 2017)</ref>, MNLI <ref type="bibr" target="#b114">(Williams et al., 2018)</ref> and QQP 2 datasets. As shown in Table <ref type="table" target="#tab_6">6</ref>, our model provides better pretraining performance which is translated into downstream Question-Answering tasks' performance across all datasets. Downstream Low Rank Finetuning DSLM continues to outperform the baseline on finetuning for downstream tasks with Low Rank Adapters <ref type="bibr" target="#b45">(Hu et al., 2022)</ref>, as shown in Table <ref type="table" target="#tab_7">7</ref>. Following QLoRA <ref type="bibr">(Dettmers et al., 2023)</ref>, we apply LoRA on all linear modules, with r = 32, α = 16, and searched for LR. We applied DSLM to the decoder-only GPT model, trained for 8B tokens (slightly more than Chinchilla-optimal). Similar to BERT, increasing model depth by 4x with DSLM while keeping the parameters constant results in improved performance (Table <ref type="table" target="#tab_8">8</ref>). We apply DSLM on encoder/decoder style transformer for Speech-to-Text translation task. Applying our method to speech additionally requires handling the input embeddings. Instead of theoretical estimates as in the case of text inputs (Appendix A.1), the moments for speech embedding were replaced by the empirically observed values. This input variance and correlation was observed as 2.2 and 0.29.</p><p>The baseline was trained on the MuST-C (Di <ref type="bibr" target="#b26">Gangi et al., 2019)</ref> dataset using fairseq <ref type="bibr" target="#b83">(Ott et al., 2019)</ref>. Using DSLM, we successfully train 4x deeper models which outperforms the 18-layer (12-encoder, 6-decoder layers) baseline with 9% less parameters as seen in Table <ref type="table" target="#tab_9">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Improvements on Vision Modality</head><p>Similar to speech domain, applying our method to vision modality simply requires handling the input embedding (Ap-  <ref type="bibr" target="#b89">(Recht et al., 2019)</ref>, ImageNet-R <ref type="bibr" target="#b42">(Hendrycks et al., 2021)</ref> and ImageNet-Sketch <ref type="bibr" target="#b110">(Wang et al., 2019)</ref>.  DSInit <ref type="bibr">(Zhang et al., 2019a)</ref> 15.9 diverge ADMIN <ref type="bibr">(Liu et al., 2020a)</ref> diverge 25.2 SkipInit <ref type="bibr" target="#b19">(De &amp; Smith, 2020)</ref> 15.1 13.1 ReZero <ref type="bibr" target="#b5">(Bachlechner et al., 2021)</ref> diverge diverge LayerScale <ref type="bibr">(Touvron et al., 2021b)</ref> 13.2 14.4 µP-Tensor Programs V <ref type="bibr" target="#b120">(Yang et al., 2021)</ref> diverge diverge DeepNorm <ref type="bibr" target="#b112">(Wang et al., 2024)</ref> 14.4 13.4 <ref type="bibr" target="#b81">Noci et al. (2022)</ref> diverge diverge Bamboo <ref type="bibr" target="#b118">(Xue et al., 2023)</ref> 17.1 diverge Value-SkipInit <ref type="bibr" target="#b38">(He et al., 2023)</ref> 18.8 17.1 DeepScaleLM (ours) 12.9 11.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of DSLM</head><p>Model Quantization Similar to Unit Scaling <ref type="bibr" target="#b9">(Blake et al., 2023)</ref>, conserving unit activations and gradients from our method results in models which lose much less performance when quantized (via direct casting) to FP8 precision compared to original models. We apply 8-bit quantization to the 48-Layer 512-dim BERT baseline model and the model trained with DSLM. Table <ref type="table" target="#tab_12">12</ref> provides the performance corresponding to the full precision inference and FP8 inferences (corresponding to two different FP8 standards, E5M2 and E4M3). DSLM model can be compressed to 25% of the original size with significantly lower performance loss.    <ref type="bibr" target="#b119">(Yang &amp; Schoenholz, 2017)</ref> -having lower β (smaller k or higher α) will result in networks where observed issues (forward growth or gradient explosion/vanishing) are mitigated, but they may converge slowly/sub-optimally. <ref type="bibr" target="#b18">Davis et al. (2021)</ref> defines "sensitivity" as the variance of relative change in output for small perturbations in parameters, averaged across all parameters. If σ 2 skip = 1, sensitivity can be shown to be mean across layers of N * (1/σ 2 block ) = N * β 2 . Mean is not robust to outliers, and hence we suggest median may provide a more robust measure. For e.g., for vanilla pre-LN, <ref type="bibr" target="#b18">Davis et al. (2021)</ref>'s definition gives sensitivity as O(log(N )), whereas using median provides a more robust measure as O(1). But only the first N/10 layers have O(log(N )) sensitivity, and the last 9N/10 layers have O(1) sensitivity. We will use median in the discussion below.</p><p>In Appendix G, we show that the fall in gradient for both pre-LN and post-LN for β 2 = k/N α is O(e kN 1-α ). The sensitivity is hence kN 1-α . For DSLM, we chose α = 1, that is the sweet spot on the stability-expressivity curve where both the gradient fall bound and sensitivity expressions become independent of model depth. For higher values of α such as α = 2 (DS-Init) and, α = 1.5 (DeepNet), the gradient becomes stable using but the model expressivity reduces with depth, as shown in Table <ref type="table">3</ref>. Such models might not be able to extract better results when going deeper, as we indeed verify empirically in the comparison with prior works paragraph in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>For detailed discussion of prior works, refer to Appendix K.</p><p>Initialization Several works <ref type="bibr" target="#b33">(Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b39">He et al., 2015;</ref><ref type="bibr">Brock et al., 2021a;</ref><ref type="bibr" target="#b85">Poole et al., 2016;</ref><ref type="bibr" target="#b94">Schoenholz et al., 2017)</ref> improved the initialization of ResNets/ReLU networks. These works do not consider transformers, and are unable to handle Softmax/Attention. Others, such as ADMIN <ref type="bibr">(Liu et al., 2020a)</ref>, <ref type="bibr" target="#b74">Mishkin &amp; Matas (2016)</ref>; <ref type="bibr">Liu et al. (2020b)</ref> achieve unit variance for faster convergence by scaling the weights and/or outputs based on empirical profiling of a forward pass. <ref type="bibr" target="#b9">Blake et al. (2023)</ref> also tries to achieve this, but does not completely handle correlation and non-zero mean of ReLU. We demonstrate that this profiling is unnecessary, and can instead be done theoretically in our work.</p><p>Signal Propagation Signal propagation in Neural Networks <ref type="bibr" target="#b77">(Neal, 1995;</ref><ref type="bibr" target="#b57">LeCun et al., 1996)</ref> has a long history, such as for ResNets <ref type="bibr" target="#b39">(He et al., 2015;</ref><ref type="bibr" target="#b19">De &amp; Smith, 2020;</ref><ref type="bibr">Brock et al., 2021a;</ref><ref type="bibr" target="#b94">Schoenholz et al., 2017;</ref><ref type="bibr" target="#b43">Hoedt et al., 2022;</ref><ref type="bibr" target="#b55">Labatie et al., 2021;</ref><ref type="bibr" target="#b71">Marion et al., 2022;</ref><ref type="bibr" target="#b53">Klambauer et al., 2017;</ref><ref type="bibr" target="#b6">Balduzzi et al., 2017)</ref>, and for transformers in <ref type="bibr" target="#b117">(Xu et al., 2019;</ref><ref type="bibr" target="#b28">Dong et al., 2021;</ref><ref type="bibr" target="#b18">Davis et al., 2021;</ref><ref type="bibr" target="#b81">Noci et al., 2022;</ref><ref type="bibr" target="#b72">Martens et al., 2021;</ref><ref type="bibr" target="#b38">He et al., 2023;</ref><ref type="bibr" target="#b97">Shi et al., 2022;</ref><ref type="bibr" target="#b113">Wang et al., 2022)</ref>. Our work considers previously often neglected effects of dropout, input correlation, activation non-linearity, and QK initialization, providing closed forms with verifiable correctness of signal propagation. This allows us to constrain the output and gradient to almost exactly unit variance.</p><p>Moment Control &amp; Residual Scaling Bounded gradients have been shown to results in better/faster convergence <ref type="bibr" target="#b96">(Shen et al., 2020;</ref><ref type="bibr" target="#b124">Yu et al., 2017;</ref><ref type="bibr" target="#b122">You et al., 2017;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b102">Takase et al., 2022;</ref><ref type="bibr" target="#b98">Shleifer et al., 2021;</ref><ref type="bibr" target="#b36">Hayou et al., 2019)</ref>. Different scaling schemes for residual networks (λ for skip connections and β for residual output) have been explored by prior works, such as λ 2 +β 2 = 1 for ResNets <ref type="bibr" target="#b6">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b101">Szegedy et al., 2017;</ref><ref type="bibr" target="#b35">Hanin &amp; Rolnick, 2018;</ref><ref type="bibr" target="#b4">Arpit et al., 2019;</ref><ref type="bibr">Zhang et al., 2019b;</ref><ref type="bibr" target="#b43">Hoedt et al., 2022)</ref>. Learnable β ≈ 0 was used in SkipInit <ref type="bibr" target="#b19">(De &amp; Smith, 2020)</ref>, ReZero <ref type="bibr" target="#b5">(Bachlechner et al., 2021)</ref>, LayerScale <ref type="bibr">(Touvron et al., 2021b)</ref>, Value-SkipInit <ref type="bibr" target="#b38">(He et al., 2023)</ref>. Others proposed β 2 =O( 1 N ), where N is max/current layer was used in Arpit et al. (2019); Brock et al. (2021a); Marion et al. (2022); Zhang et al. (2022b); He et al. (2023); Noci et al. (2022); De &amp; Smith (2020); Liu et al. (2020a;b); Davis et al. (2021); Blake et al. (2023), while DSInit <ref type="bibr">(Zhang et al., 2019a)</ref>, T-Fixup <ref type="bibr">(Huang et al., 2020a)</ref>, DeepNorm <ref type="bibr" target="#b112">(Wang et al., 2024)</ref> used β 2 &lt;O( <ref type="formula" target="#formula_29">1</ref>N ). However, the optimal initialization/scaling can vary based on data/model characteristics <ref type="bibr">(Zhang et al., 2022b;</ref><ref type="bibr" target="#b71">Marion et al., 2022)</ref>. Our contribution goes beyond providing an optimal scaling schemeour theory enables informed choices about these initialization/scaling schemes based on their expressivity-trainability trade-off. Some works such as DeepNet, ADMIN show performance improvements by making the model deeper, but much larger. In this work, we explore a stricter setting of keeping transformer parameters and compute constant while making the model deeper.</p><p>Other Network modifications for Deep Networks Architectural modifications such as <ref type="bibr" target="#b125">Zhai et al. (2023)</ref>; <ref type="bibr" target="#b132">Zhou et al. (2021)</ref>; <ref type="bibr" target="#b98">Shleifer et al. (2021)</ref> can only stabilize the model later during training and not at initialization. They are orthogonal to our approach, and our equations can be easily extended to cover these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We theoretically derive closed forms for the growth of variances for forward and backward pass through individual transformer components as well as the entire transformer model. These formulae enable us to identify and solve the key reasons for vanishing/exploding gradients and rank collapse in very deep transformers. Via scaling and correct initialization, we also enable training very deep transformers with 1000 layers. Our experiments suggest that deeper transformers should be explored -using our method, models with 100s of layers outperform larger standard models across multiple modalities, tasks, and transformer variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, some which we feel must be specifically highlighted here. Using crawled web data for pre-training language models is questionable, something which society has yet to finalize its views on. Language modelling in particular suffers from hallucinations, and may be used for misinformation.</p><p>E.6.2 Backward Pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 F Rank Collapse and Correlation Analysis 73 G Discussion of Relative Strength 74 H Applying DeepscaleLM to Vision Transformers 74 I Compute 75 I.1 Theoretical compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 I.2 Wall Clock times . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 J Statistical Significance 76 J.1 Error Bars for Pre-Training Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 J.2 Statistical Significance for Fine-tuning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K Related Works 76 K.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K.2 Signal Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K.3 Moment Control &amp; Residual Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 K.4 Other Network modifications for Deep Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 L Discussion of Approximations and Assumptions 78 L.1 Illustrative Approximations of Full Formulae in Main Paper . . . . . . . . . . . . . . . . . . . . . . . . 78 L.2 Assumptions and Approximations in Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 M DeepScaleLM Pseudocode 79 N Hyper-parameters 80 O Notations 82</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Moment Propagation through Transformer Components</head><p>We provide detailed proofs of the closed-form expression for each of the transformer component -Linear layer, Dropout, ReLU, GeLU, LayerNorm, and Softmax.</p><p>For any component, input is represented as x in and x out is the output. The gradient flowing in into the component from the output side is represented as g out and the backpropagated gradient towards the input is g in . We switch from vector to matrix notation (X in , X out ) whenever needed. We assume that the input is distributed normally N (0, σ xin ). No assumptions are made regarding the covariance of the input -it is not assumed to be IID, and it may/may-not have covariance both along the sequence length and hidden dimension. Additional assumptions needed to derive the proofs for softmax and attention can be found in the respective proofs. A detailed list of terms/notations used in the proofs is provided at the end of this work in Appendix O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Embeddings</head><p>The BERT model's embedding component consists of 3 look up tables -token embeddings, position embeddings, and segment embeddings. For a given input token, each of these 3 embeddings are added before being passed to the transformer model. Other transformer models, such as decoder-only GPT lack some (eg. segment) of these, but the derivations remain similar. In the general case, these theoretical derivations can be replaced by the empirically observed moments of the inputs fed to the transformer model (as we did for Speech-to-Text translation). We derive formulae for each of these embedding types below.</p><p>Token Embeddings We do not assume the input embeddings to be IID. Repetition of same token introduces correlation across the sequence length. We assume that the input tokens have been sampled from a multinomial distribution. The words / token ids are distributed almost according to Zipf's law <ref type="bibr" target="#b52">(Kingsley, 1935)</ref>. Assuming we initialize all the embeddings with variance σ 2 w embd , the relevant statistics for word embeddings output x outwe are as follows</p><formula xml:id="formula_4">µ xout we = 0 σ 2 xout we = σ 2 wembd Cov l (x outwe ) = N i * (N i -1) L * (L -1) * σ 2 wembd r l (x outwe ) = N i * (N i -1) L * (L -1) Cov d (x outwe ) = 0 Assume ith word occurs N i times, it contributes Ni * (Ni-1) L * (L-1))</formula><p>to the covariance along sequence length. Similarly, we can calculate the correlation for segment-type embeddings output x outse . Zipf's law states that the probability for each token is inversely proportional to its rank. For the word with rank i, p i = c i , where c =</p><formula xml:id="formula_5">1 i 1 i = 1 γ+log(|V |)</formula><p>, where γ ≈ 0.58 is the Euler's constant.</p><p>For a sentence of length L, the token with probability p i is expected to occur p i .L times. Hence, for a given vocabulary size |V |, we can calculate the correlation as follows</p><formula xml:id="formula_6">r l (x outwe ) = N i * (N i -1) L * (L -1) = |V | i p i L * (p i L -1) L * (L -1) = i p 2 i * L -1 L -1 = i c 2 i 2 * L -1 L -1 ≈ Lπ 2 6.(γ+log(|V |)) 2 -1 L -1 ≈ π 2 6.log(|V |) 2 , assuming γ ≈ 0.58 &lt;&lt; log(|V |) ≈ 10.4, L &gt;&gt; 1</formula><p>Segment Type Embeddings Similarly, the segment type embeddings have two possible values denoting the sentence order. If first sentence has length x, we can consider this as a special case of the analysis performed above with two possible tokens, where N 1 = x and N 2 = Lx. Assuming x is distributed uniformly between 0 to L, Lx also has the same distribution. Hence,</p><formula xml:id="formula_7">r l (x outse , N 1 , N 2 ) = N 2 1 + N 2 2 -L L * (L -1)</formula><p>Taking expectation, we get</p><formula xml:id="formula_8">r l (x outse ) = 2 3 * L 2 -L L * (L -1) ≈ 2 3</formula><p>Position Embeddings Since learnt position embeddings are lookup tables with unique inputs, the correlation from position embeddings is 0.</p><p>Final Model Input Embeddings Each of the above embeddings are added before being passed to the transformer model. Since the variance is same for all embedding types, the final correlation is the average of the three. Hence:</p><formula xml:id="formula_9">r l (x out ) = 1 3 (r l (x outwe ) + r l (x outse )) = π 2 18 * log(|V |) 2 + 2 9</formula><p>For our case, |V | = 32000 and sequence length L = 256, the theoretically predicted correlation r l xin = 0.227 which is within 3% of the empirically observed correlation (0.221).</p><p>Hence, the final moments for the embedding output are</p><formula xml:id="formula_10">µ xout = 0 σ 2 xout = 3 * σ 2 wembd Cov l xout = ( π 2 18 * log(|V |) 2 + 2 9 )σ 2 xout Cov d xout = 0 A.2 Linear</formula><p>For linear layer with d in dimensional input x in , and d out dimensional output x out , we can define the forward pass mathematically as,</p><formula xml:id="formula_11">x out = x in W =⇒ x outj = din i=1 x ini W i,j</formula><p>Similarly, we define the backward pass as,</p><formula xml:id="formula_12">g in = g out W T =⇒ g inj = dout i=1 g outi W j,i</formula><p>For expectation of output we have,</p><formula xml:id="formula_13">E[x outj ] = E[ din i=1 x ini W i,j ] = din i=1 E[x ini W i,j ] = din i=1 E[x ini ]E[W i,j ] = µ xin µ w</formula><p>(As weights and input are independent of each other)</p><formula xml:id="formula_14">µ xout = 0 (∀j)</formula><p>To get variance of the output of forward pass we have,</p><formula xml:id="formula_15">Var(x outj ) = Var( din i=1 x ini W i,j )</formula><p>As the weights are initialized independently each term in summation is independent of each other</p><formula xml:id="formula_16">= din i=1 (Var(x ini W i,j )) = din i=1 ((σ 2 xin + µ 2 xin )(σ 2 w + µ 2 w ) -µ 2 xin µ 2 w )</formula><p>(As weights and input are independent of each other)</p><formula xml:id="formula_17">= din i=1 (σ 2 xin + µ 2 xin )σ 2 w Var(x outj ) = d in (σ 2 xin + µ 2 xin )σ 2 w (∀j) σ 2 xout = d in (σ 2 xin + µ 2 xin )σ 2 w</formula><p>If we have two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = x in W and y out = y in W. Then for any j we have</p><formula xml:id="formula_18">Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] σ 2 xout σ 2 xout = E[ din i=1 x ini W i,j din k=1 y in k W k,j ] σ 2 xout = E[ din i=1 x ini y ini W 2 i,j + din k=1,k̸ =i din i=1 x ini y in k W i,j W k,j ] σ 2 xout</formula><p>In second summation all terms are independent of each other and as the expectation of weights is 0 we have</p><formula xml:id="formula_19">Corr(x outj , y outj ) = E[ din i=1 x ini y ini W 2 i,j ] σ 2 xout = din i=1 E[x ini y ini W 2 i,j ] σ 2 xout (Independence of weight initialization) = din i=1 E[x ini y ini ]E[W 2 i,j ] σ 2 xout = din i=1 (r l xin σ 2 xin + µ 2 xin )σ 2 w σ 2 xout (Definition of correlation) = d in (r l xin σ 2 xin + µ 2 xin )σ 2 w d in (σ 2 xin + µ 2 xin )σ 2 w Corr(x outj , y outj ) = r l xin σ 2 xin + µ 2 xin σ 2 xin + µ 2 xin r l xout = r l xin σ 2 xin + µ 2 xin σ 2 xin + µ 2 xin</formula><p>As the backward pass has similar structure, assuming µ gout = 0 we can use the same analysis to get,</p><formula xml:id="formula_20">µ gin = 0 σ 2 gin = d out σ 2 gout σ 2 w A.3 Dropout</formula><p>We can define Dropout mathematically as,</p><formula xml:id="formula_21">x out = Dropout(x in ) =⇒ x outi = xin i (1-p) with probability 1 -p 0 else</formula><p>To calculate expectation of dropout,</p><formula xml:id="formula_22">E[x outi ] = 0 * p + (1 -p) * E[ x ini (1 -p) ] µ xout = µ xin</formula><p>For variance,</p><formula xml:id="formula_23">Var(x outi ) = E[x 2 outi ] -E[x outi ] 2 = 0 * p + (1 -p) * E[ x 2 ini (1 -p) 2 ] -µ 2 xin = E[x 2 ini ] (1 -p) -µ 2 x = σ 2 xin + µ 2 xin (1 -p) -µ 2 xin σ 2 xout = σ 2 xin + pµ 2 xin (1 -p)</formula><p>If we have two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = Dropout(x in ) and y out = Dropout(y in ). Then for any j we have</p><formula xml:id="formula_24">Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] -µ xout µ xout σ 2 xout σ 2 xout = p 2 * 0 + 2 * p * (1 -p) * 0 + (1 -p) 2 * E[ xin j yin j (1-p) * (1-p) ] -µ 2 xout σ 2 xout = E[x inj y inj ] -µ 2 xout σ 2 xout Corr(x outj , y outj ) = (r l xin σ 2 xin )(1 -p) σ 2 xin + pµ 2 xin = r l xout</formula><p>We can define the backward pass of Dropout as,</p><formula xml:id="formula_25">g ini = gout i (1-p) if x i isn't dropped out (which has probability (1 -p)) 0 else</formula><p>Again we can see that backward has similar definition to that of forward pass. Assuming µ gx out = 0 and using similar analysis we get, µ gin = 0</p><formula xml:id="formula_26">σ 2 gin = σ 2 gout (1 -p) A.4 ReLU</formula><p>Formuale functionally equivalent to ours for µ x , σ 2 x , and σ 2 g have also been derived in <ref type="bibr" target="#b3">Arpit et al. (2016)</ref>. We can define ReLU mathematically as,</p><formula xml:id="formula_27">x out = ReLU(x in ) =⇒ x outi = x ini if x ini &gt; 0 0 else</formula><p>For getting expectation of output of ReLU for normally distributed input we have,</p><formula xml:id="formula_28">E[x outi ] = ∞ -∞ ReLU(x ini ) exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 -∞ 0 * exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini + ∞ 0 x ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = ∞ 0 x ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini Substituting t = x 2 ini 2σ 2 xin we have dt = x ini dx ini σ 2 xin we get, E[x outi ] = ∞ 0 σ xin exp (-t)dt √ 2π = σ xin √ 2π [-exp (-t)] ∞ 0 = σ xin √ 2π</formula><p>Hence, the mean of output</p><formula xml:id="formula_29">µ xout = σ xin √ 2π<label>(1)</label></formula><p>Variance of output can be calculated by,</p><formula xml:id="formula_30">Var(x outi ) = E[x outi 2 ] -E[x outi ] 2 = ∞ -∞ (ReLU(x ini )) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π = 0 -∞ 0 * exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini + ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π = ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π Let I = ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini , then substituting t = -x ini we have, I = -∞ 0 -t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt = 0 -∞ t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt =⇒ I + I = 0 -∞ t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt + ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini 2I = ∞ -∞ x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = σ 2 xin =⇒ Var(x outi ) = σ 2 xin 2 - σ 2 xin 2π = σ 2 xin 2 (1 - 1 π ) σ 2 xout = σ 2 xin 2 (1 - 1 π )</formula><p>Now for two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = ReLU(x in ) and y out = ReLU(y in ). Then for any j we have,</p><formula xml:id="formula_31">Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) E[x outj y outj ] = ∞ 0 ∞ 0 x inj y inj 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x 2 inj + y 2 inj -2r l xin x inj y inj ) 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = ∞ 0 ∞ 0 x inj y inj 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -y 2 inj 2σ<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xin</head><p>)dx inj dy inj Substituting t = x injr l xin y inj , and assuming y inj is constant for the inner integral,dx inj = dt</p><formula xml:id="formula_32">E[x outj y outj ] = = ∞ 0 y inj exp ( -y 2 in j 2σ 2 x in ) √ 2πσ xin ∞ -r l x in yin j t + r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj + ∞ 0 y inj √ 2πσ x exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 )</formula><p>)dtdy inj Let us first define I 1 and I 2 as:</p><formula xml:id="formula_33">I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj I 2 = ∞ 0 y inj √ 2πσ x exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj Substituting p = t 2 2σ 2 xin (1 -(r l xin ) 2 ) we have dp = tdt σ 2 xin (1 -(r l xin ) 2 ) I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ (r l x in y in j ) 2 2σ 2 x in (1-(r l x in ) 2 ) σ xin (1 -(r l xin ) 2 ) √ 2π exp (-p)dpdy inj = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) σ xin (1 -(r l xin ) 2 ) √ 2π exp ( -(r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ 0 y inj (1 -(r l xin ) 2 ) 2π exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj Substituting m = y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) , dm = y inj dy inj σ 2 xin (1 -(r l xin ) 2 )</formula><p>,</p><formula xml:id="formula_34">I 1 = ∞ 0 (1 -(r l xin ) 2 ) 2π (1 -(r l xin ) 2 )σ 2 xin exp (-m)dm = (1 -(r l xin ) 2 ) 3 2 σ 2 xin 2π I 2 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj Substituting p = -t, where Φ is CDF of Standard Normal Distribution I 2 = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) -∞ r l x in yin j -1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) r l x in yin j -∞ 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )Φ( r l xin y inj σ xin 1 -(r l xin ) 2 )dy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )[ 1 2 (1 + erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 ) ))]dy inj = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj + r l xin 2 √ 2πσ xin ∞ 0 y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 )</formula><p>)dy inj Let us define I 2,1 and I 2,2 as</p><formula xml:id="formula_35">I 2,1 = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj I 2,2 = r l xin 2 √ 2πσ xin ∞ 0 y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 ) )dy inj I 2,1 = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj I 2,1 = r l xin σ 2 xin 4</formula><p>(Same integral as in variance calculation)</p><p>From <ref type="bibr" target="#b79">Ng &amp; Geller (1969)</ref> we have</p><formula xml:id="formula_36">∞ 0 x 2 exp (-b 2 x 2 )erf(ax)dx = √ π 4b 3 - tan -1 ( b a ) 2 √ πb 3 + a 2 √ πb 2 (a 2 + b 2 ) . Hence, putting a = r l xin σ xin 2(1 -(r l xin ) 2 ) and b = 1 σ xin √ 2 we get, I 2,2 = r l xin 2 √ 2πσ xin [ 2 √ 2σ 3 xin 4 - tan -1 ( √ (1-(r l x in ) 2 ) r l x in )2 √ 2σ 3 xin 2 √ π + √ 2r l xin σ 3 xin (1 -(r l xin ) 2 ) √ π ] = r l xin σ 2 xin 4 - r l xin cos -1 (r l xin )σ 2 xin 2π + (r l xin ) 2 (1 -(r l xin ) 2 )σ 2 xin 2π E[x outj y outj ] = I 1 + I 2,1 + I 2,2 = (1 -(r l xin ) 2 ) 3 2 σ 2 xin 2π + 2 * r l xin σ 2 xin 4 - r l xin cos -1 (r l xin )σ 2 xin 2π + (r l xin ) 2 (1 -(r l xin ) 2 )σ 2 xin 2π = r l xin σ 2 xin 2 - r l xin cos -1 (r l xin )σ 2 x 2π + (1 -(r l xin ) 2 )σ 2 xin 2π Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = r l xin σ 2 xin 2 - r l xin cos -1 (r l xin )σ 2 xin 2π + (1 -(r l xin ) 2 )σ 2 x 2π - σ 2 xin 2π σ 2 xin 2 (1 - 1 π ) r l xout = πr l x in 2 + r l xin sin -1 (r l xin ) + (1 -(r l xin ) 2 ) -1 π -1</formula><p>Backward pass on ReLU can be defined as,</p><formula xml:id="formula_37">g ini = g outi if x ini &gt; 0 (which has probability 1 2 ) 0 else Assuming µ gout = 0, E[g ini ] = 1 2 * 0 + 1 2 * E[g outi ] µ gin = 0 Var(g ini ) = E[g 2 ini ] -E[g ini ] 2 = E[g 2 ini ] = 1 2 * 0 + 1 2 * E[g 2 out ] σ 2 gin = σ 2 gout 2</formula><p>If for two inputs x in and y in for all i we have Corr(g outx i , g outy i ) = r l gout , and g inx i , g iny i be the gradient after passing through ReLU layer. Then we have,</p><formula xml:id="formula_38">E[g inx i g iny i ] = P(x ini &gt; 0, y ini &gt; 0)E[g outx i g outy i ] = P(x ini &gt; 0, y ini &gt; 0)r l gout σ 2 gout P(x ini &gt; 0, y ini &gt; 0) = = ∞ 0 ∞ 0 x ini y ini 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x 2 ini + y 2 ini -2r l xin x ini y ini ) 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini dy ini = ∞ 0 ∞ 0 x ini y ini 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x ini -r l xin y ini ) 2 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -y 2 ini 2σ<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xin</head><p>)dx ini dy ini Substituting t = x inir l xin y ini , and assuming y ini is constant for the inner integral,dx ini = dt</p><formula xml:id="formula_39">P(x ini &gt; 0, y ini &gt; 0) = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) ∞ -r l x in yin i 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy ini Substituting p = -t, where Φ is CDF of Standard Normal Distribution P(x ini &gt; 0, y ini &gt; 0) = = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) -∞ r l x in yin i -1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) r l x in yin i -∞ 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )Φ( r l xin y ini σ xin 1 -(r l xin ) 2 )dy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )[ 1 2 (1 + erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 ) ))]dy ini = 1 2 ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini + 1 2 √ 2πσ xin ∞ 0 exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 ) )dy ini = 1 4 + 1 2 √ 2πσ xin ∞ 0 exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 )</formula><p>)dy ini</p><p>From <ref type="bibr" target="#b79">Ng &amp; Geller (1969)</ref> we have</p><formula xml:id="formula_40">∞ 0 exp (-b 2 x 2 )erf(ax)dx = √ π 2b - 1 b √ π tan -1 ( b a ) Putting a = r l xin σ xin 2(1 -(r l xin ) 2 ) and b = 1 σ xin √ 2 we get, P(x ini &gt; 0, y ini &gt; 0) = 1 4 + 1 2 √ 2πσ xin [ √ πσ xin √ 2 2 - σ xin √ 2 √ π tan -1 ( (1 -(r l xin ) 2 ) r l xin )] = 1 4 + 1 2π [ π 2 -cos -1 (r l xin )] = 1 4 + sin -1 (r l xin ) 2π =⇒ E[g inx i g iny i ] = ( 1 4 + sin -1 (r l xin ) 2π )r l gout σ 2 gout Corr(g inx i , g iny i ) = ( 1 4 + sin -1 (r l x in ) 2π )r l gout σ 2 gout σ 2 g out 2 r l gout = ( 1 2 + sin -1 (r l xin ) π )r l gout A.5 GeLU</formula><p>Forward pass through GeLU is defined as,</p><formula xml:id="formula_41">x out = GeLU(x in ) =⇒ x outi = x ini Φ(x ini ) where Φ(x) is CDF of Standard Normal Distribution at x = x ini 2 1 + erf( x ini √ 2 )</formula><p>To get the mean of output of GeLU, we have</p><formula xml:id="formula_42">E[x outi ] = ∞ -∞ x outi √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini (1 + erf( xin i √ 2 )) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞ x ini erf( xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini erf( xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini (Integral of odd function) = 1 2 √ 2πσ xin ∞ -∞ x ini erf( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.6.1.4 of Korotkov &amp; Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a 1 √ a 2 + a 1 Substituting, a = 1 √ 2 , a 1 = 1 2σ 2 xin , we have E[x outi ] = 1 2 √ 2πσ xin 1 √ 2 1 2σ 2 x in 1 2 + 1 2σ 2 x in = 1 2 √ 2πσ xin 2σ 3 xin σ 2 xin + 1 µ xout = σ 2 xin 2π(σ 2 xin + 1)</formula><p>For calculating variance of output,</p><formula xml:id="formula_43">E[x 2 outi ] = ∞ -∞ x 2 outi √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x 2 ini (1 + erf( xin i √ 2 )) 2 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x 2 ini 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞</formula><p>x 2 ini erf(</p><formula xml:id="formula_44">xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞ x 2 ini erf 2 ( xin i √ 2 ) 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = σ 2 xin 4 + ∞ -∞ x 2 ini erf 2 ( xin i √ 2 ) 4 √ 2πσ xin exp ( -x 2 ini 2σ<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xin</head><p>)dx ini (Definition of variance, and integral of odd function)</p><formula xml:id="formula_45">= σ 2 xin 4 + 1 4 √ 2πσ xin ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.7.3.3 of Korotkov &amp; Korotkov (2020) ∞ -∞ z 2 exp (-az 2 )erf(a 1 z)erf(a 2 z) = 1 √ π ( 1 a √ a tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) + a 1 a 2 (2a + a 2 1 + a 2 2 ) a a + a 2 1 + a 2 2 (a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 ) ) Substituting a = 1 2σ 2 x in , a 1 = a 2 = 1 √ 2 ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini = 1 √ π (2 √ 2σ 3 xin tan -1 ( 1 2 1 4σ 4 x in + 1 2σ 2 x in ) + 1 2 ( 1 σ 2 x in + 1) 1 2σ 2 x in 1 2σ 2 x in + 1( 1 4σ 4 x in + 1 2σ 2 x in + 1 4 ) ) = 1 √ π (2 √ 2σ 3 xin tan -1 ( σ 2 xin (σ 2 xin + 1) 2 -σ 4 xin ) + 4 √ 2σ 5 xin (σ 2 xin + 1) 2σ 2 xin + 1(σ 4 xin + 2σ 2 xin + 1) ) = 1 √ π (2 √ 2σ 3 xin sin -1 ( σ 2 xin σ 2 xin + 1 ) + 4 √ 2σ 5 xin 2σ 2 xin + 1(σ 2 xin + 1) ) = 2 √ 2σ 3 xin √ π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1)</formula><p>)</p><p>)</p><formula xml:id="formula_46">E[x 2 outi ] = σ 2 xin 4 + 1 4 √ 2πσ xin ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini = σ 2 xin 4 + 1 4 √ 2πσ xin 2 √ 2σ 3 xin √ π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1)</formula><p>)</p><p>)</p><formula xml:id="formula_47">E[x 2 outi ] = σ 2 xin 4 + σ 2 xin 2π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1) )) Var(x outi ) = E[x 2 outi ] -(E[x outi ]) 2 σ 2 xout = σ 2 xin 2π ( π 2 - σ 2 xin 1 + σ 2 xin + sin -1 ( σ 2 xin 1 + σ 2 xin ) + 2σ 2 xin (1 + σ 2 xin ) 1 + 2σ 2 xin )</formula><p>Now if we have two inputs x in and y in such that for all values of i, we have Corr(x ini , y ini ) = r l xin , then we can calculate the covariance Cov(x outj , y outj ) for any j as,</p><formula xml:id="formula_48">Cov(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] E[x outj y outj ] = ∞ -∞ x outj y outj 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 inj + 2r l xin x inj y inj -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = I = ∞ -∞ x inj (1 + erf( xin j √ 2 ))y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 inj + 2r l xin x inj y inj -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy inj Where I X = ∞ -∞ x inj (1 + erf( x inj √ 2 )) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X = ∞ -∞ x inj (1 + erf( x inj √ 2 )) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj + ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj Let, I X,1 = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X,2 = ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X,1 = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -(r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) ∞ -∞ x inj exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) I X,2 = ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj From 2.7.2.4 of Korotkov &amp; Korotkov (2020), ∞ -∞ zerf(a 1 z) exp (-az 2 + bz)dz = = √ πb 2a √ a exp ( b 2 4a )erf( a 1 b 2 a 2 + aa 2 1 ) + a 1 a a + a 2 1 exp ( b 2 4a + 4a 2 1 ) Substituting a 1 = 1 √ 2 , a = 1 2σ 2 x in (1-(r l x in ) 2 ) , b = r l x in yin j σ 2 x in (1-(r l x in</formula><p>) 2 ) , we get</p><formula xml:id="formula_49">I X,2 = √ π r l x in yin j σ 2 x in (1-(r l x in ) 2 ) 2 1 2 √ 2σ 3 x in (1-(r l x in ) 2 ) 3 2 exp ( (r l x in ) 2 y 2 in j σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) )erf( r l x in yin j √ 2σ 2 x in (1-(r l x in ) 2 ) 2 1 4σ 4 x in (1-(r l x in ) 2 ) 2 + 1 4σ 2 x in (1-(r l x in ) 2 ) ) + 1 √ 2 1 2σ 2 x in (1-(r l x in ) 2 ) 1 2σ 2 x in (1-(r l x in ) 2 ) + 1 2 exp ( (r l x in ) 2 y 2 in j σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) + 4 2 ) = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) + 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )</formula><p>Let us define I X,2,1 and I X,2,2 as:</p><formula xml:id="formula_50">I X,2,1 = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) I X,2,2 = 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) ) I = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy inj = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )(I X,1 + I X,2,1 + I X,2,2 )dy inj I 1 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X,1 dy inj I 2 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X,2,1 dy inj I 3 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 )</formula><p>)I X,2,2 dy inj</p><p>We have </p><formula xml:id="formula_51">I = I 1 + I 2 + I 3 I 1 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj = r l xin 4 ∞ -∞ y 2 inj (1 + erf( yin j √ 2 )) √ 2πσ 2 xin exp ( -y 2 inj 2σ<label>2</label></formula><formula xml:id="formula_52">I 2 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj (1 + erf( y inj √ 2 )) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj + r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj erf( y inj √ 2 ) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj erf( y inj √ 2 ) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj (Integral of Odd function) From 2.7.3.3 of Korotkov &amp; Korotkov (2020), ∞ -∞ z 2 exp (-az 2 )erf(a 1 z)erf(a 2 z) = 1 √ π ( 1 a √ a tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) + a 1 a 2 (2a + a 2 1 + a 2 2 ) a a + a 2 1 + a 2 2 (a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 )</formula><p>)</p><formula xml:id="formula_53">Substituting a = 1 2σ 2 x in , a 1 = 1 √ 2 , a 2 = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) a 1 a 2 = r l xin 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><formula xml:id="formula_54">a 2 + aa 2 1 + aa 2 2 = 1 4σ 4 xin + 1 4σ 2 xin + (r l xin ) 2 4σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = σ 2 xin (1 -(r l xin ) 2 ) + 1 + σ 4 xin (1 -(r l xin ) 2 ) + σ 2 xin + (r l xin ) 2 σ 2 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) a + a 2 1 + a 2 2 = a 2 + aa 2 1 + aa 2 2 a = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) * 2σ 2 xin = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><formula xml:id="formula_55">a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) + (r l xin ) 2 4(σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + (r l xin ) 2 σ 4 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 2a + a 2 1 + a 2 2 = 1 2σ 2 xin + (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + σ 2 xin (1 -(r l xin ) 2 ) + 1 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 + σ 2 xin + 1 -(r l xin σ 2 xin ) 2 -σ 2 xin (r l xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1)(σ 2 xin + 2) -(r l xin ) 2 σ 2 xin (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1)(σ 2 xin (1 -(r l xin ) 2 ) + 2) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) I 2 = r l xin 4 √ 2πσ xin (2 √ 2σ 3 xin tan -1 ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 4σ 4 x in (σ 2 x in (1-(r l x in ) 2 )+1) )) + r l xin 4 √ 2πσ xin ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1)(σ 2 x in (1-(r l x in ) 2 )+2) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2σ 2 x in (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2 4σ 4 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) = r l xin 4 √ 2πσ xin (2 √ 2σ 3 xin tan -1 ( r l xin σ 2 xin (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 )) + r l xin 4 √ 2πσ xin ( 2 √ 2r l xin σ 5 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2</formula><p>)</p><formula xml:id="formula_56">I 2 = r l xin σ 2 xin 2π (sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I 3 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ -∞ σ xin (1 -(r l xin ) 2 )y inj (1 + erf( yin j √ 2 )) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( -y 2 inj (σ 2 xin (1 -(r l xin ) 2 ) + 1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ -∞ σ xin (1 -(r l xin ) 2 )y inj (1 + erf( yin j √ 2 )) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( -y 2 inj (σ 2 xin + 1)(1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj (1 + erf( y inj √ 2 )) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj + σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj erf( y inj √ 2 ) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj erf( y inj √ 2 ) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj (Integral of Odd function) From 2.6.1.4 of Korotkov &amp; Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a 1 √ a 2 + a 1 Substituting, a = 1 √ 2 , a 1 = (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>, we have</p><formula xml:id="formula_57">I 3 = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ( 1 √ 2 (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2 + (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 2σ 3 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 (σ 2 xin + 1) σ 4 xin (1 -(r l xin ) 2 ) + σ 2 xin + σ 2 xin + 1 I 3 = σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)(1 -(r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2</formula><p>Finally we have,</p><formula xml:id="formula_58">I = I 1 + I 2 + I 3 = r l xin σ 2 xin 4 + r l xin σ 2 xin 2π (sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) + σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)(1 -(r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I = r l xin σ 2 xin 4 + r l xin σ 2 xin 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I = σ 2 xin 4   r l xin + 2r l xin π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2  </formula><p>We have,</p><formula xml:id="formula_59">Cov(x outj , y outj ) = I -E[x outj ]E[y outj ] Cov(x outj , y outj ) = I - σ 4 xin 2π(σ 2 xin + 1) Cov(x outj , y outj ) = σ 2 xin 4π (πr l xin + 2r l xin sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 - 2σ 2 xin (σ 2 xin + 1)</formula><p>)</p><p>The backward pass through GeLU is defined as,</p><formula xml:id="formula_60">g ini = (Φ(x ini ) + x ini √ 2π exp ( -x 2 ini 2 ))g outi = ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outi</formula><p>So the mean of gradient is obtained as following,</p><formula xml:id="formula_61">E[g ini ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))]E[g outi ] = 0 µ gin = 0</formula><p>Similarly for variance,</p><formula xml:id="formula_62">E[g 2 ini ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 g 2 outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ]E[g 2 outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ]σ 2 gout I = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ] = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I = ∞ -∞ ( 1 4 + erf 2 ( xin i √ 2 ) 4 + x 2 ini exp (-x 2 ini ) 2π + erf( xin i √ 2 ) 2 + x ini exp ( -x 2 in i 2 ) √ 2π + x ini exp ( -x 2 in i 2 )erf( xin i √ 2 ) √ 2π ) exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I 1 = ∞ -∞ 1 4 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I 1 = 1 4 I 2 = ∞ -∞ erf 2 ( xin i √ 2 ) 4 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 4 √ 2πσ xin ∞ -∞ erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.7.1.3 of Korotkov &amp; Korotkov (2020), ∞ -∞ erf(a 1 z)erf(a 2 z) exp (-az 2 )dz = 2 √ πa tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) Substituting a = 1 2σ 2 x in , a 1 = a 2 = 1 √ 2 I 2 = 1 4 √ 2πσ xin 2 π 1 2σ 2 x in tan -1 ( 1 2 1 4σ 4 x in + 1 4σ 2 x in + 1 4σ 2 x in ) = 1 2π tan -1 ( σ 2 xin 2σ 2 xin + 1 ) = 1 2π tan -1 ( σ 2 xin (σ 2 xin + 1) 2 -σ 4 xin ) I 2 = 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) I 3 = ∞ -∞ x 2 ini exp (-x 2 ini ) 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 2πσ xin ∞ -∞ x 2 ini √ 2π exp ( -x 2 ini (2σ 2 xin + 1) 2σ 2 xin )dx ini = 1 2πσ xin σ xin (2σ 2 xin + 1) ∞ -∞ x 2 ini √ 2π σx in √ (2σ 2 x in +1) exp ( -x 2 ini (2σ 2 xin + 1) 2σ 2 xin )dx ini = 1 2πσ xin σ xin (2σ 2 xin + 1) σ 2 xin (2σ 2 xin + 1)</formula><p>(Definition of variance)</p><formula xml:id="formula_63">I 3 = σ 2 xin 2π(2σ 2 xin + 1) 3 2 I 4 = ∞ -∞ erf( xin i √ 2 ) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 (Integral of odd function) I 5 = ∞ -∞ x ini exp ( -x 2 in i 2 ) √ 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 (Integral of odd function) I 6 = ∞ -∞ x ini exp ( -x 2 in i 2 )erf( xin i √ 2 ) √ 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 2πσ xin ∞ -∞ x ini erf( x ini √ 2 ) exp ( -x 2 ini (σ 2 xin + 1) 2σ 2 xin )dx ini From 2.6.1.4 of Korotkov &amp; Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = 1 √ 2 , a 1 = (σ 2 x in +1) 2σ 2</formula><p>x in , we have</p><formula xml:id="formula_64">I 6 = 1 2πσ xin 1 √ 2 (σ 2 x in +1) 2σ 2 x in 1 2 + (σ 2 x in +1) 2σ 2 x in = 1 2πσ xin 2σ 3 xin (σ 2 xin + 1) 2σ 2 xin + 1 I 6 = σ 2 xin π(σ 2 xin + 1) 2σ 2 xin + 1 I = I 1 + I 2 + I 3 + I 4 + I 5 + I 6 = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin 2π(2σ 2 xin + 1) 3 2 + σ 2 xin π(σ 2 xin + 1) 2σ 2 xin + 1 = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (4σ 2 xin + 2 + σ 2 xin + 1) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2 I = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (5σ 2 xin + 3) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2</formula><p>So the variance of gradient of input of GeLU comes out to be</p><formula xml:id="formula_65">E[g 2 ini ] = Iσ 2 gout σ 2 gin = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (5σ 2 xin + 3) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2 σ 2 gout</formula><p>If for two inputs x in and y in for all i we have Corr(g outx i , g outy i ) = r l gout , and g inx i , g iny i be the gradient after passing through GeLU layer. Then we have,</p><formula xml:id="formula_66">E[g inx i g iny i ] = = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outx i ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))g outy i ] E[g inx i g iny i ] = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))]E[g outx i g outy i ] = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))]r l gout σ 2 gout I = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))] = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))p xin i ,yin i dx ini dy ini Where p xin i ,yin i = 1 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 ini + 2r l xin x ini y ini -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) I = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy ini</formula><p>Where,</p><formula xml:id="formula_67">I X = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini I X,1 = ∞ -∞ 1 2 exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 ∞ -∞ exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -(r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) ∞ -∞ exp ( -(x ini -r l xin y ini ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) ∞ -∞ exp ( -(xin i -r l x in yin i ) 2 2σ 2 x in (1-(r l x in ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) dx ini I X,1 = √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) I X,2 = ∞ -∞ erf( xin i √ 2 ) 2 exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 ∞ -∞ erf( x ini √ 2 ) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini From 2.7.1.6 of Korotkov &amp; Korotkov (2020), ∞ -∞ erf(a 1 z) exp (-az 2 + bz)dz = π a exp ( b 2 4a )erf( a 1 b 2 a 2 + aa 2 1 ) Substituting a 1 = 1 √ 2 , a = 1 2σ 2 x in (1-(r l x in ) 2 ) , b = r l x in yin i σ 2 x in (1-(r l x in ) 2 ) I X,2 = 1 2 π 1 2σ 2 x in (1-(r l x in ) 2 ) exp ( (r l x in ) 2 y 2 in i σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) )erf( r l x in yin i √ 2σ 2 x in (1-(r l x in ) 2 ) 2 1 4σ 4 x in (1-(r l x in ) 2 ) 2 + 1 4σ 2 x in (1-(r l x in ) 2 ) ) I X,2 = √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)</p><formula xml:id="formula_68">I X,3 = ∞ -∞ x ini √ 2π exp ( -x 2 ini 2 ) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini (σ 2 xin (1 -(r l xin ) 2 ) + 1) + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini + 2r l x in xin i yin i (σ 2 x in (1-(r l x in ) 2 )+1) 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini + 2r l x in xin i yin i (σ 2 x in (1-(r l x in ) 2 )+1) 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) ) exp ( -(r l x in ) 2 y 2 in i (σ 2 x in (1-(r l x in ) 2 )+1) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) ) * exp ( (r l x in ) 2 y 2 in i (σ 2 x in (1-(r l x in ) 2 )+1) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) * ∞ -∞ x ini √ 2π exp ( -(x ini - r l x in yin i (σ 2 x in (1-(r l x in ) 2 )+1) ) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) ∞ -∞ x ini √ 2π σx in √ 1-(r l x in ) 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) exp ( -(x ini - r l x in yin i (σ 2 x in (1-(r l x in ) 2 )+1) ) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>).</p><formula xml:id="formula_69">σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) r l xin y ini (σ 2 xin (1 -(r l xin ) 2 ) + 1) I X,3 = r l xin y ini σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) I = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )(I X,1 + I X,2 + I X,3 )dy ini I 1 = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,1 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )dy ini = 1 2 ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini I 1,1 = 1 4 ∞ -∞ 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini = 1 4 I 1,2 = 1 4 ∞ -∞ erf( yin i √ 2 ) √ 2πσ xin exp ( -y 2 ini 2σ<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xin</head><p>)dy ini = 0 (Integral of odd function)</p><formula xml:id="formula_70">I 1,3 = 1 2 ∞ -∞ y ini exp ( -y 2 in i 2 ) 2πσ xin exp ( -y 2 ini 2σ<label>2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xin</head><p>)dy ini = 0 (Integral of odd function)</p><formula xml:id="formula_71">I 2 = ∞ -∞</formula><p>( 1 2 (1 + erf(</p><formula xml:id="formula_72">yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,2 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = 1 2 ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ). erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini I 2,1 = 1 4 ∞ -∞ 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)dy ini = 0 (Integral of odd function)</p><formula xml:id="formula_73">I 2,2 = 1 4 √ 2πσ xin ∞ -∞ erf( y ini √ 2 ) exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini From 2.7.1.3 of Korotkov &amp; Korotkov (2020), ∞ -∞ erf(a 1 z)erf(a 2 z) exp (-az 2 )dz = 2 √ πa tan -1 ( a1a2 √ a 2 +aa 2 1 +aa 2 2 ) Substituting a = 1 2σ 2 x in , a 1 = 1 √ 2 , a 2 = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) I 2,2 = 1 4 √ 2πσ xin 2 π 1 2σ 2 x in tan -1 ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) 1 4σ 4 x in + 1 4σ 2 x in + (r l x in ) 2 4σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) I 2,2 = 1 2π tan -1 ( r l xin σ 2 xin σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin ) = 1 2π tan -1 ( r l xin σ 2 xin (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I 2,2 = 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1</formula><p>)</p><formula xml:id="formula_74">I 2,3 = 1 4πσ xin ∞ -∞ y ini exp ( -y 2 ini 2 ) exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = 1 4πσ xin ∞ -∞ y ini exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)dy ini From 2.6.1.4 of <ref type="bibr" target="#b54">Korotkov &amp; Korotkov (2020)</ref>,</p><formula xml:id="formula_75">∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) , a 1 = (σ 2 x in +1) 2σ 2</formula><p>x in , we have</p><formula xml:id="formula_76">I 2,3 = 1 4πσ xin r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2σ 2 x in (r l x in ) 2 2(σ 2 x in (1-(r l x in ) 2 )+1) + (σ 2 x in +1) 2σ 2 x in = r l xin σ 2 xin 2π(σ 2 xin + 1) σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin I 2,3 = r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I 3 = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,3 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) r l xin y ini σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini (σ 2 xin (1 -(r l xin ) 2 ) + 1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)dy ini</p><formula xml:id="formula_77">I 3,1 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)dy ini = 0 (Integral of odd function)</p><formula xml:id="formula_78">I 3,2 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini erf( y ini √ 2 ) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)</formula><p>)dy ini From 2.6.1.4 of <ref type="bibr" target="#b54">Korotkov &amp; Korotkov (2020)</ref>,</p><formula xml:id="formula_79">∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = 1 √ 2 , a 1 = (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in</formula><p>) 2 )+1) , we have</p><formula xml:id="formula_80">I 3,2 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 1 √ 2 (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2 + (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) = r l xin σ 2 xin 2π(σ 2 xin + 1) σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin I 3,2 = r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I 3,3 = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 . ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini 2 ) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 . ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini (σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 σ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ∞ -∞ y 2 ini √ 2π σx in √ (σ 2 x in (1-(r l x in ) 2 )+1) √ (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 exp ( -y 2 ini ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 σ 3 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 I 3,3 = r l xin σ 2 xin 2π((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I = I 1 + I 2 + I 3 = I 1,1 + I 1,2 + I 1,3 + I 2,1 + I 2,2 + I 2,3 + I 3,1 + I 3,2 + I 3,3 I = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 )+ 2r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + r l xin σ 2 xin 2π((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 I = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin ((2σ 2 xin + 3)(σ 2 xin + 1) -2(r l xin σ 2 xin ) 2 ) 2π(σ 2 xin + 1)((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2</formula><p>We defined Cov(g inx i , g iny i ), as</p><formula xml:id="formula_81">Cov(g inx i , g iny i ) = Ir l gout σ 2 gout Cov(g inx i , g iny i ) = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin ((2σ 2 xin + 3)(σ 2 xin + 1) -2(r l xin σ 2 xin ) 2 ) 2π(σ 2 xin + 1)((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 r l gout σ 2 gout A.6 LayerNorm</formula><p>The affine transformation for layernorm are typically initialized with 1 scale and 0 bias, so they do not change any of our derivations below and are ignored henceforth. For an input x in the forward pass of LayerNorm is,</p><formula xml:id="formula_82">x out = LayerNorm(x in ) =⇒ x outi = x ini -xin σxin Where xin = din i=1 x ini d in σxin = din i=1 (x ini -xin ) 2 d in</formula><p>To get expectation of output of LayerNorm,</p><formula xml:id="formula_83">E[x outi ] = E[ x ini -xin σxin ] din i=1 E[x outi ] = din i=1 E[ x ini -xin σxin ] = E[ din i=1 x ini -xin σxin ] = E[ din i=1 (x ini -xin ) σxin ] din i=1 E[x outi ] = 0</formula><p>By symmetry for any i, j and i ̸ = j we have</p><formula xml:id="formula_84">E[x outi ] = E[x outj ] = µ xout =⇒ d in µ xout = 0 µ xout = 0</formula><p>Similarly we calculate variance of output by,</p><formula xml:id="formula_85">Var(x outi ) = E[x 2 outi ] -E[x outi ] 2 = E[x 2 outi ] E[x 2 outi ] = E[ (x ini -xin ) 2 σ2 xin ] din i=1 E[x 2 outi ] = din i=1 E[ (x ini -xin ) 2 σ2 xin ] = E[ din i=1 (x ini -xin ) 2 σ2 xin ] = E[ din i=1 (x ini -xin ) 2 σ2 xin ] din i=1 E[x 2 outi ] = d in</formula><p>By symmetry for any i, j and i ̸ = j we have</p><formula xml:id="formula_86">E[x 2 outi ] = E[x 2 outj ] = σ 2 xout =⇒ d in σ 2 xout = d in σ 2 xout = 1</formula><p>Now we have σxin a.s -→ σ xin for large d in . So for large values of d in we can treat σxin as a constant which has value σ xin . We use this approximation to get the following results. For two inputs x in and y in such that for all i, Corr(x ini , y ini ) = r l xin . For all j we have,</p><formula xml:id="formula_87">Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] -µ xout µ xout σ 2 xout σ 2 xout = E[x outj y outj ] -0 √ 1 = E[x outj y outj ] = E[ (x inj -xin )(y inj -ȳin ) σxin σyin ] ≈ E[ (x inj -xin )(y inj -ȳin ) σ xin σ xin ] = E[(x inj -xin )(y inj -ȳin )] σ 2 xin = E[(x inj - d in k=1 xin k din )(y inj - d in l=1 yin l din )] σ 2 xin = E[x inj y inj -y inj d in k=1 xin k din -x inj d in l=1 yin l din + d in k=1 xin k din d in l=1 yin l din ] σ 2 xin</formula><p>Elements belonging to different dimensions from x in and y in are independent of each other and hence for i, j and i ̸ = j we have</p><formula xml:id="formula_88">E[x ini y inj ] = µ 2 xin . = E[x inj y inj ] -E[y inj d in k=1 xin k din ] -E[x inj d in l=1 yin l din ] + E[ d in k=1 xin k din d in l=1 yin l din ] σ 2 xin = r l xin σ 2 xin + µ 2 xin - r l x in σ 2 x in +dinµ 2 x in din - r l x in σ 2 x in +dinµ 2 x in din + r l x in dinσ 2 x in +d 2 in µ 2 x in d 2 in σ 2 xin = r l xin σ 2 xin (1 -1 din ) σ 2 xin Corr(x outj , y outj ) = r l xin (1 - 1 d in ) ≈ r l xin = r l xout</formula><p>From Xu et al. ( <ref type="formula">2019</ref>) (Eq. 17), the backward pass through LayerNorm is,</p><formula xml:id="formula_89">g in = g out σxin (I din - 1 T din 1 din + x T out x out d in ) ≈ g out σ xin (I din - 1 T din 1 din + x T out x out d in )</formula><p>We have lim</p><formula xml:id="formula_90">din→∞ 1 T din 1 din + x T out x out d in = O din,din where O din,din is zero matrix with shape d in × d in g in ≈ g out σ xin (I din ) = g out σ xin =⇒ g ini = g outi σ xin If µ gout = 0, µ gin = 0 σ 2 gin = σ 2 gout σ 2 xin A.7 Softmax</formula><p>Assumption: Other than assuming normally distributed inputs, we also assume that L is large L &gt;&gt; 1 to derive softmax variance.</p><p>The forward pass of Softmax can be defined as</p><formula xml:id="formula_91">x out = Softmax(x in ) x outi = e xin i L j=1 e xin j</formula><p>For calculating mean we can easily see that, L i=1</p><p>x outi = 1</p><p>Taking expectation both sides, we get</p><formula xml:id="formula_92">E[ L i=1 x outi ] = 1 L i=1 E[x outi ] = 1</formula><p>By symmetry we can assume that for any i, j, i ̸ = j, we have</p><formula xml:id="formula_93">E[x outi ] = E[x outj ] LE[x outi ] = 1 µ xout = 1 L</formula><p>Let us define z = j e yj where y j = x jx i is normally distributed N (0, σ j ). Hence, each e yj is log-normally distributed, and z is a sum of correlated log-normals. Following <ref type="bibr" target="#b68">(Lo, 2013)</ref>, this sum of log-normals can be approximated as another log-normal random variable, LogN (µ z , σ z ), where µ z and σ z are as follows -</p><formula xml:id="formula_94">S + = E[ j y j ] = j e σ 2 j 2 σ 2 z = 1 S 2 + j,k corr j,k σ j σ k e 1 2 (σ 2 j +σ 2 k ) µ z = ln(S + ) - σ 2 z 2</formula><p>Since the difference of two normals x j and x i is also normal, from the M.G.F. of normal distribution, we have σ 2 j = 2σ 2</p><p>xin (1r xin ) if j ̸ = i, and σ 2 j = 0 if j = i.</p><p>Also, corr j,k = 0 if j = i or k = i, else corr j,k = 1 2 . We can substitute these values in the above equations, to get</p><formula xml:id="formula_95">S + = (L -1)e σ 2 x in (1-rx in ) + 1 σ 2 z = σ 2 xin (1 -r xin ) L L -1 µ z = ln(S + ) - σ 2 z 2</formula><p>Since z is log-normal, x out = 1 z is also log-normal with LogN (-µ z , σ z ). The variance of log-normal distribution can be obtained from standard formulae for log-normal distribution as (e σ 2 z -1)e σ 2 z -2µz .</p><p>Substituting the values of µ z and σ z from above, we get</p><formula xml:id="formula_96">σ 2 xout = (e σ 2 z -1)e 2 * σ 2 z S 2 + = (e σ 2 x in (1-rx in ) L L-1 -1)e 2σ 2 x in (1-rx in ) L L-1 ((L -1)e σ 2 x in (1-rx in ) + 1) 2</formula><p>For large L, we can ignore the 1 in the denominator -</p><formula xml:id="formula_97">σ 2 xout = (e σ 2 x in (1-rx in ) L L-1 -1) (L -1) 2 If L &gt;&gt; 1 and σ 2</formula><p>xin is small, we get the more simplified formula as -</p><formula xml:id="formula_98">σ 2 xout ≈ (e (1-r d x in )σ 2 x in -1) L 2 (Assuming L &gt;&gt; 1)</formula><p>Using the mean and variances, we can calculate the scale of softmax output as follows-</p><formula xml:id="formula_99">E[x 2 out ] = σ 2 xout + µ 2 xout = (e (1-r d x in )σ 2 x in ) L 2</formula><p>The Jacobian of Softmax can be calculated as ( <ref type="bibr" target="#b51">(Kim et al., 2021</ref>)):</p><formula xml:id="formula_100">J i,j = x outi (1 -x outi ) if i = j -x outi x outj else</formula><p>For large values of L this approximately becomes</p><formula xml:id="formula_101">J ≈ diag(x out ) g in = g out J g ini ≈ g outi x outi E[g ini ] ≈ E[g outi x outi ] = E[g outi ]E[x outi ] = 0 = µ gin E[g 2 ini ] ≈ E[g 2 outi x 2 outi ] = E[g 2 outi ]E[x 2 outi ] σ 2 gin = σ 2 gout (e (1-r d x in )σ 2 x in ) L 2 A.8 Scaled Dot-Product Attention</formula><p>Inapplicability of Direct Usage of Softmax Derivations for SHA: One may be tempted to assume attention scores to be independent of values. This then enables the use of our previous LogNormal-based softmax derivation, to easily derive the forward variances.</p><p>But the theoretically calculated moments strongly disagree with empirical simulations. This is because SHA is</p><formula xml:id="formula_102">X out = Dropout(SoftMax( X in W Q W K T X T in √ d k</formula><p>))X in W V , and the W K T X T in term cannot be treated independently of the X in W V term. A simple verification of this can be checked by simply simulating (XW T )X, and verifying that the variances of the results do not match that of L * σ 2 ((XW)), but do if the second X is replaced by another random tensor.</p><p>This necessitates an alternate methodology to derive SHA, where the components are treated as a unified whole.</p><p>Assumption: We assume that L and d in are very large when compared to scale of scores being passed to the Softmax. These approximations hold true for small values of σ q and σ k , and the resulting formulae are fairly accurate, as shown in the numerical verification section.</p><p>The forward pass of Scaled Dot-Product Attention is</p><formula xml:id="formula_103">X out = Dropout(SoftMax( QK T d i,k ))V</formula><p>Where,</p><formula xml:id="formula_104">Q = X in W Q K = X in W K V = X in W V X out = Dropout(SoftMax( X in W Q W K T X T in d i,k ))X in W V Let, O = Dropout(SoftMax( X in W Q W K T X T in d i,k ))X in W = X in W Q W K T d i,k O = Dropout(SoftMax(WX T in ))X in Using results from Linear Layer we have σ 2 w = d in σ 2 xin σ 2 q σ 2 k = d in σ 2 xin σ 2 qk O i,j = L k=1 Dropout(SoftMax(WX T in )) i,k X in k,j = L k=1 Dropout( exp ((WX T in ) i,k ) L m=1 exp ((WX T in ) i,m ) )X in k,j = L k=1 Dropout(exp ((WX T in ) i,k )) L m=1 exp ((WX T in ) i,m ) X in k,j = L k=1 Dropout(exp ((WX T in ) i,k ))X in k,j L m=1 exp ((WX T in ) i,m ) = L k=1 Dropout(exp ( din l=1 W i,l X in k,l ))X in k,j L m=1 exp ( din n=1 W i,n X inm,n ) = L k=1 Dropout(exp ( din l=1 W i,l X in k,l )X in k,j ) L m=1 exp ( din n=1 W i,n X inm,n )</formula><p>Each X ini,j can be written as:</p><formula xml:id="formula_105">X ini,j = ϵ j + δ i,j</formula><p>Where ϵ j and δ i,j are all independent and defined as</p><formula xml:id="formula_106">ϵ j ∼ N (0, r l xin σ 2 xin ) δ i,j ∼ N (0, (1 -r l xin )σ 2 xin ) O i,j = L k=1 Dropout(exp ( din l=1 W i,l X in k,l )X in k,j ) L k=1 exp ( din l=1 W i,l X in k,l ) = L k=1 (1 -d i,k )(exp ( din l=1 W i,l X in k,l )X in k,j ) (1 -p) L k=1 exp ( din l=1 W i,l X in k,l )</formula><p>Where d i,k is Bernoulli random variable which is 1 with probability p</p><formula xml:id="formula_107">= L k=1 (1 -d i,k ) exp ( din l=1 W i,l (ϵ l + δ k,l ))(ϵ j + δ k,j ) (1 -p) L k=1 exp ( din l=1 W i,l (ϵ l + δ k,l )) = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) + L k=1 (1 -d i,k ) exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) + L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) Let v 1 = ϵ j L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) (1-p) L k=1 exp ( d in l=1 W i,l δ k,l ) and v 2 = L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j (1-p) L k=1 exp ( d in l=1 W i,l δ k,l )</formula><p>. We have,</p><formula xml:id="formula_108">O i,j = v 1 + v 2</formula><p>Given a fixed ϵ, W , we have</p><formula xml:id="formula_109">v 1 |ϵ, W = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) = ϵ j L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) L (1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L By WLLN, L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )],</formula><p>and</p><formula xml:id="formula_110">(1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )] Thus, we have v 1 |ϵ, W p →ϵ j v 2 |ϵ, W = L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) = 1 √ L √ L L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j L (1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L Let µ num = E δ,d [(1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j ], σ 2 num = Var δ,d ((1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j ). By central limit theorem for large L, √ L L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L = √ L L k=1 (1 -d i,k )(exp ( din l=1 W i,l δ k,l )δ k,j -µ num ) L + √ Lµ num √ L L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L d →N (0, σ 2 num ) + √ Lµ num L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L d →N (µ num , σ 2 num L ) µ num = E d [1 -d i,k ]( l=d l=1,l̸ =j E δ [exp (W i,l δ k,l )])E δ [exp (W i,j δ k,j )δ k,j ] E δ [exp (W i,l δ k,l )] = exp ( W 2 i,l σ 2 δ 2 ) (MGF of gaussian) E δ [exp (W i,j δ k,j )δ k,j ] = ∞ -∞ exp (W i,j δ k,j )δ k,j √ 2πσ δ exp (- δ 2 k,j 2σ 2 δ )dδ k,j = ∞ -∞ exp ( W 2 i,j σ 2 δ 2 ) δ k,j √ 2πσ δ exp (- (δ k,j -W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp ( W 2 i,j σ 2 δ 2 ) ∞ -∞ δ k,j √ 2πσ δ exp (- (δ k,j -W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp ( W 2 i,j σ 2 δ 2 )W i,j σ 2 δ µ num = (1 -p) exp ( din l=1 W 2 i,l σ 2 δ 2 )W i,j σ 2 δ σ 2 num = E d [(1 -d i,k ) 2 ]( l=d l=1,l̸ =j E δ [exp (2W i,l δ k,l )]E δ [exp (2W i,j δ k,j )δ 2 k,j ] -µ 2 num ) E δ k,l [exp (2W i,l δ k,l )] = exp (2W 2 i,l σ 2 δ ) (MGF of gaussian) E δ k,j [exp (2W i,j δ k,j )δ 2 k,j ] = ∞ -∞ exp (2W i,j δ k,j )δ 2 k,j √ 2πσ δ exp (- δ 2 k,j 2σ 2 δ )dδ k,j = ∞ -∞ exp (2W 2 i,j σ 2 δ ) δ 2 k,j √ 2πσ δ exp (- (δ k,j -2W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp (2W 2 i,j σ 2 δ ) ∞ -∞ δ 2 k,j √ 2πσ δ exp (- (δ k,j -2W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp (2W 2 i,j σ 2 δ )(4W 2 i,j σ 4 δ + σ 2 δ ) σ 2 num = (1 -p) exp (2 din l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ + σ 2 δ ) -(1 -p) 2 exp ( din l=1 W 2 i,l σ 2 δ )W 2 i,j σ 4 δ Similarly, L k=1 exp ( din l=1 W i,l δ k,l</formula><p>) is also a sum of L i.i.d. random variables for fixed W . By WLLN we have,</p><formula xml:id="formula_111">(1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )] p →(1 -p)( l=d l=1 E δ [exp (W i,l δ k,l )]) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) L p →(1 -p) exp ( din l=1 W 2 i,l σ 2 δ 2 ) v 2 = L k=1 exp ( d in l=1 W i,l δ k,l )δ k,j L L k=1 exp ( d in l=1 W i,l δ k,l ) L</formula><p>As for a given W, ϵ, both the numerator and denominator converge in distribution and denominator is converging to a constant by Slutskys theorem,</p><formula xml:id="formula_112">v 2 |W, ϵ d →N ( µ num (1 -p) exp ( d in l=1 W 2 i,l σ 2 δ 2 ) , σ 2 num L(1 -p) 2 exp ( din l=1 W 2 i,l σ 2 δ ) ) v 2 |W, ϵ d →N (W i,j σ 2 δ , exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L )</formula><p>Thus we have,</p><formula xml:id="formula_113">O i,j |W, ϵ ∼ N (W i,j σ 2 δ , exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L ) + ϵ j</formula><p>We have,</p><formula xml:id="formula_114">E[O i,j |W ] = W i,j σ 2 δ + 0 = W i,j σ 2 δ E[O 2 i,j |W ] = exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L + σ 2 ϵ E[O i,j ] = E W [O i,j |W ] = E W [W i,j σ 2 δ ] = 0 E[O 2 i,j ] = E W [O 2 i,j |W ] = E W [W 2 i,j σ 4 δ + exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L + σ 2 ϵ ]</formula><p>For large d in by WLLN and continuous mapping theorem exp (</p><formula xml:id="formula_115">din l=1 W 2 i,l σ 2 δ ) ≈ exp (d in σ 2 w σ 2 δ ) = (L -1)σ 2 w σ 4 δ + exp (dinσ 2 w σ 2 δ )(4σ 2 w σ 4 δ +σ 2 δ ) (1-p) L + σ 2 ϵ = (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 qk + exp ((1-r l x in )d 2 in σ 4 x in σ 2 qk )(4(1-r l x in ) 2 dinσ 6 x in σ 2 qk +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin</formula><p>Hence,</p><formula xml:id="formula_116">µ xout = 0 σ 2 xout = (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 qk + exp ((1-r l x in )d 2 in σ 4 x in σ 2 qk )(4(1-r l x in ) 2 dinσ 6 x in σ 2 qk +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin</formula><p>Now to get covariance we make two approximations. As the term</p><formula xml:id="formula_117">L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) (1-p) L k=1 exp ( d in l=1 W i,l δ k,l )</formula><p>converges to 1, we approximate v 1i,j ≈ ϵ j . Also we will treat</p><formula xml:id="formula_118">L k=1 exp ( din l=1 W i,l δ k,l ) ≈ exp ( d in l=1 W 2 i,l σ 2 δ 2</formula><p>). Then, we have</p><formula xml:id="formula_119">v 1i,j ≈ ϵ j v 2i,j ≈ L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j L (1 -p) exp ( d in l=1 W 2 i,l σ 2 δ 2 )</formula><p>This makes v 1i,j and v 2i,j independent. For covariance</p><formula xml:id="formula_120">E[O i,j O m,j ] = E W [E[O i,j O m,j |W ]] O i,j O m,j |W = (v 1i,j + v 2i,j )(v 1m,j + v 2m,j ) = v 1i,j v 1m,j + v 1i,j v 2m,j + v 2i,j v 1m,j + v 2i,j v 2m,j v 1i,j v 1m,j = ϵ 2 j E[v 1i,j v 1m,j |W ] = σ 2 ϵ As v 1i,j = v 1m,j = ϵ j , v 1i,j v 2m,j + v 2i,j v 1m,j = ϵ j (v 2i,j + v 2m,j</formula><p>), and ϵ j is independent of (v 2i,j + v 2m,j ). Thus, we have</p><formula xml:id="formula_121">E[v 1i,j v 2m,j + v 2i,j v 1m,j |W ] = E[ϵ j |W ]E[(v 2i,j + v 2m,j )|W ] = 0 * E[(v 2i,j + v 2m,j )|W ] = 0 v 2i,j v 2m,j = L k1=1 (1-d i,k 1 ) exp ( d in l=1 W i,l δ k 1 ,l )δ k 1 ,j L L k2=1 (1-d m,k 2 ) exp ( d in l=1 W m,l δ k 2 ,l )δ k 2 ,j L (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) E[v 2i,j v 2m,j |W ] = E[ L k1=1 (1 -d i,k1 ) exp ( din l=1 W i,l δ k1,l )δ k1,j L k2=1 (1 -d m,k2 ) exp ( din l=1 W m,l δ k2,l )δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 )</formula><p>Breaking summation into two parts:</p><formula xml:id="formula_122">k 1 = k 2 = k and k 1 ̸ = k 2 , we get = E[ L k=1 (1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + E[ L k1=1 L k2=1,k2̸ =k1 (1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) = L k=1 E[(1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + L k1=1 L k2=1,k2̸ =k1 E[(1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) E[(1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] = = E[(1 -d i,k )]E[(1 -d m,k )]E[exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] = (1 -p) 2 exp ( din l=1 (W i,l + W m,l ) 2 σ 2 δ 2 )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) E[(1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] = E[(1 -d i,k1 )]E[(1 -d m,k2 )]E[exp ( din l=1 W i,l δ k1,l )δ k1,j ]E[exp ( din l=1 W m,l δ k2,l )δ k2,j ] = (1 -p) 2 exp ( din l=1 (W 2 i,l + W 2 m,l )σ 2 δ 2 )W i,j W m,j σ 4 δ E[v 2i,j v 2m,j |W ] = exp ( d in l=1 (W i,l +W m,l ) 2 σ 2 δ 2 )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + (L -1) exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 )W i,j W m,j σ 4 δ L exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) = exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L So, we have E[O i,j O m,j |W ] = σ 2 ϵ + exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L E[O i,j O m,j ] = E W [E[O i,j O m,j |W ]] = E W [σ 2 ϵ + exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L ] = σ 2 ϵ + E W [ exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L ]</formula><p>For large values of d in by WLLN and continuous mapping theorem we have exp ( din l=1 W i,l W m,l σ 2 δ ) ≈ 1. Thus, we have</p><formula xml:id="formula_123">E[O i,j O m,j ] = σ 2 ϵ + (2σ 2 w σ 4 δ + σ 2 δ ) L E[O i,j O m,j ] = r l xin σ 2 xin + (2(1 -r l xin ) 2 d in σ 6 xin σ 2 qk + (1 -r l xin )σ 2 xin ) L</formula><p>The convergence arguments we have made require the scale of the variables to be small when compared to L and d in . The growth in scale can be controlled easily by controlling σ qk , and we observe that if we let σ qk become arbitrarily large the scores passed to Softmax diverge leading to degenerate attention only attending to one token which has the highest score. To avoid this degenerate attention, we choose smaller values of σ q , σ k and in that scenario, the approximate value for variance and covariance are,</p><formula xml:id="formula_124">σ 2 xout ≈ r l xin σ 2 xin Cov l xout ≈ r l xin σ 2 xin</formula><p>To get the final variance and covariance we can use results of Linear layer to account for W V . If we initialize σ q and σ k to be small, in initial phase of training the output of Softmax layer can be treated as being a constant =</p><formula xml:id="formula_125">1 T L 1 L L .</formula><p>Using this assumption we have,</p><formula xml:id="formula_126">X out ≈ Dropout( 1 T L 1 L L )X in W V =⇒ g Xin ≈ Dropout( 1 T L 1 L L ) T g Xout W V T = Dropout( 1 T L 1 L L )g Xout W V T µ gin = 0 σ 2 gin = σ 2 gout dσ 2 v L(1 -p) (1 + (L -1)r l gout (1 -p)) Cov l gin = σ 2 gout dσ 2 v L (1 + (L -1)r l gout )</formula><p>B Moment Propagation through Transformer Blocks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Transformer Attention Block</head><p>A forward pass through attention block consists of LayerNorm, followed by Scaled Dot-Product Attention, followed by an output projection layer (a Linear Layer), and finally a Dropout. Using the results from above we get,</p><formula xml:id="formula_127">µ xout = 0 * 0 * 0 * 0 = 0 σ 2 xout =   (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 dinσ 6 x in σ 2 q σ 2 k +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin   .d in σ 2 v . d in σ 2 o (1 -p) = d 2 in σ 2 o σ 2 v σ 2 xin (1 -p)   (1 -r l xin ) 2 (L -1)d in σ 4 xin σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin   Cov l xout = r l xin σ 2 xin + (2(1 -r l xin ) 2 d in σ 6 xin σ 2 q σ 2 k + (1 -r l xin )σ 2 xin ) L .d in σ 2 v .d in σ 2 o .1 = d 2 in σ 2 o σ 2 v σ 2 xin r l xin + (2(1 -r l xin ) 2 d in σ 4 xin σ 2 q σ 2 k + (1 -r l xin )) L σ 2 gin = σ 2 gout * 1 (1 -p) * d in σ 2 o * d in σ 2 v L(1 -p) (1 + (L -1)r l gout (1 -p)) = d 2 in σ 2 gout σ 2 v σ 2 o L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) Cov l gin = σ 2 gout * 1 * d in σ 2 o * d in σ 2 v L (1 + (L -1)r l gout ) = d 2 in σ 2 gout σ 2 v σ 2 o L (1 + (L -1)r l gout )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Transformer FFN Block</head><p>A forward pass through the FFN block of a transfer has a LayerNorm, then a Linear layer from d to 4d, which is then passed through a ReLU gate, the output of which is the projected back to d dimension using another Linear layer, and eventually passed through a Dropout. Again using the results from above we get, µ xout = 0 (Last Linear Layer makes it 0)</p><formula xml:id="formula_128">σ 2 xout = 1 * d in σ 2 w1 * ( π -1 2π + 1 2π ) * 4d in σ 2 w2 * 1 (1 -p) * σ 2 xin = 2d 2 in σ 2 w1 σ 2 w2 (1 -p) σ 2 xin Cov l xout = d in σ 2 w1 * ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π - 1 2π + 1 2π ) * 4d in σ 2 w2 * σ 2 xin = 4d 2 in σ 2 w1 σ 2 w2 σ 2 xin ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π ) r l xout = 2 * (1 -p) * ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π ) ≈ (1 -p) * ( r l xin 2 + 1 π + ( 1 2 - 1 π )r l xin 2 ) (Fitting a 2-nd order polynomial) σ 2 gin = σ 2 gout * 1 (1 -p) * d in σ 2 w2 * 1 2 * 4d in σ 2 w1 = 2d 2 in σ 2 w1 σ 2 w2 σ 2 gout (1 -p) Cov l gin = Cov l gout * 1 * d in σ 2 w2 * ( 1 4 + sin -1 (r l xin ) 2π ) * 4d in σ 2 w1 = 4d 2 in σ 2 w1 σ 2 w2 Cov l gout ( 1 4 + sin -1 (r l xin ) 2π )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Summary Table of Moment Propagation through Transformer Components</head><p>In Table <ref type="table" target="#tab_17">15</ref>, Table <ref type="table" target="#tab_18">16</ref>, Table <ref type="table" target="#tab_19">17</ref>, Table <ref type="table" target="#tab_8">18</ref>, Table <ref type="table" target="#tab_20">19</ref> and Table <ref type="table" target="#tab_21">20</ref>, we summarize the signal propagation formulae for all the transformer components.  </p><formula xml:id="formula_129">ReLU (π -1) (2π) σ 2 xin GeLU σ 2 x in 2π ( π 2 - σ 2 x in 1+σ 2 x in + sin -1 ( σ 2 x in 1+σ 2 x in ) + 2σ 2 x in (1+σ 2 x in ) √ 1+2σ 2 x in ) Layer Norm (d) 1 Dropout (p) σ 2 xin + pµ 2 xin 1 -p Softmax (e σ 2 x in (1-r l x in ) L L-1 -1)e 2σ 2 x in (1-r l x in ) L L-1 ((L-1)e σ 2 x in (1-r l x in ) +1) 2 SHA (without V) d in σ 2 xin (1 -p) (1-r l x in ) 2 (L-1)dinσ 4 x in σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 d in σ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin Attn Block (Approx) d 2 in σ 2 o σ 2 v σ 2 xin (1 -p) (1-r l x in ) 2 (L-1)dinσ 4 x in σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 d in σ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin FFN Block 2d 2 in σ 2 w1 σ 2 w2 σ 2 xin (1 -p)</formula><formula xml:id="formula_130">+ 1 2π sin -1 ( σ 2 x in σ 2 x in +1 ) + σ 2 x in (5σ 2 x in +3) 2π(σ 2 x in +1)(2σ 2 x in +1) 3 2 σ 2 gout LayerNorm (d) σ 2 gout σ 2 xin Dropout (p) 1 1 -p σ 2 gout Softmax ( (e σ 2 x in (1-r l x in ) L L-1 -1)e 2σ 2 x in (1-r l x in ) L L-1 ((L-1)e σ 2 x in (1-r l x in ) +1) 2 + 1 L 2 )σ 2 gout SHA Block (without V) d in σ 2 gout L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) Attn Block (Approx) d 2 in σ 2 gout σ 2 v σ 2 o L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) FFN Block 2d 2 in σ 2 w1 σ 2 w2 σ 2 gout (1 -p)</formula><p>Table <ref type="table" target="#tab_8">18</ref>. Covariance (along sequence length) propagation through the components of transformer model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component</head><formula xml:id="formula_131">Cov l xout Embeddings N i * (N i -1) L * (L -1)) * σ 2 wembd FFN (d 1 .d 2 ) d 1 σ 2 w (Cov l xin + µ 2 xin ) ReLU ( 1 4 + sin -1 (r l xin ) 2π )Cov l xin -(1 -(1 -(r l xin ) 2 )) σ 2 xin 2π GeLU σ 2 x in 4π (πr l xin + 2r l xin sin -1 ( r l x in σ 2 x in σ 2 x in +1 ) + 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1+(r l x in ) 2 ) (σ 2 x in +1) √ (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 - 2σ 2 x in (σ 2 x in +1) ) LayerNorm (d) (1 - 1 d ) Cov l xin σ 2 xin Dropout (p) Cov l xin SHA (without V) d in σ 2 xin r l xin + (2(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) L Attn Block (Approx) d 2 in σ 2 o σ 2 v σ 2 xin r l xin + (2(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) L FFN Block 4d in σ 2 w1 σ 2 w2 σ 2 xin ( r l xin 4 + (1 -(r l xin ) 2 2π + r l xin sin -1 (r l xin ) 2π )</formula><formula xml:id="formula_132">+ 1 2π sin -1 ( r l x in σ 2 x in σ 2 x in +1 ) + r l x in σ 2 x in ((2σ 2 x in +3)(σ 2 x in +1)-2(r l x in σ 2 x in ) 2 ) 2π(σ 2 x in +1)((σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 ) 3 2 r l gout σ 2 gout LayerNorm (d) Cov l gout σ 2 xin Dropout (p) Cov l gout SHA Block (without V) d in σ 2 gout L (1 + (L -1)r l gout ) Attn Block (Approx) d 2 in σ 2 gout σ 2 v σ 2 o L (1 + (L -1)r l gout ) FFN Block 4d 2 in σ 2 w1 σ 2 w2 Cov l gout ( 1 4 + sin -1 (r l xin ) 2π )</formula><p>Table <ref type="table" target="#tab_1">22</ref>. Range of input variance/correlations used for theoretical formula verification reported in Table <ref type="table" target="#tab_1">21</ref> for the theoretical formulas corresponding to forward and backward pass through components of the transformer model. The dropout probability range was [0, 1) for Dropout and Single-Head Attention, and σ 2 w for FFN was [10 -2 , 10 2 ]/din. We will use the approximations listed in Table <ref type="table" target="#tab_1">2</ref> here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1.1 FORWARD PASS</head><p>For forward pass, a Transformer Pre-LN has LayerNorm followed by the Attention block, residual connection, LayerNorm, and then the FFN block. Let σ 2 layer be the output variance after 1 such layer, and σ 2 model be the output variance after the entire model of N layers.</p><formula xml:id="formula_133">σ 2 xattn = d 2 σ 2 o σ 2 v * r l xin (1 -p) σ 2 xffn = 2d 2 σ 2 w1 σ 2 w2 (1 -p) σ 2 xlayer = σ 2 xin + σ 2 xattn + σ 2 xffn = σ 2 xin + d 2 σ 2 o σ 2 v * r l xin (1 -p) + 2d 2 σ 2 w1 σ 2 w2 (1 -p) Let, C 1 = d 2 σ 2 o σ 2 v (1 -p) , C 2 = 2d 2 σ 2 w1 σ 2 w2 (1 -p)bu , Then, σ 2 xlayer = σ 2 xin + C 1 * r l xin + C 2</formula><p>As we discuss in Section 3.4, the correlation r l xin quickly reaches a stable constant maximum value r l xmax , which can be found using the calculations in Appendix F. Let r l xmin &gt; 0 be the minimum value of this correlation, let</p><formula xml:id="formula_134">C 3 = C 1 * r l xmax + C 2 , and C 4 = C 1 * r l xmin + C 2 . Then, σ 2 xin + C 4 ≤ σ 2 xlayer ≤ σ 2 xx in + C 3</formula><p>Hence after N layers,</p><formula xml:id="formula_135">σ 2 xin + N * C 4 ≤ σ 2 xmodel ≤ σ 2 xin + N * C 3 =⇒ σ 2 xmodel = Θ(N )<label>(2)</label></formula><p>This shows that output variance of Pre-LN will increase linearly with number of layers N .</p><p>In practice, because the correlation quickly reaches r l xmax , the variance of the entire model σ 2 xmodel ≈ σ 2 xin + N * C 3 .</p><p>Discussion: This has the effect that transformer blocks near the output can affect the model output much less, as the skip connection variance increases but block output variance is constant. We conjecture that parameters in these are hence not being utilized to their full potential. Specifically in case of Xavier initialization, C 1 = 2.2, C 2 = 0.4, r l xmax = 0.85. For large d, σ 2 xin will be negligibly small compared to σ 2 xlayer , so we have -</p><formula xml:id="formula_136">σ 2 xmodel ≈ C 3 * N ≈ (2.2 * 0.85 + 0.4)N ≈ 2.2N E.1.2 BACKWARD PASS</formula><p>For the backward pass, a Transformer Pre-LN gradient will first backpropagate through the FFN block, then gets rescaled by Layernorm, and added with the skip connection. It then backpropagates through the Attention block, gets rescaled by Layernorm, and finally added with the skip connection. Let σ 2 g,n be the gradient variance backpropagating from the n th layer, and σ 2 gmodel be the gradient variance after the entire model of N layers. For the Attention block, let σ 2 gattn,n-1 be the gradient backpropagating from the block. Then for long sequence length L we have -</p><formula xml:id="formula_137">σ 2 gattn,n-1 = d 2 σ 2 o σ 2 v * σ 2 gout,n L(1 -p) (1 + (L -1)r l gout,n ) ≈ d 2 σ 2 o σ 2 v * r l gout,l * σ 2 gout,n</formula><p>(1p) σ 2 gattn,n-1 is then rescaled by the Layernorm to give σ 2 gattn-layernorm,n-1 . As Layernorm scales gradient by the inverse of the input variance σ 2 xin,n-1 , which from the section above, we know is approximately σ 2 xin,n-1 = C 3 * (n -1). Then</p><formula xml:id="formula_138">σ 2 gattn,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1 ≈ C 1 * r l gout,n * σ 2 gout,n C 3 * (n -1)</formula><p>Therefore, the final gradient σ 2 gattn-layer,n-1 after addition with the skip connection is</p><formula xml:id="formula_139">σ 2 gattn-layer,n-1 = (1 + C 1 * r l gout,n C 3 * (n -1) )σ 2 gout,n</formula><p>Similarly, we can get σ 2 gffn-layer,n-1 for the ffn block. Then to get the gradient backpropagated through the entire layer σ 2 gout,n-1 , we have,</p><formula xml:id="formula_140">σ 2 gffn-layer,n-1 = (1 + C 2 C 3 * (n -1) )σ 2 gout,n σ 2 gout,n-1 = (1 + C 1 * r l gout,n C 3 * (n -1) )(1 + C 2 C 3 * (n -1) )σ 2 gout,n σ 2 gout,n-1 ≈ (1 + C 1 * r l gout,n C 3 * (n -1) + C 2 C 3 * (n -1) )σ 2 gout,n = (1 + C 1 * r l gout,n + C 2 C 3 * (n -1) )σ 2 gout,n = (1 + C 1 * r l gout,n + C 2 (C 1 * r l xin,n + C 2 ) * (n -1) )σ 2 gout,n = (1 + C gpre,n n -1 )σ 2 gout,n</formula><p>Where we ignore higher order terms for large n, and define C gpre,n = C1 * r l g out ,n +C2 C1 * r l x in ,n +C2 . Since C gpre,n &gt; 0, we will witness an increase in gradient going backward, and this increase is inversely proportional to the current layer n, matching with empirically observed growth (Figure <ref type="figure">2</ref>). Let C gpre,min = C2 C1+C2 = 0.15 be the minimum value of C gpre,n , and C gpre,max = C1+C2 C2 = 6.5 be the maximum. Then the above equation is bounded by:</p><formula xml:id="formula_141">(1 + C gpre,min n -1 )σ 2 gout,n ≤ σ 2 gout,n-1 ≤ (1 + C gpre,min n -1 )σ 2 gout,max</formula><p>Applying the above equation repeatedly until the final layer N , this recurrence can be approximately solved by treating σ 2 gout,n as a continuous function of n, taking logarithm of both sides, and integrating. This gives the following solution for σ 2 gout,n :</p><formula xml:id="formula_142">σ 2 gout,N * ( N n ) Cg pre,min ≤ σ 2 gout,n ≤ σ 2 gout,N * ( N n ) Cg pre,max</formula><p>If the correlation r l gout,n quickly reaches a stable constant maximum value r l gmax (approximately equal to but slightly less than r l xmax (Appendix F)), C gpre ≈ 1, and we get exactly hyperbolic growth as shown below:</p><formula xml:id="formula_143">σ 2 gout,n = σ 2 gout,N * ( N n )</formula><p>The gradient variance will increase hyberbolically with number of layers N while going backwards.</p><p>Discussion: This has the effect that much lower learning rate is required for the entire model, because the gradients near the input layers are much higher, slowing down learning and making the model unstable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Vanilla Post-LN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 FORWARD PASS</head><p>The forward pass of Post-LN is trivially always 1 at initialization, because the skip connection does not cross the LayerNorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 BACKWARD PASS</head><p>Following an analysis similar to that for Pre-LN, we get</p><formula xml:id="formula_144">σ 2 gffn-layer,n-1 = 1 + C 2 1 + C 1 * r l xout,n-1 σ 2 gout,n σ 2 gattn-layer,n-1 = 1 + C 1 * r l gout,n 1 + C 2 σ 2 gout,n σ 2 gout,n-1 = 1 + C 1 * r l gout,n 1 + C 2 * 1 + C 2 1 + C 1 * r l xout,n-1 * σ 2 gout,n = 1 + C 1 * r l gout,n 1 + C 1 * r l xout,n-1 σ 2 gout,n Let C 5,n = 1+C1 * r l g out,n</formula><p>1+C1 * r l</p><p>x out ,n-1</p><p>. As we discuss in Appendix F, the correlations both quickly reach a maximum stable value. But the r l gout,n 's maximum value r l gmax is slightly different than</p><formula xml:id="formula_145">r l xmax . Let C 5 = 1+C1 * r l gmax 1+C1 * r l xmax</formula><p>, then C 5 can be either greater or smaller than 1. Hence, we get</p><formula xml:id="formula_146">σ 2 gattn-layer,n-1 = C 5,n σ 2 gout,n = N i=n C 5,i σ 2 gout,N ≈ C (N -n) 5 σ 2 gout,N σ 2 gattn-layer,n-1 = C (N -n) 5 σ 2 gout,N<label>(3)</label></formula><p>This shows that gradient variance of Post-LN will decrease/increase exponentially with number of layers N while going backwards. Even very slightly different value of C 5 from 1, such as 0.96, will cause a 2000x fall in gradient after 200 layers.</p><p>Discussion: This shows why Post-LN transformer is much more difficult to train for deeper models than Pre-LN. While for Pre-LN the backwards gradient increases hyber-bolically to a maximum of N , in Post-LN the gradient can increase or decrease exponentially, stopping the model from converging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 DeepScaleLM Pre-LN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.1 FORWARD PASS</head><p>In DeepScaleLM, the weight initialization are chosen specifically so that σ 2 xattn and σ 2 xffn are both equal to 1 for all layers, by iteratively calculating r l xin as detailed in Appendix M. Also, the embeddings are initialized so that σ 2 xin is also 1. Hence,</p><formula xml:id="formula_147">σ 2 layer = λ 2 * σ 2 skip + β 2 * σ 2 block = λ 2 + β 2 = 1</formula><p>Hence the forward pass variance remains 1 throughout the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2 BACKWARD PASS</head><p>For the FFN-block, we have σ 2 xin,n-1 = σ 2 xout,n-1 = 1, as per equations in Table <ref type="table" target="#tab_1">2</ref> of the main paper. Similar to Vanilla-PreLN, we arrive at</p><formula xml:id="formula_148">σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1</formula><p>Here, σ 2 xin,n-1 = 1 as shown above, and since weights are initialized so that C1 * r</p><formula xml:id="formula_149">l xin = 1. Let C 6,n = r l g out ,n r l x out ,n-1 : σ 2 gattn-layernorm,n-1 = r l gout,n r l xin,n-1 * σ 2 gout,n = C 6,n * σ 2 gout,n</formula><p>Therefore, assuming no covariance between block gradients and skip connection (which will be true at initialization), the final gradient σ 2 gattn-layer,n-1 after addition with the skip connection is</p><formula xml:id="formula_150">σ 2 gattn-layer,n-1 = λ 2 σ 2 gout,n + β 2 σ 2 gattn-layernorm,n-1 = λ 2 σ 2 gout,n + β 2 C 6,n σ 2 gout,n = (λ 2 + β 2 C 6,n ) * σ 2 gout,n = (1 + C 6,n -1 N ) * σ 2 gout,n</formula><p>Similarly for the FFN layer, σ 2 gffn-layer,n-1 = σ 2 gout,n , as</p><formula xml:id="formula_151">σ 2 xin,n-1 = σ 2 xout,n-1 = 1. Hence, σ 2 gout,n-1 = (1 + C 6,n -1 N ) * σ 2 gout,n , σ 2 gout,1 = N i=1 (1 + C 6,n -1 N ) * σ 2 gout,N , ≈ N i=1 (1 + C 6 -1 N ) * σ 2 gout,N , ≈ (1 + C 6 -1 N ) N -1 * σ 2 gout,N , = e C6-1 * σ 2 gout,N ≈ σ 2 gout,N</formula><p>, where we applied (1 -k N ) N ≈ e -k , and C 6 ≈ 1.</p><p>Discussion: Hence for DeepScaleLM, the backward variance of gradient remains constant (bounded by a constant) across all layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 DeepScaleLM Post-LN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.1 FORWARD PASS</head><p>Same as vanilla Post-LN, this will remain preserved at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.2 BACKWARD PASS</head><p>Following an analysis similar to that for Vanilla Post-LN, we get</p><formula xml:id="formula_152">σ 2 gffn-layer,n-1 = σ 2 gout,n σ 2 gattn-layer,n-1 = (λ 2 * 1 + β 2 * C 1 * r l gout,n )σ 2 gout,n = (λ 2 + β 2 * r l gout,n r l xin,n )σ 2 gout,n σ 2 gout,n-1 = (λ 2 + β 2 * r l gout,n r l xin,n )σ 2 gout,n</formula><p>Similar to Pre-LN, we use the maximum value of these correlations, and assume C 6 = 1. We get</p><formula xml:id="formula_153">σ 2 gout,n-1 = (λ 2 + β 2 * r l gmax r l xmax )σ 2 gout,n = (λ 2 + β 2 C 6 )σ 2 gout,n ≈ (λ 2 + β 2 )σ 2 gout,n = σ 2 gout,n</formula><p>Hence for DeepScaleLM, the backward variance of gradient remains constant across all layers.</p><p>Discussion: Similar to DeepScale-LM Pre-LN, the assumption C 6 = 1 is not required, and yields the same constant bound if we do not assume it to be 1.</p><p>E.5 DeepScaleLM (Simplified) Pre-LN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5.1 FORWARD PASS</head><p>For simplified DeepScaleLM, the initialization for the FFN block does not change, so its output remains 1 same as DeepScaleLM. For the Attention block, we changed its initialization to mimic that of the FFN block. We will show that initially, simplified DeepScaleLM's forward pass is bounded.</p><formula xml:id="formula_154">σ 2 xffn = 1 as DeepScaleLM, σ 2 xattn = r l</formula><p>x in 2 . Therefore, the output variance after layer n will be</p><formula xml:id="formula_155">σ 2 xattn-skip,n = λ 2 * σ 2 xlayer,n-1 + β 2 * σ 2 xattn = (1 - 2 N ) * σ 2 xlayer,n-1 + 1 N * r l xin</formula><p>Similarly after the FFN block, the output skip will be -</p><formula xml:id="formula_156">σ 2 xlayer,n = λ 2 * σ 2 xattn-skip,n + β 2 * σ 2 xffn = (1 - 2 N ) * ((1 - 2 N ) * σ 2 xlayer,n-1 + 1 N * r l xin ) + 2 N * 1 = (1 - 2 N ) 2 * σ 2 xlayer,n-1 + (1 - 2 N ) * 1 N * r l xin + 2 N As correlation coefficient r l xin ≤ 1, we get, σ 2 xlayer,n ≤ (1 - 2 N ) 2 * σ 2 xlayer,n-1 + (1 - 2 N ) * 1 N * 1 + 2 N = (1 - 2 N ) 2 * σ 2 xlayer,n-1 + 3 N - 2 N 2 ≤ (1 - 2 N ) 2 * σ 2 xlayer,n-1 + 3 N</formula><p>Applying the above recurrence equation N times, we get</p><formula xml:id="formula_157">σ 2 xlayer,N ≤ (1 - 2 N ) 2N * σ 2 xlayer,0 + 3 N * N i=0 (1 - 2 N ) 2i = (1 - 2 N ) 2N * σ 2 xlayer,0 + 3 N * 1 -(1 -2 N ) 2N 1 -(1 -2 N ) 2</formula><p>Since λ 2 + β 2 = 1 and β 2 is small for large N. We can rewrite the above equations completely in terms of β as follows</p><formula xml:id="formula_158">σ 2 xlayer,N = (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 2 β 2 * 1 -(1 -β 2 ) 2N 1 -(1 -β 2 ) 2 (4) ≈ (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 4 (1 -(1 -β 2 ) 2N )<label>(5)</label></formula><p>For large N , we know (1 -k N ) N ≈ e -k . So the above becomes -</p><formula xml:id="formula_159">σ 2 xlayer,N ≈ e -4 * σ 2 xlayer,0 + 3 N * 1 -e -4 4 N -4 N 2 ≤ e -4 * σ 2 xlayer,0 + 3 N * 1 -e -4 4 N = e -4 * 1 + 3 4 * (1 -e -4 ) = 3 4 + 1 4e<label>4</label></formula><p>This gives us an upper bound on the output variance after N layers. By setting r l xin = 0 instead of 1 in the equation above, and proceeding similarly, we can also arrive at a lower bound of 1 2 + 1 2e 4 .</p><formula xml:id="formula_160">1 2 + 1 2e 4 ≤ σ 2 xlayer,N ≤ 3 4 + 1 4e 4<label>(6)</label></formula><p>Discussion Informally, this is because the attention block output variance will be between 0 and 0.5, and ffn block output always 1. Because of our λ, β scaling, the output will slowly converge to be in between the two outputs.</p><p>Note that the above derivation assumes no correlation between the block output and the skip connection. As we mentioned in our main paper, we do observe correlation between the input and the output. As such, theoretically, after every block, the variance σ 2 xlayer,n can increase by σ 2 xblock + σ 2 xlayer,n . This will cause the final output variance to increase by factors of 2 * √ N . In practice however, we observe the output variances to not grow too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5.2 BACKWARD PASS</head><p>Similar to DeepScaleLM Pre-LN, we arrive at</p><formula xml:id="formula_161">σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1 ≈ 0.5 * C 6 σ 2 xin,n-1 * σ 2 gout,n σ 2 gattn-layer,n-1 = λ 2 σ 2 gout,n + β 2 σ 2 gattn-layernorm,n-1 = (λ 2 + β 2 * 0.5 * C 6 σ 2 xin,n-1 ) * σ 2 gout,n = (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 -1)) * σ 2 gout,n</formula><p>Similarly, for the FFN layer, we get</p><formula xml:id="formula_162">σ 2 gffn-layer,n-1 = (1 + 2 N * ( 1 σ 2 xin,n-1 -1)) * σ 2 gout,n</formula><p>Multiplying these, we get</p><formula xml:id="formula_163">σ 2 gout,n-1 = (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 -1)) * (1 + 2 N * ( 1 σ 2 xin,n-1 -1)) * σ 2 gout,n ≈ (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 + 1 σ 2 xin,n-1 -2)) * σ 2 gout,n As 0.5 ≤ σ 2 xin,n-1 , we get -4 ≤ ( C6 σ 2 x in ,n-1 + 2 σ 2</formula><p>x in ,n-1 -4) ≤ 2C 6 + 2. Hence, on applying the above recurrence N times, we get</p><formula xml:id="formula_164">e -4 * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ e 2C6+2 * σ 2 gout,N</formula><p>Hence, we show that even for simplified DeepScaleLM Pre-LN, the maximum relative increase/fall in gradient variance is bounded across layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion:</head><p>The above derivations will also be valid if there is correlation in the input. Correlation will cause σ 2 xin,n-1 to increase, effectively decreasing the backpropagated gradient through the block to decrease (as Layernorm will scale by inverse of σ 2 xin,n-1 ). However, even in that case, our gradient will still be bounded by the above lower-bound. Intuitively, as the gradient can flow freely through the skip connection, hence, σ 2 gout,n-1 ≥ λ 4 * σ 2 gout,n , which when applied N times, yields σ 2 gout,1 ≥ e -4 * σ 2 </p><p>As -2 ≤ (r l gout,nr l xin,n ) ≤ 2, applying the above recurrence N times we get e -2 * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ e 2 * σ 2 gout,N Discussion: The above derivations assume no correlation in the input, and hence is only correct at initialization. However, if there is correlation between the block output and skip connection (r x ), the layernorm will cause σ 2 gout,n-1 to be down-scaled by a factor of 1 + 2 * rx √ N , where c is some constant, as opposed to 1 + 2 N above. However, if there is also correlation in the gradients of the block and skip connection (r g ), the numerator in the equations above for σ 2 gout,n-1 will also be increased, by a factor of 1 + 2 * rg √ N . Hence if the correlations among the gradients and among the output are similar, the above bounds will remain. If β 2 is set as 1 N 2 , then even if input correlations exist, the backward gradient will be bounded, following a similar derivation as above. However, we conjecture that this decreases the ability of the transformer layers to modify the skip connection too strongly, decreasing the "expressivity" of the model. This is similar to the approach of DSInit, which we show in our main paper does indeed decrease model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Rank Collapse and Correlation Analysis</head><p>In the previous sections, we derived the formulas that determine how the correlation will change through the Attention and FFN blocks both for forward and backward pass. Both attention and FFN blocks modify the correlation as shown in the Table <ref type="table" target="#tab_1">2</ref>.</p><p>Simplifying the formulae in the table above, we rewrite the output variance for the attention block as σ 2 xattn = C 1 * r l xin * σ 2 xin , and the output of the FFN block is σ 2 xffn = C 2 * σ 2 xin , where C 1 and C 2 are defined as follows.</p><formula xml:id="formula_166">C 1 = d 2 σ 2 o σ 2 v (1 -p) , C 2 = 2d 2 σ 2 w1 σ 2 w2 (1 -p) ,</formula><p>This also helps us to rewrite the backward pass as the σ 2 gattn = C 1 * r l gout * σ 2 gout and σ 2 gffn = C 2 * σ 2 gout . Specifically in case of Xavier initialization with 0.1 dropout, C 1 = 2.2, C 2 = 0.4.</p><p>Assuming a dropout of 0.1, the FFN block (with the ReLU) will reduce the correlation if it rises above 0.64 (where r l xout &lt; r l xin for FFN block). And the attention block will never output a correlation higher than 0.9. Hence correlation will never reach 1, but rather a steady, stable value between ReLU's maximum correlation and that of the attention block. Dropout's effect in preventing rank collapse was also observed in <ref type="bibr" target="#b91">(Rong et al., 2020)</ref>.</p><p>We can approximate the stable value of correlation after many layers based on the weightage average of the correlation in the Attention output and FFN output. When the attention output is added to the skip connection, the new correlation will be a weighted (by variance) average of the correlation among the tokens of attention output and among the tokens in the skip connection. And the same will happen after the FFN block.</p><p>A weighted average of the correlations of FFN and attention blocks gives the stable asymptotic correlation r l xmax</p><formula xml:id="formula_167">r l xmax = C 1 * (1 -p) + C 2 * (1 -p)( 1 π + r l xmax 2 + ( 1 2 - 1 π )r l xmax 2 ) C 1 + C 2</formula><p>Specifically for the case of xavier initialization, solving the above equation with C 1 = 2.2, C 2 = 0.4, gives r l xmax ≈ 0.88, as empirically verified in Figure <ref type="figure" target="#fig_3">8</ref>.</p><p>Similarly, the correlation for backward gradient will also converge at a stable value r l gmax , obtained by solving the below equation -</p><formula xml:id="formula_168">r l gmax = C 1 * (1 -p) + C 2 * (1 -p)( 1 2 + sin -1 (r l xmax ) π )r l gmax C 1 + C 2</formula><p>Specifically for the case of xavier initialization, this gives r l gmax = 0.87. Note how r l gmax ≈ r l xmax .</p><p>Discussion on rank collapse observed in <ref type="bibr" target="#b81">Noci et al. (2022</ref><ref type="bibr" target="#b81">) Noci et al. (2022)</ref> focuses primarily on linear activation, we theoretically analyze the change in output correlation caused by ReLU. We find that ReLU (or any asymmetric non-linearity in general) critically affects correlation. As our closed form expressions suggest, both FFN block (because of ReLU) and dropout reduce the correlation. While <ref type="bibr" target="#b81">Noci et al. (2022)</ref> mentions the use of dropout, as we show above and observe empirically in Figure <ref type="figure" target="#fig_3">8</ref>, rank will not collapse with dropout, and perhaps <ref type="bibr" target="#b81">Noci et al. (2022)</ref> did not use dropout.</p><p>We replicated the experimental settings of <ref type="bibr" target="#b81">Noci et al. (2022)</ref> without dropout, and observed that the rank collapse occurs due to incorrect initialization. They use a rather non-standard version of xavier initialization -instead of 2 f anin+f anout , they use 1 f anout . Hence, they initialize a much higher value for V as f an in is much greater than f an out ("Number of heads" times greater), and this results in variance of the output of the attention block C1 being much higher than FFN C2. As attention block outputs a much higher correlation than the FFN block, increasing its output variance without using dropout will result in rank collapse. This highlights the criticality of correct initialization, as well as the explainability power of our theoretical framework proposed in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Discussion of Relative Strength</head><p>In Equation <ref type="formula" target="#formula_159">4</ref>, we discussed that the backward recurrence equation for PreLN can be written as</p><formula xml:id="formula_169">σ 2 xlayer,N ≈ (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 4 (1 -(1 -β 2 ) 2N )</formula><p>Replacing β 2 = k N α and using (1 + k N α ) N = e kN 1-α , we get Hence, for N layers, the gradient fall/growth is again O(e ±kN 1-α ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Applying DeepscaleLM to Vision Transformers</head><p>Applying our method to vision transformers (for eg. ViT <ref type="bibr" target="#b29">(Dosovitskiy et al., 2021)</ref> or DeiT <ref type="bibr">(Touvron et al., 2021a)</ref>) will only require handling the input embeddings section Appendix A.1 -For ViT, this is a simple linear projection. Given normalized image inputs, our Linear section Appendix A.2 provides formulae to calculate the variance and correlation of the embeddings which are input to the model.</p><p>We empirically verified that for images from ImageNet, the embeddings after the linear projection do indeed follow the normal distribution, with an R 2 of 0.95. Furthermore, normalizing images to have approximately unit variance, given linear weights initialized by 1 d , the output variance was observed as 1.02 (within 2% error). While we used Zipf's law to estimate input embedding correlation for text, this could simply be empirically measured for vision after the embedding layer -we measured this to be 0.46 using the code provided by <ref type="bibr" target="#b7">Beyer et al. (2022)</ref>.</p><p>Using this measured value of input correlation, we can apply our DSLM method to ViT. As we show in Figure <ref type="figure">11</ref>, our method successfully controls both the forward and backward moments for the ViT model with 100s of layers. but our work will also prevent rank collapse at initialization caused by the very structure of the transformer model, in particular increase in correlation caused by both attention and ReLU/GeLU. The methods in these works are orthogonal to our approach, and our equations can be easily extended to cover the architectural modifications suggested in these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Discussion of Approximations and Assumptions</head><p>L.1 Illustrative Approximations of Full Formulae in Main Paper Some values listed in Table <ref type="table" target="#tab_0">1</ref> are approximations/illustrative simplifications of their full closed forms in Appendix C and Appendix A. We discuss all of these below.</p><p>• For ReLU forward correlation, we used a simple polynomial regression of the closed form formula. This simple regression is a remarkably good fit, as shown in figure Figure <ref type="figure">12</ref>, and can be reproduced in using our released code.</p><p>• For layernorm, we ignored the factor of 1 compared to d, or 1/d compared to 1, assuming large enough hidden dimension d.</p><p>• For SHA without V, we used the final simplified formulae for σ 2 xout and output correlation from Appendix A.8. For the gradient, we further simplified the formulae in Appendix A.8, assuming L ≈ L -1. Furthermore, the formulae provided in Table <ref type="table" target="#tab_1">2</ref> are approximate versions of the full formulae provided in Appendix C. In Table <ref type="table" target="#tab_1">2</ref>, we applied a similar approximation as done in Table <ref type="table" target="#tab_0">1</ref> for ReLU, from the full formula in Appendix C for output correlation. This polynomial approximation is also a very good fit, as shown in Figure <ref type="figure" target="#fig_0">13</ref>, and can be reproduced using our released code.</p><p>Our exact formulae for blocks and components also account for IID cases -as can be verified by our simulations, in which we do cover cases IID inputs with exactly 0 correlation, as noted in Corr l xin column in Table <ref type="table" target="#tab_1">22</ref>. In the simplified formulae, and in DeepScaleLM initialization and model, we simplified our formulae so that they only remain accurate for non-IID inputs. This was because of three considerations:</p><p>1. In NLP domain, most text will inevitably be non-IID due to repeated common words. This was encountered in all our experiments.</p><p>2. In Vision domain, for ViT in particular, there will be correlation among pixel intensities across patch embeddings, as discussed in common response section.</p><p>3. In Speech domain, similar to text, most speech will inevitably be non-IID due to repeated common sounds. N Hyper-parameters BERT Pretraining We used Megatron-LM's default BertWordPieceLowerCase tokenizer, with the original BERT lowercased vocab, and with trainable position embeddings. The same hyper-parameters (including LR schedule, warmup) were used for all models, and LR search over the range below was performed for all models. The final best models always had optimal LR within the range and not at the boundary of the LR range for all of our experiments. Detailed hyper-params are provided in Table <ref type="table" target="#tab_5">25</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 1. Pre-LN: Variance of forward signal increases linearly across layers N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. DeepScaleLM: The variances remain conserved for both forward and backward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 5. Backward gradient variance increases hyperbolically after 150k train steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>NFigure 8 .</head><label>8</label><figDesc>Figure 7. Forward r l x out for FFN and Attention blocks with p = 0.1. FFN reduces r l x out for r lx in &gt; 0.65, and attention always has r lx out &lt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure9. Gradient vanishes using λ 2 = 0.9 and β 2 = 0.1, after 50k training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>variance, and integral of odd function)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>10 2 , 10 3 ] [32, 64, 128, 256] [300, 10 4 ] E Moment Propagation through the Entire Transformer Model E.1 Vanilla Pre-LN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>variance for Post-LN is trivially bounded.E.6.2 BACKWARD PASSFollowing an analysis similar to that for DeepScaleLM Post-LN, we getσ 2 gout,n-1 = λ 2 + 0.5 * β 2 * r l gout,n λ 2 + 0.5 * β 2 * r lThe above equation can be rewritten in terms of β as followsσ 2 gout,n-1 = (1 + β 2 2 (r l gout,nr l xin,n ))σ 2 gout,n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>fall in gradient for β 2 = k N α is O(e kN 1-α ). Similarly for PostLN, we can use Equation7β2 ) * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ (1 + β 2 ) * σ 2 gout,N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Approximation of the Relu forward correlation formula</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 .</head><label>15</label><figDesc>Figure15. Pseudo-code for our proposed method DeepScaleLM: We scale the block output and the skip connection before adding, and keep track of correlation across layers. We appropriately initialize the weights. (N : num of layers, d: model hidden dimension, p: dropout probability, r lx in is calculated based on expressions provided in subsection A.1.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Signal propagation for forward and backward passes through components of a transformer (GeLU in Appendix A.5). The expressions here are illustrative simplification of full closed form formulae in Appendices A and C.</figDesc><table><row><cell>Component</cell><cell>µ xout</cell><cell>σ 2 xout</cell><cell></cell><cell cols="2">σ 2 gin</cell><cell>r l xout</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>r l gin</cell><cell></cell></row><row><cell>Embeddings Linear (d in → d out )</cell><cell>0 0</cell><cell cols="2">σ 2 wembd w (σ 2 d in σ 2 xin + µ 2 xin )</cell><cell cols="2">-d out σ 2 w σ 2 gout</cell><cell>π 2 18  *  log(|V |) 2 + r l xin + µ 2 xin /σ 2 xin 1 + µ 2 xin /σ 2 xin</cell><cell>2 9</cell><cell></cell><cell></cell><cell></cell><cell>-r l gout</cell><cell></cell></row><row><cell>ReLU</cell><cell>σ xin (2π)</cell><cell>(π -1) (2π)</cell><cell>σ 2 xin</cell><cell>1 2</cell><cell>σ 2 gout</cell><cell>0.7r l xin + 0.3r l xin</cell><cell>2</cell><cell>(</cell><cell>1 2</cell><cell>+</cell><cell>sin -1 (r l xin ) π</cell><cell>)r l gout</cell></row><row><cell>LayerNorm (d)</cell><cell>0</cell><cell>1</cell><cell></cell><cell cols="2">σ 2 gout σ 2 xin</cell><cell>r l xin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>r l gout</cell><cell></cell></row><row><cell>Dropout (p) SHA-without V</cell><cell>µ xin 0</cell><cell cols="2">σ 2 xin + pµ 2 xin 1 -p r l xin σ 2 xin</cell><cell cols="2">1 1 -p r l gout σ 2 σ 2 gout gout</cell><cell>r l xin (1 -p) 1 + pµ 2 xin /σ 2 xin 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1 -p)r l gout 1</cell><cell></cell></row><row><cell>Softmax</cell><cell>L 1</cell><cell>e (1-r d x in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Moment Propagation through the blocks of a transformer layer. Exact closed forms / proofs are provided in Appendices B and C.</figDesc><table><row><cell>Component</cell><cell>σ 2 xout</cell><cell>r l xout</cell><cell>σ 2 gin</cell><cell>r l gin</cell></row><row><cell>Attention Block</cell><cell>d 2 σ 2 o σ 2 v σ 2 xin  *  r l xin</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance (perplexity) of BERT models with different shapes. Deep-Thin models provide large improvements with fewer parameters.model outperforms the vanilla 12-layer, and our 96 layer outperforms the vanilla 24-layer model. The 160M 192-layer model outperforms the vanilla 24-layer 336M model with more than 2× the params.Pre-training Improvements for Pre-LN We also applied DSLM to the deep Pre-LN models, trained for 3.3B tokens. Table5show that DSLM significantly improves the performance of the Pre-LN model across a range of model depths.</figDesc><table><row><cell cols="5">Model N/D 12/1024 48/512 192/256 768/128</cell></row><row><cell>(# Params)</cell><cell>(185M)</cell><cell>(168M)</cell><cell>(160M)</cell><cell>(156M)</cell></row><row><cell>Baseline</cell><cell>14.2</cell><cell>14.8</cell><cell>17.2</cell><cell>diverge</cell></row><row><cell>DSLM</cell><cell>15.5</cell><cell>13.1</cell><cell>12.9</cell><cell>18.4</cell></row><row><cell cols="4">Model N/D 24/1024 96/512 384/256</cell><cell>-</cell></row><row><cell>(# Params)</cell><cell>(336M)</cell><cell>(319M)</cell><cell>(311M)</cell><cell>-</cell></row><row><cell>Baseline</cell><cell>13.2</cell><cell cols="2">diverge diverge</cell><cell>-</cell></row><row><cell>DSLM</cell><cell>14.0</cell><cell>11.7</cell><cell>12.3</cell><cell>-</cell></row></table><note><p>Our method is comparable to the baseline for shallow models but starts to outperform as the model gets deeper. Our</p><p>192-layer</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>DSLM with Pre-LN Models.</figDesc><table><row><cell cols="5">Model N/D 12/512 96/512 192/256 768/128</cell></row><row><cell>Baseline</cell><cell>29.4</cell><cell>20.6</cell><cell>19.8</cell><cell>26.9</cell></row><row><cell>DSLM</cell><cell>26.0</cell><cell>15.4</cell><cell>17.0</cell><cell>25.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>BERT-base (trained for 64B tokens) pre-training and finetuning results (mean accuracy across 5 runs with stderr).</figDesc><table><row><cell>Dataset</cell><cell>Baseline</cell><cell>DSLM</cell></row><row><cell cols="3">Pretraining Performance</cell></row><row><cell>Validation PPL</cell><cell>8.3</cell><cell>7.8</cell></row><row><cell cols="2">Finetuning Accuracy</cell><cell></cell></row><row><cell>MNLI</cell><cell>82.4 ± 0.1</cell><cell>83.7 ± 0.1</cell></row><row><cell>QQP</cell><cell cols="2">90.8 ± 0.03 91.1 ± 0.05</cell></row><row><cell cols="2">RACE-Middle 71.1 ± 0.2</cell><cell>74.0 ± 0.3</cell></row><row><cell>RACE-High</cell><cell>63.7 ± 0.1</cell><cell>65.7 ± 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Accuracy on MNLI after low rank finetuning using LoRA</figDesc><table><row><cell>Model</cell><cell></cell><cell>Model Size</cell><cell>Score (Accuracy)</cell></row><row><cell></cell><cell cols="2">Layers (N) Hidden Dim (d)</cell><cell></cell></row><row><cell>Baseline DSLM</cell><cell>12 48</cell><cell>768 384</cell><cell>82.2 ±0.1 82.9 ±0.1</cell></row><row><cell cols="4">4.2 Improvements on Decoder-only Models (GPT)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Application of DSLM to Decoder-only model (GPT), while increasing model depth to 4x (token-level PPL).</figDesc><table><row><cell>Model</cell><cell></cell><cell>Model Size</cell><cell></cell><cell cols="2">LM Perplexity</cell></row><row><cell></cell><cell cols="5">Layers (N) Dim (d) Params Pre-LN Post-LN</cell></row><row><cell>Baseline</cell><cell>12</cell><cell>1024</cell><cell>204M</cell><cell>11.6</cell><cell>12.7</cell></row><row><cell>DSLM</cell><cell>12</cell><cell>1024</cell><cell>204M</cell><cell>11.5</cell><cell>11.5</cell></row><row><cell>DSLM</cell><cell>48</cell><cell>512</cell><cell>178M</cell><cell>11.2</cell><cell>11.7</cell></row><row><cell>Baseline</cell><cell>24</cell><cell>1024</cell><cell>355M</cell><cell>10.4</cell><cell>11.6</cell></row><row><cell>DSLM</cell><cell>24</cell><cell>1024</cell><cell>355M</cell><cell>10.2</cell><cell>10.5</cell></row><row><cell>DSLM</cell><cell>96</cell><cell>512</cell><cell>329M</cell><cell>10.1</cell><cell>10.6</cell></row><row><cell cols="6">4.3 Improvements on Speech (Encoder-Decoder)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Application of DSLM to Speech-to-Text translation. Nenc and Ndec refer to number of layers in the encoder and the decoder respectively. For models marked with *, maximum source sequence length was limited to 1024 due to compute limitations, and longer examples were discarded for both train and test.</figDesc><table><row><cell>Model</cell><cell>Lang</cell><cell cols="2">Model Size</cell><cell></cell><cell>BLEU</cell></row><row><cell></cell><cell></cell><cell cols="3">N enc , N dec Dim (d) Params</cell><cell></cell></row><row><cell>Baseline Pre-LN</cell><cell>en→de</cell><cell>12,6</cell><cell>256</cell><cell>31.1M</cell><cell>24.9</cell></row><row><cell>DSLM Pre-LN</cell><cell>en→de</cell><cell>48,24</cell><cell>128</cell><cell>28.4M</cell><cell>25.6</cell></row><row><cell cols="2">Baseline Post-LN en→de</cell><cell>12,6</cell><cell>256</cell><cell>31.1M</cell><cell>21.9</cell></row><row><cell>DSLM Post-LN</cell><cell>en→de</cell><cell>48,24</cell><cell>128</cell><cell>28.4M</cell><cell>23.8</cell></row><row><cell cols="2">Baseline Pre-LN* en → es DSLM Pre-LN* en → es Baseline Pre-LN* en → fr DSLM Pre-LN* en → fr</cell><cell>12,6 48,24 12,6 48,24</cell><cell>256 128 256 128</cell><cell>31.1M 28.4M 31.1M 28.4M</cell><cell>21.61 23.03 23.74 26.30</cell></row><row><cell cols="6">pendix H). Using ImageNet-1k (Russakovsky et al., 2015)</cell></row><row><cell cols="6">data with ViT (Dosovitskiy et al., 2021) model, our method</cell></row><row><cell cols="6">can also constrain the growth of moments in Vision Trans-</cell></row><row><cell cols="3">formers, as we show in Figure 11.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">We train our models on the Image Classification task us-</cell></row><row><cell cols="6">ing ViT baselines provided by Beyer et al. (2022), and</cell></row><row><cell cols="6">trained a 4x deeper model with same params. The deeper</cell></row><row><cell cols="6">DSLM model outperforms the baseline ViT both in both</cell></row><row><cell cols="6">90 and 300 epoch settings. The improvements also trans-</cell></row><row><cell cols="5">late to improved robustness on ImageNet-v2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Applying DSLM to Image classification using ViT.</figDesc><table><row><cell>Eval Set</cell><cell cols="2">90-epoch</cell><cell cols="2">300-epoch</cell></row><row><cell></cell><cell cols="4">Baseline DSLM Baseline DSLM</cell></row><row><cell>ImageNet</cell><cell>76.5</cell><cell>77.2</cell><cell>79.8</cell><cell>80.3</cell></row><row><cell>ImageNet-Real</cell><cell>83.2</cell><cell>83.8</cell><cell>85.4</cell><cell>85.5</cell></row><row><cell>ImageNet-v2</cell><cell>63.7</cell><cell>65.2</cell><cell>67.9</cell><cell>68.3</cell></row><row><cell>ImageNet-R</cell><cell>23.9</cell><cell>24.4</cell><cell>27.8</cell><cell>28.3</cell></row><row><cell>ImageNet-Sketch</cell><cell>24.4</cell><cell>25.5</cell><cell>28.7</cell><cell>29.9</cell></row><row><cell cols="3">4.5 Comparison with Prior Methods</cell><cell></cell><cell></cell></row><row><cell cols="5">In Table 11, we compare DSLM with several prior meth-</cell></row><row><cell cols="5">ods for deep transformers. DSInit and DeepNet stabilize</cell></row><row><cell cols="5">the model training at the expense of reduced "sensitivity"</cell></row><row><cell cols="5">(Section 4.6) by using smaller effective values of β 2 , at</cell></row><row><cell cols="5">O(N -2 ) and O(N -1.5 ) respectively. Interestingly, 96-layer model diverges with DSInit, despite DSInit using a</cell></row><row><cell cols="5">smaller β asymptotically -this is because the constants hid-</cell></row><row><cell cols="5">den in O(N -2 ) are much larger for DSInit. Our method, by analysing signal propagation, sets constants exactly at 1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Comparison with prior methods for deep Transformers.</figDesc><table><row><cell>Method</cell><cell>192/256 96/512</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 .</head><label>12</label><figDesc>Model performance on direct casting to FP8</figDesc><table><row><cell>Model</cell><cell>FP32</cell><cell>E5M2</cell><cell>E4M3</cell></row><row><cell cols="4">Baseline 14.8 42.5 (∆ 27.7) 16.5 (∆ 1.7)</cell></row><row><cell>DSLM</cell><cell cols="3">13.1 21.4 (∆ 8.3) 13.9 (∆ 0.8)</cell></row><row><cell cols="4">Ablation of Residual Scaling Table 13 provides the re-</cell></row><row><cell cols="4">sults corresponding to the different components of our pro-</cell></row><row><cell cols="4">posed DSLM scheme for training 96-layer 512-d model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 .</head><label>13</label><figDesc>Ablation of various DeepScaleLM components.</figDesc><table><row><cell>Model</cell><cell>Perf</cell></row><row><cell cols="2">Vanilla Xavier (with or w/o β 2 = 0.5) diverge</cell></row><row><cell>DSLM-Init (with or w/o β 2 = 0.5)</cell><cell>diverge</cell></row><row><cell>DSLM-Init + β 2 = 2 N (learnable β) DSLM-Init + β 2 = 2 N (fixed β)</cell><cell>12.2 11.7</cell></row><row><cell cols="2">Ablation of Initialization Table 14 provides ablation re-</cell></row><row><cell cols="2">sults for our proposed initialization. All experiments in</cell></row><row><cell cols="2">Table 14 were conducted for the Pre-LN model with our</cell></row><row><cell cols="2">proposed scaling (λ, β), since the Post-LN model diverged</cell></row><row><cell cols="2">with Xavier initialization. Xavier initialization performs</cell></row><row><cell cols="2">significantly worse for very deep models, due to higher QK</cell></row><row><cell cols="2">initialization. BERT default initialization with σ = 0.02</cell></row><row><cell cols="2">also performs worse. Finally, DSLM simpler initialization</cell></row><row><cell>performs comparably to DSLM.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 .</head><label>14</label><figDesc>Ablation of the initializations.</figDesc><table><row><cell>Model</cell><cell cols="2">Model Size (N/d) Perf</cell></row><row><cell>Xavier</cell><cell>192/256 (160M)</cell><cell>38.2</cell></row><row><cell>DSLM</cell><cell>192/256 (160M)</cell><cell>17.0</cell></row><row><cell>DSLM (simple)</cell><cell>192/256 (160M)</cell><cell>17.9</cell></row><row><cell>Fixed σ = 0.02</cell><cell>96/512 (319M)</cell><cell>20.5</cell></row><row><cell>DSLM</cell><cell>96/512 (319M)</cell><cell>17.9</cell></row><row><cell cols="3">Compute Appendix I provides detailed theoretical and</cell></row><row><cell cols="3">wall-clock compute overheads for making models deeper.</cell></row><row><cell cols="3">We observe that up to 200 layers, the theoretical compute</cell></row><row><cell cols="3">is within 6 -7% and wall-clock times is within 15% of the original shallow model. While our 192-layer 256-d model</cell></row><row><cell cols="3">requires 6% extra compute than the 12-layer 1024-d model,</cell></row><row><cell cols="3">it manages to outperform the 24-layer 1024-d model, that</cell></row><row><cell cols="3">has 62.5% more parameters, at equal wall-clock time and at</cell></row><row><cell>equal number of tokens.</cell><cell></cell><cell></cell></row><row><cell cols="3">Discussion of Relative Strength In general, for a β of the</cell></row><row><cell cols="3">form β 2 = k N α , we can choose from a wide range of values for the constant k and exponent α. There is an expressivity-</cell></row><row><cell cols="3">trainability trade-off in training deep networks</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 .</head><label>15</label><figDesc>Moment Propagation (mean) during forward pass through components of transformer model.</figDesc><table><row><cell>Component</cell><cell>µ xout</cell></row><row><cell>Embeddings</cell><cell>0</cell></row><row><cell>FFN (d 1 .d 2 )</cell><cell>0</cell></row><row><cell>ReLU</cell><cell>σ xin (2π)</cell></row><row><cell>GeLU</cell><cell>σ 2 xin 2π(σ 2 xin + 1)</cell></row><row><cell>LayerNorm (d)</cell><cell>0</cell></row><row><cell>Dropout (p)</cell><cell>µ xin</cell></row><row><cell>Softmax</cell><cell>1 L</cell></row><row><cell>SHA Block (without V)</cell><cell>0</cell></row><row><cell>Attn Block</cell><cell>0</cell></row><row><cell>FFN Block</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 .</head><label>16</label><figDesc>Moment Propagation (variance) during forward pass through components of transformer model.</figDesc><table><row><cell>Component</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 .</head><label>17</label><figDesc>Moment Propagation (variance) during backwards pass through components of transformer model.</figDesc><table><row><cell>Component</cell><cell cols="2">σ 2 gin</cell></row><row><cell>Embeddings</cell><cell></cell><cell>-</cell></row><row><cell>FFN (d 1 .d 2 )</cell><cell cols="2">d 2 σ 2 w σ 2 gout</cell></row><row><cell>ReLU</cell><cell>1 2</cell><cell>σ 2 gout</cell></row><row><cell>GeLU</cell><cell>1 4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 19 .</head><label>19</label><figDesc>Covariance (hidden dimension) propagation through the components of transformer model.</figDesc><table><row><cell>Component</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cov d xout</cell><cell></cell></row><row><cell>Embeddings</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>FFN (d 1 .d 2 )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>ReLU</cell><cell>(</cell><cell>1 4</cell><cell>+</cell><cell>sin -1 (r d xin ) 2π</cell><cell>)Cov d xin -(1 -(1 -(r d xin ) 2 ))</cell><cell>σ 2 xin 2π</cell></row><row><cell>GeLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LayerNorm (d) Dropout (p)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 d -1 Cov d -xin</cell><cell></cell></row><row><cell>SHA Block(without V )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>Attn Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>FFN Block</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 20 .</head><label>20</label><figDesc>Gradient covariance (along sequence length) propagation through the components of transformer model.</figDesc><table><row><cell>Component</cell><cell></cell><cell></cell><cell></cell><cell>Cov l gin</cell><cell></cell></row><row><cell>Embeddings</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>FFN (d 1 .d 2 )</cell><cell></cell><cell></cell><cell></cell><cell cols="2">d 2 σ 2 w Cov l gout</cell></row><row><cell>ReLU</cell><cell>(</cell><cell>1 4</cell><cell>+</cell><cell>sin -1 (r l xin ) 2π</cell><cell>)Cov l gout</cell></row><row><cell>GeLU</cell><cell>1 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code: https://github.com/akhilkedia/TranformersGetStable</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Quora Question Pairs dataset</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Dr. Kangwook Lee</rs> and <rs type="person">Dr. Joohyung Lee</rs> of <rs type="affiliation">Samsung Research, Seoul, Korea,</rs> for their guidance and leadership. We would also like to thank all the reviewers for their valuable feedback and suggestions, which helped greatly improve the paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Numerical Verification</head><p>We perform numerical verification for the formulae reported in Table <ref type="table">15</ref>, Table <ref type="table">16</ref>, Table <ref type="table">17</ref>, Table <ref type="table">18</ref>, Table <ref type="table">19</ref> and <ref type="table">Table 20</ref>. The parameter ranges have been provided in Table <ref type="table">22</ref>. For each parameter, 3-5 values were sampled uniformly (or log uniformly) across the range for numerical simulation. Table <ref type="table">21</ref> provides the percentage error corresponding to the 50 th , 90 th and 99 th percentile. These simulation results are all fully reproducible using our released code. Even at 99 percentile, no error (other than SHA backwards) is larger than 10%, verifying our assumptions.  <ref type="bibr">.4, 4.1, 7.8] [2.2, 13.3, 44.5] [1.3, 3.9, 7.4] [1.6, 4.5, 8</ref>.2] Table <ref type="table">23</ref> provides the exact compute for the models reported in Table <ref type="table">4</ref>. We follow the code provided by Electra <ref type="bibr" target="#b15">(Clark et al., 2020)</ref> to calculate the each model's compute (FLOPs). We observe that up to 200 layers, the extra compute is within 6 -7% of the original shallow model. This overhead will decrease the bigger the original model size, and become much smaller. For example, for a 5B params model with 24-Layers-4096d (a reasonable shape in contemporary models, for example, LLaMA 7B has 32L-4096D) has much less compute overhead -only 6.6% overhead at 96 layers, and 13.6% overhead at 192 layers.</p><p>Despite this wall-clock time overhead, due to large performance gains from increasing depth, the 160M params 192-L model from Table <ref type="table">4</ref> outperforms the vanilla 336M BERT-large 24-L model with 2x more params, even at equal wall times.</p><p>Furthermore, a large fraction of the performance improvements mentioned happen when increasing the number of model layers by 4x -and as shown above, the wall clock time overhead is minimal. Making standard models 4x more deep to 50 -100 layers, will provide a large fraction of performance gains without much overhead.</p><p>Note that this performance overhead seems to be dependent on the framework used -some frameworks may be less optimized for such deeper models and may incur additional overhead for small but deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Statistical Significance J.1 Error Bars for Pre-Training Experiments</head><p>In our initial experiments, we observed very little variation in performance across different runs -we conjecture that the model is trained on a large enough number of tokens for differences in initialization/data seed to not matter. We provide mean and standard error for the 12L-1024D Post-LN and DSLM models from Table <ref type="table">4</ref> below: As the variation was so small, and due to compute limitations, we did not run multiple runs for other experiments thereafter.</p><p>We also reported the best score for Baseline Post-LN, and the worst score for DSLM for the 12L-1024D models Table <ref type="table">4</ref> for a conservative comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J.2 Statistical Significance for Fine-tuning Experiments</head><p>Mean and standard errors for all downstream fine-tuning experiments were reported in Table <ref type="table">6</ref>. The differences are statistically significant at p &lt; 5% for all datasets except QQP. We demonstrate that this profiling is unnecessary, and can instead be done theoretically in DeepScaleLM. Furthermore, where output or gradient increases in some prior works with more layers (eg. for ADMIN <ref type="bibr">(Liu et al., 2020a)</ref>, grad decreases by O(N ) (increases by O(log(N )) for Pre-LN)), our method allows maintaining both unit output and equal gradient across all layers at initialization, and bounded during training. <ref type="bibr" target="#b120">Yang et al. (2021)</ref> proposed µP initialization such that updates to a layer are of the same order regardless of width. Their work was focused on enabling transfer of hyper-parameters across model widtd, and does not target solving pathologies inherent in deeper architectures -they do not model the impact of ReLU and Attention on correlation, and hence are unable to prevent rank-collapse at large depths. When applied to 100s of layers, µP diverges with rank collapse at initialization.  <ref type="bibr">(2022)</ref>. Our work also considers previously neglected effects of dropout, input correlation between tokens, non-linearity, QK initialization, and provides closed forms with verifiable correctness of this signal propagation. Ours is the first work to theoretically constrain the output and gradient to almost exactly unit without any profiling passes, showing the validity of our formulae and of our assumptions. <ref type="bibr" target="#b38">He et al. (2023)</ref> extends neural kernel methods of DKS <ref type="bibr" target="#b72">(Martens et al., 2021)</ref> to Transformers to model network behaviour, assuming the MLP to be linear in its effect on attention with respect to correlation. Q/C maps in kernel methods are similar to signal propagation, as expected moments are equivalent to q and m values of kernels <ref type="bibr" target="#b72">(Martens et al., 2021)</ref>. Our method relaxes these assumptions, and we show that considering the impact of ReLU/GeLU on correlation is critical to correctly modelling attention. In particular, our formulae show that an MLP block with GeLU will also increase correlation in the absence of dropout (the same setting as used in He et al. ( <ref type="formula">2023</ref>) ). At large depths, He et al. ( <ref type="formula">2023</ref>)'s method suffers from rank collapse (with their deeper models under-performing shallower ones), which our method successfully prevents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2 Signal Propagation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signal propagation in</head><p>We also account for cases with non-IID inputs that may occur due to segment/position embeddings or due to non-uniform token distributions in real data (that are distributed approximately per Zipf's law <ref type="bibr" target="#b52">Kingsley (1935)</ref>) -and find that this strongly affects output variance of the attention block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.3 Moment Control &amp; Residual Scaling</head><p>Bounded gradients, or normalizing per-layer gradients, have been shown to results in better/faster convergence <ref type="bibr" target="#b96">(Shen et al., 2020;</ref><ref type="bibr" target="#b124">Yu et al., 2017;</ref><ref type="bibr" target="#b122">You et al., 2017;</ref><ref type="bibr">2020)</ref> Scaling with λ 2 + β 2 = 1 to control moments have often been used for ResNets <ref type="bibr" target="#b6">(Balduzzi et al., 2017;</ref><ref type="bibr" target="#b101">Szegedy et al., 2017;</ref><ref type="bibr" target="#b35">Hanin &amp; Rolnick, 2018;</ref><ref type="bibr" target="#b4">Arpit et al., 2019;</ref><ref type="bibr">Zhang et al., 2019b;</ref><ref type="bibr" target="#b43">Hoedt et al., 2022)</ref>. <ref type="bibr" target="#b101">Szegedy et al. (2017)</ref> proposed to use any small β, <ref type="bibr" target="#b6">Balduzzi et al. (2017)</ref> proposed to set β 2 = 0.5, Bachlechner et al. ( <ref type="formula">2021</ref>) sets β = 0 and learnable. <ref type="bibr" target="#b19">De &amp; Smith (2020)</ref> showed that λ 2 = 0.5 is not sufficient to solve vanishing gradients.  <ref type="formula">2023</ref>), but this results in logarithmic bounds instead of constant for forward propagation if λ = 1 is used, and vanishing gradient for backward propagation otherwise.</p><p>Values of β 2 &lt; k N , such as (effectively) 1 N 2 for DSInit <ref type="bibr">(Zhang et al., 2019a)</ref> or 1 N 1.5 for DeepNet <ref type="bibr" target="#b112">(Wang et al., 2024)</ref> decrease sensitivity of the model, and may result in the model becoming "too linear". DeepNet shows performance improvements by making the model deeper, but keeping the hidden dimension constant. Our setting is much more strict -we keep the number of parameters (and hence compute) constant, and our method still show performance improves on making the model deeper. For example, DeepNet's 200 layer model is 3.2B params, whereas our 192 layer model is 160M params (20x smaller).</p><p>Sometimes, these β values are used in conjunction with λ = 1, such as in <ref type="bibr">Liu et al. (2020a;</ref><ref type="bibr">b)</ref>, but as shown in <ref type="bibr" target="#b38">He et al. (2023)</ref>, fully normalized residual connections with λ 2 + β 2 = 1 often perform better than those with λ = 1. We also observed lower performance with λ = 1 in our initial experiments, and hence we fully normalize the residual connections.</p><p>Our contribution goes beyond providing an optimal scaling scheme. Using the theoretical framework and closed-form expressions for moment propagation through both Pre-LN and Post-LN developed in this work, practitioners can make informed choices about using any of the scaling factors above based on the stability-performance tradeoffs, such as using a lower β for scenarios with high correlation, or using higher β with uncorrelated inputs. 4. Lastly, even if there is exactly 0 correlation in input, the very first attention layer and the first FFN layer in particular, will add correlations to the output, ensuring our simplified formulae hold reasonably accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.4 Other Network modifications for Deep Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.2 Assumptions and Approximations in Derivations</head><p>• Except for attention, softmax and LayerNorm all other derivations of transformer components -Embeddings, FFN, ReLU/GeLU, Dropout, FFN Block are fully exact, assuming only normal distribution of inputs, weights and gradients. We justify this normality assumption below:</p><p>1. Inputs: As the embeddings are lookup tables of token-ids, and embedding weights are initialized from Normal distribution in Xavier, the inputs to the transformer are normally distributed.</p><p>2. Gradients: As the model outputs are Normal, the softmax of the classification head results in a Log-Normal distribution for probabilities p, as shown in Appendix A.7. Since the cross-entropy loss is -log(p), we expect the loss (and hence the final gradient being back-propagated) being log(Log-Normal distribution), to be a Normal distribution. We also verify this empirically by checking the normality of the backpropagated gradients to the deepest transformer layer, and the gradients match the best-fit Normal distribution with an R 2 of 0.999, showing that the gradients are indeed Normally distributed.</p><p>3. Weights: Weights are initialized from Normal distribution in Xavier, and are hence Normal.</p><p>• For attention, softmax and LayerNorm, we assume the sequence length L and the hidden dimension d are large.</p><p>• For embeddings, we assumed Zipf's law to calculate initial input correlation in tokens, as well as assumed uniform distribution for segment lengths for next sentence prediction task of BERT. Note that this assumption is not strictly required, and can also be empirically observed and given as input to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M DeepScaleLM Pseudocode</head><p>## Define constants of DeepScaleLM  Reproducible Longer Pre-training and Finetuning Our released code provides exact scripts for both pre-training and all fine-tuning. We used all original/official hyper-params of BERT, except LR was increased for DSLM as mentioned previously.</p><p>Downstream Low Rank Finetuning Following QLoRA <ref type="bibr">(Dettmers et al., 2023)</ref>, we apply LoRA on all linear modules, with r = 32, α = 16, and searched for LR. All other hyper-parameters were kept the same as finetuning. We used the same number of epochs as finetuning for LoRA, but perhaps more epochs may result in even better scores -Hu et al. ( <ref type="formula">2022</ref>) used 30 epochs for LoRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vision ViT Training</head><p>We used ViT-S Baseline from <ref type="bibr" target="#b7">(Beyer et al., 2022)</ref> for ImageNet-1k along with its default hyperparameters. It uses an MLP head, a Global AvgPool and a fixed 2D sin-cos position embeddings. The same hyper-parameters were used for all the models. Detailed hyper-parameters are provided in   <ref type="table">27</ref> provides the hyperparameters used to train the Speech translation models, following those of official fairseq. The same hyper-parameters were used for all the models. We report the BLEU by averaging the weights of the last 10 checkpoints at the end of training.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sorting out lipschitz function approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/anil19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On exact computation with an infinitely wide neural net</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/dbc" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8139" to="8148" />
		</imprint>
	</monogr>
	<note>d84bfcfe2284ba11beffb853a8c4-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/arora19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalization propagation: A parametric technique for removing internal covariate shift in deep networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Kota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/arpitb16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1168" to="1176" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">How to initialize your network? robust initialization for weightnorm i&amp; resnets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="10900" to="10909" />
		</imprint>
	</monogr>
	<note>cc/paper/2019/hash/ e520f70ac3930490458892665cda6620-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rezero is all you need: fast convergence at large depth</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bachlechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v161/bachlechner21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Campos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Quaeghebeur</surname></persName>
		</editor>
		<meeting>the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2021-07-30">27-30 July 2021. 2021</date>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/balduzzi17b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11">2017. Australia, 6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Better plain vit baselines for imagenet-1k</title>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoInit: Analytic signal-preserving weight initialization for neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i6.25836</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting>the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="6823" to="6833" />
		</imprint>
	</monogr>
	<note>of AAAI&apos;23/IAAI&apos;23/EAAI&apos;23</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unit scaling: Out-ofthe-box low-precision training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luschi</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/blake23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="2548" to="2576" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pehlevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Characterizing signal propagation to close the performance gap in unnormalized resnets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=IX3Nnir2omJ" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Highperformance large-scale image recognition without normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v139/brock21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1059" to="1071" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20v.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiedel</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/22-1144.html" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="240" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ELECTRA: pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lipschitz normalization for self-attention layers with application to graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dasoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Virmaux</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/dasoulas21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2456" to="2466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-analytical approximations to statistical moments of sigmoid and softmax mappings of normal variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.00091" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Catformer: Designing stable transformers via sensitivity analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/davis21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2489" to="2499" />
		</imprint>
	</monogr>
	<note>Virtual Event PMLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>cc/paper/2020/hash/ e6b738eca0e6792ba8a9cbcba6c1881d-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pavetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Scaling vision transformers to 22 billion parameters</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">In</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Engelhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/dehghani23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="7480" to="7512" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Guiding attention for self-supervised learning with transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.419</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.419" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4676" to="4686" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The case for 4-bit precision: k-bit inference scaling laws</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/dettmers23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="7750" to="7774" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient finetuning of quantized LLMs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Qlora</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OUIFPHEgJU" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MuST-C: a Multilingual Speech Translation Corpus</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1202</idno>
		<ptr target="https://aclanthology.org/N19-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2012" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Effective Theory of Transformers at Initialization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.02034" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is not all you need: pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/dong21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2793" to="2803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A note on the implicit bias towards minimal depth of deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Galanti</surname></persName>
		</author>
		<idno>abs/2202.09028</idno>
		<ptr target="https://arxiv.org/abs/2202.09028" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The pile: An 800gb dataset of diverse text for language modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leahy</surname></persName>
		</author>
		<idno>abs/2101.00027</idno>
		<ptr target="https://arxiv.org/abs/2101.00027" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cramming: Training a language model on a single GPU in one day</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/geiping23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="11117" to="11143" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v9/glorot10a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
		<title level="s">JMLR Proceedings</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">May 13-15, 2010. 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The unreasonable ineffectiveness of the deeper layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gromov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shapourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<idno>abs/2403.17887</idno>
		<ptr target="https://arxiv.org/abs/2403.17887" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How to start training: The effect of initialization and architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="569" to="579" />
		</imprint>
	</monogr>
	<note>d81f9c1be2e08964bf9f24b15f0e4900-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the impact of the activation function on deep neural networks training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hayou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/hayou19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simplifying transformer blocks</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>id=RtDok9eS3s</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=NPrsUQgMjKK" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.123" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">December 7-13, 2015. 2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deberta: decodingenhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XPZIaotutsD" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net, 2021a</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-Former: Transformer likes residual attention</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ainslie</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.81</idno>
		<ptr target="https://aclanthology.org/2021.findings-acl.81" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="929" to="943" />
		</imprint>
	</monogr>
	<note>Online, 2021b</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00823</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00823" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">October 10-17, 2021</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Normalisation is dead, long live normalisation</title>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Hoedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<ptr target="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/" />
	</analytic>
	<monogr>
		<title level="m">ICLR Blog Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An empirical analysis of compute-optimal large language model training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2022/hash/c" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. November 28 -December 9, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Lora</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nZeVKeeFYf9" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/huang20f.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4475" to="4483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving transformer optimization through better initialization</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v119/huang20f.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4475" to="4483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Effect of Initial Configuration of Weights on Training and Function of Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Dorogovtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F F</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Aguiar</surname></persName>
		</author>
		<idno>doi: 10/gsshxg</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<idno type="ISSN">2227-7390</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">2246</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Training Dynamics of Diffusion Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2312.02696" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The lipschitz constant of self-attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/kim21i.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021. 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5562" to="5571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The psycho-biology of language: an introduction to dynamic philology</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Kingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/5" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
	<note>d44ee6f2c3f71b73125876103c8f6c4-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Integrals related to the error function</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Korotkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Korotkov</surname></persName>
		</author>
		<idno type="DOI">10.1201/9780367809232/integrals-related-error-function-nikolai-korotkov-alexander-korotkov</idno>
		<ptr target="https://www.taylorfrancis.com/books/mono/10.1201/9780367809232/integrals-related-error-function-nikolai-korotkov-alexander-korotkov" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Proxy-normalizing activations to match batch normalization while removing batch dependence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Labatie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Vaughan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/8" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="16990" to="17006" />
		</imprint>
	</monogr>
	<note>d2a5f7d4afa5d0530789d3066945330-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1082</idno>
		<ptr target="https://aclanthology.org/D17-1082" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Effiicient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-49430-8_2</idno>
		<ptr target="https://doi.org/10.1007/3-540-49430-8_2" />
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019. 2019</date>
			<biblScope unit="page" from="8570" to="8581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Limits to depth efficiencies of self-attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Balcan</title>
		<author>
			<persName><forename type="first">In</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/ff" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
	<note>dfdf5904e920ce52b48c1cef97829-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Understanding Collapse in Non-Contrastive Siamese Representation Learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2209.15007" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/54" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="8168" to="8177" />
		</imprint>
	</monogr>
	<note>fe976ba170c19ebae453679b362263-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Why spectral normalization stabilizes gans: Analysis and improvements</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Vaughan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/4" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="9625" to="9638" />
		</imprint>
	</monogr>
	<note>ffb0d2ba92f664c2281970110a2e071-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Self-adaptive scaling for learnable residual structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1080</idno>
		<ptr target="https://aclanthology.org/K19-1080" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="862" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training transformers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.463</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.463" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5747" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Very deep transformers for neural machine translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/2008.07772</idno>
		<ptr target="https://arxiv.org/abs/2008.07772" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<ptr target="https://arxiv.org/abs/1907.11692" />
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">WKB approximation for the sum of two correlated lognormal random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.12988/ams.2013.39511</idno>
		<ptr target="http://www.m-hikari.com/ams/ams-2013/ams-125-128-2013/39511.html" />
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="6355" to="6367" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">What makes pre-trained language models better zero-shot learners</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Namee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.128</idno>
		<ptr target="https://doi.org/10.18653/v1/2023.acl-long.128" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023. 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2288" to="2303" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The expressive power of neural networks: A view from the width</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/32" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="6231" to="6239" />
		</imprint>
	</monogr>
	<note>cbf687880eb1674a07bf717761dd3a-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Scaling resnets in the large-depth regime</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fermanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vert</surname></persName>
		</author>
		<idno>abs/2206.06929</idno>
		<ptr target="https://arxiv.org/abs/2206.06929" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Rapid training of deep neural networks without skip connections or normalization layers using deep kernel shaping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<idno>abs/2110.01765</idno>
		<ptr target="https://arxiv.org/abs/2110.01765" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">FP8 formats for deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cornea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grisenthwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamalu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/2209.05433</idno>
		<ptr target="https://arxiv.org/abs/2209.05433" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">All you need is a good init</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06422" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">May 2-4, 2016. 2016</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">A theory on adam instability in large-scale machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2304.09871</idno>
		<ptr target="https://arxiv.org/abs/2304.09871" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Montúfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/hash/109" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
	<note>dd3608f669ca17920c511c2a41e-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<ptr target="https://librarysearch.library.utoronto.ca/permalink/01UTORONTO_" />
	</analytic>
	<monogr>
		<title level="j">Canada</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><surname>Inst/</surname></persName>
		</author>
		<imprint>
			<date>14bjeso/alma991106438365706196</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A table of integrals of the Error functions</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geller</surname></persName>
		</author>
		<idno>doi: 10</idno>
		<ptr target="https://nvlpubs.nist.gov/nistpubs/jres/73B/jresv73Bn1p1_A1b.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards, Section B: Mathematical Sciences</title>
		<idno type="ISSN">0098-8979</idno>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2019.iwslt-1.17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong, 2019. Association for Computational Linguistics</title>
		<meeting>the 16th International Conference on Spoken Language Translation, Hong Kong, 2019. Association for Computational Linguistics</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Signal propagation in transformers: Theoretical perspectives and the role of rank collapse</title>
		<author>
			<persName><forename type="first">L</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anagnostidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper\_files/paper/2022/hash/ae0cba" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>b60c4052359b3d52a2cff7f-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The shaped transformer: Attention models in the infinite depth-and-width limit</title>
		<author>
			<persName><forename type="first">L</forename><surname>Noci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/aa31" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>dc84098add7dd2ffdd20646f2043-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
		<ptr target="https://aclanthology.org/N19-4009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Improving the trainability of deep neural networks through layerwise batchentropy regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stabinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Piater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rodríguez-Sánchez</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=LJohl5DnZf" />
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Exponential expressivity in deep neural networks through transient chaos</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/hash/148510031349642" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3360" to="3368" />
		</imprint>
	</monogr>
	<note>de5ca0c544f31b2ef-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Lipsformer: Introducing lipschitz continuity to vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=cHf1DcCwcH3" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.11446" />
		<imprint/>
	</monogr>
	<note>Scaling language models: Methods, analysis &amp; insights from training gopher. ArXiv preprint, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/raghu17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Australia, 6-11 August 2017. 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/recht19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019, 9-15 June 2019. 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">The principles of deep learning theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hanin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.10165" />
		<imprint>
			<date type="published" when="2021">abs/2106.10165, 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Dropedge</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkx1qkrKPr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kirchhoff</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.240</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.240" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2699" to="2712" />
		</imprint>
	</monogr>
	<note>Masked language model scoring</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep information propagation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1W1UN9gg" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Is normalization indispensable for training deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, NIPS&apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13434" to="13444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Rethinking batch normalization in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><surname>Powernorm</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/shen20e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">13-18 July 2020. 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="8741" to="8751" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Revisiting over-smoothing in BERT from the perspective of graph</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=dUV91uaXm3" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Normformer: Improved transformer pretraining with extra normalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<idno>abs/2110.09456</idno>
		<ptr target="https://arxiv.org/abs/2110.09456" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multibillion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/1909.08053</idno>
		<ptr target="https://arxiv.org/abs/1909.08053" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing NLG 530b, A large-scale generative language model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>abs/2201.11990</idno>
		<ptr target="https://arxiv.org/abs/2201.11990" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</editor>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">On layer normalizations and residual connections in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<idno>abs/2206.00330</idno>
		<ptr target="https://arxiv.org/abs/2206.00330" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Scale efficiently: Insights from pretraining and finetuning transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=f2OYVDyfIB" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Scaling laws vs model architectures: How does inductive bias influence scaling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2023.findings-emnlp.825" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023. 2023</date>
			<biblScope unit="page" from="12342" to="12364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/touvron21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00010</idno>
		<ptr target="https://doi.org/10.1109/ICCV48922.2021.00010" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021</title>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">October 10-17, 2021. 2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Zarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<idno>abs/2307.09288</idno>
		<ptr target="https://arxiv.org/abs/2307.09288" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Mimetic initialization of self-attention layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/trockman23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="34456" to="34468" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
	<note>eefceb8087e964f89c2d59e8a249915-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Magneto: A foundation transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Benhaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/wang23u.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="36077" to="36092" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Deepnet: Scaling transformers to 1,000 layers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Antioversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=O476oWmiNNp" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A broadcoverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<ptr target="https://aclanthology.org/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>id=d8w0pmvXbZ</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/xiong20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">18 July 2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
	<note>Virtual Event PMLR</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Understanding and improving layer normalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/2" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="4383" to="4393" />
		</imprint>
	</monogr>
	<note>f4fe03d77724a7217006e5d16728874-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A study on transformer configuration and training objective</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/xue23b.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="38913" to="38925" />
		</imprint>
	</monogr>
	<note>Honolulu PMLR</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Mean field residual networks: On the edge of chaos</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Vishwanathan</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/81" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
			<biblScope unit="page" from="7103" to="7114" />
		</imprint>
	</monogr>
	<note>c650caac28cdefce4de5ddc18befa0-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="17084" to="17097" />
		</imprint>
	</monogr>
	<note>2e3c3c3be098ef7b382bd2c37ba-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Tensor programs VI: Feature learning in infinite depth neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hayou</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=17pVDnpwwl" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Scaling SGD batch size to 32k for imagenet training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno>abs/1708.03888</idno>
		<ptr target="https://arxiv.org/abs/1708.03888" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syx4wnEtvH" />
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<pubPlace>Addis Ababa, Ethiopia</pubPlace>
		</imprint>
	</monogr>
	<note>In 8th International Conference on Learning Representations, ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Normalized gradient with adaptive stepsize method for deep neural network training</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<idno>abs/1707.04822</idno>
		<ptr target="https://arxiv.org/abs/1707.04822" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Stabilizing transformer training by preventing attention entropy collapse</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v202/zhai23a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">23-29 July 2023. 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="40770" to="40803" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Improving deep transformer with depth-scaled initialization and merged attention</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1083</idno>
		<ptr target="https://aclanthology.org/D19-1083" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="898" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Deep learning without shortcuts: Shaping the kernel with tailored rectifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=U0k7XNTiFEq" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Fixup initialization: Residual learning without normalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=H1gsz30cKX" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Stabilize deep resnet with a sharp scaling factor τ</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-022-06192-x</idno>
		<ptr target="https://doi.org/10.1007/s10994-022-06192-x" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3359" to="3392" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">OPT: open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/2205.01068</idno>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Are more layers beneficial to graph transformers?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=uagC-X9XMi8" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">May 1-5, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">Towards deeper vision transformer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Deepvit</surname></persName>
		</author>
		<idno>abs/2103.11886</idno>
		<ptr target="https://arxiv.org/abs/2103.11886" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Gradinit: Learning to initialize neural networks for stable and efficient training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2021/hash/88" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">December 6-14, 2021. 2021</date>
			<biblScope unit="page" from="16410" to="16422" />
		</imprint>
	</monogr>
	<note>ae6372cfdc5df69a976e893f4d554b-Abstract.html</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
