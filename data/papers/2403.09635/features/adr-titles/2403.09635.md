- Decision on the theoretical framework for signal propagation in transformers
- Choice of components analyzed (Embeddings, FFN, ReLU/GeLU, LayerNorm, Dropout, Softmax, Single-Head Attention)
- Assumptions made regarding input data distribution and correlations
- Selection of initialization scheme (DeepScaleLM)
- Approach to mitigate vanishing/exploding gradients
- Method for deriving closed-form expressions for moments of outputs and gradients
- Validation strategy for theoretical results using real/synthetic data
- Decision to focus on Pre-LN vs. Post-LN transformer architectures
- Strategy for addressing rank collapse in token representations
- Choice of dropout as a mechanism to prevent rank collapse
- Scaling strategy for residual connections
- Empirical verification of theoretical predictions post-training
- Decision to analyze the impact of large QK values on training stability
- Approach to numerical validation of theoretical results
- Methodology for applying the framework across different modalities (text, vision, speech)
- Decision on the significance of correct initialization in model performance
- Choice of datasets for empirical validation (Language Modeling, Speech Translation, Image Classification)
- Decision to release code for reproducibility of results
- Strategy for addressing the challenges of training very deep models (1000 layers)