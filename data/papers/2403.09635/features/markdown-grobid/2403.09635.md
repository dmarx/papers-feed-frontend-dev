# Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

## Abstract

## 

In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose Deep-ScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 1000 layers. We find that transformer models could be much deeper -our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across encoder-only, decoder-only and encoder-decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These improvements also translate into improved performance on downstream Question Answering tasks and improved robustness for Image Classification.

## Introduction

Transformer models are extremely popular across different domains of machine learning, however, deep transformers are plagued with issues of gradient explosion/vanishing [(Rae et al., 2021;](#)[Shleifer et al., 2021;](#b98)[Smith et al., 2022;](#b100)[Takase et al., 2022;](#b102)[Smith et al., 2022;](#b100)[Zhang et al., 2022c;](#)[Dehghani et al., 2023;](#b104)[Chowdhery et al., 2023;](#b14)[Molybog et al., 2023;](#b75)[Wortsman et al., 2024)](#b115) and rank collapse [(Zhou et al., 2021;](#b132)[Noci et al., 2022)](#b81) that adversely affect training stability. Proposed remedies include residual scaling, changing initialization or extra/modified layernorms [(Zhang et al., 2019a;](#)[Xiong et al., 2020;](#b116)[Bachlechner et al., 2021;](#b5)[Wang et al., 2024;](#b112)[Dehghani et al., 2023)](#b104).

Theoretical analysis via signal propagation and kernel methods has led to an improved understanding of these issues. Several works in the signal propagation domain [(Glorot & Bengio, 2010;](#b33)[Arpit et al., 2016;](#b3)[Xu et al., 2019;](#b117)[Dong et al., 2021;](#b28)[Davis et al., 2021;](#b18)[Wang et al., 2022)](#b113) have analysed the propagation of moments for some components of deep transformers, but often make simplifying assumptions of IID inputs, uncorrelated outputs, ignoring effect of query/key initialization, simplifying non-linearity, etc. We observed break down of each of these assumptions with real world data, adversely affecting model stability.

These issues highlight the need for a holistic theoretical framework that can fully explain signal propagation through transformer models with real data. In this work, we provide a framework to fully explain signal propagation through transformer models, by deriving closed-form expressions for the first and second-order moments (mean and variance) of the outputs and gradients of each of the components of the transformer model (Embeddings, FFN, ReLU/GeLU, LayerNorm, Dropout, Softmax, Single-Head Attention), Attention and FFN blocks, and through the entire model. Our derived equations are empirically verified within strict error bounds with real world data[foot_0](#foot_0) .

We apply this framework to understand and mitigate instability issues with deep transformers -vanishing/exploding gradients, rank collapse, and instability caused by high QK values. To harness the improved complexity of deeper models [(Montúfar et al., 2014;](#b76)[Poole et al., 2016;](#b85)[Raghu et al., 2017)](#b88), we propose DeepScaleLM, a novel initialization scheme that augments residual/output scaling, and ensures the moments of outputs and gradients remain fully conserved throughout the model. DSLM enables us to break the depth barrier and train models with 100s of layers which outperform shallow models for BERT, GPT, Encoder-Decoder models across text, vision and speech modalities.  

$)σ 2 x in -1 L 2 e (1-r d x in )σ 2 x in L 2 σ 2 gout - -$$(1 -p) 1 -p d 2 σ 2 o σ 2 v * σ 2 gout (1 -p) r l gout 1 -p FFN Block 2d 2 σ 2 w1 σ 2 w2 σ 2 xin (1 -p) (1 -p)( 1 π + r l xin 2 + ( 1 2 - 1 π )r l xin 2 ) σ 2 xout * σ 2 gout (1 -p)( 1 2 + sin -1 (r l x in ) π$)r l gout 2 Moments of Transformer Models

## Moments of Transformer Components

Following an analysis similar to that of Xavier initialization [(Glorot & Bengio, 2010)](#b33), we derive closed-form expressions for the mean and variance of the output and of the backpropagated gradient for all the components of the transformer model in Table [1](#tab_0).

Here µ xin , σ 2 xin , µ xout , σ 2 xout are the mean and variance of the input/outputs, σ 2 gout , σ 2 gin are the variance of the gradient back-propagated to/from the component, and r l , r d are the correlations across sequence length and hidden dimension. p is the dropout probability, L sequence length, d in , d out input/output dimensions of Linear layer, σ 2 w , σ 2 wembd variances of the weights of the Linear layer and the Embeddings table. At the input side, r l xin originates from repeated tokens. For text, we estimate input correlation theoretically by assuming that input tokens follow Zipf [(Kingsley, 1935)](#b52) distribution. Detailed proofs are provided in Appendix A, and all assumptions are summarized in Appendix L.2.

## Moments of Transformer Blocks

Combining the expressions reported in Table [1](#tab_0), we derive closed-form expressions for the moment transformation during the forward and backward pass of the transformer Attention and FFN blocks. The Attention block refers to the Q, K, V projection, followed by Multi-Head Attention and Output-Projection Layer. The FFN block refers to the Linear layer followed by non-linearity (ReLU) and output Linear layer. Table [2](#tab_1) provides our derived equations for these, where σ 2 v , σ 2 o , σ 2 w1 , σ 2 w2 are the variances for V weights, Output-Projection weights, and weights of FFN block Linear layers, and d is model the hidden size. These results show that considering correlation r l , dropout p and effects  of non-linearity are crucial for correctly modelling signal propagation through Transformer blocks.

## Moments of Entire Transformer Model

By repeatedly applying the expressions in Table [2](#tab_1) for each layer, we calculate the propagation of moments of outputs and gradients through the entire transformer model. We do this for both Pre-LN style transformers, in which the skip connection bypasses the LayerNorm, and for Post-LN style transformers, in which the Layernorm is applied before the skip-connection. The method is fully detailed in Appendices E.1 and E.2. Figures 1, 2 and 3 provide the forward (left to right) and backward (right to left) signal propagation at initialization through the layers of a very deep 192-layer model with Xavier initialization.

## Numerical Validation of Theoretical Results

We verify the theoretical formulae of transformer components and blocks by running simulations with real/synthetic data, (detailed in Appendix D, code released). Even at 99 percentile, no error (other than SHA gradient σ 2 ) is larger than 10%, verifying our assumptions.

All our derivations are modality-agnostic. We verify our formulae for the entire transformer model using real textual MLM data, as shown in Figures 1, 2 and 3 (Reproducible using our released code), and using ImageNet data (as shown in Appendix H). Our formulae predict the observed gradient and forward/backward norms with remarkable accuracy, with mean and median relative errors of 6.8% and 5.2% respectively, and an R 2 of 0.998. We further verify that for model depths in range [1 -768], and model dimensions [[128 -6096]](#), the reported formulae are within 10% error, even across 768 layers of the transformer model.

## Validity of Theoretical Predictions even after Training

Interestingly, our theoretical estimates hold approximately even after the models have been trained for a large number of steps. The model stays in the regime it is initialized with (as has also been shown in [Li & Liang (2018)](#b62); [Arora et al. (2019a)](#); [Lee et al. (2019)](#b58); [Jesus et al. (2021)](#b49); [Arora et al. (2019b)](#); [Dettmers et al. (2023)](#)), highlighting the importance of correct initialization. We analyze gradient explosion in a 30B parameter 64-layer PreLN model (after 150k training steps) and use our theory to predict the moments. Our hyperbolic estimation for the gradient explosion match closely with the observed moments as shown in Figure [5](#). Similarly, forward growth in a 48-layer 1024-d PreLN model (after 100k training steps) matches our linear estimations (Figure [6](#fig_2)). 

## Applications

## Explaining Variance Explosion in Transformer

Our approach theoretically proves the gradient vanishing/explosion (Table [3](#)) for both Pre-LN and Post-LN transformers.

Exploding Output and Gradient in Pre-LN The forward output for Pre-LN transformer increases linearly with increasing depth N (Appendix E.1) since each layer's output is directly added to the skip connection, as seen in Figure [1](#). For the backward pass, the gradient increases hyperbolically with increasing N , as seen in Figure [2](#). Intuitively, this is because the gradient increases in every layer when a block's gradient is added to the skip connection, and the fractional increase in gradient is inversely proportional to the forward variance (which increases by N ) because of LayerNorm.

Vanishing/Exploding Gradient in Post-LN While layernorm solves the explosion in the forward pass of networks with residual connections [(De & Smith, 2020)](#b19), it has the opposite impact on the gradient. As proved in Appendix E.2, the gradient in a Post-LN transformer grows/decays exponentially with the number of layers (Figure [3](#fig_0)).

Intuitively, the gradient is first transformed within the layer and then at the LayerNorm placed before the layer. The multiplicative factor is applied repeatedly, and causes gradient to vanish or explode exponentially, as was also observed in [Schoenholz et al. (2017)](#b94). This explains why Post-LN models are more challenging to train than Pre-LN for deeper networks [(Wang et al., 2024;](#b112)[Shleifer et al., 2021;](#b98)[Takase et al., 2022)](#b102).

Table [3](#). Comparison of maximum theoretical forward pass and backward pass growth in variance for the entire transformer model across methods (See Appendix E for proofs). Here β is the initial value of residual scaling for LayerScale.

## Method

Post-LN Pre-LN Backward Sensitivity Forward Backward Sensitivity

$Vanilla O(c ±N ) O(N ) O(N ) O(N ) O(logN ) DSInit O(1) O(N -1 ) O(1) O(1) O(N -1 ) LayerScale O(1) O(βN ) O(1) O(1) O(βN ) DeepNet O(1) O(N -0.5 ) - - - DSLM (Ours) O(1) O(1) 1 O(1) O(1)$3.2 Explaining Higher Pruning of Deeper Layers [Gromov et al. (2024)](#b34) found that LLMs such as Llama-2-70B [(Touvron et al., 2023)](#b107) have minimal degradation in performance on Question Answering tasks until almost half the deeper layers are removed -suggesting that parameters in deeper layers are less effective in current LLMs. As we prove in Appendix E.1, the output of a Pre-LN transformer grows proportionally with depth (Figure [1](#)). For an 80-layer model like Llama-2, this implies the deeper layers will have a significantly reduced impact on changing the output.

## Explaining Impact of Large QK Values

In [Dehghani et al. (2023)](#b104), the authors observed large QK values destabilized the training, and solved this empirically by adding a layernorm after attention scores. Unlike prior works [(Wang et al., 2024;](#b112)[Noci et al., 2022)](#b81), note from our derivations of softmax(Appendix A.7) that the backwards gradients from Q/K are exponentially related to their variance, highlighting the critical significance of correct initialization of Q/K. For example, by initializing them to only 2x the xavier values (all other initializations the same), backwards gradients exploded 10000x through a 192 layer model. Our theory explains these empirical observations, and sug-gests a simple initialization strategy to fix this problem, achieving the same variance on QK without the overhead of LayerNorm (Section 3.5).

## Explaining and Mitigating Rank Collapse

Similar to our work, [Noci et al. (2022)](#b81) also analyze moment propagation through the transformer, and observed the rank collapse of the token's representations at initialization after just a few layers, i.e., all the token representations became the same (r l x ≈ 1 after just 12 layers) at initialization. This has also been reported in [Shi et al. (2022)](#b97) Our theory suggests a very simple solution -Dropout. As our closed form expressions show, both FFN block (because of ReLU) and dropout reduce the correlation (Figure [7](#)). With dropout, our method shows that such a rank collapse will not occur, and r l

x will quickly reach a stable value < 1 (Appendix F), as verified empirically in Figure [8](#fig_3).

Alternatively, scaling the block output by β = 1 √ N , or equivalently initializing the weights very small in Post-LN will also prevent rank collapse, even without Dropout. For Pre-LN, λ = 1 slows down increase in r l compared to λ 2 = 1 -1 N (but the same slowdown can be achieved by decreasing β). This highlights the criticality of correct initialization, dropout and scaling for deep transformer models, as well as the explainability power of our theoretical framework.

## DeepScaleLM: Enabling Deep Transformers

We propose DeepScaleLM (DSLM), a new initialization / scaling scheme that alleviates the issues discussed above.

Residual/Skip-Connection Scaling Let σ 2 skip , σ 2 block , σ 2 model be the variances of the skip connection, the block, and the output of the final layer of the model, respectively. Let σ 2 skip = σ 2 block , and we scale them by scalars λ and β respectively. Then, as has been proven in numerous works (Appendix K.3), if λ 2 + β 2 = 1, this scaling will maintain the variance after addition of the residual.

Initialization However while ensuring σ 2 skip = σ 2 block (and equal to the variance of model input) has been done for ResNets (Appendix K.1), it is difficult to achieve theoretically for transformers. By leveraging the equations in Table 2, our theory provides us the tools to achieve this. We modify the initialization of the components of the transformer FFN and Attention blocks such that the variance of their output is 1, as further detailed in Appendix M -1. We set the variance of embedding weights as σ 2 e = 1-p numembd , where num embd is the number of embeddings types. As embeddings are followed by a dropout, this ensures the input variance to the model is 1.

2. We set σ 2 w2 = σ 2 w1 = 1 d * 1-p 2 , to make the output of the FFN block 1.

3. We iteratively calculate layer-by-layer r l xin , r l xout using expressions from Table [2](#tab_1), and calculate the initial variance of the attention block weights to make the output variance 1.

This initialization of transformer blocks, combined with the scaling of the skip connection and residual, and correct initialization of the embeddings, results σ 2 model = 1, irrespective of the number of layer N . This initialization also preserves the backward gradient, as proved for Pre-LN and Post-LN, in Appendices E.3 and E.4. Empirically, we show the backward gradient being preserved for both Pre-LN and Post-LN even across 192 layers at initialization (Figure [4](#fig_1)).

## Choice of Scaling Parameters

While any choice of β will work at initialization, higher values of β, for example β 2 = 0.5 causes gradients to vanish (Figure [9](#), Table [4](#tab_4)). This is because covariance between residual and skip connection increases the forward variance, which causes normalization to decrease backward gradient [(De & Smith, 2020)](#b19).

Similar to other prior works (Appendix K.3), we use

$β 2 = k$N in all our experiments, where k is some small constant. This enables us to bound the fall in gradient (Appendix E.3) for Pre-LN. For Post-LN, β 2 ≤ k N 2 is theoretically required to bound the gradient (Appendix E.6). In practice, with β 2 = 2 N , even with 768 layers, we empirically observed the final output variance from the model does not exceed 30, and all our models converge. We hence use β 2 = k N (Figure [10](#)), but a practitioner may choose β 2 = k N α , with α > 1 if more stability is required at the expense of performance/"sensitivity" (Refer to discussion of relative strength in Section 4.6 and comparison to prior works in Section 4.5). While the above analysis assumes positive covariance (which we always observed experimentally), negative covariance follows a similar reasoning, and will cause gradient explosion instead. Preventing Rank Collapse For DSLM, applying block equations iteratively shows that r l x < 1 -1 e 2 after N layers.

Simpler Initialization Another avenue to handle the covariance between residual and skip connection could be to set λ 2 + β 2 < 1. We therefore also consider a simpler initialization method(Appendix M), in which we modify the initialization of attention value and output matrices to be the same as those of FFN block. This decreases the "effective" β of the attention block, but as the attention block has 2x fewer params than FFN, this change in weightage seems reasonable. As we show in Appendices E.5 and E.6 while variances are no longer unit at initialization, they are still bounded. This change does not impact performance significantly, as we show in Table [14](#tab_14). All further experiments in Section 4 used this simpler initialization.

Folding Scaling into Weights for Inference The scaling parameters introduced here can be fully absorbed into the model checkpoint weights by recursively scaling layernorm gain and output linear weights, hence and do not require any changes to vanilla transformers inference code.

DeepScaleLM enables training deeper-narrower models with 100s of layers, outperforming standard models across transformer variants, tasks and modalities.

4 DeepScaleLM Results

## Improvements on Encoder-only Models (BERT)

Implementation Details We test our method on the Masked Language Modelling task with the BERT [(Devlin et al., 2019)](#b25) model. Pile-CC dataset [(Gao et al., 2021)](#b31) was used to pre-train our model. We use k = 2 for β while keep-ing all the original hyper-parameters of BERT the same, except for learning rate (LR). We find that higher LR is needed for our deeper-narrower models (similar to [Yang et al. (2021)](#b120)). Hence, we search for LR for all the models. The training steps were decided based on Chinchilla [(Hoffmann et al., 2022)](#b44), at 6.6B tokens.

Table 25 provides all hyper-parameter details. For DSLM, model output was down-scaled by √ d before being passed to the LM-head. We train different language models with the same number of parameters and compute -while increasing the depth (N ), we reduce the hidden dimension d keeping number of transformer parameters (N d[foot_1](#foot_1) ) constant. When changing from 12-layer 1024-d model to 192-layer 256-d model, compute negligibly increases by only 6.6% when keeping N d 2 constant (Table 23), while the number of parameters decreases by 5 -15% due to decreased embedding parameters. Evaluation Metrics Pre-training Perplexity (exponential of pre-training test-set loss) is often used to measure MLM pre-training performance (RoBERTa (Liu et al., 2019b), Megatron-LM (Shoeybi et al., 2019), Tay et al. (2023), or similar variants in Salazar et al. (2020); Lu et al. ( [2023](#))), and is well-correlated with downstream performance [(Geiping & Goldstein, 2023)](#b32). We use the perplexity as reported by Megatron-LM here. Calling this measure "perplexity" is a slight abuse of notation (as previous words which are masked are not available, and future words are). For downstream fine-tuning, we use accuracy while for Speech-to-Text translation, we use BLEU score.

## Pre-Training Improvements In

Table 4, we provide the results obtained on scaling model depth after applying DSLM to Post-LN. Post-LN models often diverge while scaling model depth. DSLM stabilizes the training of Post-LN models, and even a 768 layer Post-LN model (with 2300 Linear and 768 attention layers) converges.  Sustained Improvements after Longer Pre-training Due to compute limitations, our models were trained for Chinchilla optimal steps. To ensure reproducibility of our work (scripts provided in released code), and demonstrate sustained improvements for standard models, we trained the BERT-base model using public Wikipedia data for 64B tokens (30x chinchilla tokens). We train a 4x deeper, 10% smaller model using DSLM (N /d = 48 / 384). We finetune these models on the public RACE-M, RACE-H [(Lai et al., 2017)](#b56), MNLI [(Williams et al., 2018)](#b114) and QQP 2 datasets. As shown in Table [6](#tab_6), our model provides better pretraining performance which is translated into downstream Question-Answering tasks' performance across all datasets. Downstream Low Rank Finetuning DSLM continues to outperform the baseline on finetuning for downstream tasks with Low Rank Adapters [(Hu et al., 2022)](#b45), as shown in Table [7](#tab_7). Following QLoRA [(Dettmers et al., 2023)](#), we apply LoRA on all linear modules, with r = 32, α = 16, and searched for LR. We applied DSLM to the decoder-only GPT model, trained for 8B tokens (slightly more than Chinchilla-optimal). Similar to BERT, increasing model depth by 4x with DSLM while keeping the parameters constant results in improved performance (Table [8](#tab_8)). We apply DSLM on encoder/decoder style transformer for Speech-to-Text translation task. Applying our method to speech additionally requires handling the input embeddings. Instead of theoretical estimates as in the case of text inputs (Appendix A.1), the moments for speech embedding were replaced by the empirically observed values. This input variance and correlation was observed as 2.2 and 0.29.

The baseline was trained on the MuST-C (Di [Gangi et al., 2019)](#b26) dataset using fairseq [(Ott et al., 2019)](#b83). Using DSLM, we successfully train 4x deeper models which outperforms the 18-layer (12-encoder, 6-decoder layers) baseline with 9% less parameters as seen in Table [9](#tab_9).

## Improvements on Vision Modality

Similar to speech domain, applying our method to vision modality simply requires handling the input embedding (Ap-  [(Recht et al., 2019)](#b89), ImageNet-R [(Hendrycks et al., 2021)](#b42) and ImageNet-Sketch [(Wang et al., 2019)](#b110).  DSInit [(Zhang et al., 2019a)](#) 15.9 diverge ADMIN [(Liu et al., 2020a)](#) diverge 25.2 SkipInit [(De & Smith, 2020)](#b19) 15.1 13.1 ReZero [(Bachlechner et al., 2021)](#b5) diverge diverge LayerScale [(Touvron et al., 2021b)](#) 13.2 14.4 µP-Tensor Programs V [(Yang et al., 2021)](#b120) diverge diverge DeepNorm [(Wang et al., 2024)](#b112) 14.4 13.4 [Noci et al. (2022)](#b81) diverge diverge Bamboo [(Xue et al., 2023)](#b118) 17.1 diverge Value-SkipInit [(He et al., 2023)](#b38) 18.8 17.1 DeepScaleLM (ours) 12.9 11.7

## Analysis of DSLM

Model Quantization Similar to Unit Scaling [(Blake et al., 2023)](#b9), conserving unit activations and gradients from our method results in models which lose much less performance when quantized (via direct casting) to FP8 precision compared to original models. We apply 8-bit quantization to the 48-Layer 512-dim BERT baseline model and the model trained with DSLM. Table [12](#tab_12) provides the performance corresponding to the full precision inference and FP8 inferences (corresponding to two different FP8 standards, E5M2 and E4M3). DSLM model can be compressed to 25% of the original size with significantly lower performance loss.    [(Yang & Schoenholz, 2017)](#b119) -having lower β (smaller k or higher α) will result in networks where observed issues (forward growth or gradient explosion/vanishing) are mitigated, but they may converge slowly/sub-optimally. [Davis et al. (2021)](#b18) defines "sensitivity" as the variance of relative change in output for small perturbations in parameters, averaged across all parameters. If σ 2 skip = 1, sensitivity can be shown to be mean across layers of N * (1/σ 2 block ) = N * β 2 . Mean is not robust to outliers, and hence we suggest median may provide a more robust measure. For e.g., for vanilla pre-LN, [Davis et al. (2021)](#b18)'s definition gives sensitivity as O(log(N )), whereas using median provides a more robust measure as O(1). But only the first N/10 layers have O(log(N )) sensitivity, and the last 9N/10 layers have O(1) sensitivity. We will use median in the discussion below.

In Appendix G, we show that the fall in gradient for both pre-LN and post-LN for β 2 = k/N α is O(e kN 1-α ). The sensitivity is hence kN 1-α . For DSLM, we chose α = 1, that is the sweet spot on the stability-expressivity curve where both the gradient fall bound and sensitivity expressions become independent of model depth. For higher values of α such as α = 2 (DS-Init) and, α = 1.5 (DeepNet), the gradient becomes stable using but the model expressivity reduces with depth, as shown in Table [3](#). Such models might not be able to extract better results when going deeper, as we indeed verify empirically in the comparison with prior works paragraph in Section 4.5.

## Related Works

For detailed discussion of prior works, refer to Appendix K.

Initialization Several works [(Glorot & Bengio, 2010;](#b33)[He et al., 2015;](#b39)[Brock et al., 2021a;](#)[Poole et al., 2016;](#b85)[Schoenholz et al., 2017)](#b94) improved the initialization of ResNets/ReLU networks. These works do not consider transformers, and are unable to handle Softmax/Attention. Others, such as ADMIN [(Liu et al., 2020a)](#), [Mishkin & Matas (2016)](#b74); [Liu et al. (2020b)](#) achieve unit variance for faster convergence by scaling the weights and/or outputs based on empirical profiling of a forward pass. [Blake et al. (2023)](#b9) also tries to achieve this, but does not completely handle correlation and non-zero mean of ReLU. We demonstrate that this profiling is unnecessary, and can instead be done theoretically in our work.

Signal Propagation Signal propagation in Neural Networks [(Neal, 1995;](#b77)[LeCun et al., 1996)](#b57) has a long history, such as for ResNets [(He et al., 2015;](#b39)[De & Smith, 2020;](#b19)[Brock et al., 2021a;](#)[Schoenholz et al., 2017;](#b94)[Hoedt et al., 2022;](#b43)[Labatie et al., 2021;](#b55)[Marion et al., 2022;](#b71)[Klambauer et al., 2017;](#b53)[Balduzzi et al., 2017)](#b6), and for transformers in [(Xu et al., 2019;](#b117)[Dong et al., 2021;](#b28)[Davis et al., 2021;](#b18)[Noci et al., 2022;](#b81)[Martens et al., 2021;](#b72)[He et al., 2023;](#b38)[Shi et al., 2022;](#b97)[Wang et al., 2022)](#b113). Our work considers previously often neglected effects of dropout, input correlation, activation non-linearity, and QK initialization, providing closed forms with verifiable correctness of signal propagation. This allows us to constrain the output and gradient to almost exactly unit variance.

Moment Control & Residual Scaling Bounded gradients have been shown to results in better/faster convergence [(Shen et al., 2020;](#b96)[Yu et al., 2017;](#b124)[You et al., 2017;](#b122)[2020;](#)[Takase et al., 2022;](#b102)[Shleifer et al., 2021;](#b98)[Hayou et al., 2019)](#b36). Different scaling schemes for residual networks (λ for skip connections and β for residual output) have been explored by prior works, such as λ 2 +β 2 = 1 for ResNets [(Balduzzi et al., 2017;](#b6)[Szegedy et al., 2017;](#b101)[Hanin & Rolnick, 2018;](#b35)[Arpit et al., 2019;](#b4)[Zhang et al., 2019b;](#)[Hoedt et al., 2022)](#b43). Learnable β ≈ 0 was used in SkipInit [(De & Smith, 2020)](#b19), ReZero [(Bachlechner et al., 2021)](#b5), LayerScale [(Touvron et al., 2021b)](#), Value-SkipInit [(He et al., 2023)](#b38). Others proposed β 2 =O( 1 N ), where N is max/current layer was used in Arpit et al. (2019); Brock et al. (2021a); Marion et al. (2022); Zhang et al. (2022b); He et al. (2023); Noci et al. (2022); De & Smith (2020); Liu et al. (2020a;b); Davis et al. (2021); Blake et al. (2023), while DSInit [(Zhang et al., 2019a)](#), T-Fixup [(Huang et al., 2020a)](#), DeepNorm [(Wang et al., 2024)](#b112) used β 2 <O( [1](#formula_29)N ). However, the optimal initialization/scaling can vary based on data/model characteristics [(Zhang et al., 2022b;](#)[Marion et al., 2022)](#b71). Our contribution goes beyond providing an optimal scaling schemeour theory enables informed choices about these initialization/scaling schemes based on their expressivity-trainability trade-off. Some works such as DeepNet, ADMIN show performance improvements by making the model deeper, but much larger. In this work, we explore a stricter setting of keeping transformer parameters and compute constant while making the model deeper.

Other Network modifications for Deep Networks Architectural modifications such as [Zhai et al. (2023)](#b125); [Zhou et al. (2021)](#b132); [Shleifer et al. (2021)](#b98) can only stabilize the model later during training and not at initialization. They are orthogonal to our approach, and our equations can be easily extended to cover these.

## Conclusion

We theoretically derive closed forms for the growth of variances for forward and backward pass through individual transformer components as well as the entire transformer model. These formulae enable us to identify and solve the key reasons for vanishing/exploding gradients and rank collapse in very deep transformers. Via scaling and correct initialization, we also enable training very deep transformers with 1000 layers. Our experiments suggest that deeper transformers should be explored -using our method, models with 100s of layers outperform larger standard models across multiple modalities, tasks, and transformer variants.

## Impact Statement

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, some which we feel must be specifically highlighted here. Using crawled web data for pre-training language models is questionable, something which society has yet to finalize its views on. Language modelling in particular suffers from hallucinations, and may be used for misinformation.

E.6.2 Backward Pass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 F Rank Collapse and Correlation Analysis 73 G Discussion of Relative Strength 74 H Applying DeepscaleLM to Vision Transformers 74 I Compute 75 I.1 Theoretical compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 I.2 Wall Clock times . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 J Statistical Significance 76 J.1 Error Bars for Pre-Training Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 J.2 Statistical Significance for Fine-tuning Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K Related Works 76 K.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K.2 Signal Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 K.3 Moment Control & Residual Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 K.4 Other Network modifications for Deep Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 L Discussion of Approximations and Assumptions 78 L.1 Illustrative Approximations of Full Formulae in Main Paper . . . . . . . . . . . . . . . . . . . . . . . . 78 L.2 Assumptions and Approximations in Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 M DeepScaleLM Pseudocode 79 N Hyper-parameters 80 O Notations 82

## A Moment Propagation through Transformer Components

We provide detailed proofs of the closed-form expression for each of the transformer component -Linear layer, Dropout, ReLU, GeLU, LayerNorm, and Softmax.

For any component, input is represented as x in and x out is the output. The gradient flowing in into the component from the output side is represented as g out and the backpropagated gradient towards the input is g in . We switch from vector to matrix notation (X in , X out ) whenever needed. We assume that the input is distributed normally N (0, σ xin ). No assumptions are made regarding the covariance of the input -it is not assumed to be IID, and it may/may-not have covariance both along the sequence length and hidden dimension. Additional assumptions needed to derive the proofs for softmax and attention can be found in the respective proofs. A detailed list of terms/notations used in the proofs is provided at the end of this work in Appendix O.

## A.1 Embeddings

The BERT model's embedding component consists of 3 look up tables -token embeddings, position embeddings, and segment embeddings. For a given input token, each of these 3 embeddings are added before being passed to the transformer model. Other transformer models, such as decoder-only GPT lack some (eg. segment) of these, but the derivations remain similar. In the general case, these theoretical derivations can be replaced by the empirically observed moments of the inputs fed to the transformer model (as we did for Speech-to-Text translation). We derive formulae for each of these embedding types below.

Token Embeddings We do not assume the input embeddings to be IID. Repetition of same token introduces correlation across the sequence length. We assume that the input tokens have been sampled from a multinomial distribution. The words / token ids are distributed almost according to Zipf's law [(Kingsley, 1935)](#b52). Assuming we initialize all the embeddings with variance σ 2 w embd , the relevant statistics for word embeddings output x outwe are as follows

$µ xout we = 0 σ 2 xout we = σ 2 wembd Cov l (x outwe ) = N i * (N i -1) L * (L -1) * σ 2 wembd r l (x outwe ) = N i * (N i -1) L * (L -1) Cov d (x outwe ) = 0 Assume ith word occurs N i times, it contributes Ni * (Ni-1) L * (L-1))$to the covariance along sequence length. Similarly, we can calculate the correlation for segment-type embeddings output x outse . Zipf's law states that the probability for each token is inversely proportional to its rank. For the word with rank i, p i = c i , where c =

$1 i 1 i = 1 γ+log(|V |)$, where γ ≈ 0.58 is the Euler's constant.

For a sentence of length L, the token with probability p i is expected to occur p i .L times. Hence, for a given vocabulary size |V |, we can calculate the correlation as follows

$r l (x outwe ) = N i * (N i -1) L * (L -1) = |V | i p i L * (p i L -1) L * (L -1) = i p 2 i * L -1 L -1 = i c 2 i 2 * L -1 L -1 ≈ Lπ 2 6.(γ+log(|V |)) 2 -1 L -1 ≈ π 2 6.log(|V |) 2 , assuming γ ≈ 0.58 << log(|V |) ≈ 10.4, L >> 1$Segment Type Embeddings Similarly, the segment type embeddings have two possible values denoting the sentence order. If first sentence has length x, we can consider this as a special case of the analysis performed above with two possible tokens, where N 1 = x and N 2 = Lx. Assuming x is distributed uniformly between 0 to L, Lx also has the same distribution. Hence,

$r l (x outse , N 1 , N 2 ) = N 2 1 + N 2 2 -L L * (L -1)$Taking expectation, we get

$r l (x outse ) = 2 3 * L 2 -L L * (L -1) ≈ 2 3$Position Embeddings Since learnt position embeddings are lookup tables with unique inputs, the correlation from position embeddings is 0.

Final Model Input Embeddings Each of the above embeddings are added before being passed to the transformer model. Since the variance is same for all embedding types, the final correlation is the average of the three. Hence:

$r l (x out ) = 1 3 (r l (x outwe ) + r l (x outse )) = π 2 18 * log(|V |) 2 + 2 9$For our case, |V | = 32000 and sequence length L = 256, the theoretically predicted correlation r l xin = 0.227 which is within 3% of the empirically observed correlation (0.221).

Hence, the final moments for the embedding output are

$µ xout = 0 σ 2 xout = 3 * σ 2 wembd Cov l xout = ( π 2 18 * log(|V |) 2 + 2 9 )σ 2 xout Cov d xout = 0 A.2 Linear$For linear layer with d in dimensional input x in , and d out dimensional output x out , we can define the forward pass mathematically as,

$x out = x in W =⇒ x outj = din i=1 x ini W i,j$Similarly, we define the backward pass as,

$g in = g out W T =⇒ g inj = dout i=1 g outi W j,i$For expectation of output we have,

$E[x outj ] = E[ din i=1 x ini W i,j ] = din i=1 E[x ini W i,j ] = din i=1 E[x ini ]E[W i,j ] = µ xin µ w$(As weights and input are independent of each other)

$µ xout = 0 (∀j)$To get variance of the output of forward pass we have,

$Var(x outj ) = Var( din i=1 x ini W i,j )$As the weights are initialized independently each term in summation is independent of each other

$= din i=1 (Var(x ini W i,j )) = din i=1 ((σ 2 xin + µ 2 xin )(σ 2 w + µ 2 w ) -µ 2 xin µ 2 w )$(As weights and input are independent of each other)

$= din i=1 (σ 2 xin + µ 2 xin )σ 2 w Var(x outj ) = d in (σ 2 xin + µ 2 xin )σ 2 w (∀j) σ 2 xout = d in (σ 2 xin + µ 2 xin )σ 2 w$If we have two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = x in W and y out = y in W. Then for any j we have

$Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] σ 2 xout σ 2 xout = E[ din i=1 x ini W i,j din k=1 y in k W k,j ] σ 2 xout = E[ din i=1 x ini y ini W 2 i,j + din k=1,k̸ =i din i=1 x ini y in k W i,j W k,j ] σ 2 xout$In second summation all terms are independent of each other and as the expectation of weights is 0 we have

$Corr(x outj , y outj ) = E[ din i=1 x ini y ini W 2 i,j ] σ 2 xout = din i=1 E[x ini y ini W 2 i,j ] σ 2 xout (Independence of weight initialization) = din i=1 E[x ini y ini ]E[W 2 i,j ] σ 2 xout = din i=1 (r l xin σ 2 xin + µ 2 xin )σ 2 w σ 2 xout (Definition of correlation) = d in (r l xin σ 2 xin + µ 2 xin )σ 2 w d in (σ 2 xin + µ 2 xin )σ 2 w Corr(x outj , y outj ) = r l xin σ 2 xin + µ 2 xin σ 2 xin + µ 2 xin r l xout = r l xin σ 2 xin + µ 2 xin σ 2 xin + µ 2 xin$As the backward pass has similar structure, assuming µ gout = 0 we can use the same analysis to get,

$µ gin = 0 σ 2 gin = d out σ 2 gout σ 2 w A.3 Dropout$We can define Dropout mathematically as,

$x out = Dropout(x in ) =⇒ x outi = xin i (1-p) with probability 1 -p 0 else$To calculate expectation of dropout,

$E[x outi ] = 0 * p + (1 -p) * E[ x ini (1 -p) ] µ xout = µ xin$For variance,

$Var(x outi ) = E[x 2 outi ] -E[x outi ] 2 = 0 * p + (1 -p) * E[ x 2 ini (1 -p) 2 ] -µ 2 xin = E[x 2 ini ] (1 -p) -µ 2 x = σ 2 xin + µ 2 xin (1 -p) -µ 2 xin σ 2 xout = σ 2 xin + pµ 2 xin (1 -p)$If we have two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = Dropout(x in ) and y out = Dropout(y in ). Then for any j we have

$Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] -µ xout µ xout σ 2 xout σ 2 xout = p 2 * 0 + 2 * p * (1 -p) * 0 + (1 -p) 2 * E[ xin j yin j (1-p) * (1-p) ] -µ 2 xout σ 2 xout = E[x inj y inj ] -µ 2 xout σ 2 xout Corr(x outj , y outj ) = (r l xin σ 2 xin )(1 -p) σ 2 xin + pµ 2 xin = r l xout$We can define the backward pass of Dropout as,

$g ini = gout i (1-p) if x i isn't dropped out (which has probability (1 -p)) 0 else$Again we can see that backward has similar definition to that of forward pass. Assuming µ gx out = 0 and using similar analysis we get, µ gin = 0

$σ 2 gin = σ 2 gout (1 -p) A.4 ReLU$Formuale functionally equivalent to ours for µ x , σ 2 x , and σ 2 g have also been derived in [Arpit et al. (2016)](#b3). We can define ReLU mathematically as,

$x out = ReLU(x in ) =⇒ x outi = x ini if x ini > 0 0 else$For getting expectation of output of ReLU for normally distributed input we have,

$E[x outi ] = ∞ -∞ ReLU(x ini ) exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 -∞ 0 * exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini + ∞ 0 x ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = ∞ 0 x ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini Substituting t = x 2 ini 2σ 2 xin we have dt = x ini dx ini σ 2 xin we get, E[x outi ] = ∞ 0 σ xin exp (-t)dt √ 2π = σ xin √ 2π [-exp (-t)] ∞ 0 = σ xin √ 2π$Hence, the mean of output

$µ xout = σ xin √ 2π(1)$Variance of output can be calculated by,

$Var(x outi ) = E[x outi 2 ] -E[x outi ] 2 = ∞ -∞ (ReLU(x ini )) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π = 0 -∞ 0 * exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini + ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π = ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini - σ 2 xin 2π Let I = ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini , then substituting t = -x ini we have, I = -∞ 0 -t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt = 0 -∞ t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt =⇒ I + I = 0 -∞ t 2 exp ( -t 2 2σ 2 x in ) √ 2πσ xin dt + ∞ 0 x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini 2I = ∞ -∞ x 2 ini exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = σ 2 xin =⇒ Var(x outi ) = σ 2 xin 2 - σ 2 xin 2π = σ 2 xin 2 (1 - 1 π ) σ 2 xout = σ 2 xin 2 (1 - 1 π )$Now for two inputs x in and y in such that for all i we have Corr(x ini , y ini ) = r l xin , and x out = ReLU(x in ) and y out = ReLU(y in ). Then for any j we have,

$Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) E[x outj y outj ] = ∞ 0 ∞ 0 x inj y inj 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x 2 inj + y 2 inj -2r l xin x inj y inj ) 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = ∞ 0 ∞ 0 x inj y inj 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -y 2 inj 2σ2$
## xin

)dx inj dy inj Substituting t = x injr l xin y inj , and assuming y inj is constant for the inner integral,dx inj = dt

$E[x outj y outj ] = = ∞ 0 y inj exp ( -y 2 in j 2σ 2 x in ) √ 2πσ xin ∞ -r l x in yin j t + r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj + ∞ 0 y inj √ 2πσ x exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 )$)dtdy inj Let us first define I 1 and I 2 as:

$I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj I 2 = ∞ 0 y inj √ 2πσ x exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j t √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj Substituting p = t 2 2σ 2 xin (1 -(r l xin ) 2 ) we have dp = tdt σ 2 xin (1 -(r l xin ) 2 ) I 1 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ (r l x in y in j ) 2 2σ 2 x in (1-(r l x in ) 2 ) σ xin (1 -(r l xin ) 2 ) √ 2π exp (-p)dpdy inj = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) σ xin (1 -(r l xin ) 2 ) √ 2π exp ( -(r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ 0 y inj (1 -(r l xin ) 2 ) 2π exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj Substituting m = y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) , dm = y inj dy inj σ 2 xin (1 -(r l xin ) 2 )$,

$I 1 = ∞ 0 (1 -(r l xin ) 2 ) 2π (1 -(r l xin ) 2 )σ 2 xin exp (-m)dm = (1 -(r l xin ) 2 ) 3 2 σ 2 xin 2π I 2 = ∞ 0 y inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j r l xin y inj √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) ∞ -r l x in yin j 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy inj Substituting p = -t, where Φ is CDF of Standard Normal Distribution I 2 = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) -∞ r l x in yin j -1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin ) r l x in yin j -∞ 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )Φ( r l xin y inj σ xin 1 -(r l xin ) 2 )dy inj = ∞ 0 r l xin y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )[ 1 2 (1 + erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 ) ))]dy inj = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj + r l xin 2 √ 2πσ xin ∞ 0 y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 )$)dy inj Let us define I 2,1 and I 2,2 as

$I 2,1 = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj I 2,2 = r l xin 2 √ 2πσ xin ∞ 0 y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj σ xin 2(1 -(r l xin ) 2 ) )dy inj I 2,1 = r l xin 2 ∞ 0 y 2 inj √ 2πσ xin exp ( -y 2 inj 2σ 2 xin )dy inj I 2,1 = r l xin σ 2 xin 4$(Same integral as in variance calculation)

From [Ng & Geller (1969)](#b79) we have

$∞ 0 x 2 exp (-b 2 x 2 )erf(ax)dx = √ π 4b 3 - tan -1 ( b a ) 2 √ πb 3 + a 2 √ πb 2 (a 2 + b 2 ) . Hence, putting a = r l xin σ xin 2(1 -(r l xin ) 2 ) and b = 1 σ xin √ 2 we get, I 2,2 = r l xin 2 √ 2πσ xin [ 2 √ 2σ 3 xin 4 - tan -1 ( √ (1-(r l x in ) 2 ) r l x in )2 √ 2σ 3 xin 2 √ π + √ 2r l xin σ 3 xin (1 -(r l xin ) 2 ) √ π ] = r l xin σ 2 xin 4 - r l xin cos -1 (r l xin )σ 2 xin 2π + (r l xin ) 2 (1 -(r l xin ) 2 )σ 2 xin 2π E[x outj y outj ] = I 1 + I 2,1 + I 2,2 = (1 -(r l xin ) 2 ) 3 2 σ 2 xin 2π + 2 * r l xin σ 2 xin 4 - r l xin cos -1 (r l xin )σ 2 xin 2π + (r l xin ) 2 (1 -(r l xin ) 2 )σ 2 xin 2π = r l xin σ 2 xin 2 - r l xin cos -1 (r l xin )σ 2 x 2π + (1 -(r l xin ) 2 )σ 2 xin 2π Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = r l xin σ 2 xin 2 - r l xin cos -1 (r l xin )σ 2 xin 2π + (1 -(r l xin ) 2 )σ 2 x 2π - σ 2 xin 2π σ 2 xin 2 (1 - 1 π ) r l xout = πr l x in 2 + r l xin sin -1 (r l xin ) + (1 -(r l xin ) 2 ) -1 π -1$Backward pass on ReLU can be defined as,

$g ini = g outi if x ini > 0 (which has probability 1 2 ) 0 else Assuming µ gout = 0, E[g ini ] = 1 2 * 0 + 1 2 * E[g outi ] µ gin = 0 Var(g ini ) = E[g 2 ini ] -E[g ini ] 2 = E[g 2 ini ] = 1 2 * 0 + 1 2 * E[g 2 out ] σ 2 gin = σ 2 gout 2$If for two inputs x in and y in for all i we have Corr(g outx i , g outy i ) = r l gout , and g inx i , g iny i be the gradient after passing through ReLU layer. Then we have,

$E[g inx i g iny i ] = P(x ini > 0, y ini > 0)E[g outx i g outy i ] = P(x ini > 0, y ini > 0)r l gout σ 2 gout P(x ini > 0, y ini > 0) = = ∞ 0 ∞ 0 x ini y ini 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x 2 ini + y 2 ini -2r l xin x ini y ini ) 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini dy ini = ∞ 0 ∞ 0 x ini y ini 2πσ 2 xin 1 -(r l xin ) 2 exp ( -(x ini -r l xin y ini ) 2 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -y 2 ini 2σ2$
## xin

)dx ini dy ini Substituting t = x inir l xin y ini , and assuming y ini is constant for the inner integral,dx ini = dt

$P(x ini > 0, y ini > 0) = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) ∞ -r l x in yin i 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -t 2 2σ 2 xin (1 -(r l xin ) 2 ) )dtdy ini Substituting p = -t, where Φ is CDF of Standard Normal Distribution P(x ini > 0, y ini > 0) = = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) -∞ r l x in yin i -1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ) r l x in yin i -∞ 1 √ 2πσ xin 1 -(r l xin ) 2 exp ( -p 2 2σ 2 xin (1 -(r l xin ) 2 ) )dpdy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )Φ( r l xin y ini σ xin 1 -(r l xin ) 2 )dy ini = ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )[ 1 2 (1 + erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 ) ))]dy ini = 1 2 ∞ 0 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini + 1 2 √ 2πσ xin ∞ 0 exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 ) )dy ini = 1 4 + 1 2 √ 2πσ xin ∞ 0 exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini σ xin 2(1 -(r l xin ) 2 )$)dy ini

From [Ng & Geller (1969)](#b79) we have

$∞ 0 exp (-b 2 x 2 )erf(ax)dx = √ π 2b - 1 b √ π tan -1 ( b a ) Putting a = r l xin σ xin 2(1 -(r l xin ) 2 ) and b = 1 σ xin √ 2 we get, P(x ini > 0, y ini > 0) = 1 4 + 1 2 √ 2πσ xin [ √ πσ xin √ 2 2 - σ xin √ 2 √ π tan -1 ( (1 -(r l xin ) 2 ) r l xin )] = 1 4 + 1 2π [ π 2 -cos -1 (r l xin )] = 1 4 + sin -1 (r l xin ) 2π =⇒ E[g inx i g iny i ] = ( 1 4 + sin -1 (r l xin ) 2π )r l gout σ 2 gout Corr(g inx i , g iny i ) = ( 1 4 + sin -1 (r l x in ) 2π )r l gout σ 2 gout σ 2 g out 2 r l gout = ( 1 2 + sin -1 (r l xin ) π )r l gout A.5 GeLU$Forward pass through GeLU is defined as,

$x out = GeLU(x in ) =⇒ x outi = x ini Φ(x ini ) where Φ(x) is CDF of Standard Normal Distribution at x = x ini 2 1 + erf( x ini √ 2 )$To get the mean of output of GeLU, we have

$E[x outi ] = ∞ -∞ x outi √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini (1 + erf( xin i √ 2 )) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞ x ini erf( xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x ini erf( xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini (Integral of odd function) = 1 2 √ 2πσ xin ∞ -∞ x ini erf( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.6.1.4 of Korotkov & Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a 1 √ a 2 + a 1 Substituting, a = 1 √ 2 , a 1 = 1 2σ 2 xin , we have E[x outi ] = 1 2 √ 2πσ xin 1 √ 2 1 2σ 2 x in 1 2 + 1 2σ 2 x in = 1 2 √ 2πσ xin 2σ 3 xin σ 2 xin + 1 µ xout = σ 2 xin 2π(σ 2 xin + 1)$For calculating variance of output,

$E[x 2 outi ] = ∞ -∞ x 2 outi √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x 2 ini (1 + erf( xin i √ 2 )) 2 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = ∞ -∞ x 2 ini 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞$x 2 ini erf(

$xin i √ 2 ) 2 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini + ∞ -∞ x 2 ini erf 2 ( xin i √ 2 ) 4 √ 2πσ xin exp ( -x 2 ini 2σ 2 xin )dx ini = σ 2 xin 4 + ∞ -∞ x 2 ini erf 2 ( xin i √ 2 ) 4 √ 2πσ xin exp ( -x 2 ini 2σ2$
## xin

)dx ini (Definition of variance, and integral of odd function)

$= σ 2 xin 4 + 1 4 √ 2πσ xin ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.7.3.3 of Korotkov & Korotkov (2020) ∞ -∞ z 2 exp (-az 2 )erf(a 1 z)erf(a 2 z) = 1 √ π ( 1 a √ a tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) + a 1 a 2 (2a + a 2 1 + a 2 2 ) a a + a 2 1 + a 2 2 (a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 ) ) Substituting a = 1 2σ 2 x in , a 1 = a 2 = 1 √ 2 ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini = 1 √ π (2 √ 2σ 3 xin tan -1 ( 1 2 1 4σ 4 x in + 1 2σ 2 x in ) + 1 2 ( 1 σ 2 x in + 1) 1 2σ 2 x in 1 2σ 2 x in + 1( 1 4σ 4 x in + 1 2σ 2 x in + 1 4 ) ) = 1 √ π (2 √ 2σ 3 xin tan -1 ( σ 2 xin (σ 2 xin + 1) 2 -σ 4 xin ) + 4 √ 2σ 5 xin (σ 2 xin + 1) 2σ 2 xin + 1(σ 4 xin + 2σ 2 xin + 1) ) = 1 √ π (2 √ 2σ 3 xin sin -1 ( σ 2 xin σ 2 xin + 1 ) + 4 √ 2σ 5 xin 2σ 2 xin + 1(σ 2 xin + 1) ) = 2 √ 2σ 3 xin √ π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1)$)

)

$E[x 2 outi ] = σ 2 xin 4 + 1 4 √ 2πσ xin ∞ -∞ x 2 ini erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini = σ 2 xin 4 + 1 4 √ 2πσ xin 2 √ 2σ 3 xin √ π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1)$)

)

$E[x 2 outi ] = σ 2 xin 4 + σ 2 xin 2π (sin -1 ( σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin 2σ 2 xin + 1(σ 2 xin + 1) )) Var(x outi ) = E[x 2 outi ] -(E[x outi ]) 2 σ 2 xout = σ 2 xin 2π ( π 2 - σ 2 xin 1 + σ 2 xin + sin -1 ( σ 2 xin 1 + σ 2 xin ) + 2σ 2 xin (1 + σ 2 xin ) 1 + 2σ 2 xin )$Now if we have two inputs x in and y in such that for all values of i, we have Corr(x ini , y ini ) = r l xin , then we can calculate the covariance Cov(x outj , y outj ) for any j as,

$Cov(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] E[x outj y outj ] = ∞ -∞ x outj y outj 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 inj + 2r l xin x inj y inj -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = I = ∞ -∞ x inj (1 + erf( xin j √ 2 ))y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 inj + 2r l xin x inj y inj -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj dy inj = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy inj Where I X = ∞ -∞ x inj (1 + erf( x inj √ 2 )) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X = ∞ -∞ x inj (1 + erf( x inj √ 2 )) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj + ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj Let, I X,1 = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X,2 = ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj I X,1 = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -(r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) ∞ -∞ x inj exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = ∞ -∞ x inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( -(x inj -r l xin y inj ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) I X,2 = ∞ -∞ x inj erf( x inj √ 2 ) exp ( -x 2 inj + 2r l xin x inj y inj 2σ 2 xin (1 -(r l xin ) 2 ) )dx inj From 2.7.2.4 of Korotkov & Korotkov (2020), ∞ -∞ zerf(a 1 z) exp (-az 2 + bz)dz = = √ πb 2a √ a exp ( b 2 4a )erf( a 1 b 2 a 2 + aa 2 1 ) + a 1 a a + a 2 1 exp ( b 2 4a + 4a 2 1 ) Substituting a 1 = 1 √ 2 , a = 1 2σ 2 x in (1-(r l x in ) 2 ) , b = r l x in yin j σ 2 x in (1-(r l x in$) 2 ) , we get

$I X,2 = √ π r l x in yin j σ 2 x in (1-(r l x in ) 2 ) 2 1 2 √ 2σ 3 x in (1-(r l x in ) 2 ) 3 2 exp ( (r l x in ) 2 y 2 in j σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) )erf( r l x in yin j √ 2σ 2 x in (1-(r l x in ) 2 ) 2 1 4σ 4 x in (1-(r l x in ) 2 ) 2 + 1 4σ 2 x in (1-(r l x in ) 2 ) ) + 1 √ 2 1 2σ 2 x in (1-(r l x in ) 2 ) 1 2σ 2 x in (1-(r l x in ) 2 ) + 1 2 exp ( (r l x in ) 2 y 2 in j σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) + 4 2 ) = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) + 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )$Let us define I X,2,1 and I X,2,2 as:

$I X,2,1 = r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) I X,2,2 = 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) ) I = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy inj = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )(I X,1 + I X,2,1 + I X,2,2 )dy inj I 1 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X,1 dy inj I 2 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )I X,2,1 dy inj I 3 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 )$)I X,2,2 dy inj

We have 

$I = I 1 + I 2 + I 3 I 1 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )dy inj = r l xin 4 ∞ -∞ y 2 inj (1 + erf( yin j √ 2 )) √ 2πσ 2 xin exp ( -y 2 inj 2σ2$$I 2 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )r l xin y inj √ 2πσ xin (1 -(r l xin ) 2 ) exp ( (r l xin ) 2 y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj (1 + erf( y inj √ 2 )) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj + r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj erf( y inj √ 2 ) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj = r l xin 4 √ 2πσ xin ∞ -∞ y 2 inj erf( y inj √ 2 ) exp ( -y 2 inj 2σ 2 xin )erf( r l xin y inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy inj (Integral of Odd function) From 2.7.3.3 of Korotkov & Korotkov (2020), ∞ -∞ z 2 exp (-az 2 )erf(a 1 z)erf(a 2 z) = 1 √ π ( 1 a √ a tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) + a 1 a 2 (2a + a 2 1 + a 2 2 ) a a + a 2 1 + a 2 2 (a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 )$)

$Substituting a = 1 2σ 2 x in , a 1 = 1 √ 2 , a 2 = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) a 1 a 2 = r l xin 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1)$$a 2 + aa 2 1 + aa 2 2 = 1 4σ 4 xin + 1 4σ 2 xin + (r l xin ) 2 4σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = σ 2 xin (1 -(r l xin ) 2 ) + 1 + σ 4 xin (1 -(r l xin ) 2 ) + σ 2 xin + (r l xin ) 2 σ 2 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) a + a 2 1 + a 2 2 = a 2 + aa 2 1 + aa 2 2 a = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) * 2σ 2 xin = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)$$a 2 + aa 2 1 + aa 2 2 + a 2 1 a 2 2 = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) + (r l xin ) 2 4(σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + (r l xin ) 2 σ 4 xin 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 4σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 2a + a 2 1 + a 2 2 = 1 2σ 2 xin + (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + σ 2 xin (1 -(r l xin ) 2 ) + 1 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1) 2 + σ 2 xin + 1 -(r l xin σ 2 xin ) 2 -σ 2 xin (r l xin ) 2 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1)(σ 2 xin + 2) -(r l xin ) 2 σ 2 xin (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) = (σ 2 xin + 1)(σ 2 xin (1 -(r l xin ) 2 ) + 2) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) I 2 = r l xin 4 √ 2πσ xin (2 √ 2σ 3 xin tan -1 ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 4σ 4 x in (σ 2 x in (1-(r l x in ) 2 )+1) )) + r l xin 4 √ 2πσ xin ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1)(σ 2 x in (1-(r l x in ) 2 )+2) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2σ 2 x in (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2 4σ 4 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) = r l xin 4 √ 2πσ xin (2 √ 2σ 3 xin tan -1 ( r l xin σ 2 xin (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 )) + r l xin 4 √ 2πσ xin ( 2 √ 2r l xin σ 5 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2$)

$I 2 = r l xin σ 2 xin 2π (sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I 3 = ∞ -∞ y inj (1 + erf( yin j √ 2 )) 8πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 inj 2σ 2 xin (1 -(r l xin ) 2 ) ) 2σ 3 xin (1 -(r l xin ) 2 ) 3 2 σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( (r l xin ) 2 y 2 inj 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ -∞ σ xin (1 -(r l xin ) 2 )y inj (1 + erf( yin j √ 2 )) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( -y 2 inj (σ 2 xin (1 -(r l xin ) 2 ) + 1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = ∞ -∞ σ xin (1 -(r l xin ) 2 )y inj (1 + erf( yin j √ 2 )) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 exp ( -y 2 inj (σ 2 xin + 1)(1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj (1 + erf( y inj √ 2 )) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj + σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj erf( y inj √ 2 ) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ∞ -∞ y inj erf( y inj √ 2 ) exp ( -y 2 inj (σ 2 xin + 1) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin )dy inj (Integral of Odd function) From 2.6.1.4 of Korotkov & Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a 1 √ a 2 + a 1 Substituting, a = 1 √ 2 , a 1 = (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)$, we have

$I 3 = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 ( 1 √ 2 (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2 + (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) = σ xin (1 -(r l xin ) 2 ) 4π σ 2 xin (1 -(r l xin ) 2 ) + 1 2σ 3 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 (σ 2 xin + 1) σ 4 xin (1 -(r l xin ) 2 ) + σ 2 xin + σ 2 xin + 1 I 3 = σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)(1 -(r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2$Finally we have,

$I = I 1 + I 2 + I 3 = r l xin σ 2 xin 4 + r l xin σ 2 xin 2π (sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 2) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) + σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)(1 -(r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I = r l xin σ 2 xin 4 + r l xin σ 2 xin 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + σ 4 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I = σ 2 xin 4   r l xin + 2r l xin π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2  $We have,

$Cov(x outj , y outj ) = I -E[x outj ]E[y outj ] Cov(x outj , y outj ) = I - σ 4 xin 2π(σ 2 xin + 1) Cov(x outj , y outj ) = σ 2 xin 4π (πr l xin + 2r l xin sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1 + (r l xin ) 2 ) (σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 - 2σ 2 xin (σ 2 xin + 1)$)

The backward pass through GeLU is defined as,

$g ini = (Φ(x ini ) + x ini √ 2π exp ( -x 2 ini 2 ))g outi = ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outi$So the mean of gradient is obtained as following,

$E[g ini ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))]E[g outi ] = 0 µ gin = 0$Similarly for variance,

$E[g 2 ini ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 g 2 outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ]E[g 2 outi ] = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ]σ 2 gout I = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 ] = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I = ∞ -∞ ( 1 4 + erf 2 ( xin i √ 2 ) 4 + x 2 ini exp (-x 2 ini ) 2π + erf( xin i √ 2 ) 2 + x ini exp ( -x 2 in i 2 ) √ 2π + x ini exp ( -x 2 in i 2 )erf( xin i √ 2 ) √ 2π ) exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I 1 = ∞ -∞ 1 4 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini I 1 = 1 4 I 2 = ∞ -∞ erf 2 ( xin i √ 2 ) 4 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 4 √ 2πσ xin ∞ -∞ erf 2 ( x ini √ 2 ) exp ( -x 2 ini 2σ 2 xin )dx ini From 2.7.1.3 of Korotkov & Korotkov (2020), ∞ -∞ erf(a 1 z)erf(a 2 z) exp (-az 2 )dz = 2 √ πa tan -1 ( a 1 a 2 a 2 + aa 2 1 + aa 2 2 ) Substituting a = 1 2σ 2 x in , a 1 = a 2 = 1 √ 2 I 2 = 1 4 √ 2πσ xin 2 π 1 2σ 2 x in tan -1 ( 1 2 1 4σ 4 x in + 1 4σ 2 x in + 1 4σ 2 x in ) = 1 2π tan -1 ( σ 2 xin 2σ 2 xin + 1 ) = 1 2π tan -1 ( σ 2 xin (σ 2 xin + 1) 2 -σ 4 xin ) I 2 = 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) I 3 = ∞ -∞ x 2 ini exp (-x 2 ini ) 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 2πσ xin ∞ -∞ x 2 ini √ 2π exp ( -x 2 ini (2σ 2 xin + 1) 2σ 2 xin )dx ini = 1 2πσ xin σ xin (2σ 2 xin + 1) ∞ -∞ x 2 ini √ 2π σx in √ (2σ 2 x in +1) exp ( -x 2 ini (2σ 2 xin + 1) 2σ 2 xin )dx ini = 1 2πσ xin σ xin (2σ 2 xin + 1) σ 2 xin (2σ 2 xin + 1)$(Definition of variance)

$I 3 = σ 2 xin 2π(2σ 2 xin + 1) 3 2 I 4 = ∞ -∞ erf( xin i √ 2 ) 2 exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 (Integral of odd function) I 5 = ∞ -∞ x ini exp ( -x 2 in i 2 ) √ 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 0 (Integral of odd function) I 6 = ∞ -∞ x ini exp ( -x 2 in i 2 )erf( xin i √ 2 ) √ 2π exp ( -x 2 in i 2σ 2 x in ) √ 2πσ xin dx ini = 1 2πσ xin ∞ -∞ x ini erf( x ini √ 2 ) exp ( -x 2 ini (σ 2 xin + 1) 2σ 2 xin )dx ini From 2.6.1.4 of Korotkov & Korotkov (2020), ∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = 1 √ 2 , a 1 = (σ 2 x in +1) 2σ 2$x in , we have

$I 6 = 1 2πσ xin 1 √ 2 (σ 2 x in +1) 2σ 2 x in 1 2 + (σ 2 x in +1) 2σ 2 x in = 1 2πσ xin 2σ 3 xin (σ 2 xin + 1) 2σ 2 xin + 1 I 6 = σ 2 xin π(σ 2 xin + 1) 2σ 2 xin + 1 I = I 1 + I 2 + I 3 + I 4 + I 5 + I 6 = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin 2π(2σ 2 xin + 1) 3 2 + σ 2 xin π(σ 2 xin + 1) 2σ 2 xin + 1 = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (4σ 2 xin + 2 + σ 2 xin + 1) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2 I = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (5σ 2 xin + 3) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2$So the variance of gradient of input of GeLU comes out to be

$E[g 2 ini ] = Iσ 2 gout σ 2 gin = 1 4 + 1 2π sin -1 ( σ 2 xin σ 2 xin + 1 ) + σ 2 xin (5σ 2 xin + 3) 2π(σ 2 xin + 1)(2σ 2 xin + 1) 3 2 σ 2 gout$If for two inputs x in and y in for all i we have Corr(g outx i , g outy i ) = r l gout , and g inx i , g iny i be the gradient after passing through GeLU layer. Then we have,

$E[g inx i g iny i ] = = E[( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 ))g outx i ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))g outy i ] E[g inx i g iny i ] = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))]E[g outx i g outy i ] = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))]r l gout σ 2 gout I = E[( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))] = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 ))+ x ini √ 2π exp ( -x 2 ini 2 ))( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 ))p xin i ,yin i dx ini dy ini Where p xin i ,yin i = 1 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -x 2 ini + 2r l xin x ini y ini -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) I = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X dy ini$Where,

$I X = ∞ -∞ ( 1 2 (1 + erf( x ini √ 2 )) + x ini √ 2π exp ( -x 2 ini 2 )) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini I X,1 = ∞ -∞ 1 2 exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 ∞ -∞ exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( -(r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) ∞ -∞ exp ( -(x ini -r l xin y ini ) 2 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) ∞ -∞ exp ( -(xin i -r l x in yin i ) 2 2σ 2 x in (1-(r l x in ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) dx ini I X,1 = √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) I X,2 = ∞ -∞ erf( xin i √ 2 ) 2 exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = 1 2 ∞ -∞ erf( x ini √ 2 ) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini From 2.7.1.6 of Korotkov & Korotkov (2020), ∞ -∞ erf(a 1 z) exp (-az 2 + bz)dz = π a exp ( b 2 4a )erf( a 1 b 2 a 2 + aa 2 1 ) Substituting a 1 = 1 √ 2 , a = 1 2σ 2 x in (1-(r l x in ) 2 ) , b = r l x in yin i σ 2 x in (1-(r l x in ) 2 ) I X,2 = 1 2 π 1 2σ 2 x in (1-(r l x in ) 2 ) exp ( (r l x in ) 2 y 2 in i σ 4 x in (1-(r l x in ) 2 ) 2 4 1 2σ 2 x in (1-(r l x in ) 2 ) )erf( r l x in yin i √ 2σ 2 x in (1-(r l x in ) 2 ) 2 1 4σ 4 x in (1-(r l x in ) 2 ) 2 + 1 4σ 2 x in (1-(r l x in ) 2 ) ) I X,2 = √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)$)

$I X,3 = ∞ -∞ x ini √ 2π exp ( -x 2 ini 2 ) exp ( -x 2 ini + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini (σ 2 xin (1 -(r l xin ) 2 ) + 1) + 2r l xin x ini y ini 2σ 2 xin (1 -(r l xin ) 2 ) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini + 2r l x in xin i yin i (σ 2 x in (1-(r l x in ) 2 )+1) 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = ∞ -∞ x ini √ 2π exp ( -x 2 ini + 2r l x in xin i yin i (σ 2 x in (1-(r l x in ) 2 )+1) 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) ) exp ( -(r l x in ) 2 y 2 in i (σ 2 x in (1-(r l x in ) 2 )+1) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) ) * exp ( (r l x in ) 2 y 2 in i (σ 2 x in (1-(r l x in ) 2 )+1) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) * ∞ -∞ x ini √ 2π exp ( -(x ini - r l x in yin i (σ 2 x in (1-(r l x in ) 2 )+1) ) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) ∞ -∞ x ini √ 2π σx in √ 1-(r l x in ) 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) exp ( -(x ini - r l x in yin i (σ 2 x in (1-(r l x in ) 2 )+1) ) 2 2σ 2 x in (1-(r l x in ) 2 ) (σ 2 x in (1-(r l x in ) 2 )+1) )dx ini = exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1)$).

$σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) r l xin y ini (σ 2 xin (1 -(r l xin ) 2 ) + 1) I X,3 = r l xin y ini σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) ) I = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )(I X,1 + I X,2 + I X,3 )dy ini I 1 = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,1 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )dy ini = 1 2 ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini I 1,1 = 1 4 ∞ -∞ 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )dy ini = 1 4 I 1,2 = 1 4 ∞ -∞ erf( yin i √ 2 ) √ 2πσ xin exp ( -y 2 ini 2σ2$
## xin

)dy ini = 0 (Integral of odd function)

$I 1,3 = 1 2 ∞ -∞ y ini exp ( -y 2 in i 2 ) 2πσ xin exp ( -y 2 ini 2σ2$
## xin

)dy ini = 0 (Integral of odd function)

$I 2 = ∞ -∞$( 1 2 (1 + erf(

$yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,2 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) √ 2πσ xin (1 -(r l xin ) 2 ) 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = 1 2 ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) √ 2πσ xin exp ( -y 2 ini 2σ 2 xin ). erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini I 2,1 = 1 4 ∞ -∞ 1 √ 2πσ xin exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)$)dy ini = 0 (Integral of odd function)

$I 2,2 = 1 4 √ 2πσ xin ∞ -∞ erf( y ini √ 2 ) exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini From 2.7.1.3 of Korotkov & Korotkov (2020), ∞ -∞ erf(a 1 z)erf(a 2 z) exp (-az 2 )dz = 2 √ πa tan -1 ( a1a2 √ a 2 +aa 2 1 +aa 2 2 ) Substituting a = 1 2σ 2 x in , a 1 = 1 √ 2 , a 2 = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) I 2,2 = 1 4 √ 2πσ xin 2 π 1 2σ 2 x in tan -1 ( r l x in 2 √ (σ 2 x in (1-(r l x in ) 2 )+1) 1 4σ 4 x in + 1 4σ 2 x in + (r l x in ) 2 4σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) ) I 2,2 = 1 2π tan -1 ( r l xin σ 2 xin σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin ) = 1 2π tan -1 ( r l xin σ 2 xin (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I 2,2 = 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1$)

$I 2,3 = 1 4πσ xin ∞ -∞ y ini exp ( -y 2 ini 2 ) exp ( -y 2 ini 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = 1 4πσ xin ∞ -∞ y ini exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin )erf( r l xin y ini 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)$)dy ini From 2.6.1.4 of [Korotkov & Korotkov (2020)](#b54),

$∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) , a 1 = (σ 2 x in +1) 2σ 2$x in , we have

$I 2,3 = 1 4πσ xin r l x in √ 2(σ 2 x in (1-(r l x in ) 2 )+1) (σ 2 x in +1) 2σ 2 x in (r l x in ) 2 2(σ 2 x in (1-(r l x in ) 2 )+1) + (σ 2 x in +1) 2σ 2 x in = r l xin σ 2 xin 2π(σ 2 xin + 1) σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin I 2,3 = r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I 3 = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) )I X,3 dy ini = ∞ -∞ ( 1 2 (1 + erf( yin i √ 2 )) + yin i √ 2π exp ( -y 2 in i 2 )) 2πσ 2 xin (1 -(r l xin ) 2 ) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) r l xin y ini σ xin 1 -(r l xin ) 2 (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini 2σ 2 xin (1 -(r l xin ) 2 ) ) exp ( (r l xin ) 2 y 2 ini 2σ 2 xin (1 -(r l xin ) 2 )(σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini (σ 2 xin (1 -(r l xin ) 2 ) + 1 -(r l xin ) 2 ) 2(σ 2 xin (1 -(r l xin ) 2 ) + 1)σ 2 xin (1 -(r l xin ) 2 ) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini ( 1 2 (1 + erf( y ini √ 2 )) + y ini √ 2π exp ( -y 2 ini 2 )) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)$)dy ini

$I 3,1 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)$)dy ini = 0 (Integral of odd function)

$I 3,2 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y ini erf( y ini √ 2 ) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1)$)dy ini From 2.6.1.4 of [Korotkov & Korotkov (2020)](#b54),

$∞ -∞ zerf(az) exp (-a 1 z 2 )dz = a a1 √ a 2 +a1 Substituting, a = 1 √ 2 , a 1 = (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in$) 2 )+1) , we have

$I 3,2 = r l xin 4πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 1 √ 2 (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) 1 2 + (σ 2 x in +1) 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1) = r l xin σ 2 xin 2π(σ 2 xin + 1) σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin I 3,2 = r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 I 3,3 = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 . ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini 2 ) exp ( -y 2 ini (σ 2 xin + 1) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 . ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini (σ 4 xin + 2σ 2 xin + 1 -(r l xin ) 2 σ 4 xin ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ∞ -∞ y 2 ini √ 2π exp ( -y 2 ini ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 σ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ∞ -∞ y 2 ini √ 2π σx in √ (σ 2 x in (1-(r l x in ) 2 )+1) √ (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 exp ( -y 2 ini ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 2σ 2 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) )dy ini = r l xin 2πσ xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 σ 3 xin (σ 2 xin (1 -(r l xin ) 2 ) + 1) 3 2 ((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 I 3,3 = r l xin σ 2 xin 2π((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) I = I 1 + I 2 + I 3 = I 1,1 + I 1,2 + I 1,3 + I 2,1 + I 2,2 + I 2,3 + I 3,1 + I 3,2 + I 3,3 I = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 )+ 2r l xin σ 2 xin 2π(σ 2 xin + 1) (σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 + r l xin σ 2 xin 2π((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 I = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin ((2σ 2 xin + 3)(σ 2 xin + 1) -2(r l xin σ 2 xin ) 2 ) 2π(σ 2 xin + 1)((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2$We defined Cov(g inx i , g iny i ), as

$Cov(g inx i , g iny i ) = Ir l gout σ 2 gout Cov(g inx i , g iny i ) = 1 4 + 1 2π sin -1 ( r l xin σ 2 xin σ 2 xin + 1 ) + r l xin σ 2 xin ((2σ 2 xin + 3)(σ 2 xin + 1) -2(r l xin σ 2 xin ) 2 ) 2π(σ 2 xin + 1)((σ 2 xin + 1) 2 -(r l xin σ 2 xin ) 2 ) 3 2 r l gout σ 2 gout A.6 LayerNorm$The affine transformation for layernorm are typically initialized with 1 scale and 0 bias, so they do not change any of our derivations below and are ignored henceforth. For an input x in the forward pass of LayerNorm is,

$x out = LayerNorm(x in ) =⇒ x outi = x ini -xin σxin Where xin = din i=1 x ini d in σxin = din i=1 (x ini -xin ) 2 d in$To get expectation of output of LayerNorm,

$E[x outi ] = E[ x ini -xin σxin ] din i=1 E[x outi ] = din i=1 E[ x ini -xin σxin ] = E[ din i=1 x ini -xin σxin ] = E[ din i=1 (x ini -xin ) σxin ] din i=1 E[x outi ] = 0$By symmetry for any i, j and i ̸ = j we have

$E[x outi ] = E[x outj ] = µ xout =⇒ d in µ xout = 0 µ xout = 0$Similarly we calculate variance of output by,

$Var(x outi ) = E[x 2 outi ] -E[x outi ] 2 = E[x 2 outi ] E[x 2 outi ] = E[ (x ini -xin ) 2 σ2 xin ] din i=1 E[x 2 outi ] = din i=1 E[ (x ini -xin ) 2 σ2 xin ] = E[ din i=1 (x ini -xin ) 2 σ2 xin ] = E[ din i=1 (x ini -xin ) 2 σ2 xin ] din i=1 E[x 2 outi ] = d in$By symmetry for any i, j and i ̸ = j we have

$E[x 2 outi ] = E[x 2 outj ] = σ 2 xout =⇒ d in σ 2 xout = d in σ 2 xout = 1$Now we have σxin a.s -→ σ xin for large d in . So for large values of d in we can treat σxin as a constant which has value σ xin . We use this approximation to get the following results. For two inputs x in and y in such that for all i, Corr(x ini , y ini ) = r l xin . For all j we have,

$Corr(x outj , y outj ) = E[x outj y outj ] -E[x outj ]E[y outj ] Var(x outj )Var(y outj ) = E[x outj y outj ] -µ xout µ xout σ 2 xout σ 2 xout = E[x outj y outj ] -0 √ 1 = E[x outj y outj ] = E[ (x inj -xin )(y inj -ȳin ) σxin σyin ] ≈ E[ (x inj -xin )(y inj -ȳin ) σ xin σ xin ] = E[(x inj -xin )(y inj -ȳin )] σ 2 xin = E[(x inj - d in k=1 xin k din )(y inj - d in l=1 yin l din )] σ 2 xin = E[x inj y inj -y inj d in k=1 xin k din -x inj d in l=1 yin l din + d in k=1 xin k din d in l=1 yin l din ] σ 2 xin$Elements belonging to different dimensions from x in and y in are independent of each other and hence for i, j and i ̸ = j we have

$E[x ini y inj ] = µ 2 xin . = E[x inj y inj ] -E[y inj d in k=1 xin k din ] -E[x inj d in l=1 yin l din ] + E[ d in k=1 xin k din d in l=1 yin l din ] σ 2 xin = r l xin σ 2 xin + µ 2 xin - r l x in σ 2 x in +dinµ 2 x in din - r l x in σ 2 x in +dinµ 2 x in din + r l x in dinσ 2 x in +d 2 in µ 2 x in d 2 in σ 2 xin = r l xin σ 2 xin (1 -1 din ) σ 2 xin Corr(x outj , y outj ) = r l xin (1 - 1 d in ) ≈ r l xin = r l xout$From Xu et al. ( [2019](#)) (Eq. 17), the backward pass through LayerNorm is,

$g in = g out σxin (I din - 1 T din 1 din + x T out x out d in ) ≈ g out σ xin (I din - 1 T din 1 din + x T out x out d in )$We have lim

$din→∞ 1 T din 1 din + x T out x out d in = O din,din where O din,din is zero matrix with shape d in × d in g in ≈ g out σ xin (I din ) = g out σ xin =⇒ g ini = g outi σ xin If µ gout = 0, µ gin = 0 σ 2 gin = σ 2 gout σ 2 xin A.7 Softmax$Assumption: Other than assuming normally distributed inputs, we also assume that L is large L >> 1 to derive softmax variance.

The forward pass of Softmax can be defined as

$x out = Softmax(x in ) x outi = e xin i L j=1 e xin j$For calculating mean we can easily see that, L i=1

x outi = 1

Taking expectation both sides, we get

$E[ L i=1 x outi ] = 1 L i=1 E[x outi ] = 1$By symmetry we can assume that for any i, j, i ̸ = j, we have

$E[x outi ] = E[x outj ] LE[x outi ] = 1 µ xout = 1 L$Let us define z = j e yj where y j = x jx i is normally distributed N (0, σ j ). Hence, each e yj is log-normally distributed, and z is a sum of correlated log-normals. Following [(Lo, 2013)](#b68), this sum of log-normals can be approximated as another log-normal random variable, LogN (µ z , σ z ), where µ z and σ z are as follows -

$S + = E[ j y j ] = j e σ 2 j 2 σ 2 z = 1 S 2 + j,k corr j,k σ j σ k e 1 2 (σ 2 j +σ 2 k ) µ z = ln(S + ) - σ 2 z 2$Since the difference of two normals x j and x i is also normal, from the M.G.F. of normal distribution, we have σ 2 j = 2σ 2

xin (1r xin ) if j ̸ = i, and σ 2 j = 0 if j = i.

Also, corr j,k = 0 if j = i or k = i, else corr j,k = 1 2 . We can substitute these values in the above equations, to get

$S + = (L -1)e σ 2 x in (1-rx in ) + 1 σ 2 z = σ 2 xin (1 -r xin ) L L -1 µ z = ln(S + ) - σ 2 z 2$Since z is log-normal, x out = 1 z is also log-normal with LogN (-µ z , σ z ). The variance of log-normal distribution can be obtained from standard formulae for log-normal distribution as (e σ 2 z -1)e σ 2 z -2µz .

Substituting the values of µ z and σ z from above, we get

$σ 2 xout = (e σ 2 z -1)e 2 * σ 2 z S 2 + = (e σ 2 x in (1-rx in ) L L-1 -1)e 2σ 2 x in (1-rx in ) L L-1 ((L -1)e σ 2 x in (1-rx in ) + 1) 2$For large L, we can ignore the 1 in the denominator -

$σ 2 xout = (e σ 2 x in (1-rx in ) L L-1 -1) (L -1) 2 If L >> 1 and σ 2$xin is small, we get the more simplified formula as -

$σ 2 xout ≈ (e (1-r d x in )σ 2 x in -1) L 2 (Assuming L >> 1)$Using the mean and variances, we can calculate the scale of softmax output as follows-

$E[x 2 out ] = σ 2 xout + µ 2 xout = (e (1-r d x in )σ 2 x in ) L 2$The Jacobian of Softmax can be calculated as ( [(Kim et al., 2021](#b51))):

$J i,j = x outi (1 -x outi ) if i = j -x outi x outj else$For large values of L this approximately becomes

$J ≈ diag(x out ) g in = g out J g ini ≈ g outi x outi E[g ini ] ≈ E[g outi x outi ] = E[g outi ]E[x outi ] = 0 = µ gin E[g 2 ini ] ≈ E[g 2 outi x 2 outi ] = E[g 2 outi ]E[x 2 outi ] σ 2 gin = σ 2 gout (e (1-r d x in )σ 2 x in ) L 2 A.8 Scaled Dot-Product Attention$Inapplicability of Direct Usage of Softmax Derivations for SHA: One may be tempted to assume attention scores to be independent of values. This then enables the use of our previous LogNormal-based softmax derivation, to easily derive the forward variances.

But the theoretically calculated moments strongly disagree with empirical simulations. This is because SHA is

$X out = Dropout(SoftMax( X in W Q W K T X T in √ d k$))X in W V , and the W K T X T in term cannot be treated independently of the X in W V term. A simple verification of this can be checked by simply simulating (XW T )X, and verifying that the variances of the results do not match that of L * σ 2 ((XW)), but do if the second X is replaced by another random tensor.

This necessitates an alternate methodology to derive SHA, where the components are treated as a unified whole.

Assumption: We assume that L and d in are very large when compared to scale of scores being passed to the Softmax. These approximations hold true for small values of σ q and σ k , and the resulting formulae are fairly accurate, as shown in the numerical verification section.

The forward pass of Scaled Dot-Product Attention is

$X out = Dropout(SoftMax( QK T d i,k ))V$Where,

$Q = X in W Q K = X in W K V = X in W V X out = Dropout(SoftMax( X in W Q W K T X T in d i,k ))X in W V Let, O = Dropout(SoftMax( X in W Q W K T X T in d i,k ))X in W = X in W Q W K T d i,k O = Dropout(SoftMax(WX T in ))X in Using results from Linear Layer we have σ 2 w = d in σ 2 xin σ 2 q σ 2 k = d in σ 2 xin σ 2 qk O i,j = L k=1 Dropout(SoftMax(WX T in )) i,k X in k,j = L k=1 Dropout( exp ((WX T in ) i,k ) L m=1 exp ((WX T in ) i,m ) )X in k,j = L k=1 Dropout(exp ((WX T in ) i,k )) L m=1 exp ((WX T in ) i,m ) X in k,j = L k=1 Dropout(exp ((WX T in ) i,k ))X in k,j L m=1 exp ((WX T in ) i,m ) = L k=1 Dropout(exp ( din l=1 W i,l X in k,l ))X in k,j L m=1 exp ( din n=1 W i,n X inm,n ) = L k=1 Dropout(exp ( din l=1 W i,l X in k,l )X in k,j ) L m=1 exp ( din n=1 W i,n X inm,n )$Each X ini,j can be written as:

$X ini,j = ϵ j + δ i,j$Where ϵ j and δ i,j are all independent and defined as

$ϵ j ∼ N (0, r l xin σ 2 xin ) δ i,j ∼ N (0, (1 -r l xin )σ 2 xin ) O i,j = L k=1 Dropout(exp ( din l=1 W i,l X in k,l )X in k,j ) L k=1 exp ( din l=1 W i,l X in k,l ) = L k=1 (1 -d i,k )(exp ( din l=1 W i,l X in k,l )X in k,j ) (1 -p) L k=1 exp ( din l=1 W i,l X in k,l )$Where d i,k is Bernoulli random variable which is 1 with probability p

$= L k=1 (1 -d i,k ) exp ( din l=1 W i,l (ϵ l + δ k,l ))(ϵ j + δ k,j ) (1 -p) L k=1 exp ( din l=1 W i,l (ϵ l + δ k,l )) = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) + L k=1 (1 -d i,k ) exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l ϵ l ) exp ( din l=1 W i,l δ k,l ) = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) + L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) Let v 1 = ϵ j L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) (1-p) L k=1 exp ( d in l=1 W i,l δ k,l ) and v 2 = L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j (1-p) L k=1 exp ( d in l=1 W i,l δ k,l )$. We have,

$O i,j = v 1 + v 2$Given a fixed ϵ, W , we have

$v 1 |ϵ, W = ϵ j L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l ) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) = ϵ j L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) L (1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L By WLLN, L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )],$and

$(1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )] Thus, we have v 1 |ϵ, W p →ϵ j v 2 |ϵ, W = L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) = 1 √ L √ L L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j L (1 -p) L k=1 exp ( d in l=1 W i,l δ k,l ) L Let µ num = E δ,d [(1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j ], σ 2 num = Var δ,d ((1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j ). By central limit theorem for large L, √ L L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L = √ L L k=1 (1 -d i,k )(exp ( din l=1 W i,l δ k,l )δ k,j -µ num ) L + √ Lµ num √ L L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L d →N (0, σ 2 num ) + √ Lµ num L k=1 (1 -d i,k ) exp ( din l=1 W i,l δ k,l )δ k,j L d →N (µ num , σ 2 num L ) µ num = E d [1 -d i,k ]( l=d l=1,l̸ =j E δ [exp (W i,l δ k,l )])E δ [exp (W i,j δ k,j )δ k,j ] E δ [exp (W i,l δ k,l )] = exp ( W 2 i,l σ 2 δ 2 ) (MGF of gaussian) E δ [exp (W i,j δ k,j )δ k,j ] = ∞ -∞ exp (W i,j δ k,j )δ k,j √ 2πσ δ exp (- δ 2 k,j 2σ 2 δ )dδ k,j = ∞ -∞ exp ( W 2 i,j σ 2 δ 2 ) δ k,j √ 2πσ δ exp (- (δ k,j -W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp ( W 2 i,j σ 2 δ 2 ) ∞ -∞ δ k,j √ 2πσ δ exp (- (δ k,j -W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp ( W 2 i,j σ 2 δ 2 )W i,j σ 2 δ µ num = (1 -p) exp ( din l=1 W 2 i,l σ 2 δ 2 )W i,j σ 2 δ σ 2 num = E d [(1 -d i,k ) 2 ]( l=d l=1,l̸ =j E δ [exp (2W i,l δ k,l )]E δ [exp (2W i,j δ k,j )δ 2 k,j ] -µ 2 num ) E δ k,l [exp (2W i,l δ k,l )] = exp (2W 2 i,l σ 2 δ ) (MGF of gaussian) E δ k,j [exp (2W i,j δ k,j )δ 2 k,j ] = ∞ -∞ exp (2W i,j δ k,j )δ 2 k,j √ 2πσ δ exp (- δ 2 k,j 2σ 2 δ )dδ k,j = ∞ -∞ exp (2W 2 i,j σ 2 δ ) δ 2 k,j √ 2πσ δ exp (- (δ k,j -2W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp (2W 2 i,j σ 2 δ ) ∞ -∞ δ 2 k,j √ 2πσ δ exp (- (δ k,j -2W i,j σ 2 δ ) 2 2σ 2 δ )dδ k,j = exp (2W 2 i,j σ 2 δ )(4W 2 i,j σ 4 δ + σ 2 δ ) σ 2 num = (1 -p) exp (2 din l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ + σ 2 δ ) -(1 -p) 2 exp ( din l=1 W 2 i,l σ 2 δ )W 2 i,j σ 4 δ Similarly, L k=1 exp ( din l=1 W i,l δ k,l$) is also a sum of L i.i.d. random variables for fixed W . By WLLN we have,

$(1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) L p →(1 -p)E δ [exp ( din l=1 W i,l δ k,l )] p →(1 -p)( l=d l=1 E δ [exp (W i,l δ k,l )]) (1 -p) L k=1 exp ( din l=1 W i,l δ k,l ) L p →(1 -p) exp ( din l=1 W 2 i,l σ 2 δ 2 ) v 2 = L k=1 exp ( d in l=1 W i,l δ k,l )δ k,j L L k=1 exp ( d in l=1 W i,l δ k,l ) L$As for a given W, ϵ, both the numerator and denominator converge in distribution and denominator is converging to a constant by Slutskys theorem,

$v 2 |W, ϵ d →N ( µ num (1 -p) exp ( d in l=1 W 2 i,l σ 2 δ 2 ) , σ 2 num L(1 -p) 2 exp ( din l=1 W 2 i,l σ 2 δ ) ) v 2 |W, ϵ d →N (W i,j σ 2 δ , exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L )$Thus we have,

$O i,j |W, ϵ ∼ N (W i,j σ 2 δ , exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L ) + ϵ j$We have,

$E[O i,j |W ] = W i,j σ 2 δ + 0 = W i,j σ 2 δ E[O 2 i,j |W ] = exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L + σ 2 ϵ E[O i,j ] = E W [O i,j |W ] = E W [W i,j σ 2 δ ] = 0 E[O 2 i,j ] = E W [O 2 i,j |W ] = E W [W 2 i,j σ 4 δ + exp ( d in l=1 W 2 i,l σ 2 δ )(4W 2 i,j σ 4 δ +σ 2 δ ) (1-p) -W 2 i,j σ 4 δ L + σ 2 ϵ ]$For large d in by WLLN and continuous mapping theorem exp (

$din l=1 W 2 i,l σ 2 δ ) ≈ exp (d in σ 2 w σ 2 δ ) = (L -1)σ 2 w σ 4 δ + exp (dinσ 2 w σ 2 δ )(4σ 2 w σ 4 δ +σ 2 δ ) (1-p) L + σ 2 ϵ = (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 qk + exp ((1-r l x in )d 2 in σ 4 x in σ 2 qk )(4(1-r l x in ) 2 dinσ 6 x in σ 2 qk +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin$Hence,

$µ xout = 0 σ 2 xout = (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 qk + exp ((1-r l x in )d 2 in σ 4 x in σ 2 qk )(4(1-r l x in ) 2 dinσ 6 x in σ 2 qk +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin$Now to get covariance we make two approximations. As the term

$L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l ) (1-p) L k=1 exp ( d in l=1 W i,l δ k,l )$converges to 1, we approximate v 1i,j ≈ ϵ j . Also we will treat

$L k=1 exp ( din l=1 W i,l δ k,l ) ≈ exp ( d in l=1 W 2 i,l σ 2 δ 2$). Then, we have

$v 1i,j ≈ ϵ j v 2i,j ≈ L k=1 (1-d i,k ) exp ( d in l=1 W i,l δ k,l )δ k,j L (1 -p) exp ( d in l=1 W 2 i,l σ 2 δ 2 )$This makes v 1i,j and v 2i,j independent. For covariance

$E[O i,j O m,j ] = E W [E[O i,j O m,j |W ]] O i,j O m,j |W = (v 1i,j + v 2i,j )(v 1m,j + v 2m,j ) = v 1i,j v 1m,j + v 1i,j v 2m,j + v 2i,j v 1m,j + v 2i,j v 2m,j v 1i,j v 1m,j = ϵ 2 j E[v 1i,j v 1m,j |W ] = σ 2 ϵ As v 1i,j = v 1m,j = ϵ j , v 1i,j v 2m,j + v 2i,j v 1m,j = ϵ j (v 2i,j + v 2m,j$), and ϵ j is independent of (v 2i,j + v 2m,j ). Thus, we have

$E[v 1i,j v 2m,j + v 2i,j v 1m,j |W ] = E[ϵ j |W ]E[(v 2i,j + v 2m,j )|W ] = 0 * E[(v 2i,j + v 2m,j )|W ] = 0 v 2i,j v 2m,j = L k1=1 (1-d i,k 1 ) exp ( d in l=1 W i,l δ k 1 ,l )δ k 1 ,j L L k2=1 (1-d m,k 2 ) exp ( d in l=1 W m,l δ k 2 ,l )δ k 2 ,j L (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) E[v 2i,j v 2m,j |W ] = E[ L k1=1 (1 -d i,k1 ) exp ( din l=1 W i,l δ k1,l )δ k1,j L k2=1 (1 -d m,k2 ) exp ( din l=1 W m,l δ k2,l )δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 )$Breaking summation into two parts:

$k 1 = k 2 = k and k 1 ̸ = k 2 , we get = E[ L k=1 (1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + E[ L k1=1 L k2=1,k2̸ =k1 (1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) = L k=1 E[(1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + L k1=1 L k2=1,k2̸ =k1 E[(1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] L 2 (1 -p) 2 exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) E[(1 -d i,k )(1 -d m,k ) exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] = = E[(1 -d i,k )]E[(1 -d m,k )]E[exp ( din l=1 (W i,l + W m,l )δ k,l )δ 2 k,j ] = (1 -p) 2 exp ( din l=1 (W i,l + W m,l ) 2 σ 2 δ 2 )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) E[(1 -d i,k1 )(1 -d m,k2 ) exp ( din l=1 W i,l δ k1,l ) exp ( din l=1 W m,l δ k2,l )δ k1,j δ k2,j ] = E[(1 -d i,k1 )]E[(1 -d m,k2 )]E[exp ( din l=1 W i,l δ k1,l )δ k1,j ]E[exp ( din l=1 W m,l δ k2,l )δ k2,j ] = (1 -p) 2 exp ( din l=1 (W 2 i,l + W 2 m,l )σ 2 δ 2 )W i,j W m,j σ 4 δ E[v 2i,j v 2m,j |W ] = exp ( d in l=1 (W i,l +W m,l ) 2 σ 2 δ 2 )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) + (L -1) exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 )W i,j W m,j σ 4 δ L exp ( d in l=1 (W 2 i,l +W 2 m,l )σ 2 δ 2 ) = exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L So, we have E[O i,j O m,j |W ] = σ 2 ϵ + exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L E[O i,j O m,j ] = E W [E[O i,j O m,j |W ]] = E W [σ 2 ϵ + exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L + (L -1)W i,j W m,j σ 4 δ L ] = σ 2 ϵ + E W [ exp ( din l=1 W i,l W m,l σ 2 δ )((W i,j + W m,j ) 2 σ 4 δ + σ 2 δ ) L ]$For large values of d in by WLLN and continuous mapping theorem we have exp ( din l=1 W i,l W m,l σ 2 δ ) ≈ 1. Thus, we have

$E[O i,j O m,j ] = σ 2 ϵ + (2σ 2 w σ 4 δ + σ 2 δ ) L E[O i,j O m,j ] = r l xin σ 2 xin + (2(1 -r l xin ) 2 d in σ 6 xin σ 2 qk + (1 -r l xin )σ 2 xin ) L$The convergence arguments we have made require the scale of the variables to be small when compared to L and d in . The growth in scale can be controlled easily by controlling σ qk , and we observe that if we let σ qk become arbitrarily large the scores passed to Softmax diverge leading to degenerate attention only attending to one token which has the highest score. To avoid this degenerate attention, we choose smaller values of σ q , σ k and in that scenario, the approximate value for variance and covariance are,

$σ 2 xout ≈ r l xin σ 2 xin Cov l xout ≈ r l xin σ 2 xin$To get the final variance and covariance we can use results of Linear layer to account for W V . If we initialize σ q and σ k to be small, in initial phase of training the output of Softmax layer can be treated as being a constant =

$1 T L 1 L L .$Using this assumption we have,

$X out ≈ Dropout( 1 T L 1 L L )X in W V =⇒ g Xin ≈ Dropout( 1 T L 1 L L ) T g Xout W V T = Dropout( 1 T L 1 L L )g Xout W V T µ gin = 0 σ 2 gin = σ 2 gout dσ 2 v L(1 -p) (1 + (L -1)r l gout (1 -p)) Cov l gin = σ 2 gout dσ 2 v L (1 + (L -1)r l gout )$B Moment Propagation through Transformer Blocks

## B.1 Transformer Attention Block

A forward pass through attention block consists of LayerNorm, followed by Scaled Dot-Product Attention, followed by an output projection layer (a Linear Layer), and finally a Dropout. Using the results from above we get,

$µ xout = 0 * 0 * 0 * 0 = 0 σ 2 xout =   (1 -r l xin ) 2 (L -1)d in σ 6 xin σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 dinσ 6 x in σ 2 q σ 2 k +(1-r l x in )σ 2 x in ) (1-p) L + r l xin σ 2 xin   .d in σ 2 v . d in σ 2 o (1 -p) = d 2 in σ 2 o σ 2 v σ 2 xin (1 -p)   (1 -r l xin ) 2 (L -1)d in σ 4 xin σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin   Cov l xout = r l xin σ 2 xin + (2(1 -r l xin ) 2 d in σ 6 xin σ 2 q σ 2 k + (1 -r l xin )σ 2 xin ) L .d in σ 2 v .d in σ 2 o .1 = d 2 in σ 2 o σ 2 v σ 2 xin r l xin + (2(1 -r l xin ) 2 d in σ 4 xin σ 2 q σ 2 k + (1 -r l xin )) L σ 2 gin = σ 2 gout * 1 (1 -p) * d in σ 2 o * d in σ 2 v L(1 -p) (1 + (L -1)r l gout (1 -p)) = d 2 in σ 2 gout σ 2 v σ 2 o L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) Cov l gin = σ 2 gout * 1 * d in σ 2 o * d in σ 2 v L (1 + (L -1)r l gout ) = d 2 in σ 2 gout σ 2 v σ 2 o L (1 + (L -1)r l gout )$
## B.2 Transformer FFN Block

A forward pass through the FFN block of a transfer has a LayerNorm, then a Linear layer from d to 4d, which is then passed through a ReLU gate, the output of which is the projected back to d dimension using another Linear layer, and eventually passed through a Dropout. Again using the results from above we get, µ xout = 0 (Last Linear Layer makes it 0)

$σ 2 xout = 1 * d in σ 2 w1 * ( π -1 2π + 1 2π ) * 4d in σ 2 w2 * 1 (1 -p) * σ 2 xin = 2d 2 in σ 2 w1 σ 2 w2 (1 -p) σ 2 xin Cov l xout = d in σ 2 w1 * ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π - 1 2π + 1 2π ) * 4d in σ 2 w2 * σ 2 xin = 4d 2 in σ 2 w1 σ 2 w2 σ 2 xin ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π ) r l xout = 2 * (1 -p) * ( r l xin 4 + (1 -(r l xin ) 2 ) 0.5 2π + r l xin sin -1 (r l xin ) 2π ) ≈ (1 -p) * ( r l xin 2 + 1 π + ( 1 2 - 1 π )r l xin 2 ) (Fitting a 2-nd order polynomial) σ 2 gin = σ 2 gout * 1 (1 -p) * d in σ 2 w2 * 1 2 * 4d in σ 2 w1 = 2d 2 in σ 2 w1 σ 2 w2 σ 2 gout (1 -p) Cov l gin = Cov l gout * 1 * d in σ 2 w2 * ( 1 4 + sin -1 (r l xin ) 2π ) * 4d in σ 2 w1 = 4d 2 in σ 2 w1 σ 2 w2 Cov l gout ( 1 4 + sin -1 (r l xin ) 2π )$
## C Summary Table of Moment Propagation through Transformer Components

In Table [15](#tab_17), Table [16](#tab_18), Table [17](#tab_19), Table [18](#tab_8), Table [19](#tab_20) and Table [20](#tab_21), we summarize the signal propagation formulae for all the transformer components.  

$ReLU (π -1) (2π) σ 2 xin GeLU σ 2 x in 2π ( π 2 - σ 2 x in 1+σ 2 x in + sin -1 ( σ 2 x in 1+σ 2 x in ) + 2σ 2 x in (1+σ 2 x in ) √ 1+2σ 2 x in ) Layer Norm (d) 1 Dropout (p) σ 2 xin + pµ 2 xin 1 -p Softmax (e σ 2 x in (1-r l x in ) L L-1 -1)e 2σ 2 x in (1-r l x in ) L L-1 ((L-1)e σ 2 x in (1-r l x in ) +1) 2 SHA (without V) d in σ 2 xin (1 -p) (1-r l x in ) 2 (L-1)dinσ 4 x in σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 d in σ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin Attn Block (Approx) d 2 in σ 2 o σ 2 v σ 2 xin (1 -p) (1-r l x in ) 2 (L-1)dinσ 4 x in σ 2 q σ 2 k + exp ((1-r l x in )d 2 in σ 4 x in σ 2 q σ 2 k )(4(1-r l x in ) 2 d in σ 4 x in σ 2 q σ 2 k +(1-r l x in )) (1-p) L + r l xin FFN Block 2d 2 in σ 2 w1 σ 2 w2 σ 2 xin (1 -p)$$+ 1 2π sin -1 ( σ 2 x in σ 2 x in +1 ) + σ 2 x in (5σ 2 x in +3) 2π(σ 2 x in +1)(2σ 2 x in +1) 3 2 σ 2 gout LayerNorm (d) σ 2 gout σ 2 xin Dropout (p) 1 1 -p σ 2 gout Softmax ( (e σ 2 x in (1-r l x in ) L L-1 -1)e 2σ 2 x in (1-r l x in ) L L-1 ((L-1)e σ 2 x in (1-r l x in ) +1) 2 + 1 L 2 )σ 2 gout SHA Block (without V) d in σ 2 gout L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) Attn Block (Approx) d 2 in σ 2 gout σ 2 v σ 2 o L(1 -p) 2 (1 + (L -1)r l gout (1 -p)) FFN Block 2d 2 in σ 2 w1 σ 2 w2 σ 2 gout (1 -p)$Table [18](#tab_8). Covariance (along sequence length) propagation through the components of transformer model. 

## Component

$Cov l xout Embeddings N i * (N i -1) L * (L -1)) * σ 2 wembd FFN (d 1 .d 2 ) d 1 σ 2 w (Cov l xin + µ 2 xin ) ReLU ( 1 4 + sin -1 (r l xin ) 2π )Cov l xin -(1 -(1 -(r l xin ) 2 )) σ 2 xin 2π GeLU σ 2 x in 4π (πr l xin + 2r l xin sin -1 ( r l x in σ 2 x in σ 2 x in +1 ) + 2σ 2 x in (σ 2 x in (1-(r l x in ) 2 )+1+(r l x in ) 2 ) (σ 2 x in +1) √ (σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 - 2σ 2 x in (σ 2 x in +1) ) LayerNorm (d) (1 - 1 d ) Cov l xin σ 2 xin Dropout (p) Cov l xin SHA (without V) d in σ 2 xin r l xin + (2(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) L Attn Block (Approx) d 2 in σ 2 o σ 2 v σ 2 xin r l xin + (2(1-r l x in ) 2 dinσ 4 x in σ 2 q σ 2 k +(1-r l x in )) L FFN Block 4d in σ 2 w1 σ 2 w2 σ 2 xin ( r l xin 4 + (1 -(r l xin ) 2 2π + r l xin sin -1 (r l xin ) 2π )$$+ 1 2π sin -1 ( r l x in σ 2 x in σ 2 x in +1 ) + r l x in σ 2 x in ((2σ 2 x in +3)(σ 2 x in +1)-2(r l x in σ 2 x in ) 2 ) 2π(σ 2 x in +1)((σ 2 x in +1) 2 -(r l x in σ 2 x in ) 2 ) 3 2 r l gout σ 2 gout LayerNorm (d) Cov l gout σ 2 xin Dropout (p) Cov l gout SHA Block (without V) d in σ 2 gout L (1 + (L -1)r l gout ) Attn Block (Approx) d 2 in σ 2 gout σ 2 v σ 2 o L (1 + (L -1)r l gout ) FFN Block 4d 2 in σ 2 w1 σ 2 w2 Cov l gout ( 1 4 + sin -1 (r l xin ) 2π )$Table [22](#tab_1). Range of input variance/correlations used for theoretical formula verification reported in Table [21](#tab_1) for the theoretical formulas corresponding to forward and backward pass through components of the transformer model. The dropout probability range was [0, 1) for Dropout and Single-Head Attention, and σ 2 w for FFN was [10 -2 , 10 2 ]/din. We will use the approximations listed in Table [2](#tab_1) here.

## Component

## E.1.1 FORWARD PASS

For forward pass, a Transformer Pre-LN has LayerNorm followed by the Attention block, residual connection, LayerNorm, and then the FFN block. Let σ 2 layer be the output variance after 1 such layer, and σ 2 model be the output variance after the entire model of N layers.

$σ 2 xattn = d 2 σ 2 o σ 2 v * r l xin (1 -p) σ 2 xffn = 2d 2 σ 2 w1 σ 2 w2 (1 -p) σ 2 xlayer = σ 2 xin + σ 2 xattn + σ 2 xffn = σ 2 xin + d 2 σ 2 o σ 2 v * r l xin (1 -p) + 2d 2 σ 2 w1 σ 2 w2 (1 -p) Let, C 1 = d 2 σ 2 o σ 2 v (1 -p) , C 2 = 2d 2 σ 2 w1 σ 2 w2 (1 -p)bu , Then, σ 2 xlayer = σ 2 xin + C 1 * r l xin + C 2$As we discuss in Section 3.4, the correlation r l xin quickly reaches a stable constant maximum value r l xmax , which can be found using the calculations in Appendix F. Let r l xmin > 0 be the minimum value of this correlation, let

$C 3 = C 1 * r l xmax + C 2 , and C 4 = C 1 * r l xmin + C 2 . Then, σ 2 xin + C 4 ≤ σ 2 xlayer ≤ σ 2 xx in + C 3$Hence after N layers,

$σ 2 xin + N * C 4 ≤ σ 2 xmodel ≤ σ 2 xin + N * C 3 =⇒ σ 2 xmodel = Θ(N )(2)$This shows that output variance of Pre-LN will increase linearly with number of layers N .

In practice, because the correlation quickly reaches r l xmax , the variance of the entire model σ 2 xmodel ≈ σ 2 xin + N * C 3 .

Discussion: This has the effect that transformer blocks near the output can affect the model output much less, as the skip connection variance increases but block output variance is constant. We conjecture that parameters in these are hence not being utilized to their full potential. Specifically in case of Xavier initialization, C 1 = 2.2, C 2 = 0.4, r l xmax = 0.85. For large d, σ 2 xin will be negligibly small compared to σ 2 xlayer , so we have -

$σ 2 xmodel ≈ C 3 * N ≈ (2.2 * 0.85 + 0.4)N ≈ 2.2N E.1.2 BACKWARD PASS$For the backward pass, a Transformer Pre-LN gradient will first backpropagate through the FFN block, then gets rescaled by Layernorm, and added with the skip connection. It then backpropagates through the Attention block, gets rescaled by Layernorm, and finally added with the skip connection. Let σ 2 g,n be the gradient variance backpropagating from the n th layer, and σ 2 gmodel be the gradient variance after the entire model of N layers. For the Attention block, let σ 2 gattn,n-1 be the gradient backpropagating from the block. Then for long sequence length L we have -

$σ 2 gattn,n-1 = d 2 σ 2 o σ 2 v * σ 2 gout,n L(1 -p) (1 + (L -1)r l gout,n ) ≈ d 2 σ 2 o σ 2 v * r l gout,l * σ 2 gout,n$(1p) σ 2 gattn,n-1 is then rescaled by the Layernorm to give σ 2 gattn-layernorm,n-1 . As Layernorm scales gradient by the inverse of the input variance σ 2 xin,n-1 , which from the section above, we know is approximately σ 2 xin,n-1 = C 3 * (n -1). Then

$σ 2 gattn,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1 ≈ C 1 * r l gout,n * σ 2 gout,n C 3 * (n -1)$Therefore, the final gradient σ 2 gattn-layer,n-1 after addition with the skip connection is

$σ 2 gattn-layer,n-1 = (1 + C 1 * r l gout,n C 3 * (n -1) )σ 2 gout,n$Similarly, we can get σ 2 gffn-layer,n-1 for the ffn block. Then to get the gradient backpropagated through the entire layer σ 2 gout,n-1 , we have,

$σ 2 gffn-layer,n-1 = (1 + C 2 C 3 * (n -1) )σ 2 gout,n σ 2 gout,n-1 = (1 + C 1 * r l gout,n C 3 * (n -1) )(1 + C 2 C 3 * (n -1) )σ 2 gout,n σ 2 gout,n-1 ≈ (1 + C 1 * r l gout,n C 3 * (n -1) + C 2 C 3 * (n -1) )σ 2 gout,n = (1 + C 1 * r l gout,n + C 2 C 3 * (n -1) )σ 2 gout,n = (1 + C 1 * r l gout,n + C 2 (C 1 * r l xin,n + C 2 ) * (n -1) )σ 2 gout,n = (1 + C gpre,n n -1 )σ 2 gout,n$Where we ignore higher order terms for large n, and define C gpre,n = C1 * r l g out ,n +C2 C1 * r l x in ,n +C2 . Since C gpre,n > 0, we will witness an increase in gradient going backward, and this increase is inversely proportional to the current layer n, matching with empirically observed growth (Figure [2](#)). Let C gpre,min = C2 C1+C2 = 0.15 be the minimum value of C gpre,n , and C gpre,max = C1+C2 C2 = 6.5 be the maximum. Then the above equation is bounded by:

$(1 + C gpre,min n -1 )σ 2 gout,n ≤ σ 2 gout,n-1 ≤ (1 + C gpre,min n -1 )σ 2 gout,max$Applying the above equation repeatedly until the final layer N , this recurrence can be approximately solved by treating σ 2 gout,n as a continuous function of n, taking logarithm of both sides, and integrating. This gives the following solution for σ 2 gout,n :

$σ 2 gout,N * ( N n ) Cg pre,min ≤ σ 2 gout,n ≤ σ 2 gout,N * ( N n ) Cg pre,max$If the correlation r l gout,n quickly reaches a stable constant maximum value r l gmax (approximately equal to but slightly less than r l xmax (Appendix F)), C gpre ≈ 1, and we get exactly hyperbolic growth as shown below:

$σ 2 gout,n = σ 2 gout,N * ( N n )$The gradient variance will increase hyberbolically with number of layers N while going backwards.

Discussion: This has the effect that much lower learning rate is required for the entire model, because the gradients near the input layers are much higher, slowing down learning and making the model unstable.

## E.2 Vanilla Post-LN

## E.2.1 FORWARD PASS

The forward pass of Post-LN is trivially always 1 at initialization, because the skip connection does not cross the LayerNorm.

## E.2.2 BACKWARD PASS

Following an analysis similar to that for Pre-LN, we get

$σ 2 gffn-layer,n-1 = 1 + C 2 1 + C 1 * r l xout,n-1 σ 2 gout,n σ 2 gattn-layer,n-1 = 1 + C 1 * r l gout,n 1 + C 2 σ 2 gout,n σ 2 gout,n-1 = 1 + C 1 * r l gout,n 1 + C 2 * 1 + C 2 1 + C 1 * r l xout,n-1 * σ 2 gout,n = 1 + C 1 * r l gout,n 1 + C 1 * r l xout,n-1 σ 2 gout,n Let C 5,n = 1+C1 * r l g out,n$1+C1 * r l

x out ,n-1

. As we discuss in Appendix F, the correlations both quickly reach a maximum stable value. But the r l gout,n 's maximum value r l gmax is slightly different than

$r l xmax . Let C 5 = 1+C1 * r l gmax 1+C1 * r l xmax$, then C 5 can be either greater or smaller than 1. Hence, we get

$σ 2 gattn-layer,n-1 = C 5,n σ 2 gout,n = N i=n C 5,i σ 2 gout,N ≈ C (N -n) 5 σ 2 gout,N σ 2 gattn-layer,n-1 = C (N -n) 5 σ 2 gout,N(3)$This shows that gradient variance of Post-LN will decrease/increase exponentially with number of layers N while going backwards. Even very slightly different value of C 5 from 1, such as 0.96, will cause a 2000x fall in gradient after 200 layers.

Discussion: This shows why Post-LN transformer is much more difficult to train for deeper models than Pre-LN. While for Pre-LN the backwards gradient increases hyber-bolically to a maximum of N , in Post-LN the gradient can increase or decrease exponentially, stopping the model from converging.

## E.3 DeepScaleLM Pre-LN

## E.3.1 FORWARD PASS

In DeepScaleLM, the weight initialization are chosen specifically so that σ 2 xattn and σ 2 xffn are both equal to 1 for all layers, by iteratively calculating r l xin as detailed in Appendix M. Also, the embeddings are initialized so that σ 2 xin is also 1. Hence,

$σ 2 layer = λ 2 * σ 2 skip + β 2 * σ 2 block = λ 2 + β 2 = 1$Hence the forward pass variance remains 1 throughout the model.

## E.3.2 BACKWARD PASS

For the FFN-block, we have σ 2 xin,n-1 = σ 2 xout,n-1 = 1, as per equations in Table [2](#tab_1) of the main paper. Similar to Vanilla-PreLN, we arrive at

$σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1$Here, σ 2 xin,n-1 = 1 as shown above, and since weights are initialized so that C1 * r

$l xin = 1. Let C 6,n = r l g out ,n r l x out ,n-1 : σ 2 gattn-layernorm,n-1 = r l gout,n r l xin,n-1 * σ 2 gout,n = C 6,n * σ 2 gout,n$Therefore, assuming no covariance between block gradients and skip connection (which will be true at initialization), the final gradient σ 2 gattn-layer,n-1 after addition with the skip connection is

$σ 2 gattn-layer,n-1 = λ 2 σ 2 gout,n + β 2 σ 2 gattn-layernorm,n-1 = λ 2 σ 2 gout,n + β 2 C 6,n σ 2 gout,n = (λ 2 + β 2 C 6,n ) * σ 2 gout,n = (1 + C 6,n -1 N ) * σ 2 gout,n$Similarly for the FFN layer, σ 2 gffn-layer,n-1 = σ 2 gout,n , as

$σ 2 xin,n-1 = σ 2 xout,n-1 = 1. Hence, σ 2 gout,n-1 = (1 + C 6,n -1 N ) * σ 2 gout,n , σ 2 gout,1 = N i=1 (1 + C 6,n -1 N ) * σ 2 gout,N , ≈ N i=1 (1 + C 6 -1 N ) * σ 2 gout,N , ≈ (1 + C 6 -1 N ) N -1 * σ 2 gout,N , = e C6-1 * σ 2 gout,N ≈ σ 2 gout,N$, where we applied (1 -k N ) N ≈ e -k , and C 6 ≈ 1.

Discussion: Hence for DeepScaleLM, the backward variance of gradient remains constant (bounded by a constant) across all layers.

## E.4 DeepScaleLM Post-LN

## E.4.1 FORWARD PASS

Same as vanilla Post-LN, this will remain preserved at 1.

## E.4.2 BACKWARD PASS

Following an analysis similar to that for Vanilla Post-LN, we get

$σ 2 gffn-layer,n-1 = σ 2 gout,n σ 2 gattn-layer,n-1 = (λ 2 * 1 + β 2 * C 1 * r l gout,n )σ 2 gout,n = (λ 2 + β 2 * r l gout,n r l xin,n )σ 2 gout,n σ 2 gout,n-1 = (λ 2 + β 2 * r l gout,n r l xin,n )σ 2 gout,n$Similar to Pre-LN, we use the maximum value of these correlations, and assume C 6 = 1. We get

$σ 2 gout,n-1 = (λ 2 + β 2 * r l gmax r l xmax )σ 2 gout,n = (λ 2 + β 2 C 6 )σ 2 gout,n ≈ (λ 2 + β 2 )σ 2 gout,n = σ 2 gout,n$Hence for DeepScaleLM, the backward variance of gradient remains constant across all layers.

Discussion: Similar to DeepScale-LM Pre-LN, the assumption C 6 = 1 is not required, and yields the same constant bound if we do not assume it to be 1.

E.5 DeepScaleLM (Simplified) Pre-LN

## E.5.1 FORWARD PASS

For simplified DeepScaleLM, the initialization for the FFN block does not change, so its output remains 1 same as DeepScaleLM. For the Attention block, we changed its initialization to mimic that of the FFN block. We will show that initially, simplified DeepScaleLM's forward pass is bounded.

$σ 2 xffn = 1 as DeepScaleLM, σ 2 xattn = r l$x in 2 . Therefore, the output variance after layer n will be

$σ 2 xattn-skip,n = λ 2 * σ 2 xlayer,n-1 + β 2 * σ 2 xattn = (1 - 2 N ) * σ 2 xlayer,n-1 + 1 N * r l xin$Similarly after the FFN block, the output skip will be -

$σ 2 xlayer,n = λ 2 * σ 2 xattn-skip,n + β 2 * σ 2 xffn = (1 - 2 N ) * ((1 - 2 N ) * σ 2 xlayer,n-1 + 1 N * r l xin ) + 2 N * 1 = (1 - 2 N ) 2 * σ 2 xlayer,n-1 + (1 - 2 N ) * 1 N * r l xin + 2 N As correlation coefficient r l xin ≤ 1, we get, σ 2 xlayer,n ≤ (1 - 2 N ) 2 * σ 2 xlayer,n-1 + (1 - 2 N ) * 1 N * 1 + 2 N = (1 - 2 N ) 2 * σ 2 xlayer,n-1 + 3 N - 2 N 2 ≤ (1 - 2 N ) 2 * σ 2 xlayer,n-1 + 3 N$Applying the above recurrence equation N times, we get

$σ 2 xlayer,N ≤ (1 - 2 N ) 2N * σ 2 xlayer,0 + 3 N * N i=0 (1 - 2 N ) 2i = (1 - 2 N ) 2N * σ 2 xlayer,0 + 3 N * 1 -(1 -2 N ) 2N 1 -(1 -2 N ) 2$Since λ 2 + β 2 = 1 and β 2 is small for large N. We can rewrite the above equations completely in terms of β as follows

$σ 2 xlayer,N = (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 2 β 2 * 1 -(1 -β 2 ) 2N 1 -(1 -β 2 ) 2 (4) ≈ (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 4 (1 -(1 -β 2 ) 2N )(5)$For large N , we know (1 -k N ) N ≈ e -k . So the above becomes -

$σ 2 xlayer,N ≈ e -4 * σ 2 xlayer,0 + 3 N * 1 -e -4 4 N -4 N 2 ≤ e -4 * σ 2 xlayer,0 + 3 N * 1 -e -4 4 N = e -4 * 1 + 3 4 * (1 -e -4 ) = 3 4 + 1 4e4$This gives us an upper bound on the output variance after N layers. By setting r l xin = 0 instead of 1 in the equation above, and proceeding similarly, we can also arrive at a lower bound of 1 2 + 1 2e 4 .

$1 2 + 1 2e 4 ≤ σ 2 xlayer,N ≤ 3 4 + 1 4e 4(6)$Discussion Informally, this is because the attention block output variance will be between 0 and 0.5, and ffn block output always 1. Because of our λ, β scaling, the output will slowly converge to be in between the two outputs.

Note that the above derivation assumes no correlation between the block output and the skip connection. As we mentioned in our main paper, we do observe correlation between the input and the output. As such, theoretically, after every block, the variance σ 2 xlayer,n can increase by σ 2 xblock + σ 2 xlayer,n . This will cause the final output variance to increase by factors of 2 * √ N . In practice however, we observe the output variances to not grow too large.

## E.5.2 BACKWARD PASS

Similar to DeepScaleLM Pre-LN, we arrive at

$σ 2 gattn-layernorm,n-1 = C 1 * r l gout,n * σ 2 gout,n σ 2 xin,n-1 ≈ 0.5 * C 6 σ 2 xin,n-1 * σ 2 gout,n σ 2 gattn-layer,n-1 = λ 2 σ 2 gout,n + β 2 σ 2 gattn-layernorm,n-1 = (λ 2 + β 2 * 0.5 * C 6 σ 2 xin,n-1 ) * σ 2 gout,n = (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 -1)) * σ 2 gout,n$Similarly, for the FFN layer, we get

$σ 2 gffn-layer,n-1 = (1 + 2 N * ( 1 σ 2 xin,n-1 -1)) * σ 2 gout,n$Multiplying these, we get

$σ 2 gout,n-1 = (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 -1)) * (1 + 2 N * ( 1 σ 2 xin,n-1 -1)) * σ 2 gout,n ≈ (1 + 2 N * ( 0.5 * C 6 σ 2 xin,n-1 + 1 σ 2 xin,n-1 -2)) * σ 2 gout,n As 0.5 ≤ σ 2 xin,n-1 , we get -4 ≤ ( C6 σ 2 x in ,n-1 + 2 σ 2$x in ,n-1 -4) ≤ 2C 6 + 2. Hence, on applying the above recurrence N times, we get

$e -4 * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ e 2C6+2 * σ 2 gout,N$Hence, we show that even for simplified DeepScaleLM Pre-LN, the maximum relative increase/fall in gradient variance is bounded across layers.

## Discussion:

The above derivations will also be valid if there is correlation in the input. Correlation will cause σ 2 xin,n-1 to increase, effectively decreasing the backpropagated gradient through the block to decrease (as Layernorm will scale by inverse of σ 2 xin,n-1 ). However, even in that case, our gradient will still be bounded by the above lower-bound. Intuitively, as the gradient can flow freely through the skip connection, hence, σ 2 gout,n-1 ≥ λ 4 * σ 2 gout,n , which when applied N times, yields σ 2 gout,1 ≥ e -4 * σ 2 

As -2 ≤ (r l gout,nr l xin,n ) ≤ 2, applying the above recurrence N times we get e -2 * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ e 2 * σ 2 gout,N Discussion: The above derivations assume no correlation in the input, and hence is only correct at initialization. However, if there is correlation between the block output and skip connection (r x ), the layernorm will cause σ 2 gout,n-1 to be down-scaled by a factor of 1 + 2 * rx √ N , where c is some constant, as opposed to 1 + 2 N above. However, if there is also correlation in the gradients of the block and skip connection (r g ), the numerator in the equations above for σ 2 gout,n-1 will also be increased, by a factor of 1 + 2 * rg √ N . Hence if the correlations among the gradients and among the output are similar, the above bounds will remain. If β 2 is set as 1 N 2 , then even if input correlations exist, the backward gradient will be bounded, following a similar derivation as above. However, we conjecture that this decreases the ability of the transformer layers to modify the skip connection too strongly, decreasing the "expressivity" of the model. This is similar to the approach of DSInit, which we show in our main paper does indeed decrease model performance.

## F Rank Collapse and Correlation Analysis

In the previous sections, we derived the formulas that determine how the correlation will change through the Attention and FFN blocks both for forward and backward pass. Both attention and FFN blocks modify the correlation as shown in the Table [2](#tab_1).

Simplifying the formulae in the table above, we rewrite the output variance for the attention block as σ 2 xattn = C 1 * r l xin * σ 2 xin , and the output of the FFN block is σ 2 xffn = C 2 * σ 2 xin , where C 1 and C 2 are defined as follows.

$C 1 = d 2 σ 2 o σ 2 v (1 -p) , C 2 = 2d 2 σ 2 w1 σ 2 w2 (1 -p) ,$This also helps us to rewrite the backward pass as the σ 2 gattn = C 1 * r l gout * σ 2 gout and σ 2 gffn = C 2 * σ 2 gout . Specifically in case of Xavier initialization with 0.1 dropout, C 1 = 2.2, C 2 = 0.4.

Assuming a dropout of 0.1, the FFN block (with the ReLU) will reduce the correlation if it rises above 0.64 (where r l xout < r l xin for FFN block). And the attention block will never output a correlation higher than 0.9. Hence correlation will never reach 1, but rather a steady, stable value between ReLU's maximum correlation and that of the attention block. Dropout's effect in preventing rank collapse was also observed in [(Rong et al., 2020)](#b91).

We can approximate the stable value of correlation after many layers based on the weightage average of the correlation in the Attention output and FFN output. When the attention output is added to the skip connection, the new correlation will be a weighted (by variance) average of the correlation among the tokens of attention output and among the tokens in the skip connection. And the same will happen after the FFN block.

A weighted average of the correlations of FFN and attention blocks gives the stable asymptotic correlation r l xmax

$r l xmax = C 1 * (1 -p) + C 2 * (1 -p)( 1 π + r l xmax 2 + ( 1 2 - 1 π )r l xmax 2 ) C 1 + C 2$Specifically for the case of xavier initialization, solving the above equation with C 1 = 2.2, C 2 = 0.4, gives r l xmax ≈ 0.88, as empirically verified in Figure [8](#fig_3).

Similarly, the correlation for backward gradient will also converge at a stable value r l gmax , obtained by solving the below equation -

$r l gmax = C 1 * (1 -p) + C 2 * (1 -p)( 1 2 + sin -1 (r l xmax ) π )r l gmax C 1 + C 2$Specifically for the case of xavier initialization, this gives r l gmax = 0.87. Note how r l gmax ≈ r l xmax .

Discussion on rank collapse observed in [Noci et al. (2022](#b81)[) Noci et al. (2022)](#b81) focuses primarily on linear activation, we theoretically analyze the change in output correlation caused by ReLU. We find that ReLU (or any asymmetric non-linearity in general) critically affects correlation. As our closed form expressions suggest, both FFN block (because of ReLU) and dropout reduce the correlation. While [Noci et al. (2022)](#b81) mentions the use of dropout, as we show above and observe empirically in Figure [8](#fig_3), rank will not collapse with dropout, and perhaps [Noci et al. (2022)](#b81) did not use dropout.

We replicated the experimental settings of [Noci et al. (2022)](#b81) without dropout, and observed that the rank collapse occurs due to incorrect initialization. They use a rather non-standard version of xavier initialization -instead of 2 f anin+f anout , they use 1 f anout . Hence, they initialize a much higher value for V as f an in is much greater than f an out ("Number of heads" times greater), and this results in variance of the output of the attention block C1 being much higher than FFN C2. As attention block outputs a much higher correlation than the FFN block, increasing its output variance without using dropout will result in rank collapse. This highlights the criticality of correct initialization, as well as the explainability power of our theoretical framework proposed in the paper.

## G Discussion of Relative Strength

In Equation [4](#formula_159), we discussed that the backward recurrence equation for PreLN can be written as

$σ 2 xlayer,N ≈ (1 -β 2 ) 2N * σ 2 xlayer,0 + 3 4 (1 -(1 -β 2 ) 2N )$Replacing β 2 = k N α and using (1 + k N α ) N = e kN 1-α , we get Hence, for N layers, the gradient fall/growth is again O(e ±kN 1-α ).

## H Applying DeepscaleLM to Vision Transformers

Applying our method to vision transformers (for eg. ViT [(Dosovitskiy et al., 2021)](#b29) or DeiT [(Touvron et al., 2021a)](#)) will only require handling the input embeddings section Appendix A.1 -For ViT, this is a simple linear projection. Given normalized image inputs, our Linear section Appendix A.2 provides formulae to calculate the variance and correlation of the embeddings which are input to the model.

We empirically verified that for images from ImageNet, the embeddings after the linear projection do indeed follow the normal distribution, with an R 2 of 0.95. Furthermore, normalizing images to have approximately unit variance, given linear weights initialized by 1 d , the output variance was observed as 1.02 (within 2% error). While we used Zipf's law to estimate input embedding correlation for text, this could simply be empirically measured for vision after the embedding layer -we measured this to be 0.46 using the code provided by [Beyer et al. (2022)](#b7).

Using this measured value of input correlation, we can apply our DSLM method to ViT. As we show in Figure [11](#), our method successfully controls both the forward and backward moments for the ViT model with 100s of layers. but our work will also prevent rank collapse at initialization caused by the very structure of the transformer model, in particular increase in correlation caused by both attention and ReLU/GeLU. The methods in these works are orthogonal to our approach, and our equations can be easily extended to cover the architectural modifications suggested in these.

## L Discussion of Approximations and Assumptions

L.1 Illustrative Approximations of Full Formulae in Main Paper Some values listed in Table [1](#tab_0) are approximations/illustrative simplifications of their full closed forms in Appendix C and Appendix A. We discuss all of these below.

• For ReLU forward correlation, we used a simple polynomial regression of the closed form formula. This simple regression is a remarkably good fit, as shown in figure Figure [12](#), and can be reproduced in using our released code.

• For layernorm, we ignored the factor of 1 compared to d, or 1/d compared to 1, assuming large enough hidden dimension d.

• For SHA without V, we used the final simplified formulae for σ 2 xout and output correlation from Appendix A.8. For the gradient, we further simplified the formulae in Appendix A.8, assuming L ≈ L -1. Furthermore, the formulae provided in Table [2](#tab_1) are approximate versions of the full formulae provided in Appendix C. In Table [2](#tab_1), we applied a similar approximation as done in Table [1](#tab_0) for ReLU, from the full formula in Appendix C for output correlation. This polynomial approximation is also a very good fit, as shown in Figure [13](#fig_0), and can be reproduced using our released code.

Our exact formulae for blocks and components also account for IID cases -as can be verified by our simulations, in which we do cover cases IID inputs with exactly 0 correlation, as noted in Corr l xin column in Table [22](#tab_1). In the simplified formulae, and in DeepScaleLM initialization and model, we simplified our formulae so that they only remain accurate for non-IID inputs. This was because of three considerations:

1. In NLP domain, most text will inevitably be non-IID due to repeated common words. This was encountered in all our experiments.

2. In Vision domain, for ViT in particular, there will be correlation among pixel intensities across patch embeddings, as discussed in common response section.

3. In Speech domain, similar to text, most speech will inevitably be non-IID due to repeated common sounds. N Hyper-parameters BERT Pretraining We used Megatron-LM's default BertWordPieceLowerCase tokenizer, with the original BERT lowercased vocab, and with trainable position embeddings. The same hyper-parameters (including LR schedule, warmup) were used for all models, and LR search over the range below was performed for all models. The final best models always had optimal LR within the range and not at the boundary of the LR range for all of our experiments. Detailed hyper-params are provided in Table [25](#tab_5).

![Figure 1. Pre-LN: Variance of forward signal increases linearly across layers N .]()

![Figure 4. DeepScaleLM: The variances remain conserved for both forward and backward pass.]()

![Figure 5. Backward gradient variance increases hyperbolically after 150k train steps.]()

![Figure 7. Forward r l x out for FFN and Attention blocks with p = 0.1. FFN reduces r l x out for r lx in > 0.65, and attention always has r lx out < 1.]()

![Figure9. Gradient vanishes using λ 2 = 0.9 and β 2 = 0.1, after 50k training steps.]()

![variance, and integral of odd function)]()

![10 2 , 10 3 ] [32, 64, 128, 256] [300, 10 4 ] E Moment Propagation through the Entire Transformer Model E.1 Vanilla Pre-LN]()

![variance for Post-LN is trivially bounded.E.6.2 BACKWARD PASSFollowing an analysis similar to that for DeepScaleLM Post-LN, we getσ 2 gout,n-1 = λ 2 + 0.5 * β 2 * r l gout,n λ 2 + 0.5 * β 2 * r lThe above equation can be rewritten in terms of β as followsσ 2 gout,n-1 = (1 + β 2 2 (r l gout,nr l xin,n ))σ 2 gout,n]()

![fall in gradient for β 2 = k N α is O(e kN 1-α ). Similarly for PostLN, we can use Equation7β2 ) * σ 2 gout,N ≤ σ 2 gout,n-1 ≤ (1 + β 2 ) * σ 2 gout,N]()

![Figure 12. Approximation of the Relu forward correlation formula]()

![Figure15. Pseudo-code for our proposed method DeepScaleLM: We scale the block output and the skip connection before adding, and keep track of correlation across layers. We appropriately initialize the weights. (N : num of layers, d: model hidden dimension, p: dropout probability, r lx in is calculated based on expressions provided in subsection A.1.)]()

![Signal propagation for forward and backward passes through components of a transformer (GeLU in Appendix A.5). The expressions here are illustrative simplification of full closed form formulae in Appendices A and C.]()

![Moment Propagation through the blocks of a transformer layer. Exact closed forms / proofs are provided in Appendices B and C.]()

![Performance (perplexity) of BERT models with different shapes. Deep-Thin models provide large improvements with fewer parameters.model outperforms the vanilla 12-layer, and our 96 layer outperforms the vanilla 24-layer model. The 160M 192-layer model outperforms the vanilla 24-layer 336M model with more than 2× the params.Pre-training Improvements for Pre-LN We also applied DSLM to the deep Pre-LN models, trained for 3.3B tokens. Table5show that DSLM significantly improves the performance of the Pre-LN model across a range of model depths.]()

![DSLM with Pre-LN Models.]()

![BERT-base (trained for 64B tokens) pre-training and finetuning results (mean accuracy across 5 runs with stderr).]()

![Accuracy on MNLI after low rank finetuning using LoRA]()

![Application of DSLM to Decoder-only model (GPT), while increasing model depth to 4x (token-level PPL).]()

![Application of DSLM to Speech-to-Text translation. Nenc and Ndec refer to number of layers in the encoder and the decoder respectively. For models marked with *, maximum source sequence length was limited to 1024 due to compute limitations, and longer examples were discarded for both train and test.]()

![Applying DSLM to Image classification using ViT.]()

![Comparison with prior methods for deep Transformers.]()

![Model performance on direct casting to FP8]()

![Ablation of various DeepScaleLM components.]()

![Ablation of the initializations.]()

![Moment Propagation (mean) during forward pass through components of transformer model.]()

![Moment Propagation (variance) during forward pass through components of transformer model.]()

![Moment Propagation (variance) during backwards pass through components of transformer model.]()

![Covariance (hidden dimension) propagation through the components of transformer model.]()

![Gradient covariance (along sequence length) propagation through the components of transformer model.]()

Code: https://github.com/akhilkedia/TranformersGetStable

Quora Question Pairs dataset

