# Generative AI Research LIMO: Less is More for Reasoning

## Abstract

## 

We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (often > 100, 000 examples), we demonstrate a striking phenomenon: complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. This finding challenges not only the assumption of massive data requirements but also the common belief that supervised fine-tuning primarily leads to memorization rather than generalization. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance and efficiency in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on the highly challenging AIME benchmark and 94.8% on MATH, improving the performance of previous strong SFT-based models from 6.5% to 57.1% on AIME and from 59.2% to 94.8% on MATH, while only using 1% of the training data required by previous approaches. Most remarkably, LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, directly challenging the prevailing notion that SFT inherently leads to memorization rather than generalization. Synthesizing these pioneering results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is not inherently bounded by the complexity of the target reasoning task, but fundamentally determined by two key factors: ( [1](#)) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples, which serve as "cognitive templates" that show the model how to effectively utilize its existing knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at [https://github.com/GAIR-NLP/LIMO](https://github.com/GAIR-NLP/LIMO).

## 

1 Introduction

Complex reasoning has long been considered one of the most challenging capabilities to instill in large language models (LLMs). While recent work has shown that LLMs can be effectively aligned with user preferences through relatively small amounts of instruction data [(Zhou et al., 2024a)](#), teaching models to reason-particularly in mathematics and programming-is widely believed to require vastly more training examples [(Paster et al., 2023;](#b23)[Yue et al., 2024)](#b41). This conventional wisdom stems from the inherent complexity of reasoning tasks, which demand multi-step logical deduction, domain knowledge application, and structured solution paths. The resulting paradigm typically involves training on tens or hundreds of thousands of examples [(Yu et al., 2024;](#b39)[Li et al., 2024b)](#), based on two fundamental assumptions: first, that mastering such complex cognitive processes requires extensive supervised demonstrations, and second, that supervised fine-tuning leads primarily to memorization rather than true generalization [(Zhang et al., 2024;](#b42)[Xu et al., 2024;](#b36)[Chu et al., 2025)](#b4).

While this approach has shown success, it imposes substantial computational costs and data collection burdens. More importantly, we argue this data-intensive paradigm may no longer be necessary. Recent advances have fundamentally transformed how LLMs acquire, organize, and utilize reasoning knowledge, suggesting the possibility of a more efficient approach. Two key developments in particular have created the conditions for a fundamental reimagining of how we approach reasoning in LLMs:

1. Knowledge Foundation Revolution: Modern foundation models now incorporate unprecedented amounts of mathematical content during pre-training [(Qwen et al., 2025;](#)[Yang et al., 2024;](#b37)[Wang et al., 2024)](#b33). For example: Llama 2's total training data across all domains was 1.8T tokens [(Touvron et al., 2023)](#), while Llama 3 used 3.7T tokens just for mathematical reasoning [(Grattafiori et al., 2024)](#). This suggests that contemporary LLMs may already possess rich mathematical knowledge in their parameter space, transforming the challenge from knowledge acquisition to knowledge elicitation. 2. Inference-time Computation Scaling Revolution: The emergence of techniques scaling longer reasoning chains has revealed that effective reasoning requires substantial computational space during inference. Recent works (OpenAI et al., 2024; [Qin et al., 2024;](#b24)[Huang et al., 2024)](#b11) have shown that allowing models to generate extended reasoning chains significantly improves their reasoning ability. In essence, inference-time computation provides the crucial cognitive workspace where models can systematically unpack and apply their pre-trained knowledge.

We hypothesize that successful reasoning emerges from the synergy of these two factors: rich pre-trained knowledge and sufficient computational resources at inference time. These developments collectively suggest a striking possibility: if models possess rich reasoning knowledge and are given adequate computational space, then activating their reasoning capabilities may require only a small number of high-quality training samples that encourage extended deliberation, rather than massive fine-tuning datasets. Building on this insight, we propose the Less-Is-More Reasoning (LIMO) Hypothesis. This hypothesis identifies two critical factors that determine the elicitation threshold for complex reasoning: [(1)](#b0) the latent presence of prerequisite knowledge within the model's parameter space, and (2) the effectiveness of minimal exemplars in demonstrating systematic problem-solving processes that encourage extended deliberation. Critically, this suggests that the sample efficiency of eliciting advanced reasoning is not inherently bounded by the complexity of the target reasoning task, but rather by the completeness of the model's encoded knowledge foundation and its exposure to training samples that effectively utilize the inference-time computation space.

Through comprehensive experiments, we demonstrate that LIMO achieves 57.1% accuracy on the highly challenging AIME benchmark and 94.8% on MATH with merely 817 training samples, demolishing previous strong SFT-based models while using just 1% of their training data. Most remarkably, these benefits generalize across a diverse spectrum of previously unseen scenarios, with LIMO consistently outperforming models trained on 100x more data by 40.5% absolute improvement. This discovery has profound implications for artificial intelligence research: it suggests that even competition-level complex reasoning abilities can be effectively elicited through minimal but curated training samples. More fundamentally, it points to a promising technical pathway toward AGIany sophisticated reasoning capability, no matter how complex, could potentially be activated with minimal samples given two key conditions: (1) sufficient domain knowledge embedded during pre-training, and (2) optimal cognitive reasoning chains for activation. This represents not merely an argument for data efficiency, but a fundamental insight into how complex reasoning capabilities emerge in large language models.

The main contributions of this work are: [(1)](#b0) We establish the LIMO hypothesis, demonstrating that complex reasoning capabilities can be elicited through surprisingly small datasets (hundreds of examples) by leveraging rich mathematical knowledge in pre-trained models and detailed reasoning chains. [(2)](#b1) We provide systematic empirical evidence challenging current assumptions about scaling laws in reasoning tasks, showing that benefits generalize robustly to out-of-distribution problems and suggesting the acquisition of genuine reasoning capabilities rather -Optimal structure with adaptive step granularity -Strategic cognitive scaffolding for reasoning -Rigorous verification throughout solution than superficial pattern matching. (3) We identify critical factors for effective reasoning elicitation, particularly the synergy between pre-trained knowledge foundations and test-time computation scaling, providing insights into how these advances can be combined to achieve superior reasoning performance with minimal fine-tuning data. (4) We release a comprehensive open-source suite including our fine-tuned models, evaluation pipelines, training code, and carefully curated datasets with varying quality levels. This release enables systematic investigation of data efficiency in complex reasoning and facilitates reproducibility of our findings, while providing valuable resources for future research in this direction.

2 Phenomena Rethinking: Less-is-More and RL Scaling

The emergence of LIMO represents a paradigm shift in how we conceptualize and activate complex reasoning capabilities in large language models. This section examines two key comparisons that illuminate the fundamental nature of this advance: first, contrasting LIMO with LIMA to understand how Less-is-More principles extend from general alignment to complex reasoning; and second, comparing LIMO with reinforcement learning (RL) scaling approaches to highlight distinct philosophical perspectives on developing reasoning capabilities. Through these analyses, we aim to establish a deeper understanding of how complex cognitive abilities emerge in language models and the conditions that enable their efficient activation.

## LIMO vs LIMA

The emergence of Less-is-More phenomena in LLMs represents a fundamental shift in our understanding of how complex capabilities can be elicited with minimal data. While LIMA [(Zhou et al., 2024a)](#) first demonstrated this phenomenon in the context of general alignment, extending this principle to complex mathematical reasoning presents unique challenges and requirements. This section examines the critical developments that enable Less-is-More for reasoning, analyzing the essential differences between alignment and reasoning scenarios, and providing insights into the conditions necessary for efficient capability activation in large language models.

## Knowledge Foundation Revolution

The past two years have witnessed a transformation in how language models acquire and organize mathematical knowledge. While LIMA could rely on general text corpora for alignment, 

## Synergistic Convergence

The timing of LIMO's discovery reflects the necessary convergence of these two revolutions. The two-year gap between LIMA and LIMO represents not just the time needed for better pretrained models, but the essential wait for inference-time computation breakthroughs. This convergence enables a phenomenon we call the Reasoning Elicitation Threshold: when models possess both rich domain knowledge and sufficient computation space, complex reasoning capabilities can be activated with minimal but precise demonstrations.

Implications for Future Research This comparative analysis reveals Less-is-More not merely as an advocacy for using fewer data, but as a fundamental principle governing the efficient elicitation of model capabilities. The success of LIMO demonstrates that when essential prerequisites (knowledge foundation and computation framework) are met, complex capabilities can be elicited with remarkable data efficiency. This insight suggests a new research direction: systematically identifying the prerequisites and optimal activation conditions for different capabilities.

Future work should explore whether other advanced capabilities (e.g., planning, creative problem-solving) can achieve similar efficiency once their corresponding knowledge and computation foundations are established. The Less-is-More principle thus serves as both a theoretical framework for understanding capability emergence and a practical guide for pursuing data-efficient capability development across various domains.

## LIMO vs RL Scaling

The emergence of two distinct approaches to developing reasoning capabilities in large language models -RL Scaling and LIMO -represents a fundamental divergence in how we understand and enhance model intelligence. RL Scaling, exemplified by works like o1 (OpenAI, 2024), DeepSeek-R1 (Guo et al., 2025), approaches the challenge from an engineering optimization perspective. It assumes that reasoning capabilities need to be extensively trained into models through large-scale reinforcement learning. While effective, this approach essentially treats RL as a broad search mechanism to discover effective reasoning patterns through massive computational resources. In contrast, LIMO introduces a more foundational perspective: reasoning capabilities are already latent within pre-trained models, embedded during the pre-training phase. The key challenge shifts from "training" to "elicitation" -finding precise cognitive templates that can elicit these innate abilities. From this perspective, RL Scaling approaches like DeepSeek-R1 can be viewed as specific implementations of this principle, using reinforcement learning as a mechanism to search for such trajectories. While both approaches ultimately seek high-quality reasoning solutions, LIMO offers a more principled and direct path through explicit trajectory design, while RL Scaling discovers these trajectories through extensive computational exploration. This reframing suggests that various methods, including RL, expert design, or hybrid approaches, could all be understood and evaluated within LIMO's framework as different strategies for discovering optimal reasoning trajectories.

## LIMO Dataset

## The LIMO Hypothesis

We formalize the Less-Is-More Reasoning (LIMO) Hypothesis as follows: In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis rests on two fundamental premises: (I) The latent presence of prerequisite knowledge within the model's parameter space (II) The quality of reasoning chains that precisely decompose complex problems into detailed, logical steps, making the cognitive process explicit and traceable. To validate this hypothesis, we propose a systematic approach to construct a high-quality, minimal dataset that can effectively elicit the model's inherent reasoning capabilities.

## Problem Definition

In this paper, we focus on the reasoning tasks with verifiable answer. Given a question q ∈ Q, where Q represents the space of reasoning problems, the goal is to generate an answer a ∈ A and a reasoning chain r ∈ R. We define a reasoning chain r as a sequence of intermediate steps {s 1 , s 2 , ..., s n }, where each step s i represents a logical deduction that bridges the gap between the question and the final answer.

Formally, we can represent the reasoning process as a function f :

$f : Q → R × A(1)$Therefore, the quality of the resulting dataset D is determined by two fundamental yet multifaceted components: (1) the quality of questions q ∈ Q, which encompasses factors such as diversity in problem-solving approaches, appropriate difficulty levels to challenge model capabilities, and the breadth of knowledge domains covered, and (2) the quality of solutions (r, a) ∈ R × A, which encompasses aspects including pedagogical value, logical coherence, and methodological rigor. Questions should be designed to encourage sophisticated reasoning patterns and knowledge integration, while solutions should demonstrate clear logical progression and serve as effective learning examples. These interrelated quality dimensions, among others, guide our systematic data curation process detailed in the following sections.

## High-Quality Data Curation

Our data curation process focuses on constructing a high-quality dataset D = {(q i , r i , a i )} N i=1 where N is intentionally kept small to validate our LIMO hypothesis.

## Question Selection

We hypothesize that high-quality questions q ∈ Q should naturally elicit extended reasoning processes. Our selection criteria include the following:

• Level of difficulty We prioritize challenging problems that foster complex reasoning chains, diverse thought processes, and knowledge integration, enabling LLMs to effectively leverage pre-trained knowledge for highquality inference. • Generality Problems that deviate more from the model's training distribution can better challenge its fixed thinking patterns, encourage exploration of new reasoning approaches, thus expanding its inference search space. • Knowledge Diversity The selected problems should cover various mathematical domains and concepts, requiring the model to integrate and connect distant knowledge during problem-solving.

To implement these criteria effectively, we first assembled a comprehensive pool of candidate problems from various established datasets: NuminaMath-CoT, featuring meticulously annotated problems from high school to advanced competition levels; AIME historical examination problems, known for its extremely challenging and integrative problems spanning multiple mathematical domains; MATH [(Hendrycks et al., 2021)](#b10), encompassing various competitive mathematics problems from prestigious contests; and several other sources of mathematical problems.

From this rich initial collection, we employed a systematic multi-stage filtration process. Beginning with an initial pool of tens of millions of problems, we first applied a baseline difficulty filter using Qwen2.5-Math-7B-Instruct [(Yang et al., 2024)](#b37), eliminating problems that this model could solve correctly in a few attempts. This process helped establish a preliminary difficulty threshold. Subsequently, we subjected the remaining problems to a more rigorous evaluation using state-of-the-art reasoning models including R1, DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025), and models from [Huang et al. (2024)](#b11), retaining only problems where even these most capable models achieved success rates below certain threshold through multiple sampling iterations. Finally, to maintain corpus diversity, we employed strategic sampling techniques that balanced representation across mathematical domains and complexity levels while avoiding conceptual redundancy. This meticulous selection process ultimately yielded 817 carefully curated problems from an initial pool of tens of millions of candidates, with the selected problems collectively satisfying our stringent quality criteria while spanning a rich spectrum of mathematical reasoning challenges.

## Reasoning Chain Construction

Beyond high-quality questions, the quality of solutions plays a pivotal role in the training phase of large language models. To curate high-quality solutions, we adopted a comprehensive selection strategy. We began by gathering official solutions for problems where available, complemented by solutions authored by both human experts and AI specialists. Additionally, we leveraged state-of-the-art reasoning models, including DeepSeek R1, DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025), and Qwen2.5-32b-Instruct, to generate diverse solution approaches. Furthermore, following the methodology proposed in O1-Journey-Part2 (Huang et al., 2024), we utilized selfdistillation techniques based on Qwen2.5-32b-Instruct to create additional model variants, which were then used to generate supplementary problem responses. These responses were then filtered according to the correctness of the answers to establish a baseline collection of valid solutions. Subsequently, all the authors conducted a comprehensive analysis of these filtered solutions through collaborative examination. Through careful observation and systematic review, we identified several key characteristics that distinguish high-quality reasoning chains:

• Optimal Structural Organization: The solution exhibits clear and well-organized structural formatting, with adaptive granularity in step decomposition. Particularly, it allocates more tokens and detailed elaboration at crucial reasoning junctures while maintaining concise expressions for straightforward steps. This self-adaptive approach to step granularity ensures that complex transitions receive appropriate attention while avoiding unnecessary verbosity in simpler deductions. • Effective Cognitive Scaffolding: High-quality solutions provide strategic educational support by gradually building understanding through carefully structured explanations. This includes progressive concept introduction, clear articulation of key insights at critical points, and thoughtful bridging of conceptual gaps, making complex reasoning processes more accessible and learnable. • Rigorous Verification: High-quality solutions incorporate extremely frequent verification steps throughout the reasoning process. This includes validating intermediate results, cross-checking assumptions, and confirming the logical consistency of each deduction, thereby ensuring the reliability of the final answer.

Based on these identified characteristics, we developed a hybrid approach combining rule-based filtering and LLM-assisted curation to select high-quality solutions for each question identified in the previous section. This systematic process ensures that each selected solution adheres to our established quality criteria while maintaining consistency across the dataset. By focusing on a minimal yet meticulously curated set of reasoning chains, we embody the core principle of Less-Is-More: high-quality demonstrations, rather than sheer data volume, are key to unlocking complex reasoning capabilities. The resulting dataset D consists of carefully curated triples (q, r, a), where each reasoning chain r satisfies our quality criteria. By maintaining these stringent standards while limiting the dataset size |D|, we aim to demonstrate that high-quality demonstrations, rather than large quantities of training data, are crucial for unlocking complex reasoning capabilities.

## Methodology

Based on the Less-Is-More principle, a model with substantial reasoning knowledge from pre-training and the ability to perform long-chain reasoning at test time can develop robust reasoning abilities. After training on only a few hundred instances of SFT data, the model learns to integrate meta-reasoning tasks into a cohesive reasoning chain.

## Training Protocol

We fine-tune Qwen2.5-32B-Instruct using supervised fine-tuning on our LIMO dataset. The training process employs full-parameter fine-tuning with DeepSpeed ZeRO-3 optimization (Rajbhandari et al., 2020) and FlashAttention-2 (Dao, 2023), with a sequence length limit of 16,384 tokens.

## Evaluation Framework

In-domain Evaluation To comprehensively assess the models' performance across various reasoning capabilities, we have established a diverse evaluation framework encompassing both traditional and novel benchmarks. Our primary evaluation suite includes several well-established mathematical competitions and benchmarks: the American Invitational Mathematics Examination (AIME24), MATH500 [(Hendrycks et al., 2021)](#b10), and the American Mathematics Competitions (AMC23).

## Out-of-distribution Evaluation

To rigorously evaluate the models' performance on out-of-distribution (OOD) tasks, we carefully selected benchmarks that differ from our training data in various aspects. These benchmarks can be categorized into three distinct groups:

• Diverse Mathematical Competitions: We furthur selected OlympiadBench (He et al., 2024), which represents a distinct distribution of mathematical challenges to test models' OOD performance. • Novel Multilingual Benchmarks: To minimize data contamination, we constructed several benchmarks using the most recent examination problems: CHMath from the 2024 Chinese High School Mathematics League Competition, Gaokao from China's 2024 National College Entrance Examination, Kaoyan from Chinese Graduate School Entrance Examinations, and GradeSchool, our newly developed benchmark for elementary mathematical reasoning. Notably, all problems in these benchmarks are written in Chinese, while our training data contains no Chinese problems. This introduces an additional OOD dimension, assessing not only the model's ability to generalize across problem distributions but also its cross-lingual reasoning capabilities when confronted with unseen languages. • Multi-disciplinary Benchmarks: To assess broader generalization capabilities beyond mathematics (our training domain), we incorporated Miverva (Lewkowycz et al., 2022) (which includes undergraduate-level STEM problems) and GPQA (Rein et al., 2023). These benchmarks evaluate reasoning abilities across multiple disciplines and cognitive levels, providing insights into the model's capacity to transfer mathematical reasoning skills to broader contexts.

Performance metrics We evaluate performance using the pass@1 metric across our suite of benchmarks. All evaluations are conducted in a Zero-shot Chain-of-Thought (CoT) setting to better assess the model's reasoning capabilities. For benchmarks including MATH500, OlympiadBench, Gaokao, Kaoyan, GradeSchool, MinervaMath, and GPQA, we employ a straightforward approach using greedy decoding with a single sample to assess correctness. However, for the smaller benchmarks containing fewer than 50 problems each (specifically AIME24, AMC23, and CHMATH), we implement a more thorough evaluation protocol, generating 16 samples with a temperature setting of 0.7 and calculating the unbiased pass@1 metric as introduced in Chen et al. (2021). For problems where answers are well-structured numerical values, we directly apply rule-based evaluations to check for mathematical equivalence.

For more complex answer formats-such as expressions, equations, or structured solutions-we leverage an LLM-based evaluator, which we have validated for high reliability. Throughout all evaluations, we maintain a maximum output length of 32,768 tokens to minimize the potential for output truncation, ensuring our assessment captures complete problem-solving attempts. Additionally, when evaluating LIMO, we observed that inference-time scaling occasionally results in repetitive patterns at the end of lengthy outputs. In such cases, we extract the most likely final answer from the model's response for evaluation to ensure accurate assessment of its problem-solving capabilities.

## Experiment

## Baselines

We compare LIMO against a comprehensive set of baselines with the following prominent models:

• OpenAI-o1-preview (OpenAI, 2024), a large language model that has demonstrated advanced mathematical reasoning abilities across various complex tasks. • QwQ-32B-Preview (Team, 2024b), a model specifically designed for mathematical problem-solving with strong reasoning capabilities. • Qwen2.5-32B-Instruct, which serves as our base model for comparative analysis.

For evaluation, we use the OpenAI API to access OpenAI-o1-preview, while using VLLM [(Kwon et al., 2023)](#b14) to deploy other open-weight models (e.g. QwQ-32B-Preview). To ensure fair comparison, all models follow the same evaluation protocol with identical inference hyper-parameters.

To investigate the impact of training data efficiency, we conduct comparative experiments using mainstream open-source reasoning datasets for supervised fine-tuning on our base model. For a fair comparison, all experiments use the same LLM backbone as LIMO, ensuring that performance differences are solely attributable to the training data characteristics.

## Main Results

• OpenThoughts-114k:[foot_0](#foot_0) A synthetic reasoning dataset containing 114k examples covering mathematics, science, coding, and puzzles. The solutions follow a structured reasoning format generated by DeepSeek-R1. • NuminaMath-100k: A randomly selected 100k subset of NuminaMath-CoT, featuring mathematical problems ranging from Chinese high school exercises to international mathematics olympiad competitions. Each solution follows a Chain of Thought (CoT) format [(Wei et al., 2022)](#b34).

These datasets contain substantially more samples than LIMO's training set (817 examples), allowing us to examine the relationship between data quantity and model performance.

## Main Results

Our experimental results demonstrate LIMO's superior performance across both in-domain and out-of-domain tasks, as shown in Table [3](#tab_4).

## In-domain

Performance On in-domain tasks, LIMO achieves the best results across all benchmarks. For AIME24, LIMO achieves 57.1% accuracy, outperforming QwQ-32B-Preview (50.0%) and OpenAI-o1-preview (44.6%) by significant margins. Most notably, on MATH500, LIMO achieves 94.8% accuracy, surpassing QwQ-32B-Preview (89.8%) and OpenAI-o1-preview (85.5%). The performance gap is even more pronounced on AMC23, where LIMO reaches 92.0% accuracy compared to QwQ-32B-Preview's 83.6%. Out-of-domain Generalization LIMO demonstrates strong generalization capabilities across diverse out-ofdomain tasks. On OlympiadBench, LIMO achieves 66.8% accuracy, significantly outperforming QwQ-32B-Preview (58.5%) and the base model (45.3%). Similar improvements are observed on other challenging benchmarks such as CHMath (75.4% vs 68.5%) and GradeSchool (76.2% vs 63.8%). Notably, LIMO maintains competitive performance even on GPQA, where it achieves 66.7% accuracy, close to OpenAI-o1-preview's leading score of 73.3%. Comparison with Larger Datasets Our experiments reveal that despite larger scale, both baseline datasets underperform compared to LIMO. NuminaMath-100k shows significant degradation (32.3% vs. base model's 49.9%) due to uncurated reasoning chains, while OpenThoughts-114k achieves suboptimal results (58.3%) probably due to unfocused problem selection. In contrast, LIMO's carefully curated 817 problems yield superior performance (72.8%), demonstrating that targeted selection and high-quality annotations are more crucial than data quantity for developing robust reasoning capabilities.

Overall Performance LIMO achieves the highest average performance of 72.1% across all benchmarks, substantially outperforming OpenAI-o1-preview (67.8%), QwQ-32B-Preview (66.4%), and other baselines. This comprehensive evaluation demonstrates that LIMO's carefully curated training approach with just 817 examples can outperform models trained on datasets that are orders of magnitude larger.

## Analysis

## RQ1: Impact of Reasoning Chain Quality

To gain a deeper understanding of why Less-Is-More achieves such remarkable results, we investigate the quality of reasoning chains (CoT). A fundamental question naturally arises: what characteristics define a high-quality reasoning chain that leads to superior model performance? To address this, we conducted a controlled comparative study examining how solutions of varying quality for the same problem statements affect the performance of models trained on them.

Setup To conduct this analysis, we selected 500 problems from the LIMO dataset. The selection was based on the intersection of problems for which the models used in rejection sampling exhibited performance differences and those with corresponding human-annotated solutions, ensuring consistency across comparisons. For these 500 problems, we collected and categorized solutions into five distinct quality levels based on our comprehensive evaluation framework. These solutions were sourced from various origins, including human experts, AI specialists, and model-generated responses, then classified strictly based on their reasoning quality rather than their source.

Quality Measure Following the principles outlined in Section 3.3.2, we took a holistic approach to categorize the reasoning chains into five quality levels (L1-L5, with L5 being the highest). Our assessment focused on several key aspects: how well the steps were organized and connected, whether important logical transitions were properly explained, and if the solution included self-verification steps to check the work. Using these general guidelines, we classified L5 solutions as those showing excellent organization with clear, well-explained steps and thorough self-verification. L4 solutions were also well-structured but perhaps with slightly less rigorous checking. L3 solutions showed decent organization but sometimes skipped over explaining crucial logical leaps. L2 solutions often provided abbreviated reasoning without much explanation, while L1 solutions typically just listed basic steps with minimal elaboration and rarely included any verification. 

## Analysis

## Results

Our training results (Figure [2](#fig_2)) strongly correlate with the reasoning chain quality levels. Models trained on L5 quality reasoning chains achieved the highest performance on both AIME24 and MATH500, demonstrating the effectiveness of well-structured, detailed, and self-verified reasoning. Performance consistently decreased with each quality level, with L4 and L3 showing moderate success, while L2 and L1 resulted in notably lower performance. These results empirically validate our quality assessment framework and highlight the crucial role of high-quality reasoning chains in model performance. Specifically, we observed that the performance gap between L5 and L1 solutions was substantial -approximately 15 percentage points on AIME24 and 12 percentage points on MATH500. This significant difference suggests that the quality of reasoning chains plays a far more important role in model performance than previously assumed, reinforcing the importance of carefully curating training data to include well-structured, thorough solutions.

## RQ2: Impact of Question Quality

We hypothesize that more challenging problems foster complex reasoning chains, diverse thought processes, and enhanced knowledge integration, enabling LLMs to better leverage pre-trained knowledge for high-quality inference. To validate this hypothesis, we investigate how question quality affects the reasoning capabilities of models fine-tuned on these questions and their corresponding solutions.

Setup We selected three sets of problems of similar size but increasing difficulty, constructing solutions in a consistent manner to form three training datasets. Our findings indicate that models trained on more challenging datasets exhibit superior reasoning performance. Specifically, we sampled three sets of problems, each containing 500 samples, from MATH and AIME:

• Simple-500: 500 simple problems randomly selected problems from MATH levels 1 and 2.

• Complex-500: 500 complex problems randomly selected problems from MATH levels 3, 4, and 5.

• Advanced-500: 500 advanced problems randomly selected problems from past AIME tests.

## Analysis

To rigorously establish the increasing difficulty of these sets, we evaluated various LLMs on them, observing a decline in accuracy and an increase in the average length of correctly generated reasoning chains.

We then used DeepSeek-R1 to generate solutions, which represent the highest-quality solutions available, for each problem set, forming the training data for fine-tuning Qwen2.5-32B-Instruct.

## Results

We evaluate all three fine-tuned models on the AIME2024 and MATH500 benchmarks to assess their reasoning performance. The results (Figure [3](#fig_3)) indicate that modifying the selection of problems alone leads to a 16% improvement in accuracy on the challenging AIME2024 benchmark, reaching 51.5%. Furthermore, despite the absence of in-domain training data, the model fine-tuned on Advanced-500 outperforms the other two models, achieving an accuracy of 91.2% on the MATH500 benchmark. This result suggests that the improvement in reasoning ability due to increased problem difficulty generalizes across datasets. 

## RQ3: LLM Backbone

Building on our LIMO hypothesis, which emphasizes the importance of latent prerequisite knowledge within the model's parameter space, we examine how different pre-training data affect a model's capacity to leverage minimal exemplars for mathematical reasoning. This investigation allows us to assess the first key factor of our hypothesis: the role of pre-trained knowledge in enabling complex reasoning capabilities.

Setup To isolate the impact of pre-training while controlling for model architecture and fine-tuning procedures, we conduct experiments using two 32B-parameter variants of the Qwen model family: Qwen1.5-32B-Chat (Team, 2024a) and Qwen2.5-32B-Instruct (the base model of LIMO). Both models share the same architecture and parameter count, while Qwen2.5 demonstrates significant improvements in pre-training data quality, particularly in mathematical and code-related data, compared to its predecessor. We SFT both models using identical LIMO datasets and evaluation protocols, assessing their performance on the AIME2024 and MATH500 benchmarks.

## Results

Our experiments reveal that the choice of pre-trained model dramatically impacts reasoning performance, as demonstrated in 4. LIMO, built on Qwen2.5-32B-Instruct, significantly outperforms its predecessor across both benchmarks. On the challenging AIME2024 test, LIMO achieves 57.1% accuracy, a remarkable 47.1 percentage point improvement over Qwen1.5-32B-Instruct's 10.0%. Similarly, on MATH500, LIMO demonstrates exceptional performance with 94.8% accuracy, surpassing Qwen1.5-32B-Instruct by 34.4 percentage points. These substantial improvements suggest that the enhanced pre-training in Qwen2.5 creates a stronger foundation for mathematical reasoning. The results align with our LIMO hypothesis, indicating that richer pre-trained knowledge within the model's parameter space enables more effective utilization of minimal exemplars during fine-tuning.

## Case study

Qualitative Analysis Fig. [5](#fig_5) compares responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO. LIMO achieves capabilities and behaviors comparable to DeepSeek-R1, despite using minimal data and compute resources (only 817 training samples). Notably, LIMO demonstrates strong self-reflection and long chain-of-thought generation capabilities. LIMO verifies its own statements ("Wait, 24 minutes is 0.4 hours? Wait, no. Wait, 60 minutes is 1 hour, so 24 minutes is 24/60, which is 0.4 hours") and validates its calculations ("But let me check again. Maybe I made a mistake in calculations."). Furthermore, it learns to allocate additional tokens (compute) for detailed complex equation-solving ("Now let's compute the left side, • • • , multiply both sides by 2) to prevent errors. In contrast, the base model Qwen2.5-32B-Instruct exhibits limitations in its reasoning process, being unable to correct inaccurate statements and failing to cross verify equation 2 in its solution. These results Okay, let's see. So Aya goes for a 9-km walk every morning and then stops at a coffee shop. The time she takes for the walk, including the time in the coffee shop, depends on her walking speed. [omitted for brevity…]. When she walks a bit faster, at s plus 2 kilometers per hour, the total time is 2 hours and 24 minutes, which is 2 hours and 24 minutes, so that's 2.4 hours? Wait, 24 minutes is 0.4 hours? Wait, no. Wait, 60 minutes is 1 hour, so 24 minutes is 24/60, which is 0.4 hours. So 2 hours and 24 minutes is 2.4 hours. [omitted for brevity…] So, let's set up equations. Let's denote the walking time as T1 when she walks at s km/h, and T2 when she walks at s+2 km/h. Then, T1 + t = 4 hours, and T2 + t = 2.4 hours. But t is in minutes, so we need to convert that to hours or convert the total time to minutes. Maybe it's better to keep everything in hours. So t minutes is t/60 hours. [omitted for brevity…] For the first case: walking time T1 = 9/s hours, then total time T1 + t/60 = 4 hours. For the second case: walking time T2 = 9/(s+2) hours, then total time T2 + t/60 = 2.4 hours. So now we have two equations: 1) 9/s + t/60 = 4, 2) 9/(s+2) + t/60 = 2.4 Let me write that as: 9*(1/s -1/(s+2)) = 1.6. So, 1/s -1/(s+2) = 1.6/9 Let me compute 1.6 divided by 9. Quantitative Analysis Table [4](#tab_5) demonstrates the differences between models trained with varying sample quality. We observe a general trend where increasing post-training example quality leads to models that generate longer responses with more lines. Additionally, these higher-quality models employ more self-reflecting transitions (e.g., wait, perhaps, maybe, therefore) to allocate additional inference tokens (compute) for deeper thinking.

6 Background and Related Work ). These curated corpora can be composed of multiple sources, such as textbooks, scientific papers, and mathematical code, which capture diverse human cognitive patterns used to solve problems. In the post-training phase, a line of research focuses on curating large-scale instruction data to teach LLMs to reason [(Yue et al., 2023](#b40)[(Yue et al., , 2024;;](#b41)[Li et al., 2024a)](#). This includes scaling the number of questions and their corresponding solutions. The scaling approach is promising and has achieved significant performance gains. However, the reasoning ability gained through this method has been criticized for relying on the memorization of fixed patterns rather than achieving true generalization [(Mirzadeh et al., 2024;](#)[Zhang et al., 2024)](#b42). For example, [Mirzadeh et al. (2024)](#) finds that LLMs exhibit noticeable variance when responding to different instantiations of the same question, and their performance declines when only the numerical values in the question are altered. This raises doubts about the generalization capability of SFT methods [(Chu et al., 2025)](#b4) and whether LLMs can be true reasoners rather than mere knowledge retrievers (Kambhampati, 2024).

## Test-time Scaling and Long Chain Reasoning

Instead of focusing on scaling model parameters and training data [(Kaplan et al., 2020)](#b13)

## Data Efficiency in Language Models

Zhou et al. (2024a) demonstrates that with just 1,000 carefully curated prompts and responses, models can learn to follow specific formats and generalize well to unseen tasks. The findings emphasize the importance of quality over quantity in the alignment process. However, whether this lesson can be applied to reasoning tasks remains uncertain, given the potential high computational complexity of such tasks [(Merrill and Sabharwal, 2024;](#b19)[Xiang et al., 2025)](#b35). While some work on reasoning highlights the importance of quality during the curation of training data [(Zhou et al., 2024b)](#), the quantity of such data is still much larger compared to that in LIMA. Our work extends the ideology of LIMA to reasoning tasks by investigating what constitutes high-quality questions and solutions, and demonstrates that the reasoning ability of LLMs can be enhanced in a highly data-efficient manner.

## Future Work

While LIMO demonstrates remarkable success in mathematical reasoning with minimal data, several promising directions remain for future exploration.

Domain Generalization: First, extending the LIMO hypothesis to broader reasoning domains represents a critical next step. While our work focuses on mathematical reasoning, the principles of high-quality reasoning chains could potentially generalize to scientific reasoning, logical deduction, and causal inference. Understanding how these principles transfer across domains could reveal universal patterns in effective reasoning. This exploration would require adapting our quality metrics and developing domain-specific evaluation frameworks, ultimately contributing to a more comprehensive theory of machine reasoning.

Theoretical Foundations: A deeper theoretical understanding of LIMO's success is also essential. Future research should focus on formalizing the relationship between pre-training knowledge, inference-time computation, and reasoning capabilities. This includes investigating the minimum threshold of pre-trained knowledge required for effective reasoning and developing mathematical models to predict the optimal balance between reasoning chain quality and quantity. Such theoretical foundations could guide the development of more efficient training strategies and provide insights into the fundamental nature of machine reasoning.

Automated Assessment: The development of automated quality assessment tools represents another crucial direction. Current manual evaluation of reasoning chain quality, while effective, is time-consuming and difficult to scale. Future work should focus on creating automated systems that can evaluate and improve reasoning chain quality based on our proposed metrics. This could include developing algorithms that automatically enhance existing reasoning chains and generate high-quality ones with minimal human intervention, making the LIMO approach more accessible and scalable.

Multi-modal Integration: Cross-modal reasoning presents an exciting frontier for extending LIMO's principles.

As real-world reasoning often involves multiple modalities, investigating how visual information and structured data can enhance mathematical reasoning capabilities is crucial. This research direction would require developing new quality metrics for multi-modal reasoning chains and understanding how different types of information can be effectively integrated into the reasoning process.

Real-world Impact: The application of LIMO principles to real-world scenarios deserves significant attention. Future work should focus on adapting these approaches to practical problems in education, scientific research, and industrial applications. This includes developing specialized versions of LIMO for specific domains and creating tools that help human experts generate high-quality reasoning chains for complex real-world problems. Such applications could significantly impact how we approach problem-solving in various fields.

Cognitive Science Bridge: Finally, integrating insights from cognitive science could provide valuable directions for improvement. Understanding the parallels between LIMO's reasoning patterns and human cognitive processes could inform the development of more effective reasoning strategies. This includes studying how different reasoning approaches affect model performance and generalization, and incorporating cognitive science principles into the design of reasoning chains. Such research could not only improve AI systems but also provide insights into human reasoning processes. These future directions collectively aim to deepen our understanding of efficient reasoning in large language models while expanding their practical applications. By pursuing these paths, we can work toward more sophisticated, efficient, and widely applicable reasoning systems that better serve human needs across various domains.

![Figure 1: LIMO achieves substantial improvement over NuminaMath with fewer samples while excelling across diverse mathematical and multi-discipline benchmarks.]()

![Figure 2: Comparison of models trained on reasoning chains of different quality levels.]()

![Figure 3: Performance comparison on MATH and AIME benchmarks between models trained on different question quality: Simple-500, Complex-500, and Advanced-500.]()

![Figure 4: Impact of Pre-trained Model Choice on Mathematical Reasoning Performance]()

![Figure 5: Comparison between the responses generated by Qwen2.5-32B-Instruct, DeepSeek-R1, and LIMO]()

![Evolution of Mathematical Reasoning in LLMsLarge-scale training data has been the driving force behind the development of reasoning abilities in LLMs. In the pretraining phase, the reasoning ability of LLMs can be enhanced by relevant corpora(Wang et al., 2024; Azerbayev et al., 2024; Paster et al., 2023; Shao et al., 2024]()

![Comparative Analysis: Less-is-More Phenomena in Language Models]()

![Comparative Analysis of LIMO and RL Scaling Approaches]()

![Comparison of model performance (pass@1) across various mathematical reasoning benchmarks Models include state-of-the-art LLMs (OpenAI-o1-preview, QwQ-32B-Preview), our base model (Qwen2.5-32B-Instruct), and models fine-tuned on different datasets. Training data sizes are shown in parentheses. Best results for each benchmark are shown in bold. Our proposed LIMO model (highlighted in blue) achieves superior performance despite using significantly fewer training examples (817) compared to other fine-tuned models (more than 100k).]()

![, recent work has shifted to exploring test-time scaling(OpenAI, 2024;Snell et al., 2024), i.e., increasing the number of tokens to improve 6.3 Data Efficiency in Language Models Statistical analysis of models trained with examples of varying data quality. This table presents three key metrics: average token count per response, average line count per response, and frequently occurring keywords in model-generated responses. Keywords associated with reasoning transitions and uncertainty are highlighted in bold, with common stop words (e.g., "a", "the") excluded to focus on substantive language patterns. Notable differences in response length and keyword usage patterns suggest varying levels of reasoning complexity.]()

https://github.com/open-thoughts/open-thoughts

