{
  "arxivId": "2412.02595",
  "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon\n  Pretraining Dataset",
  "authors": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro",
  "abstract": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved\nsignificant benchmark gains via aggressive model-based filtering, but at the\ncost of removing 90% of data. This limits their suitability for long token\nhorizon training, such as 15T tokens for Llama 3.1. In this paper, we show how\nto achieve better trade-offs between accuracy and data quantity by a\ncombination of classifier ensembling, synthetic data rephrasing, and reduced\nreliance on heuristic filters. When training 8B parameter models for 1T tokens,\nusing a high-quality subset of our data improves MMLU by 5.6 over DCLM,\ndemonstrating the efficacy of our methods for boosting accuracies over a\nrelatively short token horizon. Furthermore, our full 6.3T token dataset\nmatches DCLM on MMLU, but contains four times more unique real tokens than\nDCLM. This unlocks state-of-the-art training over a long token horizon: an 8B\nparameter model trained for 15T tokens, of which 7.2T came from our dataset, is\nbetter than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5\non average across ten diverse tasks. The dataset is available at\nhttps://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html",
  "url": "https://arxiv.org/abs/2412.02595",
  "issue_number": 632,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/632",
  "created_at": "2025-01-04T06:53:24.608407",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T20:02:25.502Z",
  "main_tex_file": null,
  "published_date": "2024-12-03T17:28:50Z",
  "arxiv_tags": [
    "cs.CL"
  ]
}