<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset</title>
				<funder>
					<orgName type="full">Common Crawl Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-03">3 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Markus</forename><surname>Kliegl</surname></persName>
							<email>mkliegl@nvidia.com.</email>
						</author>
						<author>
							<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><forename type="middle">Catanzaro</forename><surname>Nvidia</surname></persName>
						</author>
						<title level="a" type="main">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-03">3 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">76CC59B05B4348B21FBC582AA14776A1</idno>
					<idno type="arXiv">arXiv:2412.02595v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at <ref type="url" target="https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html">https://data.commoncrawl.org/contrib/ Nemotron/Nemotron-CC/index.html</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Internet crawl is the largest source of unique tokens for training LLMs and can be seen as serving two main purposes: high-quality content and diversity. Recent English datasets derived from Common Crawl 1 such as FineWeb-Edu <ref type="bibr" target="#b28">(Penedo et al., 2024)</ref> and DCLM <ref type="bibr" target="#b39">(Li et al., 2024)</ref> have emphasized high-quality content that boosts benchmark accuracies over data quantity. They have demonstrated significant strides in achieving benchmark results competitive with some of the best closed models at a small scale (e.g., DCLM's 7B model trained over 2.6T tokens), primarily thanks to the use of model-based filters to extract high-quality educational and instructional content. However, this comes at the cost of data quantity: they remove around 90% of the data. Such aggressive pruning may not be the most effective strategy when training larger models over longer token horizons (e.g., Llama 3.1 includes 8B-405B parameter models, trained for 15T tokens <ref type="bibr" target="#b10">(Dubey et al., 2024)</ref> and Gemma 2 27B was trained for 13T tokens <ref type="bibr">(Team et al., 2024)</ref>). Both DCLM and FineWeb-Edu contain around 80% near-duplicates (1T and 0.2T unique tokens, respectively) <ref type="bibr" target="#b5">(Ben Allal, 2024;</ref><ref type="bibr" target="#b39">Li et al., 2024)</ref> and to train on these datasets for many trillions of tokens implies seeing essentially the same samples many times during training. This could lead to inferior models, as <ref type="bibr" target="#b27">Muennighoff et al. (2024)</ref> find there are diminishing returns after four epochs compared to training on more unique tokens.</p><p>In this paper, we show how to achieve a better trade-off between benchmark accuracy and data quantity with a combination of classifier ensembling, synthetic data generation, and reduced reliance on heuristic filters. Our main contributions are:</p><p>1. We propose a method for transforming English Common Crawl into a 6.3T token longhorizon pretraining dataset, consisting of 4.4T globally deduplicated original tokens and 1.9T synthetically generated tokens. We release the dataset<ref type="foot" target="#foot_0">foot_0</ref> and plan to release an implementation as part of the open-source NeMo Curator library.<ref type="foot" target="#foot_1">foot_1</ref> </p><p>2. We prove the effectiveness of this method by comparing to the state-of-the-art open English Common Crawl datasets DCLM and FineWeb-Edu (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>(a) A 1.1T-token high-quality subset of our data achieves a 5.6 MMLU improvement over DCLM, showing the superiority of our method over a relatively short token horizon. (b) Our full dataset performs on par with DCLM while having 4× as many unique real tokens. (c) This larger size enables state-of-the-art results over long token horizons: An 8B parameter model trained for 15T tokens using a weighted version of our dataset achieves higher overall accuracy than Llama 3.1 8B, and in particular MMLU 70.3 vs. Llama's 65.3. Note that Llama 3.1 8B was also trained on 15T tokens <ref type="bibr" target="#b10">(Dubey et al., 2024)</ref>.</p><p>3. We conduct ablation studies and find:</p><p>(a) Ensembling different model-based classifiers can help select a larger and more diverse set of high quality tokens. (b) Rephrasing can effectively reduce noise and errors in low-quality data and produce diverse variants with fresh unique tokens from high-quality data, leading to better results in downstream tasks. (c) Disabling traditional non-learned heuristic filters for high-quality data can further boost high quality token yield without hurting accuracy.</p><p>Finally, we remark that our overall guiding principle is to shift from a static, non-learned, heuristic pipeline towards a more learned flywheel whose performance will naturally get better over time. As our data improves, so will the LLMs we train, and these improved LLMs will in turn improve our data as we use them to generate better synthetic data and quality classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section we explain our efforts to build the best English Common Crawl pretraining dataset for LLMs. Our efforts can be splitted into three folds. First, we talk about our efforts in boosting token yield by utilizing text extractor and heuristic filters more properly in Section 2.1. Second, we introduce the model-based quality labeling pipeline methods in Section 2.2. Third, we introduce our synthetic data generation method to further improve the data quality in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">HTML-to-text Extractor &amp; Filter</head><p>Extracted texts from HTMLs are the foundation and major source of LLM pretraining dataset, so it is of great significance to analyze and understand the extraction tools for optimal data quality and token yield. Moreover, heuristic filters are often utilized to remove low-quality tokens with humandesigned heuristics <ref type="bibr" target="#b39">(Li et al., 2024;</ref><ref type="bibr">Parmar et al., 2024;</ref><ref type="bibr" target="#b28">Penedo et al., 2024;</ref><ref type="bibr" target="#b10">Dubey et al., 2024)</ref>, which may also put good tokens at the risk of being removed. We carefully examine both aspects with the assist of the FineWeb-Edu classifier <ref type="bibr" target="#b28">(Penedo et al., 2024)</ref>, a model-based quality classifier that had shown effectiveness in identifying high-quality tokens that are significant in boosting the strength of LLMs.</p><p>#Tokens #HQ tokens #HQ +% Trafilatura-filtered 994 80 -Justext-filtered 1,380 104 28.6% Justext 1,804 127 57.4%</p><p>Table 1: Extraction and filteration token count statistics (billion). Tokens counted after deduplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HTML-to-text Extraction</head><p>We test two HTMLto-text extractors, Justext <ref type="bibr" target="#b30">(Pomikálek, 2011)</ref> and Trafilatura <ref type="bibr" target="#b4">(Barbaresi, 2021)</ref>. Qualitatively, we view both extractors at the same level of quality.</p><p>While quantitatively, we calculate token yields of both extractors on 13 selected snapshots of Common Crawl (see Appendix D). The statistics were reported in Table <ref type="table">1</ref>. We see that Justext can yield more tokens, notably more high-quality tokens (+28.6%) by the standard of Fineweb-Edu classifier (score 3, 4, and 5). We highlight that, boosting unique token amount is of great importance when building long-horizon pretraining dataset, e.g., 15T tokens for Llama3.1. We keep only English text, as determined by pycld2<ref type="foot" target="#foot_2">foot_2</ref> and the FastText lid176 language classifier<ref type="foot" target="#foot_3">foot_3</ref> with threshold 0.3 <ref type="bibr">(Joulin et al., 2016a,b)</ref>. After extraction, we apply global fuzzy deduplication as well as exact substring deduplication <ref type="bibr" target="#b21">(Lee et al., 2022)</ref>, using the NeMo Curator library<ref type="foot" target="#foot_4">foot_4</ref> and the deduplicate-text-datasets library,<ref type="foot" target="#foot_5">foot_5</ref> respectively.</p><p>Filtering Conventionally, heuristic filters are leveraged to remove low-quality tokens from the pretraining dataset as a post-processing step <ref type="bibr" target="#b39">(Li et al., 2024;</ref><ref type="bibr">Parmar et al., 2024;</ref><ref type="bibr" target="#b28">Penedo et al., 2024;</ref><ref type="bibr" target="#b10">Dubey et al., 2024)</ref>. We revisit the filtering pipeline as in <ref type="bibr">(Parmar et al., 2024)</ref>. Such pipeline sequentially consists of a set of heuristic filters proposed by <ref type="bibr" target="#b32">Raffel et al. (2020)</ref>; Rae et al.</p><p>(2021) and a perplexity filter based on a KenLM model <ref type="bibr" target="#b15">(Heafield, 2011)</ref> trained on Wikipedia and books data <ref type="bibr" target="#b44">(Wenzek et al., 2020)</ref>. To quantitatively better understand the effectiveness of the filtering pipeline, we calculate the token yield and report the numbers in Table <ref type="table">1</ref>. We find the filtering pipeline removes a non-trivial portion of high-quality tokens (-18.1%) classified by FineWeb-Edu classifier from the dataset. Given the impact that the heuristic filters have on the high-quality token yield, we propose to NOT apply such filters to the high-quality tokens distinguished by model-based quality classifers (described in the next section), but only use those on the low-quality splits. In the experiment section we empirically verify the impact of both the extractor and filter on pretraining data quality through down-stream benchmarks. We refer readers to Section 3.3 for detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model-based Quality Labeling</head><p>Recent work <ref type="bibr" target="#b39">(Li et al., 2024;</ref><ref type="bibr" target="#b28">Penedo et al., 2024)</ref> use model-based classifiers to extract high-quality pretraining documents from English Common Crawl. However, both of the two quality classifiers have a limited recall (less than 10%) of highquality tokens, and this will become a bottleneck to train an LLM over a long horizon. Also, the quality labels assigned by the quality classifier are not necessarily aligned with LLM's downstream task performance. Therefore, we propose our ensemblebased quality labeling pipeline method. Specifically, we first build three quality classifiers, each of which has different high-quality preferences. Then, we ensemble the three classifiers to score all the documents, and split the crawl corpus into different quality buckets based on the quality score. Finally, we regroup the fine-grained document buckets into 5 different quality levels based on their corresponding performance on downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Classifier Training</head><p>Preparing pretraining documents with quality annotations is the first key step in building a quality classifier <ref type="bibr" target="#b10">(Dubey et al., 2024;</ref><ref type="bibr">Abdin et al., 2024;</ref><ref type="bibr" target="#b46">Yang et al., 2024)</ref>. Similar to the work <ref type="bibr" target="#b28">(Penedo et al., 2024)</ref>  <ref type="foot" target="#foot_6">8</ref> , we constructed two versions of quality annotation data. We prompt Mistral 8x22B-instruct<ref type="foot" target="#foot_7">foot_7</ref> and Nemotron-340B-instruct <ref type="bibr" target="#b2">(Adler et al., 2024)</ref>, to score web documents from FineWeb based on their educational value on a scale from 0 to 5. We then fine-tune a linear regression model on top of the Snowflakearctic-embed-m embedding model <ref type="bibr" target="#b25">(Merrick et al., 2024)</ref> using the two different version of training sets. The two models have been trained for 20 epochs with a learning rate of 3e-4, with the embedding and encoder layers frozen, and we selected the checkpoint with the highest F1 score on the held-out validation set.</p><p>We also employ the DCLM classifier which is a fastText-based classifier released by <ref type="bibr" target="#b39">Li et al. (2024)</ref>. The DCLM classifier is trained on a combination of instruction-formatted data <ref type="bibr" target="#b41">(Teknium, 2023)</ref> and high-scoring posts data from ELI5 subreddit <ref type="bibr" target="#b12">(Fan et al., 2019)</ref>, and has shown stronger performance in identifying high-quality pretraining tokens, com-pared to the FineWeb-Edu classifier <ref type="bibr" target="#b28">(Penedo et al., 2024)</ref>. The DCLM classifier will offer a new perspective in labeling high-quality pretraining documents, and will help increase the recall of highquality tokens.</p><p>Quality Scoring and Bucketing First, we use each of the three classifiers to predict the quality scores for all the documents. Then based on the ranked quality score from each classifier, we rounded the model's output score to integers from 0 to 19. So that each score bucket will have around 5% of the documents, and bucket 19 will have the top 5% highest quality documents. We then assign the final quality score for each document by ensembling the three classifiers' integer score by a maximum operation. The number of documents distribution in each buckets will be skewed by the ensemble operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Labeling</head><p>In order to assign a quality label that is more aligned with their real performance on downstream tasks, we further group the finegrained quality score predicted by three classifiers into 5 downstream quality categories. We used annealing to access each data bucket's downstream task's quality. Specifically, we measure the quality of each bucket by continuous pretraining with 50B tokens on a 70% trained 8B models. We assign 66% of weight to the default data mix and 34% to the dataset that we are evaluating. By comparing the average performance of each bucket over 9 tasks, we group the 20 buckets into 5 big categories, with the final distribution shown in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Label</head><p>Buckets Token (%) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Synthetic Data Generation</head><p>Upon reviewing samples across the quality tiers, we observe that documents with lower scores tend to contain more noise and errors, while those scoring higher generally exhibit good writing and formatting. Therefore, we employ different strategies when generating data from low-and high-quality documents.</p><p>For low-quality data, our goal is to improve the quality by reducing noise and errors while preserving useful information, thereby decreasing training compute expenses. As shown by <ref type="bibr" target="#b24">Maini et al. (2024)</ref>, rephrasing web data using a medium-sized language model yields an enhanced parallel corpus of synthetic data, thereby reducing model perplexity and boosting its accuracy on downstream tasks. Unlike existing methods that create new content such as textbooks and short stories <ref type="bibr" target="#b43">(Wang et al., 2023;</ref><ref type="bibr" target="#b11">Eldan and Li, 2023;</ref><ref type="bibr" target="#b14">Gunasekar et al., 2023)</ref>, our rephrasing-based approach does not utilize the language model as a knowledge bank but focuses on transforming provided texts into another style, allowing it to operate with a lighter-weight model. We adopt the Wikipedia style prompt from <ref type="bibr" target="#b24">(Maini et al., 2024)</ref> to rewrite low-quality documents (Prompt 5 in Appendix F), which effectively reduces errors and redundancies and improves formatting.</p><p>For high-quality data, we aim to obtain more unique tokens and condense essential knowledge. According to <ref type="bibr" target="#b27">(Muennighoff et al., 2024)</ref>, adding repeated tokens yields a diminishing return, especially after 4 epochs. For high-quality documents, we generate synthetic data using four additional prompts: (1) Diverse Question-Answer (QA) pairs: ask questions in various forms (e.g., yes/no question, open-ended question, multi-choice question) about factual information in the text and provide the correct answers; (2) Distill: rewrite the text into a concise and clear passage; (3) Extract knowledge: rewrite knowledge from the text and disregard uninformative content; (4) Knowledge list: extract key information from the text as an organized list. We require the model to provide clear and concise responses while preserving factual information and concrete details such as numbers. The full prompts are shown in Appendix F.</p><p>As we increase the length of provided text, the model shows a tendency to produce over-simplified outputs with reduced detail. Therefore, we chunk each document into segments, each of which contains one or more complete lines and is shorter than a specific token limit.<ref type="foot" target="#foot_8">foot_8</ref> Over-length lines exceeding the token limit are discarded.  Our post-processing steps include removing incomplete results, eliminating specific Markdown formatting (e.g., double asterisks), stripping away prefixes of certain patterns (e.g., "Here is a paraphrased version:" and "Paraphrased Text:"), removing quotation marks enclosing the entire response, and filtering out under-length outputs (i.e., shorter than 50 tokens). For Wikipedia results, we concatenate passages generated from segments belonging to the same original document. For Diverse QA Pairs results, we shuffle the generated question and answer pairs, retain up to a number based on the length of the segment, and append the pairs to the end of the segment.</p><p>Using the instruct version of Mistral NeMo 12B<ref type="foot" target="#foot_9">foot_9</ref> with FP8 inference, we synthesize over 1.8T tokens as Table <ref type="table" target="#tab_3">3</ref> shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents. We do not use mediumquality documents for synthetic data generation. We employ TensorRT-LLM<ref type="foot" target="#foot_10">foot_10</ref> and NeMo-Skills<ref type="foot" target="#foot_11">foot_11</ref> to enable large-scale data synthesis. Table 6: Comparison of our 8B parameter model vs Llama 3.1 8B. Both were trained for 15T tokens. The numbers for Llama 3.1 are from our own lm-evaluation-harness setup described in Section 3.1 and may not match Meta's publicly reported numbers, as Meta made various customizations to the benchmarks.</p><p>to evaluate on the following ten common sense and reasoning tasks (reported metric in parentheses): ARC-Easy and ARC-Challenge (normalized accuracy) <ref type="bibr" target="#b9">(Clark et al., 2018)</ref>, Hellaswag (normalized accuracy) <ref type="bibr" target="#b47">(Zellers et al., 2019)</ref>, Winogrande (accuracy) <ref type="bibr" target="#b33">(Sakaguchi et al., 2021)</ref>, RACE (accuracy) <ref type="bibr" target="#b20">(Lai et al., 2017)</ref>, PIQA (normalized accuracy) <ref type="bibr" target="#b6">(Bisk et al., 2020)</ref>, Social IQA (accuracy) <ref type="bibr" target="#b34">(Sap et al., 2019)</ref>, Commonsense QA (accuracy) <ref type="bibr" target="#b38">(Talmor et al., 2019)</ref>, Openbook QA (normalized accuracy) <ref type="bibr" target="#b26">(Mihaylov et al., 2018)</ref>, and MMLU (accuracy) <ref type="bibr" target="#b16">(Hendrycks et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main Results</head><p>Short Token Horizon (1T) To validate the quality of our datasets, we first train standard 8B parameter transformer LLMs over a relatively short 1T token horizon. The results are shown in Table 5. Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over DCLM and FineWeb-Edu on all tasks except RACE. In particular, there is a 5.6 MMLU and 3.1 average gain over DCLM. This shows the effectiveness of our classifier ensembling and synthetic data even in the non-dataconstrained setting. Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with DCLM. But since this dataset contains 4× more unique real tokens, we expect it to be superior in data-constrained settings like 15T token training runs.</p><p>Long Token Horizon (15T) Our dataset contributed 7.2T of the tokens used to train an 8B model for 15T tokens. As shown in Table <ref type="table">6</ref>, our model achieves a higher average accuracy than Llama 3.1 8B, which was also trained for 15T tokens, including an MMLU score of 70.3 vs. Llama's 65.3. This shows that our dataset is indeed suitable for state-of-the-art training over long token horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To further investigate the contribution and effect of each module in our method, we conducted thorough ablation studies.</p><p>Extractor &amp; Filter Comparison As we have discussed in Section 2.1, by deploying Justext instead of Trafilatura and removing filter from the postprocessing step, we can attain significantly 57.4% more high-quality tokens. We also conduct ablation studies to better understand the impact of the extractor selection and the removal of filter through downstream benchmarks. We carry out four 8B-1T experiments. We report the benchmark scores in Table <ref type="table">7</ref>. Beyond the token-yield benefit by leveraging Justext instead of Trafilatura and not using heuristic filters, we see that combining these two does not impact the downstream task accuracies with only marginal differences (comparing Trafilatura filtered vs. Justext unfiltered). Moreover, when we ONLY remove filter from high-quality tokens, the results get further improved (comparing Justext unfiltered vs. Justext HQ unfiltered). In particular, MMLU gets boosted by +2%. Note that, the motivation behind removing filter is to boost token yield, especially on high-quality tokens due to the notable scarcity of such. Given the experi-mental results and considering the overall growth in token yield, we opt to only remove filter from high-quality tokens.</p><p>Exp name MMLU Avg (non-MMLU) Trafilatura filtered 55.4 60.6 Justext filtered 54.1 60.9 Justext unfiltered 55.5 60.3 Justext HQ unfiltered 57.5 60.6 Table 7: Ablation studies on extractor and filter. HQ means high-quality data judged by FineWeb-Edu classifier (score 3,4,5). See Appendix E for more details.</p><p>Classifiers Comparison Assembling different classifiers to label the document quality is one of the key steps in constructing our datasets, so we did thorough analysis and comparison of the component.</p><p>We did a detailed comparison of two types of classifiers that we employ in our method: the FineWeb-Edu classifier which score document quality based on their educational-level, and the DCLM-based classifier which value the informativeness of the document. We compare the highquality documents predicted by the two classifiers on a randomly selected Common Crawl Snapshot (CC-MAIN-2021-21). Table <ref type="table">8</ref> shows the document statistics comparison. We can see that only 10% of the documents are predicted as high quality by both classifiers, while 35.4% documents are predicted as high quality by FineWeb-Edu classifier only, and 54.4% of documents are predicted as high-quality by DCLM classifier. Therefore, ensembling different classifiers can increase the recall of high-quality documents from Common Crawl. <ref type="foot" target="#foot_14">16</ref>We further compare each of the classifiers with the ensembled method<ref type="foot" target="#foot_15">foot_15</ref> by their downstream tasks' performances. We pretrain 8B parameters LLMs with 1T tokens, using the high-quality documents labeled by different classifiers on randomly selected 13 Common Crawl snapshots (see Appendix D). Table <ref type="table" target="#tab_9">9</ref> shows the detailed comparison on different evaluation tasks. We can see that the ensembled method greatly boost the high-quality tokens percentage from 9% to 25%, while still achieving the highest general language understanding performance on average on all the tasks. The ensembled method also outperforms the FineWeb-Edu classifier and the DCLM classifier, in terms of the high-quality token percentage, and is on-par or slightly better on the 9 evaluation tasks. This is very important since more unique high-quality tokens is the key in pretraining larger LLMs on longer tokens horizons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What</head><p>#Docs Total unique(%) Total unique in union 11,359,655 100.0% In intersection 1,152,821 10.1% In FineWeb-Edu only 4,022,294 35.4% In DCLM only 6,184,540 54.4%</p><p>Table 8: High-quality documents overlap analysis.</p><p>Evaluating Synthetic Data As Table <ref type="table">10</ref> shows, this ablation study aim to answer two questions: (1) Does rephrasing low-quality improve accuracies on downstream tasks? (2) Can synthetic data help offset the decreasing value of duplicated data reported in <ref type="bibr" target="#b27">(Muennighoff et al., 2024</ref>)? To answer these questions, we train four 8B models with the same hyperparameters on different blends of 1T tokens:</p><p>( By comparing the results between LQ-Base and LQ-Synthetic, we can see that rephraing lowquality data leads to 1.50 absolute gains on average score. We also observe noticeable boosts from 1.80% to 4.75% on ARC-Easy, ARC-Challenge, OpenbookQA, CommonsenseQA; however, we also encounter slight accuracy drops on some tasks, which may indicate potential misinformation introduced by data synthesis. Current practices typically utilize data curation approaches to detect and eliminate noisy examples. Due to time and resource constraints, we leave the detailed exploration of this issue for future efforts.</p><p>The comparison between HQ-Base and HQ-Synthetic shows that swapping 4 out of 8 epochs of high-quality data with a mix of synthetic datasets improves accuracy on most benchmarks. This improvement could potentially result from two factors: the incorporation of fresh unique tokens and styles that enable the model to learn specific abili- Table 10: Impact of incorporating synthetic data.</p><p>ties (e.g., question answering) or absorb knowledge more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The Phi series of models pioneered training on small amounts of very high quality data, including curated Web and synthetic data <ref type="bibr" target="#b14">(Gunasekar et al., 2023;</ref><ref type="bibr" target="#b23">Li et al., 2023;</ref><ref type="bibr">Abdin et al., 2024)</ref>. However, their focus is on shorter token horizons and they share limited details. FineWeb-Edu and DCLM are the main points of comparison for our paper <ref type="bibr" target="#b39">(Li et al., 2024;</ref><ref type="bibr" target="#b28">Penedo et al., 2024)</ref>. We build upon their core idea of modelbased filtering, but show how to improve the filtering and data quantity through a combination of other techniques. Other English Common Crawl datasets such as C4, DOLMA, Gopher, Refined-Web, TxT360 largely focus on extraction and nonlearned heuristics <ref type="bibr" target="#b29">(Penedo et al., 2023;</ref><ref type="bibr" target="#b37">Soldaini et al., 2024;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b32">Raffel et al., 2020;</ref><ref type="bibr" target="#b39">Tang et al., 2024)</ref>. Just as for FineWeb-Edu and DCLM, the core pipeline we started from incorporates many of these ideas, but our paper describes how to modify and go beyond these non-learned techniques to achieve state-of-the-art accuracy and diversity. Concurrent work Zyda-2 shows how to filter, cross-deduplicate, and combine the FineWeb-Edu, DCLM, Zyda-1, and Dolma-CC datasets into a higher-accuracy and larger whole <ref type="bibr" target="#b42">(Tokpanov et al., 2024)</ref>. In contrast, we focus on techniques for the creation of a new English Common Crawl dataset rather than combinations or modifications of existing datasets. Finally, many works have focused on creating multilingual datasets <ref type="bibr">(Xue et al., 2021;</ref><ref type="bibr">Brack et al., 2024;</ref><ref type="bibr" target="#b0">Abadji et al., 2022;</ref><ref type="bibr">Wen-zek et al., 2020;</ref><ref type="bibr">Kudugunta et al., 2023)</ref>. We leave extension of our ideas beyond English to the future. Synthetic datasets have been widely used in language model pre-training and post-training. In (Eldan and Li, 2023), the authors show that smaller or simpler models trained on a synthetic dataset of short stories are capable of generating fluent and consistent stories. Similarly, smaller models trained using high-quality synthetic textbook and exercise datasets can achieve impressive high accuracy on coding benchmarks <ref type="bibr" target="#b14">(Gunasekar et al., 2023;</ref><ref type="bibr" target="#b23">Li et al., 2023)</ref>. These approaches typically require a powerful language model, such as GPT-3.5 and GPT-4 in <ref type="bibr" target="#b11">(Eldan and Li, 2023)</ref>, to synthesize new contents. Instead, <ref type="bibr" target="#b24">(Maini et al., 2024)</ref> shows that compact models such as Qwen-1.8B and Mistral-7B are adequate to rephrase web data. This approach generates diverse, high-quality synthetic data that effectively lowers model perplexity and boosts performance across benchmarks. We adopt this main idea, but explore more prompts and show how to specialize them for low and high quality data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>For producing long-horizon pretraining tokens for LLM from English Common Crawl data, we showed how to improve upon the state of the art and achieve better trade-offs between benchmark accuracy and data quantity, as measured by number of unique real tokens. Specifically, we showed the efficacy of ensembling model-based quality filters, rephrasing low and high quality documents, and reducing the reliance on non-learned heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>Some of the key limitations of our work are as follows. For the model-based filter ensembling and quality bucketing, we only had time and resources to try a single strategy. Though it is effective, it is possible this could be improved upon in future work, especially to improve the sensitivity at the higher-quality end of the spectrum. For the rephrased data, we did not verify the factual accuracy or fidelity to the original contents. More work is required to understand the risks of hallucinations or loss of content diversity in this setting and how to mitigate them. We also only looked at rephrasing low and high quality data. It could be interesting to explore how to best rephrase medium quality data as well. We did not do ablations on all parts of the pipeline. There is probably room for improvement with, for example, the language identification. Finally, we tried our methods only on English text. More work is needed to adapt our methods to other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Comparison of FineWeb-Edu and DCLM Classifier</head><p>Different classifiers have different standards for high-quality documents. Thus, ensemble multiple classifiers will help increase the recall of highquality documents. We did a detailed comparison of two of the classifiers that we employ in our method: the FineWeb-Edu classifier which score document quality based on their educational-level, and the DCLM based classifier which value the informativeness of the document.</p><p>We compare the high-quality documents predicted by the two classifiers on one Common Crawl snapshot <ref type="bibr">(dated 2021-21)</ref>. Table <ref type="table">8</ref> show the document statistics comparison. We further show the detailed URL domains comparison between the two classifiers' predictions in Table <ref type="table">11</ref>. We can see that each classifier has their own high-quality domain preferences. Among the top 1k domains, only 368 domains are in the intersection. Therefore, ensemble of different classifiers can help increase retrieving more high-quality documents from Common Crawl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Bucket Comparison</head><p>To better understand the quality of data in each of our 20 data buckets, we carry out ablation studies to test their benchmark accuracies. For each study, we take a 900B-token checkpoint and continue the pretraining for 50B more tokens. For 34% of the 50B tokens we used the bucket data being tested, while we fixed the other 66% as the same data distribution of the 900B pretraining process to make sure the distribution did not shift too much. See Figure <ref type="figure">3</ref> for the results. The average accuracy is calculated across 13 downstream tasks. Note that Bucket 19 greatly outperforms all other buckets and the differences within bucket 12-18 are marginal. We used the results here as a reference when designing the quality labels in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Details</head><p>As mentioned in Section 3.1, we use the open source Megatron-LM library 18 <ref type="bibr" target="#b36">(Shoeybi et al., 2019)</ref> to train 8B parameter transformer LLMs for 1T tokens. The key hyperparameters are as follows: We use 32 transformer layers with hidden dimension 4096, 32 attention heads, and 18 <ref type="url" target="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</ref> 12 13 14 15 16 17 18 19 Bucket index 47.8 48.0 48.2 48.4 48.6 48.8 Average Acc Figure 3: Ablation study on the buckets.</p><p>SwiGLU activations <ref type="bibr" target="#b35">(Shazeer, 2020)</ref>. For the attention, we use grouped query attention with 8 query groups <ref type="bibr" target="#b3">(Ainslie et al., 2023)</ref>. We use the Adam optimizer with β 1 = 0.9, β 2 = 0.95, ϵ = 1e-8, weight decay 0.1, and the cosine learning rate schedule with peak learning rate at 3e-4 and minimum learning rate at 3e-6. A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Common Crawl Snapshots</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Extractor &amp; Filter Ablation</head><p>The Avg tasks include ARC-Easy, ARC-Challenge, Hellaswag, Winogrande, RACE, PIQA, Commonsense QA, Openbook QA.</p><p>Note that we only use FineWeb-Edu classifier for the quality labels of this ablation study and analysis. We do not use it in the final preparation of our dataset. See Section 2.2 for the details of our classifiers being used eventually to prepare the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Prompt Templates</head><p>Prompts 1-5 show the prompt templates we use for synthetic data generation.</p><p>Top domains and domain overlap analysis =&gt;368 domains are in top 1k domains of both Top 1k Domains FineWeb-Edu Top Domains Count DCLM Top Domains Count Intersection (368) In FineWeb-Edu only In DCLM only wordpress.com 39228 wordpress.com 85378 123helpme.com 111papers.com 4archive.org thefreedictionary.com 20420 stackexchange.com 64831 24houranswers.com 3dprint.com 4channel.org stackexchange.com 17853 livejournal.com 36521 abc.net.au aafp.org 4hw.com.cn britannica.com 14761 medium.com 27347 abovetopsecret.com aappublications.org 5winebar.com ipl.org 13132 fandom.com 13986 academickids.com abs.gov.au aawsat.com medium.com 11539 ipl.org 12282 adafruit.com accessgenealogy.com abc11.com nih.gov 10624 answers.com 10790 adobe.com achrnews.com abc30.com igi-global.com 9136 nih.gov 9091 alchetron.com acm.org abc7chicago.com slideplayer.com 8460 typepad.com 8078 aljazeera.com adidasshoesoutletwholesale.com able2know.org answers.com 8103 commonsensemedia.org 7772 allegancountyedc.com adslspeedtest.net aceshowbiz.com wikipedia.org 6867 wsj.com 7652 allinterview.com aero-net.org activerain.com dictionary.com 6763 imdb.com 7263 amazon.com agwired.com addicted2success.com en-academic.com 5292 theatlantic.com 7008 americanbar.org ahdictionary.com additudemag.com sciencemag.org 5254 yahoo.com 5921 angelfire.com ajol.info agingcare.com brainscape.com 5129 fanfiction.net 5499 answers.com akjournals.com agnostic.com encyclopedia.com 4698 huffpost.com 5471 antiessays.com aleteia.org airmilescalculator.com nasa.gov 4615 adobe.com 5182 apple.com alison.com airportia.com slideserve.com 4538 scribd.com 4948 archive.org all-creatures.org alarabiya.net scribd.com 4430 thefreedictionary.com 4847 arduino.cc allaboutheaven.org alex-in-wonderland.com kiddle.co 4323 mathworks.com 4655 arstechnica.com allthatsinteresting.com alexa-gueguen.com</p><p>Table 11: High Quality Documents Domains Comparison. 368 Top Domains are in the intersection.</p><p>Task: Read the text, ask questions and answer them.</p><p>Follow these instructions: 1. Ask diverse questions that require different cognitive skills or cover different aspects of the text.</p><p>2. Ask questions in various forms such as:</p><p>-Yes/No questions that require determining whether a statement is true or false.</p><p>-Open-ended questions that begin with words like what, how, when, where, why and who.</p><p>-Multi-choice questions that offers two or more options to choose from. Include the options in the question.</p><p>-Comparison questions that compare two quantities or objects and determine the relationship between them.</p><p>-Reading comprehension questions that test the ability to understand and analyze the text.</p><p>-Problem-solving questions that test the ability to solve mathematical, physical, or logical problems. 3. Focus on asking questions about factual information, important knowledge, or concrete details in the text. 4. Write questions and answers using clear and concise language. 5. Use plain text. Do not use Markdown. 6. Each question and answer pair should be on a separate line. Tag the question with "Question:" and the answer with "Answer:". Your task is to read and paraphrase the provided text following these instructions: -Aim to create a condensed but accurate and informative version of the original text, not a simplistic summary.</p><p>-Capture and preserve the crucial information, key concepts, important values, and factual details in the original text, while making it more readable and accessible.</p><p>-Retain technical terms, specialized vocabulary, and complex concepts.</p><p>-Retain examples, explanations of reasoning processes, and supporting evidence to maintain the text' s depth and context. -Only include information that is present in the original text. Do not adding new or unsubstantiated claims.</p><p>-Write in plain text.</p><p>Here is the text: [DOCUMENT SEGMENT] Task: After thoroughly reading the above text, paraphrase it in high-quality and clear English following the instructions.</p><p>Prompt 2: Prompt template: Distill.</p><p>Review the text and extract the key information. Follow these instructions: -Carefully read the above text and provide a concise and organized list of factual information, concrete details, key concepts, and important numbers and statistics extracted from the text.</p><p>-Ensure each point is clear, specific, and supported by the original text.</p><p>-Ensure the extract text is information-dense and easier to learn from.</p><p>-Do not add titles or headings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: [DOCUMENT SEGMENT]</head><p>Task: Extract the factual information, concrete details, and key concepts from the above text following the instructions.</p><p>Prompt 3: Prompt template: Knowledge list.</p><p>Your task is to rewrite knowledge from the provided text following these instructions: -Rewrite the text as a passage or passages using easy-to-understand and high-quality English like sentences in textbooks and Wikipedia.</p><p>-Focus on content in disciplines such as humanities, social sciences, natural sciences, technology, engineering, math, law and legal, business, management, art, education, agricultural sciences, politics, and history.</p><p>-Disregard content that does not contain useful facts or knowledge.</p><p>-Retain examples, explanations of reasoning processes, and supporting evidence to maintain the text' s depth and context. -Do not add or alter details. Only restate what is already in the text.</p><p>-Write in plain text.</p><p>-Do not add titles, subtitles, note, or comment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: [DOCUMENT SEGMENT]</head><p>Task: Rewrite facts and knowledge from the above text as a passage or passages following the instructions.</p><p>Prompt 4: Prompt template: Extract knowledge.</p><p>For the following paragraph give me a diverse paraphrase of the same in high quality English language as in sentences on Wikipedia. Begin your answer on a separate line with "Here is a paraphrased version:".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text: [DOCUMENT SEGMENT]</head><p>Prompt 5: Prompt template: Wikipedia-style rephrasing <ref type="bibr" target="#b24">(Maini et al., 2024)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>Figure1: MMLU scores for 8B parameter models trained for 1T tokens. Compared to DCLM, our methods enable us to either create a 4× larger dataset of similar quality or increase the MMLU using a high quality subset of the tokens. Having a larger dataset, in the sense of unique real tokens, is crucial when training over long horizons such as 15T tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Question:</head><figDesc>Which year did the United Nations implement the 2030 agenda for SDGs? Answer: January 1, 2016 Question: What are the three key dimensions of sustainable development covered by the SDGs? Answer: (a) economic growth, (b) social inclusion, and (c) environmental protection Question: Which of the following can flossing prevent? A) Cavities B) Gum disease C) Both A and B D) Neither A nor B Answer: C) Both A and B Question: Is flossing important even if you brush your teeth twice a day? Answer: Yes, flossing is important as it reaches areas that brushing alone cannot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of generated question-answer pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>1) LQ-Base: original Common Crawl data; (2) LQ-Synthetic: an augmented version of LQ-Base where the low-quality documents are rephrased; (3) HQ-Base: a blend containing eightfold high-quality documents and less low-and medium-quality documents; (4) HQ-Synthetic: a variant of HQ-Base where 4 repetitions of the high-quality documents are swapped out for synthetic datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>For</head><figDesc>the main datasets, we used the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30. The thirteen Common Crawl snapshots we use in some of the analysis and 1T token experiments are CC-MAIN-2023-23, CC-MAIN-2023-14, CC-MAIN-2023-06, CC-MAIN-2022-49, CC-MAIN-2022-27, CC-MAIN-2022-05, CC-MAIN-2021-43, CC-MAIN-2021-21, CC-MAIN-2021-04, CC-MAIN-2020-45, CC-MAIN-2020-29, CC-MAIN-2020-05, CC-MAIN-2019-35.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>above text, ask up to 8 questions and provide the correct answers following the instructions. Give your response in this format: Here are the questions and answers based on the provided text: -Question: [first question] Answer: [first answer] -Question: [second question] Answer: [second answer] .... Prompt 1: Prompt template: Diverse QA pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Common Crawl quality labels statistics.</figDesc><table><row><cell>High</cell><cell>19</cell><cell>12.63</cell></row><row><cell>Medium-High</cell><cell>18</cell><cell>11.52</cell></row><row><cell>Medium</cell><cell>12-17</cell><cell>46.24</cell></row><row><cell>Medium-Low</cell><cell>7-11</cell><cell>20.43</cell></row><row><cell>Low</cell><cell>0-6</cell><cell>9.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Synthetic data token count statistics (billion).</figDesc><table><row><cell cols="3">Source #Raw Prompt</cell><cell>#Synthetic</cell></row><row><cell>Low</cell><cell cols="2">403.0 Wikipedia</cell><cell>336.3</cell></row><row><cell></cell><cell></cell><cell>Wikipedia</cell><cell>372.9</cell></row><row><cell></cell><cell></cell><cell>Diverse QA Pairs</cell><cell>499.5</cell></row><row><cell>High</cell><cell>451.3</cell><cell>Distill</cell><cell>157.6</cell></row><row><cell></cell><cell></cell><cell>Extract Knowledge</cell><cell>303.6</cell></row><row><cell></cell><cell></cell><cell>Knowledge List</cell><cell>203.2</cell></row></table><note><p>80% fuzzy duplicates<ref type="bibr" target="#b5">(Ben Allal, 2024;</ref><ref type="bibr" target="#b39">Li et al., 2024)</ref></p><p>. To enable a fairer comparison over relatively short token horizons, we thus also consider a 1.1T token high quality subset of our data (Nemotron-CC-HQ), consisting of just the highest-scoring real and diverse QA pairs synthetic data. The size breakdown of the datasets is shown in Table4</p><p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results for 8B parameter models trained on 1T tokens (73% English Common Crawl from the tested dataset, 27% the same, fixed non-Crawl datasets). The models were evaluated on ARC-Easy, ARC-Challenge, Hellaswag, Winogrande, RACE, PIQA, Social IQA, Commonsense QA, Openbook QA, and MMLU.</figDesc><table><row><cell>Model</cell><cell cols="3">ARC-E ARC-C H</cell><cell>W</cell><cell cols="6">RACE PIQA SIQA CSQA OBQA MMLU Avg</cell></row><row><cell>Llama 3.1</cell><cell>82.4</cell><cell>55.0</cell><cell cols="3">79.3 74.7 39.1</cell><cell>81.2</cell><cell>48.3</cell><cell>70.6</cell><cell>46.0</cell><cell>65.3</cell><cell>64.2</cell></row><row><cell>Ours</cell><cell>82.7</cell><cell>58.1</cell><cell cols="3">80.8 73.8 37.8</cell><cell>81.1</cell><cell>47.4</cell><cell>69.9</cell><cell>45.4</cell><cell>70.3</cell><cell>64.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Different classifiers comparison. Our ensemble method includes the three classifiers: Ours-mistral, Oursnemotron-340B and DCLM.</figDesc><table><row><cell>Classifier</cell><cell></cell><cell cols="5">HQ(%) ARC-E ARC-C H</cell><cell cols="5">W RACE PIQA SIQA CSQA OBQA MMLU Avg</cell></row><row><cell>FineWeb-Edu</cell><cell></cell><cell>8%</cell><cell>77.7</cell><cell></cell><cell cols="3">50.1 74.9 67.3 39.5</cell><cell cols="2">78.8 45.8</cell><cell>53.6</cell><cell>43.0</cell><cell>55.4 59.0</cell></row><row><cell>DCLM</cell><cell></cell><cell>11%</cell><cell>76.0</cell><cell></cell><cell cols="3">49.2 76.5 70.2 38.2</cell><cell cols="2">80.8 33.9</cell><cell>55.2</cell><cell>45.8</cell><cell>56.0 58.4</cell></row><row><cell>Ours-mistral</cell><cell></cell><cell>9%</cell><cell>75.8</cell><cell></cell><cell cols="3">49.2 75.9 66.9 37.5</cell><cell cols="2">80.1 46.2</cell><cell>46.9</cell><cell>44.8</cell><cell>53.2 58.1</cell></row><row><cell cols="2">Ours-nemotron-340B</cell><cell>14%</cell><cell>76.3</cell><cell></cell><cell cols="3">50.3 75.6 67.5 37.8</cell><cell cols="2">80.2 34.3</cell><cell>54.0</cell><cell>46.2</cell><cell>54.9 58.0</cell></row><row><cell>Ours-ensembled</cell><cell></cell><cell>25%</cell><cell>78.0</cell><cell></cell><cell cols="3">49.7 75.3 67.1 37.2</cell><cell cols="2">79.6 45.7</cell><cell>56.8</cell><cell>44.8</cell><cell>56.4 59.4</cell></row><row><cell>Blend</cell><cell cols="3">ARC-E ARC-C</cell><cell>H</cell><cell>W</cell><cell cols="6">RACE PIQA SIQA CSQA OBQA MMLU Avg</cell></row><row><cell>LQ-Base</cell><cell>67.7</cell><cell>41.8</cell><cell cols="3">75.2 67.1</cell><cell>37.4</cell><cell>78.8</cell><cell>45.3</cell><cell>36.9</cell><cell></cell><cell>41.0</cell><cell>48.2</cell><cell>52.5</cell></row><row><cell>LQ-Synthetic</cell><cell>71.3</cell><cell>45.2</cell><cell cols="3">75.0 66.9</cell><cell>37.4</cell><cell>79.4</cell><cell>46.2</cell><cell>41.6</cell><cell></cell><cell>42.8</cell><cell>47.1</cell><cell>54.0</cell></row><row><cell>HQ-Base</cell><cell>74.2</cell><cell>47.7</cell><cell cols="3">74.8 66.9</cell><cell>37.3</cell><cell>78.2</cell><cell>46.0</cell><cell>47.3</cell><cell></cell><cell>43.6</cell><cell>53.4</cell><cell>55.8</cell></row><row><cell>HQ-Synthetic</cell><cell>76.7</cell><cell>49.2</cell><cell cols="3">74.5 67.3</cell><cell>38.2</cell><cell>78.8</cell><cell>45.2</cell><cell>47.9</cell><cell></cell><cell>45.8</cell><cell>53.6</cell><cell>56.7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://data.commoncrawl.org/contrib/Nemotron/ Nemotron-CC/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/NVIDIA/NeMo-Curator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://pypi.org/project/pycld2/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://fasttext.cc/docs/en/languageidentification.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/NVIDIA/NeMo-Curator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/google-research/ deduplicate-text-datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We use the same 460K document samples as in the FineWeb-Edu-Annotation dataset.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://mistral.ai/news/mixtral-8x22b/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>The token limit is set to 512 for Wikiepdia, 2,000 for Distill, 1,400 for Extract Knowledge and 1,000 for Diverse QA Pairs and Knowledge List, including tokens from the prompt and chat format.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>https://mistral.ai/news/mistral-nemo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>https://github.com/NVIDIA/TensorRT-LLM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>https://github.com/NVIDIA/NeMo-Skills</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>https://github.com/NVIDIA/Megatron-LM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>https://github.com/EleutherAI/lm-evaluationharness</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14"><p>Detailed URL domain comparison can be found in Appendix A</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_15"><p>Note that we did not employ FineWeb-Edu classifier in our ensemble for license issue, since it is trained with annotations from Llama3.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="funder">Common Crawl Foundation</rs> for hosting the dataset. We thank <rs type="person">Pedro Ortiz Suarez</rs> for valuable feedback that improved the paper and <rs type="person">Greg Lindahl</rs> for help with improving the data formatting and layout.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards a cleaner documentoriented multilingual crawled corpus</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Abadji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>
		<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4344" to="4355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Marah</forename><surname>Abdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">Ade</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ammar</forename><surname>Ahmad Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Awadalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bahree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Bakhtiari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.14219</idno>
		<title level="m">Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niket</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwath</forename><surname>Aithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallab</forename><surname>Dong H Anh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annika</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Brundyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharon</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Clay</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11704</idno>
		<title level="m">Nemotron-4 340b technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gqa: Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Lebron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Meth-ods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Meth-ods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4895" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Barbaresi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Most of the data is du</title>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allal</forename></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu/discussions/7" />
		<imprint>
			<date type="published" when="2024-10-24">2024. October 24, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7432" to="7439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Brack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Ostendorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Saiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñaki</forename><surname>Lacunza Castilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Palomar-Giner</surname></persName>
		</author>
		<imprint>
			<pubPlace>Alexander Shvets, Patrick</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Community OSCAR: A community effort for multilingual web data</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Rehm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)</title>
		<meeting>the Fourth Workshop on Multilingual Representation Learning (MRL 2024)<address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="232" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Abhimanyu</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiesha</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tinystories: How small can language models be and still speak coherent english</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.07759</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09190</idno>
		<title level="m">Eli5: Long form question answering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Baber Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Le Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviya</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.10256836</idno>
	</analytic>
	<monogr>
		<title level="j">Anish Thite</title>
		<imprint/>
	</monogr>
	<note>and Andy Zou. 2023. A framework for few-shot language model evaluation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caio</forename><surname>César</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teodoro</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allie</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivakanth</forename><surname>Gopi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojan</forename><surname>Javaheripi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>De Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olli</forename><surname>Saarikivi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11644</idno>
		<title level="m">Textbooks are all you need</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
		<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hérve</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">a. Fasttext.zip: Compressing text classification models</title>
		<imprint>
			<biblScope unit="page">2016</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ankur Bapna, and Orhan Firat. 2023. Madlad-400: A multilingual and document-level large audited dataset</title>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Choquette-Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derrick</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romi</forename><surname>Stella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.04662</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deduplicating training data makes language models better</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8424" to="8445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Datacomp-lm: In search of the next generation of training sets for language models</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hritik</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etash</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sedrick</forename><surname>Keh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11794</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.05463</idno>
	</analytic>
	<monogr>
		<title level="m">Textbooks are all you need ii: phi-1.5 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rephrasing the web: A recipe for compute and data-efficient language modeling</title>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Luke</forename><surname>Merrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danmei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.05374</idno>
		<title level="m">Arctic-embed: Scalable, efficient, and accurate text embedding models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scaling data-constrained language models</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06380</idno>
	</analytic>
	<monogr>
		<title level="m">Data, data everywhere: A guide for pretraining dataset construction</title>
		<editor>
			<persName><forename type="first">Jupinder</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joseph</forename><surname>Jennings</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aastha</forename><surname>Jhunjhunwala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhilin</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hynek</forename><surname>Kydlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.17557</idno>
		<title level="m">The fineweb datasets: Decanting the web for the finest text data at scale</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Penedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Malartic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruxandra</forename><surname>Cojocaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Alobeidli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Cappelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Pannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebtesam</forename><surname>Almazrouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="79155" to="79172" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Removing boilerplate and duplicate content from web corpora</title>
		<imprint>
			<date type="published" when="2011-01">Jan Pomikálek. 2011</date>
			<publisher>Masarykova univerzita</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Disertacnı práce</note>
	<note>Fakulta informatiky</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><surname>Ring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Susannah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale. Communications of the</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Social iqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00159</idno>
		<title level="m">Dolma: An open corpus of three trillion tokens for language model pretraining research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<author>
			<persName><forename type="first">Liping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Pangarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cun</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://huggingface.co/spaces/LLM360/TxT360" />
	</analytic>
	<monogr>
		<title level="m">A top-quality llm pre-training dataset requires the perfect blend</title>
		<imprint>
			<date type="published" when="2024-10-24">2024. October 24, 2024</date>
			<biblScope unit="volume">360</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreya</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pier</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Sessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léonard</forename><surname>Hussenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mesnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.00118</idno>
		<title level="m">Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants</title>
		<author>
			<persName><surname>Teknium</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/teknium/OpenHermes-2.5" />
		<imprint>
			<date type="published" when="2023-10-24">2023. October 24, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Zyda-2: a 5 trillion token high-quality dataset</title>
		<author>
			<persName><forename type="first">Yury</forename><surname>Tokpanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Glorioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beren</forename><surname>Millidge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.06068</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-instruct: Aligning language models with self-generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CCNet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10671</idno>
		<title level="m">Qwen2 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
