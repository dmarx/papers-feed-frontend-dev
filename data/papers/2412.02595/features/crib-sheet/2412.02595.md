- **Dataset Overview**: Nemotron-CC is a 6.3T token long-horizon pretraining dataset derived from English Common Crawl, consisting of 4.4T globally deduplicated original tokens and 1.9T synthetically generated tokens.

- **Key Contributions**:
  - Achieves a 5.6 MMLU improvement over DCLM with a 1.1T-token high-quality subset.
  - Matches DCLM performance while containing 4Ã— more unique real tokens.
  - An 8B parameter model trained on 15T tokens from this dataset outperforms Llama 3.1 in multiple benchmarks.

- **Training Methodology**:
  - **Classifier Ensembling**: Utilizes multiple classifiers to improve token selection diversity and quality.
  - **Synthetic Data Generation**: Employs rephrasing techniques to enhance low-quality data and generate diverse high-quality tokens.

- **Quality Labeling Pipeline**:
  - Three classifiers are used to score documents, with an ensemble method to improve recall of high-quality tokens.
  - Quality scores are bucketed into five categories based on downstream task performance.

- **HTML-to-Text Extraction**:
  - Comparison of extractors: Justext yields more tokens and higher quality tokens (+28.6%) compared to Trafilatura.
  - Importance of deduplication: Global fuzzy deduplication and exact substring deduplication are applied to enhance unique token yield.

- **Synthetic Data Generation Techniques**:
  - Low-quality data: Focus on reducing noise and errors through rephrasing.
  - High-quality data: Generates diverse QA pairs, distills information, extracts knowledge, and creates organized knowledge lists.

- **Performance Metrics**:
  - MMLU: 70.3 for the 8B model trained on Nemotron-CC vs. 65.3 for Llama 3.1.
  - ARC-Challenge: +3.1 improvement over Llama 3.1.

- **Guiding Principle**: Shift from static heuristic filtering to a learned pipeline that continuously improves as data quality and model performance enhance.

- **Dataset Availability**: The dataset is publicly accessible at [Nemotron-CC Dataset](https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html).