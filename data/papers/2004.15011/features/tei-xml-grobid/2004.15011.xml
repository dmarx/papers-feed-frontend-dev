<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TLDR: Extreme Summarization of Scientific Documents</title>
				<funder ref="#_smnPBjG #_xaAAU3f">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_HVmRzD5">
					<orgName type="full">ONR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
							<email>isabelc@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI ‡ Paul G</orgName>
								<orgName type="institution" key="instit2">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
							<email>kylel@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI ‡ Paul G</orgName>
								<orgName type="institution" key="instit2">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
							<email>armanc@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI ‡ Paul G</orgName>
								<orgName type="institution" key="instit2">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI ‡ Paul G</orgName>
								<orgName type="institution" key="instit2">Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution" key="instit3">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TLDR: Extreme Summarization of Scientific Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A8EEF4C631282E56E0CEA83C859F354B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce TLDR generation, a new form of extreme summarization, for scientific papers. TLDR generation involves high source compression and requires expert background knowledge and understanding of complex domain-specific language. To facilitate study on this task, we introduce SCITLDR, a new multi-target dataset of 5.4K TLDRs over 3.2K papers. SCITLDR contains both author-written and expert-derived TLDRs, where the latter are collected using a novel annotation protocol that produces high-quality summaries while minimizing annotation burden. We propose CATTS, a simple yet effective learning strategy for generating TLDRs that exploits titles as an auxiliary training signal. CATTS improves upon strong baselines under both automated metrics and human evaluations. Data and code are publicly available at <ref type="url" target="https://github.com/allenai/scitldr">https://github.com/allenai/scitldr</ref>.</p><p>1 TLDR is an acronym that stands for "too long; didn't read," which is often used in online informal discussion (e.g., Twitter or Reddit) about scientific papers. For visual clarity, we omit the semi-colon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We introduce TLDR 1 generation for scientific papers. An alternative to abstracts, TLDRs focus on the key aspects of the paper, such as its main contributions, eschewing nonessential background or methodological details. Given the increasing pace of publication <ref type="bibr" target="#b46">(Van Noorden, 2014)</ref> and resulting difficulty in keeping up with the literature, TLDRs can enable readers to quickly discern a paper's key points and decide whether it's worth reading. The goal of existing work in summarization of scientific documents is to generate abstracts or provide complimentary summaries to abstracts. <ref type="bibr" target="#b8">(Collins et al., 2017;</ref><ref type="bibr" target="#b4">Cohan et al., 2018;</ref><ref type="bibr" target="#b2">Chandrasekaran et al., 2019;</ref><ref type="bibr" target="#b50">Yasunaga et al., 2019)</ref>. In contrast, TLDR Intro <ref type="bibr">[...]</ref> In this work, we present the following contributions: We introduce neural persistence, a novel measure for characterizing the structural complexity of neural networks that can be e ciently computed. [...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>[...] However, this did not yield an early stopping measure because it was never triggered, thereby suggesting that neural persistence captures salient information that would otherwise be hidden among all the weights of a network [...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR</head><p>We develop a new topological complexity measure for deep neural networks and demonstrate that it captures their salient properties.</p><p>1 Figure <ref type="figure">1</ref>: An example TLDR of a scientific paper. A TLDR is typically composed of salient information (indicated by colored spans) found in the abstract, intro, and conclusion sections of a paper. generation seeks to produce an extreme (single sentence) summary <ref type="bibr" target="#b29">(Narayan et al., 2018)</ref> given the entire paper. Further, TLDR generation is a challenging natural language generation task. Writing a TLDR of a scientific paper requires expert background knowledge and understanding of complex domain-specific language to identify the salient aspects of the paper, while maintaining faithfulness to the source and correctness of the written summary. An example TLDR is provided in Figure <ref type="figure">1</ref>.</p><p>To facilitate the study of TLDR generation, we introduce SCITLDR, a new dataset of 5,411 TLDRs of computer science papers. SCITLDR is built from a combination of TLDRs written by authors of submissions on OpenReview 2 and TLDRs derived by a novel annotation protocol that asks domain experts to rewrite peer review comments for that submission. Having multiple gold summaries per paper is especially important for evaluation when there is variability in human-written gold summaries <ref type="bibr" target="#b51">(Zechner, 1996;</ref><ref type="bibr" target="#b15">Harman and Over, 2004)</ref>.</p><p>In addition to establishing strong extractive and abstractive summarization baselines using Transformer-based <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> models, we present CATTS (Controlled Abstraction for TLDRs with Title Scaffolding), a simple yet effective learning strategy for TLDR generation. CATTS incorporates ideas from scaffold tasks for multitask learning <ref type="bibr">(Swayamdipta et al., 2018a;</ref><ref type="bibr" target="#b3">Cohan et al., 2019)</ref> and control codes in conditional language generation <ref type="bibr" target="#b20">(Keskar et al., 2019)</ref> to address the problem of data scarcity in the highly-specialized scientific domain. In particular, CATTS exploits titles as an auxiliary, naturally-occurring training signal by training the model to generate both titles and TLDRs indicated by control codes. We show that CATTS applied to BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>, a state-of-the-art summarization model, results in performance improvement in both automated metrics and human evaluation. Our contributions are summarized below:</p><p>1. We introduce TLDR generation, a new form of extreme summarization, for scientific papers. With extensive analysis of properties of TLDRs, we provide insight into the types of information and amount of variability in human-written TLDRs.</p><p>2. We release SCITLDR, a new multi-target dataset of 5,411 TLDRs over 3,229 scientific papers. SCITLDR contains both author-written and expertderived TLDRs, where the latter are collected using a novel annotation protocol that produces highquality summaries while avoiding the burden of reading the full paper.</p><p>3. We establish strong baselines on SCITLDR and improve them with CATTS, a simple yet effective learning strategy for generating TLDRs that uses titles as an auxiliary training signal.</p><p>4. We perform extensive analysis and human evaluation of system-generated TLDRs, focusing on informativeness and factual correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peer review</head><p>The paper proposes variance regularizing adversarial learning (VRAL), a new method for training GANs. The motivation is to ensure that the gradient for the generator does not vanish. [...] The discriminator itself is trained through two additional meta-discriminators Are the meta-discriminators really necessary? Have you tried matching moments or using other methods [...]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derived TLDR</head><p>The paper proposes variance regularizing adversarial learning for training gans to ensure that the gradient for the generator does not vanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Figure <ref type="figure">2</ref>: Example of a reviewer comment rewritten as a TLDR (best viewed in color). A peer review comment often begins with a summary of the paper which annotators use to compose a TLDR. Annotators are trained to preserve the original reviewer's wording when possible (indicated by colored spans), and to avoid using any excess details or criticism.</p><p>summarization datasets that assume only one gold summary for a given document.</p><p>As evidenced by earlier work in summarization evaluation <ref type="bibr" target="#b6">(Cohan and Goharian, 2016)</ref>, variability in human-written summaries <ref type="bibr" target="#b51">(Zechner, 1996;</ref><ref type="bibr" target="#b15">Harman and Over, 2004)</ref> can negatively impact the reliability of automated summarization metrics like Rouge <ref type="bibr" target="#b24">(Lin, 2004)</ref>. <ref type="foot" target="#foot_1">4</ref> Considering only one gold TLDR for each paper as a basis of automated evaluation might result in inaccurate system quality assessment because content that might appear in a TLDR can have large variability. In addition, having multiple gold summaries for each document enables performing more in-depth analysis and thorough evaluation <ref type="bibr" target="#b30">(Nenkova and Passonneau, 2004)</ref>.</p><p>To address this, SCITLDR contains TLDRs written from the perspective of the author ("TLDR-Auth") and TLDRs written from the perspective of the peer reviewer("TLDR-PR"). We describe these two types of TLDRs in the following paragraphs.</p><p>Collecting TLDR-Auth pairs Scholar-written TLDRs of scientific papers are available on various online platforms. On OpenReview.org, a publicly available scientific reviewing platform, authors submit TLDRs of their papers that summarize the main content for both reviewers and other interested scholars. Scholars also share TLDRs social media platforms, such as Twitter and Reddit.</p><p>We use the OpenReview API<ref type="foot" target="#foot_2">foot_2</ref> to collect pairs of papers and author-written TLDRs, along with the</p><p>Dataset Number of documents Avg. words in document Avg. words in summary Compression ratio % novel words Multi-target Non-scientific documents DUC (Over, 2003) 624 441 11 40.1 30.0 yes NYTimes (Sandhaus, 2008) 655K 549 40 13.7 20.1 no DailyMail (Hermann et al., 2015) 220K 653 55 11.9 17.0 no CNN (Hermann et al., 2015) 93K 760 46 16.5 16.8 no XSUM (Narayan et al., 2018) 226K 431 23 18.7 35.8 no Newsroom (Grusky et al.) 1.32M 659 27 24.4 26.0 no BigPatent (Sharma et al., 2019) 1.34M 3.6K 117 30.5 13.6 no Scientific documents CLPubSum (Collins et al., 2017) 10.3K 8.2K 226 36.5 7.7 no PubMed (Cohan et al., 2018) 133K 3K 203 14.9 10.5 no ArXiv (Cohan et al., 2018) 215K 4.9K 220 22.5 8.3 no SciSummNet † (Yasunaga et al., 2019) 1.0K 4.7K 150 31.2 7.4 no TalkSumm ‡ (Lev et al., 2019) 1.7K 4.8K 965 5.0 16.5 no SCITLDR (ours) 3.2K 5K 21 238.1 15.2 yes Table 1: Comparison of SCITLDR to existing summarization datasets. (i) SCITLDR provides multiple summary targets unlike other recent summarization datasets. (ii) SCITLDR requires both extreme compression and abstraction, as evidenced by the compression ratio and novelty (% of summary words not in the source document), especially when compared with other scientific summarization datasets.</p><p>Rewriting peer reviews into TLDR-PR pairs Scaling up data collection in a specialized scientific domain is costly and challenging. To sidestep this problem, we use a novel annotation protocol that exploits natural summaries in peer review comments. Assuming the typical peer reviewer has carefully scrutinized the source paper and provided a faithful summary in their comment (often in the first paragraph), domain experts can rewrite these comments into TLDRs.</p><p>For this task, we recruit 28 undergraduate computer science students from the University of Washington with self-reported experience in reading scientific papers. Each recruited student received one hour of one-on-one writing training and then was asked to work independently. Annotators were only shown the first 128 words of a sampled<ref type="foot" target="#foot_5">foot_5</ref> peer review comment. They were instructed to keep their TLDRs between 15-25 words (similar to the length of an author written TLDR) and to skip reviews that do not contain a summary or if they did not understand the content. They were also instructed to use the original language in the review, when possible. We manually assessed every written summary, discarding TLDRs that did not adhere to the guidelines, and allowed 20/28 students who performed well to continue work beyond the first hour. Students were compensated at the local median hourly wage of $20 USD per hour. Refer to Appendix §F for full annotation instructions. Figure <ref type="figure">2</ref> contains an example of a peer review and its corresponding TLDR-PR. We discuss differences between TLDR-PR and TLDR-Auth throughout Section 3.</p><p>3 Dataset analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Compression and abstractiveness</head><p>Table <ref type="table">1</ref> compares SCITLDR with other summarization datasets in both scientific and non-scientific domains. We observe that SCITLDR has short summaries, like XSUM and NewsRoom, with long source documents, like BigPatent and the other scientific-domain datasets. This results in a much higher compression ratio compared with existing datasets. Summarization in higher compression settings is challenging as it requires capturing more precisely the salient aspects of the document <ref type="bibr">(Grusky et al.)</ref>.</p><p>Following <ref type="bibr" target="#b29">Narayan et al. (2018)</ref>; Grusky et al., we measure abstractiveness (or novelty) by percentage of words in the summary that do not appear in the source document. We observe that SCITLDR is more abstractive compared with other scientific domain datasets but less abstractive compared with non-scientific domain datasets. We also observe that SCITLDR is smaller in comparison to automatically collected datasets, such as XSUM and ArXiv, but is larger in comparison to other manually collected datasets, such as SciSummNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information content</head><p>We analyze the information content of TLDRs using an approach motivated by the nugget-based summarization evaluation framework of <ref type="bibr" target="#b30">Nenkova and Passonneau (2004)</ref>. In a similar manner, we asked two computer science researchers to read through a collection of TLDRs to both define a comprehensive set of categories of types of information present in TLDRs, which we refer to as nuggets. <ref type="foot" target="#foot_6">8</ref> We also label each TLDR with all represented nuggets. Table 2 presents this categorization, along with example phrases and nugget occurrence frequencies of SCITLDR. For simplicity, we use the category codes defined in the table (with brackets) to reference specific categories.</p><p>Most TLDRs contain between two to four nuggets (never all six), and will provide some indication of their subject area (A) and the paper's contributions (C). In fact, they are the most frequently co-occurring nuggets, appearing in 63% of TLDR-Auth and 71% of TLDR-PR. TLDR-Auth tend to include results or scientific/theoretical findings (R) and often signal the value of their work (V) by describing their contributions as novel or their results as strong or state-of-the-art. In contrast, TLDR-PR focus more on articulating problems the paper addresses (P). Interestingly, TLDR-PR place less emphasis on R and V in favor of further methodological details in the paper D. More details about nuggets in Appendix §A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variability in TLDRs</head><p>To explore variability in our human-written summaries, we examine differences between TLDRs written by authors (TLDR-Auth) and TLDRs derived from the perspective of a peer reviewer (TLDR-PR).</p><p>Lexical variation First, we note that TLDR-Auth are on average 18.9 words long, while TLDR-PR are slightly longer on average at 22.9 words. Despite similarities in length, the 1-, 2-, and 3-gram mean Jaccard indices between TLDR-Auth and TLDR-PR are 15.0%, 2.5%, and 0.7%, respectively, indicating extremely little lexical overlap between the two sources of TLDRs. We can also observe through qualitative examples in Figure <ref type="figure">3</ref> how TLDR-Auth and TLDR-PR can differ greatly, even when they contain the same information content.</p><p>Abstractiveness TLDR-PR is more abstractive with a novelty score of 20.2% compared with TLDR-Auth with a novelty score of 9.6%, where novelty is computed as the percentage of words in the TLDR not in the source paper. This is not unexpected because TLDR-PR are derived from peer review comments which themselves have already gone through one stage of abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-Auth</head><p>The authors propose a framework to learn a good policy through imitation learning from a noisy demonstration set via meta-training a demonstration suitability assessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-PR</head><p>Contributes a maml based algorithm for imitation learning which automatically determines if provided demonstrations are "suitable".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-Auth</head><p>The authors evaluate the effectiveness of having auxiliary discriminative tasks performed on top of statistics of the posterior distribution learned by variational autoencoders to enforce speaker dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-PR</head><p>Propose an autoencoder model to learn a representation for speaker verification using short-duration analysis windows.</p><p>1 Figure <ref type="figure">3</ref>: Two example TLDR-Auth and TLDR-PR pairs with colored spans corresponding to nuggets in Table <ref type="table" target="#tab_2">3</ref> -A, P, C, D. On top, we see TLDRs can have substantial lexical variation despite covering similar information content. On bottom, we naturally see even more variation when the information content differs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CATTS</head><p>We introduce CATTS (Controlled Abstraction for TLDRs with Title Scaffolding), a simple yet effective method for learning to generate TLDRs. Our approach addresses two main challenges: (1) the limited size of the training data and (2) the need for domain knowledge in order to write high-quality gold TLDRs. To address these challenges, we propose using titles of scientific papers as additional generation targets. As titles often contain key information about a paper, we hypothesize that training a model to generate titles will allow it to learn how to locate salient information in the paper that will be also useful for generating TLDRs. In addition, all papers have a title, and thus we have an abundant supply of paper-title pairs for training.</p><p>Incorporating auxiliary scaffold tasks via multitask learning has been studied before for improving span-labeling and text classification <ref type="bibr">(Swayamdipta et al., 2018b;</ref><ref type="bibr" target="#b3">Cohan et al., 2019)</ref>. Similar to multitask learning, training on heterogenous data annotated with control codes has been shown to improve controlled generation in autoregressive language models <ref type="bibr" target="#b20">(Keskar et al., 2019;</ref><ref type="bibr" target="#b12">ElSahar et al., 2020;</ref><ref type="bibr" target="#b41">Sudhakar et al., 2019;</ref><ref type="bibr" target="#b23">Li et al., 2020)</ref>. In fact, it has been shown effective for generating biomedical abstracts <ref type="bibr" target="#b44">(Sybrandt and Safro, 2020)</ref>. We demonstrate that control codes can be used to effectively incorporate scaffold tasks (e.g. title generation) for denoising autoencoders like BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>.</p><p>In order to use title generation as a scaffold task for TLDR generation, we propose shuffling SCITLDR with a title generation dataset, then appending each source with control codes |TLDR| and |TITLE| , respectively. This allows the parameters of the model to learn to generate both TLDRs and titles. This process is visualized in Frigure 4. At generation time, the appropriate control code is appended to the source. Additionally, upsampling particular tasks can be viewed as applying task-specific weights, similar to weighting losses in multitask learning setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We establish baselines for TLDR generation on SCITLDR using state-of-the-art extractive and abstractive summarization models.</p><p>Extractive methods We consider both unsupervised and supervised extractive methods. For our unsupervised baseline, we use PACSUM <ref type="bibr" target="#b54">(Zheng and Lapata, 2019)</ref>, an extension of TextRank (Mihalcea and Tarau, 2004) that uses BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> as a sentence encoder. For our supervised baselines, we use BERTSUMEXT <ref type="bibr" target="#b25">(Liu and Lapata, 2019)</ref>, which uses BERT as a sentence encoder augmented with inter-sentence Transformer layers to capture interactions, and Match-Sum <ref type="bibr" target="#b55">(Zhong et al., 2020)</ref>, which uses a BERT Siamese network to score whole summaries.</p><p>Abstractive methods Since TLDRs often contain information spread across multiple sentences, we expect abstractive summarization methods to produce strong results for this task. We focus on BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>, a Transformer-based denoising autoencoder for pretraining sequenceto-sequence models. We use BART-large, which achieves state-of-the-art results in summarization on XSUM. We additionally use BART-large finetuned on XSUM, hypothesizing that the task of extreme summarization of news articles might transfer to TLDR generation on SCITLDR.</p><p>Oracle We define a sentence-level extractive oracle: Given a paper and its multiple gold TLDRs, it selects the single sentence in the document with the highest Rouge overlap for each gold TLDR. Then it returns the single sentence that yields the maximum Rouge across all gold TLDRs. This sets an upperbound on the performance of the sentence-level extractive methods under our multi-target evaluation (Section 5.4). Our full text oracle achieves 54.5 Rouge-1, 30.6 Rouge-2, and 45.0 Rouge-L on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Input space</head><p>The input space is the context provided to the model when generating TLDRs.</p><p>Abstract-only Since the vast majority of scientific papers do not have open-access full text <ref type="bibr" target="#b27">(Lo et al., 2020)</ref>, it is worth considering the setting in which we generate TLDRs for papers given only their abstracts as input. The average length of an abstract is 159 words and resulting compression ratio is 7.6.</p><p>AIC Previous studies have found that the most salient information in a paper for writing a summary is often found in the abstract, introduction, and conclusion (AIC) sections <ref type="bibr" target="#b40">(Sharma et al., 2019)</ref>. An important consequence of this is the ability to substantially reduce computational costs<ref type="foot" target="#foot_7">foot_7</ref>  <ref type="bibr" target="#b39">(Schwartz et al., 2019)</ref> by supplying only these sections as context. The average combined length of these contexts is 993 words and resulting compression ratio is 47.3, which is still higher than other datasets surveyed in Table <ref type="table">1</ref>.</p><p>Comparing oracle results in Table <ref type="table" target="#tab_2">3</ref>, we see that increasing the input space from abstract-only to AIC improves Rouge-1 by +4.7. Yet, this is only 2.1 Rouge-1 lower than the full text oracle performance, despite requiring five times more text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training and implementation details</head><p>All experiments use Titan V or V100 GPUs. We experiment on abstract-only and AIC input spaces. Best hyperparameters for the models are selected based on dev set Rouge-1. Supervised models like BERTSUMEXT and BART are trained on SCITLDR and the best model checkpoint chosen using dev set loss. See Appendix §D for additional parameter tuning details of all models.</p><p>Extractive Methods For PACSUM, BERT-SUMEXT and MatchSum we use original code released by the authors. The first two use BERT-base and the last one uses RoBERTa-base <ref type="bibr">(Liu et al., 2019)</ref>. For MatchSum in AIC input space, following the authors, we use BERTSUMEXT to first extract 7 highly scoring sentences as the input to MatchSum. 10 Sentence segmentation is performed using ScispaCy <ref type="bibr" target="#b31">(Neumann et al., 2019)</ref>, and models select a single sentence as their predictions. We use the default hyperparameters for PACSUM.</p><p>Abstractive Methods We experiment with BART-large and BART-large finetuned on XSUM, using the Fairseq <ref type="bibr" target="#b33">(Ott et al., 2019)</ref> implementation and the released XSUM weights. We apply the CATTS training method to these two models, using an additional 20K paper-title pairs from arXiv for title generation. 11 We up-sample TLDR instances to match the size of the title scaffold data. 12 For simplicity, we refer to these as BART, BART XSUM , CATTS and CATTS XSUM , respectively. For all models, we use a learning rate of 3e-5, update frequency of 1, and max tokens per batch of 1024 13 chosen through manual tuning. We tune decoder for all models via grid search over five length penalties between 0.2 and 1.0 and 7 beam sizes 2 to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation</head><p>Automated evaluation Following recent work on extreme summarization <ref type="bibr" target="#b29">(Narayan et al., 2018;</ref><ref type="bibr" target="#b22">Lewis et al., 2020)</ref>, we use Rouge-1, Rouge-2, and Rouge-L <ref type="bibr" target="#b24">(Lin, 2004)</ref> as our automated metrics. As discussed in Section 2, we have multiple target summaries available per paper. To exploit this during evaluation, we calculate the Rouge score of the system-generated TLDR with respect to each of the gold TLDRs for the corresponding paper (including its TLDR-Auth and all of its TLDRs-PR) individually. We take the maximum Rouge score over these gold TLDRs as the final Rouge score for that paper. An alternative approach to aggregating scores would be to take the mean, but due to the 10 In abstract-only setting, MatchSum takes the full context. 11 Includes all papers on arXiv with at least one of the following tags CS.CL, CS.CV, CS.LG, CS.AI, CS.NE, and STAT.ML and have identified introduction and conclusion sections by S2ORC <ref type="bibr" target="#b27">(Lo et al., 2020)</ref>.</p><p>12 While this up-sampling may indicate that CATTS is training on more TLDRs than BART, we allow BART training up to 20 epochs and it quickly overfits within a few epochs. 13 Fairseq reports an "average batch size" of 36, which is a consequence of adaptive batching of examples based on the update frequency and max tokens per batch. Human evaluation While our multi-target setting allows us to mitigate some of the limitations of Rouge <ref type="bibr" target="#b10">(Conroy et al., 2011;</ref><ref type="bibr" target="#b6">Cohan and Goharian, 2016)</ref>, we acknowledge that relying only on automated metrics is insufficient for evaluating the quality of the models. In addition to automated metrics, we also have human experts in computer science assess system-generated TLDRs under two criteria -informativeness and correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>For informativeness, we perform the nuggetbased analysis for information content over systemgenerated TLDRs for the same 76 gold papers from Section 3.2. We use the presence (or lack) of different nuggets in predicted and gold TLDRs to quantify differences in information content. Specifically, we score each gold and system-generated TLDR by the number of unique nuggets divided by the number of tokens. This length normalization handles cases where systems returning the source document are trivially more informative. For each paper, we rank the predicted and gold TLDRs. Then, we compute overall metrics for each gold or system variant by aggregating their ranks across papers using mean reciprocal rank (MRR).</p><p>Evaluating correctness requires careful reading and understanding the source paper. To minimize this burden and have reliable evaluation, we ask the original authors of papers to assess the correctness of our system-generated TLDRs. We manually email (first or second) authors of arXiv papers and ask them to score each system-generated TLDR 14 For completeness we provide mean Rouge scores in Appendix Table <ref type="table">10</ref> to supplement our main max Rouge results in Table 3. MRR Avg. # nuggets Avg. # words TLDR-Auth (Gold) 0.53 2.5 20.5 TLDR-PR (Gold) 0.60 2.4 18.7 BARTXSUM 0.42 2.2 19.4 CATTSXSUM 0.54 2.6 20.8</p><p>Table <ref type="table">4</ref>: Human evaluation on informativeness of gold and system-generated TLDRs. Higher MRR corresponds to variants that, on average, rank higher than others by length-normalized number of nuggets.</p><p>with 1 -false or misleading, 2 -partially accurate or 3 -mostly correct, regardless of comprehensiveness. We compare the mean correctness (across papers) for each system variant. We received responses from 29 unique authors with annotations covering 64 arXiv papers.</p><p>6 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitative results</head><p>We present our main results in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extractive results</head><p>We establish baseline results for extractive methods on our new dataset SCITLDR. We observe that MatchSum has the highest extractive performance, followed by BERT-SUMEXT. We observe that increasing input space from abstract-only to AIC greatly improves PAC-SUM<ref type="foot" target="#foot_8">foot_8</ref> performance but decreases performance of both BERTSUMEXT and MatchSum. We suspect that increasing the input space makes it more difficult for these models to learn optimal parameters including new position embeddings in low-resource training. Compared to the extractive oracle scores, we see there is plenty of room for improvement. Table 6: Oracle input space experiments. ∆ are differences between oracle result and model's best performance (across abstract-only and AIC) from Table 3.</p><p>Abstractive results Abstractive methods are not limited to choosing exact sentences. For a given abstractive baseline BART or BART XSUM , our CATTS learning strategy results in improvements in both abstract-only and AIC settings. Comparing CATTS variants with their corresponding BART baselines, we observe that in the abstract-only setting, CATTS and CATTS XSUM achieve +0.5 and +1.8 Rouge-1, respectively. In the AIC setting, CATTS and CATTS XSUM achieve +2.0 and +0.9 Rouge-1, respectively. We use the two-sided paired t-test against a null hypothesis of no difference to assess these differences. To address the issue of multiple hypothesis testing over Rouge scores, we perform a Holm-Bonferroni <ref type="bibr" target="#b17">(Holm, 1979)</ref>  16 correction for determining significant p-values in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Human evaluation</head><p>We perform our human evaluation on BART XSUM and CATTS XSUM using the AIC input space on 51 sampled papers. In this setting, we have both chosen the strongest baseline and controlled for XSUM pretraining. From Table <ref type="table">4</ref>, we see that CATTS XSUM is more informative than BART XSUM and is comparable to gold TLDR-Auth, though still less informative than TLDR-PR.</p><p>In addition to informativeness, we also evaluate content accuracy of generated tldrs as explained in Section 5.4. We report no difference in correctness between BART XSUM and CATTS XSUM . We observe 42 ties, 10 cases where BART XSUM is more correct, and 12 cases where CATTS XSUM is more 16 Using the P.ADJUST library in R (R Core <ref type="bibr">Team, 2018)</ref> correct. Both models average a rating of 2.5 (scoring between partially accurate and mostly correct).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>How abstractive are the generations? From Table <ref type="table" target="#tab_4">5</ref>, we observe: (1) BART variants are less abstractive than CATTS variants. (2) Initial training on XSUM might influence models to be slightly less abstractive. (3) BART variants are more abstractive in the abstract-only setting than the longer AIC settings, while CATTS seems to have the same level of abstractiveness regardless of input space.</p><p>How long are the generations? From Table <ref type="table" target="#tab_4">5</ref>, we see the systems all generate TLDRs of similar length to the average length reported in Table <ref type="table">1</ref>.</p><p>How important is using the full text? To analyze whether one can improve abstractive model performance by improving the input space selection (compared to just using AIC), we define an oracle input space. That is, for each TLDR, we select sentences from the full text that maximize Rouge-1 with the gold TLDRs-Auth<ref type="foot" target="#foot_9">foot_9</ref> and select the top sentences to match the length of AIC. Repeating the experiments in Section 5 with this input source, we observe some performance improvement across models (Table <ref type="table">6</ref>).</p><p>Qualitative example Table <ref type="table">7</ref> contains system generations on the same paper (alongside the gold TLDRs). Curiously, despite both achieving the same Rouge-1, the generated TLDRs are quite different. BART XSUM focuses on the methodological contribution while CATTS XSUM focuses on a scientific finding. The "two hidden layer" detail by BART XSUM is from the paper introduction and the "defining the appropriate sampling distributions" from CATTS XSUM is from the conclusion.<ref type="foot" target="#foot_10">foot_10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Transformers for summarization Transformerbased models have achieved strong results in extractive and abstractive summarization. PACSUM <ref type="bibr" target="#b54">(Zheng and Lapata, 2019)</ref> combines BERT sentence representation with unsupervised text ranking; MatchSum <ref type="bibr" target="#b55">(Zhong et al., 2020)</ref> uses a Siamese BERT model to score the entire summary instead of a single extraction; and Liu and Lapata (2019)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-Auth</head><p>We propose a method for the construction of arbitrarily deep infinite-width networks, based on which we derive a novel weight initialisation scheme for finite-width networks and demonstrate its competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TLDR-PR</head><p>Proposes a weight initialization approach to enable infinitely deep and infinite-width networks with experimental results on small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARTXSUM</head><p>We propose a principled approach to weight initialisation that allows the construction of infinite-width networks with more than two hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CATTSXSUM</head><p>We study the initialisation requirements of infinite-width networks and show that the main challenge for constructing them is defining the appropriate sampling distributions for the weights.</p><p>Table 7: Examples of system generations. BART XSUM and CATTS XSUM both achieve Rouge-1 of 40.7 on this paper. Colored spans indicate text overlap.</p><p>show that BERT is effective for both extractive and abstractive summarization. <ref type="bibr" target="#b53">Zhang et al. (2019)</ref>; <ref type="bibr" target="#b1">Bi et al. (2020)</ref> introduce new pretraining objectives that improve generation. Sequence-to-sequence models <ref type="bibr" target="#b37">(Raffel et al., 2020;</ref><ref type="bibr" target="#b22">Lewis et al., 2020;</ref><ref type="bibr" target="#b0">Bao et al., 2020)</ref> have state-of-the-art performance on XSUM <ref type="bibr" target="#b29">(Narayan et al., 2018)</ref>, a dataset for extreme summarization dataset of news articles. SCITLDR is a new form of extreme summarization focused on scientific papers.</p><p>Scientific document summarization Most work in summarization of scientific papers have focused on longer summaries (i.e. 150-200 words). Existing datasets include CSPubSum for extractive summarization <ref type="bibr" target="#b8">(Collins et al., 2017)</ref>, ArXiv and PubMed for abstract generation <ref type="bibr" target="#b4">(Cohan et al., 2018)</ref>, and SciSummNet <ref type="bibr" target="#b50">(Yasunaga et al., 2019)</ref> and CL-SciSumm <ref type="bibr" target="#b18">(Jaidka et al., 2018;</ref><ref type="bibr" target="#b2">Chandrasekaran et al., 2019)</ref> datasets, which incorporate citation contexts into human-written summaries. TalkSumm <ref type="bibr" target="#b21">(Lev et al., 2019)</ref> uses recordings of conference talks to create a distantly-supervised training set for the CL-SciSumm task.</p><p>Modeling approaches in scientific document summarization include models that exploit citation contexts <ref type="bibr" target="#b35">(Qazvinian et al., 2013;</ref><ref type="bibr">Cohan and</ref><ref type="bibr">Goharian, 2015, 2017;</ref><ref type="bibr" target="#b52">Zerva et al., 2020)</ref>, automated survey generation <ref type="bibr" target="#b28">(Mohammad et al., 2009;</ref><ref type="bibr" target="#b19">Jha et al., 2015;</ref><ref type="bibr" target="#b13">Fabbri et al., 2018;</ref><ref type="bibr" target="#b48">Wang et al., 2018)</ref>, and other techniques focusing on exploiting the unique properties of scientific documents such as long length and structure <ref type="bibr" target="#b9">(Conroy and Davis, 2017;</ref><ref type="bibr" target="#b32">Nikolov et al., 2018;</ref><ref type="bibr" target="#b4">Cohan et al., 2018;</ref><ref type="bibr" target="#b49">Xiao and Carenini, 2019)</ref>. Yet, such methods have not been studied in the setting of extreme summarization (i.e. short target summaries, high compression, high abstraction), and SCITLDR is the first dataset to facilitate such research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduce TLDR generation for scientific papers, and release SCITLDR, a multi-target dataset of TLDR-paper pairs. We also present CATTS, a simple yet effective learning strategy for improving TLDR generation that exploits auxiliary training signal from paper titles. We show that our approach improves over strong modeling baselines.</p><p>Existing methods for scientific document summarization often make use of properties unique to those papers, like sections, citation contexts or scientific discourse roles. Future work can examine how best to incorporate these properties to improve TLDR generation models. Additionally, while our experiments are limited to abstract-only and AIC input spaces, we provide the full text of the source papers to support research into using longer input contexts. Furthermore, the multiple target summaries in SCITLDR reflect diverse perspectives and can be used to support summarization research into training and evaluation techniques previously unavailable with existing datasets. Finally, the idea of a TLDR can differ between academic disciplines, and we leave such exploration open for future work.</p><p>A How many nuggets in TLDRs? # categories 0 1 2 3</p><p>TLDR-Auth 2.6% 10.5% 26.3% 34.2% TLDR-PR 0.0% 9.2% 30.3% 31.6% # categories 4 5 6 TLDR-Auth 18.4% 7.9% 0.0% TLDR-PR 26.3% 2.6% 0.0% Table 9: Breakdown of venues represented by papers in SCITLDR C Background knowledge for TLDRs What a paper's TLDR looks like or what information it should include is subjective and follows <ref type="bibr">(community-specific)</ref> commonsense rather than any formally-defined procedure. Since TLDRs are inherently ultra-short, they are not necessarily selfcontained statements, and understanding them requires background expertise within their respective scientific domain. Therefore, when designing SCITLDR, we assume readers have sufficient background knowledge to follow a general research topic in a given domain. This eliminates the need for TLDRs to include explanations or clarifications of common domain-specific terms (e.g., "bounds," "LSTM," or "teacher").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional model training details</head><p>PACSUM The default hyperparameters are beta and lambda1 set to 0. We did some initial tuning of the hyperparameters using the provided tuning code, which performs a search over 10 beta values and 10 lambda1 values. This did not result in a significant difference in performance. PACSUM had a total runtime of 12 minutes on abstracts and 6.5 hours on AIC. We used the released code by authors. 19</p><p>19 <ref type="url" target="https://github.com/mswellhao/PacSum">https://github.com/mswellhao/PacSum</ref> </p><p>BERTSUMEXT We trained with a batch size of 1 sentence per batch and for 5,000 total steps for a total training time of 30 min. We use a learning rate of 2e-3 and a dropout rate of 0.1, which are the reported parameters used for XSUM. BERT-SumExt also requires a max token length for initializing position embeddings. For the abstract-only setting, we use the default number of max tokens 512, which fits the full length of all of abstracts in SCITLDR. For AIC, we first attempted 3 different truncation lengths -1024 (double the max tokens for abstracts), 1500 (90th percentile length), and 1800 (95th percentile length) tokens. We found that truncation at 1500 performs best on AIC. We used the released code by authors.<ref type="foot" target="#foot_11">foot_11</ref> </p><p>MatchSum We trained MatchSum with a batch size of 32, learning rate of 2e-5 with a linear warmup and decay scheduler, and trained the model for 15 epochs. We chose the best checkpoint based on linear combination of Rouge-1, Rouge-2 and Rouge-L. We manually tuned hyperparameters -For learning rate, we tried 2e-5 and 3e-5 and for number of epochs, we tried 5, 15, and 20. For AIC, as MatchSum requires few salient sentences as input for candidate generation, we used BERT-SUMEXT to score sentences and chose the top 7 ones as input to MatchSum. This is according to instructions by authors<ref type="foot" target="#foot_12">foot_12</ref> . Instead of training the model from scratch we used the authors released checkpoint based on the CNN/DM dataset. This resulted in about 1 Rouge-1 point improvement.</p><p>BART For BART and BART XSUM finetuning experiments, we train all the models for 500 steps with 20% warm-up for an approximate training time of 45 minutes. This is equivalent to 5 epochs, though we initially allowed BART to train for up to 20 epochs and found that the model quickly overfits to the training set (as evidenced by poor performance on the dev set).</p><p>Through manual tuning, we achieved the best results by reducing the training time. Also in manual tuning, we first ran the experiments on four learning rates, 2e-5, 3e-5, 4e-5, and 5e-5 and controlled for all other hyperparameters. We then tested three different seeds, again controlling for all other parameters. Finally, we tested two batch sizes, 2048 tokens per batch and 1024 tokens per batch. CATTS In the abstract-only setting, we train CATTS for 11,000 total steps for a total training time of 2.5 hours. For AIC, we train CATTS for 45,000 total steps for a total training time of 10 hours. This also equivalent to 5 epochs of training. We do not perform tuning on the training hyperparameters for CATTS, instead opting to use the same parameters as the baseline BART models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Mean ROUGE test results</head><p>Abstract-only AIC Method R1 R2 RL R1 R2 RL BART 31.1 10.7 24.4 30.7 10.6 24.4 BARTXSUM 30.1 10.7 24.1 31.0 10.9 24.7 CATTS 31.5 11.0 24.9 † 31.9 † 11.8 † 25.6 CATTSXSUM † 31.7 11.1 † 25.0 † 32.1 † 11.6 † 25.4</p><p>Table 10: Test set results using mean Rouge scores instead of max for abstractive methods. We use † to indicate CATTS variants that significantly (p &lt; 0.05) outperform their corresponding BART baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F TLDR-PR annotation instructions</head><p>Below are the instructions provided to annotators rewriting peer-review comments.</p><p>Task: We want to collect a dataset of short summaries of CS papers, but it's hard to get people to read and write summaries about entire papers. Instead, we collected a dataset of peer reviewer comments, in which many CS researchers have read and written reviews of papers. Often, a reviewer's comments will also include a summary of the paper they've read. Our task is given the title and first 128 words of a reviewer comment about a paper, re-write the summary (if it exists) into a single sentence or an incomplete phrase. Summaries must be no more than one sentence. Most summaries are between 15 and 25 words. The average rewritten summary is 20 words long.</p><p>What might be included in your re-write?</p><p>1. What subfield is their work in? 2. What problem are they trying to solve? 3. What did the paper do? 4. Why should you care/how is it novel?</p><p>What to exclude when re-writing a comment: Not everything in the reviewer comment belongs in the summary. We purposefully leave out:</p><p>• Reviewer decisions/opinions (accept, reject, suggestions, etc.)</p><p>-"The paper is well-written and it is quite easy to follow along with the discussion."</p><p>• Background information/ previous work -"The authors propose a method for learning node representations which, like previous work (e.g. node2vec, DeepWalk), is based on the skip-gram model." -"In particular, when node2vec has its restart probability set pretty high, the random walks tend to stay within the local neighborhood (near the starting node)."</p><p>• Excessive details about methodology -"Whereas node2vec may sample walks that have context windows containing the same node, the proposed method does not as it uses a random permutation of..."</p><p>Enter "None" for the summary for the following conditions:</p><p>• The comment is entirely the reviewer's opinions about the paper</p><p>• The reviewer's summary carries heavy sentiment about the paper -"This paper presents a method that is not novel or interesting" -This applies when the sentiment is so heavy that you are unable to write a summary.</p><p>• If the comment is about a paper that is out of your domain of expertise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training regimen for CATTS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Example categories (or nuggets) of information a TLDR might contain. Proportion of TLDRs containing each nugget estimated on 76 randomly sampled gold papers (each with its TLDR-Auth and a sampled</figDesc><table><row><cell>Category</cell><cell>Example phrase</cell><cell>% of TLDRs AUTH / PR</cell></row><row><cell>[A]rea, field or topic of study</cell><cell>reinforcement learning, dependency parsing</cell><cell>85.6 / 90.8</cell></row><row><cell>[P]roblem or motivation</cell><cell>mode collapse, catastrophic forgetting</cell><cell>29.0 / 32.9</cell></row><row><cell>Mode of [C]ontribution</cell><cell>method, dataset, proof, theorem</cell><cell>68.4 / 76.3</cell></row><row><cell>[D]etails or description</cell><cell>graph convolution ically computed graphs operations with dynam-</cell><cell>43.4 / 57.9</cell></row><row><cell></cell><cell>improved performance</cell><cell></cell></row><row><cell>[R]esults or findings</cell><cell>on ImageNet, simple defenses work on</cell><cell>29.0 / 17.1</cell></row><row><cell></cell><cell>MNIST but not CIFAR</cell><cell></cell></row><row><cell>[V]alue or significance</cell><cell>novel, state-of-the-art, easily applicable simple yet effective,</cell><cell>23.7 / 7.9</cell></row></table><note><p>TLDR-PR). Percentages do not sum to one because each TLDR can contain multiple nuggets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test set max Rouge scores of extractive and abstractive baselines and CATTS. We use † to indicate CATTS variants that significantly (p&lt;0.05) outperform their corresponding BART baseline.variability in TLDRs shown in Section 3.3, we argue the maximum operation is more appropriate -That is, matching any of the gold TLDRs is rewarded.14   </figDesc><table><row><cell>-only</cell><cell>AIC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Lexical features of system-generated TLDRs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AIC</cell><cell></cell></row><row><cell>Method</cell><cell>% novel words</cell><cell cols="2">Avg. # words</cell><cell>% novel words</cell><cell>Avg. # words</cell><cell></cell></row><row><cell>BART</cell><cell>2.9%</cell><cell></cell><cell>20.9</cell><cell>1.3%</cell><cell>20.4</cell><cell></cell></row><row><cell>BARTXSUM</cell><cell>3.7%</cell><cell></cell><cell>18.4</cell><cell>1.1%</cell><cell>18.9</cell><cell></cell></row><row><cell>CATTS</cell><cell>5.5%</cell><cell></cell><cell>19.1</cell><cell>5.3%</cell><cell>18.4</cell><cell></cell></row><row><cell>CATTSXSUM</cell><cell>5.8%</cell><cell></cell><cell>19.7</cell><cell>4.5%</cell><cell>19.7</cell><cell></cell></row><row><cell>Method</cell><cell>R1</cell><cell>∆</cell><cell>R2</cell><cell>∆</cell><cell>RL</cell><cell>∆</cell></row><row><cell>BART</cell><cell cols="6">44.9 +1.6 22.6 +1.8 37.1 +2.1</cell></row><row><cell cols="7">BARTXSUM 44.8 +1.1 21.8 +0.4 36.4 +0.4</cell></row><row><cell>CATTS</cell><cell cols="3">44.9 +0.0 21.9</cell><cell cols="2">-0.7 36.6</cell><cell>-0.7</cell></row><row><cell cols="7">CATTSXSUM 45.7 +1.1 23.0 +1.7 37.1 +1.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Number of categories represented in a TLDR</figDesc><table><row><cell cols="2">B Breakdown of venues in SCITLDR?</cell></row><row><cell>Venue</cell><cell>Proportion</cell></row><row><cell>ICLR</cell><cell>85.2%</cell></row><row><cell cols="2">NeurIPS/NIPS 5.8%</cell></row><row><cell>OpenReview</cell><cell>2.1%</cell></row><row><cell>ICML</cell><cell>2.0%</cell></row><row><cell>ICAPS</cell><cell>1.8%</cell></row><row><cell>other</cell><cell>3.1%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://openreview.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>While Rouge is capable of handling multiple targets for a given document, most summarization datasets are single target. See Table1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://github.com/openreview/openreview-py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>† SciScummNet data was later included in the CL-SciSumm shared task and dataset<ref type="bibr" target="#b18">(Jaidka et al., 2018;</ref><ref type="bibr" target="#b2">Chandrasekaran et al., 2019)</ref>, which has an additional 40 manually annotated documents and its statistics are similar to SciSummNet.‡ Unlike the other summarization datasets presented here, TalkSumm is an automatically-constructed dataset for training; the TalkSumm-supervised model in<ref type="bibr" target="#b21">Lev et al. (2019)</ref> was evaluated using CL-SciSumm<ref type="bibr" target="#b18">(Jaidka et al., 2018)</ref>.full-text PDFs 6 of those papers. We use the S2ORC pipeline<ref type="bibr" target="#b27">(Lo et al., 2020)</ref> to convert PDFs to structured, machine-readable full text. We then split the papers randomly into the previously-mentioned train, dev, and test sets; each paper at this point has an associated author-written gold TLDR.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>A small fraction of those papers (&lt; 5%) did not have an available PDF file, so we could not parse their full body text. This are still included the dataset as it is possible to generate a TLDR from an abstract alone.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Multiple peer review comments can be available for each paper on OpenReview. We focused on ensuring that each paper in dev and test had at least one TLDR-PR.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>While we adopt the term 'nugget" for convenience, we recognize that that they traditionally correspond to factoids, while here they correspond to discourse roles<ref type="bibr" target="#b45">Teufel (1999)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Especially for methods that rely on O(n 2 ) inter-sentence comparisons or wrappers around Transformer-based methods to long contexts.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_8"><p>PACSUM using the full text yields a Rouge-1 of 12.7, significantly worse than abstract-only.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_9"><p>Only TLDRs-Auth is exists for all papers. TLDRs-PR are only in dev and test.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_10"><p>See original paper: https://openreview.net/pdf?id=SkGT6sRcFX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_11"><p>https://github.com/nlpyang/PreSumm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_12"><p>https://github.com/maszhongming/ MatchSum</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the <rs type="grantName">Semantic Scholar Research</rs> team and <rs type="person">John Bohannon</rs> and <rs type="person">Oleg Vasilyev</rs> from Primer for helpful feedback and discussions. This work was supported in part by <rs type="funder">NSF</rs> Convergence Accelerator award 1936940, <rs type="funder">NSF</rs> <rs type="grantName">RAPID award</rs> <rs type="grantNumber">2040196</rs>, <rs type="funder">ONR</rs> grant <rs type="grantNumber">N00014-18-1-2193</rs>, and the <rs type="institution">University of Washington WRF/Cable Professorship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_smnPBjG">
					<orgName type="grant-name">Semantic Scholar Research</orgName>
				</org>
				<org type="funding" xml:id="_xaAAU3f">
					<idno type="grant-number">2040196</idno>
					<orgName type="grant-name">RAPID award</orgName>
				</org>
				<org type="funding" xml:id="_HVmRzD5">
					<idno type="grant-number">N00014-18-1-2193</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unilmv2: Pseudo-masked language models for unified language model pre-training</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiulei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno>ArXiv, abs/2002.12804</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Palm: Pre-training an autoencoding and autoregressive language model for contextconditioned generation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.07159</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Overview and results: Cl-scisumm shared task 2019</title>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Kumar Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NLP for Digital Libraries (BIRNDL)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1361</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A discourse-aware attention model for abstractive summarization of long documents</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scientific article summarization using citation-context and article&apos;s discourse structure</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Revisiting summarization evaluation for scientific articles</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
		<idno>ArXiv, abs/1604.00400</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scientific document summarization via citation contextualization and scientific discourse</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Goharian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="287" to="303" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A supervised approach to extractive summarisation of scientific papers</title>
		<author>
			<persName><forename type="first">Ed</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>CoNLL, abs/1706.03946</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Section mixture models for scientific document summarization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashka</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDL</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="305" to="322" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nouveau-rouge: A novelty metric for update summarization</title>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">D</forename><surname>John M Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianne P O'</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName><surname>Leary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised and controlled multi-document opinion summarization</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Gallé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.14754</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TutorialBank: A manually-collected corpus for prerequisite chains, survey extraction and resource recommendation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prawat</forename><surname>Trairatvorakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitai</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1057</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Caitlin Westerfield, and Dragomir Radev</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1065</idno>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The effects of human variation in DUC summarization evaluation</title>
		<author>
			<persName><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName><forename type="first">Sture</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Insights from clscisumm 2016: the faceted scientific document summarization shared task</title>
		<author>
			<persName><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthu</forename><surname>Kumar Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajal</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDL</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="163" to="171" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surveyor: A system for generating coherent survey articles for scientific topics</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reed</forename><surname>Coke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">CTRL: A Conditional Transformer Language Model for Controllable Generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Talksumm: A dataset and scalable annotation method for scientific paper summarization based on conference talks</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achiya</forename><surname>Jerbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Konopnicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<publisher>ACL</publisher>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conditional augmentation for aspect term extraction via masked sequence-tosequence generation</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<idno>ArXiv, abs/2004.14769</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">S2orc: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Rada Mihalcea and Paul Tarau</title>
		<meeting>ACL. Rada Mihalcea and Paul Tarau</meeting>
		<imprint>
			<date type="published" when="2004">2020. 2004</date>
		</imprint>
	</monogr>
	<note>Textrank: Bringing order into texts EMNLP</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Using citations to generate surveys of scientific paradigms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahed</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Zajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1206</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scispacy: Fast and robust models for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data-driven summarization of scientific articles</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nikola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">H R</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><surname>Hahnloser</surname></persName>
		</author>
		<idno>ArXiv, abs/1804.08875</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">fairseq: a fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<publisher>Demonstrations</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An introduction to duc 2003: Intrinsic evaluation of generic news text summarization systems</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Over</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Document Understanding Conference</title>
		<meeting>Document Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating extractive summaries of scientific paradigms</title>
		<author>
			<persName><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesun</forename><surname>Whidby</surname></persName>
		</author>
		<author>
			<persName><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<orgName type="collaboration">R Core Team</orgName>
		</author>
		<title level="m">R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The new york times annotated corpus</title>
		<author>
			<persName><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-10">2008. october 2008. 2008t19</date>
			<publisher>Ldc</publisher>
			<biblScope unit="volume">ldc catalog no.</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Green ai</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">transforming&quot; delete, retrieve, generate approach for controlled text style transfer</title>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Sudhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Maheswaran</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1322</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3269" to="3279" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">2018a. Syntactic scaffolds for semantic structures</title>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2018b. Syntactic scaffolds for semantic structures</title>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cbag: Conditional biomedical abstract generation</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sybrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Safro</surname></persName>
		</author>
		<idno>ArXiv, abs/2002.05637</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Argumentative zoning information extraction from scientific text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Global scientific output doubles every nine years</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Van Noorden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature news blog</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Citationas: A tool of automatic survey generation based on citation content</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanhong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="37" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extractive summarization of long documents by combining global and local context</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">Richard</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast generation of abstracts from general domain text corpora by extracting relevant sentences</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational</title>
		<meeting>the 16th conference on Computational</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="986" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cited text span identification for scientific summarisation using pretrained encoders</title>
		<author>
			<persName><forename type="first">Chrysoula</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Quoc</forename><surname>Nghiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nhung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1912.08777</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Xipeng Qiu, and Xuanjing Huang</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extractive summarization as text matching. ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
