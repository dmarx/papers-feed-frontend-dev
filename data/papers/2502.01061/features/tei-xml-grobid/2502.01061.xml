<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-13">13 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Liang Bytedance</surname></persName>
						</author>
						<title level="a" type="main">OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-13">13 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">C91D5FA1D26941E249CCFB88DE5B4B38</idno>
					<idno type="arXiv">arXiv:2502.01061v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://omnihuman-lab.github.io/">https://omnihuman-lab.github.io/</ref> Figure <ref type="figure">1</ref>. The video frames generated by OmniHuman based on input audio and image. The generated results feature head and gesture movements, as well as facial expressions, that match the audio. OmniHuman generates highly realistic videos with any aspect ratio and body proportion, and significantly improves gesture generation and object interaction over existing methods, due to the data scaling up enabled by omni-conditions training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage datadriven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the emergence of the Diffusion Transformer-based (DiT) video diffusion models, the field of general video generation, including Text-to-Video and Image-to-Video <ref type="bibr">[3-6, 16, 17, 22, 33, 35, 49, 60, 63, 66, 82]</ref> has made significant progress in producing highly realistic video content. A key factor driving this advancement is the large-scale training data, typically formatted as video-text pairs. Expanding the training dataset enables DiT networks to learn motion priors for various objects and scenes, resulting in strong generalization capabilities during inference.</p><p>Building upon these pretrained video diffusion networks, end-to-end human animation models, either for pose-driven human animation or audio-driven talking human generation, have developed rapidly since last year <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b69">71]</ref>. Despite achieving realistic results, these models are trained on highly filtered datasets to simplify the learning process, restricting their applicability to limited scenarios. For instance, most existing end-to-end audio-conditioned models are limited to facial or portrait animation, while most pose-conditioned models can only handle full-body images captured from a front-facing perspective with a static background. To date, no prior work has attempted to scale up training data for more generalizable human animation.</p><p>Scaling up human animation data may seem straightforward, but unfortunately it is not. Directly adding more data is not always beneficial for network training. Take audioconditioned models as an example: audio is primarily associated with facial expressions and has little correlation with body poses, background motion, camera movement, or lighting changes. As a result, raw training data must be filtered and cropped to minimize the influence of these unrelated factors. Additionally, audio-conditioned models often undergo further data cleaning based on lip-sync accuracy, which is also important to stabilize training. Similarly, pose-conditioned models require extensive filtering, cropping, and cleaning. Unfortunately, these processes discard a substantial amount of data, making dataset scaling a futile effort, despite the fact that much of the discarded data contains valuable motion patterns essential for training data expansion.</p><p>In this paper, we address the challenges of scaling up human animation data and models. Our key insight is that incorporating multiple conditioning signals, such as text, audio, and pose, during training can significantly reduce data wastage. This approach offers two main advantages. On one hand, data that would otherwise be discarded for singlecondition models (e.g., audio-or pose-conditioned) can be leveraged in tasks with weaker or more general conditions, such as text conditioning. Training on such data allows the model to learn more diverse motion patterns, mitigating the limitations imposed by data filtering. On the other hand, different conditioning signals can complement each other. For example, while audio alone cannot precisely control body poses, stronger conditions such as pose inputs can provide additional guidance. By integrating stronger conditioning signals alongside audio data during training, we aim to reduce overfitting and improve the generalization of generated results.</p><p>Based on the above considerations, we designed the omniconditions training strategy, which follows two proposed training principles: (1) stronger conditioned tasks can leverage weaker conditioned tasks and their corresponding data to achieve data scaling up during the model training process, and (2) the stronger the condition, the lower the training ratio that should be used. To implement this strategy, we built a mixed conditioned human video generation model named OmniHuman, based on the advanced video generation model architecture, DiT <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">42]</ref>. OmniHuman can train with three motion-related conditions (text, audio, and pose) from weak to strong. This approach addresses the data scaling up challenge in end-to-end frameworks, allowing the model to benefit from large-scale data training, learn natural motion patterns, and support various input forms.</p><p>Overall, our contributions can be summarized as follows: 1. We propose the OmniHuman model, a mixed-conditioned human video generation model. It leverages our omniconditions training strategy to integrate various motionrelated conditions and their corresponding data. Unlike existing methods that reduce data due to stringent filtering, our approach benefits from large-scale mixed conditioned data. 2. OmniHuman generates highly realistic and vivid human motion videos, supporting multiple modalities simultaneously. It performs well with different portrait and input aspect ratios. OmniHuman significantly improves gesture generation, a challenge for previous end-to-end models, and supports various image styles, significantly outperforming existing audio-conditioned human video generation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Generation</head><p>In recent years, the advent of technologies such as diffusion models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51]</ref> has propelled the capabilities of generative models to a practically usable level. The latest advancements in image generation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> produce results that are almost indistinguishable from reality. Consequently, a growing number of studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b74">76,</ref><ref type="bibr" target="#b80">82]</ref> are shifting their focus toward the field of video generation.</p><p>Early text-to-video works primarily centered on training-free adaptations of pre-trained text-to-image models <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b66">68]</ref> or integrated temporal layers with fine-tuning on limited video datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b80">82]</ref>. However, due to the lack of extensive data, the video generation quality of these methods often remains unsatisfactory. To better exploit scaling laws and push the boundaries of video generation models, recent works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b71">73]</ref> have optimized in three major areas. First, they have collected larger-scale, high-quality video datasets, with the data volume increasing to (O(100M)) clips of high-resolution videos. Second, they employ 3D Causal VAE [75] to compress both spatial and temporal features of video data, thereby enhancing video modeling efficiency. Third, the foundational model structure has transitioned from UNet to Transformer, improving the model's scalability. Additionally, these works utilize meticulously designed progressive training recipes and datasets to maximize the model's potential. For example, [31, 43] first pre-train on a large volume of low-resolution images and videos, leveraging data diversity to enhance the model's generalization capabilities.</p><p>They then perform fine-tuning on a subset of high-resolution, high-quality data to improve the visual quality of generated videos. Large-scale data has significantly improved the effectiveness of general video generation. However, progress in the field of human animation synthesis remains relatively slow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Human Animation</head><p>As an important task of video generation, Human Animation synthesizes human videos using human images and driving conditions such as audios or videos. Early GANbased methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b77">79]</ref> typically employ small datasets <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b81">83]</ref> consisting of tens of thousands of videos to achieve video-driven in a self-supervised manner. With the advancement of Diffusion models, several related works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b83">85]</ref> have surpassed GANbased methods in performance while using datasets of similar scale. Instead of using pixel-level videos, these methods employ 2D skeleton, 3D depth, or 3D mesh sequences as driving conditions. Audio-driven methods used to focus on portrait <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b75">77,</ref><ref type="bibr" target="#b79">81]</ref>. Despite some efforts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b53">55]</ref> to extend the frame to the full body, there are still challanges especially in hand quality.</p><p>To bypass it, most approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b53">55]</ref> adopt a twostage hybrid driving strategy, utilizing gesture sequences as a strong condition to assist hand generation. CyberHost <ref type="bibr" target="#b32">[34]</ref> attempts to achieve one-stage audio-driven talking body generation through codebook design. Most notably, existing Human Animation methods typically focus on limited-scale datasets and limited-complexity structure, generally less than a thousand hours and 2B. Although FADA <ref type="bibr" target="#b79">[81]</ref> employs a semi-supervised data strategy to utilize 1.4K hours of portrait videos, VLogger <ref type="bibr" target="#b9">[10]</ref> meticulously collects 2.2K hours of half-body videos, and Hallo3 <ref type="bibr" target="#b10">[11]</ref> initializes its weights derived from CogVideoX5B-I2V <ref type="bibr" target="#b70">[72]</ref>, their performance does not exhibit the scaling law trends observed in other tasks such as LLMs <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b56">58]</ref>, VLMs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">37]</ref>, and T2I/T2V <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">32]</ref>. Scaling effects in Human Animation haven't been investigated effectively yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce our framework, OmniHuman, which employs motion-related condition mixing during network training to scale up the training data. First, we provide an overview of the framework, including its inputs, outputs and key design elements. Next, we focus on the omni-conditions design, covering audio, pose, and reference conditions. We then detail the training strategy of OmniHuman, which leverages these omni-conditions for mixed data training, enabling the model to learn natural motion from large-scale datasets. Finally, we describe the implementation details for the inference phases of the OmniHuman model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, our approach consists of two primary parts: the OmniHuman model, a multi-condition diffusion model and the Omni-Conditions Training Strategy.</p><p>For model, The OmniHuman model begins with a pretrained Seaweed model <ref type="bibr" target="#b33">[35]</ref>, which uses MMDiT <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b40">42]</ref> and is initially trained on general text-video pairs for text-to-video and text-to-image tasks. Given a reference image, the OmniHuman model aims to generate human videos using one or more driving signals including text, audio and pose. To achieve this, we employ various strategies to integrate frame-level audio features and pose heatmap features into the Omni-Human model. The detailed procedure is explained in the following subsections. OmniHuman model utilizes a causal 3DVAE <ref type="bibr" target="#b78">[80]</ref> to project videos at their native size <ref type="bibr" target="#b11">[12]</ref> into a latent space and employs flow matching <ref type="bibr" target="#b34">[36]</ref> as the training objective to learn the video denoising process. We employ a three-stage mixed condition post-training approach to progressively transform the diffusion model from a general text-to-video model to a multi-condition human video generation model. As depicted on the left of Figure <ref type="figure" target="#fig_0">2</ref>, these stages sequentially introduce the driving modalities of text, audio, and pose according to their motion correlation strength, from weak to strong, and balance their training ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Omni-Conditions Designs</head><p>Driving Conditions. We adopted different approaches for injecting audio and pose conditions. Regarding audio condition, the wav2vec <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">45]</ref> model is employed to extract acoustic features, which are subsequently compressed using Appearance Conditions. The goal of OmniHuman is to generate video outputs that preserve both the subject's identity and the background details from a reference image. To achieve this, previous research has proposed various strategies for injecting appearance representations into the denoising process. The most widely adopted approach involves using a reference network <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b52">54]</ref>, a parallel, trainable copy of the entire diffusion UNet or DiT that integrates with the self-attention layers of the original denoising Net. While effective at transferring appearance features to the denoising process, this method requires duplicating a full set of trainable parameters, which presents scalability challenges as model size increases. To overcome this challenge, OmniHuman introduces a simple yet effective strategy for reference conditioning. Instead of constructing additional network modules, we reuse the original denoising DiT backbone to encode the reference image. Specifically, the reference image is first encoded into a latent representation using a VAE, and both the reference and noisy video latents are flattened into token sequences. These sequences are then packed together and simultaneously fed into the DiT, enabling the reference and video tokens to interact via self-attention across the entire network. To help the network distinguish between reference and video tokens, we modify the 3D Rotational Position Embeddings (RoPE) <ref type="bibr" target="#b51">[53]</ref> in the DiT by zeroing the temporal component for reference tokens, while leaving the RoPE for video tokens unchanged. This approach effectively incorporates appearance conditioning without adding extra parameters. In addition to the reference image, to support long video generation, we draw on previous methods by using motion frames <ref type="bibr" target="#b50">[52]</ref>, concatenating their features with the noise features.</p><p>After introducing these conditions, the motion-related conditions now include text, reference image, audio, and pose. Text describes the current event, the reference image defines the range of motion, audio determines the rhythm of co-speech gestures, and pose specifies the exact motion. Their correlation strength with human motions can be considered to decrease in this order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scaling up with Omni-Conditions Training</head><p>Thanks to the multi-condition design, we can divide the model training into multiple tasks, including image and text to video, image and text, audio to video, and image and text, audio, pose to video. During training, different modalities are activated for different data, allowing a broader range of data to participate in the training process and enhancing the model's generation capabilities. After the conventional textto-video pretraining phase, we follow two training principles for scaling up the conditioned human video generation task. Principle 1, stronger conditioned tasks can leverage weaker conditioned tasks and their corresponding data to achieve data scaling up during the model training process. Data excluded from audio and pose conditioned tasks due to filtering criteria like lip-sync accuracy, pose visibility, and stability can be used in text and image conditioned tasks, as they meet the standards for weaker conditions. Therefore, in the first stage 1, we drop the audio and pose conditions. Principle 2, the stronger the condition, the lower the training ratio that should be used. During training, stronger motion-related conditions, such as pose, generally train better than weaker conditions like audio due to less ambiguity. When both conditions are present, the model tends to rely on the stronger condition for motion generation, preventing the weaker condition from learning effectively. Therefore, we ensure that weaker conditions have a higher training ratio than stronger conditions. We construct stage 2 to drop only the pose condition, and in the final stage 3, use all conditions. Additionally, the training ratios for text, reference, audio, and pose are progressively halved. This approach assigns higher gradient weights to more challenging tasks and prevents overfitting to a single condition during overlapping condition training. Principle 1 allows us to significantly expand the training data, while Principle 2 ensures that the model fully utilizes the advantages of each motion-related condition during mixed conditions training and learns their motion generation capabilities. By combining Principles 1 and 2, OmniHuman can effectively train with mixed conditioned data, benefiting from data scaling up and achieving satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Inference Strategies</head><p>For audio-driven scenarios, all conditions except pose are activated. For pose-related combinations, all conditions are activated, but for pose-only driving, audio is disabled. Generally, when a condition is activated, all conditions with a lower motion-related influence are also activated unless unnecessary. During inference, to balance expressiveness and computational efficiency, we apply classifier-free guidance (CFG) <ref type="bibr" target="#b19">[20]</ref> specifically to audio and text across multiple conditions. However, we observed that an increased CFG results in pronounced wrinkles on the characters, whereas a decreased CFG compromises lip synchronization and motion expressiveness. To mitigate these issues, we propose a CFG annealing strategy that progressively reduces the CFG magnitude throughout the inference process, thereby significantly minimizing the appearance of wrinkles while ensuring that expressiveness. OmniHuman is capable of producing video segments of arbitrary length within memory constraints based on the provided reference images and various driving signals. To ensure temporal coherence and identity consistency in long videos, the last five frames of the previous segment are utilized as motion frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset. By filtering based on aesthetics, image quality, motion amplitude, etc. (common criteria for video generation), we obtained 18.7K hours of human-related data for training. Of this, 13% was selected using lipsync and pose visibility criteria, enabling audio and pose modalities. During training, the data composition was adjusted to fit the omni-condition training strategy. For testing, we conduct the evaluation following the portrait animation method Loopy <ref type="bibr" target="#b25">[26]</ref> and the half-body animation method CyberHost <ref type="bibr" target="#b32">[34]</ref>. We randomly sampled 100 videos from public portrait datasets, including CelebV-HQ <ref type="bibr" target="#b81">[83]</ref> (a diverse dataset with mixed scenes) and RAVDESS <ref type="bibr" target="#b27">[28]</ref> (an indoor dataset including speech and song) as the testset for portrait animation. For half-body animation, we used CyberHost's test set, which includes a total of 269 body videos with 119 identities, encompassing different races, ages, genders, and initial poses.</p><p>Baselines. To comprehensively evaluate OmniHuman's performance in different scenarios, we compare against portrait animation baselines including Sadtalker <ref type="bibr" target="#b75">[77]</ref>, Hallo <ref type="bibr" target="#b68">[70]</ref>, Vexpress <ref type="bibr" target="#b60">[62]</ref>, EchoMimic <ref type="bibr" target="#b7">[8]</ref>, Loopy <ref type="bibr" target="#b25">[26]</ref>, Hallo-3 <ref type="bibr" target="#b10">[11]</ref>, and body animation baselines including DiffTED <ref type="bibr" target="#b22">[23]</ref>, DiffGest <ref type="bibr" target="#b82">[84]</ref> + Mimiction <ref type="bibr" target="#b76">[78]</ref>, CyberHost <ref type="bibr" target="#b32">[34]</ref>.</p><p>Metrics. For visual quality, FID <ref type="bibr" target="#b18">[19]</ref> and FVD <ref type="bibr" target="#b57">[59]</ref> are used to evaluate the distance between the generated and labeled images and videos. We also leverage q-align <ref type="bibr" target="#b65">[67]</ref>, a VLM to evaluate the no-reference IQA(image quality) and ASE(aesthetics). For lip synchronism, we employ the widely-used Sync-C <ref type="bibr" target="#b8">[9]</ref> to calculate the confidence between visual and audio content. Besides, HKC (hand keypoint confidence) <ref type="bibr" target="#b32">[34]</ref> and HKV (hand keypoint variance) <ref type="bibr" target="#b32">[34]</ref> are employed, to represent hand quality and motion richness respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with Existing Methods</head><p>As shown in the Table <ref type="table" target="#tab_1">1</ref> and <ref type="table" target="#tab_2">2</ref>, overall, OmniHuman demonstrates superior performance compared to leading specialized models in both portrait and body animation tasks using a single model. For audio-driven animation, the generated results cannot be identical to the original video, especially when the reference image contains only a head. The model's </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies on Omni-Conditions Training</head><p>Here, we primarily analyze and explain principles 1 and 2 of the omni-condition training in OmniHuman. For the first principle, we compare training using only data that meets the requirements for audio and pose animation (i.e., 100% audio training ratio) with training data for weaker conditions (i.e., text). Our experimental results demonstrate that the ratio of these two data parts significantly affects the final performance. From the visualizations in Figure <ref type="figure" target="#fig_1">3</ref>, it is evident that a high proportion of audio condition-specific data training reduces dynamic range and can cause failures with complex input images. Including weaker condition data at a 50% ratio yields satisfactory results (e.g., accurate lip-syncing and natural motion). However, excessive weaker condition data can hinder training, resulting in poorer correlation with the audio. We also conducted a subjective evaluation to determine the optimal mix of these two data types during training. Specifically, we conducted a blind evaluation with 20 subjects who compared the samples across various dimensions to select the most satisfactory one, with an option for abstention. In total, 50 samples depicting diverse scenarios were evaluated. The results in Table <ref type="table" target="#tab_3">3</ref> were consistent with the conclusions drawn from the visualizations.</p><p>The second principle can also be simultaneously validated with the principle 1 experiment, but we additionally conduct another experiment using different ratios of pose conditions to study the effects of pose condition ratios. Visual comparisons are presented in Figure <ref type="figure" target="#fig_2">4</ref> and <ref type="figure">5</ref>. When the model is trained with a low pose condition ratio and tested with only audio conditions, the model tends to generate intense, frequent co-speech gestures, as is proven by the motion blur effects in the top row of Figure <ref type="figure">5</ref> and the incorrect fingers in the top row of Figure <ref type="figure" target="#fig_2">4</ref>. On the other hand, if we train the model with a high pose ratio, the model tends to rely on the pose condition to determine the human poses in the generated video. Consequently, given the input audio as the only driving signal, the generated results typically maintain a similar pose, as shown in the bottom rows of Figure <ref type="figure" target="#fig_2">4</ref> and <ref type="figure">5</ref>. Therefore, we set the pose ratio to 50% as our final training configuration.</p><p>Apart from analyzing the training ratios of new driving modalities in Stage 2 and Stage 3, the training ratio of the appearance condtion is equally important. We investigated the impact of reference image ratios on the generation of 30-second videos through two experiments: (1) setting the reference image ratio to 70%, lower than the text injection ratio but higher than audio; (2) setting the reference image ratio to 30%, lower than the injection ratios for both audio and text. The comparative results are shown in Figure <ref type="figure">6</ref>, revealing that a lower reference ratio leads to more pronounced error accumulation, characterized by increased noise and color shifts in the background, degrading performance. In contrast, a higher reference ratio ensures better alignment of the generated output with the quality and details of the original image. This can be explained by the fact that when the reference image training ratio is lower than that of audio, the audio dominates the video generation, making it difficult to maintain the ID information from the reference image.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Extended Visual Results</head><p>In the Figure <ref type="figure" target="#fig_4">7</ref> and Figure <ref type="figure" target="#fig_5">8</ref>, we present more visual results to demonstrate OmniHuman's powerful capabilities in human animation, which are difficult to capture through metrics and comparisons with existing methods. OmniHuman is compatible with diverse input images and maintains the motion style of the input, such as preserving the characteristic mouth movements in anime. OmniHuman also excels in object interaction, generating videos of singing while playing different musical instruments and natural gestures while holding objects. Due to its compatibility with pose conditions during training, OmniHuman can perform pose-driven video generation or a combination of pose and audio-driven generation. More video samples can be seen on our project page (highly recommended).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose OmniHuman, an end-to-end multimodalityconditioned human video generation framework that generates human videos based on a single image and motion signals (e.g., audio, video, or both). OmniHuman employs a mixed data training strategy with multimodality motion conditioning, leveraging the scalability of mixed data to overcome the scarcity of high-quality data faced by previous methods. It significantly outperforms existing approaches, producing highly realistic human videos from weak signals, especially audio. OmniHuman supports images of any aspect ratio (portraits, half-body, or full-body) delivering lifelike, high-quality results across various scenarios. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The framework of OmniHuman. It consists of two parts: (1) the OmniHuman model, which is based on the DiT architecture and supports simultaneous conditioning with multiple modalities including text, image, audio, and pose; (2) the omni-conditions training strategy, which employs progressive, multi-stage training based on the motion-related extent of the conditions. The mixed condition training allows the OmniHuman model to benefit from the scaling up of mixed data.</figDesc><graphic coords="4,58.50,72.00,494.99,224.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Ablation study on different audio condition ratios. The models are trained with different audio ratios (top: 10%, middle: 50%, bottom: 100%) and tested in an audio-driven setting with the same input image and audio.</figDesc><graphic coords="7,60.92,443.70,121.76,66.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.</figDesc><graphic coords="8,83.25,343.55,445.50,335.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Ablation study on different pose condition ratios. The models are trained with different pose ratios (top: 20%, middle: 50%, bottom: 80%) and tested in an audio-driven setting with the same input image and audio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The videos generated by OmniHuman based on input audio and images. OmniHuman is compatible with stylized humanoid and 2D cartoon characters, and can even animate non-human images in an anthropomorphic manner.</figDesc><graphic coords="9,83.25,249.69,445.49,411.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The videos generated by OmniHuman based on input audio and images. These demonstrates OmniHuman's compatibility with various environments, objects, and camera angles, producing satisfactory results.</figDesc><graphic coords="14,83.25,72.03,445.47,607.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="1,58.50,228.40,495.00,293.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with audio-conditioned portrait animation baselines.</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell>CelebV-HQ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RAVDESS</cell><cell></cell></row><row><cell></cell><cell cols="3">IQA ↑ ASE↑ Sync-C↑</cell><cell>FID↓</cell><cell>FVD↓</cell><cell cols="3">IQA ↑ ASE↑ Sync-C↑</cell><cell>FID↓</cell><cell>FVD↓</cell></row><row><cell>SadTalker [77]</cell><cell>2.953</cell><cell>1.812</cell><cell>3.843</cell><cell cols="3">36.648 171.848 3.840</cell><cell>2.277</cell><cell>4.304</cell><cell cols="2">32.343 22.516</cell></row><row><cell>Hallo [70]</cell><cell>3.505</cell><cell>2.262</cell><cell>4.130</cell><cell>35.961</cell><cell>53.992</cell><cell>4.393</cell><cell>2.688</cell><cell>4.062</cell><cell cols="2">19.826 38.471</cell></row><row><cell>VExpress [61]</cell><cell>2.946</cell><cell>1.901</cell><cell>3.547</cell><cell cols="3">65.098 117.868 3.690</cell><cell>2.331</cell><cell>5.001</cell><cell cols="2">26.736 62.388</cell></row><row><cell cols="2">EchoMimic [8] 3.307</cell><cell>2.128</cell><cell>3.136</cell><cell>35.373</cell><cell>54.715</cell><cell>4.504</cell><cell>2.742</cell><cell>3.292</cell><cell cols="2">21.058 54.115</cell></row><row><cell>Loopy [26]</cell><cell>3.780</cell><cell>2.492</cell><cell>4.849</cell><cell>33.204</cell><cell>49.153</cell><cell>4.506</cell><cell>2.658</cell><cell>4.814</cell><cell cols="2">17.017 16.134</cell></row><row><cell>Hallo-3 [11]</cell><cell>3.451</cell><cell>2.257</cell><cell>3.933</cell><cell>38.481</cell><cell>42.125</cell><cell>4.006</cell><cell>2.462</cell><cell>4.448</cell><cell cols="2">28.840 26.029</cell></row><row><cell>OmniHuman</cell><cell>3.875</cell><cell>2.656</cell><cell>5.199</cell><cell>31.435</cell><cell>46.393</cell><cell>4.564</cell><cell>2.815</cell><cell>5.255</cell><cell cols="2">16.970 15.906</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons with audio-conditioned body animation baselines.</figDesc><table><row><cell>Methods</cell><cell cols="3">IQA ↑ ASE↑ Sync-C↑</cell><cell>FID↓</cell><cell cols="3">FVD↓ HKV ↑ HKC↑</cell></row><row><cell>DiffTED [23]</cell><cell>2.701</cell><cell>1.703</cell><cell>0.926</cell><cell cols="2">95.455 58.871</cell><cell>-</cell><cell>0.769</cell></row><row><cell cols="2">DiffGest. [84]+MomicMo. [78] 4.041</cell><cell>2.897</cell><cell>0.496</cell><cell cols="3">58.953 66.785 23.409</cell><cell>0.833</cell></row><row><cell>CyberHost [34]</cell><cell>3.990</cell><cell>2.884</cell><cell>6.627</cell><cell cols="3">32.972 28.003 24.733</cell><cell>0.884</cell></row><row><cell>OmniHuman</cell><cell>4.142</cell><cell>3.024</cell><cell>7.443</cell><cell cols="3">31.641 27.031 47.561</cell><cell>0.898</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Subjective comparison of different training ratios for audio conditions.</figDesc><table><row><cell>Methods</cell><cell cols="5">Identity Consistency Lip-sync Accuracy Visual Quality Action Diversity Overall</cell></row><row><cell>10% Audio Training Ratio</cell><cell>28.84</cell><cell>11.59</cell><cell>21.59</cell><cell>11.59</cell><cell>11.59</cell></row><row><cell>50% Audio Training Ratio</cell><cell>50.87</cell><cell>53.62</cell><cell>44.93</cell><cell>40.58</cell><cell>69.57</cell></row><row><cell>100% Audio Training Ratio</cell><cell>11.59</cell><cell>30.43</cell><cell>13.04</cell><cell>36.23</cell><cell>17.93</cell></row><row><cell cols="2">varying preferences for motion styles across different sce-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">narios complicate performance measurement using a single</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">metric. By averaging the metrics across the dataset, Omni-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Human achieves the best results across all evaluated metrics,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">reflecting its overall effectiveness. Additionally, OmniHu-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">man excels across almost all metrics in specific datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Notably, existing methods use a single model for specific</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">body proportions (portrait, half-body) with fixed input sizes</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and ratios. In contrast, OmniHuman supports various in-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">put sizes, ratios and body proportions with a single model,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">achieving satisfactory results. This advantage stems from its</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">omni-conditions training, which learns from a large scale of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">diverse content and varying sizes during mixed data training.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Ceyuan Yang</rs>, <rs type="person">Zhijie Lin</rs>, <rs type="person">Yang Zhao</rs>, and <rs type="person">Lu Jiang</rs> for their discussions and suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12966</idno>
		<title level="m">Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2023" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roni</forename><surname>Paiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiran</forename><surname>Zada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12945</idno>
		<title level="m">A space-time diffusion model for video generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Align your latents: High-resolution video synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22563" to="22575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><surname>Karras</surname></persName>
		</author>
		<title level="m">Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31769" to="31781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pixart-delta: Fast and controllable image generation with latent consistency models</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayak</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.08136</idno>
		<imprint>
			<date type="published" when="2006">2024. 2, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2016 Workshops: ACCV 2016 International Workshops</title>
		<title level="s">Revised Selected Papers</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">November 20-24, 2016. 2017</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="251" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Gabriel Bazavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thiemo</forename><surname>Alldieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><surname>Vlogger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08764</idno>
		<title level="m">Multimodal diffusion for embodied avatar synthesis</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.00733</idno>
		<imprint>
			<date type="published" when="2006">2024. 3, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Patch n&apos;pack: Navit, a vision transformer for any aspect ratio and resolution</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling rectified flow transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahim</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Boesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ad-nerf: Audio driven neural radiance fields for talking head synthesis</title>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5784" to="5794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized text-to-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Photorealistic video generation with diffusion models</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meera</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.06662</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaikai</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15230</idno>
		<title level="m">Zero-shot talking avatar generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Bernhard Nessler, and Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffted: One-shot audio-driven ted talk video generation with diffusion-based co-speech gestures</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Daruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1922">1922-1931, 2024. 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cogvideo: Large-scale pretraining for text-to-video generation via transformers</title>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.15868</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Animate anyone: Consistent and controllable imageto-video synthesis for character animation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8153" to="8163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Loopy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02634</idno>
		<title level="m">Taming audio-driven portrait avatar with long-term motion dependency</title>
		<imprint>
			<date type="published" when="2006">2024. 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengkun</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.05712</idno>
		<title level="m">Mobileportrait: Real-time one-shot neural head avatars on mobile devices</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ravdess emotional speech audio</title>
		<author>
			<persName><surname>Kaggle</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Elucidating the design space of diffusion-based generative models</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="26565" to="26577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Videopoet: A large language model for zero-shot video generation</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chang</forename><surname>Chiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.14125</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rox</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangfeng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03603</idno>
		<title level="m">A systematic framework for large video generative models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video generation from text</title>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cyberhost: Taming audio-driven avatar diffusion model with region codebook attention</title>
		<author>
			<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.01876</idno>
		<imprint>
			<date type="published" when="2006">2024. 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Diffusion adversarial post-training for one-step video generation</title>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.08316</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02747</idno>
		<title level="m">Maximilian Nickel, and Matt Le. Flow matching for generative modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved baselines with visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="26296" to="26306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2209.03003, 2022. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">Rang</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.10061</idno>
	</analytic>
	<monogr>
		<title level="m">Towards striking, simplified, and semi-body human animation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Voxceleb: a largescale speaker identification dataset</title>
		<author>
			<persName><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Movie gen: A cast of media foundation models</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fatezero: Fusing attentions for zero-shot text-based video editing</title>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09535</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Human4dit: Free-view human video generation with 4d diffusion transformer</title>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youxin</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zerong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17405</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">First order motion model for image animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Motion representations for articulated animation</title>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13653" to="13662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Make-a-video: Text-to-video generation without text-video data</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oron</forename><surname>Ashual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oran</forename><surname>Gafni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14792</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Diffused heads: Diffusion models beat gans on talking-face generation</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Stypulkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Zieba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17485</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Emo2: End-effector guided audio-driven avatar video generation</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.10687</idno>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions</title>
		<author>
			<persName><forename type="first">Linrui</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="244" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">Brooks</forename><surname>Tim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peebles</forename><surname>Bill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connorm</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depue</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeim</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schnurr</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luhman</forename><surname>Troy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luhman</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarence</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Aditya</surname></persName>
		</author>
		<idno>Accessed: 2024-02-15. 3</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and fine-tuned chat models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Fvd: A new metric for video generation</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Phenaki: Variable length video generation from open domain textual descriptions</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernan</forename><surname>Moraldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taghi Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">V-express: Conditional dropout for progressive training of portrait video generation</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghang</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02511</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">V-express: Conditional dropout for progressive training of portrait video generation</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghang</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.02511</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06571</idno>
		<title level="m">Modelscope text-to-video technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Disco: Disentangled control for realistic human dance generation</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="9326" to="9336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10039" to="10049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Imaginator: Conditional spatio-temporal gan for video generation</title>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1160" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Q-align: Teaching lmms for visual scoring via discrete text-defined levels</title>
		<author>
			<persName><forename type="first">Haoning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.17090</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation</title>
		<author>
			<persName><forename type="first">Jay Zhangjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wynne</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7623" to="7633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Vfhq: A high-quality dataset and benchmark for video face super-resolution</title>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Hallo: Hierarchical audio-driven visual synthesis for portrait image animation</title>
		<author>
			<persName><forename type="first">Mingwang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.08801</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Vasa-1: Lifelike audio-driven talking faces generated in real time</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.10667</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyu</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis</title>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Language model beats diffusion-tokenizer is key to visual generation</title>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Versari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vighnesh</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Birodkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuye</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05737</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Make pixels dance: Highdynamic video generation</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8850" to="8860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning realistic 3d motion coefficients for stylized audiodriven single image talking face animation</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Sadtalker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Mimicmotion: High-quality human motion video generation with confidenceaware pose guidance</title>
		<author>
			<persName><forename type="first">Yuang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyuan</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.19680</idno>
		<imprint>
			<date type="published" when="2006">2024. 3, 5, 6</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Thin-plate spline motion model for image animation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3657" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Open-sora: Democratizing efficient video production for all</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Fada: Fast diffusion avatar synthesis with mixed-supervised multi-cfg distillation</title>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.16915</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Magicvideo: Efficient video generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanshu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.11018</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Celebv-hq: A large-scale video facial attributes dataset</title>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hao Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Taming diffusion models for audio-driven cospeech gesture generation</title>
		<author>
			<persName><forename type="first">Lingting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Champ: Controllable and consistent human image animation with 3d parametric guidance</title>
		<author>
			<persName><forename type="first">Shenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junming</forename><surname>Leo Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="145" to="162" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
