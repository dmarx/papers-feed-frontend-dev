# SOAP: IMPROVING AND STABILIZING SHAMPOO USING ADAM

## Abstract

## 

There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first-and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -a memory-efficient approximation of Adam -showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: ShampoO with Adam in the Preconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at [https://github.com/nikhilvyas/SOAP](https://github.com/nikhilvyas/SOAP).

## INTRODUCTION

With ever-increasing costs of LLM training, optimization efficiency has become a central question in the field of deep learning. Several recent works have tackled this challenge by addressing both the memory [(Zhao et al., 2024a;](#)[Wang et al., 2024)](#) and compute [(Anil et al., 2020)](#b0) footprint of optimizers. In Algoperf [(Dahl et al., 2023)](#b3), a recent optimization efficiency benchmark, Shampoo [(Gupta et al., 2018a)](#), a second-order algorithm, outperformed all other submissions, including Adam [(Kingma & Ba, 2015)](#b20), reducing wall-clock time by 28%. Higher-order preconditioning has also been applied in large-scale training runs, such as Gemini-1.5 Flash [(Gemini Team, 2024)](#b13).

The success of Shampoo has drawn increasing attention from the deep learning community. Several works have explored ways to scale Shampoo by improving its memory and compute efficiency [(Wang et al., 2024;](#)[Anil et al., 2020;](#b0)[Shi et al., 2023)](#). Other research [(Morwani et al., 2024)](#) has examined the theoretical foundations of Shampoo and proposed minor adjustments (such as using power 1/2 rather than 1/4) that align with prior empirical findings [(Anil et al., 2020)](#b0). Moreover, [Morwani et al. (2024)](#) also showed that Shampoo with the aforementioned modifications is close to the optimal Kronecker approximation of the Adagrad [(Duchi et al., 2011b)](#) optimizer.

Our first contribution is demonstrating that the variant of Shampoo proposed by [Morwani et al. (2024)](#) is equivalent 1 to running Adafactor [(Shazeer & Stern, 2018;](#b40)[Zhai et al., 2022)](#b46) in the eigenbasis provided by Shampoo's precondi-1600 3200 4800 6400 Training Steps 2.6 2.7 2.8 2.9 3.0 3.1 3.2 Train Loss 660m, 2m batch size Preconditioning Frequency=10 0.25 0.5 0.75 1.0 Wall Time (scaled by AdamW) 2.6 2.7 2.8 2.9 3.0 3.1 3.2 660m, 2m batch size Preconditioning Frequency=10 1 3 10 32 100 Preconditioning Frequency 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 360m, 2m batch size AdamW Shampoo SOAP SOAP (shorter LR schedule) Figure 1: Comparing performance of tuned runs for AdamW, Shampoo (using DistributedShampoo (Shi et al., 2023) implementation) and SOAP. In left and middle figures, Shampoo and SOAP use a preconditioning frequency of 10.

The "shorter LR schedule" plot is where we tuned the cosine decay so as to achieve the same terminal performance as AdamW. There we observe a ≥ 40% reduction in the number of iterations and a ≥ 35% reduction in wall clock time compared to AdamW, and approximately a 20% reduction in both metrics compared to Shampoo. In the right figure we ablate preconditioning frequency and observe a slower degradation of performance of SOAP as compared to Shampoo. See Section 6 for a discussion of experimental results and ablation of batch size and Section 5 for experimental methodology.

tioner. This interpretation of Shampoo connects it to a broader family of methods (e.g. [(George et al., 2018)](#b14)) that design second-order algorithms by running a first-order method in the eigenbasis provided by a second-order method.

Building on this insight, we can explore a broader design space for combining first and second order methods. Many of our design choices are a synthesis of conceptual ideas from prior works of [George et al. (2018)](#b14); [Anil et al. (2020)](#b0); [Morwani et al. (2024)](#) as well as implementation ideas from works of [Wang et al. (2024)](#); [Zhao et al. (2024a)](#).

Explicitly, we study SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) an algorithm that runs AdamW in the eigenbasis provided by Shampoo. Our main contributions are as follows:

• We make a formal connection between the Shampoo and the Adafactor algorithm. This insight leads us to consider the SOAP algorithm, which runs AdamW in the preconditioned space provided by Shampoo.

• SOAP outperforms both Shampoo and Adam in LLM pre-training tasks with model sizes 360m and 660m, even after extensive hyperparameter tuning of Shampoo.

• SOAP reduces the number of hyperparameters compared to Shampoo, resulting in only one additional hyperparameter compared to AdamW: preconditioning frequency.

• SOAP demonstrates greater robustness to large preconditioning frequency compared to Shampoo.

We should also note that while similar algorithmic variants have been discussed in the literature (e.g. see the appendix of [Anil et al. (2020)](#b0)), we are the first to systematically evaluate it.

Organization: In Section 3, we discuss related works. In Section 4, we start by showing an equivalence between Shampoo (with exponent 1/2) and running Adafactor in the eigenspace given by Shampoo, then with this equivalence as the starting point we describe SOAP. In Section 5, we provide our experimental methodology and in Section 6, we compare the performance of AdamW, Shampoo and SOAP on language modeling tasks. In Sections 7.2 and 7.3 we discuss the the space and time complexity of SOAP and how it can be improved.

## NOTATION AND BACKGROUND

We denote the weight matrix of a neural network layer by W ∈ R m×n , and the corresponding gradient by G ∈ R m×n . At a given time step t, these are denoted as W t and G t , respectively. For a batch of inputs at time t, denoted by B t , the loss and its gradient evaluated at W t are represented as ϕ Bt (W t ) and ∇ W ϕ Bt (W t ), respectively.

Adagrad [(Duchi et al., 2011b](#)) is an online learning second-order algorithm that maintains a preconditioner H ∈ R mn×mn . If the vectorized gradient at time t is denoted by g t (i.e., g t = vec(G t ) ∈ R mn ), then the update of the preconditioner and the vectorized weights w t ∈ R mn with learning rate η is given by [Ba, 2015)](#b20), a widely used first-order optimization algorithm in deep learning is a diagonal approximation of Adagrad. It maintains an exponential moving average of the gradients G t (denoted as M t ) and of element-wise squared gradients G 2 t (denoted as V t ) for a given weight matrix W . Its update rule with learning rate η is given by

$H t = H t-1 + g t g ⊤ t ; w t = w t-1 -ηH -1/2 t g t Adam (Kingma &$$W t ← W t-1 -η M t √ V t ,$where the division is performed element-wise.

Adafactor [(Shazeer & Stern, 2018;](#b40)[Zhai et al., 2022)](#b46), a variant of Adam, replaces V t with its best rank-1 approximation V ′ t to reduce memory usage. While the original Adafactor paper [(Shazeer & Stern, 2018)](#b40) proposed additional modifications, such as changes to the learning rate schedule, we focus on the version of Adafactor proposed in recent works [(Zhai et al., 2022;](#b46)[Zhao et al., 2024c)](#), whose update with learning rate η is given by

$W t ← W t-1 -η M t V ′ t .$Shampoo [(Gupta et al., 2018b](#)) is a second-order optimization algorithm that approximates Adagrad and maintains two preconditioners, L t ∈ R m×m and R t ∈ R n×n , for a given weight matrix W ∈ R m×n . The updates for the preconditioners and the weights with learning rate η are as follows:

$L t ← L t-1 + G t G T t ; R t ← R t-1 + G T t G t ; W t ← W t-1 -ηL -1/4 t G t R -1/4 t .$In practice, Shampoo is implemented with several other modifications such as layerwise learning rate grafting and exponents other than -1/4. We will use the DistributedShampoo [(Shi et al., 2023)](#) implementation which has these variations available as hyperparameters.

## RELATED WORK

We begin by discussing works that are closely related, including [George et al. (2018)](#b14); [Anil et al. (2020)](#b0) and [Zhao et al. (2024a)](#). Subsequently, we cover extended related works.

KFAC [(Martens & Grosse, 2015)](#b28) is a well-known second-order optimization algorithm designed for neural networks. E-KFAC [(George et al., 2018)](#b14) builds upon KFAC in a manner analogous to our extension of Shampoo, introducing a diagonal preconditioner that is updated between KFAC inversion steps. However, E-KFAC's algorithm is not identical to running Adam in KFAC's eigenbasis, as the diagonal preconditioner is not Adam.

Anil et al. ( [2020](#)) introduced several algorithmic and numerical improvements to develop a practical and scalable version of Shampoo [(Gupta et al., 2018b)](#). Notably, they empirically found that using an exponent of 1/2 outperforms the original exponent of 1/4 in Shampoo. Of particular interest to our work is Appendix B of [Anil et al. (2020)](#b0), where, inspired by E-KFAC, they describe an algorithm that is essentially equivalent to SOAP for 2D layers. However, no experiments were provided, and the authors claimed that unpublished experiments showed no empirical improvement over Shampoo. This discrepancy between our findings may be due to some of the implementation details of SOAP.

GaLore [(Zhao et al., 2024a)](#) was recently proposed as a method to reduce Adam's memory footprint by maintaining momentum in a low-rank subspace derived from the singular value decomposition (SVD) of the gradients. Their algorithm's full-rank version bears similarity to ours, with some notable distinctions. Firstly, their projection subspace is determined by the SVD of the current gradient, while we maintain an exponential moving average of GG T and G T G.

Secondly, we retain momentum in the original space and project it onto the preconditioned space, whereas they maintain it in the preconditioned space and do not rotate it each time the preconditioned space is updated. In Appendix B, we study GaLore's performance and find that our modifications are necessary for improving upon Shampoo. Moreover, their method only projects one side of a layer using the eigenbasis while using the identity basis on the other side. We examine the impact of one-sided projection for SOAP in Section 7.1.

Diagonal Preconditioning based Optimizers: Other than AdamW, there are other optimizers which involve diagonal preconditoning such as Lion [(Chen et al., 2023)](#b2), Sophia [(Liu et al., 2024)](#b24), and Adafactor [(Shazeer & Stern, 2018)](#b40).

Recent works of [Kaddour et al. (2023)](#b19); [Zhao et al. (2024c)](#) showed that these optimizers perform comparably to AdamW for LLM pretraining but do not surpass it. This suggests the need to explore non-diagonal preconditioners.

We discuss prior works on non-diagonal preconditioners below.

Second-Order Optimization: Research on second-order optimization in deep learning is generally divided into two categories: Hessian-free methods and methods that estimate the Hessian.

Hessian-Free Methods: Hessian-free approaches [(Martens, 2010;](#b27)[Martens & Grosse, 2015)](#b28) optimize without explicitly computing the Hessian matrix, instead employing iterative techniques to approximate the Newton step. Other recent works [(Li, 2018;](#b21)[2024;](#b22)[Pooladzandi & Li, 2024)](#b34) have focused on designing iterative preconditioners to improve the convergence specifically for stochastic optimization algorithms.

Hessian Estimation Methods: These methods maintain an efficient approximation of the Hessian for neural networks. KFAC [(Martens & Grosse, 2015)](#b28) and Shampoo [(Gupta et al., 2018b)](#) are two widely recognized methods in this area.

KFAC [(Martens & Grosse, 2015)](#b28) was one of the first approaches to go beyond diagonal preconditioners in neural networks, demonstrating that a layer-wise Kronecker-factored preconditioner approximates the layer-wise Hessian in multi-layer perceptrons (MLPs). Subsequent works [(Martens et al., 2018;](#b29)[Osawa et al., 2019)](#b31) extended KFAC to other architectures. Recent research [(George et al., 2018;](#b14)[Gao et al., 2021)](#b12) has further improved trace and diagonal estimates for KFAC. Efforts to scale up KFAC [(Ba et al., 2017;](#b1)[Puiu, 2022;](#b36)[2023;](#)[Eschenhagen et al., 2023)](#b11) have focused on making the inversion step more efficient or enhancing distributed implementations.

Shampoo [(Gupta et al., 2018b)](#), another second-order optimization algorithm, is motivated by the online learning algorithm Adagrad [(Duchi et al., 2011a)](#). Shampoo also employs a layer-wise Kronecker-factored preconditioner. A recent distributed implementation of Shampoo [(Shi et al., 2023)](#) won an optimization efficiency benchmark [(Dahl et al., 2023)](#b3), highlighting the practical utility of second-order methods in deep learning. Few recent works [(Duvvuri et al., 2024;](#b10)[Morwani et al., 2024)](#) have provided theoretical advancements on top of Shampoo. Other works [(Anil et al., 2020;](#b0)[Peirson et al., 2022;](#b33)[Lin et al., 2024;](#)[Wang et al., 2024)](#) have proposed various strategies to improve Shampoo's scalability. We defer a comparison of SOAP with these methods to future work.

## ALGORITHM

## THEORY

We begin by describing an equivalence between Shampoo and running Adafactor in the eigenbasis of the Shampoo preconditioner. For simplicity we omit momentum but the equivalence also holds with momentum. For this equivalence we use Shampoo with the following modifications from the original Shampoo optimizer [(Gupta et al., 2018b)](#):

1. We use power 1/2 instead of power 1/4. This was already recommended in practical implementations [(Anil et al., 2020;](#b0)[Shi et al., 2023)](#) and a theoretical connection between optimal Kronecker approximation of Adagrad [(Duchi et al., 2011b)](#) preconditioner and Shampoo with power 1/2 was established in [Morwani et al. (2024)](#).

2. We also use the scalar correction to per layer learning rates described in [Ren & Goldfarb (2021)](#b39); [Morwani et al. (2024)](#).

3. Instead of the running average of L and R across time steps, we use dataset averages.

With these changes in place (first occurrence of these changes is highlighted in red in the algorithm below) we formally define the two algorithms whose equivalence we show in Algorithms 1 and 2.

Algorithm 1 Single step of idealized Shampoo with power 1/2.

$1: Sample batch B t . 2: G t ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: L ← E B [G B G T B ] {Where the expectation is over a random batch B.} 4: R ← E B [G T B G B ] 5: Ĥ ← L ⊗ R/Trace(L) 6: W t ← W t-1 -η Ĥ-1/2 G t = W t-1 -ηL -1/2 G t R -1/2 /Trace(L) -1/2$Algorithm 2 Single step of idealized Adafactor in Shampoo's eigenspace.

1: Sample batch

$B t . 2: G t ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: L ← E B [G B G T B ] 4: R ← E B [G T B G B ] 5: Q L ← Eigenvectors(L) 6: Q R ← Eigenvectors(R) 7: G ′ t ← Q T L G t Q R 8: {Idealized version of code for Adafactor taking G ′ t to be the gradient} 9: G ′ Bt ← Q T L G Bt Q R 10: A = E B [G ′ B ⊙ G ′ B ]1 m where G ′ B = Q T L G B Q R 11: C = 1 ⊤ n E B [G ′ B ⊙ G ′ B ] 12: Vt = AC T 1 ⊤ n A {Elementwise division} 13: G ′′ t ← G ′ t √ Vt+ϵ {Elementwise division and square root} 14: G ′′′ t ← Q T L G ′′ t Q R {Projecting back to original space} 15: W t ← W t-1 -ηG ′′′ t$Claim 1. Algorithms 1 and 2 are equivalent.

Proof. Consider G t in the basis created after rotating by

$Q L , Q R i.e. G ′ t = Q T L G t Q R . Let the eigenvalues of E Bt [G Bt G T Bt ] and E Bt [G T Bt G Bt ]$be given by λ 1 , ..., λ m and µ 1 , ..., µ n respectively. Algorithm 1 scales the i, j coordinate by (λ i µ j /( i λ i )) -1/2 , while Algorithm 2 scales them by (A i C j /( i A i )) -1/2 . We now show that A i = λ i , an analogous argument shows C j = µ j .

$A i = e T i E B [G ′ B ⊙ G ′ B ]1 m = E B [ j (G ′ B ) 2 i,j ] = E B [ j (u T i (G B )v j ) 2 ] (Using definition of G ′ ) = E B [||u T i (G B )|| 2 ] (v j form a basis) = E B [u T i G B G T B u i ] = λ i$(By definition of λ i and u i )

While these two algorithms are equivalent in their idealized forms, practical considerations reveal some differences. Firstly, the algorithms differ when using running averages instead of dataset averages. Secondly, and more significantly in practice, we do not invert or compute the eigenvector decomposition of L and R at every step. This means that the Algorithm 3 Single step of SOAP for a m × n layer. Per layer, we maintain four matrices: L ∈ R m×m , R ∈ R n×n and V, M ∈ R m×n . For simplicity we ignore the initialization and other boundary effects such as bias correction.

Hyperparameters: Learning rate η, betas = (β 1 , β 2 ), epsilon ϵ, and preconditioning frequency f . An implementation of SOAP is available at [https://github.com/nikhilvyas/SOAP](https://github.com/nikhilvyas/SOAP).

$1: Sample batch B t . 2: G ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: G ′ ← Q T L GQ R 4: M ← β 1 M + (1 -β 1 )G 5: M ′ ← Q T L M Q R 6: {Now we "run" Adam on G ′ } 7: V ← β 2 V + (1 -β 2 )(G ′ ⊙ G ′ ) {Elementwise multiplication} 8: N ′ ← M ′ √$Vt+ϵ {Elementwise division and square root} 9: {Now that we have preconditioned by Adam in the rotated space, we go back to the original space.} 10:

$N ← Q L N ′ Q T R 11: W ← W -ηN 12:${End of gradient step, we now update L and R and possibly also

$Q L and Q R . } 13: L ← β 2 L + (1 -β 2 )GG T 14: R ← β 2 R + (1 -β 2 )G T G 15: if t % f == 0 then 16: Q L ← Eigenvectors(L, Q L ) 17: Q R ← Eigenvectors(R, Q R ) 18: end if$Algorithm 4 Eigenvectors function, implemented using power iteration and QR decomposition. Inputs: PSD matrix P and estimate of eigenvectors Q. If the estimate was exact we would have P = QDQ T where D is the diagonal matrix with eigenvalues.

1:

$S ← P Q 2: Q ← QR(S)$"adaptivity" of learning rates in Shampoo is limited[foot_0](#foot_0) to the updates of L and R. In contrast, with Adafactor in Shampoo's eigenspace, the second moment estimates (i.e., A and C in Algorithm 2) can be updated at every step as they are computationally inexpensive. Additionally, instead of using Adafactor, we can opt[foot_1](#foot_1) for Adam, which offers more generality. Combining these insights leads to Algorithm 3 which can be interpreted as running Adam in Shampoo's eigenspace.

We now describe some additional implementation details:

1. Algorithm 3 describes the behavior of the algorithm for 2D layers. Following [Zhao et al. (2024a)](#), for 1D layers we run standard AdamW. This reduces the overhead as compared to standard implementations of Shampoo which solve an eigenvector problem for 1D layers too.

2. Following [Wang et al. (2024)](#), we compute eigenvectors of L (and R) using one step of power method (Algorithm 4). This requires doing one matrix multiplication followed by QR decomposition. QR decomposition is faster (Documentation, 2024) than standard eigenvector decomposition in PyTorch. For the first iteration, eigenvectors are initialized by doing a standard eigenvector decomposition.

3. For layers with huge dimensions such as the first and last layer in language modeling transformers, maintaining the eigenvectors would be space and time prohibitive. For such dimensions we fix the rotation matrix (Q L or Q R ) to be identity. Note that if we fix both Q L and Q R to be identity for a 2D layer, we would recover Adam.

4. Algorithm 3 omits bias correction and weight decay for simplicity, but these are used in the actual implementation, identical to their use in AdamW.

The main focus of the next sections will be to explore the empirical performance of this algorithm and its variations.

## EXPERIMENTAL METHODOLOGY

Hyperparameter tuning: We begin with hyperparameter values suggested by prior research for both AdamW and Distributed Shampoo (e.g., β 2 = 0.95). Initially, we conduct a learning rate sweep to determine the optimal learning rate for each optimizer. Once the optimal learning rate is identified, we perform two-dimensional sweeps for each of the remaining hyperparameters, where we vary the selected hyperparameter alongside the learning rate. The purpose of these sweeps is to demonstrate that our default hyperparameter settings are near-optimal, disregarding potential interactions between two non-learning-rate hyperparameters. A detailed discussion of the hyperparameter sweeps is provided in Appendix A.

Throughput Measurement: We evaluate the throughput of each optimizer by measuring the number of tokens processed per second. At present, we perform these measurements on a single H100 GPU and utilize gradient accumulation to accommodate large batch sizes. While this approach may seem to disadvantage AdamW-as the overhead of Shampoo/SOAP is compared against multiple gradient accumulation steps-it is important to note that the overhead of Shampoo/SOAP can be amortized across layers by distributing the updates across multiple GPUs. This technique is employed in the distributed implementation of Shampoo [(Shi et al., 2023)](#). A comprehensive comparison of distributed implementations of these algorithms is left to future work.

Efficiency Benefits: Simply running SOAP for the same duration as Shampoo and AdamW cannot be directly used to calculate the efficiency benefit (in terms of training steps or wall-clock time) of using SOAP since we use a cosine schedule. Therefore, we run SOAP on .5, .625, .75 and .875 fraction of the training data and fit a scaling law of the form a + bN -β through the final losses obtained, where N represents the number of training points and a, b, β are the parameters of the fit. We show these points and the corresponding scaling laws obtained in Figure [2](#). This scaling law is then used to calculate the efficiency benefit in terms of training steps and wallclock time as shown in Figure [2](#).

Here, the horizontal lines represent the final losses of AdamW and Shampoo.

## LANGUAGE MODELING EXPERIMENTS

In this section we focus on empirically comparing AdamW, DistributedShampoo, and SOAP on language modeling tasks.

## MEASURING EFFICIENCY BENEFITS

In Figure [1](#) (left and middle) and Figure [3](#fig_0) we show train loss curves of 360m and 660m models with 2m token batch size for AdamW, Shampoo, and SOAP, where SOAP outperforms the other two. To directly calculate the efficiency benefit of SOAP, we also run SOAP with cosine decay for a shorter lr schedule, as shown in Figures [1](#) and [3](#fig_0). This allows us to approximate the following efficiency benefits (when setting batch size to 2m and preconditioning frequency to 10): ≥ 40% reduction in number of iterations and ≥ 35% reduction in wall clock time as compared to AdamW; ≈ 20% reduction in iterations and wall clock time as compared to Shampoo. Precise efficiency benefit calculations are presented in Figure [2](#)(left and middle).

## EFFECT OF FREQUENCY OF FINDING EIGENVECTORS/INVERSE

In Figure [1](#) (right), we compare SOAP and Shampoo with respect to preconditioning frequency. We observe the following:

• For all frequencies we tried from 1 to 100, both optimizers outperform AdamW.

• At frequency 1, SOAP and Shampoo are quite close in performance.

• At higher frequencies, the performance of both SOAP and Shampoo degrades but SOAP's performance degrades significantly slower than Shampoo's.

0.5 0.75 1.0 Total Training Steps (scaled) 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 360m, 2m batch size 0.565 0.8 1.0 Total Training Steps (scaled) 2.66 2.68 2.70 2.72 2.74 2.76 660m, 2m batch size 0.72 0.875 1.0 Total Training Steps (scaled) 2.78 2.80 2.82 2.84 2.86 2.88 2.90 360m, 256k batch size 0.52 0.78 1.01.06 Total Wall Time (scaled by AdamW) 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 0.62 0.875 1.0 1.12 Total Wall Time (scaled by AdamW) 2.66 2.68 2.70 2.72 2.74 2.76 0.83 1.0 1.11 Total Wall Time (scaled by AdamW) 2.78 2.80 2.82 2.84 2.86 2.88 2.90 AdamW SOAP Shampoo

Figure [2](#): Precise efficiency benefits of SOAP over AdamW and Shampoo for 360m (at 256k and 2m batch size) and 660m (at 2m batch size) model. For the precise methodology, refer to Section 5.

## EFFECT OF BATCH SIZE

In this section, we examine the impact of batch size on the performance of the Shampoo and SOAP optimizers. Specifically, we reduce the batch size by a factor of 8, from 2m to 256k. To maintain the same FLOPS overhead for the eigenvector decomposition steps as in the 2m setting, we increase the preconditioning frequency by a factor of 8, from 10 to 80. In Figure [4](#fig_1), we present the optimal runs for each optimizer. Our results show that SOAP consistently outperforms both Shampoo and AdamW, demonstrating a reduction of 25% or more in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. In Figure [2](#) (right), we show that SOAP also improves in wall-clock time by ≥ 15% over AdamW and approximately 10% over Shampoo. Note that we present these results as a preliminary analysis for small batch size runs. It is quite likely that our increase in preconditioning frequency by a factor of 8 is not optimal and a better trade-off is achievable. Furthermore, the overhead of SOAP can likely be ameliorated by doing L and R updates in lower precision (instead of fp32).  We observe a ≥ 25% reduction in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. See Figure [2](#) (right) for wall-clock time improvement and Section 5 for detailed calculation of efficiency improvement.

We also note that the decrease in efficiency improvements at smaller batch sizes for second-order methods is consistent with prior works [(Zhang et al., 2019;](#b47)[Ishikawa & Yokota, 2024)](#).

## FURTHER EFFICIENCY IMPROVEMENTS

In this section, we discuss space and time complexity of and provide an overview of potential avenues for further space and compute efficiency improvements in SOAP.

## ONE SIDED EIGENBASIS

As described in Section 3, [Zhao et al. (2024a)](#) have an algorithm similar to ours. One of the differences is that they only project the smaller side of the layer using the eigenbasis while using identity as the rotation matrix for the larger side i.e. if m < n we set Q R = I n in Algorithm 3 and if m > n we set Q L = I m . Doing this leads to a reduction in space usage as well as reduction of optimizer time overhead, which is discussed in Sections 7.2.1 and 7.3.1.

In Figure [5](#fig_2), it is evident that the one-sided projection results in slightly reduced performance compared to the original SOAP optimizer. However, it still performs on par with, or marginally better than, Shampoo, while maintaining greater computational efficiency. Further investigation into the potential for these variants to surpass the computational efficiency of original SOAP optimizer is left for future work. Combines both of these changes. We observe that while using Adafactor instead of Adam causes a negligible increase in loss, using the one-sided variant causes a larger increase. However, the one-sided variant also has much larger reduction in time and space overhead. For computational benefits of these variants see Sections 7.2 and 7.3.

## SPACE USAGE OF SOAP

For a m × n matrix where m > n we require 2m 2 (for L, Q L ) + 2n 2 (for R, Q R ) + 3mn (for gradient, M, V ) space usage[foot_2](#foot_2) (beyond weights and activations), specifically for L, Q L , R, Q R , momentum (M ), AdamW's second order estimate (V ), and the gradient. This is the same space usage as DistributedShampoo while AdamW uses 3mn.

## IMPROVING SPACE USAGE OF SOAP

The direct way to reduce memory is using low precision to store the L, R, Q L , Q R , V matrices, which is done by [Dettmers et al. (2022)](#b6); [Wang et al. (2024)](#). Orthogonal to the low precision approaches, there are two algorithmic approaches to improving the space usage of SOAP:

• Using Adafactor instead of Adam as the diagonal preconditioner after rotating by Q L and Q R . This reduces the space usage by mn.

• Using one sided version of SOAP (Section 7.1). This reduces space usage from 2m 2 + 2n 2 + 3mn to 2 min(m, n) 2 + 3mn.

• Combining these approaches yields space usage of 2 min(m, n) 2 + 2mn.

For standard transformer architectures the last variant which combines the two approaches would yield less space usage overall compared to AdamW (which uses 3mn).

We try these approaches in Figure [5](#fig_2). We observe that using Adafactor instead of AdamW yields very small reductions in performance while using one-sided preconditioner results in larger reductions. Nonetheless even after combining these two approaches the resulting optimizer outperforms AdamW while having a smaller space requirement than AdamW. Regarding space usage we also note that Adafactor (with momentum added back) itself utilizes only 2mn space usage and has been shown to perform comparable to AdamW for ViT training [(Zhai et al., 2022)](#b46) and for language model training [(Zhao et al., 2024c)](#). Further space reduction beyond Adafactor has been studied in the Adalomo [(Lv et al., 2024a)](#), GaLore [(Zhao et al., 2024a)](#), and AdaMeM [(Vyas et al., 2024](#b43)) papers.

## TIME OVERHEAD OF SOAP

There are two types of overhead of Shampoo and SOAP over AdamW: the overhead per step and the overhead when changing the preconditioner (or for SOAP, the preconditioner's eigenbasis). Let us first analyze the first one. For SOAP per step for a layer of size m × n we have an overhead of m 3 (updating L) + n 3 (updating R) + (2m 2 n + 2mn 2 ) (projecting and projecting back on both sides).

We note that this is more than the overhead of Shampoo which is m 3 + n 3 + m 2 n + n 2 m. This can be observed in Figure [2](#) (bottom, right) but not in the other figures since there the second type of overhead is the dominant term.

The second type of overhead is due to changing the preconditioner for Shampoo (or for SOAP, preconditioner's eigenbasis i.e. Q L and Q R ). The DistributedShampoo [(Shi et al., 2023)](#) implementation of Shampoo uses a direct call to torch.linalg.eigh for this. Following [Wang et al. (2024)](#) we use Algorithm 4 which uses power iteration based approach which calls torch.linalg.qr. We note that torch.linalg.qr is faster than torch.linalg.eigh (Documentation, 2024). In Figure [6](#) (right) we see that using power iteration based approach (torch.linalg.qr) performs as well as fresh eigenvector decomposition (torch.linalg.eigh).

Effect of frequency on overhead: In Figure [6](#) (left), we observe that the overhead decreases as the preconditioning frequency increases, i.e., the frequency of invoking Algorithm 4. If the only additional computation occurred in Algorithm 4, we would expect the overhead to scale as 1.0/(preconditioning frequency), approaching zero. However, empirical results (Figure [6](#) left) show that the overhead approaches an asymptote greater than zero. This is attributable to the additional matrix multiplications required to update L, update R, project the gradient, and reproject the gradient (for each layer) in the optimizer. Currently, these operations are performed in float32; reducing the precision of these operations, as proposed in [Wang et al. (2024)](#), could lower this asymptote.

10 0 10 1 10 2 Preconditioning Frequency 1 2 4 8 16 32 %overhead in training over AdamW Frequency vs overhead SOAP 1 3 10 32 100 Preconditioning Frequency 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss AdamW SOAP (default, QR) SOAP (eigh) Shampoo Figure 6: (Left) Depicting the overhead of SOAP over AdamW as a function of preconditioning frequency (Right) Comparing the performance of SOAP with torch.linalg.eigh for computing the eigenvectors with Algorithm 4, which uses torch.linalg.qr. Note that torch.linalg.qr is computationally more efficient than torch.linalg.eigh (as mentioned in Documentation ( [2024](#))); however, both seem to have comparable performance throughout the preconditioning frequency spectrum.

## IMPROVING TIME OVERHEAD OF SOAP

The per step overhead of SOAP can be reduced by using low precision to store the L, R, Q L , Q R , V matrices [(Dettmers et al., 2022;](#b6)[Wang et al., 2024)](#), which in turn will speed up computation done using these matrices. This approach cannot be used for reducing the overhead for the preconditioner update in popular deep learning frameworks such as Pytorch since torch.linalg.qr does not support precision lower than float32. Orthogonal to the low precision approach we can improve the per step time overhead of SOAP by the following algorithmic approaches:

• Using Adafactor instead of Adam (Section 7.2) as the diagonal preconditioner after rotating by Q L and Q R .

In this version of SOAP the overhead can be improved by from m 3 + n 3 + 2m 2 n + 2n 2 m to m 3 + n 3 + m 2 n + n 2 m + max(m, n) 2 min(m, n) + min(m, n) 3 by merging the project and project back steps for the smaller dimension. • Using one sided version of SOAP (Section 7.1). This reduces overhead from m 3 + n 3 + 2m 2 n + 2n 2 m to min(m, n) 3 + 2 min(m, n) 2 max(m, n). • Combining these approaches yields an overhead of min(m, n) 2 max(m, n) + 2 min(m, n) 3

Using one-sided version also reduces the second type of overhead from a calls to torch.linalg.qr on a m × m and a n × n matrix to only a single call to min(m, n) × min(m, n) matrix.

![Figure 3: Comparing performance of tuned runs forAdamW, Shampoo (using DistributedShampoo (Shi et al., 2023) implementation) and SOAP. Shampoo and SOAP use preconditioning frequency of 10. We observe a ≥ 40% reduction in the number of iterations and a ≥ 35% reduction in wall clock time compared to AdamW, and approximately a 20% reduction in both metrics compared to Shampoo. See Figure1for 660m results, Sections 6.2 and 6.3 for ablations of preconditioning frequency and batch size respectively, and Section 5 for detailed calculation of efficiency improvement and experimental methodology.]()

![Figure 4: Comparing performance of tuned runs for AdamW, Shampoo (using DistributedShampoo (Shi et al., 2023) implementation) and SOAP for token batch size of 256k. Shampoo and SOAP use preconditioning frequency of 80.We observe a ≥ 25% reduction in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. See Figure2(right) for wall-clock time improvement and Section 5 for detailed calculation of efficiency improvement.]()

![Figure5: Performance of variants of SOAP which improve space usage or time overhead. 1. SOAP (factorized): Uses Adafactor instead of Adam in Shampoo's eigenbasis and 2. SOAP (one-sided): Uses Q = I (i.e. no rotation) on the large side of weight matrix and 3. SOAP (factorized, one-sided): Combines both of these changes. We observe that while using Adafactor instead of Adam causes a negligible increase in loss, using the one-sided variant causes a larger increase. However, the one-sided variant also has much larger reduction in time and space overhead. For computational benefits of these variants see Sections 7.2 and 7.3.]()

We note that practical implementations of Shampoo use grafting which allows for learning rate adaptivity at every step, but this adaptivity is restricted to a single scalar per layer.

Though using AdamW over Adafactor only gives very small improvements in performance, see Figure5and Section 7.2. We also note that one can use any other diagonal preconditioner based optimizer in place of Adam, such as Lion[(Chen et al., 2023)](#b2), Sophia[(Liu et al., 2024)](#b24) or Schedule-Free AdamW(Defazio et al., 2024).

One mn is for storing the gradients, this can be avoided (as long as there is no gradient accumulation) by applying gradients along with backprop(Lv et al., 2024b)  but this is not implemented by default in standard deep learning frameworks such as PyTorch. Hence we will include this term in all of our calculations.

