The introduction of SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) represents a significant advancement in optimization algorithms for deep learning, particularly in the context of large language models (LLMs). Below is a detailed technical explanation and rationale for the key contributions and decisions made by the researchers in developing SOAP.

### Key Contribution: Introduction of SOAP

**Rationale**: The primary motivation behind SOAP is to leverage the strengths of both Shampoo and Adam. Shampoo is a second-order optimization method that utilizes a Kronecker-factored preconditioner, which can lead to faster convergence in certain scenarios. However, it also introduces additional computational overhead and hyperparameters that complicate its use. On the other hand, Adam is a first-order method that is widely used due to its simplicity and efficiency but lacks the second-order information that can enhance optimization performance.

By combining Shampoo's preconditioning with Adam's efficiency, SOAP aims to create a hybrid algorithm that retains the benefits of second-order methods while simplifying the optimization process. This combination allows for improved convergence rates and reduced computational costs, making it particularly suitable for training large models.

### Equivalence: SOAP and Adafactor

**Rationale**: Establishing that SOAP is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner provides a theoretical foundation for the algorithm. This equivalence not only connects SOAP to existing methods but also justifies its design choices. By demonstrating that Shampoo can be interpreted as a variant of Adafactor, the researchers highlight the potential for broader applications of their findings within the optimization landscape.

This formal connection allows for a deeper understanding of how different optimization strategies can be integrated and optimized, paving the way for future research that may explore other combinations of first and second-order methods.

### Performance Metrics

**Rationale**: The empirical results showing that SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, and approximately 20% improvements compared to Shampoo, underscore the effectiveness of the algorithm. These metrics are crucial for demonstrating the practical benefits of SOAP in real-world applications, particularly in the context of LLM pre-training, where computational resources and time are significant considerations.

The substantial improvements in performance metrics validate the researchers' hypothesis that combining the strengths of Shampoo and Adam can lead to more efficient optimization processes. This is particularly relevant in the current landscape of deep learning, where training large models can be prohibitively expensive.

### Hyperparameters

**Rationale**: The introduction of only one additional hyperparameter (preconditioning frequency) compared to Adam simplifies the tuning process. This is a critical design choice, as hyperparameter tuning can often be a bottleneck in the optimization process. By minimizing the number of hyperparameters, the researchers make SOAP more accessible to practitioners who may not have the resources or expertise to conduct extensive hyperparameter searches.

This simplification also enhances the usability of SOAP in various applications, allowing it to be adopted more readily in different contexts without the need for complex tuning procedures.

### Eigendecomposition

**Rationale**: The performance degradation of Shampoo when eigendecomposition is computed less frequently is a known limitation of the method. SOAP addresses this issue by continually updating the running average of the second moment, akin to Adam's approach. This decision is crucial for maintaining the performance of the algorithm, as it allows for a more stable and robust optimization process.

By mitigating the degradation associated with infrequent eigendecomposition, SOAP ensures that the benefits of second-order information are preserved, even in scenarios where computational resources may be limited.

### Algorithm Updates

**Rationale**: The update rules for SOAP, Shampoo, and Adafactor are designed to efficiently incorporate second-order information while maintaining computational feasibility. The use of running averages and the specific forms of the update rules reflect a careful consideration of the trade-offs between computational complexity and optimization performance.

The researchers' choice to adapt the update rules from existing methods while introducing modifications that align with the goals of SOAP demonstrates a thoughtful approach to algorithm design. This allows SOAP to inherit the strengths of its predecessors while addressing their limitations.

### Empirical Evaluation

**Rationale**: The empirical evaluation of SOAP on language model pre-training with models of sizes 360m and 660m provides concrete evidence of its effectiveness. By testing the algorithm on large-scale models, the researchers ensure that the findings are relevant to contemporary deep learning challenges.

This evaluation not only validates the theoretical contributions of the paper but also positions SOAP as a viable alternative to existing optimization methods in practical applications.

### Computational Efficiency

**Rationale**: The design of SOAP aims to improve computational efficiency while maintaining the benefits of second-order methods. This focus on efficiency is particularly important in the context of deep learning, where resource constraints can significantly impact the feasibility of training large models.

By optimizing the algorithm for computational efficiency, the researchers contribute to the ongoing efforts to make deep learning more accessible and sustainable.

### Related Works

**Rationale**: The comparison with KFAC, E-KFAC, and prior works on Shampoo and Adafactor highlights the novelty of