<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SOAP: IMPROVING AND STABILIZING SHAMPOO USING ADAM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-09-17">17 Sep 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nikhil</forename><surname>Vyas</surname></persName>
							<email>nikhil@g.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Depen</forename><surname>Morwani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rosie</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Itai</forename><surname>Shapira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Janson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sham</forename><surname>Kakade</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
								<orgName type="institution" key="instit5">Kempner Institute at Harvard University</orgName>
								<orgName type="institution" key="instit6">Harvard University</orgName>
								<orgName type="institution" key="instit7">Kempner Institute at Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SOAP: IMPROVING AND STABILIZING SHAMPOO USING ADAM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-17">17 Sep 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">EA5EDF47F1D47BD7BFB03A067954B1BC</idno>
					<idno type="arXiv">arXiv:2409.11321v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first-and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor -a memory-efficient approximation of Adam -showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm: ShampoO with Adam in the Preconditioner's eigenbasis (SOAP). With regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available at <ref type="url" target="https://github.com/nikhilvyas/SOAP">https://github.com/nikhilvyas/SOAP</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With ever-increasing costs of LLM training, optimization efficiency has become a central question in the field of deep learning. Several recent works have tackled this challenge by addressing both the memory <ref type="bibr">(Zhao et al., 2024a;</ref><ref type="bibr">Wang et al., 2024)</ref> and compute <ref type="bibr" target="#b0">(Anil et al., 2020)</ref> footprint of optimizers. In Algoperf <ref type="bibr" target="#b3">(Dahl et al., 2023)</ref>, a recent optimization efficiency benchmark, Shampoo <ref type="bibr">(Gupta et al., 2018a)</ref>, a second-order algorithm, outperformed all other submissions, including Adam <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2015)</ref>, reducing wall-clock time by 28%. Higher-order preconditioning has also been applied in large-scale training runs, such as Gemini-1.5 Flash <ref type="bibr" target="#b13">(Gemini Team, 2024)</ref>.</p><p>The success of Shampoo has drawn increasing attention from the deep learning community. Several works have explored ways to scale Shampoo by improving its memory and compute efficiency <ref type="bibr">(Wang et al., 2024;</ref><ref type="bibr" target="#b0">Anil et al., 2020;</ref><ref type="bibr">Shi et al., 2023)</ref>. Other research <ref type="bibr">(Morwani et al., 2024)</ref> has examined the theoretical foundations of Shampoo and proposed minor adjustments (such as using power 1/2 rather than 1/4) that align with prior empirical findings <ref type="bibr" target="#b0">(Anil et al., 2020)</ref>. Moreover, <ref type="bibr">Morwani et al. (2024)</ref> also showed that Shampoo with the aforementioned modifications is close to the optimal Kronecker approximation of the Adagrad <ref type="bibr">(Duchi et al., 2011b)</ref> optimizer.</p><p>Our first contribution is demonstrating that the variant of Shampoo proposed by <ref type="bibr">Morwani et al. (2024)</ref> is equivalent 1 to running Adafactor <ref type="bibr" target="#b40">(Shazeer &amp; Stern, 2018;</ref><ref type="bibr" target="#b46">Zhai et al., 2022)</ref> in the eigenbasis provided by Shampoo's precondi-1600 3200 4800 6400 Training Steps 2.6 2.7 2.8 2.9 3.0 3.1 3.2 Train Loss 660m, 2m batch size Preconditioning Frequency=10 0.25 0.5 0.75 1.0 Wall Time (scaled by AdamW) 2.6 2.7 2.8 2.9 3.0 3.1 3.2 660m, 2m batch size Preconditioning Frequency=10 1 3 10 32 100 Preconditioning Frequency 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 360m, 2m batch size AdamW Shampoo SOAP SOAP (shorter LR schedule) Figure 1: Comparing performance of tuned runs for AdamW, Shampoo (using DistributedShampoo (Shi et al., 2023) implementation) and SOAP. In left and middle figures, Shampoo and SOAP use a preconditioning frequency of 10.</p><p>The "shorter LR schedule" plot is where we tuned the cosine decay so as to achieve the same terminal performance as AdamW. There we observe a ≥ 40% reduction in the number of iterations and a ≥ 35% reduction in wall clock time compared to AdamW, and approximately a 20% reduction in both metrics compared to Shampoo. In the right figure we ablate preconditioning frequency and observe a slower degradation of performance of SOAP as compared to Shampoo. See Section 6 for a discussion of experimental results and ablation of batch size and Section 5 for experimental methodology.</p><p>tioner. This interpretation of Shampoo connects it to a broader family of methods (e.g. <ref type="bibr" target="#b14">(George et al., 2018)</ref>) that design second-order algorithms by running a first-order method in the eigenbasis provided by a second-order method.</p><p>Building on this insight, we can explore a broader design space for combining first and second order methods. Many of our design choices are a synthesis of conceptual ideas from prior works of <ref type="bibr" target="#b14">George et al. (2018)</ref>; <ref type="bibr" target="#b0">Anil et al. (2020)</ref>; <ref type="bibr">Morwani et al. (2024)</ref> as well as implementation ideas from works of <ref type="bibr">Wang et al. (2024)</ref>; <ref type="bibr">Zhao et al. (2024a)</ref>.</p><p>Explicitly, we study SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) an algorithm that runs AdamW in the eigenbasis provided by Shampoo. Our main contributions are as follows:</p><p>• We make a formal connection between the Shampoo and the Adafactor algorithm. This insight leads us to consider the SOAP algorithm, which runs AdamW in the preconditioned space provided by Shampoo.</p><p>• SOAP outperforms both Shampoo and Adam in LLM pre-training tasks with model sizes 360m and 660m, even after extensive hyperparameter tuning of Shampoo.</p><p>• SOAP reduces the number of hyperparameters compared to Shampoo, resulting in only one additional hyperparameter compared to AdamW: preconditioning frequency.</p><p>• SOAP demonstrates greater robustness to large preconditioning frequency compared to Shampoo.</p><p>We should also note that while similar algorithmic variants have been discussed in the literature (e.g. see the appendix of <ref type="bibr" target="#b0">Anil et al. (2020)</ref>), we are the first to systematically evaluate it.</p><p>Organization: In Section 3, we discuss related works. In Section 4, we start by showing an equivalence between Shampoo (with exponent 1/2) and running Adafactor in the eigenspace given by Shampoo, then with this equivalence as the starting point we describe SOAP. In Section 5, we provide our experimental methodology and in Section 6, we compare the performance of AdamW, Shampoo and SOAP on language modeling tasks. In Sections 7.2 and 7.3 we discuss the the space and time complexity of SOAP and how it can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATION AND BACKGROUND</head><p>We denote the weight matrix of a neural network layer by W ∈ R m×n , and the corresponding gradient by G ∈ R m×n . At a given time step t, these are denoted as W t and G t , respectively. For a batch of inputs at time t, denoted by B t , the loss and its gradient evaluated at W t are represented as ϕ Bt (W t ) and ∇ W ϕ Bt (W t ), respectively.</p><p>Adagrad <ref type="bibr">(Duchi et al., 2011b</ref>) is an online learning second-order algorithm that maintains a preconditioner H ∈ R mn×mn . If the vectorized gradient at time t is denoted by g t (i.e., g t = vec(G t ) ∈ R mn ), then the update of the preconditioner and the vectorized weights w t ∈ R mn with learning rate η is given by <ref type="bibr" target="#b20">Ba, 2015)</ref>, a widely used first-order optimization algorithm in deep learning is a diagonal approximation of Adagrad. It maintains an exponential moving average of the gradients G t (denoted as M t ) and of element-wise squared gradients G 2 t (denoted as V t ) for a given weight matrix W . Its update rule with learning rate η is given by</p><formula xml:id="formula_0">H t = H t-1 + g t g ⊤ t ; w t = w t-1 -ηH -1/2 t g t Adam (Kingma &amp;</formula><formula xml:id="formula_1">W t ← W t-1 -η M t √ V t ,</formula><p>where the division is performed element-wise.</p><p>Adafactor <ref type="bibr" target="#b40">(Shazeer &amp; Stern, 2018;</ref><ref type="bibr" target="#b46">Zhai et al., 2022)</ref>, a variant of Adam, replaces V t with its best rank-1 approximation V ′ t to reduce memory usage. While the original Adafactor paper <ref type="bibr" target="#b40">(Shazeer &amp; Stern, 2018)</ref> proposed additional modifications, such as changes to the learning rate schedule, we focus on the version of Adafactor proposed in recent works <ref type="bibr" target="#b46">(Zhai et al., 2022;</ref><ref type="bibr">Zhao et al., 2024c)</ref>, whose update with learning rate η is given by</p><formula xml:id="formula_2">W t ← W t-1 -η M t V ′ t .</formula><p>Shampoo <ref type="bibr">(Gupta et al., 2018b</ref>) is a second-order optimization algorithm that approximates Adagrad and maintains two preconditioners, L t ∈ R m×m and R t ∈ R n×n , for a given weight matrix W ∈ R m×n . The updates for the preconditioners and the weights with learning rate η are as follows:</p><formula xml:id="formula_3">L t ← L t-1 + G t G T t ; R t ← R t-1 + G T t G t ; W t ← W t-1 -ηL -1/4 t G t R -1/4 t .</formula><p>In practice, Shampoo is implemented with several other modifications such as layerwise learning rate grafting and exponents other than -1/4. We will use the DistributedShampoo <ref type="bibr">(Shi et al., 2023)</ref> implementation which has these variations available as hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>We begin by discussing works that are closely related, including <ref type="bibr" target="#b14">George et al. (2018)</ref>; <ref type="bibr" target="#b0">Anil et al. (2020)</ref> and <ref type="bibr">Zhao et al. (2024a)</ref>. Subsequently, we cover extended related works.</p><p>KFAC <ref type="bibr" target="#b28">(Martens &amp; Grosse, 2015)</ref> is a well-known second-order optimization algorithm designed for neural networks. E-KFAC <ref type="bibr" target="#b14">(George et al., 2018)</ref> builds upon KFAC in a manner analogous to our extension of Shampoo, introducing a diagonal preconditioner that is updated between KFAC inversion steps. However, E-KFAC's algorithm is not identical to running Adam in KFAC's eigenbasis, as the diagonal preconditioner is not Adam.</p><p>Anil et al. ( <ref type="formula">2020</ref>) introduced several algorithmic and numerical improvements to develop a practical and scalable version of Shampoo <ref type="bibr">(Gupta et al., 2018b)</ref>. Notably, they empirically found that using an exponent of 1/2 outperforms the original exponent of 1/4 in Shampoo. Of particular interest to our work is Appendix B of <ref type="bibr" target="#b0">Anil et al. (2020)</ref>, where, inspired by E-KFAC, they describe an algorithm that is essentially equivalent to SOAP for 2D layers. However, no experiments were provided, and the authors claimed that unpublished experiments showed no empirical improvement over Shampoo. This discrepancy between our findings may be due to some of the implementation details of SOAP.</p><p>GaLore <ref type="bibr">(Zhao et al., 2024a)</ref> was recently proposed as a method to reduce Adam's memory footprint by maintaining momentum in a low-rank subspace derived from the singular value decomposition (SVD) of the gradients. Their algorithm's full-rank version bears similarity to ours, with some notable distinctions. Firstly, their projection subspace is determined by the SVD of the current gradient, while we maintain an exponential moving average of GG T and G T G.</p><p>Secondly, we retain momentum in the original space and project it onto the preconditioned space, whereas they maintain it in the preconditioned space and do not rotate it each time the preconditioned space is updated. In Appendix B, we study GaLore's performance and find that our modifications are necessary for improving upon Shampoo. Moreover, their method only projects one side of a layer using the eigenbasis while using the identity basis on the other side. We examine the impact of one-sided projection for SOAP in Section 7.1.</p><p>Diagonal Preconditioning based Optimizers: Other than AdamW, there are other optimizers which involve diagonal preconditoning such as Lion <ref type="bibr" target="#b2">(Chen et al., 2023)</ref>, Sophia <ref type="bibr" target="#b24">(Liu et al., 2024)</ref>, and Adafactor <ref type="bibr" target="#b40">(Shazeer &amp; Stern, 2018)</ref>.</p><p>Recent works of <ref type="bibr" target="#b19">Kaddour et al. (2023)</ref>; <ref type="bibr">Zhao et al. (2024c)</ref> showed that these optimizers perform comparably to AdamW for LLM pretraining but do not surpass it. This suggests the need to explore non-diagonal preconditioners.</p><p>We discuss prior works on non-diagonal preconditioners below.</p><p>Second-Order Optimization: Research on second-order optimization in deep learning is generally divided into two categories: Hessian-free methods and methods that estimate the Hessian.</p><p>Hessian-Free Methods: Hessian-free approaches <ref type="bibr" target="#b27">(Martens, 2010;</ref><ref type="bibr" target="#b28">Martens &amp; Grosse, 2015)</ref> optimize without explicitly computing the Hessian matrix, instead employing iterative techniques to approximate the Newton step. Other recent works <ref type="bibr" target="#b21">(Li, 2018;</ref><ref type="bibr" target="#b22">2024;</ref><ref type="bibr" target="#b34">Pooladzandi &amp; Li, 2024)</ref> have focused on designing iterative preconditioners to improve the convergence specifically for stochastic optimization algorithms.</p><p>Hessian Estimation Methods: These methods maintain an efficient approximation of the Hessian for neural networks. KFAC <ref type="bibr" target="#b28">(Martens &amp; Grosse, 2015)</ref> and Shampoo <ref type="bibr">(Gupta et al., 2018b)</ref> are two widely recognized methods in this area.</p><p>KFAC <ref type="bibr" target="#b28">(Martens &amp; Grosse, 2015)</ref> was one of the first approaches to go beyond diagonal preconditioners in neural networks, demonstrating that a layer-wise Kronecker-factored preconditioner approximates the layer-wise Hessian in multi-layer perceptrons (MLPs). Subsequent works <ref type="bibr" target="#b29">(Martens et al., 2018;</ref><ref type="bibr" target="#b31">Osawa et al., 2019)</ref> extended KFAC to other architectures. Recent research <ref type="bibr" target="#b14">(George et al., 2018;</ref><ref type="bibr" target="#b12">Gao et al., 2021)</ref> has further improved trace and diagonal estimates for KFAC. Efforts to scale up KFAC <ref type="bibr" target="#b1">(Ba et al., 2017;</ref><ref type="bibr" target="#b36">Puiu, 2022;</ref><ref type="bibr">2023;</ref><ref type="bibr" target="#b11">Eschenhagen et al., 2023)</ref> have focused on making the inversion step more efficient or enhancing distributed implementations.</p><p>Shampoo <ref type="bibr">(Gupta et al., 2018b)</ref>, another second-order optimization algorithm, is motivated by the online learning algorithm Adagrad <ref type="bibr">(Duchi et al., 2011a)</ref>. Shampoo also employs a layer-wise Kronecker-factored preconditioner. A recent distributed implementation of Shampoo <ref type="bibr">(Shi et al., 2023)</ref> won an optimization efficiency benchmark <ref type="bibr" target="#b3">(Dahl et al., 2023)</ref>, highlighting the practical utility of second-order methods in deep learning. Few recent works <ref type="bibr" target="#b10">(Duvvuri et al., 2024;</ref><ref type="bibr">Morwani et al., 2024)</ref> have provided theoretical advancements on top of Shampoo. Other works <ref type="bibr" target="#b0">(Anil et al., 2020;</ref><ref type="bibr" target="#b33">Peirson et al., 2022;</ref><ref type="bibr">Lin et al., 2024;</ref><ref type="bibr">Wang et al., 2024)</ref> have proposed various strategies to improve Shampoo's scalability. We defer a comparison of SOAP with these methods to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THEORY</head><p>We begin by describing an equivalence between Shampoo and running Adafactor in the eigenbasis of the Shampoo preconditioner. For simplicity we omit momentum but the equivalence also holds with momentum. For this equivalence we use Shampoo with the following modifications from the original Shampoo optimizer <ref type="bibr">(Gupta et al., 2018b)</ref>:</p><p>1. We use power 1/2 instead of power 1/4. This was already recommended in practical implementations <ref type="bibr" target="#b0">(Anil et al., 2020;</ref><ref type="bibr">Shi et al., 2023)</ref> and a theoretical connection between optimal Kronecker approximation of Adagrad <ref type="bibr">(Duchi et al., 2011b)</ref> preconditioner and Shampoo with power 1/2 was established in <ref type="bibr">Morwani et al. (2024)</ref>.</p><p>2. We also use the scalar correction to per layer learning rates described in <ref type="bibr" target="#b39">Ren &amp; Goldfarb (2021)</ref>; <ref type="bibr">Morwani et al. (2024)</ref>.</p><p>3. Instead of the running average of L and R across time steps, we use dataset averages.</p><p>With these changes in place (first occurrence of these changes is highlighted in red in the algorithm below) we formally define the two algorithms whose equivalence we show in Algorithms 1 and 2.</p><p>Algorithm 1 Single step of idealized Shampoo with power 1/2.</p><formula xml:id="formula_4">1: Sample batch B t . 2: G t ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: L ← E B [G B G T B ] {Where the expectation is over a random batch B.} 4: R ← E B [G T B G B ] 5: Ĥ ← L ⊗ R/Trace(L) 6: W t ← W t-1 -η Ĥ-1/2 G t = W t-1 -ηL -1/2 G t R -1/2 /Trace(L) -1/2</formula><p>Algorithm 2 Single step of idealized Adafactor in Shampoo's eigenspace.</p><p>1: Sample batch</p><formula xml:id="formula_5">B t . 2: G t ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: L ← E B [G B G T B ] 4: R ← E B [G T B G B ] 5: Q L ← Eigenvectors(L) 6: Q R ← Eigenvectors(R) 7: G ′ t ← Q T L G t Q R 8: {Idealized version of code for Adafactor taking G ′ t to be the gradient} 9: G ′ Bt ← Q T L G Bt Q R 10: A = E B [G ′ B ⊙ G ′ B ]1 m where G ′ B = Q T L G B Q R 11: C = 1 ⊤ n E B [G ′ B ⊙ G ′ B ] 12: Vt = AC T 1 ⊤ n A {Elementwise division} 13: G ′′ t ← G ′ t √ Vt+ϵ {Elementwise division and square root} 14: G ′′′ t ← Q T L G ′′ t Q R {Projecting back to original space} 15: W t ← W t-1 -ηG ′′′ t</formula><p>Claim 1. Algorithms 1 and 2 are equivalent.</p><p>Proof. Consider G t in the basis created after rotating by</p><formula xml:id="formula_6">Q L , Q R i.e. G ′ t = Q T L G t Q R . Let the eigenvalues of E Bt [G Bt G T Bt ] and E Bt [G T Bt G Bt ]</formula><p>be given by λ 1 , ..., λ m and µ 1 , ..., µ n respectively. Algorithm 1 scales the i, j coordinate by (λ i µ j /( i λ i )) -1/2 , while Algorithm 2 scales them by (A i C j /( i A i )) -1/2 . We now show that A i = λ i , an analogous argument shows C j = µ j .</p><formula xml:id="formula_7">A i = e T i E B [G ′ B ⊙ G ′ B ]1 m = E B [ j (G ′ B ) 2 i,j ] = E B [ j (u T i (G B )v j ) 2 ] (Using definition of G ′ ) = E B [||u T i (G B )|| 2 ] (v j form a basis) = E B [u T i G B G T B u i ] = λ i</formula><p>(By definition of λ i and u i )</p><p>While these two algorithms are equivalent in their idealized forms, practical considerations reveal some differences. Firstly, the algorithms differ when using running averages instead of dataset averages. Secondly, and more significantly in practice, we do not invert or compute the eigenvector decomposition of L and R at every step. This means that the Algorithm 3 Single step of SOAP for a m × n layer. Per layer, we maintain four matrices: L ∈ R m×m , R ∈ R n×n and V, M ∈ R m×n . For simplicity we ignore the initialization and other boundary effects such as bias correction.</p><p>Hyperparameters: Learning rate η, betas = (β 1 , β 2 ), epsilon ϵ, and preconditioning frequency f . An implementation of SOAP is available at <ref type="url" target="https://github.com/nikhilvyas/SOAP">https://github.com/nikhilvyas/SOAP</ref>.</p><formula xml:id="formula_8">1: Sample batch B t . 2: G ∈ R m×n ← -∇ W ϕ Bt (W t ) 3: G ′ ← Q T L GQ R 4: M ← β 1 M + (1 -β 1 )G 5: M ′ ← Q T L M Q R 6: {Now we "run" Adam on G ′ } 7: V ← β 2 V + (1 -β 2 )(G ′ ⊙ G ′ ) {Elementwise multiplication} 8: N ′ ← M ′ √</formula><p>Vt+ϵ {Elementwise division and square root} 9: {Now that we have preconditioned by Adam in the rotated space, we go back to the original space.} 10:</p><formula xml:id="formula_9">N ← Q L N ′ Q T R 11: W ← W -ηN 12:</formula><p>{End of gradient step, we now update L and R and possibly also</p><formula xml:id="formula_10">Q L and Q R . } 13: L ← β 2 L + (1 -β 2 )GG T 14: R ← β 2 R + (1 -β 2 )G T G 15: if t % f == 0 then 16: Q L ← Eigenvectors(L, Q L ) 17: Q R ← Eigenvectors(R, Q R ) 18: end if</formula><p>Algorithm 4 Eigenvectors function, implemented using power iteration and QR decomposition. Inputs: PSD matrix P and estimate of eigenvectors Q. If the estimate was exact we would have P = QDQ T where D is the diagonal matrix with eigenvalues.</p><p>1:</p><formula xml:id="formula_11">S ← P Q 2: Q ← QR(S)</formula><p>"adaptivity" of learning rates in Shampoo is limited<ref type="foot" target="#foot_0">foot_0</ref> to the updates of L and R. In contrast, with Adafactor in Shampoo's eigenspace, the second moment estimates (i.e., A and C in Algorithm 2) can be updated at every step as they are computationally inexpensive. Additionally, instead of using Adafactor, we can opt<ref type="foot" target="#foot_1">foot_1</ref> for Adam, which offers more generality. Combining these insights leads to Algorithm 3 which can be interpreted as running Adam in Shampoo's eigenspace.</p><p>We now describe some additional implementation details:</p><p>1. Algorithm 3 describes the behavior of the algorithm for 2D layers. Following <ref type="bibr">Zhao et al. (2024a)</ref>, for 1D layers we run standard AdamW. This reduces the overhead as compared to standard implementations of Shampoo which solve an eigenvector problem for 1D layers too.</p><p>2. Following <ref type="bibr">Wang et al. (2024)</ref>, we compute eigenvectors of L (and R) using one step of power method (Algorithm 4). This requires doing one matrix multiplication followed by QR decomposition. QR decomposition is faster (Documentation, 2024) than standard eigenvector decomposition in PyTorch. For the first iteration, eigenvectors are initialized by doing a standard eigenvector decomposition.</p><p>3. For layers with huge dimensions such as the first and last layer in language modeling transformers, maintaining the eigenvectors would be space and time prohibitive. For such dimensions we fix the rotation matrix (Q L or Q R ) to be identity. Note that if we fix both Q L and Q R to be identity for a 2D layer, we would recover Adam.</p><p>4. Algorithm 3 omits bias correction and weight decay for simplicity, but these are used in the actual implementation, identical to their use in AdamW.</p><p>The main focus of the next sections will be to explore the empirical performance of this algorithm and its variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL METHODOLOGY</head><p>Hyperparameter tuning: We begin with hyperparameter values suggested by prior research for both AdamW and Distributed Shampoo (e.g., β 2 = 0.95). Initially, we conduct a learning rate sweep to determine the optimal learning rate for each optimizer. Once the optimal learning rate is identified, we perform two-dimensional sweeps for each of the remaining hyperparameters, where we vary the selected hyperparameter alongside the learning rate. The purpose of these sweeps is to demonstrate that our default hyperparameter settings are near-optimal, disregarding potential interactions between two non-learning-rate hyperparameters. A detailed discussion of the hyperparameter sweeps is provided in Appendix A.</p><p>Throughput Measurement: We evaluate the throughput of each optimizer by measuring the number of tokens processed per second. At present, we perform these measurements on a single H100 GPU and utilize gradient accumulation to accommodate large batch sizes. While this approach may seem to disadvantage AdamW-as the overhead of Shampoo/SOAP is compared against multiple gradient accumulation steps-it is important to note that the overhead of Shampoo/SOAP can be amortized across layers by distributing the updates across multiple GPUs. This technique is employed in the distributed implementation of Shampoo <ref type="bibr">(Shi et al., 2023)</ref>. A comprehensive comparison of distributed implementations of these algorithms is left to future work.</p><p>Efficiency Benefits: Simply running SOAP for the same duration as Shampoo and AdamW cannot be directly used to calculate the efficiency benefit (in terms of training steps or wall-clock time) of using SOAP since we use a cosine schedule. Therefore, we run SOAP on .5, .625, .75 and .875 fraction of the training data and fit a scaling law of the form a + bN -β through the final losses obtained, where N represents the number of training points and a, b, β are the parameters of the fit. We show these points and the corresponding scaling laws obtained in Figure <ref type="figure">2</ref>. This scaling law is then used to calculate the efficiency benefit in terms of training steps and wallclock time as shown in Figure <ref type="figure">2</ref>.</p><p>Here, the horizontal lines represent the final losses of AdamW and Shampoo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LANGUAGE MODELING EXPERIMENTS</head><p>In this section we focus on empirically comparing AdamW, DistributedShampoo, and SOAP on language modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">MEASURING EFFICIENCY BENEFITS</head><p>In Figure <ref type="figure">1</ref> (left and middle) and Figure <ref type="figure" target="#fig_0">3</ref> we show train loss curves of 360m and 660m models with 2m token batch size for AdamW, Shampoo, and SOAP, where SOAP outperforms the other two. To directly calculate the efficiency benefit of SOAP, we also run SOAP with cosine decay for a shorter lr schedule, as shown in Figures <ref type="figure">1</ref> and <ref type="figure" target="#fig_0">3</ref>. This allows us to approximate the following efficiency benefits (when setting batch size to 2m and preconditioning frequency to 10): ≥ 40% reduction in number of iterations and ≥ 35% reduction in wall clock time as compared to AdamW; ≈ 20% reduction in iterations and wall clock time as compared to Shampoo. Precise efficiency benefit calculations are presented in Figure <ref type="figure">2</ref>(left and middle).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EFFECT OF FREQUENCY OF FINDING EIGENVECTORS/INVERSE</head><p>In Figure <ref type="figure">1</ref> (right), we compare SOAP and Shampoo with respect to preconditioning frequency. We observe the following:</p><p>• For all frequencies we tried from 1 to 100, both optimizers outperform AdamW.</p><p>• At frequency 1, SOAP and Shampoo are quite close in performance.</p><p>• At higher frequencies, the performance of both SOAP and Shampoo degrades but SOAP's performance degrades significantly slower than Shampoo's.</p><p>0.5 0.75 1.0 Total Training Steps (scaled) 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 360m, 2m batch size 0.565 0.8 1.0 Total Training Steps (scaled) 2.66 2.68 2.70 2.72 2.74 2.76 660m, 2m batch size 0.72 0.875 1.0 Total Training Steps (scaled) 2.78 2.80 2.82 2.84 2.86 2.88 2.90 360m, 256k batch size 0.52 0.78 1.01.06 Total Wall Time (scaled by AdamW) 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss 0.62 0.875 1.0 1.12 Total Wall Time (scaled by AdamW) 2.66 2.68 2.70 2.72 2.74 2.76 0.83 1.0 1.11 Total Wall Time (scaled by AdamW) 2.78 2.80 2.82 2.84 2.86 2.88 2.90 AdamW SOAP Shampoo</p><p>Figure <ref type="figure">2</ref>: Precise efficiency benefits of SOAP over AdamW and Shampoo for 360m (at 256k and 2m batch size) and 660m (at 2m batch size) model. For the precise methodology, refer to Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">EFFECT OF BATCH SIZE</head><p>In this section, we examine the impact of batch size on the performance of the Shampoo and SOAP optimizers. Specifically, we reduce the batch size by a factor of 8, from 2m to 256k. To maintain the same FLOPS overhead for the eigenvector decomposition steps as in the 2m setting, we increase the preconditioning frequency by a factor of 8, from 10 to 80. In Figure <ref type="figure" target="#fig_1">4</ref>, we present the optimal runs for each optimizer. Our results show that SOAP consistently outperforms both Shampoo and AdamW, demonstrating a reduction of 25% or more in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. In Figure <ref type="figure">2</ref> (right), we show that SOAP also improves in wall-clock time by ≥ 15% over AdamW and approximately 10% over Shampoo. Note that we present these results as a preliminary analysis for small batch size runs. It is quite likely that our increase in preconditioning frequency by a factor of 8 is not optimal and a better trade-off is achievable. Furthermore, the overhead of SOAP can likely be ameliorated by doing L and R updates in lower precision (instead of fp32).  We observe a ≥ 25% reduction in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. See Figure <ref type="figure">2</ref> (right) for wall-clock time improvement and Section 5 for detailed calculation of efficiency improvement.</p><p>We also note that the decrease in efficiency improvements at smaller batch sizes for second-order methods is consistent with prior works <ref type="bibr" target="#b47">(Zhang et al., 2019;</ref><ref type="bibr">Ishikawa &amp; Yokota, 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FURTHER EFFICIENCY IMPROVEMENTS</head><p>In this section, we discuss space and time complexity of and provide an overview of potential avenues for further space and compute efficiency improvements in SOAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">ONE SIDED EIGENBASIS</head><p>As described in Section 3, <ref type="bibr">Zhao et al. (2024a)</ref> have an algorithm similar to ours. One of the differences is that they only project the smaller side of the layer using the eigenbasis while using identity as the rotation matrix for the larger side i.e. if m &lt; n we set Q R = I n in Algorithm 3 and if m &gt; n we set Q L = I m . Doing this leads to a reduction in space usage as well as reduction of optimizer time overhead, which is discussed in Sections 7.2.1 and 7.3.1.</p><p>In Figure <ref type="figure" target="#fig_2">5</ref>, it is evident that the one-sided projection results in slightly reduced performance compared to the original SOAP optimizer. However, it still performs on par with, or marginally better than, Shampoo, while maintaining greater computational efficiency. Further investigation into the potential for these variants to surpass the computational efficiency of original SOAP optimizer is left for future work. Combines both of these changes. We observe that while using Adafactor instead of Adam causes a negligible increase in loss, using the one-sided variant causes a larger increase. However, the one-sided variant also has much larger reduction in time and space overhead. For computational benefits of these variants see Sections 7.2 and 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">SPACE USAGE OF SOAP</head><p>For a m × n matrix where m &gt; n we require 2m 2 (for L, Q L ) + 2n 2 (for R, Q R ) + 3mn (for gradient, M, V ) space usage<ref type="foot" target="#foot_2">foot_2</ref> (beyond weights and activations), specifically for L, Q L , R, Q R , momentum (M ), AdamW's second order estimate (V ), and the gradient. This is the same space usage as DistributedShampoo while AdamW uses 3mn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">IMPROVING SPACE USAGE OF SOAP</head><p>The direct way to reduce memory is using low precision to store the L, R, Q L , Q R , V matrices, which is done by <ref type="bibr" target="#b6">Dettmers et al. (2022)</ref>; <ref type="bibr">Wang et al. (2024)</ref>. Orthogonal to the low precision approaches, there are two algorithmic approaches to improving the space usage of SOAP:</p><p>• Using Adafactor instead of Adam as the diagonal preconditioner after rotating by Q L and Q R . This reduces the space usage by mn.</p><p>• Using one sided version of SOAP (Section 7.1). This reduces space usage from 2m 2 + 2n 2 + 3mn to 2 min(m, n) 2 + 3mn.</p><p>• Combining these approaches yields space usage of 2 min(m, n) 2 + 2mn.</p><p>For standard transformer architectures the last variant which combines the two approaches would yield less space usage overall compared to AdamW (which uses 3mn).</p><p>We try these approaches in Figure <ref type="figure" target="#fig_2">5</ref>. We observe that using Adafactor instead of AdamW yields very small reductions in performance while using one-sided preconditioner results in larger reductions. Nonetheless even after combining these two approaches the resulting optimizer outperforms AdamW while having a smaller space requirement than AdamW. Regarding space usage we also note that Adafactor (with momentum added back) itself utilizes only 2mn space usage and has been shown to perform comparable to AdamW for ViT training <ref type="bibr" target="#b46">(Zhai et al., 2022)</ref> and for language model training <ref type="bibr">(Zhao et al., 2024c)</ref>. Further space reduction beyond Adafactor has been studied in the Adalomo <ref type="bibr">(Lv et al., 2024a)</ref>, GaLore <ref type="bibr">(Zhao et al., 2024a)</ref>, and AdaMeM <ref type="bibr" target="#b43">(Vyas et al., 2024</ref>) papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">TIME OVERHEAD OF SOAP</head><p>There are two types of overhead of Shampoo and SOAP over AdamW: the overhead per step and the overhead when changing the preconditioner (or for SOAP, the preconditioner's eigenbasis). Let us first analyze the first one. For SOAP per step for a layer of size m × n we have an overhead of m 3 (updating L) + n 3 (updating R) + (2m 2 n + 2mn 2 ) (projecting and projecting back on both sides).</p><p>We note that this is more than the overhead of Shampoo which is m 3 + n 3 + m 2 n + n 2 m. This can be observed in Figure <ref type="figure">2</ref> (bottom, right) but not in the other figures since there the second type of overhead is the dominant term.</p><p>The second type of overhead is due to changing the preconditioner for Shampoo (or for SOAP, preconditioner's eigenbasis i.e. Q L and Q R ). The DistributedShampoo <ref type="bibr">(Shi et al., 2023)</ref> implementation of Shampoo uses a direct call to torch.linalg.eigh for this. Following <ref type="bibr">Wang et al. (2024)</ref> we use Algorithm 4 which uses power iteration based approach which calls torch.linalg.qr. We note that torch.linalg.qr is faster than torch.linalg.eigh (Documentation, 2024). In Figure <ref type="figure">6</ref> (right) we see that using power iteration based approach (torch.linalg.qr) performs as well as fresh eigenvector decomposition (torch.linalg.eigh).</p><p>Effect of frequency on overhead: In Figure <ref type="figure">6</ref> (left), we observe that the overhead decreases as the preconditioning frequency increases, i.e., the frequency of invoking Algorithm 4. If the only additional computation occurred in Algorithm 4, we would expect the overhead to scale as 1.0/(preconditioning frequency), approaching zero. However, empirical results (Figure <ref type="figure">6</ref> left) show that the overhead approaches an asymptote greater than zero. This is attributable to the additional matrix multiplications required to update L, update R, project the gradient, and reproject the gradient (for each layer) in the optimizer. Currently, these operations are performed in float32; reducing the precision of these operations, as proposed in <ref type="bibr">Wang et al. (2024)</ref>, could lower this asymptote.</p><p>10 0 10 1 10 2 Preconditioning Frequency 1 2 4 8 16 32 %overhead in training over AdamW Frequency vs overhead SOAP 1 3 10 32 100 Preconditioning Frequency 2.82 2.84 2.86 2.88 2.90 2.92 2.94 Final Test Loss AdamW SOAP (default, QR) SOAP (eigh) Shampoo Figure 6: (Left) Depicting the overhead of SOAP over AdamW as a function of preconditioning frequency (Right) Comparing the performance of SOAP with torch.linalg.eigh for computing the eigenvectors with Algorithm 4, which uses torch.linalg.qr. Note that torch.linalg.qr is computationally more efficient than torch.linalg.eigh (as mentioned in Documentation ( <ref type="formula">2024</ref>)); however, both seem to have comparable performance throughout the preconditioning frequency spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">IMPROVING TIME OVERHEAD OF SOAP</head><p>The per step overhead of SOAP can be reduced by using low precision to store the L, R, Q L , Q R , V matrices <ref type="bibr" target="#b6">(Dettmers et al., 2022;</ref><ref type="bibr">Wang et al., 2024)</ref>, which in turn will speed up computation done using these matrices. This approach cannot be used for reducing the overhead for the preconditioner update in popular deep learning frameworks such as Pytorch since torch.linalg.qr does not support precision lower than float32. Orthogonal to the low precision approach we can improve the per step time overhead of SOAP by the following algorithmic approaches:</p><p>• Using Adafactor instead of Adam (Section 7.2) as the diagonal preconditioner after rotating by Q L and Q R .</p><p>In this version of SOAP the overhead can be improved by from m 3 + n 3 + 2m 2 n + 2n 2 m to m 3 + n 3 + m 2 n + n 2 m + max(m, n) 2 min(m, n) + min(m, n) 3 by merging the project and project back steps for the smaller dimension. • Using one sided version of SOAP (Section 7.1). This reduces overhead from m 3 + n 3 + 2m 2 n + 2n 2 m to min(m, n) 3 + 2 min(m, n) 2 max(m, n). • Combining these approaches yields an overhead of min(m, n) 2 max(m, n) + 2 min(m, n) 3</p><p>Using one-sided version also reduces the second type of overhead from a calls to torch.linalg.qr on a m × m and a n × n matrix to only a single call to min(m, n) × min(m, n) matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparing performance of tuned runs forAdamW, Shampoo (using DistributedShampoo (Shi et al., 2023)   implementation) and SOAP. Shampoo and SOAP use preconditioning frequency of 10. We observe a ≥ 40% reduction in the number of iterations and a ≥ 35% reduction in wall clock time compared to AdamW, and approximately a 20% reduction in both metrics compared to Shampoo. See Figure1for 660m results, Sections 6.2 and 6.3 for ablations of preconditioning frequency and batch size respectively, and Section 5 for detailed calculation of efficiency improvement and experimental methodology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparing performance of tuned runs for AdamW, Shampoo (using DistributedShampoo (Shi et al., 2023) implementation) and SOAP for token batch size of 256k. Shampoo and SOAP use preconditioning frequency of 80.We observe a ≥ 25% reduction in the number of iterations compared to AdamW, and approximately a 10% reduction compared to Shampoo. See Figure2(right) for wall-clock time improvement and Section 5 for detailed calculation of efficiency improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Performance of variants of SOAP which improve space usage or time overhead. 1. SOAP (factorized): Uses Adafactor instead of Adam in Shampoo's eigenbasis and 2. SOAP (one-sided): Uses Q = I (i.e. no rotation) on the large side of weight matrix and 3. SOAP (factorized, one-sided): Combines both of these changes. We observe that while using Adafactor instead of Adam causes a negligible increase in loss, using the one-sided variant causes a larger increase. However, the one-sided variant also has much larger reduction in time and space overhead. For computational benefits of these variants see Sections 7.2 and 7.3.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We note that practical implementations of Shampoo use grafting which allows for learning rate adaptivity at every step, but this adaptivity is restricted to a single scalar per layer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Though using AdamW over Adafactor only gives very small improvements in performance, see Figure5and Section 7.2. We also note that one can use any other diagonal preconditioner based optimizer in place of Adam, such as Lion<ref type="bibr" target="#b2">(Chen et al., 2023)</ref>, Sophia<ref type="bibr" target="#b24">(Liu et al., 2024)</ref> or Schedule-Free AdamW(Defazio et al., 2024).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>One mn is for storing the gradients, this can be avoided (as long as there is no gradient accumulation) by applying gradients along with backprop(Lv et al., 2024b)  but this is not implemented by default in standard deep learning frameworks such as PyTorch. Hence we will include this term in all of our calculations.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8">DISCUSSION AND FUTURE WORK</head><p>We study an optimizer called SOAP: ShampoO with Adam in the Preconditioner's eigenbasis. We show that SOAP outperforms both AdamW and Shampoo in language modeling tasks and show that it is more robust to changes in preconditioning frequency than <rs type="person">Shampoo. For</rs> future work, we would like to explore further improvements to the design of SOAP, in particular, related to using lower precision for the preconditioners as well as a better distributed implementation. We would also like to explore the performance of SOAP on other domains such as vision.</p><p>2. We did a cross product sweep over learning rate (3.16e -4, 1e -3, 3.16e -3, 1e -2), both sided and one sided versions with β 2 = .99 instead of .95 and preconditioning frequency 200.</p><p>3. We did a cross product sweep over learning rate (3.16e -4, 1e -3, 3.16e -3, 1e -2), both sided and one sided versions, preconditioning frequency (50, 200) with β 1 = .9 instead of .95.</p><p>The best performing run among all of these achieved a final loss of 3.12 while the best Shampoo run achieved a final loss of 3.10.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models. We start from the OLMo codebase <ref type="bibr" target="#b15">(Groeneveld et al., 2024)</ref> and train decoder-only transformer models of two sizes: 210m, 360m, and 660m, where the parameter count refers to non-embedding parameters. The models have widths of <ref type="bibr">1024, 1024, and 1408 and depths of 12, 24, 24</ref>. We used the 210m model to explore various ablations, most of our reported results are on 360m and 660m. The MLP hidden dimension is 4x of the width. The activation function is GeLU <ref type="bibr" target="#b18">(Hendrycks &amp; Gimpel, 2016)</ref>. We use RoPE positional encodings <ref type="bibr" target="#b42">(Su et al., 2024)</ref>. Attention heads are always dimension 64. We use PyTorch default LayerNorm. We use QK layer norm <ref type="bibr" target="#b5">(Dehghani et al., 2023)</ref>. Following <ref type="bibr" target="#b45">Wortsman et al. (2024)</ref> we do not learn biases for the linear layers or LayerNorms. We train in mixed precision with bfloat16.</p><p>Algorithms. We use the standard Pytorch implementation of AdamW <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref>, the DistributedShampoo Shi et al. ( <ref type="formula">2023</ref>) implementation of Shampoo. We implement ourselves SOAP and GaLore starting from an older version of Pytorch implementation of AdamW and the official GaLore implementation <ref type="bibr">Zhao et al. (2024b)</ref>.</p><p>Default hyperparameters. We use β 1 = 0.95, as we found it to outperform β 1 = 0.9 in our sweeps for the 360m model. Following <ref type="bibr" target="#b45">Wortsman et al. (2024)</ref> we use decoupled weight decay with coefficient 1e-4 and z-loss with coefficient 1e-4. We use the default value of ϵ = 1e -8 in AdamW (actual or when used for grafting), SOAP and GaLore. We use warmup followed by cosine decay as our scheduler. We start the warmup and end the cosine decay at 0.1x the maximum learning rate.</p><p>Default hyperparameters for DistributedShampoo Shi et al. ( <ref type="formula">2023</ref>) state that they find the optimal exponent to be either -1/2 or -1.82/4 ≈ -1/2.2. Our preliminary findings were similar to this. Hence we set the default values of exponent to be -1/2.5 for both 1D and 2D parameters. We set ϵ shampoo = 1e-12 and β shampoo = 0.95 based on our initial set of experiments on the 210m model.</p><p>Default hyperparameters for GaLore GaLore introduces an additional hyperparameter called scale (α) since due to low rank updates the overall update magnitude decreases. Since we are running a full rank version of GaLore we set α = 1.</p><p>Token counts. For all of our runs we use a sequence length of 1024. For all models (except in Section 6.3), we use a token batch size of 2048k ≈ 2m. We default to training models for the approximately "chinchilla optimal" number of tokens that is ≈20 times the number of parameters. Explicitly, this means for our default batch size of 2m, the 210m models are trained for 1600 steps or ≈ 3.3b tokens. The 360m models are trained for 3200 steps, the 660m models are trained for 6400 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 SWEEPING OVER HYPERPARAMETERS</head><p>AdamW, 2m batch size: Starting from the default hyperparameters above we do the following sweeps:</p><p>1. We sweep over learning rate in {.1, .0316, .01, . . . , 3.16e-4}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">(360m)</head><p>We sweep over the cross product of best 3 learning rates and β 1 ∈ {0.9, 0.95, 0.99}. 3. (360m) We sweep over the cross product of best 3 learning rates and β 2 ∈ {0.9, 0.95, 0.99}.</p><p>The last two of the sweeps did not yield any benefit for the 360m model with 2m batch size hence we only sweep over learning rate for the 660m model with 2m batch size.</p><p>DistributedShampoo, 2m batch size: Starting from the default hyperparameters above we do the following sweeps:</p><p>1. We sweep over learning rate in {.1, .0316, .01, . . . , 3.16e-4}. 2. (360m) We sweep over over the cross product of best 3 learning rates from above and ϵ shampoo ∈ {1e-11, 1e-12, 1e-13}. 3. (360m) We sweep over over the cross product of best 3 learning rates from above and β shampoo ∈ {.9, .95, .975}. 4. Let e 1 , e 2 denote the exponents used in DistributedShampoo for 1D and 2D parameters respectively. We also sweep over the cross product of best 3 learning rates from above and (e 1 , e 2 ) in {(2, 2), (2.5, 2.5), (3, 3), (2, 4)}.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scalable second order optimization for deep learning</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09018</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed second-order optimization using kronecker-factored approximations</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkkTMpjex" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Symbolic discovery of optimization algorithms</title>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/9" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>a39b4925e35cf447ccba8757137d84f-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandramouli Shama</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Medapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runa</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Kasimbeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><forename type="middle">L</forename><surname>Peirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Snider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kongtao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakshith</forename><surname>Vasudev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Badura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Benchmarking neural network training algorithms</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The road less scheduled</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.15682</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.15682" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to 22 billion parameters</title>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Padlewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">Peter</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7480" to="7512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">8-bit optimizers via block-wise quantization</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=shpkpVXzo3h" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Documentation. torch.linalg.eigh documentation</orgName>
		</author>
		<ptr target="https://web.archive.org/web/20240519213242/https://pytorch.org/docs/stable/generated/torch.linalg.eigh.html" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/duchi11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.5555/1953048.2021068</idno>
		<ptr target="https://dl.acm.org/doi/10.5555/1953048.2021068" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining axes preconditioners through kronecker approximation for deep learning</title>
		<author>
			<persName><forename type="first">Surya</forename><surname>Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fnu</forename><surname>Duvvuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Devvrit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Dhillon</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8j9hz8DVi8" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kronecker-factored approximate curvature for modern neural network architectures</title>
		<author>
			<persName><forename type="first">Runa</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Immer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Ex3oJEKS53" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A trace-restricted kronecker-factored approximation to natural gradient</title>
		<author>
			<persName><forename type="first">Kaixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dachuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v35i9.16921</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16921" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7519" to="7527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</title>
		<author>
			<persName><forename type="first">Google</forename><surname>Gemini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<ptr target="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" />
		<imprint>
			<date type="published" when="2024-05">2024. May-2024</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast approximate natural gradient descent in a kronecker factored eigenbasis</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bouthillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2018/hash/48000647" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="9573" to="9583" />
		</imprint>
	</monogr>
	<note>Samy Bengio b315f6f00f913caa757a70b3-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshita</forename><surname>Bhagia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Harsh Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamish</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.00838</idno>
		<title level="m">Accelerating the science of language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shampoo: Preconditioned stochastic tensor optimization</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shampoo: Preconditioned stochastic tensor optimization</title>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v80/gupta18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1837" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When does second-order optimization speed up training?</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<ptr target="https://openreview.net/forum?id=NLrfEsSZNb" />
	</analytic>
	<monogr>
		<title level="m">The Second Tiny Papers Track at ICLR 2024, 2024</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Satoki Ishikawa and Rio Yokota Gaussian error linear units (gelus)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">No train no gain: Revisiting efficient training algorithms for transformer-based language models</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Key</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Nawrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper_files/paper/2023/hash/51" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<editor>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</editor>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023. 2023</date>
		</imprint>
	</monogr>
	<note>f3d6252706100325ddc435ba0ade0e-Abstract-Conference.html</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preconditioned stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Xi-Lin</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2672978</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1454" to="1466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Xi-Lin</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.11858" />
		<title level="m">Stochastic hessian fittings with lie groups</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Can we remove the square-root in adaptive gradient methods? A second-order perspective</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runa</forename><surname>Dangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Eschenhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><surname>Makhzani</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v235/lin24e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katherine</forename><surname>Heller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nuria</forename><surname>Oliver</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jonathan</forename><surname>Scarlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Felix</forename><surname>Berkenkamp</surname></persName>
		</editor>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024-07">Jul 2024</date>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="21" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sophia: A scalable stochastic secondorder optimizer for language model pre-training</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3xHDeA8Noi" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adalomo: Low-memory optimization with adaptive learning rate</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.findings-acl.742" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="page" from="12486" to="12502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Full parameter fine-tuning for large language models with limited resources</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2024.acl-long.445" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Lun-Wei</forename><surname>Ku</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</editor>
		<meeting>the 62nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Bangkok, Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">August 11-16, 2024. 2024</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8187" to="8198" />
		</imprint>
	</monogr>
	<note>ACL 2024</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML&apos;10</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning, ICML&apos;10<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="735" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing neural networks with kronecker-factored approximate curvature</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v37/martens15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kronecker-factored curvature approximations for recurrent neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyMTkQZAb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A new perspective on shampoo&apos;s preconditioner</title>
		<author>
			<persName><forename type="first">Depen</forename><surname>Morwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><surname>Janson</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.17748</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2406.17748" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Osawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohei</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichiro</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akira</forename><surname>Naruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rio</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Matsuoka</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01264</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12351" to="12359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fishy: Layerwise fisher approximation for higher-order neural network optimization</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Abel Peirson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatong</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName><surname>Anil</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=cScb-RrBQC" />
	</analytic>
	<monogr>
		<title level="m">Has it Trained Yet? NeurIPS 2022 Workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Curvature-informed SGD via general purpose lie-group preconditioners</title>
		<author>
			<persName><forename type="first">Omead</forename><surname>Pooladzandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi-Lin</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=sawjxRnVpF" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Resolving discrepancies in compute-optimal scaling of language models</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Tomer Porian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenia</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><surname>Carmon</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2406.19146</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2406.19146" />
		<imprint>
			<date type="published" when="2024">CoRR, abs/2406.19146, 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Randomized k-facs: Speeding up k-fac with randomized numerical linear algebra</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puiu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Data Engineering and Automated Learning -IDEAL 2022</title>
		<editor>
			<persName><forename type="first">Hujun</forename><surname>Yin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Camacho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Tino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Brand new k-facs: Speeding up k-fac with online decomposition updates</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puiu</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.08494" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensor normal training for deep learning models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Goldfarb</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2021/file" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26040" to="26052" />
		</imprint>
	</monogr>
	<note>a37ecfb7b0aeb0e4-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v80/shazeer18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
		<editor>
			<persName><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4603" to="4611" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hao-Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shintaro</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijing</forename><surname>Gallego-Posada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Rangadurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><surname>Rabbat</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.06497</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.06497" />
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murtadha</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adamem: Memory efficient momentum for adafactor</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depen</forename><surname>Morwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fZqMVTz7K5" />
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ICML 2024)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">4-bit shampoo for memory-efficient network training</title>
		<author>
			<persName><forename type="first">Sike</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2405.18144</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2405.18144" />
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Small-scale proxies for large-scale transformer training instabilities</title>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">E</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Everett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><surname>Kornblith</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>id=d8w0pmvXbZ</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scaling vision transformers</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01179</idno>
		<ptr target="https://doi.org/10.1109/CVPR52688.2022.01179" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">June 18-24, 2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2022</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model</title>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushant</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Shallue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="8194" to="8205" />
		</imprint>
	</monogr>
	<note>e0eacd983971634327ae1819ea8b6214-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Galore: Memory-efficient LLM training by gradient low-rank projection</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2403.03507</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2403.03507" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/jiaweizzhao/GaLore,2024b" />
		<title level="m">Anima Anandkumar, and Yuandong Tian. (code) galore: Memory-efficient LLM training by gradient low-rank projection</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deconstructing what makes a good optimizer for language models</title>
		<author>
			<persName><forename type="first">Rosie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depen</forename><surname>Morwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brandfonbrener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2407.07972</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2407.07972" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">We will restate those details verbatim for completeness. We train language models on C4 tokenized with the T5 tokenizer (Raffel et al., 2020) and report results in terms of validation loss. These sweeps did not yield any significant improvement in performance (&lt; .004) for the 360m model</title>
		<imprint/>
	</monogr>
	<note>A EXPERIMENTAL SETUP Many aspects of our setup such as models are the same as in Zhao et al. (2024c) Hence we only sweep over the learning rate for the 660m model</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">2m batch size: Starting from the default hyperparameters above we sweep over learning rate in {.1</title>
		<author>
			<persName><surname>Soap</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0316, .01, . . . , 3.16e-4}</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">256k batch size: For the 360m model with 256 batch size we start from the default hyperparameters and do the following sweeps: 1. We sweep over learning rate in {.1</title>
		<author>
			<persName><surname>Adamw</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0316, .01, . . . , 3.16e-4}</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">We sweep over the cross product of best 3 learning rates and β 2 ∈ {0.95, 0.99}. In the second sweep we observe small improvements in performance by using β 2 = .99, so our final numbers use β 2 = .99. This (small) improvement in performance by using a larger β 2 at smaller batch sizes was</title>
		<editor>Porian et al.</editor>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Zhao et al. (2024c</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">256k batch size: For the 360m model with 256 batch size we start from the default hyperparameters and do the following sweeps: 1. We sweep over learning rate in {.1</title>
		<author>
			<persName><surname>Distributedshampoo</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0316, .01, . . . , 3.16e-4}</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">We sweep over the cross product of best 3 learning rates and (β 2 , β shampoo ) ∈ {(.95</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">In the second sweep we observe small improvements in performance by using β 2 = β shampoo = .99, so our final numbers use β 2 = β shampoo</title>
		<imprint>
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">256k batch size: For the 360m model with 256 batch size we start from the default hyperparameters and do the following sweeps: 1. We sweep over learning rate in {.1</title>
		<author>
			<persName><surname>Soap</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0316, .01, . . . , 3.16e-4}</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">We sweep over the cross product of best 3 learning rates and β 2 ∈ {.95</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Preconditioning frequency sweeps: For the preconditioning frequency experiments of SOAP and Shampoo ( Figure 1 (right)), for each frequency we do a learning rate sweep over the best 3 learning rates found at preconditioning frequency 10. Other hyperparameters are set to their optimal values obtained using the precondition frequency 10 sweeps. 360m and 660m shorter runs: For each of the shorter runs of 360m and 660m models for the SOAP optimizer (Figure 2), we did learning rate sweep over the best 3 learning rates found for the standard length run. Other hyperparameters are set to their optimal values obtained using the standard length run. Warmup: The warmup duration for the 360m and 660m models were 600 and 1200 steps respectively. For the shorter runs (Figure 2), for 360m model, the warmup durations were 400</title>
	</analytic>
	<monogr>
		<title level="m">the second sweep we observe small improvements in performance by using β 2 = .99, so our final numbers use β 2 = .99</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
		</imprint>
	</monogr>
	<note>and 525 steps for 0.5, 0.625, 0.75 and 0.875 runs respectively. For the 660m model, the warmup durations were 600, 750, 900 and 1050 steps for 0.5, 0.625, 0.75 and 0.875 runs respectively</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">We tried GaLore for 210m model, and while it outperformed AdamW it performed worse than Shampoo. Hence we do not try GaLore for higher model sizes. Hyperparameter sweeps: We did the following sweeps: 1. We swept the cross product over learning rate</title>
		<author>
			<persName><forename type="first">B</forename><surname>Galore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="0200">10, 50, 200</date>
		</imprint>
	</monogr>
	<note>3.16e-4, 1e-3, 3.16e-3, 1e-2), preconditioning frequency ), both sided and one sided versions Frequency 200 had the best results matching the observation of Zhao et al. (2024a</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
