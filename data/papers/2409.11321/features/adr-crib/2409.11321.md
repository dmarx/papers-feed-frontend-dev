The decisions made by the researchers in the development of SOAP (ShampoO with Adam in the Preconditioner's eigenbasis) reflect a careful consideration of both theoretical foundations and practical implications in the context of deep learning optimization. Below are detailed technical explanations and justifications for each of the key decisions mentioned:

### 1. Decision to establish a formal connection between Shampoo and Adafactor
The researchers aimed to bridge the gap between two prominent optimization algorithms—Shampoo and Adafactor. By establishing a formal connection, they could leverage the strengths of both methods. This connection allows for a deeper understanding of how Shampoo's higher-order preconditioning can be interpreted in the context of Adafactor's memory-efficient approach. This insight is crucial for developing SOAP, which combines the benefits of both algorithms while addressing their limitations.

### 2. Choice of using the 1/2 power in Shampoo implementation
The choice of using the 1/2 power in the Shampoo implementation is grounded in empirical findings that suggest this exponent yields better performance than the original exponent of 1/4. This decision aligns with the goal of optimizing convergence rates and stability during training. The 1/2 power effectively balances the trade-off between computational efficiency and the robustness of the optimization process.

### 3. Design decision to create SOAP as a combination of Shampoo and Adam
The design of SOAP as a combination of Shampoo and Adam is motivated by the desire to create an optimizer that retains the advantages of second-order methods while simplifying the hyperparameter landscape. By running Adam in the eigenbasis provided by Shampoo, SOAP can achieve improved convergence properties without the computational overhead typically associated with second-order methods.

### 4. Selection of preconditioning frequency as the sole additional hyperparameter
The researchers opted to introduce only one additional hyperparameter—preconditioning frequency—compared to Adam. This decision was made to simplify the tuning process for practitioners, making SOAP more accessible for real-world applications. By focusing on a single hyperparameter, the researchers aimed to maintain the ease of use characteristic of first-order methods while still allowing for some degree of customization in the optimization process.

### 5. Decision to continually update the running average of the second moment
Continually updating the running average of the second moment is a key feature that distinguishes SOAP from traditional Shampoo implementations. This decision helps mitigate performance degradation associated with less frequent eigendecomposition. By maintaining an up-to-date estimate of the second moment, SOAP can adapt more effectively to changes in the optimization landscape, leading to improved convergence rates.

### 6. Choice of empirical evaluation on language model pre-training with specific model sizes
The choice to empirically evaluate SOAP on language model pre-training with specific model sizes (360m and 660m) was driven by the need to assess the optimizer's performance in a relevant and challenging context. Language models are known for their high computational demands, making them an ideal testbed for evaluating the efficiency and effectiveness of optimization algorithms.

### 7. Decision to compare SOAP against AdamW and Shampoo in experiments
Comparing SOAP against AdamW and Shampoo allows the researchers to demonstrate the relative advantages of their proposed method. By benchmarking against established optimizers, they can provide empirical evidence of SOAP's performance improvements, thereby validating their theoretical claims and enhancing the credibility of their findings.

### 8. Choice of using DistributedShampoo implementation for empirical results
The use of the DistributedShampoo implementation for empirical results reflects a commitment to scalability and practical applicability. Distributed implementations are essential for training large models efficiently, and leveraging this framework allows the researchers to evaluate SOAP in realistic scenarios that mirror contemporary deep learning practices.

### 9. Decision to focus on computational efficiency improvements in SOAP
Focusing on computational efficiency improvements in SOAP is crucial given the increasing costs associated with training large language models. By prioritizing efficiency, the researchers aim to make SOAP a viable alternative to existing optimizers, particularly in resource-constrained environments where training time and computational resources are at a premium.

### 10. Assumption regarding the performance degradation with less frequent eigendecomposition
The researchers assumed that reducing the frequency of eigendecomposition would lead to performance degradation, a hypothesis supported by empirical observations. This assumption underscores the importance of maintaining a balance between computational efficiency and the accuracy of the preconditioner, guiding the design of SOAP to continually update the second moment.

### 11. Decision to explore the broader design space for combining first and second-order methods
Exploring the broader design space for combining first and second-order methods reflects an innovative approach to optimization. By synthesizing ideas from prior works, the researchers can identify new opportunities for enhancing optimization algorithms, potentially leading to novel methods that outperform existing solutions.

### 12. Choice to document the robustness of SOAP to large preconditioning frequency
Documenting the robustness of SOAP to large preconditioning frequency is essential for establishing its reliability in various training scenarios. This decision provides practitioners with confidence in the optimizer's performance, particularly in situations where hyperparameter tuning may be limited.

### 13. Decision to include ablation studies on preconditioning frequency and batch size
Including ablation studies on pre