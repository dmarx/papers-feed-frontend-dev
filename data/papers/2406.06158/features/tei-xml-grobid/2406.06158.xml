<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning</title>
				<funder ref="#_gYRE2zD #_rpsHhPw">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">NTT Research</orgName>
				</funder>
				<funder>
					<orgName type="full">Open Philanthropy AI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-12">12 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
							<email>kunin@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Allan</forename><surname>Raventós</surname></persName>
							<email>aravento@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Clémentine</forename><surname>Dominé</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Spring Harbor Laboratory</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>3 Cold</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Klindt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Saxe</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Spring Harbor Laboratory</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>3 Cold</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-12">12 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">56B164E547A4390C5BDCFD46F4AB3FA4</idno>
					<idno type="arXiv">arXiv:2406.06158v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While the impressive performance of modern neural networks is often attributed to their capacity to efficiently extract task-relevant features from data, the mechanisms underlying this rich feature learning regime remain elusive, with much of our theoretical understanding stemming from the opposing lazy regime. In this work, we derive exact solutions to a minimal model that transitions between lazy and rich learning, precisely elucidating how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space. We extend our analysis to more complex linear models with multiple neurons, outputs, and layers and to shallow nonlinear networks with piecewise linear activation functions. In linear networks, rapid feature learning only occurs from balanced initializations, where all layers learn at similar speeds. While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning. Through a series of experiments, we provide evidence that this unbalanced rich regime drives feature learning in deep finite-width networks, promotes interpretability of early layers in CNNs, reduces the sample complexity of learning hierarchical data, and decreases the time to grokking in modular arithmetic. Our theory motivates further exploration of unbalanced initializations to enhance efficient feature learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has transformed machine learning, demonstrating remarkable capabilities in a myriad of tasks ranging from image recognition to natural language processing. It's widely believed that the impressive performance of these models lies in their capacity to efficiently extract task-relevant features from data. However, understanding this feature acquisition requires unraveling a complex interplay between datasets, network architectures, and optimization algorithms. Within this framework, two distinct regimes, determined at initialization, have emerged: the lazy and the rich.</p><p>Lazy regime. Various investigations have revealed a notable phenomenon in overparameterized neural networks, where throughout training the networks remain close to their linearization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Seminal work by Jacot et al. <ref type="bibr" target="#b5">[6]</ref>, demonstrated that in the infinite-width limit, the Neural Tangent Kernel (NTK), which describes the evolution of the neural network through training, converges to a deterministic limit. Consequently, the network learns a solution akin to kernel regression with the NTK matrix. Termed the lazy or kernel regime, this domain has been characterized by a deterministic NTK <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, minimal movement in parameter space <ref type="bibr" target="#b7">[8]</ref>, static hidden representations, exponential learning curves, and implicit biases aligned with a reproducing kernel Hilbert space (RKHS) norm <ref type="bibr" target="#b8">[9]</ref>. However, Chizat et al. <ref type="bibr" target="#b7">[8]</ref> challenged this understanding, asserting that the lazy regime isn't a Kernel Distance (b) A complex phase portrait of feature learning Figure <ref type="figure">1</ref>: Unbalanced initializations lead to rapid rich learning and generalization. We follow the experimental setup used in Fig. <ref type="figure">1</ref> of Chizat et al. <ref type="bibr" target="#b7">[8]</ref> -a wide two-layer student ReLU network f (x; θ) = h i=1 a i max(0, w ⊺ i x) trained on a dataset generated from a narrow two-layer teacher ReLU network. The student parameters are initialized as w i ∼ Unif(S d-1 ( τ α )) and a i = ±ατ , such that τ &gt; 0 controls the overall scale of the function, while α &gt; 0 controls the relative scale of the first and second layers through the conserved quantity δ = τ 2 (α 2 -α -2 ). (a) Shows the training trajectories of |a i |w i (color denotes sgn(a i )) when d = 2 for four different settings of τ, δ. The left plot confirms that small overall scale leads to rich and large overall scale to lazy. The right plot shows that even at small overall scale, the relative scale can move the network between rich and lazy as well.</p><p>Here an upstream initialization δ &gt; 0 shows striking alignment to the teacher (dotted lines), while a downstream initialization δ &lt; 0 shows no alignment. (b) Shows the test loss and kernel distance from initialization computed through training over a sweep of τ and δ when d = 100. Lazy learning happens when τ is large, rich learning happens when τ is small, and rapid rich learning happens when both τ is small and δ is large -an upstream initialization. This initialization also leads to the smallest test loss. See Fig. <ref type="figure">10</ref> in Appendix D.1 for supporting figures. product of the infinite-width architecture, but is contingent on the overall scale of the network at initialization. They demonstrated that given any finite-width model f (x; θ) whose output is zero at initialization, a scaled version of the model τ f (x; θ) will enter the lazy regime as the scale τ diverges. However, they also noted that these scaled models often perform worse in test error. While the lazy regime offers insights into the network's convergence to a global minimum, it does not fully capture the generalization capabilities of neural networks trained with standard initializations. It is thus widely believed that a different regime, driven by small or vanishing initializations, underlies the many successes of neural networks.</p><p>Rich regime. In contrast to the lazy regime, the rich or feature-learning or active regime is distinguished by a learned NTK that evolves through training, non-convex dynamics traversing between saddle points <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, sigmoidal learning curves, and simplicity biases such as low-rankness <ref type="bibr" target="#b12">[13]</ref> or sparsity <ref type="bibr" target="#b13">[14]</ref>. Yet, the exact characterization of rich learning and the features it learns frequently depends on the specific problem at hand, with its definition commonly simplified as what it is not: lazy. Recent analyses have shown that beyond overall scale, other aspects of the initialization can substantially impact the extent of feature learning, such as the effective rank <ref type="bibr" target="#b14">[15]</ref>, layer-specific initialization variances <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, and large learning rates <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> demonstrated that in two-layer linear networks, the relative difference in weight magnitudes between the first and second layer, termed the relative scale in our work, can impact feature learning, with balanced initializations yielding rich learning dynamics, while unbalanced ones tend to induce lazy dynamics. However, as shown in Fig. <ref type="figure">1</ref>, for nonlinear networks unbalanced initializations can induce both rich and lazy dynamics, creating a complex phase portrait of learning regimes influenced by both overall and relative scale. Building on these observations, our study aims to precisely understand how layer-specific initialization variances and learning rates determine the transition between lazy and rich learning in finite-width networks. Moreover, we endeavor to gain insights into the inductive biases of both regimes, and the transition between them, during training and at interpolation, with the ultimate goal of elucidating how the rich regime acquires features that facilitate generalization.</p><p>Our contributions. Our work begins with an exploration of the two-layer single-neuron linear network proposed by Azulay et al. <ref type="bibr" target="#b8">[9]</ref> as a minimal model displaying both lazy and rich learning. In Section 3, we derive exact solutions for the gradient flow dynamics with layer-specific learning rates of this model by employing a combination of hyperbolic and spherical coordinate transformations.</p><p>Alongside recent work by Xu and Ziyin <ref type="bibr" target="#b22">[23]</ref> <ref type="foot" target="#foot_0">foot_0</ref> , our analysis stands out as one of the few analytically tractable models for the transition between lazy and rich learning in a finite-width network, marking a notable contribution to the field. Our analysis reveals that the layer-specific initialization variances and learning rates conspire to influence the learning regime through a simple set of conserved quantities that constrain the geometry of learning trajectories. Additionally, it reveals that a crucial aspect of the relative scale overlooked in prior analysis is its directionality. While a balanced initialization results in all layers learning at similar rates, an unbalanced initialization can cause faster learning in either earlier layers, referred to as an upstream initialization, or later layers, referred to as a downstream initialization. Due to the depth-dependent expressivity of layers in a network, upstream and downstream initializations often exhibit fundamentally distinct learning trajectories. In Section 4 we extend our analysis of the relative scale developed in the single-neuron model to more complex linear models with multiple neurons, outputs, and layers and in Section 5 to two-layer nonlinear networks with piecewise linear activation functions. We find that in linear networks, rapid rich learning can only occur from balanced initializations, while in nonlinear networks, upstream initializations can actually accelerate rich learning. Finally, through a series of experiments, we provide evidence that upstream initializations drive feature learning in deep finite-width networks, promote interpretability of early layers in CNNs, reduce the sample complexity of learning hierarchical data, and decrease the time to grokking in modular arithmetic.</p><p>Notation. In this work, we consider a feedforward network f (x; θ) : R d → R c parameterized by θ ∈ R m . Unless otherwise specified, c = 1. The network is trained by gradient flow θ = -η θ • ∇ θ L(θ), with an initialization θ 0 and layer-specific learning rate η θ ∈ R m + , to minimize the mean squared error L(θ) = 1 2 n i=1 (f (x i ; θ)-y i ) 2 computed over a dataset {(x 1 , y 1 ), . . . , (x n , y n )} of size n. We denote the input matrix as X ∈ R n×d with rows x i ∈ R d and the label vector as y ∈ R n . The network's output f (x; θ) evolves according to the differential equation, ∂ t f (x; θ) = n i=1 Θ(x, x i ; θ)(y i -f (x i ; θ)), where Θ(x, x ′ ; θ) : R d × R d → R is the Neural Tangent Kernel (NTK), defined as Θ(x, x ′ ; θ) = m p=1 η θ p ∂ θp f (x; θ)∂ θp f (x ′ ; θ). The NTK quantifies how one gradient step with data point x ′ affects the evolution of the networks's output evaluated at another data point x. When η θ p is shared by all parameters, the NTK is the kernel associated with the feature map ∇ θ f (x; θ) ∈ R m . We also define the NTK matrix K ∈ R n×n , which is computed across the training data such that K ij = Θ(x i , x j ; θ). The NTK matrix evolves from its initialization K 0 to convergence K ∞ through training. Lazy and rich learning exist on a spectrum, with the extent of this evolution serving as the distinguishing factor. Various studies have proposed different metrics to track the evolution of the NTK matrix <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. We use kernel distance <ref type="bibr" target="#b26">[27]</ref>, defined as S(t 1 , t 2 ) = 1 -⟨K t1 , K t2 ⟩/ (∥K t1 ∥ F ∥K t2 ∥ F ), which is a scale invariant measure of similarity between the NTK at two times. In the lazy regime S(0, t) ≈ 0, while in the rich regime 0 ≪ S(0, t) ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Linear networks. Significant progress in studying the rich regime has been achieved in the context of linear networks. In this setting, f (x; θ) = β(θ) ⊺ x is linear in its input x, but can exhibit highly nonlinear dynamics in parameter θ and function β(θ) space. Foundational work by Saxe et al. <ref type="bibr" target="#b9">[10]</ref> provided exact solutions to gradient flow dynamics in linear networks with task-aligned initializations. They achieved this by solving a system of Bernoulli differential equations that prioritize learning the most salient features first, which can be beneficial for generalization <ref type="bibr" target="#b27">[28]</ref>. This analysis has been extended to wide <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> and deep <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> linear networks with more flexible initialization schemes <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. It has also been applied to study the evolution of the NTK <ref type="bibr" target="#b36">[37]</ref> and the influence of the scale on the transition between lazy and rich learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>. In this work, we present novel exact solutions for a minimal model utilizing a mix of Bernoulli and Riccati equations to showcase a complex phase portrait of lazy and rich learning with separate alignment and fitting phases.</p><p>Implicit bias. An effective analysis approach to understanding the rich regime studies how the initialization influences the inductive bias at interpolation. The aim is to identify a function Q(θ) such that the network converges to a first-order KKT point minimizing Q(θ) among all possible interpolating solutions. Foundational work by Soudry et al. <ref type="bibr" target="#b37">[38]</ref> pioneered this approach for a linear classifier trained with gradient descent, revealing a max margin bias. These findings have been extended to deep linear networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, homogeneous networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, and quasihomogeneous networks <ref type="bibr" target="#b44">[45]</ref>. A similar line of research expresses the learning dynamics of networks trained with mean squared error as a mirror flow for some potential Φ(β), such that the inductive bias can be expressed as a Bregman divergence <ref type="bibr" target="#b45">[46]</ref>. This approach has been applied to diagonal linear networks, revealing an inductive bias that interpolates between ℓ 1 and ℓ<ref type="foot" target="#foot_1">foot_1</ref> norms in the rich and lazy regimes respectively <ref type="bibr" target="#b13">[14]</ref>. However, finding the potential Φ(β) is problem-specific and requires solving a second-order differential equation, which may not be solvable even in simple settings <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> extended this analysis to a time-warped mirror flow, enabling the study of a broader class of architectures. In this work we derive exact expressions for the inductive bias of our minimal model and extend the results in Azulay et al. <ref type="bibr" target="#b8">[9]</ref> to wide and deep linear networks.</p><p>Two-layer networks. Two-layer, or single-hidden layer, piecewise linear networks have emerged as a key setting for advancing our understanding of the rich regime. Maennel et al. <ref type="bibr" target="#b48">[49]</ref> observed that in training two-layer ReLU networks from small initializations, the first-layer weights concentrate along fixed directions determined by the training data, irrespective of network width. This phenomenon, termed quantization, has been proposed as a simplicity bias inherent to the rich regime, driving the network towards low-rank solutions when feasible. Subsequent studies have aimed to precisely elucidate this effect by introducing structural constraints on the training data <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>. Across these analyses, a consistent observation is that the learning dynamics involve distinct phases: an initial alignment phase characterized by quantization, followed by fitting phases where the task is learned. All of these studies assumed a balanced (or nearly balanced) initialization between the first and second layer. In this study, we explore how unbalanced initializations influence the phases of learning, demonstrating that it can eliminate or augment the quantization effect.</p><p>Infinite-width networks. Many recent advancements in understanding the rich regime have come from studying how the initialization variance and layer-wise learning rates should scale in the infinitewidth limit to ensure constant movement in the activations, gradients, and outputs. In this limit, analyzing dynamics becomes simpler in several respects: random variables concentrate and quantities will either vanish to zero, remain constant, or diverge to infinity <ref type="bibr" target="#b16">[17]</ref>. A set of works used tools from statistical mechanics to provide analytic solutions for the rich population dynamics of two-layer nonlinear neural networks initialized according to the mean field parameterization <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59</ref>]. These ideas were extended to deeper networks through a tensor program framework, leading to the derivation of maximal update parametrization (µP) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. The µP parameterization has also been derived through a self-consistent dynamical mean field theory <ref type="bibr" target="#b59">[60]</ref> and a spectral scaling analysis <ref type="bibr" target="#b60">[61]</ref>. In this study, we focus on finite-width neural networks, but discuss the connection between our work and these width-dependent parameterizations in Section 5. The quantity δ = η w a 2 -η a ∥w∥ 2 is conserved through gradient flow, which constrains the trajectory to: (a) a onesheeted hyperboloid for downstream initializations, (b) a double cone for balanced initializations, and (c) a twosheeted hyperboloid for upstream initializations. Gradient flow dynamics for three different initializations a 0 , w 0 with the same product β 0 = a 0 w 0 are shown. The minima manifold is shown in red and the manifold of equivalent β 0 initializations in gray. The surface is colored according to training loss, with blue representing higher loss and red representing lower loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Minimal Model of Lazy and Rich Learning with Exact Solutions</head><p>Here we explore an illustrative setting simple enough to admit exact gradient flow dynamics, yet complex enough to showcase lazy and rich learning regimes.</p><p>We study a two-layer linear network with a single hidden neuron defined by the map f (x; θ) = aw ⊺ x where a ∈ R, w ∈ R d are the parameters. We examine how the parameter initializations a 0 , w 0 and the layer-wise learning rates η a , η w influence the training trajectory in parameter space, function space (defined by the product β = aw), and the evolution of the the NTK matrix,</p><formula xml:id="formula_0">K = X η w a 2 I d + η a ww ⊺ X ⊺ . (1)</formula><p>Except for a measure zero set of initializations which converge to saddle points 2 , all gradient flow trajectories will converge to a global minimum, determined by the normal equations X ⊺ Xaw = X ⊺ y. However, even when X ⊺ X is invertible such that the global minimum β * is unique, the rescaling symmetry between a and w results in a manifold of minima in parameter space. The minima manifold is a one-dimensional hyperbola where w ∝ β * and has two distinct branches for positive and negative a. The symmetry also imposes a constraint on the network's trajectory, maintaining the difference δ = η w a 2 -η a ∥w∥ 2 ∈ R throughout training (see Appendix A.1 for details). This confines the parameter dynamics to the surface of a hyperboloid where the magnitude and sign of the conserved quantity determines the geometry, as shown in Fig. <ref type="figure">2</ref>. An upstream initialization occurs when δ &gt; 0, a balanced initialization when δ = 0, and a downstream initialization when δ &lt; 0.</p><p>Deriving exact solutions in parameter space. We initially assume<ref type="foot" target="#foot_2">foot_2</ref> whitened input X ⊺ X = I d such that the ordinary least squares solution is β * = X ⊺ y, and the gradient flow dynamics simplify to ȧ = η a w ⊺ β * -a∥w∥ 2 , ẇ = η w aβ * -a 2 w . Notice that w(t) ∈ span({w 0 , β * }), and through training, w aligns in direction to ±β * depending on the basin of attraction <ref type="foot" target="#foot_3">4</ref> the parameters are initialized in. Therefore, we can monitor the dynamics by tracking the hyperbolic geometry between a and ∥w(t)∥ and the spherical angle between w(t) and β * . We study the variables µ = a∥w∥, an invariant under the rescale symmetry, and ϕ = w ⊺ β * ∥w∥∥β * ∥ , the cosine of the spherical angle. From these two scalar quantities µ(t), ϕ(t) and the initialization a 0 , w 0 , we can determine the trajectory a(t) and w(t) in parameter space. The dynamics for µ, ϕ are given by the coupled nonlinear ODEs,</p><formula xml:id="formula_1">μ = δ 2 + 4η a η w µ 2 (ϕ∥β * ∥ -µ) , φ = η a η w 2µ∥β * ∥ δ 2 + 4η a η w µ 2 -δ 1 -ϕ 2 .</formula><p>(2) Amazingly, this system can be solved exactly, as discussed in Appendix A.2, and shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Without delving into the specifics, we can develop an intuitive understanding of the solutions by examining the influence of the relative scale δ.  Our theoretical predictions (black dashed lines) agree with gradient flow simulations (solid lines, color-coded based on δ values), shown here for three key metrics: µ (left), ϕ (middle), and S(0, t) (right). Each metric starts at the same value for all δ, but varying δ has a pronounced effect on the metric's dynamics. For upstream initializations (δ ≫ 0), µ changes only slightly, ϕ exponentially aligns, and S remains near zero, indicative of the lazy regime. For balanced initializations (δ = 0), both µ and ϕ change significantly and S quickly moves away from zero, indicative of the rich regime. For downstream initializations (δ ≪ 0), µ quickly drops to zero, then µ and ϕ slowly climb back to one. Similarly, S remains small before a sudden transition towards one, indicative of a delayed rich regime. See Appendix A.2 for further details.</p><p>Upstream. When δ ≫ 0, the updates for both µ and ϕ diverge, but ϕ updates much more rapidly. We can decouple the dynamics of µ and ϕ by separation of their time scales and assume ϕ has reached its steady-state of ±1 before µ has updated. Then, the dynamics of µ is linear and proceeds exponentially to ±∥β * ∥. This regime exhibits minimal kernel movement (see Fig. <ref type="figure" target="#fig_3">3 (c</ref>)) because the kernel is dominated by the η w a 2 I d term, whereas it is mainly w that updates.</p><p>Balanced. When δ = 0, µ follows a Bernoulli differential equation driven by a time-dependent signal ϕ∥β * ∥, and ϕ follows a Riccati equation evolving from an initial value to ±1 depending on the basin of attraction. For vanishing initialization ∥β 0 ∥ → 0, the temporal dynamics of µ and ϕ decouple such that there are two phases of learning: an initial alignment phase where ϕ → ±1, followed by a fitting phase where µ → ±∥β * ∥.</p><p>In the first phase, w aligns to β * resulting in a rank-one update to the NTK, identical to the silent alignment effect described in Atanasov et al. <ref type="bibr" target="#b36">[37]</ref>. In the second phase, the dynamics of µ simplify to the Bernoulli equation studied in Saxe et al. <ref type="bibr" target="#b9">[10]</ref> and the kernel evolves solely in overall scale.</p><p>Downstream. When δ ≪ 0, the updates for µ diverge, while the updates for ϕ vanishes. In this regime the dynamics proceed by an initial fast phase where µ converges exponentially to its steady state of ϕ∥β * ∥. Plugging this steady state into the dynamics of ϕ gives a Bernoulli differential equation</p><formula xml:id="formula_2">φ = η a η w ∥β * ∥ 2 |δ| -1 ϕ(1 -ϕ 2 )</formula><p>. Due to the coefficient |δ| -1 , the second alignment phase proceeds very slowly as ϕ approaches ±1, assuming ϕ, µ ̸ = 0, which is a saddle point. In this regime, the dynamics proceed by an initial lazy fitting phase, followed by a rich alignment phase, where the delay is determined by the magnitude of δ. Here we show the dynamics of β = aw with different values of δ, but the same initial β 0 . When X ⊺ X is whitened (left), we can solve for the dynamics exactly using our expressions for µ, ϕ (black dashed lines). Upstream initializations follow the trajectory of gradient flow on β, downstream initializations first move in the direction of β 0 before sweeping around towards β * , and balanced initializations take an intermediate trajectory between these two. When X ⊺ X is low-rank (right), then we can only predict the trajectories in the limit of δ = ±∞. If the interpolating manifold is onedimensional, then we can solve for the solution in terms of δ exactly (black dots). See Appendix A.4 for details.</p><p>Identifying regimes of learning in function space. Here we take an alternative route towards understanding the influence of the relative scale by directly examining the dynamics in function space, an analysis strategy we will generalize to broader setups in Sections 4 and 5. The network's function is determined by the product β = aw and governed by the ODE,</p><formula xml:id="formula_3">β = -η w a 2 I d + η a ww ⊺ M X ⊺ ρ,<label>(3)</label></formula><p>where ρ = Xβ -y is the residual. These dynamics can be interpreted as preconditioned gradient flow on the loss in function space where the preconditioning matrix M depends on time through its dependence on a 2 and ww ⊺ . Whenever ∥β∥ ̸ = 0, we can express M directly in terms of β and δ as</p><formula xml:id="formula_4">M = κ + δ 2 I d + κ -δ 2 ββ ⊺ ∥β∥ 2 ,<label>(4)</label></formula><p>where κ = δ 2 + 4η a η w ∥β∥ 2 (see Appendix A.3 for a derivation). This establishes a self-consistent equation for the dynamics of β regulated by δ. Additionally, notice that M characterizes the NTK matrix Eq. (1). Thus, understanding the evolution of M along the trajectory β 0 to β * offers a method to discern between lazy and rich learning. Upstream. When δ ≫ 0, M ≈ δI d , and the dynamics of β converge to the trajectory of linear regression trained by gradient flow. Along this trajectory the NTK matrix remains constant, confirming the dynamics are lazy. Balanced. When δ = 0, M = √ η a η w ∥β∥(I d + ββ ⊺ ∥β∥ 2 ). Here the dynamics balance between following the lazy trajectory and attempting to fit the task by only changing in norm. As a result the NTK changes in both magnitude and direction through training, confirming the dynamics are rich. Downstream. When δ ≪ 0, M ≈ |δ| ββ ⊺ ∥β∥ 2 , and β follows a projected gradient descent trajectory, attempting to reach β * in the direction of β 0 . Along this trajectory the NTK matrix doesn't evolve. However, if β 0 is not aligned to β * , then at some point the dynamics of β will slowly align. In this second alignment phase the NTK matrix will change, confirming the dynamics are initially lazy followed by a delayed rich phase. See Appendix A.3.1 for a derivation of the NTK dynamics K.</p><p>Determining the implicit bias via mirror flow. So far we have considered whitened or full rank X ⊺ X, ensuring the existence of a unique least squares solution β * . In this setting, δ influences the trajectory the model takes from β 0 to β * , as shown in Fig. <ref type="figure" target="#fig_4">4 (a)</ref>. Now we consider low-rank X ⊺ X, such that there exist infinitely many interpolating solutions in function space. By studying the structure of M , we can characterize how δ determines the interpolating solution the dynamics converge to. Extending a time-warped mirror flow analysis strategy pioneered by Azulay et al. <ref type="bibr" target="#b8">[9]</ref> to allow δ &lt; 0 (see Appendix A.4 for details), we prove the following theorem, which shows a tradeoff between reaching the minimum norm solution and preserving the direction of the initialization β 0 . Theorem 3.1 (Extending Theorem 2 in Azulay et al. <ref type="bibr" target="#b8">[9]</ref>). For a single hidden neuron linear network, for any δ ∈ R, and initialization β 0 such that β(t) ̸ = 0 for all t ≥ 0, if the gradient flow solution β(∞) satisfies Xβ(∞) = y, then,</p><formula xml:id="formula_5">β(∞) = arg min β∈R d Ψ δ (β) -ψ δ β0 ∥β0∥ ⊺ β s.t. Xβ = y<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">Ψ δ (β) = 1 3 δ 2 + 4∥β∥ 2 -2δ δ 2 + 4∥β∥ 2 + δ and ψ δ = δ 2 + 4∥β 0 ∥ 2 -δ.</formula><p>We observe that for vanishing initializations there is functionally no difference between the inductive bias of the upstream (δ ≫ 0) and balanced (δ = 0) settings. However, in the downstream setting (δ ≪ 0), it is the second term preserving the direction of the initialization that dominates the inductive bias. This tradeoff in inductive bias as a function of δ is presented in Fig. <ref type="figure" target="#fig_4">4 (b)</ref>, where if the null space of X ⊺ X is one-dimensional, we can solve for β(∞) in closed form (see Appendix A.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Wide and Deep Linear Networks</head><p>We now show how the analysis techniques used to study the influence of relative scale in the single-neuron setting can be applied to linear networks with multiple neurons, outputs, and layers.</p><p>Wide linear networks. We consider the dynamics of a two-layer linear network with h hidden neurons and c outputs, f (x; θ) = A ⊺ W x, where W ∈ R h×d and A ∈ R h×c . We assume h ≥ min(d, c), such that this parameterization can represent all linear maps from R d → R c . The rescaling symmetry between A and W implies the <ref type="bibr" target="#b61">[62]</ref>. Drawing insights from our analysis of the single-neuron scenario (h = c = 1), we consider the dynamics of</p><formula xml:id="formula_7">h × h matrix ∆ = η w A 0 A ⊺ 0 -η a W 0 W ⊺ 0 is conserved throughout gradient flow</formula><formula xml:id="formula_8">β = W ⊺ A ∈ R d×c , vec β = -(η w A ⊺ A ⊕ η a W ⊺ W ) M vec(X ⊺ Xβ -X ⊺ Y ),<label>(6)</label></formula><p>where vec(•) denotes the vectorization operator and ⊕ denotes the Kronecker sum <ref type="foot" target="#foot_4">5</ref> . As in the single-neuron setting, we find that the dynamics of β are preconditioned by a matrix M that depends on quadratics of A and W and characterizes the NTK matrix K = (I c ⊗ X) M (I c ⊗ X ⊺ ). We now show how M can be expressed <ref type="foot" target="#foot_5">6</ref> in terms of the rank-1 matrices</p><formula xml:id="formula_9">β k = w k a ⊺ k ∈ R d×c , which represent the contribution to β of a single neuron with parameters w k , a k and conserved quantity δ k = ∆ kk . Theorem 4.1. Whenever ∥β k ∥ F ̸ = 0 for all k ∈ [h], the matrix M can be expressed as the sum M = h k=1 M k over hidden neurons where M k is defined as, M k = δ 2 k + 4η a η w ∥β k ∥ 2 F + δ k 2 β ⊺ k β k ∥β k ∥ 2 F ⊕ δ 2 k + 4η a η w ∥β k ∥ 2 F -δ k 2 β k β ⊺ k ∥β k ∥ 2 F .<label>(7)</label></formula><p>By studying the dependence of M on the conserved quantities δ k and the dimensions d, h and c, we can determine the influence of the relative scale on the learning regime. When min(d, c) ≤ h &lt; max(d, c), and assuming independent initializations for all β k , then networks which narrow from input to output (d &gt; c) enter the lazy regime when all δ k ≫ 0, whereas networks which expand from input to output (d &lt; c) do so when all δ k ≪ 0. However, with opposite signs for δ k , and assuming all β k (0) ̸ ∝ β * , these networks enter a delayed rich regime. As elaborated in Appendix B.1.5, this occurs because in these regimes a solution β * does not exist within the space spanned by M at initialization. When h ≥ max(d, c) all networks enter the lazy regime when all δ k ≫ 0 or all δ k ≪ 0.</p><p>Conversely, as all δ k → 0, all networks transition into the rich regime regardless of dimensions. While Theorem 4.1 offers valuable insight into the learning regimes in the limits of δ k , understanding the transition between regimes remains challenging. To achieve this, we aim to express M in terms of β, rather than β k , by introducing structure on the conserved quantities ∆. Theorem 4.2.</p><formula xml:id="formula_10">When ∆ = δI h and h = d if δ &lt; 0 or h = c if δ &gt; 0, then the matrix M can be expressed as M = η a η w β ⊺ β + δ 2 4 I c ⊗ I d + I c ⊗ η a η w ββ ⊺ + δ 2 4 I d .</formula><p>From Theorem 4.2 the resulting dynamics of β simplify to a self-consistent equation regulated by δ,</p><formula xml:id="formula_11">β = -X ⊺ P η a η w β ⊺ β + δ 2 4 I c -η a η w ββ ⊺ + δ 2 4 I d X ⊺ P,<label>(8)</label></formula><p>where P = Xβ -Y is the residual. Under our isotropic assumption on the conserved quanitities ∆ = δI h , these dynamics are exact. Concurrent to our work, Tu et al. <ref type="bibr" target="#b62">[63]</ref> finds that β approximately follows these dynamics in the overparameterized setting h ≫ max(d, c) under a Gaussian initialization N (0, σ 2 ) of the parameters where σ 2 h is analogous to δ.</p><p>Equipped with a self-consistent equation for the dynamics of β we now aim to interpret these dynamics as a mirror flow with a δ-dependent potential. As presented in Theorem B.6, the dynamics of the singular values of β can be described as a mirror flow with a hyperbolic entropy potential, which smoothly interpolates between an ℓ 1 and ℓ 2 penalty on the singular values for the rich (δ → 0) and lazy (δ → ±∞) regimes respectively. This potential was first identified as the inductive bias for diagonal linear networks by Woodworth et al. <ref type="bibr" target="#b13">[14]</ref> and the same mirror flow on the singular values is derived from a different initialization choice in prior work by Varre et al. <ref type="bibr" target="#b63">[64]</ref>.</p><p>Deep linear networks. As presented in Theorem B.10, we generalize the inductive bias derived for rich two-layer linear networks by Azulay et al. <ref type="bibr" target="#b8">[9]</ref> to deep linear networks. For a depth-</p><formula xml:id="formula_12">(L + 1) linear network, f (x; θ) = A ⊺ L l=1 W l x, where β = L l=1 W ⊺ l A, we find that the inductive bias of the rich regime is Q(β) = ( L+1 L+2 )∥β∥ L+2 L+1 -∥β 0 ∥ -L L+1 β ⊺ 0 β.</formula><p>This inductive bias strikes a depth-dependent balance between attaining the minimum norm solution and preserving the initialization direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Piecewise Linear Networks</head><p>We now take a first step towards extending our analysis from linear networks to piecewise linear networks with activation functions of the form σ(z) = max(z, γz). The input-output map of a piecewise linear network with L hidden layers and h hidden neurons per layer is comprised of potentially O(h dL ) convex activation regions <ref type="bibr" target="#b64">[65]</ref>. Each region is defined by a unique activation pattern of the hidden neurons. The input-output map is linear within each region and continuous at the boundary between regions. Collectively, the activation regions form a 2-colorable<ref type="foot" target="#foot_6">foot_6</ref> convex partition of input space, as shown in Fig. <ref type="figure" target="#fig_6">5</ref>. We investigate how the relative scale influences the evolution of this partition and the linear maps within each region.</p><p>Two-layer network. We consider the dynamics of a two-layer piecewise linear network without biases, f (x; θ) = a ⊺ σ(W x), where W ∈ R h×d and a ∈ R h . Following the approach in Section 4, we consider the contribution to the input-output map from a single hidden neuron k ∈</p><formula xml:id="formula_13">[h] with parameters w k ∈ R d , a k ∈ R and conserved quantity δ k = η w a 2 k -η a ∥w k ∥ 2 [62]</formula><p>. However, unlike the linear setting, the neuron's contribution to f (x i ; θ) is regulated by whether the input x i is in the neuron's active halfspace. Let C ∈ R h×n be the matrix with elements c ki = σ ′ (w ⊺ k x i ), which determines the activation of the k th neuron for the i th data point. The dynamics of</p><formula xml:id="formula_14">β k = a k w k are, βk = -η w a 2 k I d + η a w k w ⊺ k M k n i=1 c ki x i (f (x i ; θ) -y i ) ξ k .<label>(9)</label></formula><p>The matrix M k ∈ R d×d is a preconditioning matrix on the dynamics, and when β k ̸ = 0, it can be expressed in terms of β k and δ k . Unlike the linear setting, ξ k ∈ R d driving the dynamics is not shared for all neurons because of its dependence on c ki . Additionally, the NTK matrix in this setting depends on M k and C, with elements</p><formula xml:id="formula_15">K ij = h k=1 c ki x ⊺ i M k x j c kj .</formula><p>To examine the evolution of K, we consider a signed spherical coordinate transformation separating the dynamics of β k into its directional βk = sgn(a k ) β k ∥β k ∥ and radial µ k = sgn(a k )∥β k ∥ components, such that β k = µ k βk . βk determines the direction and orientation of the halfspace where the k th neuron is active, while µ k determines the slope of the contribution in this halfspace. These coordinates evolve according to,</p><formula xml:id="formula_16">μk = -δ 2 k + 4η a η w µ 2 k β⊺ k ξ k , βk = - δ 2 k + 4η a η w µ 2 k + δ k 2µ k I d -βk β⊺ k ξ k . (<label>10</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">Downstream. When δ k ≪ 0, M k ≈ |δ k | βk β⊺ k</formula><p>, and the dynamics are approximately ∂ t βk = 0 and</p><formula xml:id="formula_19">∂ t µ k = -|δ k | β⊺ k ξ k . Irrespective of ξ k , βk (t) = βk (0)</formula><p>, which implies the overall partition map doesn't change (Fig. <ref type="figure" target="#fig_6">5</ref>, bottom), nor the activation patterns C, nor M k . Only µ k changes to fit the data, while the NTK remains constant. If the number of hidden neurons is insufficient to fit the data, there is a delayed rich alignment phase where the kernel will change, with |δ k | determining the delay.</p><formula xml:id="formula_20">Balanced. When δ k = 0, M k = √ η a η w |µ k |(I d + βk β⊺ k )</formula><p>, and the dynamics simplify to,</p><formula xml:id="formula_21">∂ t βk = - √ η a η w sgn(µ k )(I d -βk β⊺ k )ξ k and ∂ t µ k = -2 √ η a η w |µ k | β⊺ k ξ k .</formula><p>Here both the direction and magnitude of β k evolve, resulting in changes to the activation regions, patterns C, and NTK K. For vanishing initializations where ∥β k (0)∥ → 0 for all k ∈ [h], we can decouple the dynamics into two   <ref type="figure">1(b)</ref>). Rapid feature learning occurs from a small-τ upstream initialization that promotes faster learning in early layers, driving a large change in Hamming distance, but a small change in parameter space. In contrast, small-τ downstream initializations require large parameter movement to fit the data in the delayed rich regime. distinct phases of training (Fig. <ref type="figure" target="#fig_6">5</ref>, top), analogous to the rich regime discussed in Section 3. Phase I: Partition alignment. At vanishing scale, the output f (x; θ 0 ) ≈ 0 for all input x, such that the vector driving the dynamics ξ k ≈ -n i=1 c ki x i y i is independent of the other hidden neurons. At the same time, the radial dynamics slow down relative to the directional dynamics, and the function's output will remain small as each neuron aligns to certain data-dependent fixed points, decoupled from the rest. Prior works have introduced structural constraints on the training data, such as orthogonally separable <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, pair-wise orthonormal <ref type="bibr" target="#b51">[52]</ref>, linearly separable and symmetric <ref type="bibr" target="#b50">[51]</ref> or small angle <ref type="bibr" target="#b54">[55]</ref>, to analytically determine the fixed points of this alignment phase. Phase II: Data fitting. After enough time, the magnitudes of β k have grown such that we can no longer assume f (x; θ) ≈ 0 and thus the residual will depend on all β k . In this phase, the radial dynamics dominate the learning driving the network to fit the data. However, it is possible for the directions to continue to change, and therefore some prior works have further decomposed this phase into multiple stages.</p><formula xml:id="formula_22">Upstream. When δ k ≫ 0, M k ≈ δ k I d ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the dynamics are approximately</head><formula xml:id="formula_23">∂ t βk = -δ k µ -1 k (I d - βk β⊺ k )ξ k and ∂ t µ k = -δ k β⊺ k ξ k .</formula><p>Again, both the direction and magnitude of β k change. However, unlike the balanced setting, in this setting M k is independent of β k and stays constant through training. Yet, as β k change in direction, so can C, and thus the NTK. This setting is unique, because it is rich due to a changing activation pattern, but the dynamics do not move far in parameter space. Furthermore, unlike in the balanced scenario where scale adjusts the speed of radial dynamics, here it regulates the speed of directional dynamics, with vanishing initializations prompting an extremely fast alignment phase, as observed in Fig. <ref type="figure">1</ref> and in Fig. <ref type="figure" target="#fig_6">5</ref>.</p><p>Connections to infinite-width. Our study of learning regimes in finite-width two-layer ReLU networks as a function of the overall and relative scale is consistent with existing infinite-width analysis of feature learning. For example, in Luo et al. <ref type="bibr" target="#b16">[17]</ref> they consider a network f (x) =</p><formula xml:id="formula_24">1 α h k=1 a k σ(w ⊺ k x) with weights initialized as a k ∼ N (0, β 2 a ) and w k ∼ N (0, β 2 W I d ) as width h → ∞.</formula><p>They obtain a phase diagram at infinite width capturing the dependence of learning regime on the overall function scale β a β W /α and the relative initialization scale β a /β W , each suitably normalized as a function of width. The resulting phase portrait is analogous to ours in Fig. <ref type="figure">1 (b)</ref>, where we use the conserved quantity δ rather than the relative scale β a /β W . Specifically, there is a lazy regime that includes the NTK parameterization, which is always achieved at large scale (as in the large-τ regions of Fig. <ref type="figure">1 (b)</ref>), but is also achieved at small scale if the first layer variance is sufficiently larger than the second (as in the downstream initializations at small τ in Fig. <ref type="figure">1 (b)</ref>). On the other side of the phase boundary is the infinite-width analog of rapid rich learning, where all neurons condense to a few directions. This is induced either at small function scale, or at larger scales if β a /β W is sufficiently large, such that W learns fast enough relative to a. The phase boundary, in  turn, which exists only at infinite width, contains a range of parametrizations, including the mean-field parametrization. More broadly, across width-dependent parametrizations, the random initialization of weights induces a distribution over per-neuron conserved quantities. While the distinction between the NTK and the mean-field parametrizations has been extensively studied, both lead to the same distribution of per-neuron conserved quantities, which is zero in expectation with a non-vanishing variance. A more thorough study of what role the distribution of per-neuron conserved quantities plays in feature learning at finite-widths is left to future work.</p><p>Unbalanced initializations in practice. Our analysis shows that upstream initializations can drive rapid rich learning in nonlinear networks. Further experiments in Fig. <ref type="figure" target="#fig_8">6</ref> show that upstream initializations are relevant across various domains of deep learning: (a) Standard initializations see significant NTK evolution early in training <ref type="bibr" target="#b26">[27]</ref>. We show the movement is linked to changes in activation patterns rather than large parameter shifts. Adjusting the initialization variance of the first and last layers can amplify or diminish this movement. (b) Filters in CNNs trained on image classification tasks often align with edge detectors <ref type="bibr" target="#b65">[66]</ref>. We show that adjusting the learning speed of the first layer can enhance or degrade this alignment. (c) Deep learning models are believed to avoid the curse of dimensionality and learn with limited data by exploiting hierarchical structures in real-world tasks.</p><p>Using the Random Hierarchy Model, introduced by Petrini et al. <ref type="bibr" target="#b66">[67]</ref> as a framework for synthetic hierarchical tasks, we show that modifying the relative scale can decrease or increase the sample complexity of learning. (d) Networks trained on simple modular arithmetic tasks will suddenly generalize long after memorizing their training data <ref type="bibr" target="#b67">[68]</ref>. This behavior, termed grokking, is thought to result from a transition from lazy to rich learning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref> and believed to be important towards understanding emergent phenomena <ref type="bibr" target="#b71">[72]</ref>. We show that decreasing the variance of the embedding in a single-layer transformer (&lt; 6% of all parameters) significantly reduces the time to grokking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we derived exact solutions to a minimal model that can transition between lazy and rich learning to precisely elucidate how unbalanced layer-specific initialization variances and learning rates determine the degree of feature learning. We further extended our analysis to wide and deep linear networks and shallow piecewise linear networks. We find through theory and empirics that unbalanced initializations, which promote faster learning at earlier layers, can actually accelerate rich learning. Limitations. The primary limitation lies in the difficulty to extend our theory to deeper nonlinear networks. In contrast to linear networks, where additional symmetries simplify dynamics, nonlinear networks require consideration of the activation pattern's impact on subsequent layers. One potential solution involves leveraging the path framework used in Saxe et al. <ref type="bibr" target="#b72">[73]</ref>.</p><p>Another limitation is our omission of discretization and stochastic effects of SGD, which disrupt the conservation laws central to our study and introduce additional simplicity biases <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>. Future work. Our theory encourages further investigation into unbalanced initializations to optimize efficient feature learning. Understanding how the learning speed profile across layers impacts feature learning, inductive biases, and generalization is an important direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Single-Neuron Linear Network</head><p>In this section, we provide a detailed analysis of the two-layer linear network with a single hidden neuron discussed in Section 3. The network is defined by the function f (x; θ) = aw ⊺ x, where a ∈ R and w ∈ R d are the parameters. We aim to understand the impact of the initializations a 0 , w 0 and the layer-wise learning rates η a , η w on the training trajectory in parameter space, function space (defined by the product β = aw), and the evolution of the Neural Tangent Kernel (NTK) matrix K:</p><formula xml:id="formula_25">K = X η w a 2 I d + η a ww ⊺ X ⊺ . (<label>11</label></formula><formula xml:id="formula_26">)</formula><p>The gradient flow dynamics are governed by the following coupled ODEs:</p><formula xml:id="formula_27">ȧ = -η a w ⊺ (X ⊺ Xaw -X ⊺ y) , a(0) = a 0 ,<label>(12) ẇ</label></formula><formula xml:id="formula_28">= -η w a (X ⊺ Xaw -X ⊺ y) , w(0) = w 0 . (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>The global minima of this problem are determined by the normal equations X ⊺ Xaw = X ⊺ y. Even when X ⊺ X is invertible, yielding a unique global minimum in function space β * = (X ⊺ X) -1 X ⊺ y, the symmetry between a and w, permitting scaling transformations, a → aα and w → w/α for any α ̸ = 0 without changing the product aw, results in a manifold of minima in parameter space. This minima manifold is a one-dimensional hyperbola where aw = β * , with two distinct branches for positive and negative a. The set of saddle points {(a, w)} forms a (d -1)-dimensional subspace satisfying a = 0 and w ⊺ X ⊺ y = 0. Except for a measure zero set of initializations that converge to the saddle points, all gradient flow trajectories will converge to a global minimum. In Appendix A.2.5, we detail the basin of attraction for each branch of the minima manifold and the d-dimensional surface of initializations that converge to saddle points, separating the two basins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Conserved quantity</head><p>The scaling symmetry between a and w results in a conserved quantity δ ∈ R throughout training, as noted in many prior works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b73">74]</ref>, where</p><formula xml:id="formula_30">δ = η w a 2 -η a ∥w∥ 2 . (<label>14</label></formula><formula xml:id="formula_31">)</formula><p>This can be easily verified by explicitly writing out the dynamics of δ. Define ρ = (X ⊺ Xaw -X ⊺ y) for succinct notation, such that</p><formula xml:id="formula_32">δ = 2η w a ȧ -2η a w ⊺ ẇ = 2η w a (-η a w ⊺ ρ) -2η a w ⊺ (-η w aρ) = 0.</formula><p>The conserved quantity confines the parameter dynamics to the surface of a hyperboloid where the magnitude and sign of the conserved quantity determines the geometry, as shown in Fig. <ref type="figure">2</ref>. A hyperboloid of the form</p><formula xml:id="formula_33">k i=1 x 2 i - n i=k+1</formula><p>x 2 i = α, with α ≥ 0, exhibits varied topology and geometry based on k and α. It has two sheets when k = 1 and one sheet otherwise. Its geometry is primarily dictated by α: as α tends to infinity, curvature decreases, while at α = 0, a singularity occurs at the origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Exact solutions</head><p>To derive exact dynamics we assume the input data is whitened such that X ⊺ X = I d and β * = X ⊺ y such that β * ̸ = 0. The dynamics of a and w can then be simplified as</p><formula xml:id="formula_34">ȧ = η a w ⊺ β * -a∥w∥ 2 , a(0) = a 0 (15) ẇ = η w aβ * -a 2 w , w(0) = w 0 .<label>(16)</label></formula><p>A.2.1 Deriving the dynamics for µ and ϕ</p><p>As discussion in Section 3 we study the variables µ = a∥w∥, an invariant under the rescale symmetry, and ϕ = w ⊺ β * ∥w∥∥β * ∥ , the cosine of the angle between w and β * . This change of variables can also be understood as a signed spherical decomposition of β: µ is the signed magnitude of β and ϕ is the cosine angle between β and β * . Through chain rule, we obtain the dynamics for µ and ϕ, which can be expressed as</p><formula xml:id="formula_35">μ = δ 2 + 4η a η w µ 2 (ϕ∥β * ∥ -µ) , µ(0) = a 0 ∥w 0 ∥,<label>(17) φ</label></formula><formula xml:id="formula_36">= η a η w 2µ∥β * ∥ δ 2 + 4η a η w µ 2 -δ 1 -ϕ 2 , ϕ(0) = w ⊺ 0 β * ∥w 0 ∥∥β * ∥ . (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>We leave the derivation to the reader, but emphasize that a key simplification used is to express the sum η w a 2 + η a ∥w∥ 2 in terms of δ,</p><formula xml:id="formula_38">η w a 2 + η a ∥w∥ 2 = δ 2 + 4η a η w µ 2 . (<label>19</label></formula><formula xml:id="formula_39">)</formula><p>Additionally, notice that η a and η w only appear in the dynamics for µ and ϕ as the product η a η w or in the expression for δ. If we were to define µ ′ = √ η a η w µ and β ′ * = √ η a η w β * , then it is not hard to show that the product η a η w is absorbed into the dynamics. Thus, without loss of generality we can assume the product η a η w = 1, resulting in the following coupled system of nonlinear ODEs,</p><formula xml:id="formula_40">μ = δ 2 + 4µ 2 (ϕ∥β * ∥ -µ) , µ(0) = a 0 ∥w 0 ∥ (20) φ = 2µ∥β * ∥ δ 2 + 4µ 2 -δ 1 -ϕ 2 , ϕ(0) = w ⊺ 0 β * ∥w 0 ∥∥β * ∥<label>(21)</label></formula><p>We will now show how to solve this system of equations for µ and ϕ. We will solve this system when δ = 0, δ &gt; 0, and δ &lt; 0 separately. We will then in Appendix A.2.6 show a general treatment on how to obtain the individual coordinates of a and w from the solutions for µ and ϕ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Balanced δ = 0</head><p>When δ = 0, the dynamics for µ, ϕ are,</p><formula xml:id="formula_41">μ = sgn(µ)2µ(ϕ∥β * ∥ -µ), µ(0) = a 0 ∥w 0 ∥,<label>(22)</label></formula><formula xml:id="formula_42">φ = sgn(µ)∥β * ∥(1 -ϕ 2 ), ϕ(0) = w ⊺ 0 β * ∥w0∥∥β * ∥ .<label>(23)</label></formula><p>First, we show that the sign of µ cannot change through training and sgn(µ) = sgn(a). Because δ = 0, the dynamics of a and w are constrained to a double cone with a singularity at the origin (a = 0, w = 0). This point is a saddle point of the dynamics, so the trajectory cannot pass through this point to move from one cone to the other. In other words, the cone where the dynamics are initialized on is the cone they remain on. Without loss of generality, we assume a 0 &gt; 0, and solve the dynamics. The dynamics of µ is a Bernoulli differential equation driven by a time-dependent signal ϕ∥β * ∥. The dynamics of ϕ is decoupled from µ and is in the form of a Riccati equation evolving from an initial value ϕ 0 to 1, as we have assumed an initialization with positive a 0 . This ODE is separable with the solution,</p><formula xml:id="formula_43">ϕ(t) = tanh (c ϕ + ∥β * ∥t) ,<label>(24)</label></formula><p>where c ϕ = tanh -1 (ϕ 0 ). Plugging this solution into the dynamics for µ gives a Bernoulli differential equation,</p><formula xml:id="formula_44">μ = 2∥β * ∥ tanh (c ϕ + ∥β * ∥t) µ -2µ 2 , (<label>25</label></formula><formula xml:id="formula_45">)</formula><p>with the solution,</p><formula xml:id="formula_46">µ(t) = 2 cosh 2 (c ϕ + ∥β * ∥t) 2 (c ϕ + ∥β * ∥t) + sinh (2(c ϕ + ∥β * ∥t)) + c µ ,<label>(26)</label></formula><p>where c µ = 2µ -1 0 cosh 2 (c ϕ ) -(2c ϕ + sinh(2c ϕ )). Note, if ϕ 0 = -1, then φ = 0, and the dynamics of µ will be driven to 0, which is a saddle point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Upstream δ &gt; 0</head><p>When δ &gt; 0, the dynamics are constrained to a hyperboloid composed of two identical sheets determined by the sign of a 0 (as shown in Fig. <ref type="figure">2 (c)</ref>). Without loss of generality we assume a 0 &gt; 0, which ensures a(t) &gt; 0 for all t ≥ 0. However, unlike in the balanced setting, the dynamics of µ and ϕ do not decouple, making it difficult to solve. Instead, we consider ν = w ⊺ β * a , which evolves according to the Riccati equation,</p><formula xml:id="formula_47">ν = ∥β * ∥ 2 -δν -ν 2 , ν(0) = w ⊺ 0 β * a0 . (<label>27</label></formula><formula xml:id="formula_48">)</formula><p>The solution is given by,</p><formula xml:id="formula_49">ν(t) = 2Rν 0 cosh (Rt) + 2∥β * ∥ 2 -δν 0 sinh (Rt) 2R cosh (Rt) + (2ν 0 + δ) sinh (Rt) ,<label>(28)</label></formula><p>where</p><formula xml:id="formula_50">R = 1 2 δ 2 + 4∥β * ∥ 2 .</formula><p>The trajectory of a(t) is given by the Bernoulli equation,</p><formula xml:id="formula_51">ȧ = a(ν(t) + δ -a 2 ), a(0) = a 0 ,<label>(29)</label></formula><p>which can be solved analytically using ν(t). For a 0 &gt; 0, we have that</p><formula xml:id="formula_52">a(t) = 2e tδ/2 ∥β * ∥ √ δ sech 2 (Y (t)) 4e tδ ∥β * ∥ 2 - δ 2 + 4∥β * ∥ 2 ∥β * ∥ 2 δ -a 2 0 + b 2 0 b 2 0 -a 2 0 ∥β * ∥ 2 + a 0 b 0 δ -δe δt δ cosh (2Y (t)) -δ 2 + 4∥β * ∥ 2 sinh (2Y (t)) -1/2</formula><p>where b 0 = w ⊺ 0 β * , and</p><formula xml:id="formula_53">Y (t) = 1 2 δ 2 + 4∥β * ∥ 2 t + atanh 2b 0 a 0 +δ √ δ 2 +4∥β * ∥ 2 .</formula><p>From the solutions for ν, a, we can easily obtain dynamics for µ, ϕ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Downstream δ &lt; 0</head><p>When δ &lt; 0, the dynamics are constrained to a hyperboloid composed of a single sheet (as shown in Fig. <ref type="figure">2 (a)</ref>). However, unlike in the upstream setting, a may change sign. A zero-crossing in a leads to a finite time blowup in ν. Consequently, applying the approach used to solve for the dynamics in the upstream setting becomes more intricate. First we show the following lemma: Lemma A.1. If a 0 ̸ = 0 or w ⊺ 0 β * ̸ = 0, then a(t)w(t) ⊺ β * = 0 has at most one solution for t ≥ 0.</p><p>Proof. Let ω(t) = w(t) ⊺ β * . The two-dimensional dynamics of a(t) and ω(t) are given by,</p><formula xml:id="formula_54">ȧ = ω -a(a 2 -δ),<label>(30)</label></formula><formula xml:id="formula_55">ω = a∥β * ∥ 2 -a 2 ω.<label>(31)</label></formula><p>Consider the orthant O + = {(a, ω)|a &gt; 0, ω &gt; 0}. The boundary ∂O + is formed by two orthogonal subspaces. On {(a, ω)|a = 0, ω ≥ 0}, ȧ ≥ 0. On {(a, ω)|a ≥ 0, ω = 0}, ω ≥ 0. Therefore, O + is a positively invariant set. Similarly, O -= {(a, ω)|a &lt; 0, ω &lt; 0} is a positively invariant set. On the boundary ∂O + ∪ ∂O -= {(a, ω)|aω = 0}, the flow is contained only at the origin a = 0, ω = 0, which represents all saddle points of the dynamics of (a, w). By assumption, (a, w) is not initialized at a saddle point, and thus the origin is not reachable for t ≥ 0. As a result, the trajectory (a(t), ω(t)) will at most intersect the boundary ∂O + ∪ ∂O -once.</p><p>From Lemma A.1, we conclude that either a crosses zero, w ⊺ β * crosses zero, or neither crosses zero. When a doesn't cross zero, then ν is well-defined for t ≥ 0, and our argument from Appendix A.2.3 still holds, leading to solutions for µ, ϕ. When a does cross zero, instead of ν, we consider υ = a w ⊺ β * , the inverse of ν. In this case, we know from Lemma A.1 that w ⊺ β * does not cross zero and thus υ is well-defined for t ≥ 0 and evolves according to the Riccatti equation,</p><formula xml:id="formula_56">υ = 1 + δυ -∥β * ∥ 2 υ 2 , υ(0) = a0 w ⊺ 0 β * .<label>(32)</label></formula><p>These dynamics have a solution similar to Eq. ( <ref type="formula" target="#formula_49">28</ref>), which we leave to the reader. With υ(t), we can then solve for the dynamics of w ⊺ β * . Let ω = w ⊺ β * , then ω evolves according to the Bernoulli equation,</p><formula xml:id="formula_57">ω = ω υ∥β∥ 2 -υ 2 ω 2 , ω(0) = w(0) ⊺ β * ,<label>(33)</label></formula><p>which can be solved analytically using υ(t), analogous to the solution for a(t) in Appendix A.2.3.</p><p>From the solutions for υ, ω, we can easily obtain dynamics for µ, ϕ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.5 Basins of attraction</head><p>From Lemma A.1 we know that a can cross zero no more than once during its trajectory. Consequently, we can identify the basin of attraction by determining the conditions under which a changes sign. This analysis is crucial because initial conditions leading to a sign change in a correspond to scenarios where initial positive and negative values of a 0 are drawn towards the negative and positive branches of the minima manifold, respectively. From Eq. ( <ref type="formula" target="#formula_49">28</ref>) we can immediately see that a will change sign when the denominator vanishes. This can happen if</p><formula xml:id="formula_58">δ 2 + 4∥β * ∥ 2 &lt; -2ν 0 -δ. For δ &lt; 0, this is satisfied if ν 0 &lt; 1 2 -δ -δ 2 + 4∥β * ∥ 2 , which gives the hyperplane w ⊺ 0 β * + a0 2 δ + δ 2 + 4∥β * ∥ 2 =</formula><p>0 that separates between initializations for which a changes sign and initializations for which it does not (Fig. <ref type="figure" target="#fig_9">7</ref>). Consequently, letting S + be the set of initializations attracted to the minimum manifold with a &gt; 0, we have that:</p><formula xml:id="formula_59">S + = (w 0 , a 0 ) a 0 &gt; 0 if δ ≥ 0 w ⊺ 0 β * &gt; -a0 2 δ + δ 2 + 4∥β * ∥ 2 if δ &lt; 0 (<label>34</label></formula><formula xml:id="formula_60">)</formula><p>where the bottom inequality means that β 0 is sufficiently aligned to β * in the case of a 0 ≥ 0 or sufficiently misaligned in the case of a 0 ≤ 0. We can similarly define the analogous S -. An initialization on the separating hyperplane will converge to a saddle point where w ⊺ β * = a = 0. δ + δ 2 + 4∥β * ∥ 2 = 0. For a given δ, this equation describes a hyperplane through the origin. However, a given δ can only be achieved on the surface of some hyperboloid. Thus, the separating surface is the union of the intersections of a hyperplane and a hyperboloid, both parameterized by δ. This intersection is empty if δ &gt; 0. Initializations exactly on the separating surface will travel along the surface to a saddle point where w ⊺ β * = a = 0.</p><p>A.2.6 Recovering parameters (a, w) from (µ, ϕ)</p><p>We now discuss how to recover the dynamics of the parameters (a, w) from our solutions for (µ, ϕ). We can recover a and ∥w∥ from µ. Using Eq. ( <ref type="formula" target="#formula_38">19</ref>) discussed previously, we can show a = sgn(µ)</p><formula xml:id="formula_61">δ 2 + 4µ 2 + δ 2 , ∥w∥ = δ 2 + 4µ 2 -δ 2 . (<label>35</label></formula><formula xml:id="formula_62">)</formula><p>We now discuss how to obtain the vector w from ϕ. The key observation, as discussed in Section 3, is that w only moves in the span of w 0 and β * . This means we can express w(t) as</p><formula xml:id="formula_63">w(t) = c 1 (t) β * ∥β * ∥ + c 2 (t)   I d - β * β ⊺ * ∥β * ∥ 2 w0 ∥w0∥ 2 - β ⊺ * w 0 ∥β * ∥ 2  <label>(36)</label></formula><p>where c 1 (t) is the coefficient in the direction of β * and c 2 (t) is the coefficient in the direction orthogonal to β * on the two-dimensional plane defined by w 0 . From the definition of ϕ we can easily obtain the coefficients c 1 = ∥w∥ϕ and c 2 = ∥w∥ 2 -c 2 1 . We always choose the positive square root for c 2 , as c 2 (t) ≥ 0 for all t. See Appendix D.2 for experimental details of how we ran our simulations and a notebook generating these exact solutions.   The true β * is a unit vector pointing in π/4 direction; β(0) is a unit vector pointing towards 3π/2, -π/4, and π/4 directions, respectively, for each of the three rows. δ then defines how a(0) and ∥w(0)∥ are chosen for a particular β(0) where by convention we choose a(0) &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Function space dynamics of β</head><p>The network's function is determined by the product β = aw and governed by the ODE,</p><formula xml:id="formula_64">β = a ẇ + ȧw = -η w a 2 I d + η a ww ⊺ M (X ⊺ Xβ -X ⊺ y) X ⊺ ρ . (<label>37</label></formula><formula xml:id="formula_65">)</formula><p>Notice, that the vector X ⊺ ρ driving the dynamics of β is the gradient of the loss with respect to β, X ⊺ ρ = ∇ β L. Thus, these dynamics can be interpreted as preconditioned gradient flow on the loss in β space where the preconditioning matrix M depends on time through its dependence on a 2 and ww ⊺ . The matrix M also characterizes the NTK matrix, K = XM X ⊺ . As discussed in Section 3, our goal is to understand the evolution of M along a trajectory {β(t) ∈ R d : t ≥ 0} solving Eq. <ref type="bibr" target="#b36">(37)</ref>.</p><p>First, notice that by expanding ∥β∥ 2 = a 2 ∥w∥ 2 in terms of the conservation law, we can show</p><formula xml:id="formula_66">a 2 = δ 2 + 4η a η w ∥β∥ 2 + δ 2η w ,<label>(38)</label></formula><p>which is the unique positive solution of the quadratic expression η w a 4 -δa 2 -η a ∥β∥ 2 = 0. When a 2 &gt; 0 we can use this solution and the outer product ββ ⊺ = a 2 ww ⊺ to solve for ww ⊺ in terms of β,</p><formula xml:id="formula_67">ww ⊺ = δ 2 + 4η a η w ∥β∥ 2 -δ 2η a ββ ⊺ ∥β∥ 2 . (<label>39</label></formula><formula xml:id="formula_68">)</formula><p>Plugging these expressions into M gives</p><formula xml:id="formula_69">M = δ 2 + 4η a η w ∥β∥ 2 + δ 2 I d + δ 2 + 4η a η w ∥β∥ 2 -δ 2 ββ ⊺ ∥β∥ 2 . (<label>40</label></formula><formula xml:id="formula_70">)</formula><p>Thus, given any initialization a 0 , w 0 such that a(t) 2 ̸ = 0 for all t ≥ 0, we can express the dynamics of β entirely in terms of β. This is true for all initializations with δ ≥ 0, except if initialized on the saddle point at the origin. It is also true for all initializations with δ &lt; 0 where the sign of a does not switch signs. In the next section we will show how to interpret these trajectories as time-warped mirror flows for a potential that depends on δ. As a means of keeping the analysis entirely in β space, we will make the slightly more restrictive assumption to only study trajectories given any initialization β 0 such that ∥β(t)∥ &gt; 0 for all t ≥ 0.</p><p>Notice, that η a and η w only appear in the dynamics for β as the product η a η w or in the expression for δ. By defining β ′ = √ η a η w β and y ′ = √ η a η w y and studying the dynamics of β ′ , we can absorb η a η w into the β terms in M and the additional factor √ η a η w into the β and y terms in ρ. This transformation of β and y merely rescales β space without changing the loss landscape or location of critical points. As a result, from here on we will, without loss of generality, study the dynamics of β assuming η a η w = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Kernel dynamics</head><p>The dynamics of the NTK matrix K = XM X ⊺ is determined by Ṁ . From Eq. ( <ref type="formula" target="#formula_4">4</ref>), which is derived in this section, we can write Ṁ = 2∥β∥ κ (I d + β β⊺ )∂ t ∥β∥ + κ-δ 2 ∂ t ( β β⊺ ) where β = β ∥β∥ . From this expression we see that the change in M is driven by two terms, one that depends on the change in the magnitude of β and another that depends on the change in the direction of β. As done in the main text, we consider δ ≫ 0, δ ≪ 0, and δ = 0 to identify different regimes of learning. For δ ≫ 0, the coefficients in front of both terms vanish, and thus, irrespective of the trajectory taken from β(0) to β * , the change in the NTK is vanishing, indicative of a lazy regime. For δ ≪ 0, the coefficient for the first term vanishes, while the coefficient on the second term diverges. Here, the change in the NTK is driven solely by the change in the direction of β. This is why for large negative delta we observe a delayed rich regime, where the eventual alignment of β to β * leads to a dramatic change in the kernel. When δ = 0, the coefficients for both terms are roughly of the same order, and thus changes in both the magnitude and direction of β contribute to a change in the kernel, indicative of a rich regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Deriving the inductive bias</head><p>Until now, we have primarily considered that X ⊺ X is either whitened or full rank, ensuring the existence of a unique least squares solution β * . In this setting, δ influences the trajectory the model takes from initialization to convergence, but all models eventually converge to the same point, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. Now we consider the over-parameterized setting where we have more features d than observations n such that X ⊺ X is low-rank and there exists infinitely many interpolating solutions in function space. By studying the structure of M we can characterize or even predict how δ determines which interpolating solution the dynamics converge to among all possible interpolating solutions. To do this we will extend a time-warped mirror flow analysis strategy pioneered by Azulay et al. <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Overview of time-warped mirror flow analysis</head><p>Here we recap the standard analysis for determining the implicit bias of a linear network through mirror flow. As first introduced in Gunasekar et al. <ref type="bibr" target="#b45">[46]</ref>, if the learning dynamics of the predictor β can be expressed as a mirror flow for some strictly convex potential Φ α (β),</p><formula xml:id="formula_71">β = -∇ 2 Φ α (β) -1 X ⊺ ρ,<label>(41)</label></formula><p>where ρ = (Xβ -y) is the residual, then the limiting solution of the dynamics is determined by the constrained optimization problem,</p><formula xml:id="formula_72">β(∞) = arg min β∈R d D Φα (β, β(0)) s.t. Xβ = y,<label>(42)</label></formula><p>where D Φα (p, q) = Φ α (p) -Φ α (q) -⟨∇Φ α (q), p -q⟩ is the Bregman divergence defined with Φ α .</p><p>To understand the relationship between mirror flow Eq. ( <ref type="formula" target="#formula_71">41</ref>) and the optimization problem Eq. ( <ref type="formula" target="#formula_72">42</ref>), we consider an equivalent constrained optimization problem</p><formula xml:id="formula_73">β(∞) = arg min β∈R d Q(β) s.t. Xβ = y,<label>(43)</label></formula><p>where</p><formula xml:id="formula_74">Q(β) = Φ α (β) -∇Φ α (β(0)) ⊺ β</formula><p>, which is often referred to as the implicit bias. Q(β) is strictly convex, and thus it is sufficient to show that β(∞) is a first order KKT point of the constrained optimization <ref type="bibr" target="#b42">(43)</ref>. This is true iff there exists ν ∈ R n such that ∇Q(β(∞)) = X ⊺ ν. The goal is to derive ν from the mirror flow Eq. ( <ref type="formula" target="#formula_71">41</ref>). Notice, we can rewrite Eq. ( <ref type="formula" target="#formula_71">41</ref>) as, (∇Φ α (β)) = -X ⊺ ρ, which integrated over time gives</p><formula xml:id="formula_75">∇Φ α (β(∞)) -∇Φ α (β(0)) = -X ⊺ ∞ 0 ρ(t)dt. (<label>44</label></formula><formula xml:id="formula_76">)</formula><p>The LHS is ∇Q(β(∞)). Thus, by defining ν = ∞ 0 ρ(t)dt, which assumes the residual decays fast enough such that this is well defined, then we have shown the desired KKT condition. Crucial to this analysis is that there exists a solution to the second-order differential equation</p><formula xml:id="formula_77">∇ 2 Φ α (β) = (∇ θ β∇ θ β ⊺ ) -1 ,<label>(45)</label></formula><p>which even for extremely simple Jacobian maps may not be true <ref type="bibr" target="#b46">[47]</ref>. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> showed that if there exists a smooth positive function g(β) : R d → (0, ∞) such that the ODE,</p><formula xml:id="formula_78">∇ 2 Φ α (β) = g(β) (∇ θ β∇ θ β ⊺ ) -1 ,<label>(46)</label></formula><p>has a solution, then the previous interpretation holds for Φ α (β) with ν = ∞ 0 g(β(t ′ ))ρ(t ′ )dt. As before, it is crucial that this integral exists and is finite. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> further explained that this scalar function g(β) can be considered as warping time τ (t) = t 0 g(β(t ′ ))dt ′ on the trajectory taken in predictor space β(τ (t)). So long as this warped time doesn't "stall out", that is we require that τ (∞) = ∞, then this will not change the interpolating solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Applying time-warped mirror flow analysis</head><p>Here show how to apply the time-warped mirror flow analysis to the dynamics of β derived in Appendix A.3 where ∇ θ β∇ θ β ⊺ = M . We will only consider initializations β 0 such that ∥β(t)∥ &gt; 0 for all t ≥ 0, such that M can be expressed as</p><formula xml:id="formula_79">M = δ 2 + 4∥β∥ 2 + δ 2 I d + δ 2 + 4∥β∥ 2 -δ 2 ββ ⊺ ∥β∥ 2 . (<label>47</label></formula><formula xml:id="formula_80">)</formula><p>Computing M -1 . Whenever ∥β∥ &gt; 0, then M is a positive definite matrix with a unique inverse that can be derived using the Sherman-Morrison formula,</p><formula xml:id="formula_81">(A + uv ⊺ ) -1 = A -1 -A -1 uv ⊺ A -1 1+u ⊺ A -1 v .</formula><p>Here we can define A, u, and v as</p><formula xml:id="formula_82">A = δ 2 + 4∥β∥ 2 + δ 2 I d , u = δ 2 + 4∥β∥ 2 -δ 2∥β∥ 2 β, v = β (<label>48</label></formula><formula xml:id="formula_83">)</formula><p>First notice the following simplification,</p><formula xml:id="formula_84">u ⊺ A -1 v = √ δ 2 +4∥β∥ 2 -δ √ δ 2 +4∥β∥ 2 +δ</formula><p>. After some algebra, M -1 is</p><formula xml:id="formula_85">M -1 = 2 δ 2 + 4∥β∥ 2 + δ I d -     √ δ 2 +4∥β∥ 2 -δ √ δ 2 +4∥β∥ 2 +δ ∥β∥ 2 δ 2 + 4∥β∥ 2     ββ ⊺<label>(49)</label></formula><p>To make notation simpler we will define the following two scalar functions,</p><formula xml:id="formula_86">f δ (x) = 2 √ δ 2 + 4x + δ , h δ (x) = √ δ 2 + 4x -δ x √ δ 2 + 4x √ δ 2 + 4x + δ ,<label>(50)</label></formula><p>such that we can express</p><formula xml:id="formula_87">M -1 = f δ ∥β∥ 2 I d -h δ ∥β∥ 2 ββ ⊺ .</formula><p>Proving M -1 is not a Hessian map. If M -1 is the Hessian of some potential, then we can show that the dynamics of β are a mirror flow. However, from our expression for M -1 we can actually prove that it is not a Hessian map. As discussed in Gunasekar et al. <ref type="bibr" target="#b46">[47]</ref>, a symmetric matrix H(β) is the Hessian of some potential Φ(β) if and only if it satisfies the condition,</p><formula xml:id="formula_88">∀β ∈ R m , ∀i, j, k ∈ [m] ∂H ij (β) ∂β k = ∂H ik (β) ∂β j . (<label>51</label></formula><formula xml:id="formula_89">)</formula><p>We will use this property to show M -1 is not a Hessian map. First, notice this condition is trivially true when i = j = k. Second, notice that for all i</p><formula xml:id="formula_90"≯ = j ̸ = k, ∂M -1 ij ∂β k = ∂M -1 ik ∂β j = -2∇h δ ∥β∥ 2 β i β j β k<label>(52)</label></formula><p>Thus, M -1 is a Hessian map if and only if for all i ̸ = j,</p><formula xml:id="formula_91">∂M -1 ii ∂βj = ∂M -1 ij ∂βi . Using our expression for M -1 , the LHS is ∂M -1 ii ∂β j = 2∇f δ ∥β∥ 2 β j -2∇h δ ∥β∥ 2 β j β 2 i (<label>53</label></formula><formula xml:id="formula_92">)</formula><p>while the RHS is</p><formula xml:id="formula_93">∂M -1 ij ∂β i = -h δ ∥β∥ 2 β j -2∇h δ ∥β∥ 2 β j β 2 i (<label>54</label></formula><formula xml:id="formula_94">)</formula><p>Thus, M -1 is a Hessian map if and only if 2∇f δ (x) + h δ (x) = 0. Plugging in our definitions of f δ (x) and h δ (x) we find</p><formula xml:id="formula_95">2∇f δ (x) + h δ (x) = -4 √ δ 2 + 4x( √ δ 2 + 4x + δ) 2 , (<label>55</label></formula><formula xml:id="formula_96">)</formula><p>which does not equal zero and thus M -1 is not a Hessian map.</p><p>Finding a scalar function g δ (x) such that g δ (∥β∥ 2 )M -1 is a Hessian map. While we have shown that M -1 is not a Hessian map, it is very close to a Hessian map. Here we will show that there exists a scalar function g δ (x) such that g δ ∥β∥ 2 M -1 is a Hessian map. For any g δ (x) can define g δ ∥β∥ 2 M -1 in terms of two new functions fδ (x) and hδ (x) evaluated at x = ∥β∥ 2 ,</p><formula xml:id="formula_97">g δ ∥β∥ 2 M -1 = g δ ∥β∥ 2 f δ ∥β∥ 2 fδ (∥β∥ 2 ) I d -g δ ∥β∥ 2 h δ ∥β∥ 2 hδ (∥β∥ 2 ) ββ ⊺ . (<label>56</label></formula><formula xml:id="formula_98">)</formula><p>Thus, as derived in the previous section, we get the analogous condition on fδ (x) and hδ (x) for g δ ∥β∥ 2 M -1 to be a Hessian map,</p><formula xml:id="formula_99">2 (∇g δ (x)f δ (x) + g(x)∇f δ (x)) ∇ fδ (x) + g δ (x)h δ (x) hδ (x) = 0<label>(57)</label></formula><p>Rearranging terms we find that g δ (x) must solve the ODE</p><formula xml:id="formula_100">∇g δ (x) = -(2f δ (x)) -1 (2∇f δ (x) + h δ (x)) g δ (x). (<label>58</label></formula><formula xml:id="formula_101">)</formula><p>Using our previous expressions (Eq. ( <ref type="formula" target="#formula_86">50</ref>) and Eq. ( <ref type="formula" target="#formula_95">55</ref>)) we find</p><formula xml:id="formula_102">-(2f δ (x)) -1 (2∇f δ (x) + h δ (x)) = 1 √ δ 2 + 4x( √ δ 2 + 4x + δ) ,<label>(59)</label></formula><p>which implies g δ (x) solves the differential equation, ∇g δ (x) =</p><formula xml:id="formula_103">g δ (x) √ δ 2 +4x( √ δ 2 +4x+δ) . The solution is g δ (x) = c √ δ 2 + 4x + δ, where c ∈ R is a constant. Let c = 1.</formula><p>Plugging in our expressions for g δ ∥β∥ 2 , f δ ∥β∥ 2 , h δ ∥β∥ 2 , we get that</p><formula xml:id="formula_104">g δ ∥β∥ 2 M -1 =   2 δ 2 + 4∥β∥ 2 + δ   I d -     √ δ 2 +4∥β∥ 2 -δ √ δ 2 +4∥β∥ 2 +δ ∥β∥ 2 δ 2 + 4∥β∥ 2     ββ ⊺ (60)</formula><p>is a Hessian map for some unknown potential Φ δ (β).</p><p>Solving for the potential Φ δ (β). Take the ansatz that there exists some function scalar q(x) such that Φ δ (β) = q δ (∥β∥) + c δ where c δ is a constant such that Φ δ (β) &gt; 0 for all β ̸ = 0 and Φ δ (0) = 0. The Hessian of this ansatz takes the form,</p><formula xml:id="formula_105">∇ 2 Φ δ (β) = ∇q(∥β∥) ∥β∥ I d - ∇q(∥β∥) ∥β∥ 3 - ∇ 2 q(∥β∥) ∥β∥ 2 ββ ⊺ . (<label>61</label></formula><formula xml:id="formula_106">)</formula><p>Equating terms from our expression for g δ ∥β∥ 2 M -1 (equation 60) we get the expression for ∇q(∥β∥) ∇q(∥β∥) = 2∥β∥</p><formula xml:id="formula_107">δ 2 + 4∥β∥ 2 + δ ,<label>(62)</label></formula><p>which plugged into the second term gives the expression for ∇ 2 q(∥β∥),</p><formula xml:id="formula_108">∇ 2 q(∥β∥) = 2 δ 2 + 4∥β∥ 2 + δ -     √ δ 2 +4∥β∥ 2 -δ √ δ 2 +4∥β∥ 2 +δ δ 2 + 4∥β∥ 2     = δ 2 + 4∥β∥ 2 + δ δ 2 + 4∥β∥ 2 . (<label>63</label></formula><formula xml:id="formula_109">)</formula><p>We now look for a function q(x) such that both these conditions (Eq. ( <ref type="formula" target="#formula_107">62</ref>) and Eq. ( <ref type="formula" target="#formula_108">63</ref>)) are true. Consider the following function and its derivatives,</p><formula xml:id="formula_110">q(x) = 1 3 δ 2 + 4x 2 -2δ δ 2 + 4x 2 + δ (64) ∇q(x) = 2x √ δ 2 + 4x 2 + δ<label>(65)</label></formula><formula xml:id="formula_111">∇ 2 q(x) = √ δ 2 + 4x 2 + δ √ δ 2 + 4x 2<label>(66)</label></formula><p>Letting x = ∥β∥ notice ∇q(∥β∥) and ∇ 2 q(∥β∥) satisfies the previous conditions. Furthermore, ∇ 2 q(x) &gt; 0 for all δ as long as x ̸ = 0 and thus q(x) is a convex function which achieves its minimum at x = 0. Thus, the constant c δ = -q(0) is</p><formula xml:id="formula_112">c δ = 0 if δ ≤ 0 √ 2|δ| 3 2 3 if δ &gt; 0 = max 0, sgn(δ) √ 2|δ| 3 2 3 ,<label>(67)</label></formula><p>and the potential Φ δ (β) is</p><formula xml:id="formula_113">Φ δ (β) = 1 3 δ 2 + 4∥β∥ 2 -2δ δ 2 + 4∥β∥ 2 + δ + max 0, sgn(δ) √ 2|δ| 3 2 3 . (<label>68</label></formula><formula xml:id="formula_114">)</formula><p>Finally, putting it all together, we can express the inductive bias as in Theorem 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Connection to Theorem 2 in Azulay et al. [9]</head><p>We discuss how Theorem 3.1 connects to Theorem 2 in Azulay et al. <ref type="bibr" target="#b8">[9]</ref>, which we rewrite: Theorem A.2 (Theorem 2 from Azulay et al. <ref type="bibr" target="#b8">[9]</ref>). For a depth 2 fully connected network with a single hidden neuron (h = 1), any δ ≥ 0, and initialization β 0 such that β 0 ̸ = 0, if the gradient flow solution β(∞) satisfies Xβ(∞) = y, then, β(∞) = arg min</p><formula xml:id="formula_115">β∈R d q δ (∥β∥) + z ⊺ β s.t. Xβ = y (<label>69</label></formula><formula xml:id="formula_116">)</formula><p>where q δ (x) =</p><formula xml:id="formula_117">x 2 -δ 2 δ 2 + x 2 + δ 2 4 x 2 + δ 2 4 -δ 2 x and z = -3 2 ∥β 0 ∥ 2 + δ 2 4 -δ 2 β0 ∥β0∥ .</formula><p>The most striking difference is in the expressions for the inductive bias. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> take an alternative route towards deriving the inductive bias by inverting M in terms of the original parameters a and w and then simplifying M -1 in terms of β, which results in quite a different expression for their inductive bias. However, they are actually functionally equivalent. It requires a bit of algebra, but one can show that</p><formula xml:id="formula_118">Φ δ (β) = 2 √ 2 3 q δ (∥β∥) + c δ . (<label>70</label></formula><formula xml:id="formula_119">)</formula><p>Another important distinction between our two theorems lies in the assumptions we make. Azulay et al. <ref type="bibr" target="#b8">[9]</ref> consider only initializations such that δ ≥ 0 and β 0 ̸ = 0. We make a less restrictive assumption by considering initializations β 0 such that ∥β(t)∥ &gt; 0 for all t ≥ 0, which allows for both positive and negative δ. Except for a measure zero set of initializations, all initializations considered by Azulay et al. <ref type="bibr" target="#b8">[9]</ref> also satisfy our assumptions. In both cases, our assumptions ensure that M is invertible for the entire trajectory from initialization to interpolating solution. However, it is worth considering whether the theorems would hold even when there exists a point on the trajectory where M is low-rank. As discussed in Appendix A.3, this can only happen for an initialization with δ &lt; 0 and where the sign of a changes. Only at the point where a(t) = 0 does M become low-rank. A similar challenge arose in this setting when deriving the exact solutions presented in Appendix A.2.4. We were able to circumvent the issue in part by introducing Lemma A.1 proving that this sign change could only happen at most once given any initialization. This lemma was based on the setting with whitened input, but a similar statement likely holds for the general setting. If this were the case, we could define M at this unique point on the trajectory in terms of the limit of M as it approached this point. This could potentially allow us to extend the time-warped mirror flow analysis to all initializations such that ∥β 0 ∥ &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.4 Exact solution when interpolating manifold is one-dimensional</head><p>When the null space of X ⊺ X is one-dimensional, the constrained optimization problems in Theorem 3.1 and Theorem A.2 have an exact analytic solution. In this case we can parameterize all interpolating solutions β with a single scalar α ∈ R such that β = β * + αv where X ⊺ Xv = 0 and ∥v∥ = 1. Using this description of β, we can then differentiate the inductive bias with respect to α, set to zero, and solve for α. We will use the following expressions,</p><formula xml:id="formula_120">∇ x q(x) = 3 2 sign(x) x 2 + δ 2 4 - δ 2 , ∇ α ∥β∥ = α ∥β∥ , ∇ α z ⊺ β = z ⊺ v.<label>(71)</label></formula><p>We will also use the expression, ∥β∥ 2 = ∥β * ∥ 2 + α 2 . Pulling these expressions together we get the following equation for α,</p><formula xml:id="formula_121">∥β * ∥ 2 + α 2 + δ 2 4 - δ 2 α ∥β * ∥ 2 + α 2 = - 2z ⊺ v 3 .<label>(72)</label></formula><p>If we let k = -2z ⊺ v 3 , the solution for α is</p><formula xml:id="formula_122">α = k k 2 + δ 2 + k 2 + δ 2 2 + ∥β * ∥ 2 . (<label>73</label></formula><formula xml:id="formula_123">)</formula><p>This solution always works for the initializations we considered in Theorem 3.1. Interestingly, it appears that β = β * -αv also works for initializations not previously considered. This includes trajectories that pass through the origin, resulting in a change in the sign of a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Wide and Deep Linear Networks</head><p>Here we discuss how our analysis techniques, developed in the previous section for a single-neuron linear network, can be extended to linear networks with multiple neurons, outputs, and layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Wide linear networks</head><p>We consider the dynamics of a two-layer linear network with h hidden neurons and c outputs, f (x; θ) = A ⊺ W x, where W ∈ R h×d and A ∈ R h×c . We assume that h ≥ min(d, c), such that this parameterization can represent all linear maps from R d → R c . As in the single-neuron setting, the rescaling symmetry in this model between the first and second layer implies the h × h matrix ∆ = A 0 A ⊺ 0 -W 0 W ⊺ 0 determined at initialization remains conserved throughout gradient flow <ref type="bibr" target="#b61">[62]</ref>. This can be easily shown from the temporal dynamics of A and W ,</p><formula xml:id="formula_124">Ȧ = -η a W X ⊺ (Xβ -Y ),<label>(74)</label></formula><formula xml:id="formula_125">Ẇ ⊺ = -η w X ⊺ (Xβ -Y )A ⊺ .<label>(75)</label></formula><p>Extending derivations in <ref type="bibr" target="#b29">[30]</ref>, the NTK matrix can be expressed as</p><formula xml:id="formula_126">K = (I c ⊗ X) (η w A ⊺ A ⊕ η a W ⊺ W ) (I c ⊗ X ⊺ ) ,<label>(76)</label></formula><p>where ⊗ and ⊕ denote the Kronecker product and sum respectively. The Kronecker sum is defined for square matrices</p><formula xml:id="formula_127">C ∈ R c×c and D ∈ R d×d as C ⊕ D = C ⊗ I d + I c ⊗ D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Parameter space dynamics</head><p>Inspired by our analysis of the single-neuron setting, we introduce two coordinate transformations to study the parameter space dynamics of a wide two-layer linear network. In both analyses we assume whitened input X ⊺ X = I d and let η a = η w = 1. However, we will find that the analysis of the dynamics in function space, for general unwhitened data, is more tractable.</p><p>Parameter dynamics when c = 1. Drawing insights from our analysis of the single-neuron scenario (h = c = 1), we might consider a combination of hyperbolic and spherical coordinate transformations to study the parameter space dynamics of a wide two-layer linear network. We consider the following two quantities for each hidden neuron k ∈ [h]:</p><formula xml:id="formula_128">µ k = a k ∥w k ∥, ϕ k = w ⊺ k β * ∥w k ∥∥β * ∥ .<label>(77)</label></formula><p>We will also consider a new matrix quantity Q ∈ R h×h with elements</p><formula xml:id="formula_129">Q kk ′ = w ⊺ k w k ′ ∥w k ∥∥w k ′ ∥ .</formula><p>The resulting dynamics for µ and ϕ can be entirely written in terms µ, ϕ, ∆:</p><formula xml:id="formula_130">μ = Diag(∆) 2 + 4Diag(µ) 2 (ϕ -Qµ) ,<label>(78) φ</label></formula><formula xml:id="formula_131">= M Diag(µ) (∥β * ∥ 2 -ϕ ⊺ µ)I h + Diag(ϕ)Qµ -ϕ 2 ,<label>(79)</label></formula><p>where</p><formula xml:id="formula_132">M = 2 Diag(∆) 2 + 4Diag(µ) 2 -Diag(∆) -1</formula><p>. Using the conserved structure of ∆ we can express Q as a function of µ and M ,</p><formula xml:id="formula_133">Q = M µµ ⊺ M -M 1/2 ∆M 1/2 . (<label>80</label></formula><formula xml:id="formula_134">)</formula><p>This approach yields a coupled nonlinear dynamical system with 2h variables. Imposing additional assumptions on the initialization, such as permutation invariance between hidden neurons, can simplify the system of differential equations. A similar approach was used by Saad and Solla <ref type="bibr" target="#b77">[78]</ref> to derive a set of differential equations for a soft committee machine model, capturing its online learning dynamics in a teacher-student setup, which Goldt et al. <ref type="bibr" target="#b78">[79]</ref> extended to its generalization error dynamics.</p><p>Parameter dynamics when c = h. In this analysis we assume an initialization such that the conserved quantities ∆ = δI h , an assumption we will discuss further in Appendix B.1.6, and that A is invertible throughout training. Let β * = X ⊺ Y , which for whitened input, is the unique minimum of the dynamics in function space. We consider the variable ν = A -1 W β * ∈ R c×c . Using the identity that Ȧ -1 = -A -1 ȦA -1 and our assumption on ∆, we find that the matrix ν evolves according to the matrix Riccati ODE,</p><formula xml:id="formula_135">ν = β ⊺ * β * -δν -ν 2 .<label>(81)</label></formula><p>Additionally, consider the variable C = A ⊺ A, which evolves according to the matrix Bernoulli ODE</p><formula xml:id="formula_136">Ċ = C(ν + δI h ) + (ν + δI h ) ⊺ C -2C 2 . (<label>82</label></formula><formula xml:id="formula_137">)</formula><p>Taken together we have found a change of variables, analogous to the one introduced in Appendix A.2.3 for the single-neuron setting, that evolves according to a matrix Riccati and Bernoulli equation,</p><formula xml:id="formula_138">ν = β ⊺ * β * -δν -ν 2 , ν(0) = A -1 0 W 0 β * ,<label>(83)</label></formula><formula xml:id="formula_139">Ċ = C(ν + δI h ) + (ν + δI h ) ⊺ C -2C 2 , C(0) = A ⊺ 0 A 0 .<label>(84)</label></formula><p>However, solving this system exactly as we did in the single-neuron setting is challenging. Unless we assume that ν and β ⊺ * β * share the same eigenspace -allowing us to decouple the dynamics of ν into a set of scalar Riccati equations -the system cannot be easily solved. Instead, we will find that the dynamics of the product W ⊺ A in function space is more tractable and requires fewer assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Function space dynamics</head><p>We consider the dynamics of β = W ⊺ A ∈ R d×c in function space, which is governed by the ODE,</p><formula xml:id="formula_140">β = W ⊺ Ȧ + Ẇ ⊺ A = -(η w X ⊺ (Xβ -Y )A ⊺ A + η a W ⊺ W X ⊺ (Xβ -Y )) .<label>(85)</label></formula><p>Vectorizing using the identity vec(ABC) = (C ⊺ ⊗ A)vec(B) equation 85 becomes</p><formula xml:id="formula_141">vec β = -vec (η w I d X ⊺ (Xβ -Y )A ⊺ A + η a W ⊺ W X ⊺ (Xβ -Y )I c ) ,<label>(86)</label></formula><formula xml:id="formula_142">= -(η w A ⊺ A ⊗ I d + η a I c ⊗ W ⊺ W )vec(X ⊺ Xβ -X ⊺ Y ),<label>(87)</label></formula><formula xml:id="formula_143">= -(η w A ⊺ A ⊕ η a W ⊺ W ) M vec(X ⊺ Xβ -X ⊺ Y ).<label>(88)</label></formula><p>As in the single-neuron setting, we find that the dynamics of β can be expressed as gradient flow preconditioned by a matrix M that depends on quadratics of A and W . </p><formula xml:id="formula_144">⊺ k β k ∈ R c×c and β k β ⊺ k ∈ R d×d , β ⊺ k β k = ∥w k ∥ 2 a k a ⊺ k , β k β ⊺ k = ∥a k ∥ 2 w k w ⊺ k .<label>(89)</label></formula><p>Notice that we can express ∥β k ∥ 2 F as</p><formula xml:id="formula_145">∥β k ∥ 2 F = Tr(β ⊺ k β k ) = Tr(β k β ⊺ k ) = ∥a k ∥ 2 ∥w k ∥ 2<label>(90)</label></formula><p>At each hidden neuron we have the conserved quantity <ref type="foot" target="#foot_7">8</ref> </p><formula xml:id="formula_146">η w ∥a k ∥ 2 -η a ∥w k ∥ 2 = δ k where δ k ∈ R.</formula><p>Using this quantity we can invert the expression for ∥β k ∥ 2 F to get</p><formula xml:id="formula_147">∥a k ∥ 2 = δ 2 k + 4η a η w ∥β k ∥ 2 F + δ k 2η w ,<label>(91)</label></formula><formula xml:id="formula_148">∥w k ∥ 2 = δ 2 k + 4η a η w ∥β k ∥ 2 F -δ k 2η a .<label>(92)</label></formula><p>When ∥β k ∥ 2 F &gt; 0, we can use these expressions to solve for the outer products a k a ⊺ k and w k w ⊺ k in terms of β k and δ k ,</p><formula xml:id="formula_149">a k a ⊺ k = δ 2 k + 4η a η w ∥β k ∥ 2 F + δ k 2η w β ⊺ k β k ∥β k ∥ 2 F ,<label>(93)</label></formula><formula xml:id="formula_150">w k w ⊺ k = δ 2 k + 4η a η w ∥β k ∥ 2 F -δ k 2η a β k β ⊺ k ∥β k ∥ 2 F .<label>(94)</label></formula><p>By substituting these expressions into the decompositions </p><formula xml:id="formula_151">A ⊺ A = h k=1 a k a ⊺ k and W ⊺ W = h k=1 w k w ⊺ k ,</formula><formula xml:id="formula_152">M k = δ 2 k + 4η a η w ∥β k ∥ 2 F + δ k 2 β ⊺ k β k ∥β k ∥ 2 F ⊕ δ 2 k + 4η a η w ∥β k ∥ 2 F -δ k 2 β k β ⊺ k ∥β k ∥ 2 F . (95) B.1.4 Understanding M when there is a single-neuron h = 1</formula><p>When there is a single-hidden neuron h = min(d, c) = 1, the expression for M presented in Theorem 4.1 simplifies allowing us to precisely understand the influence of δ on the learning regime. When h = c = 1, then β ⊺ β ∥β∥ 2 F = 1. Therefore, Eq. ( <ref type="formula" target="#formula_9">7</ref>) simplifies to</p><formula xml:id="formula_153">M = δ 2 + η a η w 4∥β∥ 2 + δ 2 I d + δ 2 + η a η w 4∥β∥ 2 -δ<label>2</label></formula><formula xml:id="formula_154">ββ ⊺ ∥β∥ 2 ,<label>(96)</label></formula><p>and we recover Eq. ( <ref type="formula" target="#formula_4">4</ref>) presented in Section 3. When h = d = 1, then ββ ⊺ ∥β∥ 2 F = 1 and thus Eq. ( <ref type="formula" target="#formula_9">7</ref>) simplifies to,</p><formula xml:id="formula_155">M = δ 2 + η a η w 4∥β∥ 2 + δ 2 β ⊺ β ∥β∥ 2 + δ 2 + η a η w 4∥β∥ 2 -δ 2 I c .<label>(97)</label></formula><p>In both settings, M is the weighted sum of the identity matrix and a rank-one projection matrix. While these equations are strikingly similar there is an interesting distinction that arises in the limits of δ. As δ → ∞, then the first expression for M becomes proportional to I d , while the second expression for M becomes proportional to the rank-1 projection β ⊺ β ∥β∥ 2 . Conversely, as δ → -∞, then the first expression for M becomes proportional to the rank-1 projection ββ ⊺ ∥β∥ 2 , while the second expression for M becomes proportional to I c . When h = d = c = 1, then M = δ 2 + η a η w 4∥β∥ 2 and thus in both limits of δ → ±∞, M becomes a constant independent of β. In all settings, when δ = 0, M depends on β. In other words, the influence of δ on whether the dynamics are lazy, rich, or delayed rich, crucially depends on the relative sizes of dimensions d, h, and c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.5 Interpreting M in different limits and architectures</head><p>We now seek to more generally understand the influence of the conserved quantities δ i and the relative sizes of dimensions d, h and c on the learning regime. For a matrix A ∈ R d×c , let Row(A) ⊆ R c and Col(A) ⊆ R d denote the row and column space of A respectively.</p><p>Theorem B.1. The dynamics are in the lazy regime, for all t ≥ 0, if δ k → ∞ for all k ∈ [h] and there exists a least squares solution β * ∈ R d×c such that</p><formula xml:id="formula_156">Row(β * ) ⊆ Span h k=1 Row (β k (0)) ,<label>(98)</label></formula><p>or δ k → -∞ for all k ∈ [h] and there exists a solution such that</p><formula xml:id="formula_157">Col(β * ) ⊆ Span h k=1 Col (β k (0)) . (<label>99</label></formula><formula xml:id="formula_158">) Proof. As δ k → ∞, M k → |δ k | β ⊺ k β k ∥β k ∥ 2 F ⊗I d , implying βk = -|δ k | ∂L ∂β β ⊺ k β k ∥β k ∥ 2 F . Notice that β ⊺ k β k ∥β k ∥ 2 F</formula><p>is the unique orthogonal projection matrix onto the one-dimensional row space of β k . Thus, the dynamics of each β k follow a projected gradient descent in their row space. As a result, M k will not change and thus the NTK will be static. By assumption there exists a least squares solution β * such that the rows of β * are in the span of the rows of β k . Thus, a solution will be reached as t → ∞, while the M k remain static.</p><formula xml:id="formula_159">As δ k → -∞ for all k ∈ [h], M k → I c ⊗ |δ k | β k β ⊺ k ∥β k ∥ 2 F</formula><p>, and an analogous argument can be made.</p><p>Note that the assumptions in Theorem B.1 can be more intuitively expressed in terms of the parameter space (W, A). Except in highly degenerate cases, the assumption Row(β * ) ⊆ Span A direct consequence of Theorem B.1 is that networks which narrow from input to output (d &gt; c) must enter the lazy regime with probability 1 as all δ k → ∞ whenever h ≥ c and assuming independent initializations for all β k . In this case, the rows of {β 1 , . . . , β h } span all of R c and thus the condition on the least squares solution is trivially true. By the same logic, networks which expand from input to output (d &lt; c) do so as all δ k → -∞ whenever h ≥ d and assuming independent initializations for all β k . Additionally, when h ≥ max(d, c) and assuming independent initializations for all β k , then all networks enter the lazy regime as either all δ k → ∞ or all δ k → -∞.</p><p>Another interesting implication of Theorem B.1, is that if there does not exist a least squares solution β * with rows in the span of the rows of {β 1 , . . . , β h }, then the network will enter a delayed rich regime as all δ k → ∞, where the magnitude of the δ k will determine the delay. In this setting, the network is initially lazy, attempting to fit the solution within the row space of the β k , but eventually the direction of the rows must change in order to fit the problem, leading to a rich phase. A similar statement involving the columns of β * is true as all δ k → -∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.6 Simplifying M through assumptions on ∆</head><p>We now consider how introducing structures on ∆ can lead to simpler expressions for M . A natural assumption to consider is the following: Assumption B.2 (Isotropic initialization). Let A ∈ R h×c and W ∈ R h×d be initialized such that ∆ = η w A(0)A(0) ⊺ -η a W (0)W (0) ⊺ = δI h .</p><p>In square networks, where the dimensions of the input, hidden, and output layers coincide (d = h = c), and the weights are initialized as A ∼ N (0, σ 2 a /c) and W ∼ N (0, σ 2 w /d), this assumption is naturally satisfied with δ = σ 2 a -σ 2 w as the dimension h → ∞. However, a limitation of this assumption is that for general δ it requires h ≤ min(d, c). Specifically, when δ &gt; 0, the isotropic initialization requires that A(0)A(0) ⊺ ≻ 0, which implies h ≤ c. Similarly, when δ &lt; 0, the isotropic initialization requires that W (0)W (0) ⊺ ≻ 0, which implies h ≤ d. Now we prove two important implications of the isotropic initialization assumption. Lemma B.3. Let ∆ = δI h . If either δ ≥ 0 or δ &lt; 0 and h ≥ d, we have that</p><formula xml:id="formula_160">W ⊺ W = 1 η a - δ 2 I d + η a η w ββ ⊺ + δ 2 4 I d .<label>(100)</label></formula><p>Proof. The quantity η w AA ⊺ -η a W W ⊺ = δI h is conserved in gradient flow. Multiplying on the left by W ⊺ and on the right by W we have that</p><formula xml:id="formula_161">η a (W ⊺ W ) 2 + δW ⊺ W = η w ββ ⊺ . (<label>101</label></formula><formula xml:id="formula_162">)</formula><p>Completing the square by adding δ 2 4ηa I d to both sides and dividing by η a we get the equality,</p><formula xml:id="formula_163">W ⊺ W + δ 2η a I d 2 = δ 2 4η 2 a I d + η w η a ββ ⊺<label>(102)</label></formula><p>For δ ≥ 0, W ⊺ W + δ 2ηa I d ⪰ 0. For δ &lt; 0, then we know from the conserved quantity that W W ⊺ + δ 2ηa I h = ηw ηa AA ⊺ -δ 2ηa I h ≻ 0, which implies when h ≥ d that W ⊺ W + δ 2ηa I d ≻ 0. As a result, we can take the principal square root of each side,</p><formula xml:id="formula_164">W ⊺ W + δ 2η a I d = δ 2 4η 2 a I d + η w η a ββ ⊺ ,<label>(103)</label></formula><p>which rearranged gives the final result.</p><p>Lemma B.4. Let ∆ = δI h . If either δ ≤ 0 or δ &gt; 0 and h ≥ c, we have that</p><formula xml:id="formula_165">A ⊺ A = 1 η w δ 2 I c + η a η w β ⊺ β + δ 2 4 I c .<label>(104)</label></formula><p>Proof. The proof is analogous to the proof of Lemma B.3.</p><p>From Lemma B.3 and Lemma B.4 we can prove Theorem 4.2, as shown below.</p><p>Proof. We start from</p><formula xml:id="formula_166">vec β = -(η w A ⊺ A ⊕ η a W ⊺ W ) M vec(X ⊺ Xβ -X ⊺ Y ),<label>(105)</label></formula><p>Plugging in expressions for W ⊺ W from Lemma B.3 and A ⊺ A from Lemma B.4 we can directly write,</p><formula xml:id="formula_167">M = δ 2 I c + η a η w β ⊺ β + δ 2 4 I c ⊕ - δ 2 I d + η a η w ββ ⊺ + δ 2 4 I d (106) = η a η w β ⊺ β + δ 2 4 I c ⊗ I d + I c ⊗ η a η w ββ ⊺ + δ 2 4 I d<label>(107)</label></formula><p>From this expression for M (β) we can easily consider how it simplifies in limiting settings of δ:</p><formula xml:id="formula_168">M →    δI dc δ → -∞ √ η a η w β ⊺ β ⊗ I d + I c ⊗ √ η a η w ββ ⊺ δ = 0 δI dc δ → ∞.<label>(108)</label></formula><p>As δ → ±∞, M → δI dc , and the dynamics are lazy. In this limit, the dynamics of β converge to the trajectory of linear regression trained by gradient flow and along this trajectory the NTK matrix remains constant. When</p><formula xml:id="formula_169">δ = 0, M = √ η a η w β ⊺ β ⊗ I d + I c ⊗ √ η a η w ββ ⊺</formula><p>, and the dynamics are rich. Here the NTK changes in both magnitude and direction through training. In the next section we will attempt to better understand these dynamics for intermediate values of δ through the lens of a mirror flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.7 Deriving a mirror flow for the singular values of β</head><p>For a matrix β, the dynamics of one of its singular values are given by σ = u ⊺ βv, where u and v are the corresponding left and right singular vectors. This equality can be derived from chain rule and the fact that ∥u∥ = ∥v∥ = 1:</p><formula xml:id="formula_170">σ = u⊺ βv + u ⊺ βv + u ⊺ β v = u⊺ uσ + u ⊺ βv + σv ⊺ v = u ⊺ βv.<label>(109)</label></formula><p>In the last equality we used that fact that for any vector z with a fixed norm, ∥z∥ 2 = 2 ż⊺ z = 0.</p><p>Letting diag : R d×c → R min(d,c) be the operator that, given a rectangular matrix, returns a vector of the elements on the main diagonal, we can then write,</p><formula xml:id="formula_171">λ = diag(U ⊺ βV ) (110) λ = -M ∇ λ L (111)</formula><p>where M is a diagonal matrix and ∇ λ L is the gradient of the loss with respect to the singular values of β. Without loss of generality we consider η a = η w = 1.</p><p>Lemma B.5. Let ∆ = δI h . We then have that λ = -M ∇ λ L, where M ∈ R min(d,c)×min(d,c) is a diagonal matrix with</p><formula xml:id="formula_172">M ii = δ 2 + 4λ 2 i i ≤ min(d, h, c) 0 otherwise<label>(112)</label></formula><p>Proof. First note that</p><formula xml:id="formula_173">λ = diag(U ⊺ βV ) (113) = -diag (U ⊺ [X ⊺ (Xβ -Y )A ⊺ A + W ⊺ W X ⊺ (Xβ -Y )] V ) (114) = -diag U ⊺ X ⊺ (Xβ -Y )V Σ 2 A + Σ 2 W U ⊺ X ⊺ (Xβ -Y )V<label>(115)</label></formula><p>where we let</p><formula xml:id="formula_174">W ⊺ W = U Σ 2 W U ⊺ and A ⊺ A = V Σ 2 A V ⊺</formula><p>, using the fact that, under ∆ = I h , the eigenvectors of A ⊺ A are the right singular vectors of β and the eigenvectors of W ⊺ W are the left singular vectors of β. This expression rewrites as</p><formula xml:id="formula_175">λ = -M diag (U ⊺ X ⊺ (Xβ -Y )V )<label>(116)</label></formula><p>where <ref type="figure">d,</ref> <ref type="figure">c</ref>) accounting for rank deficiency of both A and W in this case. Additionally, in our setting of MSE loss, it is straightforward to show that</p><formula xml:id="formula_176">M ∈ R min(d,c)×min(d,c) is a diagonal matrix with M ii = (Σ 2 A ) ii + (Σ 2 W ) ii . For i ≤ min(d, h, c), one can show that M ii = δ 2 + 4λ 2 i . This is because for i ≤ min(d, h, c), (Σ 2 A ) ii = (Σ 2 W ) ii + δ from the conservation law and (Σ 2 W ) ii (Σ 2 A ) ii = λ 2 i from the definition of λ. Together this implies (Σ 2 W ) ii δ + (Σ 2 W ) ii = λ 2 i , which is a quadratic equation in (Σ 2 W ) ii . If h &lt; min(d, c) then M ii = 0 for i &gt; min(</formula><formula xml:id="formula_177">∂L ∂λ i = (U ⊺ X ⊺ (Xβ -Y )V ) ii<label>(117)</label></formula><p>We then have that ∇ λ L = diag (U ⊺ X ⊺ (Xβ -Y )V ), which, combined with our expression for M , completes the proof.</p><p>Leveraging Lemma B.5, we can show that the singular values of β evolve under a mirror flow in the following theorem. Theorem B.6. Let ∆ = δI h and assume h ≥ min(d, c) and δ ̸ = 0. We then have that the dynamics of λ, the singular values of β, are given by the mirror flow</p><formula xml:id="formula_178">λ = -∇ 2 Φ δ (λ) -1 ∇ λ L,<label>(118)</label></formula><p>where Φ δ (λ) = min(d,c) i=1 q δ (λ i ) and q δ is the hyperbolic entropy potential</p><formula xml:id="formula_179">q δ (x) = 1 4 2x sinh -1 2x |δ| -4x 2 + δ 2 + |δ| .<label>(119)</label></formula><p>Proof. When ∆ = δI h , then by Lemma B.5 the dynamics of the singular values of β can be expressed as λ = -M ∇ λ L. Furthermore, when h ≥ min(d, c) and δ ̸ = 0, we have that M = √ δ 2 + 4λ 2 I min(d,c) , where λ 2 is element-wise, which is always invertible. Observe, this expression for M is the inverse Hessian of the potential Φ δ (λ) = i q δ (λ i ) for q δ specified in the theorem statement. Thus, the dynamics for the singular values are the mirror flow λ = -∇ 2 Φ δ (λ)</p><p>-1 ∇ λ L.</p><p>Theorem B.6 implies that the dynamics for the singular values of β can be described as a mirror flow with a δ-dependent potential. This potential was first identified as the inductive bias for diagonal linear networks by Woodworth et al. <ref type="bibr" target="#b13">[14]</ref>. Termed hyperbolic entropy, this potential smoothly interpolates between an ℓ 1 and ℓ 2 penalty on the singular values for the rich (δ → 0) and lazy (δ → ±∞) regimes respectively. Unfortunately, in our setting we cannot adapt our mirror flow interpretation into a statement on the inductive bias at interpolation because the singular vectors evolve through training.</p><p>If we introduce additional assumptions -specifically, whitened input data (X ⊺ X = I d ) and a task-aligned initialization such that the singular vectors of β 0 are aligned with those of β * -we can ensure that the singular vectors remain constant and thus derive an inductive bias on the singular values. However, in this setting the dynamics decouple completely, implying there is no difference between applying an ℓ 1 or ℓ 2 penalty on the singular values. Consequently, even though the dynamics will depend on δ, the final interpolating solution will be independent of δ, making a statement on the inductive bias insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Deep linear networks</head><p>We now consider the influence of depth by studying a depth-(L + 1) linear network, f (x; θ) = a ⊺ L l=1 W l x, where W 1 ∈ R h×d , W l ∈ R h×h for 1 &lt; l ≤ L, and a ∈ R h . We assume that the dimensions d = h and that all parameters share the same learning rate η = 1. For this model the predictor coefficients are computed by the product β = L l=1 W ⊺ l a ∈ R d . Similar to our analysis of a two-layer setting, we assume an isotropic initializations of the parameters. Definition B.7. There exists a δ ∈ R such that aa</p><formula xml:id="formula_180">⊺ -W L W ⊺ L = δI h and for all l ∈ [L -1] W ⊺ l+1 W l+1 = W l W ⊺ l .</formula><p>This assumption can easily be achieved by setting a = 0 and W l = αO l for all l ∈ [L], where O l ∈ R d×d is an random orthogonal matrix and α ≥ 0. In this case δ = -α 2 . Further, notice this parameterization is naturally achieved in the high-dimensional limit as d → ∞ under a standard Gaussian initialization with a variance inversely proportional with width. As in the two-layer setting, this structure of the initialization will remain conserved throughout gradient flow. We now show how two natural quantities of β, its squared norm ∥β∥ 2 and its outer product ββ ⊺ , can always be expressed as polynomials of ∥a∥ 2 and W ⊺ 1 W 1 respectively. Lemma B.8. For a depth-(L+1) linear network with square width (d = h) and isotropic initialization, then for all t ≥ 0,</p><formula xml:id="formula_181">∥β∥ 2 = ∥a∥ 2 ∥a∥ 2 -δ L ,<label>(120)</label></formula><formula xml:id="formula_182">ββ ⊺ = (W ⊺ 1 W 1 ) L+1 + δ (W ⊺ 1 W 1 ) L .<label>(121)</label></formula><p>Proof. The norm of the regression coefficients is the product ∥β∥</p><formula xml:id="formula_183">2 = a ⊺ L l=1 W l L l=1 W l ⊺ a.</formula><p>Using the conservation of the initial conditions between consecutive weight matrices, W ⊺ l+1 W l+1 = W l W ⊺ l , we can express this telescoped product as</p><formula xml:id="formula_184">∥β∥ 2 = a ⊺ (W L W ⊺ L ) d a.</formula><p>When plugging in the conservation between last two layers, this implies ∥β∥ 2 = a ⊺ (aa ⊺ -δI h ) d a, which expanded gives the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The outer product of the regression coefficients is ββ</head><formula xml:id="formula_185">⊺ = L l=1 W l ⊺ aa ⊺ L l=1 W l .</formula><p>Using the conserved initial conditions of the last weights we can factor the outer product as the sum,</p><formula xml:id="formula_186">ββ ⊺ = L l=1 W l ⊺ W L W ⊺ L L l=1 W l + δ L l=1 W l ⊺ L l=1 W l .</formula><p>Both these telescoping products factor using the conservation of the initial conditions between consecutive weight matrices giving the desired result.</p><p>We now demonstrate how the quadratic terms |a| 2 and W ⊺ 1 W 1 significantly influence the dynamics of β, similar to our analysis in the two-layer setting.</p><p>Lemma B.9. The dynamics of β are given by a differential equation β = -M X ⊺ ρ where M is a positive semi-definite matrix that solely depends on ∥a∥ 2 , W ⊺ 1 W 1 , and δ,</p><formula xml:id="formula_187">M = (W ⊺ 1 W 1 ) L + ∥a∥ 2 L-1 l=0 (∥a∥ 2 -δ) l (W ⊺ 1 W 1 ) L-1-l .<label>(122)</label></formula><p>Proof. Using a similar telescoping strategy used in the previous proof we obtain the form of M .</p><p>Finally, we consider how the expression for M simplifies in the limit as δ → 0 allowing us to be precise about the inductive bias in this setting. Theorem B.10. For a depth-(L + 1) linear network with square width (d = h) and isotropic initialization β 0 such that ∥β(t)∥ &gt; 0 for all t ≥ 0, then in the limit as δ → 0, if the gradient flow solution β(∞) satisfies Xβ(∞) = y, then,</p><formula xml:id="formula_188">β(∞) = arg min β∈R d L + 1 L + 2 ∥β∥ L+2 L+1 - β(0) ∥β(0)∥ L L+1 ⊺ β s.t. Xβ = y.<label>(123)</label></formula><p>Proof. Whenever ∥β∥ &gt; 0 and in the limit as δ → 0, then we can find a unique expression for ∥a∥ 2 and W ⊺ 1 W 1 in terms of ∥β∥ 2 and ββ ⊺ ,</p><formula xml:id="formula_189">∥a∥ 2 = ∥β∥ 2 L+1 , W ⊺ 1 W 1 = ∥β∥ -2L L+1 ββ ⊺ .<label>(124)</label></formula><p>Plugged into the previous expression for M results in a positive definite rank-one perturbation to the identity, M = ∥β∥</p><formula xml:id="formula_190">2L L+1 I d + L∥β∥ -2 L+1 ββ ⊺ .</formula><p>(125) Using the Sherman-Morrison formula we find that M -1 is</p><formula xml:id="formula_191">M -1 = ∥β∥ -2L L+1 I d + L L + 1 ∥β∥ -4L+2 L+1 ββ ⊺<label>(126)</label></formula><p>We can now apply a time-warped mirror flow analysis similar to the analysis presented in Appendix A.4. Consider the time-warping function g δ (∥β∥) = ∥β∥ -L L+1 and the potential Φ(β) = L+1 L+2 ∥β∥ L+2 L+1 , then its not hard to show M -1 = g δ (∥β∥)∇ 2 Φ(β). This gives the desired result.</p><p>This theorem is a generalization of Proposition 1 derived in <ref type="bibr" target="#b8">[9]</ref> for two-layer linear networks in the rich limit to deep linear networks in the rich limit. We find that the inductive bias,</p><formula xml:id="formula_192">Q(β) = ( L+1 L+2 )∥β∥ L+2 L+1 -∥β 0 ∥ -L L+1 β ⊺ 0 β</formula><p>, strikes a depth-dependent balance between attaining the minimum norm solution and preserving the initialization direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Piecewise Linear Networks</head><p>Here, we elaborate on the theoretical results presented in Section 5. Our goal is to extend the tools developed in our analysis of linear networks to piecewise linear networks and understand their limitations. We focus on the dynamics of the input-output map, rather than on the inductive bias of the interpolating solutions. As discussed in Azulay et al. <ref type="bibr" target="#b8">[9]</ref>, Vardi and Shamir <ref type="bibr" target="#b79">[80]</ref>, extending a mirror flow style analysis directly to non-trivial piecewise linear networks is very difficult or provably impossible. In this section, we first describe the properties of the input-output map of a piecewise linear function, then describe the dynamics of a two-layer network, and finally discuss the challenges in extending this analysis to deeper networks and potential directions for future work.  The input-output map is linear within each non-empty activation region and continuous at the boundary between regions. Linearity implies that every non-empty<ref type="foot" target="#foot_8">foot_8</ref> activation region is associated with a linear predictor vector β R ∈ R d such that for all x ∈ R(A; θ), β R = ∇ x f (x; θ). Continuity implies that the boundary between regions is formed by a hyperplane determined by where the pre-activation for a neuron is exactly zero, {x : z i (x; θ) = 0}. When the neighboring regions have different linear predictors <ref type="foot" target="#foot_9">10</ref> , then this hyperplane is orthogonal to their difference, which is a vector in the span of the first-layer weights. Taken together, this implies that the union of all activation regions forms a convex partition of input space, as shown in Fig. <ref type="figure" target="#fig_17">9</ref>. We now present a surprisingly simple, yet to the best of our knowledge not previously understood property of this partition: Proposition C.2 (2-colorable). If f (x; θ) lacks redundant neurons, implying that every neuron influences an activation region, then the partition of input space can be colored with two distinct colors such that neighboring regions do not share the same color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Surface of a piecewise linear network</head><p>The justification for this proposition is straightforward. There is one color for regions with an even number of active neurons and another for regions with an odd number of active neurons. Because f (x; θ) lacks redundant neurons, there does not exist a boundary between activation regions where two neurons activations change simultaneously. In this work, we solely utilize this proposition for visualization purposes, as shown in Fig. <ref type="figure" target="#fig_17">9</ref>. Nonetheless, we believe it may be of independent interest as it strengthens the connection between the surface of piecewise linear networks and the mathematics of paper folding, a connection previously alluded to in the literature <ref type="bibr" target="#b81">[82]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Dynamics of a two-layer piecewise linear network</head><p>We consider the dynamics of a two-layer piecewise linear network without biases, f (x; θ) = a ⊺ σ(W x), where W ∈ R h×d and a ∈ R h . The activation function is σ(z) = max(z, γz) for γ ∈ [0, 1), which includes ReLU γ = 0 and Leaky ReLU γ ∈ (0, 1). We permit h &gt; d, which in the limit as h → ∞, ensures the network possesses the functional expressivity to represent any continuous nonlinear function from R d to R passing through the origin. Following a similar strategy used in Section 4, we consider the contribution to the input-output map from a single hidden neuron k ∈ [h] with parameters w k ∈ R d and a k ∈ R. As in the linear setting, each hidden neuron is associated with a conserved quantity, δ k = η w a 2 k -η a ∥w k ∥ 2 . Unlike in the linear setting, this neuron's contribution to the output f (x i ; θ) is regulated by whether the input x i is in the neuron's active halfspace, {x ∈ R d : w ⊺ k x &gt; 0}. Let C ∈ R h×n be the matrix with elements c ki = σ ′ (w ⊺ k x i ), which determines the activation of the k th neuron for the i th training data point. The subgradient σ ′ (z) = 1 if z &gt; 0, σ ′ (z) ∈ [γ, 1] if z = 0, and σ ′ (z) = γ if z &lt; 0. These activation functions exhibit positive homogeneity, implying σ(z) = σ ′ (z)z. Thus, we can express σ(w ⊺ k x i ) = c ki w ⊺ k x i , allowing us to express the gradient flow dynamics for w k and a k as</p><formula xml:id="formula_193">ȧk = -η a w ⊺ k n i=1 c ki x i ρ i , ẇk = -η w a k n i=1 c ki x i ρ i ,<label>(128)</label></formula><p>where ρ i = f (x i ; θ) -y i is the residual associated with the i th training data point. If we let β k = a k w k , which determines the contribution of each hidden neuron to the output f (x i ; θ), then its not hard to see that the gradient flow dynamics of</p><formula xml:id="formula_194">β k are βk = -η w a 2 k I d + η a w k w ⊺ k M k ( n i=1 c ki x i ρ i ) ξ k .<label>(129)</label></formula><p>As in the linear setting, the matrix M k ∈ R d×d appears as a preconditioning matrix on the dynamics Using the exact same derivation presented in Appendix A.3, whenever a 2 k ̸ = 0, we can express M k entirely in terms of β k and δ k ,</p><formula xml:id="formula_195">M k = δ 2 k + 4η a η w ∥β k ∥ 2 + δ k 2 I d + δ 2 k + 4η a η w ∥β k ∥ 2 -δ k 2 β k β ⊺ k ∥β k ∥ 2 . (<label>130</label></formula><formula xml:id="formula_196">)</formula><p>However, unlike in the linear setting, the vector ξ k ∈ R d driving the dynamics is not shared for all neurons because of its dependence on c ki . Additionally, the NTK matrix in this setting depends on M k and C, with elements K ij = h k=1 c ki x ⊺ i η w a 2 k I d + η a w k w ⊺ k x j c kj . Thus, in order to assess the temporal dynamics of the NTK matrix, we must understand the dynamics of M k and C. We consider a signed spherical coordinate transformation separating the dynamics of β k into its directional βk = sgn(a k ) β k ∥β k ∥ and radial µ k = sgn(a k )∥β k ∥ components, such that β k = µ k βk . Here, βk determines the orientation and direction of the halfspace where the k th neuron is active, while µ k determines the slope of the linear region in this halfspace. These coordinates evolve according to,</p><formula xml:id="formula_197">μk = -δ 2 k + 4η a η w µ 2 k β⊺ k ξ k , βk = - δ 2 k + 4η a η w µ 2 k + δ k 2µ k I d -βk β⊺ k ξ k .<label>(131)</label></formula><p>These equations can be derived directly from Eq. (128) through chain rule similar to Appendix A.2.1.</p><p>In fact its worth noting that the this change of coordinates is similar to the change of coordinates used in the single-neuron analysis. Expressed in terms of the parameters, βk = w k ∥w k ∥ and µ k = a k ∥w k ∥.</p><p>for 500 epochs with a learning rate of 1e-4 and a batch size of 512. The parameter distance is defined as the L 2 distance between all the parameters. To quantify the distance between the activations, we binarize the hidden activation with 1 representing an active neuron. We evaluate Hamming distance over all the binarized hidden activations normalized by the the total number of the activations. We use kernel distance <ref type="bibr" target="#b26">[27]</ref>, defined as S(t 1 , t 2 ) = 1 -⟨K t1 , K t2 ⟩/ (∥K t1 ∥ F ∥K t2 ∥ F ), which is a scale invariant measure of similarity between the NTK at two points in time. We subsample 10% of MNIST to evaluate the Hamming distance and kernel distance. All curves in the figure are averaged over 8 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gabor Filters</head><p>We are training a small ResNet based on the CIFAR10 script provided in the DAWN benchmark (code available here). The only modifications to the provided code base are we increase the convolution kernel size from 3 × 3 to 15 × 15, to better observe the learned spatial patterns, and we set the weight decay parameter to 0 to avoid confounding variables. Moreover, we are dividing the convolutional filters weights by a parameter α (after standard initialization) which controls the balancedness of the network. To quantify the smoothness of the filters, we compute the normalized Laplacian of each filter w ij ∈ R 15×15 , over input i = (1, 2, 3) and output j = (1, ..., 64) channels smoothness(w ij ) := w ij ∥w ij ∥ 2 * ∆ However, in B) we see that networks with a small initialization (α &lt; 1) learn much smoother filters, giving quantiative support to results in Fig. <ref type="figure" target="#fig_8">6</ref>. The smoothness is defined as the normalized Laplacian of the filters (see text, eq. 132).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Hierarchy Model</head><p>We refer to <ref type="bibr" target="#b66">[67]</ref>, who originally proposed the random hierarchy model (RHM) as a tool for studying how deep networks learn compositional data, for a more in-depth treatment. Here we briefly recap the setup following the notation used in <ref type="bibr" target="#b66">[67]</ref>.</p><p>An RHM essentially lets us build a random classification task with a clear hierarchical structure. The top level of the RHM specifies m equivalent high-level features for each class label in {1, . . . , n c }, where each feature has length s and n c is the number of classes. For example, suppose the vocabulary at the top level is V L = {a, b, c}, n c = 2, m = 3, and s = 2. Then in a particular instantiation of this RHM, we might have that Class 1 has ab, aa, and ca as equivalent high-level features (this is precisely the example used in Fig. <ref type="figure">1</ref> of <ref type="bibr" target="#b66">[67]</ref>). Class 2 will then have three random high-level features, with the constraint that they are not features for Class 1, for example, bb, bc, ac.</p><p>Each successive level specifies m equivalent lower-level features for each "token" in the vocabulary at the previous level. For example, if V L-1 = {d, e, f }, we might have that a can be equivalently</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>δ = 2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Balance determines geometry of trajectory.The quantity δ = η w a 2 -η a ∥w∥ 2 is conserved through gradient flow, which constrains the trajectory to: (a) a onesheeted hyperboloid for downstream initializations, (b) a double cone for balanced initializations, and (c) a twosheeted hyperboloid for upstream initializations. Gradient flow dynamics for three different initializations a 0 , w 0 with the same product β 0 = a 0 w 0 are shown. The minima manifold is shown in red and the manifold of equivalent β 0 initializations in gray. The surface is colored according to training loss, with blue representing higher loss and red representing lower loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Exact solutions for the single hidden neuron model.Our theoretical predictions (black dashed lines) agree with gradient flow simulations (solid lines, color-coded based on δ values), shown here for three key metrics: µ (left), ϕ (middle), and S(0, t) (right). Each metric starts at the same value for all δ, but varying δ has a pronounced effect on the metric's dynamics. For upstream initializations (δ ≫ 0), µ changes only slightly, ϕ exponentially aligns, and S remains near zero, indicative of the lazy regime. For balanced initializations (δ = 0), both µ and ϕ change significantly and S quickly moves away from zero, indicative of the rich regime. For downstream initializations (δ ≪ 0), µ quickly drops to zero, then µ and ϕ slowly climb back to one. Similarly, S remains small before a sudden transition towards one, indicative of a delayed rich regime. See Appendix A.2 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Balance modulates β dynamics and implicit bias.Here we show the dynamics of β = aw with different values of δ, but the same initial β 0 . When X ⊺ X is whitened (left), we can solve for the dynamics exactly using our expressions for µ, ϕ (black dashed lines). Upstream initializations follow the trajectory of gradient flow on β, downstream initializations first move in the direction of β 0 before sweeping around towards β * , and balanced initializations take an intermediate trajectory between these two. When X ⊺ X is low-rank (right), then we can only predict the trajectories in the limit of δ = ±∞. If the interpolating manifold is onedimensional, then we can solve for the solution in terms of δ exactly (black dots). See Appendix A.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Evolution of a ReLU network's input-output map (b) Hamming and parameter distance over τ -δ sweep</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Rapid feature learning is caused by large activation changes with minimal parameter movement. (a) We show the surface of a two-layer ReLU network trained on an XOR-like task, starting with a near-zero input-output map, f (x; θ 0 ) ≈ 0. The surface consists of convex conic regions, each with a distinct activation pattern, colored by the parity of active neurons. A lazy initialization (bottom) maintains a fixed activation partition throughout training, reweighting the hidden neurons to fit the data. In contrast, a rich balanced or upstream initialization (top) features an initial alignment phase where the partition map changes rapidly while the input-output map remains close to zero, followed by a data-fitting phase. (b) We show the evolution of Hamming distance in activation patterns and parameter distance, relative to t = 0, as a function of overall and relative scales (same experiments as in Fig.1(b)). Rapid feature learning occurs from a small-τ upstream initialization that promotes faster learning in early layers, driving a large change in Hamming distance, but a small change in parameter space. In contrast, small-τ downstream initializations require large parameter movement to fit the data in the delayed rich regime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of upstream initializations in practice. Here we provide evidence that an upstream initialization (a) drives feature learning through changing activation patterns, (b) promotes interpretability of early layers in CNNs, (c) reduces the sample complexity of learning hierarchical data, and (d) decreases the time to grokking in modular arithmetic.In these experiments, we regulate the first layer's learning speed relative to the rest of the network by dividing its initialization by α. For models without normalization layers, we also scale the last layer's initialization by α to preserve the input-output map. α = 1 represents standard parameterization, while α ≫ 1 and α ≪ 1 correspond to upstream and downstream initializations, respectively. See Appendix D.3 for details.</figDesc><graphic coords="10,413.54,89.56,90.88,85.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two basins of attraction. For this model, parameter space is partitioned into two basins of attraction, one for the positive and negative branch of the minima manifold. The surface separating the basins of attraction is determined by the equation w ⊺ 0 β * + a0 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Exact temporal dynamics of relevant variables in single-hidden neuron model. Our theory recovers the time evolution under gradient flow of the quantities considered in this section, specifically ν, φ, and ζ, as well as the resulting dynamics of the model parameters {a, w 1 , w 2 }. The true β * is a unit vector pointing in π/4 direction; β(0) is a unit vector pointing towards 3π/2, -π/4, and π/4 directions, respectively, for each of the three rows. δ then defines how a(0) and ∥w(0)∥ are chosen for a particular β(0) where by convention we choose a(0) &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>B. 1 . 3</head><label>13</label><figDesc>Proving Theorem 4.1 We first prove Theorem 4.1. Consider a single hidden neuron k ∈ [h] of the multi-output model defined by the parameters w k ∈ R d and a k ∈ R c . Let β k = w k a ⊺ k be the R d×c matrix representing the contribution of this hidden neuron to the input-output map of the network β = h k=1 β k . Consider the two gram matrices β</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>we derive the representation for M presented in Theorem 4.1: M = h k=1 M k where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>hk=1</head><figDesc>Row (β k (0)) is equivalent to the existence of a β * whose rows lie in the span of {a k (0)} h k=1 , or, equivalently, to the existence of a matrix W such that β * = W ⊺ A(0). Similarly, the condition Col(β * ) ⊆ Span h k=1 Col (β k (0)) is in most cases equivalent to the existence of a matrix A such that β * = W (0) ⊺ A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Surface of a ReLU network.Here we depict the surface of a three-layer ReLU network f (x; θ) : R 2 → R with twenty hidden units per layer at initialization, comparing configurations with biases (left) and without biases (right). The network with biases partitions input space into convex polytopes that tile input space. The network without biases partitions input space into convex conic sections emanating from the origin. Each region exhibits a distinct activation pattern, allowing the partition to be colored with two colors based on the parity of active neurons. The network operates linearly within each region and maintains continuity across boundaries.The input-output map of a piecewise linear network f (x; θ), with l hidden layers and h hidden neurons per layer, is comprised of potentially O(h dl ) connected linear regions, each with their own vector of predictor coefficients<ref type="bibr" target="#b64">[65]</ref>. The exploration of this complex surface has been the focus of numerous prior works, the vast majority of them focused on counting and bounding the number of linear regions as a function of the width and depth<ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b86">87]</ref>. The central object in all of these studies is the activation region, Definition C.1. For a piecewise linear network f (x; θ), comprising N hidden neurons with preactivation z i (x; θ) for i ∈ [N ], let the activation pattern A represent an assignment of signs a i ∈ {-1, 1} to each hidden neuron. The activation region R(A; θ) is the subset of input space that generates A, R(A; θ) = {x ∈ R d | ∀i a i z i (x; θ) &gt; 0}. (127)</figDesc><graphic coords="35,325.80,189.18,87.32,84.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Interpreting convolutional filters. CNN experiments on CIFAR10. We can see in A) that all networks achieve comparable training and test accuracy, despite the modification in initialization. However, in B) we see that networks with a small initialization (α &lt; 1) learn much smoother filters, giving quantiative support to results in Fig.6. The smoothness is defined as the normalized Laplacian of the filters (see text, eq. 132).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Xu and Ziyin<ref type="bibr" target="#b22">[23]</ref> presented exact NTK dynamics for a linear model trained with one-dimensional data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The set of saddle points {(a, w)} is the d -1 dimensional subspace satisfying a = 0 and w ⊺ X ⊺ y = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We relax this assumption when considering the dynamics of β in function space and their implicit bias.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The basin is given by sgn(a0) for δ ≥ 0 or sgn(w ⊺ 0 β * + a 0 2 (δ + δ 2 + 4∥β * ∥ 2 )) for δ &lt; 0. See A.2.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The Kronecker sum is defined for square matrices C ∈ R c×c and D ∈ R d×d as C ⊕ D = C ⊗ I d + Ic ⊗ D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>When h = c = 1 we can recover Eq. (4) presented in the single-neuron setting directly from Eq. (7).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>To our knowledge, this property has not been recognized before. See Appendix C.1 for a formal statement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>As long as c &gt; 1, then the surface of this d + c hyperboloid is always connected, however its topology will depend on the relationship between d and c.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>While it is trivial to see that for a network f (x; θ) with N hidden neurons there are 2 N distinct activation patterns, not all activation patterns are attainable. See Raghu et al.<ref type="bibr" target="#b64">[65]</ref> for a discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>It is possible for neighboring regions to have the same linear predictor. Some works define linear regions as maximally connected component of input space with the same linear predictor<ref type="bibr" target="#b86">[87]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We thank <rs type="person">Francisco Acosta</rs>, <rs type="person">Alex Atanasov</rs>, <rs type="person">Yasaman Bahri</rs>, <rs type="person">Abby Bertics</rs>, <rs type="person">Blake Bordelon</rs>, <rs type="person">Nan Cheng</rs>, <rs type="person">Alex Infanger</rs>, <rs type="person">Mason Kamb</rs>, <rs type="person">Guillaume Lajoie</rs>, <rs type="person">Nina Miolane</rs>, <rs type="person">Cengiz Pehlevan</rs>, <rs type="person">Ben Sorscher</rs>, <rs type="person">Javan Tahir</rs>, <rs type="person">Atsushi Yamamura</rs> for helpful discussions. D.K. thanks the <rs type="funder">Open Philanthropy AI</rs> Fellowship for support. S.G. thanks the <rs type="person">James S. McDonnell</rs> and <rs type="person">Simons Foundations</rs>, <rs type="funder">NTT Research</rs>, and an <rs type="funder">NSF</rs> <rs type="grantName">CAREER Award for support</rs>. This research was supported in part by grant <rs type="funder">NSF</rs> <rs type="grantNumber">PHY-1748958</rs> to the <rs type="institution">Kavli Institute for Theoretical Physics (KITP)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gYRE2zD">
					<orgName type="grant-name">CAREER Award for support</orgName>
				</org>
				<org type="funding" xml:id="_rpsHhPw">
					<idno type="grant-number">PHY-1748958</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>This project originated from conversations between Daniel and Allan at the Kavli Institute for Theoretical Physics. Daniel, Allan, and Feng are primarily responsible for the single neuron analysis in Section 3. Daniel, Clem, Allan, and Feng are primarily responsible for the wide and deep linear analysis in Section 4. Daniel is primarily responsible for the nonlinear analysis in Section 5. Allan, Feng, and David are primarily responsible for the empirics in Fig. <ref type="figure">1</ref> and Fig. <ref type="figure">6</ref>. Daniel is primarily responsible for writing the main sections. All authors contributed to the writing of the appendix and the polishing of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experimental Details</head><p>We used Google Cloud Platform (GCP) nodes to run all experiments. Figure <ref type="figure">1</ref> experiments were run on a node with 360 AMD Genoa CPU cores with runtime totaling approximately 90 minutes including averaging over seeds as described below. Neural network training and NTK calculation for Figure <ref type="figure">5</ref> was performed on single A100 GPU nodes. Runtime was approximately 20 hours for Figure <ref type="figure">5</ref>(a), four hours for 5(b), 12 hours for 5(c) (with individual runs ranging from five to 30 minutes depending on the number of datapoints), and 12 hours for 5(d). Figures <ref type="figure">2,</ref> <ref type="figure">3</ref>, and 4 are not compute-heavy, and these experiments were run on a personal computer. Overall, we estimate approximately 200 hours of single A100 runtime as well as 100 hours of the 360-core node accounting for failed runs and exploratory experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Figure 1: Teacher-Student with Two-layer ReLU Networks</head><p>For Fig. <ref type="figure">1</ref> we consider a student-teacher setup similar to that in <ref type="bibr" target="#b7">[8]</ref>, with one-hidden layer ReLU networks of the form f (x; θ) = m i=1 a i σ(w ⊺ i x), where f : R d → R and σ is the ReLU activation function. The teacher model, f teacher , has m = k hidden neurons initialized as w teacher i i.i.d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∼</head><p>Unif(S d-1 ) and a i i.i.d.</p><p>∼ Unif({±1}) for i ≤ k. The student, f student , in turn, has h hidden neurons.</p><p>We use a symmetrized initialization, as considered in <ref type="bibr" target="#b7">[8]</ref>, where for i ≤ h/2, we sample</p><p>∼ Unif({±1}), and then for i ≥ h</p><p>2 + 1 we symmetrize by setting w i = w i-h/2 and a i = -a i-h/2 . This ensures that f student predicts 0 on any input at initialization. Note that the base student initialization described thus far is perfectly balanced at each neuron, that is δ i = 0 for i ∈ [m]; we also define this to be our setting where the scale τ is 1. In order to transform the base initialization into a particular setting of τ and δ, we first solve for the relative layer scaling α in δ 2 = τ 2 (α 2 -α -2 ) and then scale each w i by τ /α and each a i by τ α. We obtain a training dataset {x (i) , y (i) } n i=1 by sampling x (i) i.i.d. ∼ S d-1 and computing noiseless labels as y (i) = f teacher (x (i) ; θ teacher ). The student is then trained with full-batch gradient descent on a mean square loss objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 (a).</head><p>Here the setting is: d = 2, h = 50, k = 3, and n = 20. We sample a single teacher and then train four students with the same base initialization but different configurations of τ and δ: (τ = 0.1, δ = 0) and (τ = 2, δ = 0) for the left subfigure, and (τ = 0.1, δ = 1) and (τ = 0.1, δ = -1) for the right subfigure. Training is for 1 million steps at a learning rate of 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1 (b).</head><p>Here the setting is: d = 100, m = 50, k = 3, and n = 1000, as in Fig. <ref type="figure">1c</ref> of <ref type="bibr" target="#b7">[8]</ref>. Training is performed with learning rate of 5e-3/τ 2 . Test error is computed as mean square error over a held-out set of 10,000 datapoints. We sweep over τ over a logarithmic scale in the range [0.1, 2] and δ over a linear scale in the range [-1, 1]. We average over 16 random seeds, where the seed controls the sampling of: the teacher weights θ teacher , the base initialization of θ student , and the training data {x (i) } n i=1 . In this way, each random seed is used for a sweep over all combinations of τ and δ in the sweep; we simply apply the scaling described above to get to each point on the (τ, δ) grid. The kernel distance computed is as defined in <ref type="bibr" target="#b26">[27]</ref>, where here we compute it at time t relative to the kernel at initialization, i.e. S(t) = 1 -⟨K 0 , K t ⟩/ (∥K 0 ∥ F ∥K t ∥ F ). In Fig. <ref type="figure">10</ref>, we additionally plot Hamming and parameter distances relative to initialization, as well as training loss, while training for ten times longer than in Fig. <ref type="figure">1 (b)</ref>.</p><p>Notebooks generating all two-layer experiment figures are provided here.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Distance</head><p>We trained LeNet-5 <ref type="bibr" target="#b87">[88]</ref> (with ReLU nonlinearity and Max Pooling) on MNIST <ref type="bibr" target="#b87">[88]</ref>. We use He initialization <ref type="bibr" target="#b88">[89]</ref> and divide the first layer weights by α and multiply the last layer weights by α at initialization, which keeps the network functionally the same at initialization. We trained the model represented as de, df , or f f ; b and c will each have m equivalent representations of their own. We assume that the vocabulary size, v, is the same at all levels. Therefore, sampling an RHM with hyperparameters n c , m, s, v requires sampling mn c + (L -1)mv rules.</p><p>In order to sample a datapoint from an RHM, we first sample a class label (e.g. Class 1), then uniformly sample one of the highest level features, (e.g. ab), then for each "token" in this feature we sample lower level features (e.g. a → de, b → ee), and so on recursively. The generated sample will therefore have length s L and a class label. For training a neural network to perform this classification task, each input is converted into a one-hot representation, which will be of shape (s L , v), and is then flattened.</p><p>We use the code released by <ref type="bibr" target="#b66">[67]</ref> to train an MLP of width 64 with three hidden layers to learn an RHM with L = 3, n c = 8, m = 4, s = 2, v = 8. The main change we make is allowing for scaling the initialization of the first layer by 1/α and the initialization the readout layer by α. We then sweep over α ∈ {0.03, 0.1, 0.3, 1, 3, 10} and over the number of datapoints in the training set, which is specified as a fraction of the total number of datapoints the RHM can generate. We average test accuracy, which is by default computed on a held-out set of 20,000 samples, over six random seed configurations, where each configuration seeds the RHM, the neural network, and the data generation.</p><p>We train with the default settings used in <ref type="bibr" target="#b66">[67]</ref>, that is stochastic gradient descent with momentum of 0.9, run for 250 epochs with a learning rate initialized at 6.4 (0.1 times width) and decayed with a cosine schedule down to 80% of epochs. The batch size of 128; we do not use biases or weight decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grokking</head><p>We are training a one layer transformer model on the modular arithmetic task in Power et al. <ref type="bibr" target="#b67">[68]</ref>. Our experimental code is based on an existing Pytorch implementation (code available here). The only modifications to the provided code base is that we use a single transformer layer (instead of the default 2-layer model). Prior analysis in Nanda et al. <ref type="bibr" target="#b71">[72]</ref> has shown that this model can learn a minimal (attention-based) circuit that solves the task.</p><p>We study the effects on grokking time (defined as ≥ 0.99 accuracy on the validation data) of two manipulations. Firstly, we divide the embedding weights of the positional and token embeddings by the same balancedness parameter α as in the CNN gabor experiments. Secondly, like in Kumar et al. <ref type="bibr" target="#b68">[69]</ref>, we multiply the output of the model (i.e., the logits) by a factor τ and divide the learning rate by    <ref type="bibr" target="#b68">[69]</ref> and balance α ∈ {0.1, 0.3, 1.0, 3.0, 10} on a regular grid over n = 5 random initializations with a maximal computational budget of m = 30, 000 training steps. B) Same as A), but reporting the number of training steps required until the test performance passes the predefined threshold of 99%. We clearly see the fastest grokking in an unbalanced rich setting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gradient descent provably optimizes over-parameterized neural networks</title>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Zhai</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient descent optimizes overparameterized deep relu networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongruo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="467" to="492" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tensor programs ii: Neural tangent kernel for any architecture</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14548</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On lazy training in differentiable programming</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the implicit bias of initialization shape: Beyond infinitesimal mirror descent</title>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Azulay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">E</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="468" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6120</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A mathematical theory of semantic development in deep neural networks</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Andrew M Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="11537" to="11546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Saddle-tosaddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Ged</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berfin</forename><surname>Şimşek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clément</forename><surname>Hongler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Gabriel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.15933</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09839</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel and rich regimes in overparametrized models</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3635" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">How connectivity structure shapes rich and lazy learning in neural circuits</title>
		<author>
			<persName><forename type="first">Helena</forename><surname>Yuhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Cornford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mihalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Shea-Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14522</idno>
		<title level="m">Feature learning in infinite-width neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Phase diagram for two-layer relu neural networks at infinite-width limit</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zhi-Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">71</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03466</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02218</idno>
		<title level="m">The large learning rate phase of deep learning: the catapult mechanism</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Highdimensional asymptotics of feature learning: How one gradient step improves the representation</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Erdogdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="37932" to="37946" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning</title>
		<author>
			<persName><forename type="first">Libin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adityanarayanan</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04815</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Asymptotics of feature learning in two-layer networks after one gradient-step</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Pesce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Dandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Zdeborová</surname></persName>
		</author>
		<author>
			<persName><surname>Loureiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04980</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">When does feature learning happen? perspective from an analytically solvable model</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Ziyin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07085</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Algorithms for learning kernels based on centered alignment</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="795" to="828" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangling feature and lazy training in deep neural networks</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Spigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Wyart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">113301</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit regularization via neural feature alignment</title>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2269" to="2277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel</title>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansheej</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepideh</forename><surname>Kharaghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5850" to="5861" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An analytic theory of generalization dynamics and transfer learning in deep linear networks</title>
		<author>
			<persName><forename type="first">Surya</forename><surname>Andrew K Lampinen</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10374</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Effect of batch learning in multilayer neural networks</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gen</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="1E" to="03" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exact learning dynamics of deep linear networks with prior knowledge</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clémentine</forename><surname>Carla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juliette</forename><surname>Dominé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the optimization of deep networks: Implicit acceleration by overparameterization</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep matrix factorization</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exact solutions of a deep linear network</title>
		<author>
			<persName><forename type="first">Botao</forename><surname>Liu Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24446" to="24458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Implicit regularization of discrete gradient dynamics in linear neural networks</title>
		<author>
			<persName><forename type="first">Francis</forename><surname>Gauthier Gidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding the dynamics of gradient flow in overparameterized linear models</title>
		<author>
			<persName><forename type="first">Guilherme</forename><surname>Salma Tarmoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">D</forename><surname>Franca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rene</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName><surname>Vidal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10153" to="10161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The implicit bias of depth: How incremental learning drives generalization</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Daniel Gissin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><surname>Daniely</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12051</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural networks as kernel learners: The silent alignment effect</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gradient descent aligns the layers of deep linear networks</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02032</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Implicit bias of gradient descent on linear convolutional networks</title>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Implicit bias in deep linear classification: Initialization scale vs training accuracy</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Moroshko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">E</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22182" to="22193" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gradient descent maximizes the margin of homogeneous neural networks</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05890</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Shpigel Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4683" to="4692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1305" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Daniel Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Yamamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.03820</idno>
		<title level="m">The asymmetric maximum margin bias of quasi-homogeneous neural networks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Characterizing implicit bias in terms of optimization geometry</title>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1832" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mirrorless mirror descent: A natural derivation of mirror descent</title>
		<author>
			<persName><forename type="first">Suriya</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2305" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Implicit bias of gradient descent on reparametrized models: On equivalence to mirror descent</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="34626" to="34640" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Gradient descent quantizes relu network features</title>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08367</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The inductive bias of relu networks on orthogonally separable data</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gradient descent on two-layer nets: Margin maximization and simplicity bias</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Boursier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loucas</forename><surname>Pillaud-Vivien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Flammarion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2022">20105-20118, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Early stage convergence and global convergence of training mildly parameterized neural networks</title>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="743" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Early neuron alignment in two-layer relu networks with small initialization</title>
		<author>
			<persName><forename type="first">René</forename><surname>Hancheng Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><surname>Mallada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.12851</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Understanding multi-phase optimization dynamics and rich nonlinear behaviors of relu networks</title>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A mean field view of the landscape of two-layer neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phan-Minh</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">33</biblScope>
			<biblScope unit="page" from="7665" to="E7671" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the global convergence of gradient descent for overparameterized models using optimal transport</title>
		<author>
			<persName><forename type="first">Lenaic</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Mean field analysis of neural networks: A law of large numbers</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sirignano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Spiliopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="725" to="752" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Trainability and accuracy of artificial neural networks: An interacting particle system approach</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Rotskoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Vanden-Eijnden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1889" to="1935" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Self-consistent dynamical field theory of kernel evolution in wide neural networks</title>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Pehlevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="32240" to="32256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A spectral condition for feature learning</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17813</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Simon S Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Zhenfeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Aranguri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Jacot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.17580</idno>
		<title level="m">Mixed dynamics in linear networks: Unifying the lazy and active regimes</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the spectral bias of two-layer linear networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Vardhan Varre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Luiza</forename><surname>Vladarean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loucas</forename><surname>Pillaud-Vivien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Flammarion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">On the expressive power of deep neural networks</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">In international conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Cagnetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umberto</forename><forename type="middle">M</forename><surname>Tomasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Favero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Wyart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02129</idno>
		<title level="m">How deep neural networks learn compositional data: The random hierarchy model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><surname>Grokking</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02177</idno>
		<title level="m">Generalization beyond overfitting on small algorithmic datasets</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Tanishq</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Bordelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Samuel J Gershman</surname></persName>
		</author>
		<author>
			<persName><surname>Pehlevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06110</idno>
		<title level="m">Grokking as the transition from lazy to rich training dynamics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dichotomy of early and late phase implicit biases can provably induce grokking</title>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jikai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Shaolei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Grokking as a first order phase transition in two layer networks</title>
		<author>
			<persName><forename type="first">Noa</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbar</forename><surname>Seroussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zohar</forename><surname>Ringel</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3ROGsTX3IR" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Lieberum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jess</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.05217</idno>
		<title level="m">Progress measures for grokking via mechanistic interpretability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The neural race reduction: Dynamics of abstraction in gated networks</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shagun</forename><surname>Sodhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Jay Lewallen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="19287" to="19309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Javier</forename><surname>Daniel Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Sagastuy-Brena</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidenori</forename><surname>Daniel Lk Yamins</surname></persName>
		</author>
		<author>
			<persName><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04728</idno>
		<title level="m">Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Noether&apos;s learning dynamics: Role of symmetry breaking in neural networks</title>
		<author>
			<persName><forename type="first">Hidenori</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25646" to="25660" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Stochastic collapse: How gradient noise attracts sgd dynamics towards simpler subnetworks</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kunin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Yamamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Liu Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07193</idno>
		<title level="m">The implicit bias of gradient noise: A symmetry perspective</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Exact solution for on-line learning in multilayer neural networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">4337</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Generalisation dynamics of online learning in over-parameterised neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lenka</forename><surname>Krzakala</surname></persName>
		</author>
		<author>
			<persName><surname>Zdeborová</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09085</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Implicit regularization in relu networks with the square loss</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Vardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4224" to="4258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">On the number of response regions of deep feed forward networks with piece-wise linear activations</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6098</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Guido F Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Matus</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08101</idno>
		<title level="m">Representation benefits of deep feedforward networks</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Understanding deep neural networks with rectified linear units</title>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amitabh</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poorya</forename><surname>Mianjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirbit</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01491</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Bounding and counting linear regions of deep neural networks</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Tjandraatmadja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikumar</forename><surname>Ramalingam</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4558" to="4566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Complexity of linear regions in deep networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2596" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Deep relu networks have surprisingly few activation patterns</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Hanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
