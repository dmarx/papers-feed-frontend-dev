<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supermasks in Superposition</title>
				<funder ref="#_GfdxMwn">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_Ga9q9mT #_VYqe9PQ #_fbYuqrp">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-10-22">22 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Ramanujan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Collective</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Supermask</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Task</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">!</forename><surname>! ! " ! # ! ! ! " ! #</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit3">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit4">University of Washington</orgName>
								<orgName type="institution" key="instit5">University of Washington</orgName>
								<address>
									<addrLine>NeurIPS 2020)</addrLine>
									<settlement>Vancouver</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supermasks in Superposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-22">22 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">B3626E8D327C519B4D8BBA56BE7D2CCA</idno>
					<idno type="arXiv">arXiv:2006.14769v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning many different tasks sequentially without forgetting remains a notable challenge for neural networks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b22">23]</ref>. If the weights of a neural network are trained on a new task, performance on previous tasks often degrades substantially <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>, a problem known as catastrophic forgetting. In this paper, we begin with the observation that catastrophic forgetting cannot occur if the weights of the network remain fixed and random. We leverage this to develop a flexible model capable of learning thousands of tasks: Supermasks in Superposition (SupSup). SupSup, diagrammed in Figure <ref type="figure" target="#fig_0">1</ref>, is driven by two core ideas: a) the expressive power of untrained, randomly weighted subnetworks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b38">39]</ref>, and b) inference of task-identity as a gradient-based optimization problem.</p><p>a) The expressive power of subnetworks Neural networks may be overlaid with a binary mask that selectively keeps or removes each connection, producing a subnetwork. The number of possible subnetworks is combinatorial in the number of parameters. Researchers have observed that the number of combinations is large enough that even within randomly weighted neural networks, there exist supermasks that create corresponding subnetworks which achieve good performance on complex tasks. Zhou et al. <ref type="bibr" target="#b56">[57]</ref> and Ramanujan et al. <ref type="bibr" target="#b38">[39]</ref> present two algorithms for finding these supermasks while keeping the weights of the underlying network fixed and random. SupSup scales to many tasks by finding for each task a supermask atop a shared, untrained network. b) Inference of task-identity as an optimization problem When task identity is unknown, SupSup can infer task identity to select the correct supermask. Given data from task j, we aim (right) At inference time, SupSup can infer task identity by superimposing all supermasks, each weighted by an α i , and using gradients to maximize confidence.</p><p>to recover and use the supermask originally trained for task j. This supermask should exhibit a confident (i.e. low entropy) output distribution when given data from task j <ref type="bibr" target="#b18">[19]</ref>, so we frame inference of task-identity as an optimization problem-find the convex combination of learned supermasks which minimizes the entropy of the output distribution.</p><p>In the rest of the paper we develop and evaluate SupSup via the following contributions:</p><p>1. We propose a new taxonomy of continual learning scenarios. We use it to embed and contextualize related work (Section 2).</p><p>2. When task identity (ID) is provided during train and test (later dubbed GG), SupSup is a natural extension of Mallya et al. <ref type="bibr" target="#b29">[30]</ref>. By using a randomly weighted backbone and controlling mask sparsity, SupSup surpasses recent baselines on SplitImageNet <ref type="bibr" target="#b50">[51]</ref> while requiring less storage and time costs (Section 3.2).</p><p>3. When task ID is provided during train but not test (later dubbed GN), SupSup outperforms recent methods that require task ID <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref>, scaling to 2500 permutations of MNIST without forgetting. For these uniform tasks, ID can be inferred with a single gradient computation (Section 3.3). <ref type="bibr" target="#b3">4</ref>. When task identities are not provided at all (later dubbed NNs), SupSup can even infer task boundaries and allocate new supermasks as needed (Section 3.4).</p><p>5. We introduce an extension to the basic SupSup algorithm that stores supermasks implicitly as attractors in a fixed-size Hopfield network <ref type="bibr" target="#b19">[20]</ref> (Section 3.5).</p><p>6. Finally, we empirically show that the simple trick of adding superfluous neurons results in more accurate task inference (Section 3.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Continual Learning Scenarios and Related Work</head><p>In continual learning, a model aims to solve a number of tasks sequentially <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref> without catastrophic forgetting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. Although numerous approaches have been proposed in the context of continual learning, there lacks a convention of scenarios in which methods are trained and evaluated <ref type="bibr" target="#b48">[49]</ref>. The key identifiers of scenarios include: 1) whether task identity is provided during training, 2) provided during inference, 3) whether class labels are shared during evaluation, and 4) whether the overall task space is discrete or continuous. This results in an exhaustive set of 16 possibilities, many of which are invalid or uninteresting. For example, if task identity is never provided in training, providing it in inference is no longer helpful. To that end, we highlight four applicable scenarios, each with a further breakdown of discrete vs. continuous, when applicable, as shown in Table <ref type="table">1</ref>.</p><p>We decompose continual learning scenarios via a three-letter taxonomy that explicitly addresses the three most critical scenario variations. The first two letters specify whether task identity is given during training (G if given, N if not) and during inference (G if given, N if not). The third letter specifies a subtle but important distinction: whether labels are shared (s) across tasks or not (u). In the unshared case, the model must predict both the correct task ID and the correct class within that Table <ref type="table">1</ref>: Overview of different Continual Learning scenarios. We suggest scenario names that provide an intuitive understanding of the variations in training, inference, and evaluation, while allowing a full coverage of the scenarios previously defined in <ref type="bibr" target="#b48">[49]</ref> and <ref type="bibr" target="#b54">[55]</ref>. See text for more complete description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario Description</head><p>Task space discreet or continuous?</p><p>Example methods / task names used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GG</head><p>Task Given during train and Given during inference Either PNN <ref type="bibr" target="#b41">[42]</ref>, BatchE <ref type="bibr" target="#b50">[51]</ref>, PSP <ref type="bibr" target="#b3">[4]</ref>, "Task learning" <ref type="bibr" target="#b54">[55]</ref>, "Task-IL" <ref type="bibr" target="#b48">[49]</ref> GNs Task Given during train, Not inference; shared labels Either EWC <ref type="bibr" target="#b22">[23]</ref>, SI <ref type="bibr" target="#b53">[54]</ref>, "Domain learning" <ref type="bibr" target="#b54">[55]</ref>, "Domain-IL" <ref type="bibr" target="#b48">[49]</ref> GNu Task Given during train, Not inference; unshared labels Discrete only "Class learning" <ref type="bibr" target="#b54">[55]</ref>, "Class-IL" <ref type="bibr" target="#b48">[49]</ref> NNs Task Not given during train Nor inference; shared labels Either BGD, "Continuous/discrete task agnostic learning" <ref type="bibr" target="#b54">[55]</ref> task. In the shared case, the model need only predict the correct, shared label across tasks, so it need not represent or predict which task the data came from. For example, when learning 5 permutations of MNIST in the GN scenario (task IDs given during train but not test), a shared label GNs scenario will evaluate the model on the correct predicted label across 10 possibilities, while in the unshared GNu case the model must predict across 50 possibilities, a more difficult problem.</p><p>A full expansion of possibilities entails both GGs and GGu, but as s and u describe only model evaluation, any model capable of predicting shared labels can predict unshared equally well using the provided task ID at test time. Thus these cases are equivalent, and we designate both GG. Moreover, the NNu scenario is invalid because unseen labels signal the presence of a new task (the "labels trick" in <ref type="bibr" target="#b54">[55]</ref>), making the scenario actually GNu, and so we consider only the shared label case NNs.</p><p>We leave out the discrete vs. continuous distinction as most research efforts operate within one framework or the other, and the taxonomy applies equivalently to discrete domains with integer "Task IDs" as to continue domains with "Task Embedding" or "Task Context" vectors. The remainder of this paper follows the majority of extant literature in focusing on the case with discrete task boundaries (see e.g. <ref type="bibr" target="#b54">[55]</ref> for progress in the continuous scenario). Equipped with this taxonomy, we review three existing approaches for continual learning.</p><p>(1) Regularization based methods Methods like Elastic Weight Consolidation (EWC) <ref type="bibr" target="#b22">[23]</ref> and Synaptic Intelligence (SI) <ref type="bibr" target="#b53">[54]</ref> penalize the movement of parameters that are important for solving previous tasks in order to mitigate catastrophic forgetting. Measures of parameter importance vary; e.g. EWC uses the Fisher Information matrix <ref type="bibr" target="#b35">[36]</ref>. These methods operate in the GNs scenario (Table <ref type="table">1</ref>). Regularization approaches ameliorate but do not exactly eliminate catastrophic forgetting.</p><p>(2) Using exemplars, replay, or generative models These methods aim to explicitly or implicitly (with generative models) capture data from previous tasks. For instance, <ref type="bibr" target="#b39">[40]</ref> performs classification based on the nearest-mean-of-examplars in a feature space. Additionally, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3]</ref> prevent the model from increasing loss on examples from previous tasks while <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b44">[45]</ref> respectively use memory buffers and generative models to replay past data. Exact replay of the entire dataset can trivially eliminate catastrophic forgetting but at great time and memory cost. Generative approaches can reduce catastrophic forgetting, but generators are also susceptible to forgetting. Recently, <ref type="bibr" target="#b49">[50]</ref> successfully mitigate this obstacle by parameterizing a generator with a hypernetwork <ref type="bibr" target="#b14">[15]</ref>.</p><p>(3) Task-specific model components Instead of modifying the learning objective or replaying data, various methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b50">51]</ref> use different model components for different tasks.</p><p>In Progressive Neural Networks (PNN), Dynamically Expandable Networks (DEN), and Reinforced Continual Learning (RCL) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b51">52]</ref>, the model is expanded for each new task. More efficiently, <ref type="bibr" target="#b31">[32]</ref> fixes the network size and randomly assigns which nodes are active for a given task. In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11]</ref>, the weights of disjoint subnetworks are trained for each new task. Instead of learning the weights of the subnetwork, for each new task Mallya et al. <ref type="bibr" target="#b29">[30]</ref> learn a binary mask that is applied to a network pretrained on ImageNet. Recently, Cheung et al. <ref type="bibr" target="#b3">[4]</ref> superimpose many models into one by using different (and nearly orthogonal) contexts for each task. The task parameters can then be effectively retrieved using the correct task context. Finally, BatchE <ref type="bibr" target="#b50">[51]</ref> learns a shared weight matrix on the first task and learn only a rank-one elementwise scaling matrix for each subsequent task.</p><p>Our method falls into this final approach (3) as it introduces task-specific supermasks. However, while all other methods in this category are limited to the GG scenario, SupSup can be used to achieve compelling performance in all four scenarios. We compare primarily with BatchE <ref type="bibr" target="#b50">[51]</ref> and Parameter Superposition (abbreviated PSP) <ref type="bibr" target="#b3">[4]</ref> as they are recent and performative. BatchE requires very few additional parameters for each new task while achieving comparable performance to PNN and scaling to SplitImagenet. Moreover, PSP outperforms regularization based approaches like SI <ref type="bibr" target="#b53">[54]</ref>. However,</p><p>Algorithm Avg Top 1 Bytes Accuracy (%) Upper Bound 92.55 10222.81M 89.58 195.18M SupSup (GG) 88.68 100.98M 86.37 65.50M BatchE (GG) 81.50 124.99M Single Model -102.23M 10 6 10 7 10 8 Total Number of Bytes 0.5 0.6 0.7 0.8 0.9 Accuracy Upper Bound SupSup (GG) SupSup (GG) Transfer BatchE (GG) BatchE (GG) -Rand W Separate Heads Separate Heads -Rand W Figure 2: (left) SplitImagenet performance in Scenario GG. SupSup approaches upper bound performance with significantly fewer bytes. (right) SplitCIFAR100 performance in Scenario GG shown as mean and standard deviation over 5 seed and splits. SupSup outperforms similar size baselines and benefits from transfer.</p><p>both BatchE <ref type="bibr" target="#b50">[51]</ref> and PSP <ref type="bibr" target="#b3">[4]</ref> require task identity to use task-specific weights, so they can only operate in the GG setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we detail how SupSup leverages supermasks to learn thousands of sequential tasks without forgetting. We begin with easier settings where task identity is given and gradually move to more challenging scenarios where task identity is unavailable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In a standard -way classification task, inputs x are mapped to a distribution p over output neurons {1, ..., }. We consider the general case where p = f (x, W ) for a neural network f parameterized by W and trained with a cross-entropy loss. In continual learning classification settings we have k different -way classification tasks and the input size remains constant across tasks<ref type="foot" target="#foot_0">foot_0</ref> .</p><p>Zhou et al. <ref type="bibr" target="#b56">[57]</ref> demonstrate that a trained binary mask (supermask) M can be applied to a randomly weighted neural network, resulting in a subnetwork with good performance. As further explored by Ramanujan et al. <ref type="bibr" target="#b38">[39]</ref>, supermasks can be trained at similar compute cost to training weights while achieving performance competitive with weight training.</p><p>With supermasks, outputs are given by p = f (x, W M ) where denotes an elementwise product.</p><p>W is kept frozen at its initialization: bias terms are 0 and other parameters in W are ±c with equal probability and c is the standard deviation of the corresponding Kaiming normal distribution <ref type="bibr" target="#b16">[17]</ref>. This initialization is referred to as signed Kaiming constant by <ref type="bibr" target="#b38">[39]</ref> and the constant c may be different for each layer. For completeness we detail the Edge-Popup algorithm for training supermasks <ref type="bibr" target="#b38">[39]</ref> in Section E of the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scenario GG: Task Identity Information Given During Train and Inference</head><p>When task identity is known during training we can learn a binary mask M i per task. M i are the only parameters learned as the weights remain fixed. Given data from task i, outputs are computed as</p><formula xml:id="formula_0">p = f x, W M i<label>(1)</label></formula><p>For each new task we can either initialize a new supermask randomly, or use a running mean of all supermasks learned so far. During inference for task i we then use M i . Figure <ref type="figure">2</ref> illustrates that in this scenario SupSup outperforms a number of baselines in accuracy on both SplitCIFAR100 and SplitImageNet while requiring fewer bytes to store. Experiment details are in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scenarios GNs &amp; GNu : Task Identity Information Given During Train Only</head><p>We now consider the case where input data comes from task j, but this task information is unknown to the model at inference time. During training we proceed exactly as in Scenario GG, obtaining k learned supermasks. During inference, we aim to infer task identity-correctly detect that the data belongs to task j-and select the corresponding supermask M j .</p><p>The SupSup procedure for task ID inference is as follows: first we associate each of the k learned supermasks M i with an coefficient α i ∈ [0, 1], initially set to 1/k. Each α i can be interpreted as the "belief" that supermask M i is the correct mask (equivalently the belief that the current unknown task is task i). The model's output is then be computed with a weighted superposition of all learned masks:</p><formula xml:id="formula_1">p(α) = f x, W k i=1 α i M i .<label>(2)</label></formula><p>The correct mask M j should produce a confident, low-entropy output <ref type="bibr" target="#b18">[19]</ref>. Therefore, to recover the correct mask we find the coefficients α which minimize the output entropy H of p(α). One option is to perform gradient descent on α via</p><formula xml:id="formula_2">α ← α -η∇ α H (p (α))<label>(3)</label></formula><p>where η is the step size, and αs are re-normalized to sum to one after each update. Another option is to try each mask individually and pick the one with the lowest entropy output requiring k forward passes. However, we want an optimization method with fixed sub-linear run time (w.r.t. the number of tasks k) which leads α to a corner of the probability simplex -i.e. α is 0 everywhere except for a single 1. We can then take the nonzero index to be the inferred task. To this end we consider the One-Shot and Binary algorithms.</p><p>One-Shot: The task is inferred using a single gradient. Specifically, the inferred task is given by</p><formula xml:id="formula_3">arg max i - ∂H (p (α)) ∂α i<label>(4)</label></formula><p>as entropy is decreasing maximally in this coordinate. This algorithms corresponds to one step of the Frank-Wolfe algorithm <ref type="bibr" target="#b6">[7]</ref>, or one-step of gradient descent followed by softmax re-normalization with the step size η approaching ∞. Unless noted otherwise, x is a single image and not a batch.</p><p>Binary: Resembling binary search, we infer task identity using an algorithm with log k steps.</p><p>At each step we rule out half the tasks-the tasks corresponding to entries in the bottom half of -∇ α H (p (α)). These are the coordinates in which entropy is minimally decreasing. A task i is ruled out by setting α i to zero and at each step we re-normalize the remaining entries in α so that they sum to one. Pseudo-code for both algorithms may be found in Section A of the appendix.</p><p>Once the task is inferred the corresponding mask can be used as in Equation <ref type="formula" target="#formula_0">1</ref>to obtain class probabilities p. In both Scenario GNs and GNu the class probabilities p are returned. In GNu, p forms a distribution over the classes corresponding to the inferred task. Experiments solving thousands of tasks are detailed in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scenario NNs: No Task Identity During Training or Inference</head><p>Task inference algorithms from Scenario GN enable the extension of SupSup to Scenario NNs, where task identity is entirely unknown (even during training). If SupSup is uncertain about the current task identity, it is likely that the data do not belong to any task seen so far. When this occurs a new supermask is allocated, and k (the number of tasks learned so far) is incremented.</p><p>We consider the One-Shot algorithm and say that SupSup is uncertain when performing task identity</p><formula xml:id="formula_4">inference if ν = softmax (-∇ α H (p (α))) is approximately uniform. Specifically, if k max i ν i &lt; 1 + a new mask</formula><p>is allocated and k is incremented. Otherwise mask arg max i ν i is used, which corresponds to Equation <ref type="formula" target="#formula_3">4</ref>. We conduct experiments on learning up to 2500 tasks entirely without any task information, detailed in Section 4.3. Figure <ref type="figure" target="#fig_3">4</ref> shows that SupSup in Scenario NNs achieves comparable performance even to Scenario GNu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Beyond Linear Memory Dependence</head><p>Hopfield networks <ref type="bibr" target="#b19">[20]</ref> implicitly encode a series of binary strings z i ∈ {-1, 1} d with an associated energy function</p><formula xml:id="formula_5">E Ψ (z) = uv Ψ uv z u z v . Each z i is a minima of E Ψ ,</formula><p>and can be recovered with gradient descent. Ψ ∈ R d×d is initially 0, and to encode a new string z i , Ψ ← Ψ + 1 d z i z i . 10 50 100 150 200 250 Num Tasks Learned 0.88 0.90 0.92 0.94 0.96 0.98 Accuracy 10 50 100 150 200 250 Num Tasks Learned SupSup (GNu, H) SupSup (GNu, G) PSP (GG) BatchE (GG) Upper Bound We now consider implicitly encoding the masks in a fixed-size Hopfield network Ψ for Scenario GNu. For a new task i a new mask is learned. After training on task i, this mask will be stored as an attractor in a fixed size Hopfield network. Given new data during inference we perform gradient descent on the Hopfield energy E Ψ with the output entropy H to learn a new mask m. Minimizing E Ψ will hopefully push m towards a mask learned during training while H will push m to be the correct mask. As Ψ is quadratic in mask size, we will not mask the parameters W . Instead we mask the output of every layer except the last, e.g. a network with one hidden layer and mask m is given by</p><formula xml:id="formula_6">f (x, m, W ) = softmax W 2 m σ W 1 x<label>(5)</label></formula><p>for nonlinearity σ. The Hopfield network will then be a similar size as the base neural network. We refer to this method as HopSupSup and provide additional details in Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Superfluous Neurons &amp; an Entropy Alternative</head><p>Similar to previous methods <ref type="bibr" target="#b48">[49]</ref>, HopSupSup requires k output neurons in Scenario GNu. SupSup, however, is performing k-way classification without k output neurons. Given data during inference 1) the task is inferred and 2) the corresponding mask is used to obtain outputs p. The class probabilities p correspond to the classes for the inferred task, effectively reusing the neurons in the final layer.</p><p>SupSup could use an output size of , though we find in practice that it helps significantly to add extra neurons to the final layer. Specifically we consider outputs p ∈ R s and refer to the neurons { + 1, ..., s} as superfluous neurons (s-neurons). The standard cross-entropy loss will push the values of s-neurons down throughout training. Accordingly, we consider an objective G which encourages the s-neurons to have large negative values and can be used as an alternative to entropy in Equation <ref type="formula" target="#formula_3">4</ref>.</p><p>Given data from task j, mask M j will minimize the values of the s-neurons as it was trained to do. Other masks were also trained to minimize the values of the s-neurons, but not for data from task j.</p><p>In Lemma 1 of Section I we provide the exact form of G in code (G = logsumexp (p) with masked gradients for p 1 , ..., p ) and offer an alternative perspective on why G is effective -the gradient of G for all s-neurons exactly mirrors the gradient from the supervised training loss. [51], SplitCIFAR100 randomly partitions CIFAR100 <ref type="bibr" target="#b23">[24]</ref> into 20 different 5-way classification problems. Similarly, SplitImageNet randomly splits the ImageNet <ref type="bibr" target="#b4">[5]</ref> dataset into 100 different 10-way classification tasks. Following <ref type="bibr" target="#b50">[51]</ref> we use a ResNet-18 with fewer channels for SplitCIFAR100 and a standard ResNet-50 <ref type="bibr" target="#b17">[18]</ref> for SplitImageNet. The Edge-Popup algorithm from <ref type="bibr" target="#b38">[39]</ref> is used to obtain supermasks for various sparsities with a layer-wise budget from <ref type="bibr" target="#b34">[35]</ref>. We either initialize each new mask randomly (as in <ref type="bibr" target="#b38">[39]</ref>) or use a running mean of all previous learned masks. This simple method of "Transfer" works very well, as illustrated by Figure <ref type="figure">2</ref>. Additional training details and hyperparameters are provided in Section D.</p><p>0 1000 2000 Num Tasks Learned 0.91 0.92 0.93 0.94 0.95 Accuracy Upper Bound SupSup (GNu, H) SupSup (GNu, G) Lower Bound SupSup (NNs, H) 0 500 1000 1500 2000 2500 Num Tasks Learned 0.4 0.6 0.8 Accuracy Upper Bound SupSup (GNu, H) SupSup (GNu, G) Lower Bound SupSup (NNs, H) 0 500 1000 1500 2000 2500 Num Tasks Learned 0.4 0.6 0.8 Accuracy SupSup (NNs, H) SupSup (GNu, H) SupSup (GNu, G) Upper Bound Lower Bound Computation In Scenario GG, the primary advantage of SupSup from Mallya et al. <ref type="bibr" target="#b30">[31]</ref> or Wen et al. <ref type="bibr" target="#b50">[51]</ref> is that SupSup does not require the base model W to be stored. Since W is random it suffices to store only the random seed. For a fair comparison we also train BatchE <ref type="bibr" target="#b50">[51]</ref> with random weights.</p><p>The sparse supermasks are stored in the standard scipy.sparse.csc<ref type="foot" target="#foot_1">foot_1</ref> format with 16 bit integers. Moreover, SupSup requires minimal overhead in terms of forwards pass compute. Elementwise product by a binary mask can be implemented via memory access, i.e. selecting indices. Modern GPUs have very high memory bandwidth so the time cost of this operation is small with respect to the time of a forward pass. In particular, on a 1080 Ti this operation requires ∼ 1% of the forward pass time for a ResNet-50, less than the overhead of BatchE (computation in Section D).</p><p>Baselines In Figure <ref type="figure">2</ref>, for "Separate Heads" we train different heads for each task using a trunk (all layers except the final layer) trained on the first task. In contrast "Separate Heads -Rand W" uses a random trunk. BatchE results are given with the trunk trained on the first task (as in <ref type="bibr" target="#b50">[51]</ref>) and random weights W . For "Upper Bound", individual models are trained for each task. Furthermore, the trunk for task i is trained on tasks 1, ..., i. For "Lower Bound" a shared trunk of the network is trained continuously and a separate head is trained for each task. Since catastrophic forgetting occurs we omit "Lower Bound" from Figure <ref type="figure">2</ref> (the SplitCIFAR100 accuracy is 24.5%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scenarios GNs &amp; GNu: Task Identity Information Given During Train Only</head><p>Our solutions for GNs and GNu are very similar. Because GNu is strictly more difficult, we focus on only evaluating in Scenario GNu. For relevant figures we provide a corresponding table in Section H.</p><p>Datasets Experiments are conducted on PermutedMNIST, RotatedMNIST, and SplitMNIST. For PermutedMNIST <ref type="bibr" target="#b22">[23]</ref>, new tasks are created with a fixed random permutation of the pixels of MNIST. For RotatedMNIST, images are rotated by 10 degrees to form a new task with 36 tasks in total (similar to <ref type="bibr" target="#b3">[4]</ref>). Finally SplitMNIST partitions MNIST into 5 different 2-way classification tasks, each containing consecutive classes from the original dataset.</p><p>Training We consider two architectures: 1) a fully connected network with two hidden layers of size 1024 (denoted FC 1024-1024 and used in <ref type="bibr" target="#b3">[4]</ref>) 2) the LeNet 300-100 architecture <ref type="bibr" target="#b24">[25]</ref> as used in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>. For each task we train for 1000 batches of size 128 using the RMSProp optimizer <ref type="bibr" target="#b47">[48]</ref> with learning rate 0.0001 which follows the hyperparameters of <ref type="bibr" target="#b3">[4]</ref>. Supermasks are found using the algorithm of Mallya et al. <ref type="bibr" target="#b30">[31]</ref> with threshold value 0. However, we initialize the real valued "scores" with Kaiming uniform as in <ref type="bibr" target="#b38">[39]</ref>. Training the mask is not a focus of this work, we choose this method as it is fast and we are not concerned about controlling mask sparsity as in Section 4.1.</p><p>Evaluation At test time we perform inference of task identity once for each batch. If task is not inferred correctly then accuracy is 0 for the batch. Unless noted otherwise we showcase results for the most challenging scenario -when the task identity is inferred using a single image. We use "Full Batch" to indicate that all 128 images are used to infer task identity. Moreover, we experiment with both the the entropy H and G (Section 3.6) objectives to perform task identity inference.</p><p>Results Figure <ref type="figure" target="#fig_3">4</ref> illustrates that SupSup is able to sequentially learn 2500 permutations of MNIST-SupSup succeeds in performing 25,000-way classification. This experiment is conducted with the One-Shot algorithm (requiring one gradient computation) using single images to infer task identity. The same trends hold in Figure <ref type="figure" target="#fig_1">3</ref>, where SupSup outperforms methods which operate in Scenario GG The One-Shot algorithm can be used to infer task identity for BatchE <ref type="bibr" target="#b50">[51]</ref>. Experiment conducted with FC 1024-1024 on PermutedMNIST using an output size of 500, shown as mean and stddev over 3 runs. by using the One-Shot algorithm to infer task identity. In Figure <ref type="figure" target="#fig_1">3</ref>, output sizes of 100 and 500 are respectively used for LeNet 300-100 and FC 1024-1024. The left hand side of Figure <ref type="figure" target="#fig_4">5</ref> illustrates that SupSup is able to infer task identity even when tasks are similar-SupSup is able to distinguish between rotations of 10 degrees. Since this is a more challenging problem, we use a full batch and the Binary algorithm to perform task identity inference. Figure <ref type="figure" target="#fig_7">7</ref> (appendix) shows that for HopSupSup on SplitMNIST, the new mask m converges to the correct supermask in &lt; 30 gradient steps.</p><p>Baselines &amp; Ablations Figure <ref type="figure" target="#fig_4">5</ref> (left) shows that even in Scenario GNu, SupSup is able to outperform PSP <ref type="bibr" target="#b3">[4]</ref> and BatchE <ref type="bibr" target="#b50">[51]</ref> in Scenario GG-methods using task identity. We compare SupSup in GNu with methods in this strictly easier scenario as they are more competitive. For instance, <ref type="bibr" target="#b48">[49]</ref> considers sequential learning problems with only 5-10 tasks. SupSup, after sequentially learning 250 permutations of MNIST, outperforms all non-replay methods from <ref type="bibr" target="#b2">[3]</ref> in the GNu scenario after they have learned only 10 permutations of MNIST with a similar network. In GNu, Online EWC achieves 33.88% &amp; SI achieves 29.31% on 10 permutations of MNIST <ref type="bibr" target="#b48">[49]</ref> while SupSup achieves 94.91% accuracy after 250 permutations (see Table <ref type="table">5</ref> in <ref type="bibr" target="#b48">[49]</ref> vs. Table <ref type="table">7</ref>).</p><p>In Figure <ref type="figure" target="#fig_4">5</ref> (right) we equip BatchE with task inference using our One-Shot algorithm. Instead of attaching a weight α i to each supermask, we attach a weight α i to each rank-one matrix <ref type="bibr" target="#b50">[51]</ref>. Moreover, in Section C of the appendix we augment BatchE to perform task-inference using large batch sizes. "Upper Bound" and "Lower Bound" are the same as in Section 4.1. Moreover, Figure <ref type="figure" target="#fig_5">6</ref> illustrates the importance of output size. Further investigation of this phenomena is provided by Section 3.6 and Lemma 1 of Section I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scenario NNs: No Task Identity During Training or Inference</head><p>For the NNs Scenario we consider PermutedMNIST and train on each task for 1000 batches (the model does not have access to this iteration number). Every 100 batches the model must choose to allocate a new mask or pick an existing mask using the criteria from Section 3.4 ( = 2 -3 ). Figure <ref type="figure" target="#fig_3">4</ref> illustrates that without access to any task identity (even during training) SupSup is able to learn thousands of tasks. However, a final dip is observed as a budget of 2500 supermasks total is enforced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Supermasks in Superposition (SupSup) is a flexible and compelling model applicable to a wide range of scenarios in Continual Learning. SupSup leverages the power of subnetworks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31]</ref>, and gradient-based optimization to infer task identity when unknown. SupSup achieves state-ofthe-art performance on SplitImageNet when given task identity, and performs well on thousands of permutations and almost indiscernible rotations of MNIST without any task information.</p><p>We observe limitations in applying SupSup with task identity inference to non-uniform and more challenging problems. Task inference fails when models are not well calibrated-are overly confident for the wrong task. As future work, we hope to explore automatic task inference with more calibrated models <ref type="bibr" target="#b13">[14]</ref>, as well as circumventing calibration challenges by using optimization objectives such as self-supervision <ref type="bibr" target="#b15">[16]</ref> and energy based models <ref type="bibr" target="#b12">[13]</ref>. In doing so, we hope to tackle large-scale problems in Scenarios GN and NNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>A goal of continual learning is to solve many tasks with a single model. However, it is not exactly clear what qualifies as a single model. Therefore, a concrete objective has become to learn many tasks as efficiently as possible. We believe that SupSup is a useful step in this direction. However, there are consequences to more efficient models, both positive and negative.</p><p>We begin with the positive consequences:</p><p>• Efficient models require less compute, and are therefore less harmful for the environment then learning one model per task <ref type="bibr" target="#b43">[44]</ref>. This is especially true if models are able to leverage information from past tasks, and training on new tasks is then faster.</p><p>• Efficient models may be run on the end device. This helps to preserve privacy as a user's data does not have to be sent to the cloud for computation.</p><p>• If models are more efficient then large scale research is not limited to wealthier institutions. These institutions are more likely in privileged parts of the world and may be ignorant of problems facing developing nations. Moreover, privileged institutions may not be a representative sample of the research community.</p><p>We would also like to highlight and discuss the negative consequences of models which can efficiently learn many tasks, and efficient models in general. When models are more efficient, they are also more available and less subject to regularization and study as a result. For instance, when a high-impact model is released by an institution it will hopefully be accompanied by a Model Card <ref type="bibr" target="#b33">[34]</ref> analyzing the bias and intended use of the model. By contrast, if anyone is able to train a powerful model this may no longer be the case, resulting in a proliferation of models with harmful biases or intended use. Taking the United States for instance, bias can be harmful as models show disproportionately more errors for already marginalized groups <ref type="bibr" target="#b1">[2]</ref>, furthering existing and deeply rooted structural racism.</p><formula xml:id="formula_7">Algorithm 1 One-Shot(f, x, W, k, {M i } k i=1 , H) 1: α ← 1 k 1 k ... 1 k Initialize α 2: p ← f x, W k i=1 α i M i Superimposed output 3: return arg max i -∂H(p) ∂αi</formula><p>Return coordinate for which objective maximally decreasing</p><formula xml:id="formula_8">Algorithm 2 Binary(f, x, W, k, {M i } k i=1 , H) 1: α ← 1 k 1 k ... 1 k Initialize α 2: while α 0 &gt; 1 do</formula><p>Iterate until α has a single nonzero entry 3:</p><formula xml:id="formula_9">p ← f x, W k i=1 α i M i Superimposed output 4: g ← -∇ α H (p) Gradient of objective 5:</formula><p>for i ∈ {1, ..., k} do In code this for loop is vectorized 6:</p><p>if g i ≤ median (g) then 7:</p><p>α i ← 0 Zero out α i for which objective minimally decreasing 8:</p><p>α ← α/ α 1 Re-normalize α to sum to 1 9: return arg max i α i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Algorithm pseudo-code</head><p>Algorithms 1 and 2 respectively provide pseudo-code for the One-Shot and Binary algorithms detailed in Section 3.3. Both aim to infer the task j ∈ {1, ..., k} associated with input data x by minimizing the objective H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extended Details for HopSupSup</head><p>This section provides further details and experiments for HopSupSup (introduced in Section 3.5). HopSupSup provides a method for storing the growing set of supermasks in a fixed size reservoir instead of explicitly storing each mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Training</head><p>Recall that HopSupSup operates in Scenario GNu and so task identity is known during training. Instead of explicitly storing each mask, we will instead store two fixed sized variables Ψ and µ which are both initially 0. The weights of the Hopfield network are Ψ and µ stores a running mean of all masks learned so far. For a new task k we use the same algorithm as in Section 4.2 to learn a binary mask m i which performs well for task k. Since Hopfield networks consider binary strings in {-1, 1} d and we use masks m i ∈ {0, 1} d we will consider z k = 2m k -1. In practice we then update Ψ and µ as</p><formula xml:id="formula_10">Ψ ← Ψ + 1 d z k z k -z k Ψz k -Ψz k z k -Id , µ ← k -1 k µ + 1 k z k (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where Id is the identity matrix. This update rule for Ψ is referred to as the Storkey learning rule <ref type="bibr" target="#b45">[46]</ref> and is more expressive than the alternative-the Hebbian rule <ref type="bibr" target="#b19">20]</ref> provided for brevity in Section 3.3. With either update rules the learned z i will be a minimizer of the Hopfield energy E Ψ (z) = uv Ψ uv z u z v .</p><formula xml:id="formula_12">Ψ ← Ψ + 1 d z k z k [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Inference</head><p>During inference we receive data x from some task j, but this task information is not given to the model. HopSupSup first initializes a new binary string z with µ. Next, HopSupSup uses gradient descent to minimize the Hopfield energy in conjunction with the output entropy using mask m = 1 2 z + 1, a process we refer to as Hopfield Recovery. Minimizing the energy will hopefully push m (equivalently z) towards a mask learned during training and minimizing the entropy will hopefully Step 10 20 30</p><p>Step</p><formula xml:id="formula_13">m -m 0 1 m -m 1 1 m -m 2 1 m -m 3 1 m -m 4 1 10 20 30</formula><p>Step 10 20 30</p><p>Step push m towards the correct mask m j . We may then use the recovered mask to compute the network output.</p><p>In practice we use one pass through the evaluation set (with batch size 64, requiring T ≈ 30 steps) to recover a mask and another to perform evaluation with the recovered mask. When recovering the mask we gradually increase the strength of the Hopfield term and decrease the strength of the entropy term. Otherwise the Hopfield term initially pulls z in the wrong direction or the final z does not lie at a minimum of E Ψ . For step t ∈ {1, ..., T }, and constant γ we use the objective J as</p><formula xml:id="formula_14">J (z, t) = γt T E Ψ (z) + 1 - t T H (p)<label>(7)</label></formula><p>where p denotes the output using mask m = 1 2 z + 1. Figure <ref type="figure" target="#fig_7">7</ref> illustrates that after approximately 30 steps of gradient descent on z using objective J , the mask m = 1 2 z + 1 converges to the correct mask learned during training. This experiment is conducted for 20 different random seeds on SplitMNIST (see Section 4.2) training for 1 epoch per task. Evaluation with the recovered mask for each seed is then given by Figure <ref type="figure">8</ref>. As expected, when the correct mask is successfully recovered, accuracy matches directly using the correct mask. For hyperparameters we set γ = 1.5 • 10 -3 and perform gradient descent during Hopfield recovery with learning rate 0.5 • 10 3 , momentum 0.9, and weight decay 10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Network Architecture</head><p>Let BN denote non-affine batch normalization <ref type="bibr" target="#b20">[21]</ref>, i.e. batch normalization with no learned parameters. Also recall that we are masking layer outputs instead of weights, and the weights still remain fixed (see Section 3.5). Therefore, with mask m = (m 1 , m 2 ) and weights</p><formula xml:id="formula_15">W = (W 1 , W 2 , W 3 ) we compute outputs as f (x, m, W ) = softmax W 3 σ m 2 BN W 2 σ m 1 BN W 1 x<label>(8)</label></formula><p>where σ denotes the Swish nonlinearity <ref type="bibr" target="#b37">[38]</ref>. Without masking or normalization f is a fully connected network with two hidden layers of size 2048. We also note that HopSupSup requires 10 output neurons for SplitMNIST in Scenario GNu, and the composition of non-affine batch normalization with a binary mask was inspired by BatchNets <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Augmenting BatchE For Scnario GNu</head><p>In Section 4.2 we demonstrate that BatchE <ref type="bibr" target="#b50">[51]</ref> is able to infer task identity using the One-Shot algorithm. In this section we show that, equipped with H from Section 3, BatchE can also infer task identity by using a large batch size. We refer to this method as Augmented BatchE (ABatchE).   <ref type="table">1</ref> represented in a tree graph, as in <ref type="bibr" target="#b54">[55]</ref>.</p><p>For clarity we describe ABatchE for one linear layer, i.e. we describe the application of ABatchE to</p><formula xml:id="formula_16">f (x, W ) = softmax W x<label>(9)</label></formula><p>for input data x ∈ R m and weights W ∈ R m×n . In BatchE <ref type="bibr" target="#b50">[51]</ref>, W is trained on the first task then frozen. For task i BatchE learns "fast weights" r i ∈ R m , s i ∈ R n and outputs are computed via</p><formula xml:id="formula_17">f (x, W ) = softmax W r i s i x .<label>(10)</label></formula><p>Wen et al. <ref type="bibr" target="#b50">[51]</ref> further demonstrate that Equation 10 can be vectorized as</p><formula xml:id="formula_18">f (x, W ) = softmax W (x r i ) s i<label>(11)</label></formula><p>or, for a batch of data</p><formula xml:id="formula_19">X ∈ R b×m , f (X, W ) = softmax X R b i W S b i .<label>(12)</label></formula><p>In Equation <ref type="formula" target="#formula_19">12</ref>, R b i ∈ R b×m is a matrix where each of the b rows is r i (likewise S b i ∈ R b×n is a matrix where each of the b rows is s i ).</p><p>As in Section 3.3 we now consider the case where data X ∈ R b×m comes from task j but this information is not known to the model. For ABatchE we repeat the data k times, where k is the number of tasks learned so far, and use different "fast weights" for each repetiton. Specifically, we consider repeated data X ∈ R bk×m and augmented matricies R ∈ R bk×m and S ∈ R bk×n given by</p><formula xml:id="formula_20">X =     X X . . . X     , R =      R b 1 R b 2 . . . R b k      , S =      S b 1 S b 2 . . . S b k      . (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>Outputs are then computed as where the b rows (bi, ..., bi + b -1) of the output correspond exactly to Equation <ref type="formula" target="#formula_19">12</ref>. The task may then be inferred by choosing the i for which the rows (bi, ..., b(i + 1) -1) minimize the objective H. If f (X, W ) i denotes row i of f (X, W ) then for objective H the inferred task for ABatchE is</p><formula xml:id="formula_22">f (X, W ) = softmax X R W S<label>(14)</label></formula><formula xml:id="formula_23">arg min i b-1 ω=0 H (f (X, W ) bi+ω ) .<label>(15)</label></formula><p>To extend ABatchE to deep neural networks the matricies R and S are constructed for each layer.</p><p>One advantage of ABatchE over SupSup is that no backwards pass is required. However, ABatchE uses a very large batch size for large k, and the forward pass therefore requires more compute and memory. Another disadvantage of ABatchE is that the performance of ABatchE is limited by the performance of BatchE. In Section 4.2 we demonstrate that SupSup outperforms BatchE when BatchE is given task identity information.</p><p>Since the objective for ABatchE need not be differentiable we also experiment with an alternative metric of confidence M(p) =max i p i . We showcase results for ABatchE on PermutedMNIST in Figure <ref type="figure" target="#fig_10">10</ref>  As in <ref type="bibr" target="#b50">[51]</ref> we train each model for 250 epochs per task. We use standard hyperparameters-the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with a batch size of 128 and learning rate 0.001 (no warmup, cosine decay <ref type="bibr" target="#b27">[28]</ref>). For SupSup we follow <ref type="bibr" target="#b38">[39]</ref> and use non-affine normalization so there are no learned parameters. We do have to store the running mean and variance for each task, which we include in the parameter count. We found it better to use a higher learning rate (0.1) when training BatchE (Rand W ), and the standard BatchE number is taken from <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 SplitImageNet (GG)</head><p>We use the Upper Bound and BatchE number from <ref type="bibr" target="#b50">[51]</ref>. For SupSup we train for 100 epochs with a batch size of 256 using the Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with learning rate 0.001 (5 epochs warmup, cosine decay <ref type="bibr" target="#b27">[28]</ref>). For SupSup we follow <ref type="bibr" target="#b38">[39]</ref> and use non-affine normalization so there are no learned parameters. We do have to store the running mean and variance for each task, which we include in the parameter count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 GNu Experiments</head><p>We clarify some experimental details for GNu experiments &amp; baselines. For the BatchE <ref type="bibr" target="#b50">[51]</ref> baseline we find it best to use kaiming normal initialization with a learning rate of 0.01 (0.0001 for the first task when the weights are trained). As we are considering hundreds of tasks, instead of training </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Analysis</head><p>In this section we assume a slightly more technical perspective. The aim is not to formally prove properties of the algorithm. Rather, we hope that a more mathematical language may prove useful in extending intuition. Just as the empirical work of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b38">39]</ref> was given a formal treatment in <ref type="bibr" target="#b28">[29]</ref>, we hope for more theoretical work to follow.</p><p>Our grounding intuition remains from Section 3.3-the correct mask will produce the lowest entropy output. Moreover, since entropy is differentiable, gradient based optimization can be used to recover the correct mask. However, many questions remain: Why do superfluous neurons (Section 3.6) help?</p><p>In the case of MNISTPermuation, why is a single gradient sufficient? Although it is a simple case, steps forward can be made by analyzing the training of a linear head on fixed features. With random features, training a linear head on fixed features is considered in the literature of reservoir computing <ref type="bibr" target="#b42">[43]</ref>, and more <ref type="bibr" target="#b0">[1]</ref>.</p><p>Consider k different classification problems with fixed features φ(x) ∈ R m . Traditionally, one would use learned weights W ∈ R m×n to compute logits</p><formula xml:id="formula_24">y = W φ(x)<label>(16)</label></formula><p>and output classification probabilities p = softmax(y) where</p><formula xml:id="formula_25">p v = exp(y v ) n v =1 exp(y v ) .<label>(17)</label></formula><p>Recall that with SupSup we compute the logits for task i using fixed random weights W and a learned binary mask M i ∈ {0, 1} m×n as</p><formula xml:id="formula_26">y = W M i φ(x)<label>(18)</label></formula><p>where denotes an element-wise product and no bias term is allowed. Moreover, W uv = ξ uv 2/m where ξ uv is chosen independently to be either -1 or 1 with equal probability and the constant 2/m follows Kaiming initialization <ref type="bibr" target="#b16">[17]</ref>.</p><p>Say we are given data x from task j. From now on we will refer to task j as the correct task. Recall from Section 3.3 that SupSup attempts to infer the correct task by using a weighted mixture of masks</p><formula xml:id="formula_27">y = W i α i M i φ(x)<label>(19)</label></formula><p>where the coefficients α i sum to one, and are initially set to 1/k.</p><p>To infer the correct task we attempt to construct a function G(y; α) with the following property: For fixed data, G is minimized when α = e j (e j denotes a k-length vector that is 1 in index j and 0 otherwise). We can then infer the correct task by solving a minimization problem.</p><p>As in One-Shot, we use a single gradient computation to infer the task via</p><formula xml:id="formula_28">arg max i - ∂G ∂α i .<label>(20)</label></formula><p>A series of Lemmas will reveal how a single gradient step may be sufficient when tasks are unrelated (e.g. as in PermutedMNIST). We begin with the construction of a useful function G, which will correspond exactly to G in Section 3.6. As in Section 3.6, this construction is made possible through superfluous neurons (s-neurons): The true labels are in {1, ..., }, and a typical output is therefore length . However, we add ns-neurons resulting in a vector y of length n.</p><p>Let S denote the set of s-neurons and R denote the set of real neurons where |S| = nand |R| = .</p><p>Moreover, assume that a standard cross-entropy loss is used during training, which will encourage s-neurons to have small values. Lemma I.1. It is possible to construct a function G such that the gradient matches the gradient from the supervised training loss L for all s-neurons. Specifically, ∂G ∂yv = ∂L ∂yv for all v ∈ S and 0 otherwise.</p><p>Proof. Let g v = ∂G ∂yv . It is easy to ensure that g v = 0 for all v ∈ S with a modern neural network library like PyTorch <ref type="bibr" target="#b36">[37]</ref> as detaching<ref type="foot" target="#foot_2">foot_2</ref> the outputs from the neurons v ∈ S prevents gradient signal from reaching them. In code, let y be the outputs and m be a binary vector with m v = 1 if v ∈ S and 0 otherwise, then</p><formula xml:id="formula_29">y = (1 -m) * y.detach() + m * y<label>(21)</label></formula><p>will prevent gradient signal from reaching y v for v ∈ S.</p><p>Recall that the standard cross-entropy loss is</p><formula xml:id="formula_30">L(y) = -log exp(y c ) n v =1 exp(y v ) = -y c + log n v =1 exp(y v )<label>(22)</label></formula><p>where c ∈ {1, ..., } is the correct label. The gradient of L to any s-neuron v is then</p><formula xml:id="formula_31">∂L ∂y v = exp(y v ) n v =1 exp(y v ) .<label>(23)</label></formula><p>If we define G as </p><formula xml:id="formula_32">G(y; α) = log n v =1 exp(y v )<label>(24</label></formula><p>where model(...) computes Equation <ref type="formula" target="#formula_27">19</ref>.</p><p>In the next two Lemmas we aim to show that, in expectation, -∂G ∂αi ≤ 0 for i = j while -∂G ∂αj &gt; 0. Recall that j is the correct task-the task from which the data is drawn-and we will use i to refer to a different task.</p><p>When we take expectation, it is with respect to the random variables ξ, {M ω } ω∈{1,..,k} , and x. Before we proceed further a few assumptions are formalized, e.g. what it means for tasks to be unrelated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 1:</head><p>We assume that the mask learned on task i will be independent from the data from task j: If the data is from task j then φ(x) and M i and independent random variables.</p><p>Assumption 2: We assume that a negative weight and positive weight are equally likely to be masked out. As a result, E ξ uv M i uv = 0. Note that when E [φ(x)] = 0, which will be the case for zero mean random features, there should be little doubt that this assumption should hold. Lemma I.2. If data x comes from task j and i = j then E -</p><formula xml:id="formula_34">∂G ∂α i ≤ 0<label>(26)</label></formula><p>Proof. We may write the gradient as</p><formula xml:id="formula_35">∂G ∂α i = n v=1 ∂G ∂y v ∂y v ∂α i<label>(27)</label></formula><p>and use that ∂G ∂yv = 0 for v ∈ S. Moreover, y v may be written as</p><formula xml:id="formula_36">y v = n u=1 φ(x) u W uv k i=1 α i M i uv (<label>28</label></formula><formula xml:id="formula_37">)</formula><p>with W uv = ξ uv 2/m and so Equation <ref type="formula" target="#formula_35">27</ref>becomes</p><formula xml:id="formula_38">∂G ∂α i = √ 2 √ m v∈S n u=1 ∂G ∂y v φ(x) u ξ uv M i uv .<label>(29)</label></formula><p>Taking the expectation (and using linearity) we obtain</p><formula xml:id="formula_39">E ∂G ∂α i = √ 2 √ m v∈S n u=1 E ∂G ∂y v φ(x) u ξ uv M i uv .<label>(30)</label></formula><p>In Lemma J.1 we formally show that each term in this sum is greater than or equal to 0, which completes this proof. However, we can see informally now why expectation should be close to 0 if we ignore the gradient term as</p><formula xml:id="formula_40">E φ(x) u ξ uv M i uv = E [φ(x) u ] E ξ uv M i uv = 0<label>(31)</label></formula><p>where the first equality follows from Assumption 1 and the latter follows from Assumption 2.</p><p>We have now seen that in expectation -∂G ∂αi ≤ 0 for i = j. It remains to be shown that we should expect -∂G ∂αj &gt; 0. Lemma I.3. If data x comes from the task j then</p><formula xml:id="formula_41">E - ∂G ∂α j &gt; 0.<label>(32)</label></formula><p>Proof. Following Equation <ref type="formula" target="#formula_39">30</ref>, it suffices to show that for u ∈ {1, ..., m}, v ∈ S E -∂G ∂y v φ(x) u ξ uv M j uv &gt; 0.</p><p>Since v ∈ S we may invoke Lemma I.1 to rewrite our objective as</p><formula xml:id="formula_43">E - ∂L ∂y v φ(x) u ξ uv M j uv &gt; 0 (<label>34</label></formula><formula xml:id="formula_44">)</formula><p>where L is the supervised loss used for training. Recall that in the mask training algorithm, real valued scores S j uv are associated with M j uv <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30]</ref>. The update rule for S j uv on the backward pass is then S j uv ← S j uv + η -∂L ∂y v φ(x) u ξ uv <ref type="bibr" target="#b34">(35)</ref> for some learning rate η. Following Mallya et al. <ref type="bibr" target="#b29">[30]</ref> (with threshold 0, as used in Section 4.2), we let M j uv = 1 if S j uv &gt; 0 and otherwise assign M j uv = 0. As a result, we expect that M j uv is 1 when -∂L ∂yv φ(x) u ξ uv is more consistently positive than negative. In other words, the expected product of M j uv and -∂L ∂yv φ(x) u ξ uv is positive, satisfying Equation 34.</p><p>Together, three Lemmas have demonstrated that in expectation -∂G ∂αi ≤ 0 for i = j while -∂G ∂αj &gt; 0. Accordingly, we should expect that</p><formula xml:id="formula_45">arg max i - ∂G ∂α i . (<label>36</label></formula><formula xml:id="formula_46">)</formula><p>returns the correct task j. While a full, formal treatment which includes the analysis of noise is beyond the scope of this work, we hope that this section has helped to further intuition. However, we are missing one final piece-what is the relation between G and H?</p><p>It is not difficult to imagine that H should imitate the loss, which attempts to raise the score of one logit while bringing all others down. Analytically we find that H can be decomposed into two terms as follows </p><formula xml:id="formula_47">H (p) = -</formula><p>where the latter term is G. With more and more neurons in the output layer, p v will become small moving H towards G. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Additional Technical Details</head><formula xml:id="formula_49">∂G ∂y v = p v = exp(y v ) n v =1 exp(y v )<label>(41)</label></formula><p>and so we rewrite equation 40 as</p><formula xml:id="formula_50">E p v φ(x) u ξ uv M i uv ≥ 0.<label>(42)</label></formula><p>By the law of total expectation E p v φ(x) u ξ uv M i uv = E E p v φ(x) u ξ uv M i uv φ(x) u ξ uv M i uv <ref type="bibr" target="#b42">(43)</ref> and so it suffices to show that</p><formula xml:id="formula_51">E p v φ(x) u ξ uv M i uv φ(x) u ξ uv M i uv = κ ≥ 0<label>(44)</label></formula><p>for any κ ≥ 0. In the case where where κ = 0 Equation 44 becomes</p><formula xml:id="formula_52">E 0p v φ(x) u ξ uv M i uv = 0 = 0<label>(45)</label></formula><p>and so we are only left to consider κ &gt; 0. Note that κ &gt; 0 restricts M i uv to be 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) During training SupSup learns a separate supermask (subnetwork) for each task.(right) At inference time, SupSup can infer task identity by superimposing all supermasks, each weighted by an α i , and using gradients to maximize confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Using One-Shot to infer task identity, SupSup outperforms methods with access to task identity. Results shown for PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>GG: Task Identity Information Given During Train and Inference Datasets, Models &amp; Training In this experiment we validate the performance of SupSup on SplitCIFAR100 and SplitImageNet. Following Wen et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning 2500 tasks and inferring task identity using the One-Shot algorithm. Results for both the GNu and NNs scenarios with the LeNet 300-100 model using output size 500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (left) Testing the FC 1024-1024 model on RotatedMNIST. SupSup uses Binary to infer task identity with a full batch as tasks are similar (differing by only 10 degrees). (right)The One-Shot algorithm can be used to infer task identity for BatchE<ref type="bibr" target="#b50">[51]</ref>. Experiment conducted with FC 1024-1024 on PermutedMNIST using an output size of 500, shown as mean and stddev over 3 runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The effect of output size s on SupSup performance using the One-Shot algorithm. Results shown for PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: During Hopfield Recovery the new mask m converges to the correct mask learned during training. Note that m i denotes the mask learned for task i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Evaluating (with 20 random seeds) on SplitM-NIST after finding a mask with Hopfield Recovery. Average accuracy is 97.43%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Testing ABatchE on PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right) with output size 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>for various values of b. The entropy objective H performs better than M, and forgetting is only mitigated when using 16 images (b = 16). With 250 tasks, b = 16 corresponds to a batch size of 4000. D Extended Training Details D.1 SplitCIFAR-100 (GG)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><figDesc>) then g v = ∂L ∂yv as needed. Expressed in code y = model(x); G = torch.logsumexp((1 -m) * y.detach() + m * y, dim=1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Lemma J. 1 .</head><label>1</label><figDesc>If j is the true task and i = j thenE ∂G ∂y v φ(x) u ξ uv M i uv ≥ 0(40)Proof. Recall from Lemma I.1 that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 9 :</head><label>9</label><figDesc>Accuracy on PermutedMNIST with FC 1024-1024 corresponding to Figure6(right).</figDesc><table><row><cell>Entry</cell><cell>10</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>Avg</cell></row><row><cell cols="8">SupSup (GNu s = 200 H) 96.28 96.14 96.04 95.91 95.86 95.66 95.98 SupSup (GNu s = 200 G) 96.28 96.19 96.05 96.00 95.99 95.92 96.07 SupSup (GNu s = 100 H) 95.90 94.77 94.02 93.71 93.00 92.84 94.04 SupSup (GNu s = 100 G) 96.31 95.83 95.60 95.32 95.05 94.88 95.50 82.28 69.06 64.51 60.99 58.15 57.03 65.34 SupSup (GNu s = 25 H) 96.31 93.17 91.20 90.26 89.04 88.19 91.36 SupSup (GNu s = 25 G) Lower Bound 76.89 49.40 38.93 34.53 31.30 29.36 43.40</cell></row><row><cell>Upper Bound</cell><cell cols="7">96.76 96.70 96.68 96.66 96.63 96.61 96.67</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In practice the tasks do not all need to be -way -output layers can be padded until all have the same size.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://docs.scipy.org/doc/scipy/reference/sparse.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://pytorch.org/docs/stable/autograd.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Gabriel Ilharco Magalhães</rs> and <rs type="person">Sarah Pratt</rs> for helpful comments. For valuable conversations we also thank <rs type="person">Tim Dettmers</rs>, <rs type="person">Kiana Ehsani</rs>, <rs type="person">Ana Marasović</rs>, <rs type="person">Suchin Gururangan</rs>, <rs type="person">Zoe Steine-Hanson</rs>, <rs type="person">Connor Shorten</rs>, <rs type="person">Samir Yitzhak Gadre</rs>, <rs type="person">Samuel McKinney</rs> and <rs type="person">Kishanee Haththotuwegama</rs>. This work is in part supported by <rs type="funder">NSF</rs> <rs type="grantNumber">IIS 1652052</rs>, IIS <rs type="grantNumber">17303166</rs>, <rs type="grantNumber">DARPA N66001-19-2-4031</rs>, <rs type="grantNumber">DARPA W911NF-15-1-0543</rs> and gifts from <rs type="institution">Allen Institute for Artificial Intelligence</rs>. Additional revenues: co-authors had employment with the <rs type="institution">Allen Institute for AI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GfdxMwn">
					<idno type="grant-number">IIS 1652052</idno>
				</org>
				<org type="funding" xml:id="_Ga9q9mT">
					<idno type="grant-number">17303166</idno>
				</org>
				<org type="funding" xml:id="_VYqe9PQ">
					<idno type="grant-number">DARPA N66001-19-2-4031</idno>
				</org>
				<org type="funding" xml:id="_fbYuqrp">
					<idno type="grant-number">DARPA W911NF-15-1-0543</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>separate heads per tasks when training BatchE we also apply the rank one pertubation to the final layer. PSP <ref type="bibr" target="#b3">[4]</ref> provides MNISTPerm results so we use the same hyperparameters as in their code. We compare with rotational superposition, the best performing model from PSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Speed of the Masked Forward Pass</head><p>We now provide justification for the calculation mentioned in Section 4.1-when implemented properly the masking operation should require ∼ 1% of the total time for a forward pass (for a ResNet-50 on a NVIDIA GTX 1080 Ti GPU). It is reasonable to assume that selecting indices is roughly as quick as memory access. A NVIDIA GTX 1080 Ti has a memory bandwidth of 480 GB/s. A ResNet-50 has around 2.5 • 10 7 4-byte (32-bit) parameters-roughly 0.1 GB. Therefore, indexing over a ResNet-50 requires at most 0.1 GB/ (480 GB/s) ≈ 0.21 ms. For comparison, the average forward pass of a ResNet-50 for a 3 × 224 × 224 image on the same GPU is about 25 ms.</p><p>Note that NVIDIA hardware specifications generally assume best-case performance with sequential page reads. However, even if real-world memory bandwidth speeds are 60-70% slower than advertised, the fraction of masking time would remain in the ≤ 3% range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Additional Transfer Experiment</head><p>For our transfer experiments, we initialize the score matrix (see Appendix E) for task i with the running mean of the supermasks for tasks 0 through i -1. The scores for task 0 are initialized as in <ref type="bibr" target="#b38">[39]</ref>. We further normalize by the Kaiming fan-in constant from <ref type="bibr" target="#b16">[17]</ref>, so that the norm of our supermask matrix is reasonable. If we do not perform this normalization, accuracy degrades significantly. All other training hyperparameters are the same as in Section D.1.</p><p>In Figure <ref type="figure">11</ref>, we demonstrate that Transfer enables faster learning for SplitCIFAR. In this experiment, we train task 0 for the full 250 epochs and all subsequent tasks for either 50 epochs (with transfer) or 100 epochs (without transfer). We see that adding transfer yields an improvement even while using about half the number of training iterations overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Supermask Training with Edge-Popup</head><p>For completeness we briefly recap the Edge-Popup algorithm for training supermasks as introduced by <ref type="bibr" target="#b38">[39]</ref>. Consider a linear layer with inputs x ∈ R m and outputs y = (W M ) x where W ∈ R m×n are the fixed weights and M ∈ {0, 1} m×n is the supermask. The Edge-Popup algorithm learns a score matrix S ∈ R m×n + and computes the mask via M = h(S). The function h sets the top k% of entries in S to 1 and the remaining to 0. Edge-Popup updates S via the straight through estimator-h is considered to be the identity on the backwards pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Comparing Binary and One-Shot</head><p>In Figure <ref type="figure">11</ref> (left) we interpolate between the Binary and One-Shot algorithms. We replace line 6 of Algorithm 2, g i ≤ median(g), with g i ≤ top-γ%-element(g). Then when γ = 1/2 we recover the binary algorithm (as median(g) = top-50%-element(g)) and when γ = 1/k we recover the one-shot algorithm. A performance drop is observed from binary to one-shot for the difficult task of MNISTRotate-sequentially learning 36 rotations of MNIST (each rotation differing by 10 degrees).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Tree Representation for the Continual Learning Scenarios</head><p>In Figure <ref type="figure">9</ref> the Continual Learning scenarios are represented as a tree. This resembles the formulation from <ref type="bibr" target="#b54">[55]</ref> with some modifications, i.e. "Tasks share output head?" is replaced with "Tasks share labels" as it is possible to share the output head but not labels, e.g. SupSup in GNu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Corresponding Tables</head><p>In this section we provide tabular results for figures from Section 4. Again invoking the law of total expectation we rewrite Equation <ref type="formula">45</ref>as</p><p>Moreover, since the data is from task j = i, we can use Assumption 1 and 2 to show that each of the cases above is equally likely. Formally,</p><p>and so we may factor out the probability terms in Equation <ref type="formula">46</ref>. Accordingly, it suffices to show that</p><p>Before we proceed, we will introduce a function h which we use to denote</p><p>for κ &gt; 0. We will make use of two interesting properties of h.</p><p>We first note that h ({y v }, κ) + h ({y v }, -κ) ≥ 0, which is formally shown in J.2.</p><p>Second, we note that</p><p>which we dissect in Lemma J.3.</p><p>Utilizing these two properties of h we may show that Equation <ref type="formula">53</ref>holds as</p><p>Lemma J.2. h ({y v }, κ) + h ({y v }, -κ) ≥ 0.</p><p>Proof. Recall that κ ≥ 0. Moreover,</p><p>and we may then subtract the term on the right from both sides.</p><p>Lemma J.3. Consider take i = j where j is the correct task. Then</p><p>Proof. Note that this equation is satisfied when κ = 0 (since -0 = 0). For the remainder of this proof we will instead consider the case where κ &gt; 0 (and so M i uv = 1). If we define ρ as ρ = (P (φ(x) u = κ) + P (φ(x) u = -κ))</p><p>-1 then may decompose Equation 63 into four terms. Namely, </p><p>Equality follows from the fact that term 1 and 3 are equal, as are terms 2 and 4. We will consider terms 1 and 3, as the other case is nearly identical.</p><p>Let H be the event where φ(x) u = κ, M i uv = 1 and all other random variables (except for ξ uv ) take values such that, if ξ uv = +1 then p v φ(x) u ξ uv M i uv = h ({y v }, κ). On the other hand, if ξ uv = -1 then p v φ(x) u ξ uv M i uv = h ({y v }, -κ). Then, subtracting term 3 from term 1 (and factoring out the shared term) we find P p v φ(x) u ξ uv M i uv = h ({y v }, κ) |φ(x) u = κ -P p v φ(x) u ξ uv M i uv = h ({y v }, -κ) |φ(x) u = κ (65)</p><p>= P (ξ uv = +1|H) -P (ξ uv = -1|H) = 0 (66) since ξ uv is independent of H, and ξ uv = -1 and +1 with equal probability.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convex neural networks</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Marcotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on fairness, accountability and transparency</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient lifelong learning with a-gem</title>
		<author>
			<persName><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Superposition of many models into one</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Terekhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10867" to="10876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04840</idno>
		<title level="m">Sparse networks from scratch: Faster training without losing performance</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">Marguerite</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03635</idno>
		<title level="m">The lottery ticket hypothesis: Finding sparse, trainable neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00152</idno>
		<title level="m">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Siavash</forename><surname>Golkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04476</idno>
		<title level="m">Continual learning via neural pruning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural networks and physical systems with emergent collective computational abilities</title>
		<author>
			<persName><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="2554" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
		<title level="m">Mnist handwritten digit database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Proving the lottery ticket hypothesis: Pruning is all you need</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00585</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Piggyback: Adapting a single network to multiple tasks by learning to mask weights</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillon</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Packnet: Adding multiple tasks to a single network by iterative pruning</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7765" to="7773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Nicolas Y Masse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><surname>Freedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>44):E10467-E10475</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology of learning and motivation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Model cards for model reporting</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zaldivar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Spitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inioluwa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Raji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science</title>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Decebal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phuong</forename><forename type="middle">H</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><surname>Liotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Revisiting natural gradient for deep networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3584</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Vivek Ramanujan</surname></persName>
		</author>
		<author>
			<persName><surname>Wortsman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13299</idno>
		<title level="m">Aniruddha Kembhavi, Ali Farhadi, and Mohammad Rastegari. What&apos;s hidden in a randomly weighted neural network? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001-2010, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Experience replay for continual learning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="348" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An overview of reservoir computing: theory, applications and implementations</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th european symposium on artificial neural networks</title>
		<meeting>the 15th european symposium on artificial neural networks</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="471" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green Ai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10597</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Increasing the capacity of a hopfield network without sacrificing functionality</title>
		<author>
			<persName><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="451" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Lifelong learning algorithms</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="181" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<title level="m">Three scenarios for continual learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Continual learning with hypernetworks</title>
		<author>
			<persName><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">F</forename><surname>Grewe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Batchensemble: an alternative approach to efficient ensemble and lifelong learning</title>
		<author>
			<persName><forename type="first">Yeming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06715</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Reinforced continual learning</title>
		<author>
			<persName><forename type="first">Ju</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Jaehong</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongtae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01547</idno>
		<title level="m">Lifelong learning with dynamically expandable networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Task agnostic continual learning using online variational bayes</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10123</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Incremental self-improvement for life-time multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="516" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deconstructing lottery tickets: Zeros, signs, and the supermask</title>
		<author>
			<persName><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3592" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
