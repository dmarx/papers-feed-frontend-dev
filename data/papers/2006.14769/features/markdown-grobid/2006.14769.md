# Supermasks in Superposition

## Abstract

## 

We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.

## Introduction

Learning many different tasks sequentially without forgetting remains a notable challenge for neural networks [[47,](#b46)[56,](#b55)[23]](#b22). If the weights of a neural network are trained on a new task, performance on previous tasks often degrades substantially [[33,](#b32)[10,](#b9)[12]](#b11), a problem known as catastrophic forgetting. In this paper, we begin with the observation that catastrophic forgetting cannot occur if the weights of the network remain fixed and random. We leverage this to develop a flexible model capable of learning thousands of tasks: Supermasks in Superposition (SupSup). SupSup, diagrammed in Figure [1](#fig_0), is driven by two core ideas: a) the expressive power of untrained, randomly weighted subnetworks [[57,](#b56)[39]](#b38), and b) inference of task-identity as a gradient-based optimization problem.

a) The expressive power of subnetworks Neural networks may be overlaid with a binary mask that selectively keeps or removes each connection, producing a subnetwork. The number of possible subnetworks is combinatorial in the number of parameters. Researchers have observed that the number of combinations is large enough that even within randomly weighted neural networks, there exist supermasks that create corresponding subnetworks which achieve good performance on complex tasks. Zhou et al. [[57]](#b56) and Ramanujan et al. [[39]](#b38) present two algorithms for finding these supermasks while keeping the weights of the underlying network fixed and random. SupSup scales to many tasks by finding for each task a supermask atop a shared, untrained network. b) Inference of task-identity as an optimization problem When task identity is unknown, SupSup can infer task identity to select the correct supermask. Given data from task j, we aim (right) At inference time, SupSup can infer task identity by superimposing all supermasks, each weighted by an α i , and using gradients to maximize confidence.

to recover and use the supermask originally trained for task j. This supermask should exhibit a confident (i.e. low entropy) output distribution when given data from task j [[19]](#b18), so we frame inference of task-identity as an optimization problem-find the convex combination of learned supermasks which minimizes the entropy of the output distribution.

In the rest of the paper we develop and evaluate SupSup via the following contributions:

1. We propose a new taxonomy of continual learning scenarios. We use it to embed and contextualize related work (Section 2).

2. When task identity (ID) is provided during train and test (later dubbed GG), SupSup is a natural extension of Mallya et al. [[30]](#b29). By using a randomly weighted backbone and controlling mask sparsity, SupSup surpasses recent baselines on SplitImageNet [[51]](#b50) while requiring less storage and time costs (Section 3.2).

3. When task ID is provided during train but not test (later dubbed GN), SupSup outperforms recent methods that require task ID [[26,](#b25)[23,](#b22)[4]](#b3), scaling to 2500 permutations of MNIST without forgetting. For these uniform tasks, ID can be inferred with a single gradient computation (Section 3.3). [4](#b3). When task identities are not provided at all (later dubbed NNs), SupSup can even infer task boundaries and allocate new supermasks as needed (Section 3.4).

5. We introduce an extension to the basic SupSup algorithm that stores supermasks implicitly as attractors in a fixed-size Hopfield network [[20]](#b19) (Section 3.5).

6. Finally, we empirically show that the simple trick of adding superfluous neurons results in more accurate task inference (Section 3.6).

## Continual Learning Scenarios and Related Work

In continual learning, a model aims to solve a number of tasks sequentially [[47,](#b46)[56]](#b55) without catastrophic forgetting [[10,](#b9)[23,](#b22)[33]](#b32). Although numerous approaches have been proposed in the context of continual learning, there lacks a convention of scenarios in which methods are trained and evaluated [[49]](#b48). The key identifiers of scenarios include: 1) whether task identity is provided during training, 2) provided during inference, 3) whether class labels are shared during evaluation, and 4) whether the overall task space is discrete or continuous. This results in an exhaustive set of 16 possibilities, many of which are invalid or uninteresting. For example, if task identity is never provided in training, providing it in inference is no longer helpful. To that end, we highlight four applicable scenarios, each with a further breakdown of discrete vs. continuous, when applicable, as shown in Table [1](#).

We decompose continual learning scenarios via a three-letter taxonomy that explicitly addresses the three most critical scenario variations. The first two letters specify whether task identity is given during training (G if given, N if not) and during inference (G if given, N if not). The third letter specifies a subtle but important distinction: whether labels are shared (s) across tasks or not (u). In the unshared case, the model must predict both the correct task ID and the correct class within that Table [1](#): Overview of different Continual Learning scenarios. We suggest scenario names that provide an intuitive understanding of the variations in training, inference, and evaluation, while allowing a full coverage of the scenarios previously defined in [[49]](#b48) and [[55]](#b54). See text for more complete description.

## Scenario Description

Task space discreet or continuous?

Example methods / task names used

## GG

Task Given during train and Given during inference Either PNN [[42]](#b41), BatchE [[51]](#b50), PSP [[4]](#b3), "Task learning" [[55]](#b54), "Task-IL" [[49]](#b48) GNs Task Given during train, Not inference; shared labels Either EWC [[23]](#b22), SI [[54]](#b53), "Domain learning" [[55]](#b54), "Domain-IL" [[49]](#b48) GNu Task Given during train, Not inference; unshared labels Discrete only "Class learning" [[55]](#b54), "Class-IL" [[49]](#b48) NNs Task Not given during train Nor inference; shared labels Either BGD, "Continuous/discrete task agnostic learning" [[55]](#b54) task. In the shared case, the model need only predict the correct, shared label across tasks, so it need not represent or predict which task the data came from. For example, when learning 5 permutations of MNIST in the GN scenario (task IDs given during train but not test), a shared label GNs scenario will evaluate the model on the correct predicted label across 10 possibilities, while in the unshared GNu case the model must predict across 50 possibilities, a more difficult problem.

A full expansion of possibilities entails both GGs and GGu, but as s and u describe only model evaluation, any model capable of predicting shared labels can predict unshared equally well using the provided task ID at test time. Thus these cases are equivalent, and we designate both GG. Moreover, the NNu scenario is invalid because unseen labels signal the presence of a new task (the "labels trick" in [[55]](#b54)), making the scenario actually GNu, and so we consider only the shared label case NNs.

We leave out the discrete vs. continuous distinction as most research efforts operate within one framework or the other, and the taxonomy applies equivalently to discrete domains with integer "Task IDs" as to continue domains with "Task Embedding" or "Task Context" vectors. The remainder of this paper follows the majority of extant literature in focusing on the case with discrete task boundaries (see e.g. [[55]](#b54) for progress in the continuous scenario). Equipped with this taxonomy, we review three existing approaches for continual learning.

(1) Regularization based methods Methods like Elastic Weight Consolidation (EWC) [[23]](#b22) and Synaptic Intelligence (SI) [[54]](#b53) penalize the movement of parameters that are important for solving previous tasks in order to mitigate catastrophic forgetting. Measures of parameter importance vary; e.g. EWC uses the Fisher Information matrix [[36]](#b35). These methods operate in the GNs scenario (Table [1](#)). Regularization approaches ameliorate but do not exactly eliminate catastrophic forgetting.

(2) Using exemplars, replay, or generative models These methods aim to explicitly or implicitly (with generative models) capture data from previous tasks. For instance, [[40]](#b39) performs classification based on the nearest-mean-of-examplars in a feature space. Additionally, [[27,](#b26)[3]](#b2) prevent the model from increasing loss on examples from previous tasks while [[41]](#b40) and [[45]](#b44) respectively use memory buffers and generative models to replay past data. Exact replay of the entire dataset can trivially eliminate catastrophic forgetting but at great time and memory cost. Generative approaches can reduce catastrophic forgetting, but generators are also susceptible to forgetting. Recently, [[50]](#b49) successfully mitigate this obstacle by parameterizing a generator with a hypernetwork [[15]](#b14).

(3) Task-specific model components Instead of modifying the learning objective or replaying data, various methods [[42,](#b41)[53,](#b52)[31,](#b30)[30,](#b29)[32,](#b31)[52,](#b51)[4,](#b3)[11,](#b10)[51]](#b50) use different model components for different tasks.

In Progressive Neural Networks (PNN), Dynamically Expandable Networks (DEN), and Reinforced Continual Learning (RCL) [[42,](#b41)[53,](#b52)[52]](#b51), the model is expanded for each new task. More efficiently, [[32]](#b31) fixes the network size and randomly assigns which nodes are active for a given task. In [[31,](#b30)[11]](#b10), the weights of disjoint subnetworks are trained for each new task. Instead of learning the weights of the subnetwork, for each new task Mallya et al. [[30]](#b29) learn a binary mask that is applied to a network pretrained on ImageNet. Recently, Cheung et al. [[4]](#b3) superimpose many models into one by using different (and nearly orthogonal) contexts for each task. The task parameters can then be effectively retrieved using the correct task context. Finally, BatchE [[51]](#b50) learns a shared weight matrix on the first task and learn only a rank-one elementwise scaling matrix for each subsequent task.

Our method falls into this final approach (3) as it introduces task-specific supermasks. However, while all other methods in this category are limited to the GG scenario, SupSup can be used to achieve compelling performance in all four scenarios. We compare primarily with BatchE [[51]](#b50) and Parameter Superposition (abbreviated PSP) [[4]](#b3) as they are recent and performative. BatchE requires very few additional parameters for each new task while achieving comparable performance to PNN and scaling to SplitImagenet. Moreover, PSP outperforms regularization based approaches like SI [[54]](#b53). However,

Algorithm Avg Top 1 Bytes Accuracy (%) Upper Bound 92.55 10222.81M 89.58 195.18M SupSup (GG) 88.68 100.98M 86.37 65.50M BatchE (GG) 81.50 124.99M Single Model -102.23M 10 6 10 7 10 8 Total Number of Bytes 0.5 0.6 0.7 0.8 0.9 Accuracy Upper Bound SupSup (GG) SupSup (GG) Transfer BatchE (GG) BatchE (GG) -Rand W Separate Heads Separate Heads -Rand W Figure 2: (left) SplitImagenet performance in Scenario GG. SupSup approaches upper bound performance with significantly fewer bytes. (right) SplitCIFAR100 performance in Scenario GG shown as mean and standard deviation over 5 seed and splits. SupSup outperforms similar size baselines and benefits from transfer.

both BatchE [[51]](#b50) and PSP [[4]](#b3) require task identity to use task-specific weights, so they can only operate in the GG setting.

## Methods

In this section, we detail how SupSup leverages supermasks to learn thousands of sequential tasks without forgetting. We begin with easier settings where task identity is given and gradually move to more challenging scenarios where task identity is unavailable.

## Preliminaries

In a standard -way classification task, inputs x are mapped to a distribution p over output neurons {1, ..., }. We consider the general case where p = f (x, W ) for a neural network f parameterized by W and trained with a cross-entropy loss. In continual learning classification settings we have k different -way classification tasks and the input size remains constant across tasks[foot_0](#foot_0) .

Zhou et al. [[57]](#b56) demonstrate that a trained binary mask (supermask) M can be applied to a randomly weighted neural network, resulting in a subnetwork with good performance. As further explored by Ramanujan et al. [[39]](#b38), supermasks can be trained at similar compute cost to training weights while achieving performance competitive with weight training.

With supermasks, outputs are given by p = f (x, W M ) where denotes an elementwise product.

W is kept frozen at its initialization: bias terms are 0 and other parameters in W are ±c with equal probability and c is the standard deviation of the corresponding Kaiming normal distribution [[17]](#b16). This initialization is referred to as signed Kaiming constant by [[39]](#b38) and the constant c may be different for each layer. For completeness we detail the Edge-Popup algorithm for training supermasks [[39]](#b38) in Section E of the appendix.

## Scenario GG: Task Identity Information Given During Train and Inference

When task identity is known during training we can learn a binary mask M i per task. M i are the only parameters learned as the weights remain fixed. Given data from task i, outputs are computed as

$p = f x, W M i(1)$For each new task we can either initialize a new supermask randomly, or use a running mean of all supermasks learned so far. During inference for task i we then use M i . Figure [2](#) illustrates that in this scenario SupSup outperforms a number of baselines in accuracy on both SplitCIFAR100 and SplitImageNet while requiring fewer bytes to store. Experiment details are in Section 4.1.

## Scenarios GNs & GNu : Task Identity Information Given During Train Only

We now consider the case where input data comes from task j, but this task information is unknown to the model at inference time. During training we proceed exactly as in Scenario GG, obtaining k learned supermasks. During inference, we aim to infer task identity-correctly detect that the data belongs to task j-and select the corresponding supermask M j .

The SupSup procedure for task ID inference is as follows: first we associate each of the k learned supermasks M i with an coefficient α i ∈ [0, 1], initially set to 1/k. Each α i can be interpreted as the "belief" that supermask M i is the correct mask (equivalently the belief that the current unknown task is task i). The model's output is then be computed with a weighted superposition of all learned masks:

$p(α) = f x, W k i=1 α i M i .(2)$The correct mask M j should produce a confident, low-entropy output [[19]](#b18). Therefore, to recover the correct mask we find the coefficients α which minimize the output entropy H of p(α). One option is to perform gradient descent on α via

$α ← α -η∇ α H (p (α))(3)$where η is the step size, and αs are re-normalized to sum to one after each update. Another option is to try each mask individually and pick the one with the lowest entropy output requiring k forward passes. However, we want an optimization method with fixed sub-linear run time (w.r.t. the number of tasks k) which leads α to a corner of the probability simplex -i.e. α is 0 everywhere except for a single 1. We can then take the nonzero index to be the inferred task. To this end we consider the One-Shot and Binary algorithms.

One-Shot: The task is inferred using a single gradient. Specifically, the inferred task is given by

$arg max i - ∂H (p (α)) ∂α i(4)$as entropy is decreasing maximally in this coordinate. This algorithms corresponds to one step of the Frank-Wolfe algorithm [[7]](#b6), or one-step of gradient descent followed by softmax re-normalization with the step size η approaching ∞. Unless noted otherwise, x is a single image and not a batch.

Binary: Resembling binary search, we infer task identity using an algorithm with log k steps.

At each step we rule out half the tasks-the tasks corresponding to entries in the bottom half of -∇ α H (p (α)). These are the coordinates in which entropy is minimally decreasing. A task i is ruled out by setting α i to zero and at each step we re-normalize the remaining entries in α so that they sum to one. Pseudo-code for both algorithms may be found in Section A of the appendix.

Once the task is inferred the corresponding mask can be used as in Equation [1](#formula_0)to obtain class probabilities p. In both Scenario GNs and GNu the class probabilities p are returned. In GNu, p forms a distribution over the classes corresponding to the inferred task. Experiments solving thousands of tasks are detailed in Section 4.2.

## Scenario NNs: No Task Identity During Training or Inference

Task inference algorithms from Scenario GN enable the extension of SupSup to Scenario NNs, where task identity is entirely unknown (even during training). If SupSup is uncertain about the current task identity, it is likely that the data do not belong to any task seen so far. When this occurs a new supermask is allocated, and k (the number of tasks learned so far) is incremented.

We consider the One-Shot algorithm and say that SupSup is uncertain when performing task identity

$inference if ν = softmax (-∇ α H (p (α))) is approximately uniform. Specifically, if k max i ν i < 1 + a new mask$is allocated and k is incremented. Otherwise mask arg max i ν i is used, which corresponds to Equation [4](#formula_3). We conduct experiments on learning up to 2500 tasks entirely without any task information, detailed in Section 4.3. Figure [4](#fig_3) shows that SupSup in Scenario NNs achieves comparable performance even to Scenario GNu.

## Beyond Linear Memory Dependence

Hopfield networks [[20]](#b19) implicitly encode a series of binary strings z i ∈ {-1, 1} d with an associated energy function

$E Ψ (z) = uv Ψ uv z u z v . Each z i is a minima of E Ψ ,$and can be recovered with gradient descent. Ψ ∈ R d×d is initially 0, and to encode a new string z i , Ψ ← Ψ + 1 d z i z i . 10 50 100 150 200 250 Num Tasks Learned 0.88 0.90 0.92 0.94 0.96 0.98 Accuracy 10 50 100 150 200 250 Num Tasks Learned SupSup (GNu, H) SupSup (GNu, G) PSP (GG) BatchE (GG) Upper Bound We now consider implicitly encoding the masks in a fixed-size Hopfield network Ψ for Scenario GNu. For a new task i a new mask is learned. After training on task i, this mask will be stored as an attractor in a fixed size Hopfield network. Given new data during inference we perform gradient descent on the Hopfield energy E Ψ with the output entropy H to learn a new mask m. Minimizing E Ψ will hopefully push m towards a mask learned during training while H will push m to be the correct mask. As Ψ is quadratic in mask size, we will not mask the parameters W . Instead we mask the output of every layer except the last, e.g. a network with one hidden layer and mask m is given by

$f (x, m, W ) = softmax W 2 m σ W 1 x(5)$for nonlinearity σ. The Hopfield network will then be a similar size as the base neural network. We refer to this method as HopSupSup and provide additional details in Section B.

## Superfluous Neurons & an Entropy Alternative

Similar to previous methods [[49]](#b48), HopSupSup requires k output neurons in Scenario GNu. SupSup, however, is performing k-way classification without k output neurons. Given data during inference 1) the task is inferred and 2) the corresponding mask is used to obtain outputs p. The class probabilities p correspond to the classes for the inferred task, effectively reusing the neurons in the final layer.

SupSup could use an output size of , though we find in practice that it helps significantly to add extra neurons to the final layer. Specifically we consider outputs p ∈ R s and refer to the neurons { + 1, ..., s} as superfluous neurons (s-neurons). The standard cross-entropy loss will push the values of s-neurons down throughout training. Accordingly, we consider an objective G which encourages the s-neurons to have large negative values and can be used as an alternative to entropy in Equation [4](#formula_3).

Given data from task j, mask M j will minimize the values of the s-neurons as it was trained to do. Other masks were also trained to minimize the values of the s-neurons, but not for data from task j.

In Lemma 1 of Section I we provide the exact form of G in code (G = logsumexp (p) with masked gradients for p 1 , ..., p ) and offer an alternative perspective on why G is effective -the gradient of G for all s-neurons exactly mirrors the gradient from the supervised training loss. [51], SplitCIFAR100 randomly partitions CIFAR100 [[24]](#b23) into 20 different 5-way classification problems. Similarly, SplitImageNet randomly splits the ImageNet [[5]](#b4) dataset into 100 different 10-way classification tasks. Following [[51]](#b50) we use a ResNet-18 with fewer channels for SplitCIFAR100 and a standard ResNet-50 [[18]](#b17) for SplitImageNet. The Edge-Popup algorithm from [[39]](#b38) is used to obtain supermasks for various sparsities with a layer-wise budget from [[35]](#b34). We either initialize each new mask randomly (as in [[39]](#b38)) or use a running mean of all previous learned masks. This simple method of "Transfer" works very well, as illustrated by Figure [2](#). Additional training details and hyperparameters are provided in Section D.

0 1000 2000 Num Tasks Learned 0.91 0.92 0.93 0.94 0.95 Accuracy Upper Bound SupSup (GNu, H) SupSup (GNu, G) Lower Bound SupSup (NNs, H) 0 500 1000 1500 2000 2500 Num Tasks Learned 0.4 0.6 0.8 Accuracy Upper Bound SupSup (GNu, H) SupSup (GNu, G) Lower Bound SupSup (NNs, H) 0 500 1000 1500 2000 2500 Num Tasks Learned 0.4 0.6 0.8 Accuracy SupSup (NNs, H) SupSup (GNu, H) SupSup (GNu, G) Upper Bound Lower Bound Computation In Scenario GG, the primary advantage of SupSup from Mallya et al. [[31]](#b30) or Wen et al. [[51]](#b50) is that SupSup does not require the base model W to be stored. Since W is random it suffices to store only the random seed. For a fair comparison we also train BatchE [[51]](#b50) with random weights.

The sparse supermasks are stored in the standard scipy.sparse.csc[foot_1](#foot_1) format with 16 bit integers. Moreover, SupSup requires minimal overhead in terms of forwards pass compute. Elementwise product by a binary mask can be implemented via memory access, i.e. selecting indices. Modern GPUs have very high memory bandwidth so the time cost of this operation is small with respect to the time of a forward pass. In particular, on a 1080 Ti this operation requires ∼ 1% of the forward pass time for a ResNet-50, less than the overhead of BatchE (computation in Section D).

Baselines In Figure [2](#), for "Separate Heads" we train different heads for each task using a trunk (all layers except the final layer) trained on the first task. In contrast "Separate Heads -Rand W" uses a random trunk. BatchE results are given with the trunk trained on the first task (as in [[51]](#b50)) and random weights W . For "Upper Bound", individual models are trained for each task. Furthermore, the trunk for task i is trained on tasks 1, ..., i. For "Lower Bound" a shared trunk of the network is trained continuously and a separate head is trained for each task. Since catastrophic forgetting occurs we omit "Lower Bound" from Figure [2](#) (the SplitCIFAR100 accuracy is 24.5%).

## Scenarios GNs & GNu: Task Identity Information Given During Train Only

Our solutions for GNs and GNu are very similar. Because GNu is strictly more difficult, we focus on only evaluating in Scenario GNu. For relevant figures we provide a corresponding table in Section H.

Datasets Experiments are conducted on PermutedMNIST, RotatedMNIST, and SplitMNIST. For PermutedMNIST [[23]](#b22), new tasks are created with a fixed random permutation of the pixels of MNIST. For RotatedMNIST, images are rotated by 10 degrees to form a new task with 36 tasks in total (similar to [[4]](#b3)). Finally SplitMNIST partitions MNIST into 5 different 2-way classification tasks, each containing consecutive classes from the original dataset.

Training We consider two architectures: 1) a fully connected network with two hidden layers of size 1024 (denoted FC 1024-1024 and used in [[4]](#b3)) 2) the LeNet 300-100 architecture [[25]](#b24) as used in [[8,](#b7)[6]](#b5). For each task we train for 1000 batches of size 128 using the RMSProp optimizer [[48]](#b47) with learning rate 0.0001 which follows the hyperparameters of [[4]](#b3). Supermasks are found using the algorithm of Mallya et al. [[31]](#b30) with threshold value 0. However, we initialize the real valued "scores" with Kaiming uniform as in [[39]](#b38). Training the mask is not a focus of this work, we choose this method as it is fast and we are not concerned about controlling mask sparsity as in Section 4.1.

Evaluation At test time we perform inference of task identity once for each batch. If task is not inferred correctly then accuracy is 0 for the batch. Unless noted otherwise we showcase results for the most challenging scenario -when the task identity is inferred using a single image. We use "Full Batch" to indicate that all 128 images are used to infer task identity. Moreover, we experiment with both the the entropy H and G (Section 3.6) objectives to perform task identity inference.

Results Figure [4](#fig_3) illustrates that SupSup is able to sequentially learn 2500 permutations of MNIST-SupSup succeeds in performing 25,000-way classification. This experiment is conducted with the One-Shot algorithm (requiring one gradient computation) using single images to infer task identity. The same trends hold in Figure [3](#fig_1), where SupSup outperforms methods which operate in Scenario GG The One-Shot algorithm can be used to infer task identity for BatchE [[51]](#b50). Experiment conducted with FC 1024-1024 on PermutedMNIST using an output size of 500, shown as mean and stddev over 3 runs. by using the One-Shot algorithm to infer task identity. In Figure [3](#fig_1), output sizes of 100 and 500 are respectively used for LeNet 300-100 and FC 1024-1024. The left hand side of Figure [5](#fig_4) illustrates that SupSup is able to infer task identity even when tasks are similar-SupSup is able to distinguish between rotations of 10 degrees. Since this is a more challenging problem, we use a full batch and the Binary algorithm to perform task identity inference. Figure [7](#fig_7) (appendix) shows that for HopSupSup on SplitMNIST, the new mask m converges to the correct supermask in < 30 gradient steps.

Baselines & Ablations Figure [5](#fig_4) (left) shows that even in Scenario GNu, SupSup is able to outperform PSP [[4]](#b3) and BatchE [[51]](#b50) in Scenario GG-methods using task identity. We compare SupSup in GNu with methods in this strictly easier scenario as they are more competitive. For instance, [[49]](#b48) considers sequential learning problems with only 5-10 tasks. SupSup, after sequentially learning 250 permutations of MNIST, outperforms all non-replay methods from [[3]](#b2) in the GNu scenario after they have learned only 10 permutations of MNIST with a similar network. In GNu, Online EWC achieves 33.88% & SI achieves 29.31% on 10 permutations of MNIST [[49]](#b48) while SupSup achieves 94.91% accuracy after 250 permutations (see Table [5](#) in [[49]](#b48) vs. Table [7](#)).

In Figure [5](#fig_4) (right) we equip BatchE with task inference using our One-Shot algorithm. Instead of attaching a weight α i to each supermask, we attach a weight α i to each rank-one matrix [[51]](#b50). Moreover, in Section C of the appendix we augment BatchE to perform task-inference using large batch sizes. "Upper Bound" and "Lower Bound" are the same as in Section 4.1. Moreover, Figure [6](#fig_5) illustrates the importance of output size. Further investigation of this phenomena is provided by Section 3.6 and Lemma 1 of Section I.

## Scenario NNs: No Task Identity During Training or Inference

For the NNs Scenario we consider PermutedMNIST and train on each task for 1000 batches (the model does not have access to this iteration number). Every 100 batches the model must choose to allocate a new mask or pick an existing mask using the criteria from Section 3.4 ( = 2 -3 ). Figure [4](#fig_3) illustrates that without access to any task identity (even during training) SupSup is able to learn thousands of tasks. However, a final dip is observed as a budget of 2500 supermasks total is enforced.

## Conclusion

Supermasks in Superposition (SupSup) is a flexible and compelling model applicable to a wide range of scenarios in Continual Learning. SupSup leverages the power of subnetworks [[57,](#b56)[39,](#b38)[31]](#b30), and gradient-based optimization to infer task identity when unknown. SupSup achieves state-ofthe-art performance on SplitImageNet when given task identity, and performs well on thousands of permutations and almost indiscernible rotations of MNIST without any task information.

We observe limitations in applying SupSup with task identity inference to non-uniform and more challenging problems. Task inference fails when models are not well calibrated-are overly confident for the wrong task. As future work, we hope to explore automatic task inference with more calibrated models [[14]](#b13), as well as circumventing calibration challenges by using optimization objectives such as self-supervision [[16]](#b15) and energy based models [[13]](#b12). In doing so, we hope to tackle large-scale problems in Scenarios GN and NNs.

## Broader Impact

A goal of continual learning is to solve many tasks with a single model. However, it is not exactly clear what qualifies as a single model. Therefore, a concrete objective has become to learn many tasks as efficiently as possible. We believe that SupSup is a useful step in this direction. However, there are consequences to more efficient models, both positive and negative.

We begin with the positive consequences:

• Efficient models require less compute, and are therefore less harmful for the environment then learning one model per task [[44]](#b43). This is especially true if models are able to leverage information from past tasks, and training on new tasks is then faster.

• Efficient models may be run on the end device. This helps to preserve privacy as a user's data does not have to be sent to the cloud for computation.

• If models are more efficient then large scale research is not limited to wealthier institutions. These institutions are more likely in privileged parts of the world and may be ignorant of problems facing developing nations. Moreover, privileged institutions may not be a representative sample of the research community.

We would also like to highlight and discuss the negative consequences of models which can efficiently learn many tasks, and efficient models in general. When models are more efficient, they are also more available and less subject to regularization and study as a result. For instance, when a high-impact model is released by an institution it will hopefully be accompanied by a Model Card [[34]](#b33) analyzing the bias and intended use of the model. By contrast, if anyone is able to train a powerful model this may no longer be the case, resulting in a proliferation of models with harmful biases or intended use. Taking the United States for instance, bias can be harmful as models show disproportionately more errors for already marginalized groups [[2]](#b1), furthering existing and deeply rooted structural racism.

$Algorithm 1 One-Shot(f, x, W, k, {M i } k i=1 , H) 1: α ← 1 k 1 k ... 1 k Initialize α 2: p ← f x, W k i=1 α i M i Superimposed output 3: return arg max i -∂H(p) ∂αi$Return coordinate for which objective maximally decreasing

$Algorithm 2 Binary(f, x, W, k, {M i } k i=1 , H) 1: α ← 1 k 1 k ... 1 k Initialize α 2: while α 0 > 1 do$Iterate until α has a single nonzero entry 3:

$p ← f x, W k i=1 α i M i Superimposed output 4: g ← -∇ α H (p) Gradient of objective 5:$for i ∈ {1, ..., k} do In code this for loop is vectorized 6:

if g i ≤ median (g) then 7:

α i ← 0 Zero out α i for which objective minimally decreasing 8:

α ← α/ α 1 Re-normalize α to sum to 1 9: return arg max i α i

## A Algorithm pseudo-code

Algorithms 1 and 2 respectively provide pseudo-code for the One-Shot and Binary algorithms detailed in Section 3.3. Both aim to infer the task j ∈ {1, ..., k} associated with input data x by minimizing the objective H.

## B Extended Details for HopSupSup

This section provides further details and experiments for HopSupSup (introduced in Section 3.5). HopSupSup provides a method for storing the growing set of supermasks in a fixed size reservoir instead of explicitly storing each mask.

## B.1 Training

Recall that HopSupSup operates in Scenario GNu and so task identity is known during training. Instead of explicitly storing each mask, we will instead store two fixed sized variables Ψ and µ which are both initially 0. The weights of the Hopfield network are Ψ and µ stores a running mean of all masks learned so far. For a new task k we use the same algorithm as in Section 4.2 to learn a binary mask m i which performs well for task k. Since Hopfield networks consider binary strings in {-1, 1} d and we use masks m i ∈ {0, 1} d we will consider z k = 2m k -1. In practice we then update Ψ and µ as

$Ψ ← Ψ + 1 d z k z k -z k Ψz k -Ψz k z k -Id , µ ← k -1 k µ + 1 k z k (6$$)$where Id is the identity matrix. This update rule for Ψ is referred to as the Storkey learning rule [[46]](#b45) and is more expressive than the alternative-the Hebbian rule [20]](#b19) provided for brevity in Section 3.3. With either update rules the learned z i will be a minimizer of the Hopfield energy E Ψ (z) = uv Ψ uv z u z v .

$Ψ ← Ψ + 1 d z k z k [$
## B.2 Inference

During inference we receive data x from some task j, but this task information is not given to the model. HopSupSup first initializes a new binary string z with µ. Next, HopSupSup uses gradient descent to minimize the Hopfield energy in conjunction with the output entropy using mask m = 1 2 z + 1, a process we refer to as Hopfield Recovery. Minimizing the energy will hopefully push m (equivalently z) towards a mask learned during training and minimizing the entropy will hopefully Step 10 20 30

Step

$m -m 0 1 m -m 1 1 m -m 2 1 m -m 3 1 m -m 4 1 10 20 30$Step 10 20 30

Step push m towards the correct mask m j . We may then use the recovered mask to compute the network output.

In practice we use one pass through the evaluation set (with batch size 64, requiring T ≈ 30 steps) to recover a mask and another to perform evaluation with the recovered mask. When recovering the mask we gradually increase the strength of the Hopfield term and decrease the strength of the entropy term. Otherwise the Hopfield term initially pulls z in the wrong direction or the final z does not lie at a minimum of E Ψ . For step t ∈ {1, ..., T }, and constant γ we use the objective J as

$J (z, t) = γt T E Ψ (z) + 1 - t T H (p)(7)$where p denotes the output using mask m = 1 2 z + 1. Figure [7](#fig_7) illustrates that after approximately 30 steps of gradient descent on z using objective J , the mask m = 1 2 z + 1 converges to the correct mask learned during training. This experiment is conducted for 20 different random seeds on SplitMNIST (see Section 4.2) training for 1 epoch per task. Evaluation with the recovered mask for each seed is then given by Figure [8](#). As expected, when the correct mask is successfully recovered, accuracy matches directly using the correct mask. For hyperparameters we set γ = 1.5 • 10 -3 and perform gradient descent during Hopfield recovery with learning rate 0.5 • 10 3 , momentum 0.9, and weight decay 10 -4 .

## B.3 Network Architecture

Let BN denote non-affine batch normalization [[21]](#b20), i.e. batch normalization with no learned parameters. Also recall that we are masking layer outputs instead of weights, and the weights still remain fixed (see Section 3.5). Therefore, with mask m = (m 1 , m 2 ) and weights

$W = (W 1 , W 2 , W 3 ) we compute outputs as f (x, m, W ) = softmax W 3 σ m 2 BN W 2 σ m 1 BN W 1 x(8)$where σ denotes the Swish nonlinearity [[38]](#b37). Without masking or normalization f is a fully connected network with two hidden layers of size 2048. We also note that HopSupSup requires 10 output neurons for SplitMNIST in Scenario GNu, and the composition of non-affine batch normalization with a binary mask was inspired by BatchNets [[9]](#b8).

## C Augmenting BatchE For Scnario GNu

In Section 4.2 we demonstrate that BatchE [[51]](#b50) is able to infer task identity using the One-Shot algorithm. In this section we show that, equipped with H from Section 3, BatchE can also infer task identity by using a large batch size. We refer to this method as Augmented BatchE (ABatchE).   [1](#) represented in a tree graph, as in [[55]](#b54).

For clarity we describe ABatchE for one linear layer, i.e. we describe the application of ABatchE to

$f (x, W ) = softmax W x(9)$for input data x ∈ R m and weights W ∈ R m×n . In BatchE [[51]](#b50), W is trained on the first task then frozen. For task i BatchE learns "fast weights" r i ∈ R m , s i ∈ R n and outputs are computed via

$f (x, W ) = softmax W r i s i x .(10)$Wen et al. [[51]](#b50) further demonstrate that Equation 10 can be vectorized as

$f (x, W ) = softmax W (x r i ) s i(11)$or, for a batch of data

$X ∈ R b×m , f (X, W ) = softmax X R b i W S b i .(12)$In Equation [12](#formula_19), R b i ∈ R b×m is a matrix where each of the b rows is r i (likewise S b i ∈ R b×n is a matrix where each of the b rows is s i ).

As in Section 3.3 we now consider the case where data X ∈ R b×m comes from task j but this information is not known to the model. For ABatchE we repeat the data k times, where k is the number of tasks learned so far, and use different "fast weights" for each repetiton. Specifically, we consider repeated data X ∈ R bk×m and augmented matricies R ∈ R bk×m and S ∈ R bk×n given by

$X =     X X . . . X     , R =      R b 1 R b 2 . . . R b k      , S =      S b 1 S b 2 . . . S b k      . (13$$)$Outputs are then computed as where the b rows (bi, ..., bi + b -1) of the output correspond exactly to Equation [12](#formula_19). The task may then be inferred by choosing the i for which the rows (bi, ..., b(i + 1) -1) minimize the objective H. If f (X, W ) i denotes row i of f (X, W ) then for objective H the inferred task for ABatchE is

$f (X, W ) = softmax X R W S(14)$$arg min i b-1 ω=0 H (f (X, W ) bi+ω ) .(15)$To extend ABatchE to deep neural networks the matricies R and S are constructed for each layer.

One advantage of ABatchE over SupSup is that no backwards pass is required. However, ABatchE uses a very large batch size for large k, and the forward pass therefore requires more compute and memory. Another disadvantage of ABatchE is that the performance of ABatchE is limited by the performance of BatchE. In Section 4.2 we demonstrate that SupSup outperforms BatchE when BatchE is given task identity information.

Since the objective for ABatchE need not be differentiable we also experiment with an alternative metric of confidence M(p) =max i p i . We showcase results for ABatchE on PermutedMNIST in Figure [10](#fig_10)  As in [[51]](#b50) we train each model for 250 epochs per task. We use standard hyperparameters-the Adam optimizer [[22]](#b21) with a batch size of 128 and learning rate 0.001 (no warmup, cosine decay [[28]](#b27)). For SupSup we follow [[39]](#b38) and use non-affine normalization so there are no learned parameters. We do have to store the running mean and variance for each task, which we include in the parameter count. We found it better to use a higher learning rate (0.1) when training BatchE (Rand W ), and the standard BatchE number is taken from [[51]](#b50).

## D.2 SplitImageNet (GG)

We use the Upper Bound and BatchE number from [[51]](#b50). For SupSup we train for 100 epochs with a batch size of 256 using the Adam optimizer [[22]](#b21) with learning rate 0.001 (5 epochs warmup, cosine decay [[28]](#b27)). For SupSup we follow [[39]](#b38) and use non-affine normalization so there are no learned parameters. We do have to store the running mean and variance for each task, which we include in the parameter count.

## D.3 GNu Experiments

We clarify some experimental details for GNu experiments & baselines. For the BatchE [[51]](#b50) baseline we find it best to use kaiming normal initialization with a learning rate of 0.01 (0.0001 for the first task when the weights are trained). As we are considering hundreds of tasks, instead of training 

## I Analysis

In this section we assume a slightly more technical perspective. The aim is not to formally prove properties of the algorithm. Rather, we hope that a more mathematical language may prove useful in extending intuition. Just as the empirical work of [[8,](#b7)[57,](#b56)[39]](#b38) was given a formal treatment in [[29]](#b28), we hope for more theoretical work to follow.

Our grounding intuition remains from Section 3.3-the correct mask will produce the lowest entropy output. Moreover, since entropy is differentiable, gradient based optimization can be used to recover the correct mask. However, many questions remain: Why do superfluous neurons (Section 3.6) help?

In the case of MNISTPermuation, why is a single gradient sufficient? Although it is a simple case, steps forward can be made by analyzing the training of a linear head on fixed features. With random features, training a linear head on fixed features is considered in the literature of reservoir computing [[43]](#b42), and more [[1]](#b0).

Consider k different classification problems with fixed features φ(x) ∈ R m . Traditionally, one would use learned weights W ∈ R m×n to compute logits

$y = W φ(x)(16)$and output classification probabilities p = softmax(y) where

$p v = exp(y v ) n v =1 exp(y v ) .(17)$Recall that with SupSup we compute the logits for task i using fixed random weights W and a learned binary mask M i ∈ {0, 1} m×n as

$y = W M i φ(x)(18)$where denotes an element-wise product and no bias term is allowed. Moreover, W uv = ξ uv 2/m where ξ uv is chosen independently to be either -1 or 1 with equal probability and the constant 2/m follows Kaiming initialization [[17]](#b16).

Say we are given data x from task j. From now on we will refer to task j as the correct task. Recall from Section 3.3 that SupSup attempts to infer the correct task by using a weighted mixture of masks

$y = W i α i M i φ(x)(19)$where the coefficients α i sum to one, and are initially set to 1/k.

To infer the correct task we attempt to construct a function G(y; α) with the following property: For fixed data, G is minimized when α = e j (e j denotes a k-length vector that is 1 in index j and 0 otherwise). We can then infer the correct task by solving a minimization problem.

As in One-Shot, we use a single gradient computation to infer the task via

$arg max i - ∂G ∂α i .(20)$A series of Lemmas will reveal how a single gradient step may be sufficient when tasks are unrelated (e.g. as in PermutedMNIST). We begin with the construction of a useful function G, which will correspond exactly to G in Section 3.6. As in Section 3.6, this construction is made possible through superfluous neurons (s-neurons): The true labels are in {1, ..., }, and a typical output is therefore length . However, we add ns-neurons resulting in a vector y of length n.

Let S denote the set of s-neurons and R denote the set of real neurons where |S| = nand |R| = .

Moreover, assume that a standard cross-entropy loss is used during training, which will encourage s-neurons to have small values. Lemma I.1. It is possible to construct a function G such that the gradient matches the gradient from the supervised training loss L for all s-neurons. Specifically, ∂G ∂yv = ∂L ∂yv for all v ∈ S and 0 otherwise.

Proof. Let g v = ∂G ∂yv . It is easy to ensure that g v = 0 for all v ∈ S with a modern neural network library like PyTorch [[37]](#b36) as detaching[foot_2](#foot_2) the outputs from the neurons v ∈ S prevents gradient signal from reaching them. In code, let y be the outputs and m be a binary vector with m v = 1 if v ∈ S and 0 otherwise, then

$y = (1 -m) * y.detach() + m * y(21)$will prevent gradient signal from reaching y v for v ∈ S.

Recall that the standard cross-entropy loss is

$L(y) = -log exp(y c ) n v =1 exp(y v ) = -y c + log n v =1 exp(y v )(22)$where c ∈ {1, ..., } is the correct label. The gradient of L to any s-neuron v is then

$∂L ∂y v = exp(y v ) n v =1 exp(y v ) .(23)$If we define G as 

$G(y; α) = log n v =1 exp(y v )(24$where model(...) computes Equation [19](#formula_27).

In the next two Lemmas we aim to show that, in expectation, -∂G ∂αi ≤ 0 for i = j while -∂G ∂αj > 0. Recall that j is the correct task-the task from which the data is drawn-and we will use i to refer to a different task.

When we take expectation, it is with respect to the random variables ξ, {M ω } ω∈{1,..,k} , and x. Before we proceed further a few assumptions are formalized, e.g. what it means for tasks to be unrelated.

## Assumption 1:

We assume that the mask learned on task i will be independent from the data from task j: If the data is from task j then φ(x) and M i and independent random variables.

Assumption 2: We assume that a negative weight and positive weight are equally likely to be masked out. As a result, E ξ uv M i uv = 0. Note that when E [φ(x)] = 0, which will be the case for zero mean random features, there should be little doubt that this assumption should hold. Lemma I.2. If data x comes from task j and i = j then E -

$∂G ∂α i ≤ 0(26)$Proof. We may write the gradient as

$∂G ∂α i = n v=1 ∂G ∂y v ∂y v ∂α i(27)$and use that ∂G ∂yv = 0 for v ∈ S. Moreover, y v may be written as

$y v = n u=1 φ(x) u W uv k i=1 α i M i uv (28$$)$with W uv = ξ uv 2/m and so Equation [27](#formula_35)becomes

$∂G ∂α i = √ 2 √ m v∈S n u=1 ∂G ∂y v φ(x) u ξ uv M i uv .(29)$Taking the expectation (and using linearity) we obtain

$E ∂G ∂α i = √ 2 √ m v∈S n u=1 E ∂G ∂y v φ(x) u ξ uv M i uv .(30)$In Lemma J.1 we formally show that each term in this sum is greater than or equal to 0, which completes this proof. However, we can see informally now why expectation should be close to 0 if we ignore the gradient term as

$E φ(x) u ξ uv M i uv = E [φ(x) u ] E ξ uv M i uv = 0(31)$where the first equality follows from Assumption 1 and the latter follows from Assumption 2.

We have now seen that in expectation -∂G ∂αi ≤ 0 for i = j. It remains to be shown that we should expect -∂G ∂αj > 0. Lemma I.3. If data x comes from the task j then

$E - ∂G ∂α j > 0.(32)$Proof. Following Equation [30](#formula_39), it suffices to show that for u ∈ {1, ..., m}, v ∈ S E -∂G ∂y v φ(x) u ξ uv M j uv > 0.

Since v ∈ S we may invoke Lemma I.1 to rewrite our objective as

$E - ∂L ∂y v φ(x) u ξ uv M j uv > 0 (34$$)$where L is the supervised loss used for training. Recall that in the mask training algorithm, real valued scores S j uv are associated with M j uv [[39,](#b38)[30]](#b29). The update rule for S j uv on the backward pass is then S j uv ← S j uv + η -∂L ∂y v φ(x) u ξ uv [(35)](#b34) for some learning rate η. Following Mallya et al. [[30]](#b29) (with threshold 0, as used in Section 4.2), we let M j uv = 1 if S j uv > 0 and otherwise assign M j uv = 0. As a result, we expect that M j uv is 1 when -∂L ∂yv φ(x) u ξ uv is more consistently positive than negative. In other words, the expected product of M j uv and -∂L ∂yv φ(x) u ξ uv is positive, satisfying Equation 34.

Together, three Lemmas have demonstrated that in expectation -∂G ∂αi ≤ 0 for i = j while -∂G ∂αj > 0. Accordingly, we should expect that

$arg max i - ∂G ∂α i . (36$$)$returns the correct task j. While a full, formal treatment which includes the analysis of noise is beyond the scope of this work, we hope that this section has helped to further intuition. However, we are missing one final piece-what is the relation between G and H?

It is not difficult to imagine that H should imitate the loss, which attempts to raise the score of one logit while bringing all others down. Analytically we find that H can be decomposed into two terms as follows 

$H (p) = -$where the latter term is G. With more and more neurons in the output layer, p v will become small moving H towards G. 

## J Additional Technical Details

$∂G ∂y v = p v = exp(y v ) n v =1 exp(y v )(41)$and so we rewrite equation 40 as

$E p v φ(x) u ξ uv M i uv ≥ 0.(42)$By the law of total expectation E p v φ(x) u ξ uv M i uv = E E p v φ(x) u ξ uv M i uv φ(x) u ξ uv M i uv [(43)](#b42) and so it suffices to show that

$E p v φ(x) u ξ uv M i uv φ(x) u ξ uv M i uv = κ ≥ 0(44)$for any κ ≥ 0. In the case where where κ = 0 Equation 44 becomes

$E 0p v φ(x) u ξ uv M i uv = 0 = 0(45)$and so we are only left to consider κ > 0. Note that κ > 0 restricts M i uv to be 1.

![Figure 1: (left) During training SupSup learns a separate supermask (subnetwork) for each task.(right) At inference time, SupSup can infer task identity by superimposing all supermasks, each weighted by an α i , and using gradients to maximize confidence.]()

![Figure 3: Using One-Shot to infer task identity, SupSup outperforms methods with access to task identity. Results shown for PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right).]()

![GG: Task Identity Information Given During Train and Inference Datasets, Models & Training In this experiment we validate the performance of SupSup on SplitCIFAR100 and SplitImageNet. Following Wen et al.]()

![Figure 4: Learning 2500 tasks and inferring task identity using the One-Shot algorithm. Results for both the GNu and NNs scenarios with the LeNet 300-100 model using output size 500.]()

![Figure 5: (left) Testing the FC 1024-1024 model on RotatedMNIST. SupSup uses Binary to infer task identity with a full batch as tasks are similar (differing by only 10 degrees). (right)The One-Shot algorithm can be used to infer task identity for BatchE[51]. Experiment conducted with FC 1024-1024 on PermutedMNIST using an output size of 500, shown as mean and stddev over 3 runs.]()

![Figure 6: The effect of output size s on SupSup performance using the One-Shot algorithm. Results shown for PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right).]()

![Figure 7: During Hopfield Recovery the new mask m converges to the correct mask learned during training. Note that m i denotes the mask learned for task i.]()

![Figure 8: Evaluating (with 20 random seeds) on SplitM-NIST after finding a mask with Hopfield Recovery. Average accuracy is 97.43%.]()

![Figure 10: Testing ABatchE on PermutedMNIST with LeNet 300-100 (left) and FC 1024-1024 (right) with output size 100.]()

![for various values of b. The entropy objective H performs better than M, and forgetting is only mitigated when using 16 images (b = 16). With 250 tasks, b = 16 corresponds to a batch size of 4000. D Extended Training Details D.1 SplitCIFAR-100 (GG)]()

![) then g v = ∂L ∂yv as needed. Expressed in code y = model(x); G = torch.logsumexp((1 -m) * y.detach() + m * y, dim=1)]()

![If j is the true task and i = j thenE ∂G ∂y v φ(x) u ξ uv M i uv ≥ 0(40)Proof. Recall from Lemma I.1 that]()

![Accuracy on PermutedMNIST with FC 1024-1024 corresponding to Figure6(right).]()

In practice the tasks do not all need to be -way -output layers can be padded until all have the same size.

https://docs.scipy.org/doc/scipy/reference/sparse.html

https://pytorch.org/docs/stable/autograd.html

