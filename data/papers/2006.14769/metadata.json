{
  "arxivId": "2006.14769",
  "title": "Supermasks in Superposition",
  "authors": "Mitchell Wortsman, Vivek Ramanujan, Rosanne Liu, Aniruddha Kembhavi, Mohammad Rastegari, Jason Yosinski, Ali Farhadi",
  "abstract": "We present the Supermasks in Superposition (SupSup) model, capable of\nsequentially learning thousands of tasks without catastrophic forgetting. Our\napproach uses a randomly initialized, fixed base network and for each task\nfinds a subnetwork (supermask) that achieves good performance. If task identity\nis given at test time, the correct subnetwork can be retrieved with minimal\nmemory usage. If not provided, SupSup can infer the task using gradient-based\noptimization to find a linear superposition of learned supermasks which\nminimizes the output entropy. In practice we find that a single gradient step\nis often sufficient to identify the correct mask, even among 2500 tasks. We\nalso showcase two promising extensions. First, SupSup models can be trained\nentirely without task identity information, as they may detect when they are\nuncertain about new data and allocate an additional supermask for the new\ntraining distribution. Finally the entire, growing set of supermasks can be\nstored in a constant-sized reservoir by implicitly storing them as attractors\nin a fixed-sized Hopfield network.",
  "url": "https://arxiv.org/abs/2006.14769",
  "issue_number": 210,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/210",
  "created_at": "2025-01-05T08:23:41.553406",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-24T02:37:38.778Z",
  "main_tex_file": null,
  "published_date": "2020-06-26T03:16:44Z",
  "arxiv_tags": [
    "cs.LG",
    "cs.AI",
    "stat.ML"
  ]
}