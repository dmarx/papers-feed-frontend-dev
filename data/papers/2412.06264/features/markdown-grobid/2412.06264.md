# Flow Matching Guide and Code

## Abstract

## 

Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM.

## 

Four time-continuous processes (Xt) 0≤t≤1 taking source sample X0 to a target sample X1. These are a flow in a continuous state space, a diffusion in continuous state space, a jump process in continuous state space (densities visualized with contours), and a jump process in discrete state space (states as disks, probabilities visualized with colors).

## Introduction

Flow matching (FM) [(Lipman et al., 2022;](#b44)[Albergo and Vanden-Eijnden, 2022;](#b0)[Liu et al., 2022](#b46)) is a simple framework for generative modeling framework that has pushed the state-of-the-art in various fields and large-scale applications including generation of images [(Esser et al., 2024)](#b21), videos [(Polyak et al., 2024)](#b63), speech [(Le et al., 2024)](#b43), audio [(Vyas et al., 2023)](#b84), proteins [(Huguet et al., 2024)](#b36), and robotics [(Black et al., 2024)](#b6). This manuscript and its accompanying codebase have two primary objectives. First, to serve as a comprehensive and self-contained reference to Flow Matching, detailing its design choices and numerous extensions developed by the research community. Second, to enable newcomers to quickly adopt and build upon Flow Matching for their own applications.

The framework of Flow Matching is based on learning a velocity field (also called vector field). Each velocity field defines a flow ψ t by solving an ordinary differential equation (ODE) in a process called simulation. A flow is a determinstic, time-continuous bijective transformation of the d-dimensional Euclidean space, R d . The goal of Flow Matching is to build a flow that transforms a sample X 0 ∼ p drawn from a source distribution p into a target sample X 1 := ψ 1 (X 0 ) such that X 1 ∼ q has a desired distribution q, see figure [1a](#fig_7). Flow models were introduced to the machine learning community by [(Chen et al., 2018;](#)[Grathwohl et al., 2018)](#b27) as Continuous Normalizing Flows (CNFs). Originally, flows were trained by maximizing the likelihood p(X 1 )

of training examples X 1 , resulting in the need of simulation and its differentiation during training. Due to the resulting computational burdens, later works attempted to learn CNFs without simulation [(Rozen et al., 2021;](#b68)[Ben-Hamu et al., 2022)](#b4), evolving into modern-day Flow Matching algorithms [(Lipman et al., 2022;](#b44)[Liu et al., 2022;](#b46)[Albergo and Vanden-Eijnden, 2022;](#b0)[Neklyudov et al., 2023;](#b54)[Heitz et al., 2023;](#b30)[Tong et al., 2023)](#b80). The resulting framework is a recipe comprising two steps [(Lipman et al., 2022)](#b44), see figure [2](#fig_2): First, choose a probability path p t interpolating between the source p and target q distributions. Second, train a velocity field (neural network) that defines the flow transformation ψ t implementing p t .

The principles of FM can be extended to state spaces S other than R d and even evolution processes that are not flows. Recently, Discrete Flow Matching [(Campbell et al., 2024;](#b11)[Gat et al., 2024)](#b25) develops a Flow Matching algorithm for time-continuous Markov processes on discrete state spaces, also known as Continuous Time Markov Chains (CTMC), see figure [1c](#fig_7). This advancement opens up the exciting possibility of using Flow Matching in discrete generative tasks such as language modeling. Riemannian Flow Matching (Chen and [Lipman, 2024)](#) extends Flow Matching to flows on Riemannian manifolds S = M that now became the state-of-the-art models for a wide variety of applications of machine learning in chemistry such as protein folding [(Yim et al., 2023;](#b85)[Bose et al., 2023)](#b8). Even more generally, Generator Matching [(Holderrieth et al., 2024)](#b33) shows that the Flow Matching framework works for any modality and for general Continuous Time Markov Processes (CTMPs) including, as illustrated in figure [1](#fig_0), flows, diffusions, and jump processes in continuous spaces, in addition to CTMC in discrete spaces. Remarkably, for any such CTMP, the Flow Matching recipe remains the same, namely: First, choose a path p t interpolating source p and target q on the relevant state space S. Second, train a generator, which plays a similar role to velocities for flows, and defines a CTMP process implementing p t . This generalization of Flow Matching allows us to see many existing generative models in a unified light and develop new generative models for any modality with a generative Markov process of choice.

Chronologically, Diffusion Models were the first to develop simulation-free training of a CTMP process, namely a diffusion process, figure [1b](#fig_7). Diffusion Models were originally introduced as discrete time Gaussian processes [(Sohl-Dickstein et al., 2015;](#b77)[Ho et al., 2020)](#b32) and later formulated in terms of continuous time Stochastic Differential Equations (SDEs) [(Song et al., 2021)](#). In the lens of Flow Matching, Diffusion Models build the probability path p t interpolating source and target distributions in a particular way via forward noising processes modeled by particular SDEs. These SDEs are chosen to have closed form marginal probabilities that are in turn used to parametrize the generator of the diffusion process (i.e., drift and diffusion coefficient) via the score function [(Song and Ermon, 2019)](#b78). This parameterization is based on a reversal process to the forward noising process [(Anderson, 1982)](#b3). Consequently, Diffusion Models learn the score function of the marginal probabilities. Diffusion Models' literature suggested also other parametrizations of the generator besides the score, including noise prediction, denoisers [(Kingma et al., 2021)](#b41), or v-prediction [(Salimans and Ho, 2022)](#b71)-where the latter coincides with velocity prediction for a particular choice of probability path p t .

Diffusion bridges [(Peluchetti, 2023)](#b59) offers another approach to design p t and generators for diffusion process that extends diffusion models to arbitrary source-target couplings. In particular these constructions are build again on SDEs with marginals known in closed form, and again use the score to formulate the generator (using Doob's h-transform). [Shi et al. (2023)](#b76); [Liu et al. (2023)](#b45) show that the linear version of Flow Matching can be seen as a certain limiting case of bridge matching.

The rest of this manuscript is organized as follows. Section 2 offers a self-contained "cheat-sheet" to understand and implement vanilla Flow Matching in PyTorch. Section 3 offers a rigorous treatment of flow models, arguably the simplest of all CTMPs, for continuous state spaces. In section 4 we introduce the Flow Matching framework in R d and its various design choices and extensions. We show that flows can be constructed by considering the significantly simpler conditional setting, offering great deal of flexibility in their design, e.g., by readily extending to Riemannian geometries, described in section 5. Section 6 provides an introduction to Continuous Time Markov Chains (CTMCs) and the usage as generative models on discrete state spaces. Then, section 7 discusses the extension of Flow Matching to CTMC processes. In section 8, we provide an introduction to using Continuous Time Markov Process (CTMPs) as generative models for arbitrary state spaces. In section 9, we describe Generator Matching (GM) -a generative modeling framework for arbitrary modalities that describes a scalable way of training CTMPs. GM also unifies all models in previous sections into a common framework. Finally, due to their wide-spread use, we discuss in section 10 denoising diffusion models as a specific instance of the FM family of models.

## Quick tour and key concepts

Given access to a training dataset of samples from some target distribution q over R d , our goal is to build a model capable of generating new samples from q. To address this task, Flow Matching (FM) builds a probability path (p t ) 0≤t≤1 , from a known source distribution p 0 = p to the data target distribution p 1 = q, where each p t is a distribution over R d . Specifically, FM is a simple regression objective to train the velocity field neural network describing the instantaneous velocities of samples-later used to convert the source distribution p 0 into the target distribution p 1 , along the probability path p t . After training, we generate a novel sample from the target distribution X 1 ∼ q by (i) drawing a novel sample from the source distribution X 0 ∼ p, and (ii) solving the Ordinary Differential Equation (ODE) determined by the velocity field.

More formally, an ODE is defined via a time-dependent vector field u : [0, 1] × R d → R d which, in our case, is the velocity field modeled in terms of a neural network. This velocity field determines a time-dependent flow ψ :

$[0, 1] × R d → R d , defined as d dt ψ t (x) = u t (ψ t (x)),$where ψ t := ψ(t, x) and ψ 0 (x) = x. The velocity field u t generates the probability path p t if its flow ψ t satisfies X t := ψ t (X 0 ) ∼ p t for X 0 ∼ p 0 .

(2.1)

According to the equation above, the velocity field u t is the only tool necessary to sample from p t by solving the ODE above. As illustrated in figure [2d](#fig_2), solving the ODE until t = 1 provides us with samples X 1 = ψ 1 (X 0 ), resembling the target distribution q. Therefore, and in sum, the goal of Flow Matching is to learn a vector field u θ t such that its flow ψ t generates a probability path p t with p 0 = p and p 1 = q.  Using the notations above, the goal of Flow Matching is to learn the parameters θ of a velocity field u θ t implemented in terms of a neural network. As anticipated in the introduction, we do this in two steps: design a probability path p t interpolating between p and q (see figure [2b](#fig_2)), and train a velocity field u θ t generating p t by means of regression (see figure [2c](#fig_4)).

Therefore, let us proceed with the first step of the recipe: designing the probability path p t . In this example, let the source distribution p := p 0 = N (x|0, I), and construct the probability path p t as the aggregation of the conditional probability paths p t|1 (x|x 1 ), each conditioned on one of the data examples X 1 = x 1 comprising the training dataset. (One of such conditional paths is illustrated in figure [3a](#fig_6).) The probability path p t therefore follows the expression: p t (x) = p t|1 (x|x 1 )q(x 1 )dx 1 , where p t|1 (x|x 1 ) = N (x|tx 1 , (1 -t) 2 I).

(

This path, also known as the conditional optimal-transport or linear path, enjoys some desirable properties that we will study later in this manuscript. Using this probability path, we may define the random variable X t ∼ p t by drawing X 0 ∼ p, drawing X 1 ∼ q, and taking their linear combination:

$X t = tX 1 + (1 -t)X 0 ∼ p t .(2.3)$We now continue with the second step in the Flow Matching recipe: regressing our velocity field u θ t (usually implemented in terms of a neural network) to a target velocity field u t known to generate the desired probability path p t . To this end, the Flow Matching loss reads:

$L FM (θ) = E t,Xt u θ t (X t ) -u t (X t ) 2$, where t ∼ U[0, 1] and X t ∼ p t . [(2.4)](#) In practice, one can rarely implement the objective above, because u t is a complicated object governing the joint transformation between two high-dimensional distributions. Fortunately, the objective simplifies drastically by conditioning the loss on a single target example X 1 = x 1 picked at random from the training set. To see how, borrowing equation [(2.3)](#) to realize the conditional random variables

$X t|1 = tx 1 + (1 -t)X 0 ∼ p t|1 (•|x 1 ) = N (• | tx 1 , (1 -t) 2 I).$(2.5) Using these variables, solving d dt X t|1 = u t (X t|1 |x 1 ) leads to the conditional velocity field [.6)](#) which generates the conditional probability path p t|1 (•|x 1 ). (For an illustration on these two conditional objects, see figure [3c](#fig_6).) Equipped with the simple equation [(2.6)](#) for the conditional velocity fields generating the designed conditional probability paths, we can formulate a tractable version of the Flow Matching loss in [(2.4)](#). This is the conditional Flow Matching loss:

$u t (x|x 1 ) = x 1 -x 1 -t , (2$L CFM (θ) = E t,Xt,X1 ∥u θ t (X t ) -u t (X t |X 1 )∥ 2 , where t ∼ U [0, 1], X 0 ∼ p, X 1 ∼ q, (2.7)    and X t = (1 -t)X 0 + tX 1 . Remarkably, the objectives in [(2.4](#)) and (2.7) provide the same gradients to learn u θ t , i.e., ∇ θ L FM (θ) = ∇ θ L CFM (θ).

(2.8)

Finally, by plugging u t (x|x 1 ) from (2.6) into equation (2.7), we get the simplest implementation of Flow Matching:

$L OT,Gauss CFM (θ) = E t,X0,X1 ∥u θ t (X t ) -(X 1 -X 0 ) ∥ 2$, where t ∼ U [0, 1], X 0 ∼ N (0, I), X 1 ∼ q.

(2.9)

A standalone implementation of this quick tour in pure PyTorch is provided in code 1. Later in the manuscript, we will cover more sophisticated variants and design choices, all of them implemented in the accompanying flow_matching library. 

## Flow models

This section introduces flows, the mathematical object powering the simplest forms of Flow Matching. Later parts in the manuscript will discuss Markov processes more general than flows, leading to more sophisticated generative learning paradigms introducing many more design choices to the Flow Matching framework.

The reason we start with flows is three-fold: First, flows are arguably the simplest of all CTMPs-being deterministic and having a compact parametrization via velocities-these models can transform any source distribution p into any target distribution q, as long as these two have densities. Second, flows can be sampled rather efficiently by approximating the solution of ODEs, compared, e.g., to the harder-to-simulate SDEs for diffusion processes. Third, the deterministic nature of flows allows an unbiased model likelihood estimation, while more general stochastic processes require working with lower bounds. To understand flows, we must first review some background notions in probability and differential equations theory, which we do next.

## Random vectors

Consider data in the d-dimensional Euclidean space x = (x 1 , . . . , x d ) ∈ R d with the standard Euclidean inner product ⟨x, y⟩ = d i=1 x i y i and norm ∥x∥ = ⟨x, x⟩. We will consider random variables (RVs) X ∈ R d with continuous probability density function (PDF), defined as a continuous function p X : R d → R ≥0 providing event A with probability

$P(X ∈ A) = A p X (x)dx,(3.1)$where p X (x)dx = 1. By convention, we omit the integration interval when integrating over the whole space ( ≡ R d ). To keep notation concise, we will refer to the PDF p Xt of RV X t as simply p t . We will use the notation X ∼ p or X ∼ p(X) to indicate that X is distributed according to p. One common PDF in generative modeling is the d-dimensional isotropic Gaussian:

$N (x|µ, σ 2 I) = (2πσ 2 ) -d 2 exp - ∥x -µ∥ 2 2 2σ 2 ,(3.2)$where µ ∈ R d and σ ∈ R >0 stand for the mean and the standard deviation of the distribution, respectively.

The expectation of a RV is the constant vector closest to X in the least-squares sense:

$E [X] = arg min z∈R d ∥x -z∥ 2 p X (x)dx = xp X (x)dx. (3.3)$One useful tool to compute the expectation of functions of RVs is the Law of the Unconscious Statistician:

$E [f (X)] = f (x)p X (x)dx. (3.4)$When necessary, we will indicate the random variables under expectation as E X f (X). and similarly for the conditional PDF p Y |X . Bayes' rule expresses the conditional PDF p Y |X with p X|Y by p Y |X (y|x) = p X|Y (x|y)p Y (y) p X (x) , (3.7) for p X (x) > 0. The conditional expectation E [X|Y ] is the best approximating function g ⋆ (Y ) to X in the least-squares sense: g ⋆ := arg min g:R d →R d E ∥X -g(Y )∥ 2 = arg min g:R d →R d ∥x -g(y)∥ 2 p X,Y (x, y)dxdy = arg min g:R d →R d ∥x -g(y)∥ 2 p X|Y (x|y)dx p Y (y)dy. (3.8) For y ∈ R d such that p Y (y) > 0 the conditional expectation function is therefore

## Conditional densities and expectations

$E [X|Y = y] := g ⋆ (y) = xp X|Y (x|y)dx,(3.9)$where the second equality follows from taking the minimizer of the inner brackets in equation [(3.8)](#) for Y = y, similarly to equation [(3.3)](#). Composing g ⋆ with the random variable Y , we get

$E [X|Y ] := g ⋆ (Y ),(3.10)$which is a random variable in R d . Rather confusingly, both E [X|Y = y] and E [X|Y ] are often called conditional expectation, but these are different objects. In particular,

$E [X|Y = y] is a function R d → R d , while E [X|Y ] is a random variable assuming values in R d .$To disambiguate these two terms, our discussions will employ the notations introduced here.

The tower property is an useful property that helps simplify derivations involving conditional expectations of two RVs X and Y :

$E [E [X|Y ]] = E [X] (3.11)$Because E [X|Y ] is a RV, itself a function of the RV Y , the outer expectation computes the expectation of E [X|Y ]. The tower property can be verified by using some of the definitions above:

$E [E [X|Y ]] = xp X|Y (x|y)dx p Y (y)dy (3.6) = xp X,Y (x, y)dxdy (3.5) = xp X (x)dx = E [X] .$Finally, consider a helpful property involving two RVs f (X, Y ) and Y , where X and Y are two arbitrary RVs. Then, by using the Law of the Unconscious Statistician with (3.9), we obtain the identity

$E [f (X, Y )|Y = y] = f (x, y)p X|Y (x|y)dx.$(3.12)

## Diffeomorphisms and push-forward maps

We denote by C r (R m , R n ) the collection of functions f : R m → R n with continuous partial derivatives of order r: 

$∂ r f k ∂x i1 • • • ∂x ir , k ∈ [n], i j ∈ [m],(3.13)$$∈ C r (R n , R n ) with ψ -1 ∈ C r (R n , R n ).$Then, given a RV X ∼ p X with density p X , let us consider a RV Y = ψ(X), where ψ :

$R d → R d is a C 1 diffeomorphism.$The PDF of Y , denoted p Y , is also called the push-forward of p X . Then, the PDF p Y can be computed via a change of variables:

$E [f (Y )] = E [f (ψ(X))] = f (ψ(x))p X (x)dx = f (y)p X (ψ -1 (y)) det ∂ y ψ -1 (y) dy,$where the third equality is due the change of variables x = ψ -1 (y), ∂ y ϕ(y) denotes the Jacobian matrix (of first order partial derivatives), i.e.,

$[∂ y ϕ(y)] i,j = ∂ϕ i ∂x j , i, j ∈ [d],$and det A denotes the determinant of a square matrix A ∈ R d×d . Thus, we conclude that the PDF p Y is

$p Y (y) = p X (ψ -1 (y)) det ∂ y ψ -1 (y) . (3.14)$We will denote the push-forward operator with the symbol ♯, that is

$[ψ ♯ p X ] (y) := p X (ψ -1 (y)) det ∂ y ψ -1 (y) .$(3.15)

## Flows as generative models

As mentioned in section 2, the goal of generative modeling is to transform samples X 0 = x 0 from a source distribution p into samples X 1 = x 1 from a target distribution q. In this section, we start building the tools necessary to address this problem by means of a flow mapping ψ t . More formally, a C r flow is a time-dependent mapping ψ :

$[0, 1] × R d → R d implementing ψ : (t, x) → ψ t (x). Such flow is also a C r ([0, 1] × R d , R d ) function, such that the function ψ t (x) is a C r diffeomorphism in x for all t ∈ [0, 1]. A flow model is a continuous-time$Markov process (X t ) 0≤t≤1 defined by applying a flow ψ t to the RV X 0 :

$X t = ψ t (X 0 ), t ∈ [0, 1]$, where X 0 ∼ p. [(3.16)](#) See Figure [5](#fig_9) for an illustration of a flow model. To see why X t is Markov, note that, for any choice of 0 ≤ t < s ≤ 1, we have [3.17)](#) where the last equality follows from using equation [(3.16](#)) to set X t = ψ t (X 0 ), and defining ψ s|t := ψ s • ψ -1 t , which is also a diffeomorphism. X s = ψ s|t (X t ) implies that states later than X t depend only on X t , so X t is Markov. In fact, for flow models, this dependence is deterministic. In summary, the goal generative flow modeling is to find a flow ψ t such that

$X s = ψ s (X 0 ) = ψ s (ψ -1 t (ψ t (X 0 ))) = ψ s|t (X t ),($$X 1 = ψ 1 (X 0 ) ∼ q.$(3.18)

## Equivalence between flows and velocity fields

A C r flow ψ can be defined in terms of a

$C r ([0, 1] × R d , R d ) velocity field u : [0, 1] × R d → R d implementing u : (t, x) → u t (x) via the following ODE: d dt ψ t (x) = u t (ψ t (x)) (flow ODE) (3.19a) ψ 0 (x) = x (flow initial conditions) (3.19b)$See figure [6](#fig_10) for an illustration of a flow together with its velocity field.

A standard result regarding the existence and uniqueness of solutions ψ t (x) to equation [(3.19](#)) is (see e.g., [Perko (2013)](#b60); Coddington et al. ( [1956](#))):

Theorem 1 (Flow local existence and uniqueness).

$If u is C r ([0, 1] × R d , R d ), r ≥ 1 (in particular, locally Lipschitz), then the ODE in (3.19) has a unique solution which is a C r (Ω, R d ) diffeomorphism ψ t (x) defined over an open set Ω which is super-set of {0} × R d .$This theorem guarantees only the local existence and uniqueness of a C r flow moving each point x ∈ R d by ψ t (x) during a potentially limited amount of time t ∈ [0, t x ). To guarantee a solution until t = 1 for all x ∈ R d , one must place additional assumptions beyond local Lipschitzness. For instance, one could consider global Lipschitness, guaranteed by bounded first derivatives in the C 1 case. However, we will later rely on a different condition-namely, integrability-to guarantee the existence of the flow almost everywhere, and until time t = 1.

So far, we have shown that a velocity field uniquely defines a flow. Conversely, given a C 1 flow ψ t , one can extract its defining velocity field u t (x) for arbitrary x ∈ R d by considering the equation d dt ψ t (x ′ ) = u t (ψ t (x ′ )), and using the fact that ψ t is an invertible diffeomorphism for every t ∈ [0, 1] to let x ′ = ψ -1 t (x). Therefore, the unique velocity field u t determining the flow ψ t is [3.20)](#) where ψt := d dt ψ t . In conclusion, we have shown the equivalence between C r flows ψ t and C r velocity fields u t .

$u t (x) = ψt (ψ -1 t (x)),($
## Computing target samples from source samples

Computing a target sample X 1 -or, in general, any sample X t -entails approximating the solution of the ODE in equation [(3.19](#)) starting from some initial condition X 0 = x 0 . Numerical methods for ODEs is a classical and well researched topic in numerical analysis, and a myriad of powerful methods exist [(Iserles, 2009)](#b37). One of the simplest methods is the Euler method, implementing the update rule where h = n -1 > 0 is a step size hyper-parameter with n ∈ N. To draw a sample X 1 from the target distribution, apply the Euler method starting at some X 0 ∼ p to produce the sequence X h , X 2h , . . . , X 1 . The Euler method coincides with first-order Taylor expansion of X t :

$X t+h = X t + hu t (X t ) (3.21)$$X t+h = X t + h Ẋt + o(h) = X t + hu t (X t ) + o(h),$where o(h) stands for a function growing slower than h, that is, o(h)/h → 0 as h → 0. Therefore, the Euler method accumulates o(h) error per step, and can be shown to accumulate o(1) error after n = 1/h steps. Therefore, the error of the Euler method vanishes as we consider smaller step sizes h → 0. The Euler method is just one example among many ODE solvers. Code 2 exemplifies another alternative, the second-order midpoint method, which often outperforms the Euler method in practice.

Code 2: Computing X 1 with Midpoint solver 1 from flow_matching.solver import ODESolver 2 from flow_matching.utils import ModelWrapper 3 4 class Flow(ModelWrapper): 5 def __init__(self, dim=2, h=64): 6 super().__init__() 7 self.net = torch.nn.Sequential( 8 torch.nn.Linear(dim + 1, h), torch.nn.ELU(), 9 torch.nn.Linear(h, dim)) 10 11 def forward(self, x, t): 12 t = t.view(-1, 1).expand(*x.shape[:-1], -1) 13 return self.net(torch.cat((t, x), -1)) 14 15 velocity_model = Flow() 16 17 ... # Optimize the model parameters s.t. model(x_t, t) = ut(Xt) 

## Probability paths and the Continuity Equation

We call a time-dependent probability (p t ) 0≤t≤1 a probability path. For our purposes, one important probability path is the marginal PDF of a flow model X t = ψ t (X 0 ) at time t:

$X t ∼ p t . (3.22)$For each time t ∈ [0, 1], these marginal PDFs are obtained via the push-forward formula in equation [(3.15)](#), that is,

$p t (x) = [ψ t♯ p] (x). (3.23)$Given some arbitrary probability path p t we define [.24)](#) In this way, we establish a close relationship between velocity fields, their flows, and the generated probability paths, see Figure [7](#fig_11) for an illustration. Note that we use the time interval [0, [1)](#b87), open from the right, to allow dealing with target distributions q with compact support where the velocity is not defined precisely at t = 1.

$u t generates p t if X t = ψ t (X 0 ) ∼ p t for all t ∈ [0, 1). (3$To verify that a velocity field u t generates a probability path p t , one can verify if the pair (u t , p t ) satisfies a partial differential equation (PDE) known as the Continuity Equation:

$d dt p t (x) + div(p t u t )(x) = 0, (3.25) where div(v)(x) = d i=1 ∂ x i v i (x), and v(x) = (v 1 (x), . . . , v d (x)).$The following theorem, a rephrased version of the Mass Conservation Formula [(Villani et al., 2009)](#), states that a solution u t to the Continuity Equation generates the probability path p t : Theorem 2 (Mass Conservation). Let p t be a probability path and u t a locally Lipchitz integrable vector field. Then, the following two statements are equivalent:

1. The Continuity Equation (3.25) holds for t ∈ [0, 1).

2. u t generates p t , in the sense of [(3.24)](#).

In the previous theorem, local Lipschitzness assumes that there exists a local neighbourhood over which u t (x) is Lipschitz, for all (t, x). Assuming that u is integrable means that:

$1 0 ∥u t (x)∥ p t (x)dxdt < ∞. (3.26)$Specifically, integrating a solution to the flow ODE [(3.19a)](#) across times [0, t] leads to the integral equation

$ψ t (x) = x + t 0 u s (ψ s (x))ds. (3.27)$Therefore, integrability implies

$E ∥X t ∥ (3.16) = ∥ψ t (x)∥ p(x)dx = x + t 0 u s (ψ s (x))ds p(x)dx (i) ≤ E ∥X 0 ∥ + 1 0 ∥u s (x)∥ p t (x)dt (ii) < ∞,$where (i) follows from the triangle inequality, and (ii) assumes the integrability condition [(3.26](#)) and E ∥X 0 ∥ < ∞. In sum, integrability allows assuming that X t has bounded expected norm, if X 0 also does. (3.29) This equation expresses the rate of change of total probability mass in the volume D (left-hand side) as the negative probability flux leaving the domain (right-hand side). The probability flux, defined as j t (y) = p t (y)u t (y), is the probability mass flowing through the hyperplane orthogonal to n(y) per unit of time and per unit of (possibly high-dimensional) area. See figure [8](#fig_13) for an illustration.

## Instantaneous Change of Variables

One important benefit of using flows as generative models is that they allow the tractable computation of exact likelihoods log p 1 (x), for all x ∈ R d . This feature is a consequence of the Continuity Equation called the Instantaneous Change of Variables (Chen et al., 2018):

$d dt log p t (ψ t (x)) = -div(u t )(ψ t (x)). (3.30)$This is the ODE governing the change in log-likelihood, log p t (ψ t (x)), along a sampling trajectory ψ t (x) defined by the flow ODE [(3.19a](#)). To derive [(3.30)](#), differentiate log p t (ψ t (x)) with respect to time, and apply both the Continuity Equation [(3.25)](#) and the flow ODE [(3.19a)](#). Integrating (3.30) from t = 0 to t = 1 and rearranging, we obtain

$log p 1 (ψ 1 (x)) = log p 0 (ψ 0 (x)) - 1 0 div(u t )(ψ t (x))dt. (3.31)$In practice, computing div(u t ), which equals the trace of the Jacobian matrix ∂ x u t (x) ∈ R d×d , is increasingly challenge as the dimensionality d grows. Because of this reason, previous works employ unbiased estimators such as Hutchinson's trace estimator [(Grathwohl et al., 2018)](#b27):

$div(u t )(x) = tr [∂ x u t (x)] = E Z tr Z T ∂ x u t (x)Z ,(3.32)$where Z ∈ R d×d is any random variable with E [Z] = 0 and Cov (Z, Z) = I, (for example, Z ∼ N (0, I)), and tr[Z] = d i=1 Z i,i . By plugging the equation above into [(3.31)](#) and switching the order of integral and expectation, we obtain the following unbiased log-likelihood estimator: [.33)](#) In contrast to div(u t )(ψ t (x)) in [(3.30)](#), computing tr Z T ∂ x u t (ψ t (x))Z for a fixed sample Z in the equation above can be done with a single backward pass via a vector-Jacobian product (JVP) [1](#foot_0) .

$log p 1 (ψ 1 (x)) = log p 0 (ψ 0 (x)) -E Z 1 0 tr Z T ∂ x u t (ψ t (x))Z dt. (3$In summary, computing an unbiased estimate of log p 1 (x) entails simulating the ODE

$d dt f (t) g(t) = u t (f (t)) -tr Z T ∂ x u t (f (t))Z , (3.34a) f (1) g(1) = x 0 ,(3.34b)$backwards in time, from t = 1 to t = 0, and setting:

$log p 1 (x) = log p 0 (f (0)) -g(0). (3.35)$See code 3 for an example on how to obtain log-likelihood estimates from a flow model using the flow_matching library.

Code 3: Computing the likelihood

1 from flow_matching.solver import ODESolver 2 from flow_matching.utils import ModelWrapper 3 from torch.distributions.normal import Normal 4 5 velocity_model: ModelWrapper = ... # Train the model parameters s.t. model(x_t, t) = ut(xt) 6 7 x_1 = torch.randn(batch_size, *data_dim) # Point X 1 where we wish to compute log p 1 (x) 8 9 # Define log p 0 (x) 10 gaussian_log_density = Normal(torch.zeros(size=data_dim), torch.ones(size=data_dim)).log_prob 11 12 solver = ODESolver(velocity_model=velocity_model) 13 num_steps = 100 14 x_0, log_p1 = solver.compute_likelihood( 15 x_1=x_1, 16 method='midpoint', 17 step_size=1.0 / num_steps, 18 log_p0=gaussian_log_density 19 )

## Training flow models with simulation

The Instantaneous Change of Variables, and the resulting ODE system [(3.34)](#), allows training a flow model by maximizing the log-likelihood of training data [(Chen et al., 2018;](#)[Grathwohl et al., 2018)](#b27). Specifically, let u θ t be a velocity field with learnable parameters θ ∈ R p , and consider the problem of learning θ such that

$p θ 1 ≈ q. (3.36)$We can pursue this goal, for instance, by minimizing the KL-divergence of p θ 1 and q: [(3.37)](#) where p θ 1 is the distribution of X 1 = ψ θ 1 (X 0 ), ψ θ t is defined by u θ t , and we can obtain an unbiased estimate of log p θ 1 (Y ) via the solution to the ODE system [(3.34)](#). However, computing this loss-as well as its gradientsrequires precise ODE simulations during training, where only errorless solutions constitute unbiased gradients. In contrast, Flow Matching, presented next, is a simulation-free framework to train flow generative models without the need of solving ODEs during training.

$L(θ) = D KL (q, p θ 1 ) = -E Y ∼q log p θ 1 (Y ) + constant,$
## Flow Matching

Given a source distribution p and a target distribution q, Flow Matching (FM) [(Lipman et al., 2022;](#b44)[Liu et al., 2022;](#b46)[Albergo and Vanden-Eijnden, 2022](#b0)) is a scalable approach for training a flow model, defined by a learnable velocity u θ t , and solving the Flow Matching Problem:

Find u θ t generating p t , with p 0 = p and p 1 = q. (4.1)

In the equation above, "generating" is in the sense of equation [(3.24)](#). Revisiting the Flow Matching blueprint from figure [2](#fig_2), the FM framework (a) identifies a known source distribution p and an unknown data target distribution q, (b) prescribes a probability path p t interpolating from p 0 = p to p 1 = q, (c) learns a velocity field u θ t implemented in terms of a neural network and generating the path p t , and (d) samples from the learned model by solving an ODE with u θ t . To learn the velocity field u θ t in step (c), FM minimizes the regression loss:

$L FM (θ) = E Xt∼pt D u t (X t ), u θ t (X t ) ,(4.2)$where D is a dissimilarity measure between vectors, such as the squared ℓ 2 -norm D(u, v) = ∥u -v∥ 2 . Intuitively, the FM loss encourages our learnable velocity field u θ t to match the ground truth velocity field u t known to generate the desired probability path p t . Figure [9](#) depicts the main objects in the Flow Matching framework and their dependencies. Let us start our exposition of Flow Matching by describing how to build p t and u t , as well as a practical implementation of the loss (4.2).

## Data

To reiterate, let source samples be a RV X 0 ∼ p and target samples a RV X 1 ∼ q. Commonly, source samples follow a known distribution that is easy to sample, and target samples are given to us in terms of a dataset of finite size. Depending on the application, target samples may constitute images, videos, audio segments, or other types of high-dimensional, richly structured data. Source and target samples can be independent, or originate from a general joint distribution known as the coupling (X 0 , X 1 ) ∼ π 0,1 (X 0 , X 1 ), [(4.3)](#) where, if no coupling is known, the source-target samples are following the independent coupling π 0,1 (X 0 , X 1 ) = p(X 0 )q(X 1 ). One common example of independent source-target distributions is to consider the generation of images X 1 from random Gaussian noise vectors X 0 ∼ N (0, I). As an example of a dependent coupling, consider the case of producing high-resolution images X 1 from their low resolution versions X 0 , or producing colorized videos X 1 from their gray-scale counterparts X 0 .

## Building probability paths

Flow Matching drastically simplifies the problem of designing a probability path p t -together with its corresponding velocity field u t -by adopting a conditional strategy. As a first example, consider conditioning the design of p t on a single target example X 1 = x 1 , yielding the conditional probability path p t|1 (x|x 1 ) illustrated in figure [3a](#fig_6). Then, we may construct the overall, marginal probability path p t by aggregating such conditional probability paths p t|1 :

$p t (x) = p t|1 (x|x 1 )q(x 1 )dx 1 ,(4.4)$as illustrated in figure 3b. To solve the Flow Matching Problem, we would like p t to satisfy the following boundary conditions:

$p 0 = p, p 1 = q, (4.5)$that is, the marginal probability path p t interpolates from the source distribution p at time t = 0 to the target distribution q at time t = 1. These boundary conditions can be enforced by requiring the conditional probability paths to satisfy

$p 0|1 (x|x 1 ) = π 0|1 (x|x$1 ), and p 1|1 (x|x 1 ) = δ x1 (x), (4.6) Flow ψ t (x) ψ t (x|x 1 )

$tx 1 + (1 -t)x$Velocity field

$u t (x) u t (x|x 1 ) (x 1 -x)/(1 -t)$Probability path p t (x)

$p t (x|x 1 ) N (x|tx 1 , (1 -t) 2 I)$Boundary conds.

$p 0 = p p 1 = q p 0 = p p 1 = δ x1 p 0 = N (0, I) p 1 = δ x1 Loss Flow Matching (FM) (4.22) D u t (X t ), u θ t (X t ) Conditional FM (CFM) (4.23) D u t (X t |X 1 ), u θ t (X t )$OT, Gauss CFM (2.9)

$∥u θ t (X t ) -(X 1 -X 0 )∥$2 differentiation solve ODE differentiation solve ODE Continuity (3.25) non-unique solution Continuity (3.25) non-unique solution cond. expectation ??? conditioning marginalization push-forward X0 push-forward X0 Figure 9 Main objects of the Flow Matching framework and their relationships. A Flow is represented with a Velocity field defining a random process generating a Probability path . The main idea of Flow Matching is to break down the construction of a complex flow satisfying the desired Boundary conditions (top row) to conditional flows (middle row) satisfying simpler Boundary conditions and consequently easier to solve. The arrows indicate dependencies between different objects; Blue arrows signify relationships employed by the Flow Matching framework. The Loss column lists the losses for learning the Velocity field , where the CFM loss (middle and bottom row) is what used in practice. The bottom row lists the simplest FM algorithm instantiation as described in section 2.

where the conditional coupling π 0|1 (x 0 |x 1 ) = π 0,1 (x 0 , x 1 )/q(x 1 ) and δ x1 is the delta measure centered at x 1 .

For the independent coupling π 0,1 (x 0 , x 1 ) = p(x 0 )q(x 1 ), the first constraint above reduces to p 0|1 (x|x 1 ) = p(x).

Because the delta measure does not have a density, the second constraint should be read as p t|1 (x|y)f (y)dy → f (x) as t → 1 for continuous functions f . Note that the boundary conditions (4.5) can be verified plugging (4.6) into [(4.4)](#).

A popular example of a conditional probability path satisfying the conditions in (4.6) was given in (2.2):

$N (• | tx 1 , (1 -t) 2 I) → δ x1 (•) as t → 1.$
## Deriving generating velocity fields

Equipped with a marginal probability path p t , we now build a velocity field u t generating p t . The generating velocity field u t is an average of multiple conditional velocity fields u t (x|x 1 ), illustrated in figure [3c](#fig_6), and satisfying:

$u t (•|x 1 ) generates p t|1 (•|x 1 ). (4.7)$Then, the marginal velocity field u t (x), generating the marginal path p t (x), illustrated in figure [3d](#fig_6), is given by averaging the conditional velocity fields u t (x|x 1 ) across target examples:

$u t (x) = u t (x|x 1 )p 1|t (x 1 |x)dx 1 . (4.8)$To express the equation above using known terms, recall Bayes' rule

$p 1|t (x 1 |x) = p t|1 (x|x 1 )q(x 1 ) p t (x) ,(4.9)$defined for all x with p t (x) > 0. Equation (4.8) can be interpreted as the weighted average of the conditional velocities u t (x|x 1 ), with weights p 1|t (x 1 |x) representing the posterior probability of target samples x 1 given the current sample x. Another interpretation of (4.8) can be given with conditional expectations (see section 3.2). Namely, if X t is any RV such that X t ∼ p t|1 (•|X 1 ), or equivalently, the joint distribution of (X t , X 1 ) has density p t,1 (x, x 1 ) = p t|1 (x|x 1 )q(x 1 ) then using (3.12) to write (4.8) as a conditional expectation, we obtain

$u t (x) = E [u t (X t |X 1 ) | X t = x] ,(4.10)$which yields the useful interpretation of u t (x) as the least-squares approximation to u t (X t |X 1 ) given X t = x, see section 3.2. Note, that the X t in (4.10) is in general a different RV that X t defined by the final flow model [(3.16)](#), although they share the same marginal probability p t (x).

## General conditioning and the Marginalization Trick

To justify the constructions above, we need to show that the marginal velocity field u t from equations (4.8) and (4.10) generates the marginal probability path p t from equation (4.4) under mild assumptions. The mathematical tool to prove this is the Mass Conservation Theorem (theorem 2). To proceed, let us consider a slightly more general setting that will be useful later in the manuscript. In particular, there is nothing special about building conditional probability paths and velocity fields by conditioning on X 1 = x 1 . As noted in [Tong et al. (2023)](#b80), the analysis from the previous section carries through to conditioning on any arbitrary RV Z ∈ R m with PDF p Z . This yields the marginal probability path [(4.11)](#) which in turn is generated by the marginal velocity field

$p t (x) = p t|Z (x|z)p Z (z)dz,$$u t (x) = u t (x|z)p Z|t (z|x)dz = E [u t (X t |Z) | X t = x] ,(4.12$) where u t (•|z) generates p t|Z (•|z), p Z|t (z|x) = p t|Z (x|z)p Z (z) pt(x)

follows from Bayes' rule given p t (x) > 0, and X t ∼ p t|Z (•|Z). Naturally, we can recover the constructions in previous sections by setting Z = X 1 . Before we prove the main result, we need some regularity assumptions, encapsulated as follows.

$Assumption 1. p t|Z (x|z) is C 1 ([0, 1) × R d ) and u t (x|z) is C 1 ([0, 1) × R d , R d$) as a function of (t, x). Furthermore, p Z has bounded support, that is, p Z (x) = 0 outside some bounded set in R m . Finally, p t (x) > 0 for all x ∈ R d and t ∈ [0, 1).

These are mild assumptions. For example, one can show that p t (x) > 0 by finding a condition z such that p Z (z) > 0 and p t|Z (•|z) > 0. In practice, one can satisfy this by considering (1 -(1 -t)ϵ)p t|Z + (1 -t)ϵN (0, I) for an arbitrarily small ϵ > 0. One example of p t|Z (•|z) satisfying this assumption is the path in (2.2), where we let Z = X 1 . We are now ready to state the main result: Theorem 3 (Marginalization Trick). Under assumption 1, if u t (x|z) is conditionally integrable and generates the conditional probability path p t (•|z), then the marginal velocity field u t generates the marginal probability path p t , for all t ∈ [0, 1).

In the theorem above, conditionally integrable refers to a conditional version of the integrability condition from the Mass Conservation Theorem [(3.26)](#), namely:

$1 0 ∥u t (x|z)∥ p t|Z (x|z)p Z (x)dzdxdt < ∞. (4$.13) Proof. The result follows from verifying the two conditions of the Mass Conservation in theorem 2. First, let us check that the pair (u t , p t ) satisfies the Continuity Equation (3.25). Because u t (•|x 1 ) generates p t (•|x 1 ), we have that d dt p t (x) (i) = d dt p t|Z (x|z)p Z (x)dz (4.14) (ii) = -div x u t (x|z)p t|Z (x|z) p Z (z)dz (4.15) (i) = -div x u t (x|z)p t|Z (x|z)p Z (z)dz (4.16)

$(iii) = -div x [u t (x)p t (x)] . (4.17)$Equalities (i) follows from switching differentiation ( d dt and div x , respectively) and integration, as justified by Leibniz's rule, the fact that p t|Z (x|z) and u t (x|z) are C 1 in t, x, and the fact that p Z has bounded support (so all the integrands are integrable as continuous functions over bounded sets). Equality (ii) follows from the fact that u t (•|z) generates p t|Z (•|z) and theorem 2. Equality (iii) follows from multiplying and dividing by p t (x) (strictly positive by assumption) and using the formula (4.12) for u t .

To verify the second and last condition from theorem 2, we shall prove that u t is integrable and locally Lipschitz. Because C 1 functions are locally Lipschitz, it suffices to check that u t (x) is C 1 for all (t, x). This would follow from verifying that u t (x|z) and p t|Z (x|z) are C 1 and p t (x) > 0, which hold by assumption. Furthermore, u t (x) is integrable because u t (x|z) is conditionally integrable:

$1 0 ∥u t (x)∥ p t (x)dxdt ≤ 1 0 ∥u t (x|z)∥ p t|Z (x|z)p Z (z)dzdxdt < ∞,(4.18)$where the first inequality follows from vector Jensen's inequality.

## Flow Matching loss

After having established that the target velocity field u t generates the prescribed probability path p t from p to q, the missing ingredient is a tractable loss function to learn a velocity field model u θ t as close as possible to the target u t . One major roadblock towards stating this loss function directly is that computing the target u t is infeasible, as it requires marginalizing over the entire training set (that is, integrating with respect to x 1 in equation [(4.8)](#) or with respect to z in equation (4.12)). Fortunately, a family of loss functions known as Bregman divergences provides unbiased gradients to learn u θ t (x) in terms of conditional velocities u t (x|z) alone. Bregman divergences measure dissimilarity between two vectors u, v ∈ R d as

$D(u, v) := Φ(u) -[Φ(v) + ⟨u -v, ∇Φ(v)⟩] ,(4.19)$where Φ : R d → R is a strictly convex function defined over some convex set Ω ⊂ R d . As illustrated in figure [10](#fig_14), the Bregman divergence measures the difference between Φ(u) and the linear approximation to Φ developed around v and evaluated at u. Because linear approximations are global lower bounds for convex functions, it holds that D(u, v) ≥ 0. Further, as

$Φ is strictly convex, it follows that D(u, v) = 0 if and only if u = v. The most basic Bregman divergence is the squared Euclidean distance D(u, v) = ∥u -v∥ 2 , esulting from choosing Φ(u) = ∥u∥ 2 .$The key property that makes Bregman divergences useful for Flow Matching is that their gradient with respect to the second argument is affine invariant [(Holderrieth et al., 2024)](#b33):

$∇ v D(au 1 + bu 2 , v) = a∇ v D(u 1 , v) + b∇ v D(u 2 , v), for any a + b = 1, (4.20)$as it can be verified from equation [(4.19)](#). Affine invariance allows us to swap expected values with gradients as follows:

$∇ v D(E[Y ], v) = E[∇ v D(Y, v)] for any RV Y ∈ R d . (4.21)$The Flow Matching loss employs a Bregman divergence to regress our learnable velocity u θ t (x) onto the target velocity u t (x) along the probability path p t :

$L FM (θ) = E t,Xt∼pt D(u t (X t ), u θ t (X t )),(4.22)$where time t ∼ U [0, 1]. As mentioned above, however, the target velocity u t is not tractable, so the loss above cannot be computed as is. Instead, we consider the simpler and tractable Conditional Flow Matching (CFM) loss:

$L CFM (θ) = E t,Z,Xt∼p t|Z (•|Z) D(u t (X t |Z), u θ t (X t )). (4.23)$The two losses are equivalent for learning purposes, since their gradients coincide [(Holderrieth et al., 2024)](#b33):

Theorem 4. The gradients of the Flow Matching loss and the Conditional Flow Matching loss coincide:

$∇ θ L FM (θ) = ∇ θ L CFM (θ). (4.24)$In particular, the minimizer of the Conditional Flow Matching loss is the marginal velocity u t (x).

Proof. The proof follows a direct computation:

$∇ θ L FM (θ) = ∇ θ E t,Xt∼pt D(u t (X t ), u θ t (X t )) = E t,Xt∼pt ∇ θ D(u t (X t ), u θ t (X t )) (i) = E t,Xt∼pt ∇ v D(u t (X t ), u θ t (X t ))∇ θ u θ t (X t ) (4.12) = E t,Xt∼pt ∇ v D(E Z∼p Z|t (•|Xt) [u t (X t |Z)], u θ t (X t ))∇ θ u θ t (X t ) (ii) = E t,Xt∼pt E Z∼p Z|t (•|Xt) ∇ v D(u t (X t |Z), u θ t (X t ))∇ θ u θ t (X t ) (iii) = E t,Xt∼pt E Z∼p Z|t (•|Xt) [∇ θ D(u t (X t |Z), u θ t (X t ))] (iv) = ∇ θ E t,Z∼q,Xt∼p t|Z (•|Z) [D(u t (X t |Z), u θ t (X t ))] = ∇ θ L CFM (θ)$where in (i),(iii) we used the chain rule; (ii) follows from equation ( [4](#formula_48).21) applied conditionally on X t ; and in (iv) we use Bayes' rule.

Bregman divergences for learning conditional expectations. Theorem 4 is a particular instance of a more general result utilizing Bregman divergences for learning conditional expectations described next. It will be used throughout this manuscript and provide the basis for all scalable losses behind Flow Matching:

$Proposition 1 (Bregman divergence for learning conditional expectations). Let X ∈ S X , Y ∈ S Y be RVs over state spaces S X , S Y and g : R p × S X → R n , (θ, x) → g θ (x), where θ ∈ R p denotes learnable parameters. Let D x (u, v), x ∈ S X be a Bregman divergence over a convex set Ω ⊂ R n that contains the image of f . Then, ∇ θ E X,Y D X Y , g θ (X) = ∇ θ E X D X E [Y | X], g θ (X) . (4.25)$In particular, for all x with p X (x) > 0, the global minimum of g θ (x) w.r.t. θ satisfies

$g θ (x) = E [Y | X = x]. (4.26)$Proof. We assume g θ is differentiable w.r.t. θ and that the distributions of X and Y , as well as D x , and g allow switching differentiation and integration, develop:

$∇ θ E X,Y D X Y, g θ (X) (i) = E X E ∇ v D X Y, g θ (X) ∇ θ g θ (X) | X (ii) = E X ∇ v D X E [Y | X], g θ (X) ∇ θ g θ (X) (iii) = E X ∇ θ D X E [Y | X], g θ (X) = ∇ θ E X D X E [Y | X], g θ (X) ,$where (i) follows from the chain rule and the tower property of expectations [(3.11)](#). Equality (ii) follows from [(4.21)](#). Equality (iii) uses the chain rule again. Lastly, for every x ∈ S X with p X (x) > 0 we can choose

$g θ (x) = E [Y |X = x], obtaining E X D X E [Y | X],$g θ (X) = 0, which must be the global minimum with respect to θ.

Theorem 4 is readily shown from proposition 1 by making the choices

$X = X t , Y = u t (X t |Z), g θ (x) = u θ t (x)$, and taking the expectation with respect to t ∼ U [0, 1].

General time distributions One useful variation of the FM loss is to sample times t from a distribution other than Uniform. Specifically, consider t ∼ ω(t), where ω is a PDF over [[0,](#)[1]](#b87). This leads to the following weighted objective:

$L CFM (θ) = E t∼ω,Z,Xt D(u t (X t |Z), u θ t (X t )) = E t∼U,Z,Xt ω(t)D(u t (X t |Z), u θ t (X t )). (4.27)$Although mathematically equivalent, sampling t ∼ ω leads to better performance than using weights ω(t) in large scale image generation tasks [(Esser et al., 2024)](#b21).

## Solving conditional generation with conditional flows

So far, we have reduced the problem of training a flow model u θ t to: (i) Find conditional probability paths p t|Z (x|z) yielding a marginal probability path p t (x) satisfying the boundary conditions in (4.5). (ii) Find conditional velocity fields u t (x|z) generating the conditional probability path. (iii) Train using the Conditional Flow Matching loss (see equation [(4.23)](#)). We now discuss a concrete options on how to do step (i) and (ii), i.e., design such conditional probability paths and velocity fields.

We will now propose a flexible method to design such conditional probability paths and velocity fields using a specific construction via conditional flows. The idea is as follows: Define a flow model X t|1 (similarly to [(3.16](#))) satisfying the boundary conditions (4.6), and extract the velocity field from X t|1 by differentiation [(3.20)](#). This process defines both p t|1 (x|x 1 ) and u t (x|x 1 ). In more detail, define the conditional flow model

$X t|1 = ψ t (X 0 |x 1 ), where X 0 ∼ π 0|1 (• | x 1 ),(4.28)$where

$ψ : [0, 1) × R d × R d → R d is a conditional flow defined by ψ t (x|x 1 ) = x t = 0 x 1 t = 1 ,(4.29)$smooth in (t, x), and a diffeomorphism in x. (Smooth here means that all derivatives of ψ t (x|x 1 ) with respect to t and x exist and are continuous:

$C ∞ ([0, 1) × R d , R d ))$. These conditions could be further relaxed to

$C 2 ([0, 1) × R d , R d$) at the expense of simplicity.) The push-forward formula (3.15) defines the probability density of X t|1 as

$p t|1 (x|x 1 ) := ψ t (•|x 1 ) ♯ π 0|1 (•|x 1 ) (x),(4.30)$although we will not need this expression in practical optimization of the CFM loss it is used theoretically to show that p t|1 satisfies the two boundary conditions [(4.6)](#). First, and according to (4.29), ψ 0 (•|x 1 ) is the identity map, keeping π 0|1 (•|x 1 ) intact at time t = 0. Second, ψ 1 (•|x 1 ) = x 1 is the constant map, concentrating all probability mass at x 1 as t → 1. Furthermore, note that ψ t (•|x 1 ) is a smooth diffeomorphism for t ∈ [0, 1). Therefore, by the equivalence of flows and velocity fields (section 3.4.1), there exists a unique smooth conditional velocity field (see equation [(3.20](#))) taking form:

$u t (x|x 1 ) = ψt (ψ -1 t (x|x 1 )|x 1 ). (4.31)$To summarize: we have further reduced the task of finding the conditional path and a corresponding generating velocity to simply building a conditional flow ψ t (•|x 1 ) satisfying (4.29). In section 4.7 we will pick a particularly simple ψ t (x|x 1 ) with some desirable properties (conditional Optimal Transport flow) that leads to the standard Flow Matching algorithm as seen in section 1, and in section 4.8 we will discuss a particular and well-known family of conditional flows, namely affine flows that include some known examples from the diffusion models' literature. In section 5 we will use conditional flows to define Flow Matching on manifold which showcase the flexibility of this approach.

## The Conditional Flow Matching loss, revisited

Let us revisit the CFM loss (4.23) by setting Z = X 1 and using the conditional flows way of defining the conditional probability path and velocity,

$L CFM (θ) = E t,X1,Xt∼pt(•|X1) D u t (X t |X 1 ), u θ t (X t ) (3.4) = E t,(X0,X1)∼π0,1 D ψt (X 0 |X 1 ), u θ t (X t ) (4.32)$where in the second equality we used the Law of Unconscious Statistician with X t = ψ t (X 0 |X 1 ) and

$u t (X t |X 1 ) (4.31) = ψt ψ -1 t ψ t (X 0 |X 1 ) X 1 X 1 = ψt (X 0 |X 1 ). (4.33)$The minimizer of the loss (4.32) according to proposition 1 takes the form as in [(Liu et al., 2022)](#b46),

$u t (x) = E ψt (X 0 |X 1 ) X t = x .(4.34)$In the flow_matching library the ProbPath object defines a probability path. This probability path can be sampled at (t, X 0 , X 1 ) to obtain X t and ψt (X 0 |X 1 ). Then, one can compute a Monte Carlo estimate of the CFM loss L CFM (θ). An example training loop with the CFM objective is shown in code 4.  

$p t (x) = u t (x) = X1 conditioning ψ t (X 0 |x 1 ) ∼ p t|1 (•|x 1 ) p t|1 (x|x 1 )q(x 1 )dx 1 E [u t (X t |X 1 )|X t = x] = = X0 conditioning ψ t (X 1 |x 0 ) ∼ p t|0 (•|x 0 ) p t|0 (x|x 0 )p(x 0 )dx 1 E [u t (X t |X 0 )|X t = x] = = (X0,X1) conditioning ψ t (x 0 , x 1 ) ∼ p t|0,1 (•|x 0 , x 1 ) p t|0,1 (x|x 0 , x 1 )π 0,1 (x 0 , x 1 )dx 0 dx 1 E [u t (X t |X 0 , X 1 )|X t = x]$
## The Marginalization Trick for probability paths built from conditional flows

Next, we introduce a version of the Marginalization trick for probability paths that are built from conditional flows. To this end, note that if

$π 0|1 (•|x 1 ) is C 1 , then p t (x|x 1 ) is also C 1 by construction; moreover, u t (x|x 1 ) is conditionally integrable if E t,(X0,X1)∼π0,1 ψt (X 0 |X 1 ) < ∞.(4.35)$Therefore, by setting Z = X 1 , the following corollary to theorem 3 is obtained.

$Corollary 1. Assume that q has bounded support, π 0|1 (•|x 1 ) is C 1 (R d$) and strictly positive for some x 1 with q(x 1 ) > 0, and ψ t (x|x 1 ) is a conditional flow satisfying equations (4.29) and (4.35). Then p t|1 (x|x 1 ) and u t (x|x 1 ), defined in (4.30) and (4.31), respectively, define a marginal velocity field u t (x) generating the marginal probability path p t (x) interpolating p and q.

Proof. [(4.30](#)) and (3.15) for definitions). Furthermore, u t (x|x 1 ) (defined in (4.31)) is smooth and satisfies

$If π 0|1 (•|x 1 ) > 0 for some x 1 ∈ R d such that q(x 1 ) > 0, it follows that p t|1 (x|x 1 ) > 0 for all x ∈ R d and is C 1 ([0, 1) × R d ) (see$$1 0 ∥u t (x|x 1 )∥ p t|1 (x|x 1 )q(x 1 )dx 1 dxdt = E t,X1∼q,Xt∼p t|1 (•|X1) ∥u t (X t |X 1 )∥ (3.4) = E t,X1∼q,X0∼π 0|1 (•|X1) ∥u t (ψ t (X 0 |X 1 )|X 1 )∥ (4.33) = E t,(X0,X1)∼π0,1 ψt (X 0 |X 1 ) < ∞.$Therefore, u t (x|x 1 ) is conditionally integrable (see [(4.13))](#). By theorem 3, the marginal u t generates p t . Because p t|1 (x|x 1 ) as defined by (4.30) satisfies (4.6), it follows that p t interpolates p and q.

This theorem will be used as a tool to show that particular choices of conditional flows lead to marginal velocity u t (x) generating the marginal probability path p t (x).

## Conditional flows with other conditions

Different conditioning choices Z exist but are essentially all equivalent. As illustrated in figure [11](#fig_16), main options include fixing target samples Z = X 1 [(Lipman et al., 2022)](#b44), source samples Z = X 0 [(Esser et al., 2024)](#b21), or two-sided Z = (X 0 , X 1 ) (Albergo and Vanden-Eijnden, 2022; [Liu et al., 2022;](#b46)[Pooladian et al., 2023;](#b64)[Tong et al., 2023)](#b80).

Let us focus on the two-sided condition Z = (X 0 , X 1 ). Following the FM blueprint described above, we are now looking to build a conditional probability path p t|0,1 (x|x 0 , x 1 ) and a corresponding generating velocity u t (x|x 0 , x 1 ) such that p 0|0,1 (x|x 0 , x 1 ) = δ x0 (x), and p 1|0,1 (x|x 0 , x 1 ) = δ x1 (x). (4.36)

We will keep this discussion formal as it requires usage of delta functions δ and our existing derivations so far only deals with probability densities (and not general distributions). To build such a path we can consider an interpolant (Albergo and Vanden-Eijnden, 2022) defined by X t|0,1 = ψ t (x 0 , x 1 ) for a function

$ψ : [0, 1] × R d × R d → R d satisfying conditions similar to (4.29), ψ t (x 0 , x 1 ) = x 0 t = 0 x 1 t = 1. (4.37) Therefore, ψ t (•, x 1 ) pushes δ x0 (x) to δ x1 (x)$. We now, similarly to before, define the conditional probability path to be

$p t|0,1 (•|x 0 , x 1 ) := ψ t (•, x 1 ) ♯ δ x0 (•) (4.38)$which satisfies the boundary constraints in [(4.36)](#). Albergo and Vanden-Eijnden (2022)'s stochastic interpolant is defined by

$X t = ψ t (X 0 , X 1 ) ∼ p t (•) = p t|0,1 (•|x 0 , x 1 )π 0,1 (x 0 , x 1 )dx 0 dx 1 . (4.39)$Next, the conditional velocity along this path can also be computed with [(3.20)](#) giving

$u t (x|x 0 , x 1 ) = ψt (x 0 , x 1 ) (4.40)$which is defined only for x = ψ t (x 0 , x 1 ). Ignoring for a second the extra conditions, Theorem 3 now presumably implies that the marginal velocity generating p t (x) is

$u t (x) = E [u t (X t |X 0 , X 1 ) | X t = x] = E ψt (X 0 , X 1 ) | X t = x ,$which leads to the same marginal formula as the X 1 -conditioned case (4.34), but with a seemingly more permissive conditional flow ψ t (x 0 , x 1 ) which is only required to be an interpolant now, weakening the more stringent diffeomorphism condition. However, A more careful look reveals that some extra conditions are still required to make u t (x) a generating velocity for p t (x) and simple interpolation (as defined in (4.37)) is not enough to guarantee this, not even with extra smoothness conditions, as required in Theorem 3. To see this, consider

$ψ t (x 0 , x 1 ) = (1 -2t) τ + x 0 + (2t -1) τ + x 1 , where (s) + = ReLU(s), τ > 2, a C 2 ([0, 1]$) interpolant (in time) concentrating all probability mass at location 0 at time t = 0.5 for all x 0 , x 1 . That is P(X 1 2 = 0) = 1. Therefore, assuming u t (x) indeed generates p t (x) its marginal at t = 1 2 is δ 0 and since a flow is both Markovian (as shown in (3.17)) and deterministic its marginal has to be a delta function for all t > 0.5 leading to a contradiction since X 1 = ψ 1 (X 0 , X 1 ) ∼ q, which is generally not a delta function. Albergo and Vanden-Eijnden (2022) and [Liu et al. (2022)](#b46) provide some extra conditions that guarantee that u t (x) indeed geenrates p t (x) but these are somewhat harder to verify compared to the conditions of Theorem 3. Below we will show how to practically check the conditions of Theorem 3 to validate that particular paths of interest are guaranteed to be generated by the respective marginal velocities.

Nevertheless, when ψ t (x 0 , x 1 ) is in addition a diffeomorphism in x 0 for a fixed x 1 , and in x 1 for a fixed x 0 , the three constructions leads to the same marginal velocity, defined by (4.34), and same marginal probability path p t , defined by

$X t = ψ t (X 0 , X 1 ) = ψ t (X 0 |X 1 ) = ψ t (X 1 |X 0 ), see Figure 11.$
## Optimal Transport and linear conditional flow

We now ask: how to find a useful conditional flow ψ t (x|x 1 )? One approach is to choose it as a minimizer of a natural cost functional, ideally with some desirable properties. One popular example of such cost functional is the dynamic Optimal Transport problem with quadratic cost [(Villani et al., 2009;](#)[Villani, 2021;](#b81)[Peyré et al., 2019)](#b61), formalized as

$(p ⋆ t , u ⋆ t ) = arg min pt,ut 1 0 ∥u t (x)∥ 2 p t (x)dxdt (Kinetic Energy) (4.41a) s.t. p 0 = p, p 1 = q (interpolation) (4.41b) d dt p t + div(p t u t ) = 0. (continuity equation) (4.41c)$The (p ⋆ t , u ⋆ t ) above defines a flow (via equation [(3.19](#))) with the form

$ψ ⋆ t (x) = tϕ(x) + (1 -t)x,(4.42)$called the OT displacement interpolant [(McCann, 1997)](#b53), where ϕ : R d → R d is the Optimal Transport map.

The OT displacement interpolant also solves the Flow Matching Problem (4.1) by defining the random variable

$X t = ψ ⋆ t (X 0 ) ∼ p ⋆ t when X 0 ∼ p. (4.43)$The Optimal Transport formulation promotes straight sample trajectories

$X t = ψ ⋆ t (X 0 ) = X 0 + t(ϕ(X 0 ) -X 0 ),$with a constant velocity ϕ(X 0 ) -X 0 , which are in general easier to sample with ODE solvers-in particular, the target sample X 1 is here perfectly solvable with a single step of the Euler Method [(3.21)](#).

We can now try to plug our marginal velocity formula (equation (4.34)) into the Optimal Transport problem (4.41) and search for an optimal ψ t (x|x 1 ). While this seems like a challenge, we can instead find a bound for the Kinetic Energy for which such a minimizer is readily found [(Liu et al., 2022)](#b46):

$1 0 E Xt∼pt ∥u t (X t )∥ 2 dt = 1 0 E Xt∼pt E ψt (X 0 |X 1 ) X t 2 dt (4.44) (i) ≤ 1 0 E Xt∼pt E ψt (X 0 |X 1 ) 2 X t dt (4.45) (ii) = E (X0,X1)∼π0,1 1 0 ψt (X 0 |X 1 ) 2 dt,(4.46)$where in the (i) we used Jensen's inequality, and in (ii) we used the tower property of conditional expectations (see equation [(3.11)](#)) and switch integration of t and expectation. Now the integrand in (4.46) can be minimized individually for each (X 0 , X 1 ) -this leads to the following variational problem for γ t = ψ t (x|x 1 ):

$min γ:[0,1]→R d 1 0 ∥ γt ∥ 2 dt (4.47a) s.t. γ 0 = x, γ 1 = x 1 . (4.47b)$This problem can be solved using Euler-Lagrange equations [(Gelfand et al., 2000)](#b26), which in this case take the form d 2 dt 2 γ t = 0. By incorporating the boundary conditions, we obtain the minimizer:

$ψ t (x|x 1 ) = tx 1 + (1 -t)x. (4.48)$Note that although not constrained to be, this choice of ψ t (x|x 1 ) is a diffeomorphism in x for t ∈ [0, 1) and smooth in t, x, as required from conditional flows.

Several conclusions can be drawn:

1. The linear conditional flow minimizes a bound of the Kinetic Energy among all conditional flows.

2. In case the target q consists of a single data point q(x) = δ x1 (•) we have that the linear conditional flow in (4.48) is the Optimal Transport [(Lipman et al., 2022)](#b44). Indeed, in this case X t = ψ t (X 0 |x 1 ) ∼ p t and

$X 0 = ψ -1 (X t |x 1 ) is a function of X t which makes E ψt (X 0 |x 1 ) X t = ψt (X 0 |x 1 )$and therefore (ii) becomes an equality.

Theorem 5. If q = δ x1 , then the dynamic OT problem (4.41) has an analytic solution given by the OT displacement interpolant in (4.48).

3. Plugging the linear conditional flow in (4.46) we get

$1 0 E Xt∼pt ∥u t (X t )∥ 2 dt ≤ E (X0,X1)∼π0,1 1 0 ∥X 1 -X 0 ∥ 2 dt (4.49)$showing that the Kinetic Energy of the marginal velocity u t (x) is not bigger than that of the original coupling π 0,1 [(Liu et al., 2022)](#b46).

The conditional flow in (4.48) is in particular affine and consequently motivates investigating the family of affine conditional flows, discussed next.

## Affine conditional flows

In the previous section we discovered the linear (Conditional-OT) flows as a minimizer to a bound of the Kinetic Energy among all conditional flows. The linear conditional flow is a particular instance the wider family of affine conditional flows, explored in this section.

$ψ t (x|x 1 ) = α t x 1 + σ t x,(4.50)$where α t , σ t : [0, 1] → [0, 1] are smooth functions satisfying α 0 = 0 = σ 1 , α 1 = 1 = σ 0 , and αt , -σt > 0 for t ∈ (0, 1). (4.51)

We call the pair (α t , σ t ) a scheduler. The derivative condition above ensures that α t is strictly monotonically increasing, while σ t is strictly monotonically decreasing. The conditional flow (4.50) is a simple affine map in x for each t ∈ [0, 1), which satisfies the conditions (4.29). The associated marginal velocity field (4.34) is

$u t (x) = E [ αt X 1 + σt X 0 |X t = x] . (4.52)$By virtue of corollary 1, we can prove that, if using the independent coupling and a smooth and strictly positive source density p with finite second moments-for instance, a Gaussian p = N (•|0, I)-then u t generates a probability path p t interpolating p and q. We formally state this result, significant for Flow Matching applications, as the following theorem.

Theorem 6. Assume that q has bounded support, p is C 1 (R d ) with strictly positive density with finite second moments, and these two relate by the independent coupling π 0,1 (x 0 , x 1 ) = p(x 0 )q(x 1 ). Let p t (x) = p t|1 (x|x 1 )q(x 1 )dx 1 be defined by equation (4.30), with ψ t defined by equation (4.50). Then, the marginal velocity (4.52) generates p t interpolating p and q.

Proof. We apply corollary 1. First, note that π 0|1 (•|x 1 ) = p(•) is C 1 and positive everywhere by assumption. Second, ψ t , defined in (4.50), satisfies [(4.29)](#). Third, we are left with checking (4.35):

$E t,(X0,X1) ψ(X 0 |X 1 ) = E t,(X0,X1) ∥ αt X 1 + σt X 0 ∥ ≤ E t | αt | E X1 ∥X 1 ∥ + E t | σt | E X0 ∥X 0 ∥ = E X1 ∥X 1 ∥ + E X0 ∥X 0 ∥ < ∞,$where the last inequality follows from the fact that X 1 ∼ q has bounded support and X 0 ∼ p has bounded second moments.

In this affine case, the CFM loss (4.32) takes the form 

$L CFM (θ) = E t,(X0,X1)∼π0,1 D( αt X 1 + σt X 0 , u θ t (X t )) . (4$
## Velocity parameterizations

In the affine case, the marginal velocity field u t admits multiple parametrizations, each of them learnable using the Flow Matching losses introduced in section 4.5. To derive these parametrizations, use the equivalent formulations of the affine paths

$X t = α t X 1 + σ t X 0 ⇔ X 1 = X t -σ t X 0 α t ⇔ X 0 = X t -α t X 1 σ t , (4.54)$in the marginal velocity formula (4.52), obtaining

$u t (x) = αt E [X 1 |X t = x] + σt E [X 0 |X t = x] (4.55) = σt σ t x + αt -α t σt σ t E [X 1 |X t = x] (4.56) = αt α t x + σt -σ t αt α t E [X 0 |X t = x],(4.57)$where we have used the fact that E [Z|Z = z] = z. Then, denote the deterministic functions:

$x 1|t (x) = E [X 1 |X t = x] as the x 1 -prediction (target), (4.58) x 0|t (x) = E [X 0 |X t = x] as the x 0 -prediction (source). (4.59)$These provides two more opportunities to parameterize u t : via the x 1 -prediction x 1|t (4.56) and via the x 0 -prediction x 0|t (4.57). Table [1](#) offers conversion formulas between the parameterizations. These parameterizations can also be learned using a Conditional Matching loss, similar to (4.23). In particular, any function

$g t (x) := E [f t (X 0 , X 1 )|X t = x] ,(4.60)$where f t (X 0 , X 1 ) is a RV defined as a time-dependent function of X 0 and X 1 , can be learned by minimizing a Matching loss of the form

$L M (θ) = E t,Xt∼pt D g t (X t ), g θ t (X t ) . (4.61)$This loss has the same gradients as the Conditional Matching loss

$L CM (θ) = E t,(X0,X1)∼π0,1 D f t (X 0 , X 1 ), g θ t (X t ) . (4.62)$To learn x 1|t , the Conditional Matching loss employs f t (x 0 , x 1 ) = x 1 , and similarly for x 0|t . This procedure is justified by theorem 7, which is an immediate result from proposition 1 when letting X = X t , Y = f t (X 0 , X 1 ), and integrating with respect to t ∼ U [0, 1].

Theorem 7. The gradients of the Matching loss and the Conditional Matching loss coincide for arbitrary functions f t (X 0 , X 1 ) of X 0 , X 1 :

$∇ θ L M (θ) = ∇ θ L CM (θ). (4.63)$In particular, the minimizer of the Conditional Matching loss is the conditional expectation

$g θ t (x) = E [f t (X 0 , X 1 )|X t = x] . (4.64)$Code 6 shows how to train with x 1 -prediction using the flow_matching library.  Singularities in the velocity parameterizations. Seemingly, the coefficients of (4.56) would blow up as t → 1, and similarly for (4.57

$) as t → 0. If E [X 1 |X 0 = x] and E [X 0 |X 1 = x]$exist, which is the case for p(x) > 0 and q(x) > 0, these are not essential singularities in theory, meaning that the singularities in x 1|t and x 0|t would cancel with the singularities of the coefficients of the parameterization. However, these singularities could be still problematic in practice when the learnable x θ 1|t and x θ 0|t are by construction continuous and therefore do not perfectly regress their targets x 1|t and x 0|t . To understand how to fix these potential issues, recall (4.55) and consider u 0

$(x) = α0 E [X 1 |X 0 = x] + σ0 x as t → 0, and u 1 (x) = α1 x + σ1 E [X 0 |X 1 = x] as t → 1.$These can be computed in many cases of interest. Returning to our example π 0,1 (x 0 , x 1 ) = N (x 0 |0, I)q(x 1 )

and assuming E X1 X 1 = 0, it follows that u 0 (x) = σ0 x and u 1 (x) = α1 x. These expressions can be used to fix singularities when converting from x 1|t and x 0|t to u t (x) as t → 1 or t → 0, respectively.

## Post-training velocity scheduler change

Affine conditional flows admit a closed-form transformation from a marginal velocity field u t (x), based on a scheduler (α t , σ t ) and an arbitrary data coupling π 0,1 , to a marginal velocity field ūr (x), based on a different scheduler (ᾱ r , σr ) and the same data coupling π 0,1 . Such a transformation is useful to adapt a trained velocity field to a different scheduler, potentially improving sample efficiency and quality generation [(Karras et al., 2022;](#b40)[Shaul et al., 2023b;](#)[Pokle et al., 2023)](#b62). To proceed, define the scale-time (ST) transformation (s r , t r ) between the two conditional flows:

ψr (x 0 |x 1 ) = s r ψ tr (x 0 |x 1 ), [(4.65)](#) where ψ t (x 0 |x 1 ) = α t x 1 + σ t x 0 , ψr (x 0 |x 1 ) = ᾱr x 1 + σr x 0 , and s, t

$: [0, 1] → R ≥0 are time-scale reparametriza- tions. Solving (4.65) yields t r = ρ -1 (ρ(r)) s r = σr /σ tr ,(4.66)$where we define the signal-to-noise ratio by

$ρ(t) = α t σ t ρ(t) = ᾱt σt ,(4.67)$assumed to be an invertible function. The marginal velocity ūr (x) for the new scheduler (ᾱ r , σr ) follows the expression

$ūr (x) = E Ẋr Xr = x (4.65) = E ṡr X tr + s r Ẋtr ṫr s r X tr = x = ṡr E X tr X tr = x s r + s r ṫr E Ẋtr X tr = x s r = ṡr s r x + s r ṫr u tr x s r ,$where as before Xr = ψr (X 0 |X 1 ) and X t = ψ t (X 0 |X 1 ). This last term can be used to change a schedular post-training. Code 7 shows how to change the scheduler of a velocity field trained with a variance preserving schedule to the conditional Optimal Transport schedule using the flow_matching library.

Equivalence of schedulers. One additional important consequence of the above formula is that all schedulers theoretically lead to the same sampling at time t = 1 [(Shaul et al., 2023a)](#). That is,

$ψ1 (x 0 ) = ψ 1 (x 0 ), for all x 0 ∈ R d . (4.68)$To see that, denote ψr (x) the flow defined by ūt (x), and differentiate ψr (x) := s r ψ tr (x) w.r.t. r and note that it also satisfies d dt ψr (x) = ūr ( ψr (x)). (4.69)

Therefore, from uniqueness of ODE solutions we have that ψr (x) = ψr (x) = s r ψ tr (x). Now, to avoid dealing with infinite signal-to-noise ratio assume the schedulers satisfy σ 1 = ϵ = σ1 for arbitrary ϵ > 0 (in addition to (4.51)), then for r = 1 we have t 1 = 1 and s 1 = 1 and therefore equation (4.68) holds. 

## Gaussian paths

At the time of writing, the most popular class of affine probability paths is instantiated by the independent coupling π 0,1 (x 0 , x 1 ) = p(x 0 )q(x 1 ) and a Gaussian source distribution p(x) = N (x|0, σ 2 I). Because Gaussians are invariant to affine transformations, the resulting conditional probability paths take form

$p t|1 (x|x 1 ) = N (x|α t x 1 , σ 2 t I). (4.70)$This case subsumes probability paths generated by standard diffusion models (although in diffusion the generation is stochastic and follows an SDE, it has the same marginal probabilities). Two examples are the Variance Preserving (VP) and Variance Exploding (VE) paths [(Song et al., 2021)](#), defined by choosing the following schedulers:

$α t ≡ 1, σ 0 ≫ 1, σ 1 = 0; (VP) α t = e -1 2 βt , σ t = 1 -e -βt , β 0 ≫ 1, β 1 = 0. (VE)$In the previous equations, "≫ 1" requires a sufficiently large scalar such that p 0 (x) = p 0|1 (x|x 1 )q(x 1 )dx 1 is close to a known Gaussian distribution for t = 0-that is, the Gaussian N (•|0, σ 2 0 I) for VE, and N (•|0, I) for VP. Note that in both cases, p t (x) does not exactly reproduce p at t = 0, in contrast to the FM paths in (4.51).

One useful quantity admitting a simple form in the Gaussian case is the score, defined as the gradient of the log probability. Specifically, the score of the conditional path in (4.70) follows the expression

$∇ log p t|1 (x|x 1 ) = - 1 σ 2 t (x -α t x 1 ) . (4.71) B A velocity x 1 -prediction x 0 -prediction score velocity 0, 1 σt σt , αtσt-σtαt σt αt αt , σtαt-αtσt αt αt αt , -σtσtαt-αtσ 2 t αt x 1 -prediction 0, 1 1 αt , -σt αt 1 αt , σ 2 t αt x 0 -prediction 0, 1 0, -σ t score 0, 1$Table [1](#) Conversion between different model parameterizations: (at, bt) corresponds to f B t (x) = atx + btf A t (x). The colors indicate the tranformation is relevant for all paths , Affine paths and Gaussian paths . The lower diagonal is computed from the upper diagonal using the inverse transformation:

$f A t (x) = 1 b t -atx + f B t (x)$which can be expressed as the paira t b t , 1 b t . Note some of these conversions have singularities, as discussed at the end of Section 4.8.1.

The score of the corresponding marginal probability path (4.4) is

$∇ log p t (x) = ∇p t|1 (x|x 1 )q(x 1 ) p t (x) dx 1 (4.72) = ∇ log p t|1 (x|x 1 ) p t|1 (x|x 1 )q(x 1 ) p t (x) dx 1 (4.73) = E ∇ log p t|1 (X t |X 1 ) | X t = x (4.74) (4.71) = E - 1 σ 2 t (X t -α t X 1 ) | X t = x (4.75) (4.54) = E - 1 σ t X 0 X t = x (4.76) (4.59) = - 1 σ t x 0|t (x),(4.77)$where we borrow the notation x 0|t from (4.59). The literature on diffusion refers to x 0 -prediction (x 0|t ) as noise-prediction, or ϵ-prediction. The formula above shows that the score is proportional to the x 0 -prediction, and provides a conversion rule-for the Gaussian path case-from score to other parametrizations, as shown in table [1](#).

Kinetic optimality of marginal velocity. A consequence of the conversion formulas developed above (table [1](#)) is that the marginal velocity for Gaussian paths can be written in the form

$u t (x) = αt α t x - σt σ t α t -αt σ 2 t α t ∇ log p t (x) (4.78) = ∇ αt 2α t ∥x∥ 2 - σt σ t α t -$αt σ 2 t α t log p t (x) (4.79) that shows u t (x) is a gradient and therefore Kinetic Optimal for the fixed marginalized Gaussian probability path p t (x) defined by p t|1 (x|x 1 ) = N (x|α t x 1 , σ 2 t I) (see e.g., Villani (2021) Section 8.1.2, or Neklyudov et al. (2023) Theorem 2.1).

## Data couplings

In developing the Flow Matching training algorithm, we have assumed we can draw samples (X 0 , X 1 ) ∼ π 0,1 (X 0 , X 1 ) from some coupling π 0,1 (x 0 , x 1 ) of the source p and target q distributions. For example, independent samples π 0,1 (x 0 , x 1 ) = p(x 0 )q(x 1 ), the simplest coupling preserving the marginal distributions p and q, or paired samples (X 0 , X 1 ) ∼ π 0,1 provided as part of the dataset. This section explores two examples of concrete couplings that can be used to train Flow Matching models.

## Paired data

Dependent couplings arise naturally in learning tasks on paired data. Consider, for instance, the task of image in-painting, where q is a distribution of natural images, and p is the distribution of those same images with a square region masked-out. Rather than transforming noise into data, our goal here is to learn a mapping from masked-out images x 0 to their filled counterparts x 1 . As this is an ill-defined problem-many filled images x 1 are compatible with each masked-out image x 0 -solving this task can casted as learning to sample from the unknown, data-dependent coupling π 1|0 (x 1 |x 0 ).

Based on these insights, [Liu et al. (2023)](#b45); [Albergo et al. (2024)](#) propose learning a bridge or flow model with data-dependent couplings, a simple modification enabling a new regime of applications. While the object of interest π 1|0 (x 1 |x 0 ) is unavailable to sample, it is often the case that one can sample from the reverse dependency, π 0|1 (x 0 |x 1 ). Returning to the example of image in-painting, it is easy to mask out a filled image X 1 ∼ q (target sample) to produce a source sample X 0 ∼ p. To this end, specify π 0,1 (x 0 , x 1 ) = π 0|1 (x 0 |x 1 )q(x 1 ).

(4.80)

Thus, we can obtain a pair (X 0 , X 1 ) by (i) drawing X 1 ∼ q, and (ii) applying a predefined randomized transformation to obtain X 0 from X 1 . To satisfy the conditions of corollary 1 (making sure the source is a density) and to encourage diversity, we add noise when sampling from π 0|1 (x 0 |x 1 ). [(Liu et al., 2023;](#b45)[Albergo et al., 2024)](#) demonstrated the capability of this approach on various applications, such as image super-resolution, in-painting, and de-blurring, outperforming methods based on guided diffusion [(Saharia et al., 2022)](#b70).

## Multisample couplings

As discussed in section 4.7, straight probability paths yield ODEs simulations with smaller errors. Therefore, it is natural ask: how could we change the training algorithm, so the learned velocity field induces straight(er) trajectories?

As hinted above, straight trajectories are related to the Optimal Transport (OT) problem. Specifically, consider a convex cost functional c : R d → R ≥0 and the conditional OT flow ψ t (x 0 |x 1 ) = tx 1 + (1 -t)x 0 .

Then, the transport cost of the coupling admits an upper bound on the marginal transport cost [(Liu et al., 2022;](#b46)[Pooladian et al., 2023)](#b64), that is:

$E [c(ψ 1 (X 0 ) -X 0 )] ≤ E [c(X 1 -X 0 )] ,(4.81)$where the case of c(x) = ∥x∥ 2 can be understood from the bound in (4.49) after plugging the OT solution in the l.h.s. that satisfies u t (X t ) = u t (ψ t (x)) = ϕ(x) -x, where ϕ is the OT map. Therefore, one could construct low-cost marginal transport maps by reducing the coupling cost. To this end, [Pooladian et al. (2023)](#b64) propose multisample couplings, a process to implicitly construct non-trivial joints π 0,1 (x 0 , x 1 ) introducing dependencies between source and target distributions:

$1. Sample X (i) 0 ∼ p and X (i) 1 ∼ q, i ∈ [k] independently. 2. Construct π k ∈ B k by π k := arg min π∈B k E π c(X (i) 0 -X (j) 1 ) . 3. Sample a pair (X (i) 0 , X (j) 0 ) uniformly at random from (X (i) 0 , X (j) 1 ) for which π k (i, j) = 1.$where B k is the polytope of k × k doubly stochastic matrices.

The process above implicitly defines a joint distribution π k 0,1 (x 0 , x 1 ) by means of sampling. This implicit joint preserves the marginals and obeys an optimality constraint (step 2) [(Pooladian et al., 2023)](#b64). For k = 1, the method reduces to independent couplings. For k > 1, [Pooladian et al. (2023)](#b64) show that the transport cost is reduced compared to independent couplings, that is,

$E (x0,x1)∼π k 0,1 (x0,x1) [c(x 1 -x 0 )] ≤ E X0∼p,X1∼q [c(X 1 -X 0 )].$Furthermore, for the quadratic cost function, multisample couplings approach the Optimal Transport cost and induces straight trajectories as k → ∞ [(Pooladian et al., 2023;](#b64)[Tong et al., 2023)](#b80).

## Conditional generation and guidance

We now consider training a generative model under a guiding signal to further control the produced samples. This technique has proved valuable in numerous practical applications, such as image-to-image translation [(Saharia et al., 2022](#b70)) and text-to-image generation [(Nichol et al., 2022;](#b56)[Esser et al., 2024)](#b21). In this subsection, we assume access to labeled target samples (x 1 , y), where y ∈ Y ⊆ R k is a label or guidance variable.

## Conditional models

One natural way to train a generative model under guidance is to learn to sample from the conditional distribution q(x 1 |y), as demonstrated by both diffusion and FM models [(Zheng et al., 2023)](#b86). Following the FM blueprint in figure [2](#fig_2), consider samples from the conditional target distribution q(x 1 |y) and prescribe a simple-typically but not necessarily Gaussian-source distribution p. Next, design a guided probability path as the aggregation of conditional probability paths:

$p t|Y (x|y) = p t|1 (x|x 1 )q(x 1 |y)dx 1 . (4.82)$where we assume p t,1|Y (x, x 1 |y) = p t|1 (x|x 1 )q(x 1 |y), meaning that the conditional path does not depend on Y . The resulting guided probability path is conditioned on the guidance variable Y ∼ p Y , and satisfies the marginal endpoints

$p 0|Y (•|y) = p(•), p 1|Y (•|y) = q(•|y). (4.83)$The guided velocity field takes form

$u t (x|y) = u t (x|x 1 )p 1|t,Y (x 1 |x, y)dx 1 ,(4.84)$where, by Bayes' Rule, it follows

$p 1|t,Y (x 1 |x, y) = p t|1 (x|x 1 )q(x 1 |y) p t|Y (x|y) . (4$
## .85)

To show that u t (x|y) generates p t|Y (x|y), plug (4.82) and (4.84) into (4.14), and realize that the FM/CFM losses remain unchanged for the guided case, and enable the same steps appearing in the proof of theorem 4.

In practice, we train a single neural network u θ t : R d × R k → R d to model the guided marginal velocity field for all values of y. Then, the guided version of the CFM loss (4.32) follows the expression

$L CFM (θ) = E t,(X0,X1,Y )∼π 0,1,Y D ψt (X 0 |X 1 ), u θ t (X t |Y ) . (4.86)$In practice, the literature in diffusion models shows that guidance is most effective in applications where a large amount of target samples X 1 share the same guiding signal Y , such as in class guidance (Nichol and Dhariwal, 2021). However, guiding is more challenging in settings where the guidance variable Y is non-repeating and complex, such as image captions.

## Classifier guidance and classifier-free guidance

For flows trained with Gaussian paths, classifier guidance [(Song et al., 2021;](#)[Dhariwal and Nichol, 2021)](#b55) and classifier-free guidance [(Ho and Salimans, 2021](#b31)) can be applied utilizing the transformations between velocity fields and score functions for conditional distributions shown in table 1 [(Zheng et al., 2023)](#b86):

$u t (x|y) = a t x + b t ∇$log p t|Y (x|y). (4.87) Using Bayes' rule over the guided probability path yields p t|Y (x|y) = p Y |t (y|x)p t (x) p Y (y) . (4.88) Taking logarithms and gradients with respect to x, ∇ = ∇ x , we arrive at the fundamental relation between the scores of the probability path p t (x) and its guided counterpart p t|Y (x|y): conditional score ∇ log p t|Y (x|y) = ∇ classifier log p Y |t (y|x) + unconditional score ∇ log p t (x) . (4.89) Namely, the two are related by means of the score of a classifier model p Y |t (y|x) attempting to predict the guidance variable y given a sample x.

Based on this relation, [Song et al. (2021)](#) propose classifier guidance, that is sampling from the conditional model q(x 1 |y) by guiding an unconditional model (parameterized with ∇ log p t (x)) with a time-dependent classifier (predicting the guiding variable y given x ∼ p t (x)). The corresponding velocity field then translates to:

$ũθ,ϕ t (x|y) = a t x + b t ∇ log p ϕ Y |t (y|x) + ∇ log p θ t (x) = u θ t (x) + b t ∇ log p ϕ Y |t (y|x),(4.90)$where u θ t (x) is a velocity field trained on the unconditional target q(x), and log p ϕ Y |t (y|x) is a time-dependent classifier with parameters ϕ ∈ R m . [Dhariwal and Nichol (2021)](#b55) show that this approach outperforms the conditional model from section 4.10.1 for both class-and text-conditioning [(Nichol et al., 2022)](#b56). In practice, because the classifier and the unconditional score are learned separately, it is often necessary to calibrate the classifier guidance as ũθ,ϕ

$t (x|y) = u θ t (x) + b t w∇ log p ϕ Y |t (y|x),(4.91)$where w ∈ R is the classifier scale, typically chosen to be w > 1 (Dhariwal and Nichol, 2021).

In a later work, [(Ho and Salimans, 2021)](#b31) propose a pure generative approach called classifier-free guidance.

By simply re-arranging (4.89), we obtain

$∇ classifier log p Y |t (y|x) = conditional score ∇ log p t|Y (x|y) - unconditional score ∇ log p t (x),(4.92)$revealing that the score of the classifier can be implicitly approximated by the difference between the scores of the vanilla and guided probability paths. Then, the authors propose to learn the conditional and unconditional scores simultaneously using the same model. In terms of velocities, [Zheng et al. (2023)](#b86) show one can also plug 4.92 into 4.91 and use the conversion from scores to velocities as in table 1 to get:

$ũθ t (x|y) = (1 -w)u θ t (x|∅) + wu θ t (x|y),(4.93)$where w is once again the guidance calibration scale. Now, only a single model is trained, u θ t (x|y), where y ∈ {Y, ∅}, ∅ is a place-holder value denoting the null-condition, and u θ t (x|∅) is the velocity field generating the unconditional probability path p t (x). The resulting loss reads:

$L CFM (θ) = E t,ξ,(X0,X1,Y )∼π 0,1,Y D ψt (X 0 |X 1 ), u θ t (X t |(1 -ξ) • Y + ξ • ∅) ,(4.94)$where ξ ∼ Bernoulli(p uncond ), and p uncond is the probability of drawing the null condition ∅ during training.

The exact distribution which CFG samples from is unknown, with some works proposing different intuitive or theoretical justifications for CFG sampling [(Dieleman, 2022;](#b19)[Guo et al., 2024;](#b28)[Chidambaram et al., 2024;](#b14)[Bradley and Nakkiran, 2024)](#b9). Despite this, at the time of writing, CFG is the most popular approach to training a conditional model. [Esser et al. (2024)](#b21); [Polyak et al. (2024)](#b63) show the application of classifier-free guidance to train large-scale guided FM models.

## Non-Euclidean Flow Matching

This section extends Flow Matching from Euclidean spaces R d to general Riemannian manifolds M. Informally, Riemannian manifolds are spaces behaving locally like Euclidean spaces, and are equipped with a generalized notion of distances and angles. Riemannian manifolds are useful to model various types of data. For example, probabilities of natural phenomena on Earth can be modeled on the sphere [Mathieu and Nickel (2020)](#b51), and protein backbones are often parameterized inn terms of matrix Lie groups [Jumper et al. (2021)](#b39). The extension of flows to Riemannian manifolds is due to [Mathieu and Nickel (2020)](#b51); [Lou et al. (2020)](#b48). However, their original training algorithms required expensive ODE simulations. Following Chen and Lipman (2024), the Flow Matching solutions in this section provide a scalable, simulation-free training algorithm to learn generative models on Riemannian manifolds.

## Riemannian manifolds

We consider complete connected, smooth Riemannian manifolds M with a metric g. The tangent space at point x ∈ M, a vector space containing all tangent vectors to M at x, is denoted with T x M. The Riemannian metric g defines an inner product over T x M denoted by ⟨u, v⟩ g , for u, v ∈ T x M. Let T M = ∪ x∈M {x} × T x M be the tangent bundle that collects all the tangent planes of the manifold. In the following, vector fields defined on tangent spaces are important objects to build flows on manifolds with velocity fields. We denote by U = {u t } the space of time-dependent smooth vector fields (VFs) u t : [0, 1] × M → T M, where u t (x) ∈ T x M for all x ∈ M. Also, div g (u t ) is the Riemannian divergence with respect to the spatial (x) argument. Finally, we denote by dvol x the volume element over M, and integration of a function f : M → R over M is denoted f (x)dvol x .

## Probabilities, flows and velocities on manifolds

Probability density functions over a manifold M are continuous non-negative functions p : M → R ≥0 integrating to 1, namely M p(x)dvol x = 1. We define a probability path in time p t as a time-dependent curve in probability space P, namely p t : [0, 1] → P. A time-dependent flow, ψ : [0, 1] × M → M, similar to the Euclidean space, defines a global diffeomorphism on M for every t.

Remarkably, constructing flow-based models via velocity fields naturally applies to general Riemannian manifolds. Formally, and rephrasing Proposition 1 from Mathieu and Nickel (2020):

Theorem 8 (Flow local existence and uniqueness). Let M a smooth complete manifold and a velocity field

$u t ∈ U. If u is C ∞ ([0, 1] × M, T M) (in particular, locally Lipschitz), then the ODE in (3.19) has a unique solution which is a C ∞ (Ω, M) diffeomorphism ψ t (x) defined over the open set Ω ⊃ {0} × M.$Similar to theorem 1, flow ODEs generally only define a local diffeomorphism on the manifold, meaning that ψ t (x) may be defined on a maximal interval in time [0, t x ]) for different values of x ∈ M. Similar to the Euclidean case we will work with the semi-open time interval t ∈ [0, 1) to allow q to have compact support (for which u t is not everywhere defined). To ensure existence for the desired time interval, [0, 1), we add the integrability constraint (see theorem 2) and rely on the Mass Conservation theorem once again. For a Riemannian manifold with metric g, the Riemannian continuity equation reads

$d dt p t (x) + div g (p t u t )(x) = 0, (5.1)$and the corresponding Manifold Mass Conservation theorem [(Villani et al., 2009)](#) is stated as follows.

Theorem 9 (Manifold Mass Conservation). Let p t be a probability path and u t ∈ U a locally Lipchitz integrable vector field over a Riemannian manifold M with metric g. Then the following are equivalent 1. The Continuity Equation (5.1) holds for t ∈ [0, 1).

2. u t generates p t in the sense of 3.24.

where, by Bayes' Rule for PDFs, we obtain

$p 1|t (x 1 |x) = p t|1 (x|x 1 )q(x 1 ) p t (x) , (5.11)$which is defined for all x ∈ M for which p t (x) > 0.

The Marginalization Trick (theorem 3) for the Riemannian case requires adjusting assumption 1 as follows:

$Assumption 2. p t|1 (x|x 1 ) is C ∞ ([0, 1) × M) and u t (x|x 1 ) is C ∞ ([0, 1) × M, M$) as function of (t, x). Furthermore, we assume either q has bounded support, i.e., q(x 1 ) = 0 outside some bounded set or M is compact; and p t (x) > 0 for all x ∈ M and t ∈ [0, 1).

We are now ready to state the Manifold Marginalization Trick theorem: Theorem 10 (Manifold Marginalization Trick). Under Assumption 2, if u t (x|x 1 ) is conditionally integrable and generates the conditional probability path p t (•|x 1 ) then the marginal velocity field u t (•) generates the marginal probability path p t (•).

By conditionally integrable, we mean a conditioned version of the integrability condition from the Mass Conservation Theorem (5.2):

$1 0 M M ∥u t (x|x 1 )∥ g p t|1 (x|x 1 )q(x 1 )dvol x1 dvol x dt < ∞ (5.12)$The proof of theorem 10 is repeating the arguments of theorem 3 and is given in appendix A.2.

## Riemannian Flow Matching loss

The Riemannian Conditional Flow Matching (RCFM) loss reads

$L RCFM (θ) = E t,X1,Xt∼p t|1 (•|X1) D Xt u t (X t |X 1 ), u θ t (X t ) .$(5.13)

Once again, we have the equivalence:

Theorem 11. The gradients of the Riemannian Flow Matching loss and the Riemannian Conditional Flow Matching loss coincide:

$∇ θ L RFM (θ) = ∇ θ L RCFM (θ).$(5.14)

The above theorem can be proved using proposition 1 with X = X t , Y = u t (X t |X 1 ), g θ (x) = u θ t (x), and integrating w.r.t. t ∈ [0, 1].

## Conditional flows through premetrics

Having established how to learn a flow model with the RCFM loss, we are left with specifying the conditional probability path and its generating velocity field. Similar to section 4.6, we begin by stating the requirements for the corresponding conditional flow ψ : [0, 1) × M × M → M, such that p t|1 (•|x 1 ) satisfies the boundary conditions (5.8). The conditional flow model is

$X t|1 = ψ t (X 0 |x 1 ), where X 0 ∼ π 0|1 (•|x 1 ), (5.15)$where the conditional flow is

$ψ t (x|x 1 ) = x t = 0 x 1 t = 1$, is smooth in t, x and diffeomorphism in x on M.

(5.16)

Our analysis in Euclidean space focused on affine conditional flows, as these served as a rich class of easily computable (simulation-free) conditional flows. Unfortunately, combinations α t x 1 + σ t x 0 for α t + σ t ̸ = 1 are not naturally defined on manifolds. The manifold analog for the case α t + σ t = 1 would be using geodesic interpolation. Indeed, Chen and Lipman (2024) proposed building conditional flows by moving along geodesic curves, in particular, generalizing the conditional OT paths moving along straight lines in Euclidean space (see theorem 5). Geodesics represent the shortest paths between two points on a manifold, reducing to straight lines in Euclidean spaces. For manifolds, we define the geodesic conditional flow as

$ψ t (x 0 |x 1 ) = exp x0 (κ(t) log x0 (x 1 )), t ∈ [0, 1],$(5.17)

where κ(t) : [0, 1] → [0, 1] is a monotonically increasing scheduler satisfying κ(0) = 0 and κ(1) = 1, making sure all x 0 are pushed to x 1 at t = 1. The exponential map, evaluated at x ∈ M, exp x : T x M → M, v → exp x (v), returns the endpoint at time t = 1 of the unique geodesic starting at x with initial speed v. The logarithmic map log x : M → T x M , y → log x (y), is the inverse of the exponential map. In Euclidean space, the exponential map is simply vector addition, and the logarithmic map is vector subtraction. Now, if we plug these in (5.17), we get ψ t (x 0 |x 1 ) = x 0 + κ(t)(x 1 -x 0 ), and by choosing κ(t) = t we recover the conditional OT flow.

For simple manifolds with closed-form exponential and logarithmic maps, this construction allows a simulationfree recipe for training flows on manifolds, an arguably clear advantage compared to diffusion models approaches built on manifolds [(De Bortoli et al., 2022;](#)[Huang et al., 2022b;](#)[Lou et al., 2023)](#b49). In particular, manifold diffusion models require in-training simulation to sample from p t , and have to resort to approximations of the score function on the manifold.

Nevertheless, while building geodesic conditional flows is a natural construction, geodesics may be hard to compute for general manifolds that do not have closed-form exponential and logarithmic maps and/or introduce undesired bias such as concentrating probability at boundary points. To overcome the difficulty in computing geodesics and/or inject a desired implicit bias, one may seek an alternative notion of smooth distance function, d(•, •) : M × M → R ≥0 , and require that the conditional flow satisfies

$d(ψ t (x 0 |x 1 ), x 1 ) = κ(t)d(x 0 , x 1 ),(5.18)$where κ(t) = 1 -κ(t). This will assure that the conditional flow concentrates all the probability at x 1 at time t = 1 if the following conditions hold:

1. Non-negative: d(x, y) ≥ 0 for all x, y ∈ M.

2. Positive: d(x, y) = 0 if and only if x = y.

3. Non-degenerate: ∇d(x, y) ̸ = 0 if and only if x ̸ = y.

Chen and Lipman (2024) showed that the minimal norm conditional velocity field corresponding to a flow that satisfies (5.18) has the form:

$u t (x|x 1 ) = d log κ(t) dt d(x, x 1 ) ∇d(x, x 1 ) ∥∇d(x, x 1 )∥ 2 g , (5$
## .19)

Figure [12](#fig_7) Conditional flows on the manifold M.

where the non-degeneracy requirement of the premetric ensures that the velocity field has no discontinuities, since u t (x|x 1 ) ∝ 1/∥∇d(x, x 1 )∥ g . In particular, note that the geodesic conditional flow in (5.17) satisfies (5.18) for the choice d = d g , where d g is the geodesic distance. An example of a choice of alternative premetrics is using spectral distances on general geometries (Chen and Lipman, 2024), where the conditional velocity offers a way to sample from p t (x|x 1 ) by simulation. Importantly, although conditional flows with premetrics require in-training simulation-like diffusion models on manifolds-the velocity field can still be accurately recovered compared to approximations of the score function.

Another issue, is that both conditional flows defined via geodesic interpolation and premetric can suffer from singularities, e.g., for compact manifolds. For example on the 2-sphere the geodesic function d(x, x 1 ) is not differentiable at the antipodal point x = -x 1 . Furthermore, any smooth function such as x → d(x, x 1 ) will showcase at-least two critical points (maximum and minimum) where the velocity in (5.19) is not-defined. However, the set of such problematic points is generally very small (in fact of zero volume usually). Therefore, this issue does not cause problems in practice, at-least in use cases we are aware of.

In any case, to deal with this issue, we can include an augmented scheduler in the geodesic conditional flow. That is, use κ(t, x, x 1 ), that depends also on x, x 1 to make (5.17) globally smooth. To deal with the zero gradient issue of the premetric conditional flow we can relax the non-degeneracy requirement as follows:

3. Non-degenerate (relaxed): The volume of the set A y = {x ∈ M | ∇d(x, y) = 0 and x ̸ = y} is 0 for all y ∈ M. This section presents the Continuous Time Markov Chains (CTMCs) as an alternative generative model to flow, with the use-case of generating discrete data, i.e., data residing in a discrete (and finite) state space. CTMC are Markov processes that form the building blocks behind the generative model paradigm of Discrete Flow Matching (DFM) Campbell et al. (2024); Gat et al. (2024), later discussed in section 7. Therefore, this section is analogous to section 3, where we presented flows as the building blocks behind the generative model paradigm of Flow Matching (FM). 6.1 Discrete state spaces and random variables Consider a finite version of R d as our state space S = T d , where T = [K] = {1, 2, . . . , K}, sometimes called vocabulary. Samples and states are denoted by x = (x 1 , . . . , x d ) ∈ S, where x i ∈ T is single coordinate or a token. We will similarly use states y, z ∈ S. Next, X denotes a random variable taking values in the state space S, with probabilities governed by the probability mass function (PMF) p X : S → R ≥0 , such that x∈S p X (x) = 1, and the probability of an event A ⊂ S being

$P(X ∈ A) = x∈A p X (x). (6.1)$The notations X ∼ p X or X ∼ p X (X) indicate that X has the PMF p X . The δ PMF in the discrete case is defined by

$δ(x, z) = 1 x = z, 0 else. (6.2)$where we sometimes also define δ PMFs on tokens, such as in δ(x i , y i ), for some x i , y i ∈ T .

## The CTMC generative model

The CTMC model is an S-valued time-dependent family of random variables (X t ) 0≤t≤1 that a form a Markov chain characterized by the probability transition kernel p t+h|t defined via p t+h|t (y|x) := P(X t+h = y|X t = x) = δ(y, x) + hu t (y, x) + o(h), and P(X 0 = x) = p(x), (6.3)

Figure [13](#fig_7) The CTMC model is defined by prescribing rates (velocities) of probability between states.

where the PMF p indicates the initial distribution of the process at time t = 0, and o(h) is an arbitrary function satisfying o(h)/h → 0 as t → 0.

The values u t (y, x), called rates or velocities, indicate the speed at which the probability transitions between states as a function of time. By fully characterized, we mean that all the joints P(X t1 = x 1 , . . . , X tn = x n ), for arbitrary

$0 ≤ t 1 < • • • < t n ≤ 1 and x i ∈ S, i ∈ [n],$are defined this way.

To make sure the transition probabilities p t+h|t (y|x) are defined via (6.3), velocities needs to satisfy the following rate conditions: u t (y, x) ≥ 0 for all y ̸ = x, and y u t (y, x) = 0. (6.4)

If one of these conditions were to fail, then the transition probabilities p t+h|t (•|x) would become negative or sum to c ̸ = 1 for arbitrary small h > 0. Equation (6.3) plays he same role as equation [(3.16](#)) and equation [(3.19)](#) when we were defining the flow generative modeling. The marginal probability of the process X t is denoted by the PMF p t (x) for time t ∈ [0, 1]. Then, similarly to equation [(3.24)](#) for the case of flows, we say that u t generates p t if there exists p t+h|t satisfying (6.3) with marginals p t .

(6.5)

Simulating CTMC. To sample X t , sample X 0 ∼ p and take steps using the (naive) Euler method:

P(X t+h = y | X t ) = δ(y, X t ) + hu t (y, X t ). (6.6)

According to (6.3), these steps introduce o(h) errors to the update probabilities. In practice, this means that we would need a sufficiently small h > 0 to ensure that the right-hand side in (6.6) remains a valid PMF. One possible remedy to assure that any choice of h > 0 results in a valid PMF, and maintains the o(h) local error in probabilities is the following Euler method:

$P(X t+h = y | X t ) = exp [hu t (X t , X t )] y = X t ut(y,Xt) |ut(Xt,Xt)| (1 -exp [hu t (X t , X t )]) y ̸ = X t . (6$.7) 6.3 Probability paths and Kolmogorov Equation Similarly to Continuity Equation in the continuous case, the marginal probabilities p t of the CTMC model (X t ) 0≤t≤1 are characterized by the Kolmogorov Equation d dt p t (y) = x u t (y, x)p t (x). (6.8)

The following classical theorem (see also Theorems 5. [1 and 5.2 in Coddington et al. (1956)](#)) describes the existence of unique solutions for this linear homogeneous system of ODEs.

Theorem 12 (Linear ODE existence and uniqueness). If u t (y, x) are in C([0, 1)) (continuous with respect to time), then there exists a unique solution p t (x) to the Kolmogorov Equation (6.8), for t ∈ [0, 1) and satisfying p 0 (x) = p(x).

For the CTMC, the solution is guaranteed to exist for all times t ∈ [0, 1) and no extra conditions are required (unlike the non-linear case in theorem 1). The Kolmogorov Equation has an intimate connection with the Continuity Equation [(3.25)](#). Rearranging the right-hand side of (6.8) by means of the rate conditions yields where j t (y, x) := u t (y, x)p t (x) is the probability flux describing the probability of moving from state x to state y per unit of time. The excess of outgoing flux is defined as the divergence, giving the Kolmogorov Equation the same structure as the one described in section 3.5 for the Continuity Equation [(Gat et al., 2024)](#b25).

The following result is the main tool to build probability paths and velocities in the CTMC framework:

Theorem 13 (Discrete Mass Conservation). Let u t (y, x) be in C([0, 1)) and p t (x) a PMF in C 1 ([0, 1)) in time t. Then, the following are equivalent:

1. p t , u t satisfy the Kolmogorov Equation (6.8) for t ∈ [0, 1), and u t satisfies the rate conditions (6.4).

2. u t generates p t in the sense of 6.5 for t ∈ [0, 1).

The proof of theorem 13 is given in appendix A.1.

## Probability preserving velocities

As a consequence of the Discrete Mass Conservation (theorem 13), if velocity u t (y, x) generates the probability path p t (x), then ũt (y, x) = u t (y, x) + v t (y, x) generates p t (x), (6.9) as long as v t (y, x) satisfies the rate conditions (6.4) and solves the divergence-free velocity equation x v t (y, x)p t (x) = 0. (6.10)

In fact, ũt (y, x) solves the Kolmogorov Equation [](#)x ũt (y, x)p t (x) =

x u t (y, x)p t (x) = ṗt (y),

showing that one may add divergence-free velocities during sampling without changing the marginal probability. This will be a useful fact when sampling from discrete Flow Matching models, described next.

## Discrete Flow Matching

Remarkably, the Flow Matching blueprint in figure [2](#fig_2) carries out seamlessly from the continuous case to the discrete case, yielding the Discrete Flow Matching (DFM) framework [(Campbell et al., 2024;](#b11)[Gat et al., 2024)](#b25).

In analogy to the continuous case, start by defining a probability path p t interpolating between a source PMF p and a target PMF q. Second, we would like to find a CTMC model (X t ) 0≤t≤1 , defined by a learnable velocity u θ t , that generates the probability path p t . Finally, we train u θ t by minimizing a Bregman divergence that defines the Discrete Flow Matching loss. In sum, this is to solve the discrete version of the Flow Matching problem (4.1).

## Data and coupling

Our goal is to transfer samples X 0 ∼ p from a source PMF p to samples X 1 ∈ q from a target PMF q, where X 0 , X 1 ∈ S are two RVs each taking values in the state space S. Source and target samples can be related by means of the independent coupling (X 0 , X 1 ) ∼ p(X 0 )q(X 1 ), or associate by means of a general PMF coupling π 0,1 (x 0 , x 1 ). For example, text translation data considers coupled data (x 0 , x 1 ) representing the same document written in two different languages. Another application, such as text generation, concerns independent pairing where p(x 0 ) is either the uniform probability over S giving all states equal probability, or adding a special token m to the vocabulary T , i.e., T ∪ {m}, and considering π 0,1 (x 0 , x 1 ) = δ(x 0 , m)q(x 1 ). Any RV X 0 ∼ δ(X 0 , m) is the constant RV X 0 = (m, . . . , m).

## Discrete probability paths

The next step in the FM recipe is, as usual, to prescribe a probability path p t interpolating p and q. Following section 4.4, we condition these objects on a general conditioning RV Z ∼ p Z taking values in some arbitrary space Z. The marginal probability path takes form

$p t (x) = z∈Z p t|Z (x|z)p Z (z), (7.1)$where p t|Z (•|z) is a conditional PMF, and the marginal probability path satisfies the boundary constraints p 0 = p and p 1 = q.

## The Marginalization Trick

The Marginalization Trick (see section 4.4) transfers to the discrete case as-is [(Campbell et al., 2024;](#b11)[Gat et al., 2024)](#b25). Assuming that the conditional velocity field u t (•, •|z) generates p t (•|z) in the sense of (6.5), we obtain the marginal velocity field

$u t (y, x) = z u t (y, x|z)p Z|t (z|x) = E [u t (y, X t |Z) | X t = x] , (7.2)$defined for all x, y ∈ S where p t (x) > 0, and RV X t ∼ p t|Z (•|Z). By using Bayes' rule, we get

$p Z|t (z|x) = p t|Z (x|z)p Z (z) p t (x) . (7.3)$To prove the discrete version of the Marginalization Trick Theorem (theorem 3), assume: [)](#)), and p t (x) > 0 for all x ∈ S and t ∈ [0, 1).

$Assumption 3. p t|Z (x|z) ∈ C 1 ([0, 1)), u t (y, x|z) ∈ C([0,1$As it happened in the continuous case, the assumption p t > 0 is in practice mild, because we can always use

$(1 -(1 -t)ϵ) • p Z|t + (1 -t)ϵ • p uni$, where p uni is the uniform distribution over S, and ϵ > 0 is arbitrary small. We are no ready to state and prove the result.

Theorem 14 (Discrete Marginalization Trick). Under Assumption 3, if u t (y, x|z) generates p t|Z (x|z) then the marginal velocity u t (y, x) in (7.2) generates p t (x) in (7.1) for t ∈ [0, 1).

Proof. The proof is conceptually similar to the continuous case. Start by computing:

$d dt p t (y) = z d dt p t|Z (y|z)p Z (z) (i) = z x u t (y, x|z)p t|Z (x|z) p Z (z) (ii) = x z u t (y, x|z) p t|Z (x|z)p Z (z) p t (x) p t (x) (Bayes) = x ut(y,x) z u t (y, x|z)p Z|t (z|x) p t (x),$Equality (i) follows from theorem 13 and the fact that u t (y, x|z) generates p t|Z (y|z). Equality (ii) follows from multiplying and dividing by p t (x) which is assumed positive. Therefore, u t (y, x) satisfies the Kolmogorov Equation with p t . Also, u t (y, x) satisfies the rate conditions (6.4), because each u t (y, x|z) satisfies them. Lastly, u t (y, x) ∈ C([0, 1)) because both u t (y, x|z) and p Z|t (z|x) are in C([0, 1)). In particular, p Z|t (z|x) ∈ C([0, 1)) follows from assuming p t (x) > 0 for t ∈ [0, 1). By theorem 13, and because u t (x, y) satisfies the Kolmogorov Equation with p t and the rate conditions, it generates p t in the sense of (6.5).

## Discrete Flow Matching loss

To construct a CTMC generative model (X t ) 0≤t≤1 we parameterize a velocity field u θ t (y, x) with parameters θ, e.g., using a neural network. One would construct the neural network to satisfy the rate conditions equation (6.4). The Discrete Flow Matching loss to train the CTMC model is defined as:

$L DFM (θ) = E t,Xt∼pt D Xt (u t (•, X t ), u θ t (•, X t )), (7.4)$for t ∼ U [0, 1] and u t (•, x) ∈ R S satisfying the rate conditions. This means that u t (•, x) ∈ Ω x , where

$Ω x =    v ∈ R S v(y) ≥ 0 ∀y ̸ = x, and v(x) = - y̸ =x v(y)    ⊂ R S , (7.5)$is a convex set, and D x (u, v) is a Bregman divergence defined using a convex function Φ x : Ω x → R. The Conditional Discrete Flow Matching loss takes form

$L CDFM (θ) = E t,Z,Xt∼p t|Z D Xt (u t (•, X t |Z), u θ t (•, X t )). (7.6)$Once again, the two losses (7.4) and (7.6) both provide the same learning gradients.

Theorem 15. The gradients of the Discrete Flow Matching loss and the Conditional Discrete Flow Matching loss coincide:

$∇ θ L DFM (θ) = ∇ θ L CDFM (θ). (7.7)$In particular, the minimizer of the Conditional Discrete Matching loss is the marginal velocity

$u θ t (y, x) = E [u t (y, X t |Z) | X t = x] . (7.8)$The proof follows by applying proposition 1 when setting X = X t , Y = (X t , Z), defining f : S 2 → R S as (x, z) → u t (•, x|z) ∈ R S , and integrating with respect to t ∈ [0, 1].

## Factorized paths and velocities

Figure [14](#fig_15) Factorized CTMC model allows non-zero rates (velocities) only between states that differ in at-most one coordinate (token).

If implementing DFM as presented, we would require a learnable model u θ t (y, x)-for instance, a neural network-that outputs a rate for all possible states y ∈ S = T d . This would result in a huge output dimension K d , infeasible for common sequence lengths d and vocabulary sizes K. One remedy to this issue is to consider factorized velocities [(Campbell et al., 2022)](#b10), [(7.9)](#) where ī = (1, . . . , i -1, i + 1, . . . , d) denotes all indices excluding i. Therefore, the factorized velocity above connects state x to state y only if these differ in at most one single token. When using factorized velocities, we only require to model u i t (y i , x), as these fully define u t (y, x). In turn, each u i t (y i , x) is a learnable model accepting x ∈ S and returning a scalar

$u t (y, x) = i δ(y ī, x ī)u i t (y i , x),$$u i t (y i , x) ∈ R, for all i ∈ [d] = {1, 2, .$. . , d} and y i ∈ T . Therefore, the output of the model has a tractable dimension d • K. The rate conditions for factorized velocities u i t (y, x) are now required per dimension i ∈ [d]:

$u i t (y i , x$) ≥ 0 for all y i ̸ = x i , and

$y i ∈T u i t (y i , x) = 0 for all x ∈ S.$(7.10)

## Simulating CTMC with factorized velocities

When using factorized velocities, we can sample CTMC models coordinate-wise [(Campbell et al., 2024)](#b11):

$P(X t+h = y | X t = x) = δ(y, x) + h i δ(y ī, x ī)u i t (y i , x) + o(h) = i δ(y i , x i ) + hu i t (y i , x) + o(h) ,$where the second equality follows from δ(y, x) = i δ(y i , x i ) and the identity

$i a i + hb i = i a i + h i ( j̸ =i a j )b i + o(h).$Therefore, and up to an o(h) order, the transition kernel factorizes to coordinate-wise independent transitions

$P(X i t+h = y i | X t = x) = δ(y i , x i ) + hu i (y i , x) + o(h). (7.11)$These can be sampled with the Euler method (6.7) per coordinate. Interestingly, continuous Flow Matching also enjoys a similar factorization u t (x) = [u 1 t (x), . . . , u d t (x)] ∈ R d , where Ẋi t (x) = u i t (X t ) determines the change for coordinate i, and can be sampled independently (the "samples" in continuous FM are just deterministic).

## Building probability paths with factorized velocities

If we construct probability paths in a certain way, it turns out to have factorized velocities (equation (7.9)) by construction. We explain this construction next. For this, we define a factorized probability path as a probability path of the form: q t (x) = i q i t (x i ).

(7.12)

Then, the following result shows that these factorized probability paths have factorized velocities.

Proposition 2. Let q t (x) be a factorized probability path as in (7.12), where u i t (y i , x i ) ∈ C([0, 1)) generates q i t (x i ). Then q t has a factorized generating velocity of the form

$u t (y, x) = i δ(y ī, x ī)u i t (y i , x i ). (7.13)$To proceed with the proof, let us denote the marginal distributions of a PMF q(x) by

$q i (x i ) := x ī q(x) q ī(x ī) := x i q(x) (7.14)$Proof. Let q t be a factorized probability path (7.12). Let u i t (y i , x i ) be the generating velocity of q i t (x i ). Differentiating with respect to t yields

$d dt q t (y) = i q ī t (y ī) d dt q i t (y i ) (i) = i   x ī δ(y ī, x ī)q ī t (x ī)   x i u i t (y i , x i )q i t (x i ) (ii) = x i δ(y ī, x ī)u i t (y i , x i ) q t (x),$Equality (i) follows from q ī t (y ī) = x ī δ(y ī, x ī)q ī t (x ī) and the Kolmogorov Equation (6.8). Equality (ii) follows from changing the summation order and noting that, by the definition of q t , we have q ī t (x ī)q i t (x i ) = q t (x) and

x ī

x i = x .

We are now ready to show the main tool for constructing paths p t with factorized velocities that interpolate between arbitrary p and q [(Campbell et al., 2024;](#b11)[Gat et al., 2024)](#b25).

Theorem 16 (Discrete Factorized Marginalization Trick). Consider a marginal probability path constructed via [(7.15)](#) i.e., where the conditional path factorizes in the sense of equation (7.12). Further, assume that [)](#)), and p t (x) > 0 for all x ∈ S and t ∈ [0, 1). Then, the marginal velocity is

$p t (x) = z p t|Z (x|z)p Z (z), with p t|Z (x|z) = i p i t|Z (x i |z),$$u i t (y i , x i |z) is C([0, 1)) generates p i t|Z (x i |z) in C 1 ([0,1$$u t (y, x) = i δ(y ī, x ī)u i t (y i , x) (7.16) with u i t (y i , x) = z u i t (y i , x i |z)p Z|t (z|x) = E u i t (y i , X i t |Z)|X t = x (7.17) generates p t (x).$Proof. According to proposition 2, the factorized conditional paths p t|Z (x|z) have factorized generating velocities u t (y, x|z) = i δ(y ī, x ī)u i t (y i , x i |z). Therefore,

$u t (y, x) (i) = z u t (y, x|z)p Z|t (z|x) (ii) = z i δ(y ī, x ī)u i t (y i , x i |z) p Z|t (x|z) (iii) = i δ(y ī, x ī) z u i t (y i , x i |z)p Z|t (z|x) .$Equality (i) follows from (7.2). Equality (ii) follows from assuming that p t|Z has factorized velocity. Equality (iii) follows from changing summation order. Because [)](#)). Therefore, theorem 14 implies that u t (y, x) generates p t (x), as required.

$p i t|Z (x i |z) ∈ C 1 ([0, 1)) and p t (x) > 0, it follows that p t|Z (x|z) ∈ C 1 ([0, 1)). Similarly, because u i t (y i , x i |z) ∈ C([0, 1)), it follows that u t (y, x|z) ∈ C([0,1$By using theorem 16, we can design a probability path p t with factorized velocities interpolating between a source PMF p and a target PMF q as follows.

1. Find factorized probability conditional paths p t|Z (x|z) = i p i t|Z (x i |z) such that the marginal p t (x) satisfies p 0 = p and p 1 = q.

2. Find generating velocities u i t (y i , x i |z) to p i t|Z (x i |z). This can be done by finding solution u i t (y i , x i |z) to the Kolmogorov Equation:

$x i u i t (y i , x i |z)p i t|Z (x i |z) = d dt p i t|Z (y i |z),(7.18)$for all y i ∈ T , fixed values of i ∈ [d], z ∈ Z, and t ∈ [0, 1). As a remark, (7.18) is an under-determined linear system of equations with |T | unknowns (significantly less unknowns than the entire state space |S|).

## Conditional Discrete Flow Matching loss for factorized velocities

Representing the marginal velocity u θ t in terms of factorized velocities u θ,i t enables the following Conditional Flow Matching loss

$L CDFM (θ) = E t,Z,Xt∼p t|Z i D i Xt u i t (•, X t |Z), u θ,i t (•, X t ) ,(7.19)$where t ∼ U [0, 1], and u i t (•, x|z), u θ,i t (•, x) ∈ R T satisfy the rate conditions. This means that u i t (•, x|z), u θ,i t (•, x) ∈ Ω x i where, for α ∈ T , we define

$Ω α =    v ∈ R T v(β) ≥ 0 ∀β ∈ T \ {α} , and v(α) = - β̸ =α v(β)    ⊂ R T . (7.20)$This is a convex set, and

$D i x (u, v) is a Bregman divergence defined by a convex function Φ i x : Ω x i → R.$As before, we justify this loss using proposition 1 and setting

$X = X t , Y = u i t (•, X t , Z) ∈ R T , letting D i x (u, v) be a Bregman divergence over Ω x i ⊂ R T ,$and integrating with respect to t ∈ [0, 1].

## Mixture paths

It is time to implement section 7.5.2 to build practical probability paths and their corresponding conditional velocities. Following [Gat et al. (2024)](#b25), we condition on Z = (X 0 , X 1 ) to accommodate arbitrary data couplings (X 0 , X 1 ) ∼ π 0,1 (X 0 , X 1 ). Then, we build the factorized conditional paths

$p t|0,1 (x|x 0 , x 1 ) = i p i t|0,1 (x i |x 0 , x 1 ) (7.21)$as mixtures

$p i t|0,1 (x i |x 0 , x 1 ) = κ t δ(x i , x i 1 ) + (1 -κ t )δ(x i , x i 0 ),(7.22)$where κ :

$[0, 1] → [0, 1] is a C 1 ([0, 1]) scheduler. Note that a RV X i t ∼ p i t|0,1 (•|x 0 , x 1 ) follows X i t = x i 1 with prob κ t x i 0 with prob (1 -κ t ) ,(7.23)$i.e. it assumes either the source or the target states with a probability depending on the time t.

If κ 0 = 0 and κ 1 = 1, then the marginal p t (x) in (7.1) satisfies the boundary constraints. We also need generating velocities u i t (y i , x i |x 0 , x 1 ) for p i t|0,1 (x i |x 0 , x 1 ), which are solutions to (7.18). We derive these as follows:

$d dt p i t|Z (y i |z) (7.22) = κt δ(y i , x i 1 ) -δ(y i , x i 0 ) (7.22) = κt δ(y i , x i 1 ) - p i t|Z (y i |z) -κ t δ(y i , x i 1 ) 1 -κ t = κt 1 -κ t δ(y i , x i 1 ) -p i t|Z (y i |z) = x i κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) p i t|Z (x i |z),$where we have used z = (x 0 , x 1 ) and Z = (X 0 , X 1 ) interchangeably to keep notation concise. In conclusion, we have found a conditional velocity generating the path in (7.22), namely

$u i t (y i , x i |x 0 , x 1 ) = κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) .(7.24)$Code 9 shows how mixture paths are defined in the library flow_matching library.  Velocity posterior parameterization. Similar to the continuous case (e.g., section 4.8.1), we can choose to parameterize our velocity u i t (y i , x) in different ways. The first approach is to parameterize it directly, akin to velocities in flows. Another way, which we take here, is motivated by the following computation of the mixture marginal velocity following (7.17):

$) 14 sample.x_0 # X 0 is [0, 0] 15 sample.x_1 # X 1 is [1, 2]$$u i t (y i , x) = x0,x1 κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) p 0,1|t (x 0 , x 1 |x) = x i 1 κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) p i 1|t (x i 1 |x),(7.25)$where for the second equality we denote the marginal of the posterior p 0,1|t

$p i 1|t (x i 1 |x) = x0,x ī 1 p 0,1|t (x 0 , x 1 |x) (7.26) = E δ(x i 1 , X i 1 ) | X t = x . (7.27)$This derivation represents the marginal u i t (y i , x) using a learnable posterior p θ,i 1|t (x i 1 |x), which can be understood as a discrete version of x 1 -prediction (section 4.8.1). Next, we explore loss functions to learn this posterior.

CDFM losses for mixture paths We present two options for learning p θ,i 1|t (x i 1 |x), both justified by proposition 1. First, the marginal posterior (7.26) and (7.27) can be learned by the conditional matching loss

$L CM (θ) = E t,X0,X1,Xt D Xt δ(•, X i 1 ), p θ,i 1|t (•|X t ) (7.28) Since δ(•, X i 1 ), p θ,i 1|t (•|X t )$are PMFs. Therefore, we can set the Bregman divergence to be the KL-divergence D(p, q) = α∈T p(α) log p(α) q(α) comparing PMFs, obtaining

$L CM (θ) = -E t,X0,X1,Xt log p θ,i 1|t (X i 1 |X t ) + const. (7.29)$Alternatively, we may follow section 7.5.3 and use the factorized loss in (7.19) with u θ,i t parametrized by p θ,i 1|t . In this case, we can set the Bregman divergence to be the generalized KL comparing general (not necessarily probability) vectors u, v ∈ R m ≥0 : [(7.30)](#) For this choice of D, we get

$D(u, v) = j u j log u j v j - j u j + j v j .$$D u i t (•, x i |x 0 , x 1 ), u θ,i t (•, x) = κt 1 -κ t (δ(x i 1 , x i ) -1) log p θ,i 1|t (x i 1 |x) + δ(x i 1 , x i ) -p θ,i 1|t (x i |x) (7.31)$which implements the loss (7.19) when conditioning on Z = (X 0 , X 1 ). The generalized KL loss (7.31) also provides an evidence lower bound (ELBO) on the likelihood of the target distribution [(Shaul et al., 2024)](#b75),

$-log p θ 1 (x 1 ) ≤ E t,X0,Xt∼p t|0,1 i D u i t (•, X i t |X 0 , x 1 ), u θ,i t (•, X t ) ,(7.32)$where p θ 1 is the marginal generated by the model at time t = 1. Hence, in addition to training, the generalized KL loss is commonly used for evaluation.

Sampling mixture paths The parametrization based on the posterior p θ,i 1|t leads to the following sampling algorithm. As indicated in section 7.5.1 working with factorized velocities enables the coordinate-wise sampling (7.11). According to (7.17) and (7.25),

$P(X i t+h = y i | X t = x) = δ(y i , x i ) + hu i (y i , x) + o(h) (7.33) = x i 1 δ(y i , x i ) + h κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) + o(h) p i 1|t (x i 1 |x). (7.34)$Consequently, and given X t = x, we may perform one step of sampling by performing the next two steps

$for each i ∈ [d]: (i) draw X i 1 ∼ p i 1|t (X i1$|x); and (ii) update X i t+h according to the Euler step in (6.7) with the velocity κt 1-κt δ(y i , X i 1 ) -δ(y i , x i ) . Intuitively, (ii) decides whether to set X i t+h = X i 1 or remain at

$X i t+h = X i t .$One-sided mixture paths and probability preserving velocities It is often useful to extend the design space of the sampling algorithm by adding some divergence-free component, as described in section 6.3.1. For factorized paths, the divergence-free velocity v i t needs to satisfy (7.18), namely,

$x i v i t (y i , x i |z)p i t|Z (x i |z) = 0. (7.35)$In general it could challenging to find such probability-preserving velocities without learning additional quantities, e.g., p i 0|t . However, one useful case where a probability preserving velocity can be found in closed form is when assuming iid source distribution i.e., p(x) = i p(x i ) and independent coupling π 0,1 (x 0 , x 1 ) = p(x 0 )q(x 1 ). In this case the marginal mixture path takes the form

$p t (x) = x1 p t|1 (x|x 1 )q(x 1 ), where p t|1 (x|x 1 ) = i p i t|1 (x i |x 1 )$,

$where p i t|1 (x i |x 1 ) = κ t δ(x i , x i 1 ) + (1 -κ t )p(x i$) . The conditional velocity in (7.24), i.e.,

$u i t (y i , x i |x 1 ) = κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) (7.36)$also generates p i t|1 (x i |x 1 ). To find a divergence-free velocity we can, for example, subtract from this velocity a backward-time velocity ũi t for p i t|1 (y i , x i |x 1 ) [(Gat et al., 2024)](#b25), in the sense that it satisfies the Kolmogorov equation with p i t|1 (y i , x i |x 1 ) and -ũ i t satisfies the rate conditions. Such a velocity can be found in a fashion to equation (7.24),

$ũi t (y i , x i |x 1 ) = κt κ t δ(y i , x i ) -p(x i ) . (7.37)$Therefore, a divergence-free velocity for p i t|1 (x i |x 1 ) conditional path can be defined via

$v i t (y i , x i |x 1 ) = u i t (y i , x i |x 1 ) -ũi t (y i , x i |x 1 ). (7.38)$According to section 6.3.1, if we add a divergence-free field v i t (y i , x i |x 1 ) to the velocity u i t (y i , x i |x 1 ), the latter still generates the same probability path p i t|1 (x i |x 1 ). Consequently, theorem 16 implies that the marginal velocity u i t (y i , x) defined by

$u i t (y i , x) = x1 u i t (y i , x i |x 1 ) + c t v i t (y i , x i |x 1 ) p 1|t (x 1 |x) = x i 1 u i t (y i , x i |x i 1 ) + c t v i t (y i , x i |x i 1 ) p i 1|t (x i |x),$still generates the same marginal path p t (x), where the second equality follows from u i t (y i , x i |x 1 ) = u i t (y i , x i |x i 1 ) for mixture paths, and similarly for v i t (y i , x i |x i 1 ). In conclusion, and given X t = x, a single step of the generalized sampling algorithm consists in (i) drawing X i 1 ∼ p i 1|t (X i 1 |x) and (ii) taking an Euler step (6.7) with the velocity

$u i t (y i , x i |x 1 ) = κt 1 -κ t δ(y i , X i 1 ) -δ(y i , x i ) + c t κt 1 -κ t δ(y i , x i 1 ) -δ(y i , x i ) - κt κ t δ(y i , x i ) -p(x i ) ,$where c t > 0 is a time dependent constant.

Similar to the continuous flow matching example in code 1, we provide a standalone implementation of discrete flow matching in pure PyTorch in code 11. Code 10 illustrates how to train a discrete flow with arbitrary data coupling using the flow_matching library. In the previous sections, we have developed a flow model on R d and Riemannian manifolds (see section 3 and section 5) and a CTMC model for discrete data (see section 6). In this section, we want to unify and extend these models to a generative model that works for (1) general state spaces and ( [2](#formula_1)) general Markov processes. This generative model will allow us to extend the principles of Flow Matching to a wide variety of generative models for a variety of modalities in section 9.

## General state spaces and random variables

Working with general modalities. Our explicit goal is not specify the modality we use. Hence, throughout this section, let S be a general state space. Important examples are S = R d (e.g., images, vectors), S discrete (e.g., language), S a Riemannian manifold (e.g., geometric data) or their products for generation of multiple data modalities jointly (multimodal models). For all modalities, we can define a metric (or distance function) d : S × S → R ≥0 , (x, y) → d(x, y) on S. For example, for S discrete, the metric is simply d(x, y) = 1 if y ̸ = x and d(x, x) = 0 for all x ∈ S. For S = R d , we use d(x, y) = ∥x -y∥. We need to make a technical assumption that (S, d) is a Polish metric space, i.e., it is complete (i.e., any Cauchy sequence converges) and separable (i.e., it has a countable dense subset). Any modality of interest for machine learning has that property.

Densities over general state spaces. So far, in this work we assumed that a probability distribution p over S is represented by a density p : S → R ≥0 . For general state spaces, we use a general reference measure ν and the density becomes the Radon-Nikodym derivative dp dν . In other words, probabilities can be expressed as integrals with respect to ν

$P(A) = A p(x)ν(dx) for all measurable A ⊂ S$For S discrete, ν was the counting measure (so the integrals are just sums) and p(x) is just the probability mass function (PMF). For S = R d , ν was the Lebesgue measure (so the integrals are just the "usual" integral) and p(x) is just the probability density function (PDF). The above generalizes that to arbitrary state spaces.

(Optional) Working with arbitrary distributions. It is important to note that not every probability distribution admits a density with respect to a reference measure. For the reader unfamiliar with general measure theory, it is safe ignore this possibility as a technical remark as long one works with distributions of interest that have a density p(x). However, note that these are not just pathological examples but there are real cases of interest for machine learning applications where this matters: A simple example is a probability path of the form p t = δ (1-t)x+ty on S = R d connecting two points x, y ∈ R d in a straight line -this cannot be represented by a density. Another example would be probability distributions over S = C([0, 1], R), e.g., for trajectory modeling, that often do not have a density with respect to a common reference measure. To mathematically handle such cases, we develop our framework for general probability measures p over S. For this, we use the notation p(dx) where "dx" is a symbolic expression denoting integration with respect to p in a variable x. For example, for a bounded measurable function f : S → R we write

$E X∼p [f (X)] = f (x)p(dx)$for the Lebesgue integral or the expected value of f under p. As before, we use (with a slight abuse of notation) the same notation p(x) to denote the density of the measure p(dx).

## The CTMP generative model

Similarly, our explicit goal is build a model that works for arbitrary evolution process -regardless of whether we use a flow, diffusion, a CTMC, a combination, or something else. Therefore, we define a evolution process in this section that is general but satisfies the necessary regularity assumptions to build a generative model. For t ∈ [0, 1], let X t ∈ S be a random variable. We call (X t ) 0≤t≤1 a Continuous-time Markov process (CTMP) if it fulfills the following condition:

Table [2](#) Some examples of CTMP generative models and how they can be learnt with Generator Matching. This list is not exhaustive. Derivations are in section 8. For diffusion, we assume zero drift as this is covered by the "Flow" column. KFE is listed in its adjoint version, i.e., assumes jump kernel Qt(y, x) and density pt(x) exists with respect to reference measure ν. "p.s.d.": Positive semi-definite.

$P[X tn+1 ∈ A|X t1 , X t2 , . . . , X tn ] = P[X tn+1 ∈ A|X tn ] (0 ≤ t 1 < • • • < t n+1 ≤ 1, A ⊆ S) (8.1)$Informally, the above condition says that the process has no memory. If we know the present, knowing the past will not influence our prediction of the future. In table 2, we give an overview over important classes of Markov processes. For example, a flow on a manifold is a Markov process with deterministic transitions, a diffusion is a Markov process with transitions driven by Brownian motion, and CTMCs are Markov processes determined by rates (we will explain this in detail in section 8.2.2). Each Markov process has a transition kernel (p t+h|t ) 0≤t<t+h≤1 that assigns every x ∈ S a probability distribution p t+h|t (•|x) such that

$P[X t+h ∈ A|X t = x] = p t+h|t (A|x) for all t, h ≥ 0, A ⊂ S measurable (8.2)$Due to the Markov assumption, a Markov process is uniquely determined by the transition kernel and the distribution of X 0 . Conversely, any transition kernel and initial distribution defines a Markov process. Therefore, there is a 1:1 correspondence.

Our next goal is to define a corresponding generalization of a velocity field for CTMPs. Informally, it would be the 1st-order derivative of the transition kernel in t:

$L t := d dh h=0 p t+h|t (8.3)$We call the 1st-order derivative L t the generator of p t+h|t [(Ethier and Kurtz, 2009;](#b22)[Rüschendorf et al., 2016)](#b69). Similar to derivatives, generators are first-order linear approximations and easier to parameterize than p t+h|t . As we will see, diffusion, flows, and other generative models can all be seen as algorithms to learn the generator of a Markov process (see table [2](#)). This leads to the general form of the CTMP generative model given by CTMP model (informal): p t+h|t (•|x) := δ x + hL t (x) + o(h), and X 0 ∼ p. (8.4)

However, as a transition kernel p t+h|t is not a real function, equation 8. [3 and 8.4](#) are only heuristic and not well-defined yet. Therefore the first goal of this section is to provide a formal definition of the generator and the CTMP generative model.

## Formal definition of generator

The first problem in equation (8.3) is that derivatives are usually defined with respect to functions mapping to vector spaces but p t+h|t maps to a distribution. However, this can be alleviated by using test functions.

Test functions are a way to "probe" a probability distribution. They serve as a theoretical tool to handle distributions as if they were real-valued functions. Specifically, a set of test functions is a family T of bounded, measurable functions f : S → R that characterize probability distributions fully, i.e., for two probability distributions µ 1 , µ 2 on S it holds

$µ 1 = µ 2 ⇔ E X∼µ1 [f (X)] = E X∼µ2 [f (X)] for all f ∈ T (8.5)$Generally speaking, one chooses T to be as "nice" (or regular) as possible. For example, if S = R d , the space T = C ∞ c (R d ) of infinitely differentiable functions with compact support fulfills that property. For S discrete, T = R S simply consists of all functions (which are just vectors in this case). Let X t ∼ p t . We define the marginal action and transition action as

$⟨p t , f ⟩ := f (x)p t (dx) = E X∼pt [f (X)] (8.6) p t+h|t , f (x) := p t+h|t (•|x), f = E [f (X t+h )|X t = x] (8.7)$where the marginal action maps each test function f to a scalar ⟨p t , f ⟩ ∈ R, while the transition action maps a real-valued function x → f (x) to a another real-valued function x → p t+h|t , f (x). The tower property implies that p t , p t+h|t , f = ⟨p t+h , f ⟩. We note that the above is only a "symbolic" dot product but becomes a "proper" dot product if a density p t (x) exists, i.e., ⟨p t , f ⟩ = f (x)p t (x)ν(dx).

The second step to formally define a derivative as in equation ( [8](#).3) is that we need to impose some notion of "smoothness" on a Markov process that we define now. Let C 0 (S) be the space of continuous functions f : S → R that vanish at infinity, i.e., for all ϵ > 0 there exists a compact set K ⊂ S such that |f (x)| < ϵ for all x ∈ S \ K. We use the supremum norm ∥ • ∥ ∞ on C 0 (S). A CTMP X t is called a Feller process if it fulfils the following two conditions [(Feller, 1955;](#b23)[Rüschendorf et al., 2016](#b69)):

1. Strong continuity: The action of p t+h|t is continuous in time:

$lim h ′ →h,t ′ →t ∥ p t ′ +h ′ |t ′ , f -p t+h|t , f ∥ ∞ = 0 for all h, t ≥ 0, f ∈ C 0 (S)$
## 2.

No return from infinity: The action of p t+h|t preserves functions that vanish at infinity:

$p t+h|t , f ∈ C 0 (S) for all h, t ≥ 0, f ∈ C 0 (S)$Assumption 4. The CTMP (X t ) 0≤t≤1 is a Feller process. This is a reasonable assumption given that we want to use X t in a machine learning model: We define probability paths where the distribution of the generative process X t vary smoothly, and all our data usually lies in some bounded (compact) set.

Let us now revisit (8.3) and try define the derivative of p t+h|t . With the test function perspective in mind, we can take derivatives of p t+h|t , f (x) per x ∈ S and define

$d dh h=0 p t+h|t , f (x) = lim h→0 p t+h|t , f (x) -f (x) h := [L t f ](x). (8.8)$We call this action the generator L t and define it for all f for which the above limit exists uniformly, i.e., in the norm ∥ • ∥ ∞ . Intuitively, a generator is defined as an operator of test functions. In table 2, there are several examples of generators that we derive in section 8.2.2. There is a 1:1 correspondence between a generator and a Feller process [(Rogers and Williams, 2000;](#b67)[Ethier and Kurtz, 2009;](#b22)[Pazy, 2012)](#b58) -in the same way as there is a correspondence between a flow and a vector field (see theorem 1). This will later allows us to parameterize a Feller process via a generator in a neural network.

With this definition, the CTMP model in (8.4) has the, now well-defined, form as

$p t+h|t , f = f + hL t f + o(h) (for all f ∈ T ) and X 0 ∼ p (8.9)$where o(h) describes an error terms E(h) ∈ C 0 (S) such that lim h→0 1 h ∥E(h)∥ ∞ = 0. Similarly to equation (3.24) for the case of flows and to equation (6.5) for the case of CTMCs, we say that L t generates p t if there exists a p t+h|t satisfying (8.9) with CTMP X t such that X t ∼ p t (8.10)

In other words, a generator L t generates the probability path p t if a Markov process that is (1) initialized with p = p 0 and ( [2](#formula_1)) simulated with L t has marginals p t .

## Examples of CTMP models

We go through several examples to illustrate how to compute a generator of a Markov process. The results from this section are summarized in table [2](#).

$Flows. Let S = R d and u : [0, 1] × R d → R d , (t, x) → u t ($x) be a time-dependent velocity field defining a flow ψ t (see section 3). Let T = C ∞ c (R d ) be the space of infinitely differentiable and smooth functions with compact support. Then we can compute the generator via [(8.16)](#) where (i) follows from a Euler approximation of the flow and (ii) follows from a first-order Taylor approximation of f around X t . Therefore, the flow generator is given by

$[L t f ](x) = lim h→0 E [f (X t+h )|X t = x] -f (x) h (8.11) (i) = lim h→0 E [f (X t + hu t (X t ) + o(h))|X t = x] -f (x) h (8.12) (ii) = lim h→0 E f (X t ) + h∇f (X t ) T u t (X t ) + o(h)|X t = x -f (x) h (8.13) = lim h→0 f (x) + h∇f (x) T u t (x) + o(h) -f (x) h (8.14) (8.15) = ∇f (x) T u t (x),$$L t f (x) = ∇f (x) T u t (x).$(8.17)

$Diffusion. Let S = R d and σ t : [0, 1] × R d → R d×d , (t, x) → σ t ($x) be a time-dependent function mapping to symmetric positive semi-definite matrices σ t in a continuous fashion. A diffusion process with diffusion coefficient σ t is defined via the SDE dX t = σ t (X t )dW t for a Wiener process W t [(Øksendal, 2003)](#). This process can be approximated via the infinitesimal sampling procedure:

$X t+h = X t + √ hσ t (X t )ϵ t , ϵ t ∼ N (0, I) (8.18) Let again be T = C ∞ c (R d ).$Then we can compute the generator via

$[L t f ](x) = lim h→0 E f (X t + √ hσ t (X t )ϵ t + o(h))|X t = x -f (x) h (8.19) (i) = lim h→0 E ϵt [f (x) + ∇f (x) T √ hσ t (x)ϵ t + 1 2 h[σ t (x)ϵ t ] T ∇ 2 f (x)[σ t (x)ϵ t ] + o(h) -f (x)] h (8.20) = lim h→0 ∇f (x) T √ hσ t (x)E ϵt [ϵ t ] + E ϵt [ 1 2 h[σ t (x)ϵ t ] T ∇ 2 f (x)[σ t (x)ϵ t ]] h (8.21) = 1 2 E ϵt [ϵ T t [σ t (x)] T ∇ 2 f (x)[σ t (x)]ϵ t ]] (8.22) (ii) = 1 2 tr σ t (x) T ∇ 2 f (x)σ t (x) (8.23) (iii) = 1 2 tr(σ t (x)σ t (x) T ∇ 2 f (x)) (8.24) (iv) = 1 2 σ 2 t (x) • ∇ 2 f (x) (8.25)$where in (i) we use a 2nd order Taylor approximation (2nd order because

$E[∥ √ hϵ t ∥ 2 ] ∝ h), in (ii) the identity tr(A) = E ϵt [ϵ T t Aϵ t ] for A ∈ R d×d , in($iii) the cyclic property of the trace, and in (iv) the symmetry of σ t . Further, we use A • B := tr(A T B) to denote the matrix inner product for matrices A, B ∈ R d×d . Therefore, the diffusion generator is given by

$L t f (x) = 1 2 σ 2 t (x) • ∇ 2 f (x). (8.26)$Jumps. Next, let S be arbitrary and let us consider a jump process. A jump process is defined by a time-dependent kernel Q t (dy, x), i.e., for every 0 ≤ t ≤ 1 and every x ∈ S, Q t (dy, x) is a positive measure over S \ {x}. The idea of a jump process is that the total volume assigned to S \ {x}

$λ t (x) = Q t (dy, x) (8.27)$gives the jump intensity, i.e., the infinitesimal likelihood of jumping. Further, if λ t (x) > 0, we can assign a jump distribution by normalizing Q t to a probability kernel

$J t (dy, x) = Q t (dy, x) λ t (x) .$(8.28)

A jump process can be approximated via the infinitesimal sampling procedure as follows:

$X t+h = X t with probability 1 -hλ t (X t ) + o(h) ∼ J t (dy, X t ) with probability hλ t (X t ) + o(h) (8.29)$For a rigorous treatment of jump processes, see for example [(Davis, 1984)](#b17). The generator is then given by

$L t f (x) = lim h→0 E[f (X t+h ) -f (X t )|X t = x] h (8.30) = lim h→0 E[f (X t+h ) -f (X t )|X t = x, Jump in [t, t + h)]P[Jump in [t, t + h)|X t = x] h (8.31) + lim h→0 E[f (X t+h ) -f (X t )|X t = x, No jump in [t, t + h)]P[No jump in [t, t + h)|X t = x] h =0 (8.32) = lim h→0 E Y ∼Jt(dy,x) [f (Y ) -f (x)] hλ t (x) h (8.33) = E Y ∼Jt(dy,x) [f (Y ) -f (x)] λ t (x) (8.34) = (f (y) -f (x))Q t (dy, x) (8.35)$where we have used that if X t does not jump in [t, t + h], then X t+h = X t . Therefore, the jump generator is given by

$L t f (x) = (f (y) -f (x))Q t (dy, x) = λ t (x)E Y ∼Jt(dy,x) [f (Y ) -f (x)]. (8.36)$Continuous-time Markov chain (CTMC). Let us consider a continuous-time Markov chain X t on a discrete space S with |S| < ∞. In fact, this is simply a jump process on a discrete state space with a specific parameterization. To see this, consider a vanilla jump kernel on a discrete state space S given by a matrix

$Q t ∈ R S×S ≥0$and using equation (8.36), the generator is given by

$L t f (x) = y∈S [f (y) -f (x)]Q t (y, x) = y̸ =x [f (y) -f (x)]Q t (y, x) for all x ∈ S, f ∈ R S (8.37)$i.e., the value of Q t (x, x) does not matter and is underdetermined. Therefore, a natural convention is to reparameterize the jump kernel on discrete state spaces by rates:

$u t (y, x) =    Q t (y, x) if y ̸ = x - z̸ =x Q t (z; x) if y = x$With this, we recover the rates u t (y, x) from section 6 fulfilling the rate conditions in 6.4 by construction. Therefore, this shows that a jump model on a discrete space coincides with the CTMC model (section 6). Applying this on equation (8.37), we get that the CTMC generator is given by

$L t f (x) = y∈S f (y)u t (y, x) = f T u t (8.38)$where we consider f = (f (x)) x∈S as a column vector and u t ∈ R S×S as a matrix. Therefore, the generator function is simply vector multiplication from the left.

Flows on manifolds. Next, we consider flows on Riemannian manifolds S = M as in section 5. A flow ψ : [0, 1] × M → M is defined via a vector field u : [0, 1] × M → T M via the ODE in [(3.19)](#). Let us denote the transition from time s to t via ψ t|s (x) = ψ t (ψ -1 s (x)) (as in (3.17)). Then, for a smooth function f : M → R we have that the Riemannian flow generator is given via

$L t f (x) = lim h→0 f (ψ t+h|t (x)) -f (x) h = ∇f (x), d dh h=0 ψ t+h|t (x) g = ⟨∇f (x), u t (x)⟩ g (8.39)$where ⟨•, •⟩ g describes the dot product defining the Riemannian metric g and ∇f describes the gradient of f with respect to g. In fact, the generator coincides with the Lie derivative of a function [(Jost, 2008)](#b38), a fundamental concept in differential geometry.

## Probability paths and Kolmogorov Equation

For Flow Matching on S = R d , the Continuity Equation (see [3.25)](#) is the central mathematical equation that allows us to construct velocity fields that generate a desired probability path (see section 3.5). In this section, we derive a corresponding -more general -equation for CTMPs. Let X t be a CTMP with generator L t and let X t ∼ p t , then we know that:

$d dt ⟨p t , f ⟩ = d dh h=0 ⟨p t+h , f ⟩ = d dh h=0 p t , p t+h|t , f = p t , d dh h=0 p t+h|t , f = ⟨p t , L t f ⟩$where we used that the ⟨p t , •⟩ operation is linear to swap the derivative, and that by the tower property p t , p t+h|t , f = ⟨p t+h , f ⟩. This shows that given a generator L t of a Markov process X t we can recover its marginal probabilities via their infinitesimal change, i.e., we arrive at the

Kolmogorov Forward Equation (KFE) d dt ⟨p t , f ⟩ = ⟨p t , L t f ⟩ for all f ∈ T (8.40) The version of the KFE in equation (8.40) determines the evolution of expectations of test functions f . This is necessary if we use probability distributions that do not have a density. If a density exists, a more familiar version of the KFE can be used that directly prescribes the change of the probability densities. To present it, we introduce the adjoint generator L * t , which acts on probability densities p t (x) with respect to a reference measure ν, namely L * t p t (x) is (implicitly) defined by the identity p t (x)L t f (x)ν(dx) = L * t p t (x)f (x)ν(dx) ∀f ∈ T (8.41) Further, we need to assume that p t is differentiable in t. Now, (8.41) applied to the KFE (8.40) we get d dt p t (x)f (x)ν(dx) = d dt p t (x)f (x)ν(dx) (8.42) = d dt ⟨p t , f ⟩ (8.43) = ⟨p t , L t f ⟩ (8.44) = p t (x)L t f (x)ν(dx) (8.45) = L * t p t (x)f (x)ν(dx) (8.46) As this holds for all test functions f , we can conclude using equation (8.5) that this is equivalent to the adjoint KFE d dt p t (x) = L * t p t (x) for all x ∈ S (8.47) As we will derive in the following examples, the adjoint KFE generalizes many famous equations used to develop generative models such as the Continuity Equation or the Fokker-Planck Equation (Song et al., 2021; Lipman et al., 2022) (see table 2). Whenever a probability density exists, we use the adjoint KFE -to avoid using test functions and work with probability densities directly. We summarize our findings in the following Theorem 17 (General Mass Conservation). Let L t be a generator of (X t ) 0≤t≤1 . Informally, the following conditions are equivalent: 1. p t , L t satisfies the KFE (8.40). 2. dpt dν (x), L t satisfy the adjoint KFE (8.47). 3. L t generates p t in the sense of equation (8.10).

Formally, ( [1](#formula_169)) and ( [2](#formula_1)) are equivalent whenever dpt dν exists and is continuously differentiable in t. Further, (3) implies ( [1](#formula_169)) for arbitrary state spaces. There are weak regularity assumptions that ensure that (1) implies (3) (see appendix A.3 for a list). In this work, we assume that these hold, i.e., we assume that [(3)](#b89) implies [(1)](#b87).

To the best of our knowledge, there is no known result for abstract general state spaces that ensures that in theorem 17 condition (3) implies [(1)](#b87). This is why we simply assume it here. For the machine learning researcher, this assumption holds for any state space of interest and should therefore be of no concern (see appendix A.3).

## Examples of KFEs

Adjoint KFE for Flows. Let us set S = R d and assume that p t has a density p t (x) with respect to the Lebesgue measure that is bounded and continuously differentiable. Then we can compute the adjoint generator

$L * t via ⟨p t , L t f ⟩ =E x∼pt [L t f (x)] (8.48) = L t f (x)p t (x)dx (8.49) (i) = ∇f (x) T u t (x)p t (x)dx (8.50) (ii) = f (x) [-div(p t u t )(x)] =:L * t pt(x) dx (8.51) = f (x)L * t p t ($x)dx (8.52) where (i) follows by equation (8.15) and (ii) by integration by parts. The above derivation shows that the adjoint generator is given by L * t p t = -div(p t u t )(x) (because it fulfils the condition in equation (8.41)). Using the adjoint KFE, we recover the Continuity Equation (see equation (3.25)) d dt p t (x) = -div(p t u t )(x), (8.53) an equation that we extensively studied in section 3.4. Adjoint for diffusion. Let's set S = R d and assume that p t has a density p t (x) with respect to the Lebesgue measure that is bounded and continuously differentiable. We can compute the adjoint generator L * t via ⟨p t , L t f ⟩ =E x∼pt [L t f (x)] (8.54) = L t f (x)p t (x)dx (8.55)

$(i) = 1 2 σ 2 t (x) • ∇ 2 f (x)p t (x)dx (8.56) (ii) = f (x) 1 2 ∇ 2 • (p t σ 2 t )(x) =:L * t pt(x) dx (8.57) = f (x)L * t p t ($x)dx (8.58) by (i) follows by equation (8.26) and (ii) follows by applying integration by parts twice. The above derivation shows that the adjoint generator is given by L * t p t = 1 2 ∇ 2 • (p t σ 2 t )(x) (because it fulfils the condition in equation (8.41)). The adjoint KFE then recovers the well-known Fokker-Planck equation d dt p t (x) = 1 2 ∇ 2 • (p t σ 2 t )(x) (8.59) Adjoint KFE for jumps. Let's assume that p t has a density p t (x) with respect to the Lebesgue measure that is bounded and continuously differentiable. Let's assume that the jump measures Q t (dy, x) is given via a kernel Q t

$: S × S → R ≥0 , (y, x) → Q t (y, x) such that f (y)Q t (dy, x) = f (y)Q t (y, x)ν(dy) for all integrable f : S → R. (8.60)$Then we can derive the adjoint generator as follows:

$⟨p t , L t f ⟩ (x) = (f (y) -f (x))Q t (y, x)ν(dy)p t (x)ν(dx) (8.61) = f (y)Q t (y, x)p t (x)ν(dy)ν(dx) - f (x)Q t (y, x)p t (x)ν(dy)ν(dx) (8.62) (i) = f (x)Q t (x, y)p t (y)ν(dy)ν(dx) - f (x)Q t (y, x)p t (x)ν(dy)ν(dx) (8.63) = f (x) Q t (x, y)p t (y) -Q t (y, x)p t (x)ν(dy) =:L * t pt ν(dx) (8.64) = f (x)L * t p t (x)ν(dx) (8.65)$where in (i) we simply swap the variables x and y. The above derivation shows that L * t as defined above fulfils the condition in equation (8.41) and indeed describes the adjoint generator for jumps. With this, the adjoint KFE becomes the Jump Continuity equation:

$d dt p t (x) = [Q t (x, y)p t (y) -Q t (y, x)p t (x)] ν(dy) = λ t (y)J t (x, y)p t (y)ν(dy) -λ t (x)p t (x) (8.66)$where we use the decomposition Q t (y, x) = λ t (x)J t (y, x) into a jump intensity λ t and a jump distribution J t (see equation (8.28)).

Adjoint KFE for CTMCs. For S discrete and generator given by f T u t as in equation ( [8](#).38), we get that

$⟨p t , L t f ⟩ = p t (x)L t f (x)ν(dx) = x∈S p t (x) y∈S u t (y, x)f (y) = y∈S [ x∈S p t (x)u t (y, x)] =:L * t pt(x) f (y) = L * t p t (y)f (y)ν(dy)$where ν here just denotes the counting measure. Therefore, the adjoint is KFE simply given by

$d dt p t (x) = y∈S u t (x, y)p t (y) (8.67)$This recovers the KFE for CTMCs as derived in equation (6.8) (with x and y switched to keep consistency with the derivations in this section).

## Generator Matching

In this section, we describe Generator Matching (GM) [(Holderrieth et al., 2024)](#b33), a generative modeling framework for (1) arbitrary data modalities and ( [2](#formula_1)) general Markov processes. GM unifies the vast majority of generative models developed in recent years, including diffusion models, "discrete diffusion" models, and the FM variants described in previous sections. To introduce GM, we defined the CTMP generative model in section 8 that is constructed via a generator of a Markov process. GM describes a scalable algorithm to train generators -giving the method its name. Beyond providing a unifying framework, GM gives rise to a variety of new models, allows us to combine models of different classes, as well as allows to build models for arbitrary modalities including models across multiple data modalities.

## Data and coupling

As before, our goal is to transfer samples X 0 ∼ p from a distribution p to samples X 1 ∼ q from a target distribution q, where X 0 , X 1 ∈ S are two RVs each taking values in the state space S. Source and target samples can be related by means of the independent coupling (X 0 , X 1 ) ∼ p ⊗ q (product distribution), or associated by means of a general PMF coupling π 0,1 , i.e., distribution over S × S with marginal π 0 = p and π 1 = q. The only difference to before is that S is a general state space and that p, q can be arbitrary probability measures.

## General probability paths

The next step in the GM recipe is, as before, to prescribe a probability path p t interpolating p and q. Following section 4.4, we use a conditional probability path p t|Z (dx|z), i.e., a set of time-varying probability measures dependent on a latent state z ∈ Z. Given a distribution p Z over Z, we consider the corresponding marginal probability path p t (dx) defined via the hierarchical sampling procedure:

$Z ∼ p Z , X t ∼ p t|Z (dx|z) ⇒ X t ∼ p t (dx)$i.e., we obtain a sample from p t by first sampling Z from p Z and then sampling X t from p t|Z (dx|z). As before, the marginal probability path is constructed to satisfy the boundary constraints p 0 = p and p 1 = q.

We have seen already two common constructions for Z = S and p Z = q: First, affine conditional flows for S = R d (as used in continuous FM; section 4) defined via

$Z ∼ q, X 0 ∼ p, X t = σ t X 0 + α t Z ⇒ X t ∼ p t (dx) (9.1)$where α t , σ t ∈ R ≥0 are differentiable functions satisfying α 0 = σ 1 = 0 and α 1 = σ 0 = 1. Second, for arbitrary S, we can use mixtures as used in discrete FM for discrete state spaces (equation (7.22)):

$Z ∼ q, X 0 ∼ p, X t ∼ Z with prob κ t X 0 with prob (1 -κ t ) ⇒ X t ∼ p t (dx) (9.2)$where κ t ∈ R ≥0 is a differentiable functions satisfying κ 0 = 0 and κ 1 = 1 and 0 ≤ κ t ≤ 1. One can easily see that the affine conditional and mixture probability paths interpolate p and q, i.e., p 0 = p and p 1 = q.

## Parameterizing a generator via a neural network

Given a probability path p t , our goal is construct a CTMP model specified by a generator L t that generates this probability path (see equation (8.10)). To train a neural network for that, we first need to explain how to parameterize a generator L t with a neural network L θ t with parameters θ. We will do this in this section. Let T again be a family of test functions (see section 8.2.1). A linear parameterization of L t is defined as follows: for every x ∈ S there is (1) a convex closed set Ω x ⊂ V x that is a subset of a vector space V x with an inner product ⟨•, •⟩ x and (2) a linear operator K : T → C(S; V x ) such that every considered generator L t can be written as

$L t f (x) = ⟨Kf (x), F t (x)⟩ x (9.3)$for a function F t such that F t (x) ∈ Ω x for every x ∈ S. Crucially, the operator K cannot depend on L t , i.e., only F t has to be learned. This leads to the Parameterized generator: L θ t f (x) = Kf (x), F θ t (x) x with neural network F θ t and parameters θ, (9.4)

where again F θ t maps an element x ∈ S to F θ t (x) ∈ Ω x . We list several examples to make this definition more concrete.

Linear parameterization of flows. Let S = R d and Ω x = R d = V x . Let's consider all flows, i.e., the family of generators is given by (see equation (8.15)):

$L t f = ∇f T u t , u t : R d → R d .$(9.5)

Setting Kf = ∇f and F t = u t we recover the shape of equation ( [9](#formula_272).3). This gives a natural linear parameterization of flow generators via their vector fields.

Linear parameterization of diffusion. Let S = R d and Ω

$x = S ++ d ⊂ R d×d = V x , where S ++ d$denotes the set of all positive semi-definite matrices. Then a diffusion generator is given by (see equation (8.26)):

$L t f = ∇ 2 f • σ 2 t , σ t : R d → S ++ d (9.6)$Setting Kf = ∇ 2 f and F t = σ 2 t we recover the shape of equation ( [9](#formula_272).3). This gives a natural linear parameterization of diffusion generators.

## Linear parameterization of jumps. Let

$Ω x = {a : S \ {x} → R ≥0 | a integrable} ⊂ L 2 (S \ {x}) = V x with dot product ⟨a, b⟩ x = S\{x} a(x)b(x)ν(dx).$Then the jump generator is given by (see equation (8.36)):

$L t f (x) = [f (y) -f (x)]Q t (y, x)ν(dy) = ⟨Kf (x), Q t (•; x)⟩ x (9.7)$where we set Kf (x) as the function y → f (y) -f (x). Setting F t = Q t we recover the shape of equation (9.3)giving a linear parameterization of jump generators. We note that the above only parameterizes jumps with a jump kernel Q t (y, x), which does not necessarily include all jump measures.

Linear parameterization of CTMCs. Let S be discrete and u t ∈ R S×S be a rate matrix of a continuous-time Markov chain. As for discrete FM (see equation (7.5)), we define

$Ω x =    v ∈ R S v(y) ≥ 0 ∀y ̸ = x, and v(x) = - y̸ =x v(y)    ⊂ V x = R S . (9.8)$Then by equation (8.38), the generator is given for f ∈ R S by

$L t f (x) = f T u t (•, x) = ⟨f, u t (•, x)⟩ x (9.9)$where V x = R S and Kf = f and ⟨•, •⟩ x is the standard Euclidean dot product. With this, we recover the shape of equation (9.3). Therefore, this gives a natural linear parameterization of CTMCs via their rates u t .

Linear parameterization of flows on manifolds. Let S = M be a Riemannian manifold and as in section 5, let us consider flows on Riemannian manifolds. By equation (8.39), the generator is given by L t f (x) = ⟨∇f (x), u t (x)⟩ g (9.10) with u t being a time-dependent smooth vector field u t : [0, 1] × M → T M, and u t (x) ∈ T x M for all x ∈ M.

Setting Ω x = V x = T x M and K = ∇f the gradient operator, we recover the shape of equation (9.3). Therefore, this gives a natural linear parameterization of Riemannian flow generators.

## Marginal and conditional generators

In this section, we show how to find generators for marginal probability paths. The recipe is as follows: We can find generators for conditional probability paths p t|Z (dx|z), often analytically, and use these to construct generators for the marginal path. Specifically, let us assume that for every z ∈ Z we found a (conditional) generator L z that generates p t|Z (dx|z), i.e., by theorem 17 this equivalent to the KFE (equation (8.40)):

$d dt p t|Z (•|z), f = p t|Z (•|z), L z t f for all f ∈ T . (9.11)$Further, let us assume that we found a linear parameterization (see equation (9.3)) as follows:

$L z t f (x) = ⟨Kf (x), F t (x|z)⟩ x z ∈ Z (9.12) for functions F t (x|z) ∈ Ω x ⊂ V x .$For example, F t (x|z) could be conditional velocity field in continuous FM (see section 4.3) or the conditional rates in discrete FM (see equation (7.2)). This allows us to find a formula for a generator that generates the marginal path: Theorem 19 (General Marginalization Trick). The marginal probability path (p t ) 0≤t≤1 is generated by a Markov process X t with generator

$L t f (x) = E Z∼p Z|t (•|x) [L Z t f (x)] (9.13)$where p Z|t (dz|x) is the posterior distribution (i.e., the conditional distribution of z given x). The generator L t has a linear parameterization given by

$F t (x) = E Z∼p Z|t (•|x) [F t (x|Z)].(9.14)$The above theorem gives us our training target: to approximate L t in equation ( [9](#formula_272).13) with a neural network.

The marginalization tricks seen in previous chapters (theorem 3, theorem 10, theorem 14) are special cases of this theorem. We give a proof here and then show a few examples of novel instantiations.

Proof. To prove that L t generates p t , we need to show by theorem 17 that the KFE is fulfilled. Let p t+h|t (•|x, z) be the transition kernel of L z t . Then:

$d dt ⟨p t , f ⟩ = lim h→0 1 h (⟨p t+h , f ⟩ -⟨p t , f ⟩) = lim h→0 1 h (E Z∼p Z ,X ′ ∼p t+h|Z (•|Z) (f (X ′ )) -E Z∼p Z ,X∼p t|Z (•|z) (f (X))) = lim h→0 1 h (E Z∼p Z ,X∼p t|Z (•|Z),X ′ ∼p t+h|t (•|X,Z) (f (X ′ )) -E Z∼p Z ,X∼p t|Z (•|z) (f (X))) = lim h→0 1 h (E Z∼p Z ,X∼p t|Z (•|Z),X ′ ∼p t+h|t (•|X,Z) (f (X ′ ) -f (X))) = lim h→0 1 h E X∼pt E Z∼p t|Z (•|X) E( X ′ ∼p t+h|t (•|X,Z) (f (X ′ )) -f (X) = E X∼pt E Z∼p Z|t (•|X) lim h→0 1 h E X ′ ∼p t+h|t (•|X,Z) (f (X ′ )) -f (X) = E X∼pt (E Z∼p Z|t (•|X) (L z t f (X)) =:Ltf (X) ) = ⟨p t , L t f ⟩$The proof for the form of F t follows then by

$E Z∼p Z|t (•|X) (L z t f (x)) (9.12) = E Z∼p Z|t (•|X) (⟨Kf (x), F t (x|z)⟩ x ) = Kf (x), E Z∼p Z|t (•|X) (F t (x|z)) x = ⟨Kf (x), F t (x)⟩ x$where we use the linearity of the dot product to swap it with the expected value. This shows that F t is a linear parameterization (see equation (9.3)) of the marginal generator.

Example -Jumps. Let S be arbitrary and Q t (y, x|z) be a conditional jump kernel on S for y, x ∈ S, z ∈ Z generating the conditional probability path p t|Z (dx|z). Using the linear parameterization of the jump kernel (see equation (9.7)), we get that the marginal jump kernel

$Q t (y, x) = E Z∼p Z|t (•|x) [Q t (y, x|z)].$generates the marginal probability p t (dx).

Example -Marginal diffusion coefficient. Let S = R d and σ 2 t (x|z) be a diffusion coefficient generating the conditional probability path p t|Z (dx|z). Using the linear parameterization of the diffusion coefficient (see equation (9.6)), we get that the marginal diffusion coefficient

$σ 2 t (x) = E Z∼p Z|t (•|x) [σ 2 t (x|Z)]$generates the marginal probability path p t (dx).

## Generator Matching loss

Our next goal is to develop a training objective for learning a CTMP model. Let us assume that we have a neural network F θ t that gives us a generator parameterization L θ t as in equation (9.4). As derived in theorem 19, our goal is to approximate the true marginal linear parameterization F t given by equation (9.14).

As before, let us assume that for every x ∈ S we have a Bregman divergence D

$x : Ω x × Ω x → R defined via D x (a, b) = Φ x (a) -[Φ x (b) + ⟨a -b, ∇Φ x (b)⟩], a, b ∈ Ω x (9.15)$for a strictly convex function Φ x : Ω x → R (see figure [10](#fig_14)). The Generator Matching loss to train the CTMP model is defined as

$L GM (θ) = E t,Xt∼pt D Xt (F t (X t ), F θ t (X t )),(9.16)$for t ∼ U [0, 1]. Unfortunately, the above training objective is intractable as we do not know the marginal generator L t and also not the parameterization F t thereof (we only know the intractable formula in equation (9.14)). Hence, we introduce the Conditional Generator Matching loss as a tractable alternative that takes the form L CGM (θ) = E t,Z,Xt∼p t|Z D Xt (F t (X t |Z), F θ t (X t )).

(9.17)

This objective is tractable as we can derive F t (x|z) analytically in many cases (see section 9.6). As the next theorem shows, the two losses (9.16) and (9.17) both provide the same learning gradients.

Theorem 20. The gradients of the Generator Matching loss and the Conditional Generator Matching loss coincide: ∇ θ L GM (θ) = ∇ θ L CGM (θ).

(9.18)

In particular, the minimizer of the Conditional Generator Matching loss is the linear parameterization of the marginal generator (equation (9.14)):

$F θ t (x) = E Z∼p Z|t (•|x) [F t (x|Z)]. (9.19)$Furthermore, for these properties to hold, D x must necessarily be a Bregman divergence.

The above theorem generalizes theorem 4, theorem 11, and theorem 15 derived in previous sections to general CTMP models. It allows us to easily train any CTMP model parameterized by a neural network F θ t in a scalable fashion by minimizing the Conditional Generator Matching loss. In addition, it universally characterizes the space of loss functions. The proof of theorem 20 is identical as the proof of theorem 4 with u t replaced by F t . For the proof of the necessity of D being a Bregman divergence, we refer to [(Holderrieth et al., 2024)](#b33).

i.e., the jump intensity is given by λ t and once we decided to jump, we jump straight to z ∈ S. To show this, we show that the above jump process fulfils the KFE. We can derive:

$E X∼p t|Z (•|z) λ t (X)E Y ∼Jt(•,X) [f (Y ) -f (X)] = κt 1 -κ t E X∼p t|Z (•|z) [f (z) -f (X)] = κt 1 -κ t [f (z) -E X∼p t|Z (•|z) [f (X)]] = κt 1 -κ t [f (z) -[κ t f (z) + (1 -κ t )E X∼p f (X)] = κt f (z) -κt E X∼p f (X) = d dt [κ t f (z) + (1 -κ t )E x∼p [f (x)]] = d dt E X∼p t|Z (•|z) [f (X)] = d dt p t|Z (•|z), f .$Therefore, we see that the process fulfills the jump KFE (equation (9.26)). Therefore, we have established a jump model. We have seen a special example of that model for discrete state spaces in equation (7.24). Here, we have shown that one could also build a similar jump model for Euclidean space R d , for example.

Jump models for arbitrary paths with densities. Let us assume that we have a probability p t|Z (dx|z) that admits a density p t|Z (x|z) with respect to a reference measure ν on S and that is differentiable in t (note that the mixture path in equation (9.25) would not fulfill that for S = R d ). Further, we restrict ourselves to jump kernels J t (y, x) that admit a density.

With this, the adjoint KFE becomes the Jump Continuity Equation (equation (8.66)): d dt p t|Z (x|z) = λ t (y)J t (x, y)p t|Z (y|z)dy -p t|Z (x|z)λ t (x) (9.27) ⇔ p t|Z (x|z) d dt log p t|Z (x|z) + λ t (x) = λ t (y)J t (x, y)p t|Z (y|z)dy (9.28) Making J t (x, y) = J t (x) ("target-state-independent") and using ∂ t = d dt , we get that this equivalent to: p t|Z (x|z) ∂ t log p t|Z (x|z) + λ t (x) = J t (x) λ t (y)p t|Z (y|z)ν(dy) (9.29) ⇔ p t|Z (x|z) ∂ t log p t|Z (x|z) + λ t (x) λ t (y)p t|Z (y|z)ν(dy) = J t (x) (9.30)

In order to define a valid jump process, we require λ t , J t to satisfy λ t (x) ≥ 0, J t (x) ≥ 0. Therefore, we get:

$λ t (x) ≥0, J t (x) ≥ 0 ⇔ λ t ($x) ≥ [-∂ t log p t (x|z)] + (9.31) where [x] + = max(x, 0) describes the ReLU operation. Further, we require J t to define a valid jump distribution, i.e., integrate to 1. This can be seen to hold: 1 = J t (x)dx ⇔ λ t (x)p t|Z (x|z)ν(dx) = p t|Z (x|z) ∂ t log p t|Z (x|z) + λ t (x) ν(dx) ⇔ 0 = ∂ t p t|Z (x|z)ν(dx) ⇔ 0 = ∂ t p t|Z (x|z)ν(dx) ⇔ 0 = 0 i.e., J t indeed integrates to 1. Choosing the minimal λ t (x), we get that a jump model defined via λ t (x) = -∂ t log p t|Z (x|z) + , J t (x) = p t|Z (x|z)[∂ t log p t|Z (x|z)] + p t|Z (y|z)[∂ t log p t|Z (y|z)] + ν(dy) = [∂ t p t|Z (x|z)] + [∂ t p t|Z (y|z)] + ν(dy)

is a solution to the jump continuity equation and therefore generates the conditional probability path p t|Z (x|z). At first, it seems dissatisfying that jump distribution is independent of the location. However, if we extend this model to multiple dimensions, the jump distribution will depend on the location and leads to a powerful generation model [(Holderrieth et al., 2024)](#b33).

## Combining Models

In this section, we explain how GM allows us to combine generative models for the same state space S in different ways. The underlying principle is simple: the generator is a linear operator and the KFE ∂ t ⟨p t , f ⟩ = ⟨p t , L t f ⟩ is a linear equation -so essentially, we can combine solutions for this equation like we do for matrix equations in linear algebra. Specifically, let L t , L ′ t be two generators of two Markov processes that solve the KFE for a probability path p t . Then for α 1 t , α

2 t ∈ R with α 1 t + α 2 t = 1 it holds that: p t , (α 1 t L t + α 2 t L ′ t )f (9.32) = α 1 t ⟨p t , L t f ⟩ + α 2 t ⟨p t , L ′ t f ⟩ (9.33) = α 1 t ∂ t ⟨p t , f ⟩ + α 2 t ∂ t ⟨p t , f ⟩ (9.34) = (α 1 t + α 2 t )∂ t ⟨p t , f ⟩ (9.35) = ∂ t ⟨p t , f ⟩ , (9.36) i.e., α 1 t L t + α 2 t L ′ t is again a solution of the KFE. A small but important detail is whether α 1 t , α 2 t are positive or negative and whether L t , L ′ t correspond to Markov processes in forward or backward time. This leads to the following Proposition 3 (Combining models). Let p t be a marginal probability path, then the following generators solve the KFE for p t and consequently define a generative model with p t as marginal: 1. Markov superposition: α 1 t L t + α 2 t L ′ t , where L t , L ′ t are two generators of Markov processes solving the KFE for p t , and α 1 t , α 2 t ≥ 0 satisfy α 1 t + α 2 t = 1. 2. Divergence-free components: L t + β t L div t , where L div t

is a generator such that p t , L div t f = 0 for all f ∈ T , and β t ≥ 0. We call such L div t divergence-free.

## 3.

Predictor-corrector: α 1 t L t + α 2 t Lt , where L t is a generator solving the KFE for p t in forward-time and Lt is a generator solving the KFE in backward time, and α 1 t , α 2 t ≥ 0 with α 1 t -α 2 t = 1.

We give examples for a Markov superposition and a divergenc-free component here to illustrate proposition 3. An example of the power of a predictor-corrector scheme can be found in [(Gat et al., 2024)](#b25).

Markov superposition example -combining jump and flow. Markov superpositions can be used to combine generative models of different classes. These can be 2 networks trained separately or we can train two GM models in one network simultaneously [(Holderrieth et al., 2024)](#b33). We illustrate this here by combining a jump model and a flow model on S = R d . Let us assume that we have two models where each of them generates the probability path p t : (1) a flow model u t and (2) a jump model with jump intensity λ t and jump distribution J t . By proposition 3, for α 1 t , α 2 t ≥ 0 with α 1 t + α 2 t = 1, it holds that the following generator defines a valid GM model generating p t :

$L t f (x) = α 1 t L jump t f (x) + α 2 t L flow t f (x) = (α 1 t λ t (x))E Y ∼Jt(•,x) [f (Y ) -f (x)] + ∇f T (x)(α 2 t u t (x))$where we have used equation (8.17) and equation (8.36). In fact, the above generator describes a piecewisedeterministic Markov process, a combined ODE and jump model [(Davis, 1984)](#b17). As the equation above shows, we have to scale the jump intensity by α 1 t and the vector field by α 2 t . We can sample from the resulting GM model with the following sampling procedure: X 0 ∼ p 0 = p X t+h = ∼ J t (dy, X t ) with probability hα 1 t λ t (X t ) X t + hα 2 t u t (X t ) with probability 1 -hα 1 t λ t (X t )

In [(Holderrieth et al., 2024)](#b33), several examples of Markov superpositions of jump and flow are given and shown to lead to performance improvements.

Divergence-free example -MCMC algorithms. To find divergence-free components, one can use existing Markov-Chain Monte-Carlo (MCMC) algorithms -all of these algorithms describe a general recipe to find a divergence-free component. We illustrate this with 2 famous examples. Let us assume that we are given a general probability path p t with density p t (x). Then for a generator L div t to be divergence-free is equivalent to that its adjoint maps p t to zero: p t , L div t f = 0 for all f ∈ T ⇔ [L div t ] * p t (x) = 0 for all x ∈ S (9.37)

First, let us consider S = R d . Langevin dynamics correspond to an SDE with velocity field

1 2 β 2 t ∇ log p t (x) and diffusion coefficient β t , i.e., the dynamics are given via dX t = 1 2 β 2 t ∇ log p t (x)dt + β t dW t (9.38) The adjoint generator of this SDE is given by [L div t ] * p t (i) = -div(p t 1 2 β 2 t ∇ log p t )(x) + 1 2 β 2 t ∆p t (x) (ii) = -1 2 div(β 2 t ∇p t )(x) + 1 2 β 2 t ∆p t (x) (iii) = -1 2 β 2 t ∆p t (x) + 1 2 β 2 t ∆p t (x) = 0

where (i) holds by shape of flow and diffusion adjoint derived in section 8.3.1, (ii) holds because ∇ log p t = ∇p t /p t , and (iii) holds by the identity div∇ = ∆. The above shows that the generator of Langevin dynamics fulfils equation (9.37) and is therefore divergence-free in sense of proposition 3. This fact is widely applied in statistical physics and Markov chain Monte Carlo [(Roberts and Tweedie, 1996)](#b66). Proposition 3 shows that we can add these dynamics for arbitrary β t ≥ 0 to any generative model. In section 10, we use this to derive stochastic sampling for diffusion models.

Second, let S be a general state space again. The Metropolis-Hastings algorithm [(Hastings, 1970)](#b29) wdescribes the construction of a jump process with jump kernel Q t (y, x) that satisfies the detailed balance condition:

Q t (y, x)p t (x) = Q t (x, y)p t (y) for all x, y ∈ S ⇒ [L div t ] * p t (x)

$(i) = Q t (y, x)p t (x) -Q t (x, y)p t (y) = 0$where in (i) we used equation (8.66). This shows that equation (9.37) is fulfilled and Q t is divergence-free.

Proposition 3 shows that one can arbitrarily add such a Metropolis-scheme to any GM model following the probability path p t . [et al., 2022)](#). This assumption allows us to express the conditional distribution of X r given X 0 = z ∈ R d as a Gaussian distribution [(Särkkä and Solin, 2019;](#b72)[Song et al., 2021;](#)[Karras et al., 2022)](#b40): Note that we have discussed such probability paths extensively in section 4.8.3 as affine Gaussian probability paths, i.e., they are constructed via the affine conditional flow (see section 4.8):

ψ t (x|x 1 ) = α t z + σ t x, z ∼ q, x ∼ N (α 0 , σ 2 0 I).

(10.9) Therefore, we can see that:

Forward processes with affine drift coefficients correspond to using Gaussian probability paths (see section 4.8.3) defined by equation (10.8).

Note that for diffusion models in the above time parameterization, there is no finite times r < +∞ at which the marginal pr (x) is an exact Gaussian.

## Training a diffusion model

We now discuss how we can recover the training algorithm of diffusion models as a special case of FM training. In section 4.8, we discussed several options of how to parameterize and train a FM model with Gaussian probability paths (see theorem 7). One particular option was x 0 -prediction where the neural network x θ 0|t is trained to approximate

$x θ 0|t ≈ E[X 0 |X t =$x] via the following training algorithm L CM (θ) = E t,Z∼q,X0∼p ∥x θ 0|t (α t X 0 + σ t Z =Xt ) -X 0 ∥ 2 (i) = E t,Z∼q,Xt∼p t|1 (•|Z) σ 2 t ∥s θ t (X t ) -[-1 σ 2 t (X t -α t Z)]∥ 2 (ii) = E t,Z∼q,Xt∼p t|1 (•|Z) σ 2 t ∥s θ t (X t ) -∇ log p t|1 (X t |Z)∥ 2 ,

where in (i) we reparameterized the neural network via s θ t = -x θ 0|t /σ t and in (ii) we used equation (4.71). The above loss is also called the Denoising Score Matching [(Vincent, 2011)](#b82) loss and is the fundamental loss function with which diffusion models are trained. By theorem 7, we get that the minimizer θ * fulfills that i.e., at minimal loss, s θ t equals the score function ∇ log p t (x) of the marginal probability path. Therefore, the network s θ t is also called the score network. We can therefore summarize:

$s θ * t (x) = - 1 σ t E[X 0 |X t = x]$The training algorithm for diffusion models is equivalent to training a specific FM model with x 0 -prediction. Specifically, in addition to reparameterizing time, it is the same as training a FM model with:

(1) Probability path: Using a Gaussian probability path with independent coupling constructed via an SDE with affine drift coefficients (α t , σ t defined via (10.7)).

(2) Score parameterization: Reparameterizing the marginal velocity field via the score function.

In table [1](#), we list how one can easily convert a score network to other velocity parameterizations. Therefore, different parameterizations are theoretically equivalent and one can even swap parameterizations post-training (see section [4.8.1)](#). Note however, that the score and x 0 prediction parameterizations introduce a singularity at time t = 0 (near noise).

## Sampling

Next, we discuss sampling from a diffusion model and how it relates to sampling from FM or GM model.

Deterministic sampling with ODE. If we consider the diffusion model a FM model, we would sample by sampling from the marginal vector field. In (4.78) we expressed the marginal vector field via the score function (for Gaussian paths): u t (x) = αt α t x -σt σ t -σ[foot_3](#foot_3) t αt α t ∇ log p t (x). (10.11) Using the specific form of equation (10.7) for α t , σ t , one derive the equivalent identity: u t (x) = k(t) a t x -g 2 t 2 ∇ log p t (x) . (10.12) Alternatively, one insert the above u t (x) into the Continuity Equation to validate this directly. The corresponding ODE to u t is also called the Probability Flow ODE (Song et al., 2021): dX t = k(t) a t X t -g 2 t 2 s θ t (X t ) dt, (10.13)

where we set s θ t (x) = ∇ log p t (x) as the learned score function. Note that we use here the notation for ODEs that is common for SDEs for a reason that becomes clear next. We also note that in equation ( [10](#).11) we add the term k(t) compared to [(Song et al., 2021)](#) because of the time reparameterization.

Stochastic sampling with SDE. In section 9.7, we have derived that we can add the Langevin dynamics

1 2 β 2 t ∇ log p t (x)dt + β t dW t (10.14) to any CTMP generative model and we will obtain a generative model following the same probability path. We can apply this to the Probability Flow ODE to get a whole family of SDEs that generate the probability path p t : dX t = k(t)α t X t + β 2 t -k(t)g 2 t 2 ∇ log p t (X t ) dt + β t dW t . (10.15)

The above results in stochastic sampling of a diffusion model. In theory, all models above lead to the same marginals for every β t ≥ 0. This is a mathematical fact about the ground truth underlying stochastic process.

In practice, we need to simulate the SDE

$dX t = k(t)α t X t + β 2 t -k(t)$g 2 t 2 s θ t (X t ) dt + β t dW t (10.16)

with a trained network s θ t . We have both estimation errors (i.e., imperfect training of s θ t ) as well as simulation errors (i.e., imperfect sampling of the underlying SDE). Therefore, there is an optimal unknown noise level β t (see e.g., [(Albergo and Vanden-Eijnden, 2022, equation (2.45)](#))) that can be determined empirically [(Karras et al., 2022)](#b40) and theoretically [(Ma et al., 2024)](#b50). Therefore, we get:

![Figure1Four time-continuous processes (Xt) 0≤t≤1 taking source sample X0 to a target sample X1. These are a flow in a continuous state space, a diffusion in continuous state space, a jump process in continuous state space (densities visualized with contours), and a jump process in discrete state space (states as disks, probabilities visualized with colors).]()

![Figure 2 The Flow Matching blueprint. (a) The goal is to find a flow mapping samples X0 from a known source or noise distribution q into samples X1 from an unknown target or data distribution q. (b) To do so, design a time-continuous probability path (pt) 0≤t≤1 interpolating between p := p0 and q := p1. (c) During training, use regression to estimate the velocity field ut known to generate pt. (d) To draw a novel target sample X1 ∼ q, integrate the estimated velocity field u θ t (Xt) from t = 0 to t = 1, where X0 ∼ p is a novel source sample.]()

![Conditional velocity field ut(x|x1).]()

![Figure 3 Path design in Flow Matching. Given a fixed target sample X = x1, its conditional velocity field ut(x|x1) generates the conditional probability path pt(x|x1). The (marginal) velocity field ut(x) results from the aggregation of all conditional velocity fields-and similarly for the probability path pt(x).]()

![fig, axes = plt.subplots(1, n_steps + 1, figsize=(30, 4), sharex=True, sharey=True) time_steps = torch.linspace(0, 1.0, n_steps + 1)axes[0].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10) axes[0].set_title(f't ={time_steps[0]:.2f}') axes[0].set_xlim(-3.0, 3.0) axes[0].set_ylim(-3.0, 3.0) for i in range(n_steps): x = flow.step(x, time_steps[i], time_steps[i + 1]) axes[i + 1].scatter(x.detach()[:, 0], x.detach()[:, 1], s=10) axes[i + 1].set_title(f't = {time_steps[i + 1]:.2f}') plt.tight_layout() plt.show()]()

![Figure 4 Joint PDF pX,Y (in shades) and its marginals pX and pY (in black lines).]()

![Figure5A flow model Xt = ψt(X0) is defined by a diffeomorphism ψt : R d → R d (visualized with a brown square grid) pushing samples from a source RV X0 (left, black points) toward some target distribution q (right). We show three different times t.]()

![Figure 6 A flow ψt : R d → R d (square grid) is defined by a velocity field ut : R d → R d (visualized with blue arrows) that prescribes its instantaneous movements at all locations. We show three different times t.]()

![Figure 7 A velocity field ut (in blue) generates a probability path pt (PDFs shown as contours) if the flow defined by ut (square grid) reshapes p (left) to pt at all times t ∈ [0, 1).]()

![.sample(x_init=x_0, method='midpoint', step_size=1.0 / num_steps)]()

![Figure 8 The continuity equation asserts that the local change in probability equals minus the net outgoing probability flux.]()

![Figure 10 Bregman divergence.]()

![Training with the conditional flow matching (CFM) loss = ... # The flow_matching library implements the most common probability paths 6 velocity_model: torch.nn.Module = ... # Initialize the velocity model 7 optimizer = torch.optim.Adam(velocity_model.parameters()) 8 9 for x_0, x_1 in dataloader: # Samples from π 0,1 of shape [batch_size, *data_dim] 10 t = torch.rand(batch_size) # Randomize time t ∼ U [0, 1] 11 sample: PathSample = path.sample(t=t, x_0=x_0, x_1=x_1) 12 x_t = sample.x_t 13 dx_t = sample.dx_t # dX_t is ψt(X0|X1).14 # If D is the Euclidean distance, the CFM objective corresponds to the mean-squared error 15 cfm_loss = torch.pow(velocity_model(x_t, t) -dx_t, 2).mean()]()

![Figure 11 Different forms of conditioning in Flow Matching and path design with corresponding conditional flows. When the conditional flows are a diffeomorphism, all constructions are equivalent. When, they are not, extra conditions are required to validate that the marginal velocity generates the marginal path, see text for more details.]()

![.53) Code 5: Examples of affine probability paths in the flow_matching library 1 from flow_matching.path import AffineProbPath, CondOTPath 2 from flow_matching.path.scheduler import ( 3 CondOTScheduler, PolynomialConvexScheduler, LinearVPScheduler, CosineScheduler) Conditional Optimal Transport schedule with αt = t, σt = 1 -t 6 path = AffineProbPath(scheduler=CondOTScheduler()) 7 path = CondOTPath() # Shorthand for the affine path with the CondOTScheduler above 8 Polynomial schedule with αt = t n , σt = 1 -t n 10 path = AffineProbPath(scheduler=PolynomialConvexScheduler(n=1.0)) 11 12 # Linear variance preserving schedule with αt = t, σt = √ 1 -t 2 13 path = AffineProbPath(scheduler=LinearVPScheduler()) 14 15 # Cosine schedule with αt = sin(0.5tπ), σt = cos(0.5tπ) 16 path = AffineProbPath(scheduler=CosineScheduler())]()

![Training an X 1 -prediction model using the Conditional Matching (CM) objective 1 import torch 2 from flow_matching.path import AffineProbPath 3 from flow_matching.solver import ODESolver 4 from flow_matching.utils import ModelWrapper 5 6 path: AffineProbPath = ...]()

![denoiser_model: torch.nn.Module = ... # Initialize the denoiser 8 optimizer = torch.optim.Adam(velocity_model.parameters()) 9 10 for x_0, x_1 in dataloader: # Samples from π 0,1 of shape [batch_size, *data_dim] 11 t = torch.rand(batch_size) # Randomize time t ∼ U [0, 1] 12 sample = path.sample(t=t, x_0=x_0, x_1=x_1) # Sample the conditional path 13 cm_loss = torch.pow(model(sample.x_t, t) -sample.x_1, 2).mean() Convert from denoiser to velocity prediction 19 class VelocityModel(ModelWrapper): 20 def __init__(self, denoiser: nn.Module, path: AffineProbPath): 21 super().__init__(model=denoiser) 22 self.path=path 23 24 def forward(self, x: torch.Tensor, t: torch.Tensor, **extras) -> torch.Tensor: 25 x_1_prediction = super().forward(x, t, **extras) 26 return self.path.target_to_velocity(x_1=x_1_prediction, x_t=x, t=t) .randn(batch_size, *data_dim) # Specify the initial condition 31 solver = ODESolver(velocity_model=velocity_model) 32 num_steps = 100 33 x_1 = solver.sample(x_init=x_0, method='midpoint', step_size=1.0 / num_steps)]()

![Sample the transformed model with the conditional OT schedule20 solver = ODESolver(velocity_model=transformed_model) 21 x_0 = torch.randn(batch_size, *data_dim) # Specify the initial condition 22 solver = ODESolver(velocity_model=velocity_model) 23 num_steps = 100 24 x_1 = solver.sample(x_init=x_0, method='midpoint', step_size=1.0 / num_steps)]()

![Training with geodesic flows on a Sphere using the CFM objective .. # Define a trainable velocity model 7 optimizer = torch.optim.Adam(model.parameters()) 8 loss_fn = torch.nn.MSELoss() # Any Bregman divergence x_1 in dataloader: # Samples from π 0,1 of shape [batch_size, *data_dim] 15 t = torch.rand(batch_size) # Randomize time t ∼ U [0, 1] 16 sample: PathSample = path.sample(t=t, x_0=x_0, x_1=x_1) # Sample the conditional path 17 18 model_output = model(sample.x_t, sample.t) 19 projected_model_output = manifold.proju(sample.x_t, model_output) # Project to tangent space]()

![(x, y) -j t (y, x)] ,]()

![= path.sample(t=t, x_0=x_0, x_1=x_1]()

![Training and sampling DFM with mixture paths and arbitrary data coupling. .. # Define a trainable velocity model 11 optimizer = torch.optim.Adam(model.parameters()) x_1 in dataloader: # Samples from π 0,1 of shape [batch_size, *data_dim] 18 t = torch.rand(batch_size) * (1.0 -1e-3) # Randomize time t ∼ U [0, 1 -10 -3 ] 19 sample: DiscretePathSample = path.sample(t=t, x_0=x_0, x_1=x_1) # Sample the conditional path 20 model_output = model(sample.x_t, sample.t) 21 22 loss = loss_fn(logits=model_output, x_1=sample.x_1, x_t=sample.x_t, t=sample.t) # CDFM loss self, x: torch.Tensor, t: torch.Tensor, **extras) -> torch.Tensor: 30 logits = self.model(x, t, **extras) 31 return torch.nn.functional.softmax(logits.float(), dim=-1)]()

![p t|1 (x|z) = N α t z, σ 2 t I α t = αk(t) , σ t = σk(t) (10.8)]()

E.g., see https://pytorch.org/docs/stable/generated/torch.autograd.functional.vjp.html.

x_1 = solver.sample(x_init=x_0, step_size=step_size, time_grid=torch.tensor([0.0, 1.0-1e-3]))

ODE sampling: For a Gaussian source, independent coupling, fixing α t , σ t according to (10.7), and score parameterization, sampling from a diffusion model with the Probability Flow ODE is the same as sampling from a FM model.

SDE sampling: Under the same conditions, sampling from a diffusion model with stochastic SDE sampling is equivalent to sampling from GM model defined via equation (10.15).

