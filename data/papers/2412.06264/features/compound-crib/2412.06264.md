### Flow Matching (FM) Overview

Flow Matching (FM) is a generative modeling framework that has demonstrated state-of-the-art performance across various domains, including image, video, audio, and biological structures. The core idea behind FM is to learn a transformation that maps samples from a source distribution to a target distribution through a continuous flow defined by a velocity field. This approach allows for efficient sampling and generation of new data points that resemble the target distribution.

### Mathematical Foundation

The mathematical foundation of FM is built upon the concept of a flow defined by an ordinary differential equation (ODE). The flow \( \psi_t \) is governed by the velocity field \( u_t \), which describes how points in the space move over time. The ODE is expressed as:

\[
\frac{d}{dt} \psi_t(x) = u_t(\psi_t(x)), \quad \psi_0(x) = x
\]

This equation states that the rate of change of the flow at any point \( x \) is determined by the velocity field \( u_t \) evaluated at the current position \( \psi_t(x) \). The initial condition \( \psi_0(x) = x \) ensures that the flow starts from the original position.

### Objective

The primary objective of FM is to transform a sample \( X_0 \) drawn from a source distribution \( p \) into a sample \( X_1 = \psi_1(X_0) \) that follows a target distribution \( q \). This transformation is achieved by learning the velocity field \( u_t \) that defines the flow, allowing for the generation of new samples that closely match the desired distribution.

### Flow Matching Recipe

The FM framework follows a two-step recipe:

1. **Choose a Probability Path \( p_t \)**: This path interpolates between the source distribution \( p \) and the target distribution \( q \). The choice of path is crucial as it dictates how the transformation occurs over time.

2. **Train a Velocity Field \( u_t \)**: The velocity field is trained to define the flow transformation. This is typically done using a regression approach, where the model learns to approximate the true velocity field that generates the desired probability path.

### Flow Matching Loss

The loss function used to train the velocity field is defined as:

\[
L_{FM}(\theta) = \mathbb{E}_{t,X_t} \left[ \| u^\theta_t(X_t) - u_t(X_t) \|^2 \right]
\]

Here, \( u^\theta_t(X_t) \) is the predicted velocity field parameterized by \( \theta \), and \( u_t(X_t) \) is the true velocity field. The loss measures the discrepancy between the predicted and true velocity fields, encouraging the model to learn the correct transformation.

### Conditional Flow Matching Loss

An extension of the FM loss is the Conditional Flow Matching Loss, defined as:

\[
L_{CFM}(\theta) = \mathbb{E}_{t,X_t,X_1} \left[ \| u^\theta_t(X_t) - u_t(X_t | X_1) \|^2 \right]
\]

This loss incorporates conditioning on a specific target example \( X_1 \), allowing for more targeted learning of the velocity field based on the relationship between the source and target distributions.

### Key Extensions

FM has several key extensions that broaden its applicability:

- **Discrete Flow Matching**: Adapts FM for Continuous Time Markov Chains (CTMC), enabling its use in discrete generative tasks such as language modeling.

- **Riemannian Flow Matching**: Extends FM to Riemannian manifolds, making it suitable for applications in chemistry, such as protein folding.

- **Generator Matching**: Unifies various generative models under the FM framework, allowing for a more comprehensive understanding of generative processes.

### Diffusion Models

Diffusion models were the first to implement simulation-free training of CTMP processes, utilizing Stochastic Differential Equations (SDEs) to model forward noising processes. In the context of FM, diffusion models construct the probability path \( p_t \) through specific SDEs, which are chosen to have closed-form marginal probabilities.

### Probability Path Construction

For a Gaussian source distribution \( p_0 = N(x|0, I) \), the probability path \( p_t(x) \) can be constructed as:

\[
p_t(x) = \int p_{t|1}(x|x_1) q(x_1) dx_1
\]

where \( p_{t|1}(x|x_1) = N(x|tx_1, (1-t)^2 I) \). This construction allows for a smooth interpolation between the source and target distributions.

### Sampling from Target Distribution

To generate new samples from the target distribution, one can draw a sample from the source distribution \( X_0 \sim p