<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-31">31 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-31">31 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">222BC1CAB48D364A0634FBBF98128A47</idno>
					<idno type="arXiv">arXiv:2312.00752v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p><p>* Alphabetical by first name.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics <ref type="bibr" target="#b11">(Brown et al. 2020;</ref><ref type="bibr" target="#b25">Dosovitskiy et al. 2020;</ref><ref type="bibr" target="#b53">Ismail Fawaz et al. 2019;</ref><ref type="bibr" target="#b77">Oord et al. 2016;</ref><ref type="bibr" target="#b83">Poli et al. 2023</ref>; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer <ref type="bibr" target="#b105">(Vaswani et al. 2017</ref>) and its core attention layer <ref type="bibr" target="#b4">(Bahdanau, Cho, and Bengio 2015)</ref> The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks <ref type="bibr" target="#b103">(Tay, Dehghani, Bahri, et al. 2022)</ref>, but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains.</p><p>Recently, structured state space sequence models (SSMs) <ref type="bibr">(Gu, Goel, and Ré 2022;</ref><ref type="bibr" target="#b39">Gu, Johnson, Goel, et al. 2021</ref>) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models <ref type="bibr" target="#b56">(Kalman 1960)</ref>. This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies <ref type="bibr" target="#b35">(Gu, Dao, et al. 2020</ref>) in certain data modalities, and have dominated benchmarks such as the Long Range Arena <ref type="bibr" target="#b102">(Tay, Dehghani, Abnar, et al. 2021</ref>). Many flavors of SSMs <ref type="bibr">(Gu, Goel, and</ref> Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision <ref type="bibr" target="#b34">(Goel et al. 2022;</ref><ref type="bibr" target="#b74">Nguyen, Goel, et al. 2022</ref>; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text.</p><p>We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.</p><p>Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely.</p><p>Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time-and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3× faster on A100 GPUs).</p><p>Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures <ref type="bibr" target="#b20">(Dao, Fu, Saab, et al. 2023</ref>) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces.</p><p>Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1M.</p><p>We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:</p><p>• Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long (&gt;1M tokens).</p><p>• Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences.</p><p>• Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa <ref type="bibr" target="#b104">(Touvron et al. 2023)</ref>. Our Mamba language model has 5× generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).</p><p>Model code and pre-trained checkpoints are open-sourced at <ref type="url" target="https://github.com/state-spaces/mamba">https://github.com/state-spaces/mamba</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State Space Models</head><p>Structured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection Mechanism</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPU SRAM GPU HBM</head><p>∆ !</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective State Space Model with Hardware-aware State Expansion</head><p>Figure <ref type="figure">1</ref>: (Overview.) Structured SSMs independently map each channel (e.g. 𝐷 = 5) of an input 𝑥 to output 𝑦 through a higher dimensional latent state ℎ (e.g. 𝑁 = 4). Prior SSMs avoid materializing this large effective state (𝐷𝑁 , times batch size 𝐵 and sequence length 𝐿) through clever alternate computation paths requiring time-invariance: the (Δ, 𝑨, 𝑩, 𝑪) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy.</p><p>1-dimensional function or sequence 𝑥 (𝑡) ∈ R ↦ → 𝑦 (𝑡) ∈ R through an implicit latent state ℎ(𝑡) ∈ R 𝑁 .</p><p>Concretely, S4 models are defined with four parameters (Δ, 𝑨, 𝑩, 𝑪), which define a sequence-to-sequence transformation in two stages.</p><p>ℎ ′ (𝑡) = 𝑨ℎ(𝑡) + 𝑩𝑥 (𝑡) (1a)</p><formula xml:id="formula_0">𝑦 (𝑡) = 𝑪ℎ(𝑡)<label>(1b)</label></formula><p>ℎ 𝑡 = 𝑨ℎ 𝑡 -1 + 𝑩𝑥 𝑡 (2a) Discretization. The first stage transforms the "continuous parameters" (Δ, 𝑨, 𝑩) to "discrete parameters" (𝑨, 𝑩) through fixed formulas 𝑨 = 𝑓 𝐴 (Δ, 𝑨) and 𝑩 = 𝑓 𝐵 (Δ, 𝑨, 𝑩), where the pair (𝑓 𝐴 , 𝑓 𝐵 ) is called a discretization rule. Various rules can be used such as the zero-order hold (ZOH) defined in equation <ref type="bibr" target="#b3">(4)</ref>. Commonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are seen one timestep at a time).</p><formula xml:id="formula_1">𝑦 𝑡 = 𝑪ℎ 𝑡<label>(</label></formula><formula xml:id="formula_2">𝑨 = exp(Δ𝑨) 𝑩 = (Δ𝑨) -1 (exp(Δ𝑨) -𝑰 ) • Δ𝑩<label>(4)</label></formula><p>Linear Time Invariance (LTI). An important property of equations ( <ref type="formula" target="#formula_0">1</ref>) to ( <ref type="formula">3</ref>) is that the model's dynamics are constant through time. In other words (Δ, 𝑨, 𝑩, 𝑪), and consequently (𝑨, 𝑩) as well, are fixed for all time-steps. This property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models.</p><p>Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency bottlenecks.</p><p>Structure and Dimensions. Finally, we note that structured SSMs are so named because computing them efficiently also requires imposing structure on the 𝑨 matrix. The most popular form of structure is diagonal <ref type="bibr">(Gu, Gupta, et</ref> al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use. In this case, the 𝑨 ∈ R 𝑁 ×𝑁 , 𝑩 ∈ R 𝑁 ×1 , 𝑪 ∈ R 1×𝑁 matrices can all be represented by 𝑁 numbers. To operate over an input sequence 𝑥 of batch size 𝐵 and length 𝐿 with 𝐷 channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension 𝐷𝑁 per input, and computing it over the sequence length requires 𝑂 (𝐵𝐿𝐷𝑁 ) time and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3. General State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning). Throughout this entire paper we use the term "SSM" to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.</p><p>SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines.</p><p>• Linear attention <ref type="bibr">(Katharopoulos et al. 2020</ref>) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM.</p><p>• H3 <ref type="bibr" target="#b20">(Dao, Fu, Saab, et al. 2023</ref>) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure <ref type="figure">3</ref>). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer.</p><p>• Hyena <ref type="bibr" target="#b83">(Poli et al. 2023</ref>) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021).</p><p>• RetNet (Y. <ref type="bibr" target="#b99">Sun et al. 2023</ref>) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.</p><p>• RWKV (B. <ref type="bibr" target="#b81">Peng et al. 2023</ref>) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. <ref type="bibr" target="#b112">Zhai et al. 2021</ref>). Its main "WKV" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs.</p><p>Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN <ref type="bibr" target="#b10">(Bradbury et al. 2016</ref>), and SRU <ref type="bibr" target="#b63">(Lei et al. 2017</ref>), which we view as the most closely related methods to our core selective SSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Selective State Space Models</head><p>We motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation: Selection as a Means of Compression</head><p>We argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training. However, their effectiveness is limited by how well this state has compressed the context.</p><p>To understand this principle, we focus on two running examples of synthetic tasks (Figure <ref type="figure" target="#fig_2">2</ref>).</p><p>• The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white).</p><p>• The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs <ref type="bibr" target="#b76">(Olsson et al. 2022</ref>). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black).</p><p>These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the (𝑨, 𝑩) transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure <ref type="figure" target="#fig_2">2</ref>). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels.</p><p>In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improving SSMs with Selection</head><p>One method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.</p><p>Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters Δ, 𝑩, 𝑪 functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension 𝐿, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next.</p><p>We specifically choose 𝑠 𝐵 (𝑥) = Linear 𝑁 (𝑥), 𝑠 𝐶 (𝑥) = Linear 𝑁 (𝑥), 𝑠 Δ (𝑥) = Broadcast 𝐷 (Linear</p><p>1 (𝑥)), and 𝜏 Δ = softplus, where Linear 𝑑 is a parameterized projection to dimension 𝑑. The choice of 𝑠 Δ and 𝜏 Δ is due to a connection to RNN gating mechanisms explained in Section 3.5. Input Output ? Output Copying Selective Copying Input Induction Heads Solution Perfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Implementation of Selective SSMs</head><p>Hardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention <ref type="bibr" target="#b4">(Bahdanau, Cho, and Bengio 2015;</ref><ref type="bibr" target="#b105">Vaswani et al. 2017</ref>) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting Δ vary over time in recurrent SSMs <ref type="bibr" target="#b35">(Gu, Dao, et al. 2020</ref>). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Motivation of Prior Models</head><p>We first revisit this motivation and overview our approach to overcome limitations of prior methods.</p><p>• At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs.</p><p>• Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) (Gu, Goel, and Ré 2022; Gu, Johnson, <ref type="bibr" target="#b39">Goel, et al. 2021</ref>). However, this would require computing and materializing the latent state ℎ with shape (B, L, D, N), which is much larger (by a factor of 𝑁 , the SSM state dimension) than the input 𝑥 and output 𝑦 of shape (B, L, D). Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D).</p><p>• Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of 𝑁 (≈ 10 -100), much larger than traditional RNNs, without efficiency penalties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Overview of Selective Scan: Hardware-Aware State Expansion</head><p>The selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:</p><p>• The naive recurrent computation uses 𝑂 (𝐵𝐿𝐷𝑁 ) FLOPs while the convolutional computation uses 𝑂 (𝐵𝐿𝐷 log(𝐿)) FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension 𝑁 , the recurrent mode can actually use fewer FLOPs.</p><p>• The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state ℎ.</p><p>The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state ℎ only in more efficient levels of the memory hierarchy.</p><p>In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input (𝑨, 𝑩) of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters (Δ, 𝑨, 𝑩, 𝑪) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).</p><p>Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention.</p><p>Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A Simplified SSM Architecture</head><p>As with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure <ref type="figure">3</ref>). This is inspired by the gated attention unit (GAU) <ref type="bibr">(Hua et al. 2022</ref>), which did something similar for attention.</p><p>This architecture involves expanding the model dimension 𝐷 by a controllable expansion factor 𝐸. For each block, most of the parameters (3𝐸𝐷 2 ) are in the linear projections (2𝐸𝐷 2 for input projections, 𝐸𝐷 2 for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for Δ, 𝑩, 𝑪, and the matrix 𝑨) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to 𝐸 = 2 in our experiments and use two stacks of the block to match the 12𝐷</p><p>2 parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular "SwiGLU" variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023). Finally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNet's usage of a normalization layer in a similar location (Y. Sun et al. 2023).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Properties of Selection Mechanisms</head><p>The selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or CNNs, to different parameters (e.g. 𝑨 in Algorithm 2), or using different transformations 𝑠 (𝑥).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated MLP Mamba</head><p>Linear projection Sequence transformation</p><formula xml:id="formula_3">Nonlinearity (activation or multiplication) X X X ! X Conv SSM X ! ! Conv SSM ⨂ Figure 3: (Architecture.</formula><p>) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block homogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch. For 𝜎 we use the SiLU / Swish activation (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Connection to Gating Mechanisms</head><p>We highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of Gu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in Appendix C). More broadly, Δ in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms.</p><p>Theorem 1. When 𝑁 = 1, 𝑨 = -1, 𝑩 = 1, 𝑠 Δ = Linear(𝑥), and 𝜏 Δ = softplus, then the selective SSM recurrence (Algorithm 2) takes the form 𝑔 𝑡 = 𝜎 (Linear(𝑥 𝑡 ))</p><formula xml:id="formula_4">ℎ 𝑡 = (1 -𝑔 𝑡 )ℎ 𝑡 -1 + 𝑔 𝑡 𝑥 𝑡 .<label>(5)</label></formula><p>As mentioned in Section 3.2, our specific choices of 𝑠 Δ , 𝜏 Δ is from this connection. In particular, note that if a given input 𝑥 𝑡 should be completely ignored (as necessary in the synthetic tasks), all 𝐷 channels should ignore it, and so we project the input down to 1 dimension before repeating/broadcasting with Δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Interpretation of Selection Mechanisms</head><p>We elaborate on three particular mechanistic effects of selection.</p><p>Variable Spacing. Selectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest. This is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for discrete data -for example the presence of language fillers such as "um". This property arises because the model can mechanistically filter out any particular input 𝑥 𝑡 , for example in the gated RNN case (Theorem 1) when 𝑔 𝑡 → 0.</p><p>Filtering Context. It has been empirically observed that many sequence models do not improve with longer context (F. <ref type="bibr" target="#b28">Shi et al. 2023)</ref>, despite the principle that more context should lead to strictly better performance. An explanation is that many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are global convolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g. Section 4.3.2).</p><p>Boundary Resetting. In settings where multiple independent sequences are stitched together, Transformers can keep them separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences. Selective SSMs can also reset their state at boundaries (e.g. Δ 𝑡 → ∞, or Theorem 1 when 𝑔 𝑡 → 1). These settings may occur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in reinforcement learning <ref type="bibr" target="#b67">(Lu et al. 2023)</ref>).</p><p>Additionally, we elaborate on effects of each selective parameter.</p><p>Interpretation of Δ. In general, Δ controls the balance between how much to focus or ignore the current input 𝑥 𝑡 . It generalizes RNN gates (e.g. 𝑔 𝑡 in Theorem 1): mechanically, a large Δ resets the state ℎ and focuses on the current input 𝑥, while a small Δ persists the state and ignores the current input. SSMs ( <ref type="formula" target="#formula_0">1</ref>)-( <ref type="formula">2</ref>) can be interpreted as a continuous system discretized by a timestep Δ, and in this context the intuition is that large Δ → ∞ represents the system focusing on the current input for longer (thus "selecting" it and forgetting its current state) while a small Δ → 0 represents a transient input that is ignored.</p><p>Interpretation of 𝑨. We remark that while the 𝑨 parameter could also be selective, it ultimately affects the model only through its interaction with Δ via 𝑨 = exp(Δ𝑨) (the discretization ( <ref type="formula" target="#formula_2">4</ref>)). Thus selectivity in Δ is enough to ensure selectivity in (𝑨, 𝑩), and is the main source of improvement. We hypothesize that making 𝑨 selective in addition to (or instead of) Δ would have similar performance, and leave it out for simplicity.</p><p>Interpretation of 𝑩 and 𝑪. As discussed in Section 3.1, the most important property of selectivity is filtering out irrelevant information so that a sequence model's context can be compressed into an efficient state. In an SSM, modifying 𝑩 and 𝑪 to be selective allows finer-grained control over whether to let an input 𝑥 𝑡 into the state ℎ 𝑡 , or the state into the output 𝑦 𝑡 . These can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input) and context (hidden states) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Additional Model Details</head><p>Real vs. Complex. Most prior SSMs use complex numbers in their state ℎ, which is necessary for strong performance on many tasks in perceptual modalities (Gu, Goel, and Ré 2022). However, it has been empirically observed that completely real-valued SSMs seem to work fine, and possibly even better, in some settings <ref type="bibr" target="#b69">(Ma et al. 2023</ref>). We use real values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoff is related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio, video) but not discrete (e.g. text, DNA).</p><p>Initialization. Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can help in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real case is S4D-Real <ref type="bibr">(Gu, Gupta, et al. 2022)</ref>, which is based on the HIPPO theory <ref type="bibr" target="#b35">(Gu, Dao, et al. 2020</ref>). These define the 𝑛-th element of 𝑨 as -1/2 + 𝑛𝑖 and -(𝑛 + 1) respectively. However, we expect many initializations to work fine, particularly in the large-data and real-valued SSM regimes; some ablations are considered in Section 4.6.</p><p>Parameterization of Δ. We defined the selective adjustment to Δ as 𝑠 Δ (𝑥) = Broadcast 𝐷 (Linear 1 (𝑥)), which was motivated by the mechanics of Δ (Section 3.5). We observe that it can be generalized from dimension 1 to a larger dimension R. We set this to be a small fraction of D, which uses a negligible number of parameters compared to the main Linear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another Linear projection, initialized to a specific pattern of 1's and 0's; if this projection is trainable, this leads to the alternative 𝑠 Δ (𝑥) = Linear 𝐷 (Linear 𝑅 (𝑥)), which can be viewed as a low-rank projection.</p><p>In our experiments, the Δ parameter (which can be viewed as a bias term) is initialized to 𝜏 -1 Δ (Uniform( [0.001, 0.1])), following prior work on SSMs <ref type="bibr" target="#b40">(Gu, Johnson, Timalsina, et al. 2023</ref>).</p><p>Remark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they are S4 models with a selection mechanism and computed with a scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>In Section 4.1 we test Mamba's ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on three domains, each evaluated on autoregressive pretraining as well as downstream tasks.</p><p>• Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.</p><p>• Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.</p><p>• Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.</p><p>Finally, Section 4.5 shows Mamba's computational efficiency at both training and inference time, and Section 4.6 ablates various components of the architecture and selective SSMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Tasks</head><p>Full experiment details for these tasks including task details and training protocol are in Appendix E.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Selective Copying</head><p>The Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test the memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and global convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by constructing a convolution kernel of exactly the right length (Figure <ref type="figure" target="#fig_2">2</ref>). This was explicitly validated in earlier work on global convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising task <ref type="bibr" target="#b55">(Jing et al. 2019</ref>).</p><p>Note that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with "data-dependence" and solve related tasks <ref type="bibr" target="#b20">(Dao, Fu, Saab, et al. 2023;</ref><ref type="bibr" target="#b83">Poli et al. 2023</ref>). However, we find this explanation insufficient intuitively because such gating does not interact along the sequence axis, and cannot affect the spacing between tokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A).</p><p>Table <ref type="table">1</ref> confirms that gated architectures such as H3 and Mamba only partially improve performance, while the selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Induction Heads</head><p>Induction heads <ref type="bibr" target="#b76">(Olsson et al. 2022</ref>) is a simple task from the mechanistic interpretability lens <ref type="bibr" target="#b26">(Elhage et al. 2021</ref>) that is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy: for example, if the model has seen a bigram such as "Harry Potter" in the sequence, then the next time "Harry" appears in the same sequence, the model should be able to predict "Potter" by copying from history.</p><p>Dataset. We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths from 2 6 = 64 up to 2 20 = 1048576 at test time.</p><p>Models. Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically solve the induction heads task <ref type="bibr" target="#b76">(Olsson et al. 2022</ref>). We test both multi-head attention (8 heads, with various positional encodings) and SSM variants. We use a model dimension 𝐷 of 64 for Mamba and 128 for the other models.</p><p>Results. Table <ref type="table" target="#tab_6">2</ref> shows that Mamba-or more precisely, its selective SSM layer-has the ability to solve the task perfectly because of its ability to selectively remember the relevant token while ignoring everything else in between. It generalizes perfectly to million-length sequences, or 4000× longer than it saw during training, while no other method goes beyond 2×.</p><p>Model Arch. Layer Acc. S4 No gate S4 18.3 -No gate S6 97.0 H3 H3 S4 57.0 Hyena H3 Hyena 30.1 -H3 S6 99.7 -Mamba S4 56.4 -Mamba Hyena 28.4 Mamba Mamba S6 99.8 Table 1: (Selective Copying.) Accuracy for combinations of architectures and inner sequence layers. 8IWX7IUYIRGI0IRKXL %GGYVEG] -RHYGXMSR,IEHW)\XVETSPEXMSR 1,%%FWSPYXI 1,%6S4) 1,%\4SW , ,]IRE 1EQFE 6ERHSQ 8VEMR0IRKXL Figure <ref type="figure">4</ref>: (Scaling Laws.) Models of size ≈ 125𝑀 to ≈ 1.3𝐵 parameters, trained on the Pile. Mamba scales better than all other attention-free models and is the first to match the performance of a very strong "Transformer++" recipe that has now become standard, particularly as the sequence length grows.</p><p>Out of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly better than the others; also note that all attention models were only tested up to sequence length 2 14 = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>We evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3 specifications. We use the Pile dataset (L.</p><p>Gao, Biderman, et al. 2020), and follow the training recipe described in Brown et al. (2020). All training details are in Appendix E.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Scaling Laws</head><p>For baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We also compare against other recent subquadratic architectures (Figure <ref type="figure">4</ref>). All model details are in Appendix E.2.</p><p>Figure <ref type="figure">4</ref> shows scaling laws under the standard Chinchilla <ref type="bibr" target="#b51">(Hoffmann et al. 2022</ref>) protocol, on models from ≈ 125𝑀 to ≈ 1.3𝐵 parameters. Mamba is the first attention-free model to match the performance of a very strong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length grows. (We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that can also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic computation requirements.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Downstream Evaluations</head><p>Table <ref type="table" target="#tab_9">3</ref> shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare against the most well-known open source models at these sizes, most importantly Pythia <ref type="bibr">(Biderman et</ref> al. 2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length 1024.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DNA Modeling</head><p>Motivated by the success of large language models, there has been recent exploration into using the foundation model paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a finite vocabulary. It is also known for requiring long-range dependencies to model <ref type="bibr" target="#b1">(Avsec et al. 2021</ref>). We investigate Mamba as a FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA <ref type="bibr">(Nguyen, Poli, et al. 2023)</ref>. In particular, we focus on two explorations of scaling laws across model size and sequence length (Figure <ref type="figure" target="#fig_3">5</ref>), and a difficult downstream synthetic classification task requiring long context (Figure <ref type="figure">6</ref>).</p><p>For pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and model details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA <ref type="bibr">(Nguyen, Poli, et al. 2023</ref>), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base pairs) in the training split. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Scaling: Model Size</head><p>In this experiment, we investigate the scaling properties of genomics foundation models with various model backbones (Figure <ref type="figure" target="#fig_3">5</ref> Left).</p><p>Training. To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect results to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 2 20 ≈ 1𝑀 tokens per batch. Models were trained for 10𝐾 gradient steps for a total of 10𝐵 tokens.</p><p>Results. Figure <ref type="figure" target="#fig_3">5</ref> (Left) shows that Mamba's pretraining perplexity improves smoothly with model size, and that Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of ≈ 40𝑀 parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with roughly 3× to 4× fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Scaling: Context Length</head><p>In the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence lengths. We pretrain models on sequence lengths 2 10 = 1024, 2 12 = 4096, 2 14 = 16384, 2 16 = 65536, 2 18 = 262144, 2 20 = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20𝐾 gradient steps for a total of ≈ 330𝐵 tokens. The longer sequence lengths used sequence length warmup similar to <ref type="bibr">(Nguyen, Poli, et al. 2023</ref>).</p><p>Results. Figure <ref type="figure" target="#fig_3">5</ref> (Right) shows that Mamba is able to make use of longer context even up to extremely long sequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand, the HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a very long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not control for computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Synthetic Species Classification</head><p>We evaluate models on a downstream task of classifying between 5 different species by randomly sampling a contiguous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}.</p><p>We modify the task to be significantly more challenging by classifying between the five great apes species {human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Audio Modeling and Generation</head><p>For the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols <ref type="bibr" target="#b34">(Goel et al. 2022</ref>). This model comprises:</p><p>1. a U-Net backbone with two stages of pooling by a factor 𝑝 that doubles the model dimension 𝐷 per stage, 2. alternating S4 and MLP blocks in each stage.</p><p>We consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Long-Context Autoregressive Pretraining</head><p>We evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standard piano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretraining details largely follow the standard language modeling setup (Section 4.2). Figure <ref type="figure" target="#fig_10">7</ref> evaluates the effect of increasing training sequence lengths from 2 13 = 8192 to 2 20 ≈ 10 6 , while keeping computation fixed. (There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were available so the maximum sequence length is actually bounded by 60𝑠 • 16000𝐻𝑧 = 960000.)</p><p>Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is better throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a constant factor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.</p><p>We note one important detail: this is the only experiment in this paper in which we switched from the real parameterization to complex (Section 3.6). We show additional ablations in Appendix E.4. Table 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art (and much larger) GANand diffusion-based models. A larger model parameter-matched to the baselines further improves on fidelity metrics dramatically. Table 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba &gt; S4+MLP &gt; MHA+MLP in the center blocks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Autoregressive Speech Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Ablations</head><p>We perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with size ≈ 350M models at Chinchilla token counts (same setting as Figure <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Architecture</head><p>Table <ref type="table" target="#tab_14">6</ref> investigates the effects of the architecture (block) and its inner SSM layer (Figure <ref type="figure">3</ref>). We find that • Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.</p><p>• Replacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.</p><p>• Replacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of Section 3. • The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective layer).</p><p>We also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid attention architecture) in Appendix E.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Selective SSM</head><p>Table <ref type="table">7</ref> ablates the selective SSM layer by considering different combinations of selective Δ, 𝑩, and 𝑪 parameters (Algorithm 2), showing that Δ is the most important parameter due to its connection to RNN gating (Theorem 1).</p><p>Table <ref type="table">8</ref> considers different initializations of the SSM, which have been shown to make a large difference in some data modalities and settings (Gu, Goel, and Ré 2022; Gu, Gupta, et al. 2022). On language modeling, we find that simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with findings from prior work <ref type="bibr" target="#b72">(Mehta et al. 2023)</ref>.</p><p>Table <ref type="table" target="#tab_15">9</ref> and Table <ref type="table" target="#tab_16">10</ref> consider varying the dimension of the Δ and (𝑩, 𝑪) projections respectively. Changing them from static to selective provides the most benefit, while increasing the dimensions further generally improves performance modestly with a small increase in parameter count.</p><p>Of particular note is the dramatic improvement of the selective SSM when the state size 𝑁 is increased, with over a 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1 and 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We discuss related work, limitations, and some future directions.</p><p>Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion: Selection Mechanism</head><p>Our selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to "fast weights" (J. <ref type="bibr" target="#b2">Ba et al. 2016;</ref><ref type="bibr" target="#b93">Schmidhuber 1992</ref>), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying.</p><p>Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. <ref type="bibr" target="#b16">Chung et al. 2014)</ref>, or the gated equation ( <ref type="formula" target="#formula_4">5</ref>) in Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension.</p><p>However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures <ref type="bibr">(Hua et</ref> al. 2022; Mehta et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually have a very different semantic meaning. Hypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber 1992). Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023).</p><p>Example: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer 𝑦 = 𝑫𝑥, where 𝑫 is a diagonal weight parameter. Now suppose that 𝑫 is itself generated from a linear transformation of 𝑥, with an optional nonlinearity: 𝑫 = 𝜎 (𝑾𝑥). Since it is diagonal, the multiplication becomes an elementwise product:</p><formula xml:id="formula_5">𝑦 = 𝜎 (𝑾𝑥) • 𝑥.</formula><p>This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative "branch"), hypernetworks (since the parameter 𝑫 is generated by another layer), and data-dependent (since 𝑫 depends on the data 𝑥). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.</p><p>Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions-essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as well-and we find it uninformative to think of them as such.</p><p>Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of Δ ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Related Work</head><p>We overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 S4 Variants and Derivatives</head><p>We describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method.</p><p>• S4 (Gu, Goel, and Ré 2022; Gu, Johnson, <ref type="bibr" target="#b39">Goel, et al. 2021</ref>) introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) <ref type="bibr" target="#b35">(Gu, Dao, et al. 2020</ref>).</p><p>• DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D <ref type="bibr">(Gu, Gupta, et al. 2022</ref>).</p><p>• S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism.</p><p>Lu et al. ( <ref type="formula">2023</ref>) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where 𝑨 is manually set to 0, instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries.</p><p>• Mega <ref type="bibr" target="#b69">(Ma et al. 2023</ref>) introduced a simplification of S4 to be real-instead of complex-valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components.</p><p>• Liquid S4 <ref type="bibr" target="#b45">(Hasani et al. 2023</ref>) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI.</p><p>• Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SSM Architectures</head><p>We use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer.</p><p>• GSS <ref type="bibr" target="#b72">(Mehta et al. 2023</ref>) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. ( <ref type="formula">2022</ref>) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1.</p><p>• Mega <ref type="bibr" target="#b69">(Ma et al. 2023</ref>) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation.</p><p>• H3 <ref type="bibr" target="#b20">(Dao, Fu, Saab, et al. 2023</ref>) is motivated by combining S4 with linear attention <ref type="bibr">(Katharopoulos et al. 2020)</ref>. It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures.</p><p>• Selective S4 (J. <ref type="bibr" target="#b107">Wang et al. 2023</ref>) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the "selection" name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective</p><p>Copying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).</p><p>• RetNet (Y. <ref type="bibr" target="#b99">Sun et al. 2023</ref>) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a special case where the state dimension is 𝑁 = 1. Although not framed as such, its recurrence can be viewed as a special case of a linear SSM.</p><p>Its primary source of improvement is using a linear attention with large head dimension, which can be viewed as another method to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention variants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.</p><p>RetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention instead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.</p><p>• RWKV (B. <ref type="bibr" target="#b81">Peng et al. 2023</ref>) is another recent RNN designed for language modeling. It is based on AFT (attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main "WKV" mechanism involves LTI recurrences and can be seen as the ratio of two SSMs.</p><p>We also highlight the gated attention unit (GAU) from Hua et al. ( <ref type="formula">2022</ref>), which was motivated by combining the Transformer's MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Relationship to RNNs</head><p>RNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state. Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Several older</head><p>• They do not use state expansion (𝑁 = 1) or selective 𝑩, 𝑪 parameters, both of which are important for performance (Section 4.6).</p><p>• They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and initializations (Section 3.6).</p><p>Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem <ref type="bibr" target="#b48">(Hochreiter 1991</ref>; Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The former could be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latter was difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Gupta, Mehta, and Berant 2022; Kaul 2020; Orvieto et al. 2023)). We also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaff, Szlam, and LeCun 2016; Lezcano-Casado and Martínez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017</p><p>) which are motivated by constraining the 𝑨 transition matrix to be orthogonal or unitary, in order to control its eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying task <ref type="bibr" target="#b55">(Jing et al. 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Linear Attention</head><p>The Linear Attention (LA) <ref type="bibr">(Katharopoulos et</ref> al. 2020) framework is an important result popularizing kernel attention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other modifications. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and Recht 2007). Performer (Choromanski et al. 2021) finds an approximation to the exponential kernel involving only positive features, which also allows the softmax normalization term. TransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed that the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed numerator).</p><p>Aside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. ( <ref type="formula">2022</ref>) offers an extensive categorization of many of these.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Long Context Models</head><p>Long context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences. However, these are often from a computational standpoint and have not been extensively validated. These include:</p><p>• Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result is similar to our Induction Heads extrapolation experiment (Table <ref type="table" target="#tab_6">2</ref>).</p><p>• LongNet <ref type="bibr" target="#b23">(Ding et al. 2023)</ref>, which claimed to scale to 1B length but only evaluated on length &lt; 100𝐾 for actual tasks.</p><p>• Hyena and HyenaDNA <ref type="bibr">(Nguyen, Poli, et al. 2023;</ref><ref type="bibr" target="#b83">Poli et al. 2023)</ref>, which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality improvements at 1M context are due to context length or due to more data and computation.</p><p>• Sparse Transformer <ref type="bibr" target="#b13">(Child et al. 2019</ref>) showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length 2 20 = 1048576, although did not discuss performance tradeoffs when controlling for computation and model size.</p><p>In contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performance with longer context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Mechanics of Selective SSMs</head><p>Proof of Theorem 1. Consider a selective SSM (Algorithm 2) with 𝑁 = 1, 𝑨 = -1, 𝑩 = 1, 𝑠 Δ = Linear(𝑥), 𝜏 Δ = softplus. The corresponding continuous-time SSM (1) is</p><formula xml:id="formula_6">ℎ(𝑡) = -ℎ(𝑡) + 𝑥 (𝑡)</formula><p>which is also called a leaky integrator.</p><p>The discretization step size is</p><formula xml:id="formula_7">Δ 𝑡 = 𝜏 Δ (Parameter + 𝑠 Δ (𝑥 𝑡 ))</formula><p>= softplus(Parameter + Linear(𝑥 𝑡 ))</p><p>= softplus(Linear(𝑥 𝑡 ))</p><p>where we observe that the parameter can be viewed as a learnable bias and folded into the linear projection. Now applying the zero-order hold (ZOH) discretization formulas:</p><formula xml:id="formula_8">𝑨 𝑡 = exp(Δ𝑨) = 1 1 + exp(Linear(𝑥 𝑡 )) = 𝜎 (-Linear(𝑥 𝑡 )) = 1 -𝜎 (Linear(𝑥 𝑡 )) 𝑩 𝑡 = (Δ𝑨) -1 (exp(Δ𝑨) -𝑰 ) • Δ𝑩 = -(exp(Δ𝑨) -𝑰 ) = 1 -𝑨 = 𝜎 (Linear(𝑥 𝑡 )).</formula><p>Thus the final discrete recurrence (2a) is</p><formula xml:id="formula_9">𝑔 𝑡 = 𝜎 (Linear(𝑥 𝑡 )) ℎ 𝑡 = (1 -𝑔 𝑡 )ℎ 𝑡 -1 +</formula><p>𝑔 𝑡 𝑥 𝑡 as desired. □ D Hardware-aware Algorithm For Selective SSMs Without input-dependent selectivity, SSMs can be efficiently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu, Goel, and Ré 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically efficient (𝑂 (𝐵𝐿𝐷𝑁 ) FLOPs, scaling linear in 𝐿), training foundation models with selective SSMs requires them to be efficient on modern hardware (GPUs) as well. We describe how we use kernel fusion and recomputation to make SSM scan fast and memory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5, showing that it is up to 7× times faster than attention at sequence length 32K, and is as memory-efficient as the best attention implementation (FlashAttention). Speed. On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memorybandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared to a standard implementation.</p><p>The standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input 𝑨, 𝑩 of size (𝐵, 𝐿, 𝐷, 𝑁 ) in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to write the scan output of size (𝐵, 𝐿, 𝐷, 𝑁 ) to GPU HBM, then multiply that scan output with 𝑪 to produce an output of size (𝐵, 𝐿, 𝐷). However, this requires the number of memory reads/writes on the order of 𝑂 (𝐵𝐿𝐷𝑁 ). We can instead fuse the discretization step, the scan, and the multiplication with 𝑪 into one kernel:</p><p>1. We read in 𝑂 (𝐵𝐿𝐷 + 𝐷𝑁 ) bytes of memory (Δ, 𝑨, 𝑩, 𝑪) from slow HBM to fast SRAM.</p><p>2. We discretize to produce 𝑨, 𝑩 of size (𝐵, 𝐿, 𝐷, 𝑁 ) in SRAM.</p><p>3. We perform a parallel associative scan, yielding intermediate states of size (𝐵, 𝐿, 𝐷, 𝑁 ) in SRAM.</p><p>4. We multiply and sum with 𝑪, producing outputs of size (𝐵, 𝐿, 𝐷) and write it to HBM.</p><p>This way, we reduce IOs by a factor of 𝑂 (𝑁 ) (the state dimension), which in practice speeds up the operation by 20-40 times (Section 4.5).</p><p>For sequence length 𝐿 too long where we cannot fit the sequence in SRAM (which is much smaller than HBM), we split the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can continue the scan with the next chunk.</p><p>Memory. We describe how we use the classical technique of recomputation to reduce the total amount of memory required to train selective SSM layers.</p><p>From the way we fuse the forward pass, we do not save the intermediate states of size (𝐵, 𝐿, 𝐷, 𝑁 ) to avoid memory blowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those intermediate states in the backward pass. Since the inputs Δ, 𝑨, 𝑩, 𝑪 and output gradient read from HBM to SRAM are of size 𝑂 (𝐵𝐿𝑁 + 𝐷𝑁 ), and the input gradients are also of size 𝑂 (𝐵𝐿𝑁 + 𝐷𝑁 ), recomputation avoids the cost of reading 𝑂 (𝐵𝐿𝑁 𝐷) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up the computation compared to storing them and reading them from HBM.</p><p>Beyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection).</p><p>In particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details and Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Synthetic Tasks</head><p>Selective Copying. Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including the white "noise" token from Figure <ref type="figure" target="#fig_2">2</ref>) and requiring models to memorize 16 "data" tokens. We use 2 layer models with a model dimension of 𝐷 = 64. Models are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Induction Heads.</head><p>Training consists of randomly generating data every step, with a batch size of 8. We choose an "epoch" size of 8192 steps, and track the accuracy on fixed validation sets (also randomly generated) of each target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 × 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 × 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failed to improve further.</p><p>We use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2𝑒 -4 and 1𝑒 -3, and the better results are reported for each model (2𝑒 -4 for all models except Mamba). The attention and Hyena models did not learn at LR 1𝑒 -3. H3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of 2𝑒 -4. Mamba learned at both LRs, but extrapolated better at the larger LR of 1𝑒 -3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Language Modeling E.2.1 Scaling Law Details</head><p>Scaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tokenizer.</p><p>Model Sizes. Table <ref type="table" target="#tab_22">12</ref> specifies the model sizes we use for scaling laws. This is taken directly from the GPT3 specifications <ref type="bibr" target="#b11">(Brown et al. 2020)</ref>, with very minor modifications. First, we changed the batch size of the 1.3B model from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling laws <ref type="bibr" target="#b51">(Hoffmann et al. 2022)</ref>, which specify that training tokens should increase proportionally to model size.</p><p>Training Recipes. All models used the AdamW optimizer with • gradient clip value 1.0</p><p>• weight decay 0.1</p><p>• no dropout</p><p>• linear learning rate warmup with cosine decay By default, the peak learning rate is the GPT3 specification.</p><p>We give several models an "improved recipe", inspired by changes adopted by popular large language models such as PaLM <ref type="bibr" target="#b15">(Chowdhery et al. 2023</ref>) and LLaMa <ref type="bibr" target="#b104">(Touvron et al. 2023</ref>). These include:</p><p>• linear learning rate warmup with cosine decay to 1𝑒 -5, with a peak value of 5× the GPT3 value</p><p>• no linear bias terms</p><p>• RMSNorm instead of LayerNorm</p><p>• AdamW hyperparameter 𝛽 = (.9, .95) (the GPT3 value) instead of the PyTorch default of 𝛽 = (.9, .999)</p><p>Architecture and Training Details. Our models are:</p><p>• Transformer: The standard Transformer based on GPT3 (Table <ref type="table" target="#tab_22">12</ref>).</p><p>• Transformer++: A Transformer with an improved architecture, namely rotary positional encodings <ref type="bibr" target="#b98">(Su et al. 2021</ref>) and SwiGLU MLP (Shazeer 2020), and the improved training recipe above.</p><p>• Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly increased by 1.5× to preserve parameter count.</p><p>• H3++: The H3 architecture with a few modifications, including (i) using the same "thin" Hyena dimensions above (ii) the improved training recipe above (iii) a linear attention head dimension of 8.</p><p>• RWKV: The default RWKV model from B. Peng et al. ( <ref type="formula">2023</ref>), including its modified MLP block. We also used as much of its specified training recipe as possible, such as increasing the learning rates by 2× or 3× on certain parameters.</p><p>• RetNet: The default RetNet model from Y. <ref type="bibr" target="#b99">Sun et al. (2023)</ref>. We also gave it the improved training recipe above.</p><p>• Mamba: The standard Mamba architecture, with the improved training recipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 Additional Scaling Law Ablations</head><p>We perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in Figure <ref type="figure">4</ref> (Left).</p><p>Mamba Architecture: Interleaving Blocks. We test the effect of different architectural blocks combined with the Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra conv → SSM path added. This leads to two natural ablations:</p><p>• What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be interpreted as taking Mamba and removing half of the SSMs.</p><p>*034WPSKWGEPI 4IVTPI\MX]PSKWGEPI 7GEPMRK0E[WSR8LI4MPI7IUYIRGI0IRKXL 1EQFE 1EQFE104 1EQFE1,% *034WPSKWGEPI 4IVTPI\MX]PSKWGEPI 7GEPMRK0E[WSR8LI4MPI7IUYIRGI0IRKXL ,]IRE ,]IRE , , Figure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of • What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks. Figure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022). H3 Architecture: Training Recipes. Next we ablate differences between the Hyena and H3++ models, our weakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.</p><p>• Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure <ref type="figure">4</ref>).</p><p>• Hyena+: The same architecture but with the improved training recipe described above.</p><p>• H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.</p><p>• H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside the SSM recurrence but does not increase parameters.</p><p>Our general convention is that "Model+" represents the base model with the improved training recipe, and "Model++" also allows for architectural changes.</p><p>Figure <ref type="figure">9</ref> (Right) shows that</p><p>• A large improvement is achieved by the improved training recipe, which was used for many of the models in the main Figure <ref type="figure">4</ref> (RetNet, H3++, Transformer++, Mamba).</p><p>• The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.</p><p>• The head dimension expansion improves performance, consistent with one of our main themes that expanded state dimension improves performance for SSMs (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.3 Downstream Evaluation Details</head><p>This pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoX tokenizer <ref type="bibr" target="#b8">(Black et al. 2022</ref>) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent with the GPT3 specifications. We report the perplexity on the Pile validation set, and for this metric only compare to models trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.</p><p>For downstream evaluation, we use the LM evaluation harness from EleutherAI (L.</p><p>Gao, Tow, et al. 2021), as done by most work in this area. We evaluate on the following tasks/datasets that measure common sense reasoning: • LAMBADA (Paperno et al. 2016) • HellaSwag (Zellers et al. 2019) • PIQA (Bisk et al. 2020) • ARC-challenge (P. Clark et al. 2018) • ARC-easy: an easy subset of ARC-challenge • WinoGrande (Sakaguchi et al. 2021) We report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task). E.3 DNA Modeling E.3.1 Pretraining Details We describe the dataset and training procedure of the HG38 pretraining task in more detail. The dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a total of 𝑆 = 34021 segments of length 2 17 = 131072 that cover the genome, for a total of approximately 4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if necessary (e.g. to get longer segments).</p><p>We deviate from HyenaDNA when the training sequence length is not 2 17 . HyenaDNA always takes a fixed sub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is fixed to 34021 samples and doesn't necessarily go through the whole genome. On the other hand, we use the entire training data:</p><p>• When the context length 𝐿 is less than (or equal to) 2 17 , we divide up each segment into non-overlapping sub-segments of length 𝐿, so that there are 𝑆 × 2 17 𝐿 total samples and 𝑆 × 2 17 ≈ 4.5𝐵 tokens per epoch. • When the context length 𝐿 is greater than 2 17 , we turn each segment into two samples, one that begins with the prescribed segment and one that ends with the prescribed segment. Thus each epoch has 2𝑆 items and 2𝑆𝐿 tokens per epoch. For example, at sequence length 2 18 = 262144 there are 4× as many tokens as the default, and at sequence length 2 20 there are 16× as many tokens.</p><p>Other training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For example, we use the AdamW with (𝛽 1 , 𝛽 2 ) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning rate scheduler with linear warmup for 10% of total steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.2 Scaling: Model Size Details</head><p>Models. The models we consider are:</p><p>• Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings <ref type="bibr" target="#b98">(Su et al. 2021)</ref>. Informally, we found these to be noticeably better than vanilla positional encodings from <ref type="bibr" target="#b105">(Vaswani et al. 2017</ref>).</p><p>• HyenaDNA: the Hyena model from Nguyen</p><p>, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP. • Mamba: the standard Mamba architecture. Model Sizes. We use the following model sizes. Blocks 4 5 6 7 8 10 12 Model Dimension 64 96 128 192 256 384 512 Params (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M</p><p>Note that the number of blocks for Mamba is doubled, because one Transformer "layer" includes both the MHA and MLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).  Results for the Species classification task are in Table <ref type="table" target="#tab_26">13</ref>. Dataset. The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Audio Details</head><p>The dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16, and we want the resulting sequence length to be a a multiple of 8 for hardware efficiency, the longest possible sequence is 468 × 2048 = 958464. The rest of our sequence lengths are defined by successively halving this and rounding up to the nearest multiple of 2048.</p><p>Table 14 lists the specifications used in Training. Models were trained for 200𝐾 training steps with a maximum learning rate of 0.002, 20𝐾 (10%) warmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).</p><p>Additional Ablations: SSM Parameterizations. We investigate SSM parameterizations on long-form audio waveform pretraining in the setting of Figure <ref type="figure" target="#fig_10">7</ref>. The setting is modified slightly to use larger models (8 layers and 𝐷 = 64 for 6M params, the SaShiMi default), shorter sequences (2 11 = 2048 to 2 18 = 262144 instead of 2 13 to 2 20 ), lower LR (0.001 from 0.002), and shorter training cycles (100K instead of 200K steps).</p><p>Figure <ref type="figure" target="#fig_11">10</ref> shows that the change from S4 → S6 (i.e. the selection mechanism) is not always beneficial. On long-form audio waveforms, it in fact significantly hampers performance, which may be intuitive from the point of view that audio is uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.</p><p>After ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.</p><p>However, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The performance differences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio signal should be LTI, but once they are "tokenized" and compressed by the outer layers, the inner layers no longer need to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4.2 SC09 Speech Generation</head><p>Autoregressive training largely followed the autoregressive language modeling protocol, such as</p><p>• Weight decay 0.1</p><p>• Learning rate warmup for 10% of total steps</p><p>• AdamW optimizer with 𝛽 = (0.9, 0.95)</p><p>• Gradient clip value 0.1</p><p>We used a learning rate of 0.002 and 200000 training steps at a batch size of 16.</p><p>The large Mamba model in Table <ref type="table" target="#tab_12">4</ref> has 15 layers per stage with an outer dimension of 𝐷 = 96 and pooling factor 4. We note that this dataset is small (training went through 100 epochs) and for this large model, there was significant overfitting of the BPB or NLL. However, automated metrics of generated samples continually improving throughout training.</p><p>The models in the architecture ablations in Table <ref type="table" target="#tab_13">5</ref> all have 8 layers per stage with an outer dimension of D = 64 and pooling factor 4. The S4+MLP block has roughly 2𝐷 2 + 4𝐷 2 parameters (expansion factor 2 in the MLP). The Transformer block has 4𝐷 2 + 2𝐷 2 parameters (expansion factor 1 in the MLP). The Mamba block has the usual ≈ 6𝐷 2 parameters. All models have roughly 6M total parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Efficiency Benchmark</head><p>Scan Operation. We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations outside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the QKV projections in attention.</p><p>As a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the parameters 𝑨, 𝑩, 𝑪 in HBM.</p><p>Our scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large parameters in HBM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>2b) 𝑲 = (𝑪𝑩, 𝑪𝑨𝑩, . . . , 𝑪𝑨 𝑘 𝑩, . . . ) (3a) 𝑦 = 𝑥 * 𝑲 (3b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Discretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance<ref type="bibr" target="#b74">(Nguyen, Goel, et al. 2022</ref>) and automatically ensuring that the model is properly normalized<ref type="bibr" target="#b40">(Gu, Johnson, Timalsina, et al. 2023;</ref><ref type="bibr" target="#b78">Orvieto et al. 2023)</ref>. It also has connections to gating mechanisms of RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point of view discretization can simply be viewed as the first step of the computation graph in the forward pass of an SSM. Alternate flavors of SSMs can bypass the discretization step and parameterize (𝑨, 𝑩) directly instead<ref type="bibr" target="#b113">(Zhang et al. 2023</ref>), which may be easier to reason about.Computation. After the parameters have been transformed from (Δ, 𝑨, 𝑩, 𝑪) ↦ → (𝑨, 𝑩, 𝑪), the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (DNA Scaling Laws.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 2 10 = 1024 and increasing size from ≈ 200𝐾 to ≈ 40𝑀 parameters, Mamba scales better than baselines. (Right) Fixing model size and increasing sequence lengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates better performance with increasing context length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: (Great Apes DNA Classification.) Accuracy after finetuning on sequences of length 2 10 = 1024 up to 2 20 = 1048576 using pretrained models of the same context length. Numerical results in Table13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>SC09 is a benchmark</head><figDesc>speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of 1-second clips sampled at 16000 Hz of the digits "zero" through "nine" with highly variable characteristics. We largely follow the autoregressive training setup and generation protocol of<ref type="bibr" target="#b34">Goel et al. (2022)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term "gating" in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>E. 4 . 1</head><label>41</label><figDesc>YouTubeMix Audio PretrainingModel. We use a model with 3 blocks per stage (3 × 5 = 15 total Mamba blocks), pooling factor 𝑝 = 16, and outer dimension 𝐷 = 64, for about 3.5M parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Beyond the varying batch sizes, the number of valid segments in the training set varied between different sequence lengths (e.g. the number of training steps per epoch was not constant for different points in the graph), which may have contributed to kinks in the scaling curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: (Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled "continuous" signal modality, audio waveforms actually benefit from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same parameterization) (Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>(Induction Heads.) Models are trained on sequence length 28 = 256, and tested on increasing sequence lengths of 2 6 = 64 up to 220 = 1048576. Full numbers in Table11.</figDesc><table><row><cell cols="2">7GEPMRK0E[WSR8LI4MPI7IUYIRGI0IRKXL</cell><cell cols="2">7GEPMRK0E[WSR8LI4MPI7IUYIRGI0IRKXL</cell></row><row><cell>4IVTPI\MX]PSKWGEPI</cell><cell>,]IRE 6;/: 8VERWJSVQIV 6IX2IX , 8VERWJSVQIV 1EQFE</cell><cell>4IVTPI\MX]PSKWGEPI</cell><cell>,]IRE 6;/: 8VERWJSVQIV 6IX2IX , 8VERWJSVQIV 1EQFE</cell></row><row><cell>*034WPSKWGEPI</cell><cell></cell><cell>*034WPSKWGEPI</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>(Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers, trained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice the model size.</figDesc><table><row><cell>Model</cell><cell cols="2">Token. Pile</cell><cell cols="8">LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande Average</cell></row><row><cell></cell><cell></cell><cell>ppl ↓</cell><cell>ppl ↓</cell><cell>acc ↑</cell><cell>acc ↑</cell><cell cols="2">acc ↑ acc ↑</cell><cell>acc ↑</cell><cell>acc ↑</cell><cell>acc ↑</cell></row><row><cell cols="2">Hybrid H3-130M GPT2</cell><cell>-</cell><cell>89.48</cell><cell>25.77</cell><cell>31.7</cell><cell>64.2</cell><cell>44.4</cell><cell>24.2</cell><cell>50.6</cell><cell>40.1</cell></row><row><cell>Pythia-160M</cell><cell>NeoX</cell><cell>29.64</cell><cell>38.10</cell><cell>33.0</cell><cell>30.2</cell><cell>61.4</cell><cell>43.2</cell><cell>24.1</cell><cell>51.9</cell><cell>40.6</cell></row><row><cell>Mamba-130M</cell><cell>NeoX</cell><cell cols="2">10.56 16.07</cell><cell>44.3</cell><cell>35.3</cell><cell>64.5</cell><cell>48.0</cell><cell>24.3</cell><cell>51.9</cell><cell>44.7</cell></row><row><cell cols="2">Hybrid H3-360M GPT2</cell><cell>-</cell><cell>12.58</cell><cell>48.0</cell><cell>41.5</cell><cell>68.1</cell><cell>51.4</cell><cell>24.7</cell><cell>54.1</cell><cell>48.0</cell></row><row><cell>Pythia-410M</cell><cell>NeoX</cell><cell>9.95</cell><cell>10.84</cell><cell>51.4</cell><cell>40.6</cell><cell>66.9</cell><cell>52.1</cell><cell>24.6</cell><cell>53.8</cell><cell>48.2</cell></row><row><cell>Mamba-370M</cell><cell>NeoX</cell><cell>8.28</cell><cell>8.14</cell><cell>55.6</cell><cell>46.5</cell><cell>69.5</cell><cell>55.1</cell><cell>28.0</cell><cell>55.3</cell><cell>50.0</cell></row><row><cell>Pythia-1B</cell><cell>NeoX</cell><cell>7.82</cell><cell>7.92</cell><cell>56.1</cell><cell>47.2</cell><cell>70.7</cell><cell>57.0</cell><cell>27.1</cell><cell>53.5</cell><cell>51.9</cell></row><row><cell>Mamba-790M</cell><cell>NeoX</cell><cell>7.33</cell><cell>6.02</cell><cell>62.7</cell><cell>55.1</cell><cell>72.1</cell><cell>61.2</cell><cell>29.5</cell><cell>56.1</cell><cell>57.1</cell></row><row><cell>GPT-Neo 1.3B</cell><cell>GPT2</cell><cell>-</cell><cell>7.50</cell><cell>57.2</cell><cell>48.9</cell><cell>71.1</cell><cell>56.2</cell><cell>25.9</cell><cell>54.9</cell><cell>52.4</cell></row><row><cell>Hybrid H3-1.3B</cell><cell>GPT2</cell><cell>-</cell><cell>11.25</cell><cell>49.6</cell><cell>52.6</cell><cell>71.3</cell><cell>59.2</cell><cell>28.1</cell><cell>56.9</cell><cell>53.0</cell></row><row><cell>OPT-1.3B</cell><cell>OPT</cell><cell>-</cell><cell>6.64</cell><cell>58.0</cell><cell>53.7</cell><cell>72.4</cell><cell>56.7</cell><cell>29.6</cell><cell>59.5</cell><cell>55.0</cell></row><row><cell>Pythia-1.4B</cell><cell>NeoX</cell><cell>7.51</cell><cell>6.08</cell><cell>61.7</cell><cell>52.1</cell><cell>71.0</cell><cell>60.5</cell><cell>28.5</cell><cell>57.2</cell><cell>55.2</cell></row><row><cell>RWKV-1.5B</cell><cell>NeoX</cell><cell>7.70</cell><cell>7.04</cell><cell>56.4</cell><cell>52.5</cell><cell>72.4</cell><cell>60.5</cell><cell>29.4</cell><cell>54.6</cell><cell>54.3</cell></row><row><cell>Mamba-1.4B</cell><cell>NeoX</cell><cell>6.80</cell><cell>5.04</cell><cell>64.9</cell><cell>59.1</cell><cell>74.2</cell><cell>65.5</cell><cell>32.8</cell><cell>61.5</cell><cell>59.7</cell></row><row><cell>GPT-Neo 2.7B</cell><cell>GPT2</cell><cell>-</cell><cell>5.63</cell><cell>62.2</cell><cell>55.8</cell><cell>72.1</cell><cell>61.1</cell><cell>30.2</cell><cell>57.6</cell><cell>56.5</cell></row><row><cell>Hybrid H3-2.7B</cell><cell>GPT2</cell><cell>-</cell><cell>7.92</cell><cell>55.7</cell><cell>59.7</cell><cell>73.3</cell><cell>65.6</cell><cell>32.3</cell><cell>61.4</cell><cell>58.0</cell></row><row><cell>OPT-2.7B</cell><cell>OPT</cell><cell>-</cell><cell>5.12</cell><cell>63.6</cell><cell>60.6</cell><cell>74.8</cell><cell>60.8</cell><cell>31.3</cell><cell>61.0</cell><cell>58.7</cell></row><row><cell>Pythia-2.8B</cell><cell>NeoX</cell><cell>6.73</cell><cell>5.04</cell><cell>64.7</cell><cell>59.3</cell><cell>74.0</cell><cell>64.1</cell><cell>32.9</cell><cell>59.7</cell><cell>59.1</cell></row><row><cell>RWKV-3B</cell><cell>NeoX</cell><cell>7.00</cell><cell>5.24</cell><cell>63.9</cell><cell>59.6</cell><cell>73.7</cell><cell>67.8</cell><cell>33.1</cell><cell>59.6</cell><cell>59.6</cell></row><row><cell>Mamba-2.8B</cell><cell>NeoX</cell><cell>6.22</cell><cell>4.23</cell><cell>69.2</cell><cell>66.1</cell><cell>75.2</cell><cell>69.7</cell><cell>36.3</cell><cell>63.5</cell><cell>63.3</cell></row><row><cell>GPT-J-6B</cell><cell>GPT2</cell><cell>-</cell><cell>4.10</cell><cell>68.3</cell><cell>66.3</cell><cell>75.4</cell><cell>67.0</cell><cell>36.6</cell><cell>64.1</cell><cell>63.0</cell></row><row><cell>OPT-6.7B</cell><cell>OPT</cell><cell>-</cell><cell>4.25</cell><cell>67.7</cell><cell>67.2</cell><cell>76.3</cell><cell>65.6</cell><cell>34.9</cell><cell>65.5</cell><cell>62.9</cell></row><row><cell>Pythia-6.9B</cell><cell>NeoX</cell><cell>6.51</cell><cell>4.45</cell><cell>67.1</cell><cell>64.0</cell><cell>75.2</cell><cell>67.3</cell><cell>35.5</cell><cell>61.3</cell><cell>61.7</cell></row><row><cell>RWKV-7.4B</cell><cell>NeoX</cell><cell>6.31</cell><cell>4.38</cell><cell>67.2</cell><cell>65.5</cell><cell>76.1</cell><cell>67.8</cell><cell>37.5</cell><cell>61.0</cell><cell>62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 4 :</head><label>4</label><figDesc>(SC09) Automated metrics for unconditional generation on a challenging dataset of fixed-length speech clips. (Top to Bottom) Autoregressive baselines, non-autoregressive baselines, Mamba, and dataset metrics.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell>NLL ↓</cell><cell>FID ↓</cell><cell>IS ↑</cell><cell>mIS ↑</cell><cell>AM ↓</cell></row><row><cell>SampleRNN</cell><cell>35.0M</cell><cell>2.042</cell><cell>8.96</cell><cell>1.71</cell><cell>3.02</cell><cell>1.76</cell></row><row><cell>WaveNet</cell><cell>4.2M</cell><cell>1.925</cell><cell>5.08</cell><cell>2.27</cell><cell>5.80</cell><cell>1.47</cell></row><row><cell>SaShiMi</cell><cell>5.8M</cell><cell>1.873</cell><cell>1.99</cell><cell>5.13</cell><cell>42.57</cell><cell>0.74</cell></row><row><cell>WaveGAN</cell><cell>19.1M</cell><cell>-</cell><cell>2.03</cell><cell>4.90</cell><cell>36.10</cell><cell>0.80</cell></row><row><cell>DiffWave</cell><cell>24.1M</cell><cell>-</cell><cell>1.92</cell><cell>5.26</cell><cell>51.21</cell><cell>0.68</cell></row><row><cell>+ SaShiMi</cell><cell>23.0M</cell><cell>-</cell><cell>1.42</cell><cell>5.94</cell><cell>69.17</cell><cell>0.59</cell></row><row><cell>Mamba</cell><cell>6.1M</cell><cell>1.852</cell><cell>0.94</cell><cell>6.26</cell><cell>88.54</cell><cell>0.52</cell></row><row><cell>Mamba</cell><cell>24.3M</cell><cell>1.860</cell><cell>0.67</cell><cell>7.33</cell><cell>144.9</cell><cell>0.36</cell></row><row><cell>Train</cell><cell>-</cell><cell>-</cell><cell>0.00</cell><cell>8.56</cell><cell>292.5</cell><cell>0.16</cell></row><row><cell>Test</cell><cell>-</cell><cell>-</cell><cell>0.02</cell><cell>8.33</cell><cell>257.6</cell><cell>0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>(SC09 Model Ablations) Models with 6M parameters. In SaShiMi's U-Net backbone, there are 8 center blocks operating on sequence length 1000, sandwiched on each side by 8 outer blocks on sequence length 4000, sandwiched by 8 outer blocks on sequence length 16000 (40 blocks total). The architecture of the 8 center blocks are ablated independently of the rest. Note that Transformers (MHA+MLP) were not tested in the more important outer blocks because of efficiency constraints.</figDesc><table><row><cell>Outer</cell><cell>Center</cell><cell>NLL ↓</cell><cell>FID ↓</cell><cell>IS ↑</cell><cell>mIS ↑</cell><cell>AM ↓</cell></row><row><cell>S4+MLP</cell><cell>MHA+MLP</cell><cell>1.859</cell><cell>1.45</cell><cell>5.06</cell><cell>47.03</cell><cell>0.70</cell></row><row><cell>S4+MLP</cell><cell>S4+MLP</cell><cell>1.867</cell><cell>1.43</cell><cell>5.42</cell><cell>53.54</cell><cell>0.65</cell></row><row><cell>S4+MLP</cell><cell>Mamba</cell><cell>1.859</cell><cell>1.42</cell><cell>5.71</cell><cell>56.51</cell><cell>0.64</cell></row><row><cell>Mamba</cell><cell>MHA+MLP</cell><cell>1.850</cell><cell>1.37</cell><cell>5.63</cell><cell>58.23</cell><cell>0.62</cell></row><row><cell>Mamba</cell><cell>S4+MLP</cell><cell>1.853</cell><cell>1.07</cell><cell>6.05</cell><cell>73.34</cell><cell>0.55</cell></row><row><cell>Mamba</cell><cell>Mamba</cell><cell>1.852</cell><cell>0.94</cell><cell>6.26</cell><cell>88.54</cell><cell>0.52</cell></row></table><note><p>We benchmark the speed of the SSM scan operation (state expansion 𝑁 = 16), as well as the end-to-end inference throughput of Mamba, in Figure8</p><p>. Our efficient SSM scan is faster than the best attention implementation that we know of (FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40× faster than a standard scan implementation in PyTorch. Mamba achieves 4-5× higher inference throughput than a Transformer of similar size, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference throughput than a 5× smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of memory consumption. Figure 8: (Efficiency Benchmarks.) (Left) Training: our efficient scan is 40× faster than a standard implementation. (Right) Inference: as a recurrent model, Mamba can achieve 5× higher throughput than Transformers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>(Ablations: Architecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the inner layer, there is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More specifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.</figDesc><table><row><cell cols="3">Model Arch. SSM Layer</cell><cell>Perplexity</cell><cell cols="2">Model Arch.</cell><cell cols="2">SSM Layer</cell><cell>Perplexity</cell></row><row><cell cols="2">Hyena H3</cell><cell>Hyena</cell><cell>10.24</cell><cell>-</cell><cell cols="2">Mamba Hyena</cell><cell>10.75</cell></row><row><cell>H3</cell><cell>H3</cell><cell cols="2">S4 (complex) 10.30</cell><cell>-</cell><cell cols="3">Mamba S4 (complex) 10.54</cell></row><row><cell>-</cell><cell>H3</cell><cell>S4 (real)</cell><cell>10.34</cell><cell>-</cell><cell cols="2">Mamba S4 (real)</cell><cell>10.56</cell></row><row><cell>-</cell><cell>H3</cell><cell>S6</cell><cell>8.95</cell><cell cols="3">Mamba Mamba S6</cell><cell>8.69</cell></row><row><cell cols="4">Table 7: (Ablations: Selective parameters.) Δ is the most impor-</cell><cell></cell><cell cols="3">Table 8: (Ablations: Parameterization of 𝑨.) The more</cell></row><row><cell cols="4">tant parameter (Theorem 1), but using multiple selective parameters</cell><cell></cell><cell cols="3">standard initializations based on S4D-Lin (Gu, Gupta, et al.</cell></row><row><cell cols="2">together synergizes.</cell><cell></cell><cell></cell><cell></cell><cell cols="3">2022) perform worse than S4D-Real or a random initialization,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">when the SSM is selective.</cell></row><row><cell cols="4">Selective Δ Selective 𝑩 Selective 𝑪 Perplexity</cell><cell></cell><cell></cell><cell></cell></row><row><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>10.93</cell><cell></cell><cell cols="3">𝑨 𝑛 Initialization Field</cell><cell>Perplexity</cell></row><row><cell>✗</cell><cell>✓</cell><cell>✗</cell><cell>10.15</cell><cell></cell><cell cols="2">𝑨 𝑛 = -1 2 + 𝑛𝑖</cell><cell>Complex 9.16</cell></row><row><cell>✗</cell><cell>✗</cell><cell>✓</cell><cell>9.98</cell><cell></cell><cell>𝑨 𝑛 = -1/2</cell><cell></cell><cell>Real</cell><cell>8.85</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell>9.81</cell><cell></cell><cell cols="2">𝑨 𝑛 = -(𝑛 + 1)</cell><cell>Real</cell><cell>8.71</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>8.71</cell><cell></cell><cell cols="3">𝑨 𝑛 ∼ exp(N (0, 1)) Real</cell><cell>8.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>(Ablations: Expressivity of Δ.)</figDesc><table><row><cell cols="3">The selection mechanism of Δ constructs it</cell></row><row><cell cols="3">with a projection of the input. Projecting it</cell></row><row><cell cols="3">even to dim. 1 provides a large increase in</cell></row><row><cell cols="3">performance; increasing it further provides</cell></row><row><cell cols="3">further improvements at the cost of a mod-</cell></row><row><cell cols="3">est increase in parameters. State size fixed</cell></row><row><cell>to 𝑁 = 16.</cell><cell></cell><cell></cell></row><row><cell cols="3">Size of Δ proj. Params (M) Perplexity</cell></row><row><cell>-</cell><cell>358.9</cell><cell>9.12</cell></row><row><cell>1</cell><cell>359.1</cell><cell>8.97</cell></row><row><cell>2</cell><cell>359.3</cell><cell>8.97</cell></row><row><cell>4</cell><cell>359.7</cell><cell>8.91</cell></row><row><cell>8</cell><cell>360.5</cell><cell>8.83</cell></row><row><cell>16</cell><cell>362.1</cell><cell>8.84</cell></row><row><cell>32</cell><cell>365.2</cell><cell>8.80</cell></row><row><cell>64</cell><cell>371.5</cell><cell>8.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>(Ablations: SSM state dimension.) (Top) Constant 𝑩 and 𝑪 (Bottom) Selective 𝑩 and 𝑪. Increasing the SSM state dimension 𝑁 , which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when 𝑩 and 𝑪 are also selective. Size of Δ projection fixed to 64.</figDesc><table><row><cell cols="3">State dimension 𝑁 Params (M) Perplexity</cell></row><row><cell>1</cell><cell>367.1</cell><cell>9.88</cell></row><row><cell>2</cell><cell>367.4</cell><cell>9.86</cell></row><row><cell>4</cell><cell>368.0</cell><cell>9.82</cell></row><row><cell>8</cell><cell>369.1</cell><cell>9.82</cell></row><row><cell>16</cell><cell>371.5</cell><cell>9.81</cell></row><row><cell>1</cell><cell>367.1</cell><cell>9.73</cell></row><row><cell>2</cell><cell>367.4</cell><cell>9.40</cell></row><row><cell>4</cell><cell>368.0</cell><cell>9.09</cell></row><row><cell>8</cell><cell>369.1</cell><cell>8.84</cell></row><row><cell>16</cell><cell>371.5</cell><cell>8.71</cell></row></table><note><p>No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally defined as discretizations of continuous systems<ref type="bibr" target="#b0">(1)</ref></p><p>, and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoff in more detail. Downstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances.</p><p>Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama<ref type="bibr" target="#b104">(Touvron et al. 2023</ref></p><p>)) as well as other recurrent models such as RWKV (B.<ref type="bibr" target="#b81">Peng et al. 2023</ref></p><p>) and RetNet (Y.<ref type="bibr" target="#b99">Sun et al. 2023)</ref></p><p>, which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>(Induction heads.) Models are trained on sequence length 28 = 256, and tested on various sequence lengths of 2 6 = 64 up to 220 = 1048576. ✓ denotes perfect generalization accuracy, while ✗ denotes out of memory.Most of the parameters are in learnable positional encodings.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Test Accuracy (%) at Seqence Length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>2 6</cell><cell>2 7</cell><cell>2 8</cell><cell>2 9</cell><cell>2 10</cell><cell>2 11</cell><cell>2 12</cell><cell>2 13</cell><cell>2 14</cell><cell>2 15</cell><cell>2 16</cell><cell>2 17</cell><cell>2 18</cell><cell>2 19</cell><cell>2 20</cell></row><row><cell>MHA-Abs</cell><cell>137K</cell><cell>✓</cell><cell cols="6">99.6 100.0 58.6 26.6 18.8 9.8</cell><cell cols="2">10.9 7.8</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell></row><row><cell cols="2">MHA-RoPE 137K</cell><cell>✓</cell><cell>✓</cell><cell cols="5">100.0 83.6 31.3 18.4 8.6</cell><cell>9.0</cell><cell>5.5</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell></row><row><cell>MHA-xPos</cell><cell>137K</cell><cell>✓</cell><cell>✓</cell><cell cols="5">100.0 99.6 67.6 25.4 7.0</cell><cell>9.0</cell><cell>7.8</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell><cell>✗</cell></row><row><cell>H3</cell><cell>153K</cell><cell>✓</cell><cell>✓</cell><cell cols="6">100.0 80.9 39.5 23.8 14.8 8.2</cell><cell>5.9</cell><cell>6.6</cell><cell>8.2</cell><cell>4.7</cell><cell>8.2</cell><cell>6.3</cell><cell>7.4</cell></row><row><cell>Hyena</cell><cell>69M  *</cell><cell>97.7</cell><cell>✓</cell><cell>100.0</cell><cell>✓</cell><cell cols="3">44.1 12.5 6.6</cell><cell>5.1</cell><cell>7.0</cell><cell>5.9</cell><cell>6.6</cell><cell>6.6</cell><cell>5.9</cell><cell>6.3</cell><cell>9.8</cell></row><row><cell>Mamba</cell><cell>74K</cell><cell>✓</cell><cell>✓</cell><cell>100.0</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>(Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of heads applies only to Transformer models.) Params n_layers d_model n_heads / d_head Training steps Learning Rate Batch Size</figDesc><table><row><cell>Tokens</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 13 :</head><label>13</label><figDesc>(Great Apes DNA Classification.) Accuracy after fine-tuning on sequences of length 2 10 = 1024 up to 220 = 1048576 using pretrained models of the same context length. Random guessing is 20%.</figDesc><table><row><cell>Model</cell><cell>Params</cell><cell></cell><cell cols="4">Accuracy (%) at Seqence Length</cell></row><row><cell></cell><cell></cell><cell>2 10</cell><cell>2 12</cell><cell>2 14</cell><cell>2 16</cell><cell>2 18</cell><cell>2 20</cell></row><row><cell cols="2">HyenaDNA 1.4M</cell><cell cols="5">28.04 28.43 41.17 42.22 31.10 54.87</cell></row><row><cell>Mamba</cell><cell>1.4M</cell><cell cols="5">31.47 27.50 27.66 40.72 42.41 71.67</cell></row><row><cell>Mamba</cell><cell>7M</cell><cell cols="5">30.00 29.01 31.48 43.73 56.60 81.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 14 :</head><label>14</label><figDesc>YouTubeMix length scaling sequence lengths and batch sizes.long (e.g. comprising 6 out of 10 epochs for the model with context length 2 20 ); we did not experiment with this choice.</figDesc><table><row><cell cols="2">Seqence length Batch size Tokens / batch</cell></row><row><cell>468 × 2048 = 958464 1</cell><cell>958464</cell></row><row><cell>234 × 2048 = 479232 2</cell><cell>958464</cell></row><row><cell>117 × 2048 = 239616 4</cell><cell>958464</cell></row><row><cell>59 × 2048 = 120832 8</cell><cell>966656</cell></row><row><cell>30 × 2048 = 61440 16</cell><cell>983040</cell></row><row><cell>15 × 2048 = 30720 32</cell><cell>983040</cell></row><row><cell>8 × 2048 = 16384 64</cell><cell>1048576</cell></row><row><cell>4 × 2048 = 8192 128</cell><cell>1048576</cell></row><row><cell>warmup was also</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Karan Goel</rs>, <rs type="person">Arjun Desai</rs>, and <rs type="person">Kush Bhatia</rs> for helpful feedback on the draft.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training. For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1𝑒 -3, 2𝑒 -3, 4𝑒 -3, 8𝑒 -3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible that our results are still suboptimal.) Note that, in contrast to standard LM scaling laws (Table <ref type="table">12</ref>), our LR held constant across model sizes for simplicity. The optimal LR should go down for larger models, but we didn't find a noticeable effect at the small model sizes (at most a few million parameters) we considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.3 Scaling: Context Length Details</head><p>We use a total batch size of 2 24 ≈ 16𝑀 tokens per training step, for every sequence length (e.g. at length 2 20 there are 16 segments per batch and at length 2 10 there are 16384 segments per batch). This is a large batch size relative to the model size by usual LM standards, but note that a batch size of 2 23 is the minimum possible on a machine with 8 GPUs and sequence length of 2 2 0, and that HyenaDNA used much larger batches of 2 28 .</p><p>The learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length.</p><p>Sequence Length Warmup. Following <ref type="bibr">(Nguyen, Poli, et al. 2023)</ref>, we use sequence length warmup (SLW) during pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 2 10 = 1024. (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally. In particular, each stage up to length 2 17 processes the same number of tokens, but 4× as many tokens are processed at length 2 18 , 8× as many at length 2 19 , and 16× as many at length 2 20 .) Unlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively halved as the sequence lengths are doubled in each stage.</p><p>Remark E.1. We also note that the schedule was not tuned, and we never experimented with turning off sequence length warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3.4 Species (Great Apes) Classification</head><p>Models are causal and therefore only the last element (across the sequence length) of the model's output is used for the classification head. Note that we control for the total number of elements in the loss function per gradient step. The pretraining objective includes all positions across the sequence length, so that batch_size × sequence_length is held constant; in other words, the batch size decreases as the sequence length increases. However, for a classification task, since only the last position enters the loss, the batch size itself is held constant. Note that this also means that fine-tuning models with longer sequence lengths is more computationally expensive.</p><p>Training consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly picking a contiguous segment of DNA.</p><p>Following <ref type="bibr">(Nguyen, Poli, et al. 2023)</ref>, models with a maximum context length greater than 2 14 = 16384 use sequence length warmup with 1 epoch at length 2 14 = 16384, 1 epoch at length 2 15 = 32768, 1 epoch at length 2 16 = 65536, and so on up to the maximum sequence length. For example, the model with 2 20 = 1048576 context undergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length.</p><p>The learning rate for all Hyena models is 4e -5, while the learning rate for all Mamba models is 1e -4. These were found by performing learning rate sweeps for each model among {1𝑒 -5, 2𝑒 -5, 4𝑒 -5, 1𝑒 -4, 2𝑒 -4} for the smaller sequence lengths (2 10 , 2 12 , 2 14 , 2 16 ), and these values were consistently found to be the best for each model. An abridged learning rate sweep was done at length 2 18 , which agreed with these values, and a single run at length 2 20 was performed (as described above, the computational cost of these experiments is proportional to the sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of cosine decay down to 1𝑒 -6. The unusually long learning rate warmup schedule was chosen because the sequence length For convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the filters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity is 𝑂 (𝐿 log(𝐿)) for sequence length 𝐿.</p><p>For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7× faster than without causal mask, since approximately only half of the attention entries are computed.</p><p>We use batch size of 1 and increase the sequence length from 2 9 = 512, 2 10 ≈ 1𝐾, 2 11 ≈ 2𝐾, up to 2 19 ≈ 500𝐾 (some of the baselines run out of memory before reaching 500K). We use a model dimension of 𝐷 = 1024 and state dimension 𝑁 = 16.</p><p>We measure with BF16 inputs, which is the data type most commonly used for large scale training.</p><p>End-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer implementation in the Huggingface transformers library.</p><p>We set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16, 32, 64, to 128, and measure time time taken generate 128 tokens. We then calculate the throughput (tokens/s) as batch size × 128/time taken. We repeat the measurements 3 times and take the average. Measurements are done on an A100 80GB PCIe GPU.</p><p>Memory Benchmark. The memory usage simply scales proportionally to the size of the activation tensors, as with most deep sequence models. We report measurements of the training memory requirements of 125M models on 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-efficient Transformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table <ref type="table">15</ref> shows that Mamba's memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation, and we expect further improvement in Mamba's memory footprint in the future.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary Evolution Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective Gene Expression Prediction from Sequence by Integrating Long-range Interactions</title>
		<author>
			<persName><forename type="first">Žiga</forename><surname>Avsec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Visentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Joseph R Ledsam</surname></persName>
		</author>
		<author>
			<persName><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kyle R Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using Fast Weights to Attend to the Recent Past</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer Normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Strongly-typed Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<idno>PMLR. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pythia: A Suite for Analyzing Large Language Models across Training and Scaling</title>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailey</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><forename type="middle">Gregory</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbie</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Kyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Aflah Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><surname>Raff</surname></persName>
		</author>
		<idno>PMLR. 2023</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PIQA: Reasoning about Physical Commonsense in Natural Language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on Artificial Intelligence</title>
		<meeting>the AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Gpt-NeoX-20B: An Open-source Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06745</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Prefix Sums and Their Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><surname>Blelloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quasi-recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01576</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language Models are Few-shot Learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scaling Transformer to 1M tokens and Beyond with RMT</title>
		<author>
			<persName><forename type="first">Aydar</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><forename type="middle">S</forename><surname>Burtsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.11062</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating Long Sequences with Sparse Transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking Attention with Performers</title>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PaLM: Scaling Language Modeling with Pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/22-1144.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="113" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hungry Hungry Hippos: Towards Language Modeling with State Space Models</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
		<idno>PMLR. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Deepsound</surname></persName>
		</author>
		<author>
			<persName><surname>Samplernn</surname></persName>
		</author>
		<ptr target="https://github.com/deepsound-project/samplernn-pytorch.2017" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">LongNet: Scaling Transformers to 1,000,000,000 Tokens</title>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.02486</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial Audio Synthesis</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miller</forename><surname>Puckette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Mathematical Framework for Transformer Circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2021/framework/index.html" />
	</analytic>
	<monogr>
		<title level="m">Transformer Circuits Thread</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Block-State Transformer</title>
		<author>
			<persName><forename type="first">Mahan</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Pilault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09539</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-Head State Space Model for Speech Recognition</title>
		<author>
			<persName><forename type="first">Yassir</forename><surname>Fathullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junteng</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mahadeokar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozlem</forename><surname>Kalinli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2023-1036</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH 2023</title>
		<meeting>INTERSPEECH 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic Causal Modelling</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Karl J Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple Hardware-efficient Long Convolutions for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">L</forename><surname>Daniel Y Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Funahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Framework for Few-shot Language Model Evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<ptr target="https://doi.org/10.5281/zenodo.5371628" />
		<imprint/>
	</monogr>
	<note>Version v0.0.1. Sept. 2021</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">It&apos;s Raw! Audio Generation with State-Space Models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">HIPPO: Recurrent Memory with Optimal Polynomial Projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficiently Modeling Long Sequences with Structured State Spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving the Gating Mechanism of Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the Parameterization and Initialization of Diagonal State Space Models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How to Train Your HIPPO: State Space Models with Generalized Basis Projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Diagonal State Spaces are as Effective as Structured State Spaces</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22982" to="22994" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Simplifying and Understanding State Space Models with Diagonal Linear RNNs</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.00768</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HyperNetworks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dream to Control: Learning Behaviors by Latent Imagination</title>
		<author>
			<persName><forename type="first">Danijar</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Liquid Structural State-Space Models</title>
		<author>
			<persName><forename type="first">Ramin</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makram</forename><surname>Chahine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recurrent Orthogonal Networks and Long-Memory Tasks</title>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian Error Linear Units (GELUs)&quot;. In: arXiv preprint</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Untersuchungen zu dynamischen neuronalen Netzen</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diploma, Technische Universität München</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-term Dependencies</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Compute-Optimal Large Language Model Training</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="30016" to="30030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformer Quality in Linear Time</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="9099" to="9117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Time Series Classification: A Review</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="917" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Data Movement is All You Need: A Case Study on Optimizing Transformers</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="711" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gated Orthogonal Recurrent Units: On Learning to Forget</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Peurifoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marin</forename><surname>Soljacic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="765" to="783" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">A New Approach to Linear Filtering and Prediction Problems</title>
		<author>
			<persName><forename type="first">Rudolph</forename><surname>Emil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalman</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
		<title level="m">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</title>
		<imprint>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Linear Dynamical Systems as a Core Computational Primitive</title>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16808" to="16820" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series</title>
		<author>
			<persName><forename type="first">Chrysoula</forename><surname>Kosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.03210</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7633" to="7648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Simple Recurrent Units for Highly Parallelizable Recurrence</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02755</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lezcano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Casado</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Martínez-Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">What Makes Convolutional Models Great on Long Sequence Modeling?</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Time-aware Large Kernel Convolutions</title>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Lioutas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="6172" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Structured State Space Models for In-Context Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Schroecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feryal</forename><surname>Behbahani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Focus Your Attention (with Adaptive IIR Filters)</title>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Lutati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itamar</forename><surname>Zimerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14952</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Mega: Moving Average Equipped Gated Attention</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Parallelizing Linear Recurrent Neural Nets Over Sequence Length</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</title>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Long Range Language Modeling via Gated State Spaces</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Cutkosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient Orthogonal Parametrisation of Recurrent Neural Networks using Householder Reflections</title>
		<author>
			<persName><forename type="first">Zakaria</forename><surname>Mhammedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hellicar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashfaqur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<idno>PMLR. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2401" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preey</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<title level="m">S4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">HyenaDNA: Long-range Genomic Sequence Modeling at Single Nucleotide Resolution</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Faizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callum</forename><surname>Birch-Sykes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wornow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Rabideau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">In-context Learning and Induction Heads</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" />
	</analytic>
	<monogr>
		<title level="m">Transformer Circuits Thread</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">WaveNet: A Generative Model for Raw Audio</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Resurrecting Recurrent Neural Networks for Long Sequences</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Orvieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushan</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The LAMBADA Dataset: Word Prediction Requiring a Broad Discourse Context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">On the Difficulty of Training Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">RWKV: Reinventing RNNs for the Transformer Era</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Alcaide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Albalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Arcadinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Grella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kranthi</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13048</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Random Feature Attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Massaroli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<title level="m">Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>The International Conference on Machine Learning (ICML)</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Toeplitz Neural Network for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">The devil in linear transformer</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10340</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">CosFormer: Rethinking Softmax in Attention</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Random Features for Large-Scale Kernel Machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Swish: A Self-gated Activation Function</title>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.059417.1</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">CKConv: Continuous Kernel Convolution For Sequential Data</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Winogrande: An Adversarial Winograd Schema Challenge at Scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Diagonal State Space Augmented Transformers for Speech Recognition</title>
		<author>
			<persName><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Linear Transformers are Secretly Fast Weight Programmers</title>
		<author>
			<persName><forename type="first">Imanol</forename><surname>Schlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>PMLR. 2021</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="9355" to="9366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">GLU Variants Improve Transformer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Large Language Models can be Easily Distracted by Irrelevant Context</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>PMLR. 2023</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="31210" to="31227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Sequence Modeling with Multiresolution Convolutional Memory</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Fox</surname></persName>
		</author>
		<idno>PMLR. 2023</idno>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page" from="31312" to="31327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Simplified State Space Layers for Sequence Modeling</title>
		<author>
			<persName><forename type="first">Jimmy Th</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Warrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">W</forename><surname>Linderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Roformer: Enhanced Transformer with Rotary Position Embedding</title>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Murtadha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Retentive network: A successor to transformer for large language models</title>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.08621</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Can Recurrent Neural Networks Warp Time?</title>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Long Range Arena: A Benchmark for Efficient Transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Efficient Transformers: A Survey</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Llama: Open and Efficient Foundation Language Models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">On Orthogonality and Learning Recurrent Networks with Long Term Dependencies</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiheb</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<idno>PMLR. 2017</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="3570" to="3578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Selective Structured State-Spaces for Long-form Video Understanding</title>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffay</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6387" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno>ArXiv abs/1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Roofline: An Insightful Visual Performance Model for Multicore Architectures</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a Machine Really Finish Your Sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">An Attention Free Transformer</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Talbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.14103</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><surname>Ré</surname></persName>
		</author>
		<title level="m">Effectively Modeling Time Series with Simple Discrete State Spaces</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>The International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Linear complexity randomized self-attention mechanism</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno>PMLR. 2022</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="27011" to="27041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Efficient Long Sequence Modeling via State Space Augmented Transformer</title>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08136</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
