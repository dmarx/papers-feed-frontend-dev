- Decision to use reinforcement learning from human feedback (RLHF) for fine-tuning
- Choice of model architecture (GPT-3) for InstructGPT
- Selection of human labelers for data collection
- Method for collecting human-written demonstrations of desired output behavior
- Approach to ranking model outputs for reinforcement learning
- Decision to mix PPO updates with log likelihood updates to minimize performance regressions
- Evaluation criteria for model outputs (helpfulness, truthfulness, harmlessness)
- Strategy for generalizing model performance to held-out labelers
- Choice of public NLP datasets for performance comparison
- Decision to focus on explicit and implicit user intentions in model training
- Approach to measuring toxicity and bias in model outputs
- Method for qualitative probing of model capabilities
- Decision to prioritize user intent alignment over broader human values
- Approach to handling performance regressions on specific NLP tasks
- Strategy for evaluating model outputs on API prompt distribution versus public datasets
- Decision to document limitations and open questions in the research