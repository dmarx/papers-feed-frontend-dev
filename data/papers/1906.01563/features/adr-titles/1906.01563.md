- Decision to use Hamiltonian mechanics as a foundation for the neural network model
- Choice to parameterize the Hamiltonian with a neural network
- Decision to focus on unsupervised learning of conservation laws
- Selection of tasks for model evaluation (mass-spring system, ideal pendulum, real pendulum)
- Choice of optimization algorithm (Adam optimizer)
- Decision on the architecture of the neural networks (fully-connected, three layers, 200 hidden units)
- Choice of activation function (tanh)
- Decision to use L2 loss for training
- Choice of metrics for evaluation (L2 train loss, L2 test loss, mean squared error)
- Decision to integrate models using the fourth-order Runge-Kutta method
- Choice of data preprocessing techniques (adding Gaussian noise)
- Decision to use finite difference approximations for time derivatives when necessary
- Choice of learning rate (10^-3)
- Decision to set batch size equal to the total number of examples due to small training sets
- Decision to evaluate model performance on both synthetic and real-world data
- Choice to log and analyze energy conservation as a key performance metric
- Decision to explore the reversibility property of the HNN model
- Choice to investigate counterfactual scenarios using the learned Hamiltonian
- Decision to compare HNN performance against baseline neural network models