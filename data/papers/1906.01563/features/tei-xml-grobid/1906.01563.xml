<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hamiltonian Neural Networks</title>
				<funder ref="#_NVKStpC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sam</forename><surname>Greydanus</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uber AI Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Misko</forename><surname>Dzamba</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Uber AI Labs</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
							<email>yosinski@uber.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uber AI Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hamiltonian Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D777C195E2619850DB820A2CD4B25C93</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time. Ideal mass-spring system Noisy observations Baseline NN Prediction Prediction Hamiltonian NN Figure 1: Learning the Hamiltonian of a mass-spring system. The variables q and p correspond to position and momentum coordinates. As there is no friction, the baseline's inner spiral is due to model errors. By comparison, the Hamiltonian Neural Network learns to exactly conserve a quantity that is analogous to total energy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks have a remarkable ability to learn and generalize from data. This lets them excel at tasks such as image classification <ref type="bibr" target="#b21">[21]</ref>, reinforcement learning <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b37">37]</ref>, and robotic dexterity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">22]</ref>. Even though these tasks are diverse, they all share the same underlying physical laws. For example, a notion of gravity is important for reasoning about objects in an image, training an RL agent to walk, or directing a robot to manipulate objects. Based on this observation, researchers have become increasingly interested in finding physics priors that transfer across tasks <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b40">40]</ref>. Untrained neural networks do not have physics priors; they learn approximate physics knowledge directly from data. This generally prevents them from learning exact physical laws. Consider the frictionless mass-spring system shown in Figure <ref type="figure" target="#fig_3">1</ref>. Here the total energy of the system is being conserved. More specifically, this particular system conserves a quantity proportional to q<ref type="foot" target="#foot_2">foot_2</ref> + p 2 , where q is the position and p is the momentum of the mass. The baseline neural network in Figure <ref type="figure" target="#fig_3">1</ref> learns an approximation of this conservation law, and yet the approximation is imperfect enough that a forward simulation of the system drifts over time to higher or lower energy states. Can we define a class of neural networks that will precisely conserve energy-like quantities over time?</p><p>In this paper, we draw inspiration from Hamiltonian mechanics, a branch of physics concerned with conservation laws and invariances, to define Hamiltonian Neural Networks, or HNNs. We begin with an equation called the Hamiltonian, which relates the state of a system to some conserved quantity (usually energy) and lets us simulate how the system changes with time. Physicists generally use domain-specific knowledge to find this equation, but here we try a different approach:</p><p>Instead of crafting the Hamiltonian by hand, we propose parameterizing it with a neural network and then learning it directly from data.</p><p>Since almost all physical laws can be expressed as conservation laws, our approach is quite general <ref type="bibr" target="#b27">[27]</ref>. In practice, our model trains quickly and generalizes well <ref type="foot" target="#foot_1">1</ref> . Figure <ref type="figure" target="#fig_3">1</ref>, for example, shows the outcome of training an HNN on the same mass-spring system. Unlike the baseline model, it learns to conserve an energy-like quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Theory</head><p>Predicting dynamics. The hallmark of a good physics model is its ability to predict changes in a system over time. This is the challenge we now turn to. In particular, our goal is to learn the dynamics of a system using a neural network. The simplest way of doing this is by predicting the next state of a system given the current one. A variety of previous works have taken this path and produced excellent results <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b5">6]</ref>. There are, however, a few problems with this approach.</p><p>The first problem is its notion of discrete "time steps" that connect neighboring states. Since time is actually continuous, a better approach would be to express dynamics as a set of differential equations and then integrate them from an initial state at t 0 to a final state at t 1 . Equation 1 shows how this might be done, letting S denote the time derivatives of the coordinates of the system 2 . This approach has been under-explored so far, but techniques like Neural ODEs take a step in the right direction <ref type="bibr" target="#b6">[7]</ref>.</p><formula xml:id="formula_0">(q 1 , p 1 ) = (q 0 , p 0 ) + t1 t0 S(q, p) dt<label>(1)</label></formula><p>The second problem with existing methods is that they tend not to learn exact conservation laws or invariant quantities. This often causes them to drift away from the true dynamics of the system as small errors accumulate. The HNN model that we propose ameliorates both of these problems.</p><p>To see how it does this -and to situate our work in the proper context -we first briefly review Hamiltonian mechanics.</p><p>Hamiltonian Mechanics. William Hamilton introduced Hamiltonian mechanics in the 19 th century as a mathematical reformulation of classical mechanics. Its original purpose was to express classical mechanics in a more unified and general manner. Over time, though, scientists have applied it to nearly every area of physics from thermodynamics to quantum field theory <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>In Hamiltonian mechanics, we begin with a set of coordinates (q, p). Usually, q = (q 1 , ..., q N ) represents the positions of a set of objects whereas p = (p 1 , ..., p N ) denotes their momentum. Note how this gives us N coordinate pairs (q 1 , p 1 )...(q N , p N ). Taken together, they offer a complete description of the system. Next, we define a scalar function, H(q, p) called the Hamiltonian so that</p><formula xml:id="formula_1">dq dt = ∂H ∂p , dp dt = - ∂H ∂q .<label>(2)</label></formula><p>Equation 2 tells us that moving coordinates in the direction S H = ∂H ∂p , -∂H ∂q gives us the time evolution of the system. We can think of S as a vector field over the inputs of H. In fact, it is a special kind of vector field called a "symplectic gradient". Whereas moving in the direction of the gradient of H changes the output as quickly as possible, moving in the direction of the symplectic gradient keeps the output exactly constant. Hamilton used this mathematical framework to relate the position and momentum vectors (q, p) of a system to its total energy E tot = H(q, p). Then, he found S H using Equation 2 and obtained the dynamics of the system by integrating this field according to Equation 1. This is a powerful approach because it works for almost any system where the total energy is conserved.</p><p>Hamiltonian mechanics, like Newtonian mechanics, can predict the motion of a mass-spring system or a single pendulum. But its true strengths only become apparent when we tackle systems with many degrees of freedom. Celestial mechanics, which are chaotic for more than two bodies, are a good example. A few other examples include many-body quantum systems, fluid simulations, and condensed matter physics <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Hamiltonian Neural Networks. In this paper, we propose learning a parametric function for H instead of S H . In doing so, we endow our model with the ability to learn exactly conserved quantities from data in an unsupervised manner. During the forward pass, it consumes a set of coordinates and outputs a single scalar "energy-like" value. Then, before computing the loss, we take an in-graph gradient of the output with respect to the input coordinates (Figure A.1). It is with respect to this gradient that we compute and optimize an L 2 loss (Equation <ref type="formula" target="#formula_2">3</ref>).</p><formula xml:id="formula_2">L HN N = ∂H θ ∂p - ∂q ∂t 2 + ∂H θ ∂q + ∂p ∂t 2<label>(3)</label></formula><p>For a visual comparison between this approach and the baseline, refer to Figure <ref type="figure" target="#fig_3">1</ref> or Figure <ref type="figure" target="#fig_3">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>This training procedure allows HNNs to learn conserved quantities analogous to total energy straight from data. Apart from conservation laws, HNNs have several other interesting and potentially useful properties. First, they are perfectly reversible in that the mapping from (q, p) at one time to (q, p) at another time is bijective. Second, we can manipulate the HNN-conserved quantity (analogous to total energy) by integrating along the gradient of H, giving us an interesting counterfactual tool (e.g. "What would happen if we added 1 Joule of energy?"). We'll discuss these properties later in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning a Hamiltonian from Data</head><p>Optimizing the gradients of a neural network is a rare approach. There are a few previous works which do this <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b28">28]</ref>, but their scope and implementation details diverge from this work and from one another. With this in mind, our first step was to investigate the empirical properties of HNNs on three simple physics tasks.</p><p>Task 1: Ideal Mass-Spring. Our first task was to model the dynamics of the frictionless mass-spring system shown in Figure <ref type="figure" target="#fig_3">1</ref>. The system's Hamiltonian is given in Equation <ref type="formula" target="#formula_3">4</ref>where k is the spring constant and m is the mass constant. For simplicity, we set k = m = 1. Then we sampled initial coordinates with total energies uniformly distributed between [0.2, 1]. We constructed training and test sets of 25 trajectories each and added Gaussian noise with standard deviation σ 2 = 0.1 to every data point. Each trajectory had 30 observations; each observation was a concatenation of (q, p).</p><formula xml:id="formula_3">H = 1 2 kq 2 + p 2 2m<label>(4)</label></formula><p>Task 2: Ideal Pendulum. Our second task was to model a frictionless pendulum. Pendulums are nonlinear oscillators so they present a slightly more difficult problem. Writing the gravitational constant as g and the length of the pendulum as l, the general Hamiltonian is</p><formula xml:id="formula_4">H = 2mgl(1 -cos q) + l 2 p 2 2m<label>(5)</label></formula><p>Once again we set m = l = 1 for simplicity. This time, we set g = 3 and sampled initial coordinates with total energies in the range <ref type="bibr">[1.3, 2.3]</ref>. We chose these numbers in order to situate the dataset along the system's transition from linear to nonlinear dynamics. As with Task 1, we constructed training and test sets of 25 trajectories each and added the same amount of noise.</p><p>Task 3: Real Pendulum. Our third task featured the position and momentum readings from a real pendulum. We used data from a Science paper by Schmidt &amp; Lipson <ref type="bibr" target="#b35">[35]</ref> which also tackled the problem of learning conservation laws from data. This dataset was noisier than the synthetic ones and it did not strictly obey any conservation laws since the real pendulum had a small amount of friction.</p><p>Our goal here was to examine how HNNs fared on noisy and biased real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methods</head><p>In all three tasks, we trained our models with a learning rate of 10 -3 and used the Adam optimizer <ref type="bibr" target="#b20">[20]</ref>. Since the training sets were small, we set the batch size to be the total number of examples.</p><p>On each dataset we trained two fully-connected neural networks: the first was a baseline model that, given a vector input (q, p) output the vector (∂q/∂t, ∂p/∂t) directly. The second was an HNN that estimated the same vector using the derivative of a scalar quantity as shown in Equation <ref type="formula" target="#formula_1">2</ref>(also see Figure A.1). Where possible, we used analytic time derivatives as the targets. Otherwise, we calculated finite difference approximations. All of our models had three layers, 200 hidden units, and tanh activations. We trained them for 2000 gradient steps and evaluated them on the test set.</p><p>We logged three metrics: L 2 train loss, L 2 test loss, and mean squared error (MSE) between the true and predicted total energies. To determine the energy metric, we integrated our models according to Equation 1 starting from a random test point. Then we used MSE to measure how much a given model's dynamics diverged from the ground truth. Intuitively, the loss metrics measure our model's ability to fit individual data points while the energy metric measures its stability and conservation of energy over long timespans. To obtain dynamics, we integrated our models with the fourth-order Runge-Kutta integrator in scipy.integrate.solve_ivp and set the error tolerance to 10 -9 <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Ideal mass-spring</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ideal pendulum</head><p>Real pendulum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predictions MSE between coordinates Total HNN-conserved quantity Total energy</head><p>Figure <ref type="figure" target="#fig_4">2</ref>: Analysis of models trained on three simple physics tasks. In the first column, we observe that the baseline model's dynamics gradually drift away from the ground truth. The HNN retains a high degree of accuracy, even obscuring the black baseline in the first two plots. In the second column, the baseline's coordinate MSE error rapidly diverges whereas the HNN's does not. In the third column, we plot the quantity conserved by the HNN. Notice that it closely resembles the total energy of the system, which we plot in the fourth column. In consequence, the HNN roughly conserves total energy whereas the baseline does not.</p><p>We found that HNNs train as quickly as baseline models and converge to similar final losses. Table <ref type="table" target="#tab_0">1</ref> shows their relative performance over the three tasks. But even as HNNs tied with the baseline on on loss, they dramatically outperformed it on the MSE energy metric. Figure <ref type="figure" target="#fig_4">2</ref> shows why this is the case: as we integrate the two models over time, various errors accumulate in the baseline and it eventually diverges. Meanwhile, the HNN conserves a quantity that closely resembles total energy and diverges more slowly or not at all.</p><p>It's worth noting that the quantity conserved by the HNN is not equivalent to the total energy; rather, it's something very close to the total energy. The third and fourth columns of Figure <ref type="figure" target="#fig_4">2</ref> provide a useful comparison between the HNN-conserved quantity and the total energy. Looking closely at the spacing of the y axes, one can see that the HNN-conserved quantity has the same scale as total energy, but differs by a constant factor. Since energy is a relative quantity, this is perfectly acceptable <ref type="foot" target="#foot_3">3</ref> .</p><p>The total energy plot for the real pendulum shows another interesting pattern. Whereas the ground truth data does not quite conserve total energy, the HNN roughly conserves this quantity. This, in fact, is a fundamental limitation of HNNs: they assume a conserved quantity exists and thus are unable to account for things that violate this assumpation, such as friction. In order to account for friction, we would need to model it separately from the HNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Larger Systems</head><p>Having established baselines on a few simple tasks, our next step was to tackle a larger system involving more than one pair of (p, q) coordinates. One well-studied problem that fits this description is the two-body problem, which requires four (p, q) pairs.</p><formula xml:id="formula_5">H = |p CM | 2 m 1 + m 2 + |p 1 | 2 + |p 2 | 2 2µ + g m 1 m 2 |q 1 -q 2 | 2<label>(6)</label></formula><p>Task 4: Two-body problem. In the two-body problem, point particles interact with one another via an attractive force such as gravity. Once again, we let g be the gravitational constant and m represent mass. Equation <ref type="formula" target="#formula_5">6</ref>gives the Hamiltonian of the system where µ is the reduced mass and p CM is the momentum of the center of mass. As in previous tasks, we set m 1 = m 2 = g = 1 for simplicity. Furthermore, we restricted our experiments to systems where the momentum of the center of mass was zero. Even so, with eight degrees of freedom (given by the x and y position and momentum coordinates of the two bodies) this system represented an interesting challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Our first step was to generate a dataset of 1000 near-circular, two-body trajectories. We initialized every trajectory with center of mass zero, total momentum zero, and radius r = q 2 -q 1 in the range [0.5, 1.5]. In order to control the level of numerical stability, we chose initial velocities that gave perfectly circular orbits and then added Gaussian noise to them. We found that scaling this noise by a factor of σ 2 = 0.05 produced trajectories with a good balance between stability and diversity.</p><p>We used fourth-order Runge-Kutta integration to find 200 trajectories of 50 observations each and then performed an 80/20% train/test set split over trajectories. Our models and training procedure were identical to those described in Section 3 except this time we trained for 10,000 gradient steps and used a batch size of 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The HNN model scaled well to this system. The first row of Figure <ref type="figure" target="#fig_0">3</ref> suggests that it learned to conserve a quantity nearly equal to the total energy of the system whereas the baseline model did not.</p><p>The second row of Figure <ref type="figure" target="#fig_0">3</ref> gives a qualitative comparison of trajectories. After one orbit, the baseline dynamics have completely diverged from the ground truth whereas the HNN dynamics have only accumulated a small amount of error. As we continue to integrate up to t = 50 and beyond (Figure B.1), both models diverge but the HNN does so at a much slower rate. Even as the HNN diverges from the ground truth orbit, its total energy remains stable rather than decaying to zero or spiraling to infinity. We report quantitative results for this task in Table <ref type="table" target="#tab_0">1</ref>. Both train and test losses of the HNN model were about an order of magnitude lower than those of the baseline. The HNN did a better job of conserving total energy, with an energy MSE that was several orders of magnitude below the baseline. Having achieved success on the two-body problem, we ran the same set of experiments on the chaotic three-body problem. We show preliminary results in Appendix B where once again the HNN outperforms its baseline by a considerable margin. We opted to focus on the two-body results here because the three-body results still need improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning a Hamiltonian from Pixels</head><p>One of the key strengths of neural networks is that they can learn abstract representations directly from high-dimensional data such as pixels or words. Having trained HNN models on position and momentum coordinates, we were eager to see whether we could train them on arbitrary coordinates like the latent vectors of an autoencoder.</p><p>Task 5: Pixel Pendulum. With this in mind, we constructed a dataset of pixel observations of a pendulum and then combined an autoencoder with an HNN to model its dynamics. To our knowledge this is the first instance of a Hamiltonian learned directly from pixel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>In recent years, OpenAI Gym has been widely adopted by the machine learning community as a means for training and evaluating reinforcement learning agents <ref type="bibr" target="#b4">[5]</ref>. Some works have even trained world models on these environments <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Seeing these efforts as related and complimentary to our work, we used OpenAI Gym's Pendulum-v0 environment in this experiment.</p><p>First, we generated 200 trajectories of 100 frames each <ref type="foot" target="#foot_4">4</ref> . We required that the maximum absolute displacement of the pendulum arm be π 6 radians. Starting from 400 x 400 x 3 RGB pixel observations, we cropped, desaturated, and downsampled them to 28 x 28 x 1 frames and concatenated each frame with its successor so that the input to our model was a tensor of shape batch x 28 x 28 x 2. We used two frames so that velocity would be observable from the input. Without the ability to observe velocity, an autoencoder without recurrence would be unable to ascertain the system's full state space.</p><p>In designing the autoencoder portion of the model, our main objective was simplicity and trainability. We chose to use fully-connected layers in lieu of convolutional layers because they are simpler. Furthermore, convolutional layers sometimes struggle to extract even simple position information <ref type="bibr" target="#b23">[23]</ref>. Both the encoder and decoder were composed of four fully-connected layers with relu activations and residual connections. We used 200 hidden units on all layers except the latent vector z, where we used two units. As for the HNN component of this model, we used the same architecture and parameters as described in Section 3. Unless otherwise specified, we used the same training procedure as described in Section 4.1. We found that using a small amount of weight decay, 10 -5 in this case, was beneficial.</p><p>Losses. The most notable difference between this experiment and the others was the loss function. This loss function was composed of three terms: the first being the HNN loss, the second being a classic autoencoder loss (L 2 loss over pixels), and the third being an auxiliary loss on the autoencoder's latent space:</p><formula xml:id="formula_6">L CC = z t p -(z t q -z t+1 q ) 2 (7)</formula><p>The purpose of the auxiliary loss term, given in Equation <ref type="formula">7</ref>, was to make the second half of z, which we'll label z p , resemble the derivatives of the first half of z, which we'll label z q . This loss encouraged the latent vector (z q , z p ) to have roughly same properties as canonical coordinates (q, p). These properties, measured by the Poisson bracket relations, are necessary for writing a Hamiltonian. We found that the auxiliary loss did not degrade the autoencoder's performance. Furthermore, it is not domain-specific and can be used with any autoencoder with an even-sized latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>Hamiltonian NN Baseline NN We train an HNN and its baseline to predict dynamics in the latent space of an autoencoder. Then we project to pixel space for visualization. The baseline model rapidly decays to lower energy states whereas the HNN remains close to ground truth even after hundreds of frames. It mostly obscures the ground truth line in the bottom plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Unlike the baseline model, the HNN learned to conserve a scalar quantity analogous to the total energy of the system. This enabled it to predict accurate dynamics for the system over much longer timespans. Figure <ref type="figure" target="#fig_1">4</ref> shows a qualitative comparison of trajectories predicted by the two models. As in previous experiments, we computed these dynamics using Equation 2 and a fourth-order Runge-Kutta integrator. Unlike previous experiments, we performed this integration in the latent space of the autoencoder. Then, after integration, we projected to pixel space using the decoder network. The HNN and its baseline reached comparable train and test losses, but once again, the HNN dramatically outperformed the baseline on the energy metric (Table <ref type="table" target="#tab_0">1</ref>).  Adding and removing energy. So far, we have seen that integrating the symplectic gradient of the Hamiltonian can give us the time evolution of a system but we have not tried following the Riemann gradient R H = ∂H ∂q , ∂H ∂p . Intuitively, this corresponds to adding or removing some of the HNN-conserved quantity from the system. It's especially interesting to alternate between integrating R H and S H . Figure <ref type="figure" target="#fig_2">5</ref> shows how we can take advantage of this effect to "bump" the pendulum to a higher energy level. We could imagine using this technique to answer counterfactual questions e.g. "What would have happened if we applied a torque?" Perfect reversibility. As neural networks have grown in size, the memory consumption of transient activations, the intermediate activations saved for backpropagation, has become a notable bottleneck. Several works propose semireversible models that construct one layer's activations from the activations of the next <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b19">19]</ref>. Neural ODEs also have this property <ref type="bibr" target="#b6">[7]</ref>. Many of these models are only approximately reversible: their mappings are not quite bijective. Unlike those methods, our approach is guaranteed to produce trajectories that are perfectly reversible through time. We can simply refer to a result from Hamiltonian mechanics called Liouville's Theorem: the density of particles in phase space is constant. What this implies is that any mapping (q 0 , p 0 ) → (q 1 , p 1 ) is bijective/invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Learning physical laws from data. Schmidt &amp; Lipson <ref type="bibr" target="#b35">[35]</ref> used a genetic algorithm to search a space of mathematical functions for conservation laws and recovered the Lagrangians and Hamiltonians of several real systems. We were inspired by their approach, but used a neural neural network to avoid constraining our search to a set of hand-picked functions. Two recent works are similar to this paper in that the authors sought to uncover physical laws from data using neural networks <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b3">4]</ref>. Unlike our work, they did not explicitly parameterize Hamiltonians.</p><p>Physics priors for neural networks. A wealth of previous works have sought to furnish neural networks with better physics priors. Many of these works are domain-specific: the authors used domain knowledge about molecular dynamics <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">28]</ref>, quantum mechanics [36], or robotics <ref type="bibr" target="#b24">[24]</ref> to help their models train faster or generalize. Others, such as Interaction Networks or Relational Networks were meant to be fully general <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b1">2]</ref>. Here, we also aimed to keep our approach fully general while introducing a strong and theoretically-motivated prior.</p><p>Modeling energy surfaces. Physicists, particularly those studying molecular dynamics, have seen success using neural networks to model energy surfaces <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b44">44]</ref>. In particular, several works have shown dramatic computation speedups compared to density functional theory <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b7">8]</ref>. Molecular dynamics researchers integrate the derivatives of energy in order to obtain dynamics, just as we did in this work. A key difference between these approaches and our own is that 1) we emphasize the Hamiltonian formalism 2) we optimize the gradients of our model (though some works do optimize the gradients of a molecular dynamics model <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b28">28]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Whereas Hamiltonian mechanics is an old and well-established theory, the science of deep learning is still in its infancy. Whereas Hamiltonian mechanics describes the real world from first principles, deep learning does so starting from data. We believe that Hamiltonian Neural Networks, and models like them, represent a promising way of bringing together the strengths of both approaches.  Three body problem. As mentioned briefly in the body of the paper, we also trained our models on the three body problem. The results we report here show a relative advantage to using the HNN over the baseline model. However, both models struggled to accurately model the dynamics of the three-body problem, which is why we relegated these results to the Appendix. Going forward, we hope to improve these results to the point where they can play a more substantial role in Section 4. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Analysis of an example 2-body trajectory. The dynamics of the baseline model do not conserve total energy and quickly diverge from ground truth. The HNN, meanwhile, approximately conserves total energy and accrues a small amount of error after one full orbit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Predicting the dynamics of the pixel pendulum. We train an HNN and its baseline to predict dynamics in the latent space of an autoencoder. Then we project to pixel space for visualization. The baseline model rapidly decays to lower energy states whereas the HNN remains close to ground truth even after hundreds of frames. It mostly obscures the ground truth line in the bottom plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Visualizing integration in the latent space of the Pixel Pendulum model. We alternately integrate S H at low energy (blue circle), R H (purple line), and then S H at higher energy (red circle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure B. 1 :</head><label>1</label><figDesc>Figure B.1: More qualitative results for the orbit task. Numerical errors accumulate in the baseline model until the bodies end up traveling in opposite directions. The total energy diverges towards infinity as well. In comparison, the HNN's trajectory diverges from the ground truth but continues to roughly conserve the total energy of the system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure B. 2 :</head><label>2</label><figDesc>Figure B.2: Comparison of how well the HNN conserves total energy compared to the baseline its baseline on the two-body task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>Figure C.1: Latent space plots from the Pixel Pendulum model. Note that the learned latent space bears a strong resemblance to the true phase space of a pendulum. In particular, there is a faint diamond shape to the outer contour lines of Figure1(b). This pattern is reminiscent of the nonlinear dynamics we observed in the ideal pendulum phase space plot of Figure2(row 2, column 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Quantitative results across all five tasks. Whereas the HNN is competitive with the baseline on train/test loss, it dramatically outperforms the baseline on the energy metric. All values are multiplied by 10 3 unless noted otherwise. See Appendix A for a note on train/test split for Task 3.While main purpose of HNNs is to endow neural networks with better physics priors, in this section we ask what other useful properties these models might have.</figDesc><table><row><cell></cell><cell>Train loss</cell><cell></cell><cell>Test loss</cell><cell></cell><cell>Energy</cell><cell></cell></row><row><cell>Task</cell><cell>Baseline</cell><cell>HNN</cell><cell cols="2">Baseline HNN</cell><cell>Baseline</cell><cell>HNN</cell></row><row><cell cols="2">1: Ideal mass-spring 37 ± 2</cell><cell>37 ± 2</cell><cell>37 ± 2</cell><cell>36 ± 2</cell><cell>170 ± 20</cell><cell>.38 ± .1</cell></row><row><cell>2: Ideal pendulum</cell><cell>33 ± 2</cell><cell>33 ± 2</cell><cell>35 ± 2</cell><cell>36 ± 2</cell><cell>42 ± 10</cell><cell>25 ± 5</cell></row><row><cell>3: Real pendulum</cell><cell>2.7 ± .2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>9.2 ± .5 2.2 ± .3 6.0 ± .6 390 ± 7 14 ± 5 4: Two body (×10 6 ) 33 ± 1 3.0 ± .1 30 ± .1 2.8 ± .1 6.3e4 ± 3e4 39 ± 5 5: Pixel pendulum 18 ± .2 19 ± .2 17 ± .3 18 ± .3 9.3 ± 1 .15 ± .01 6 Useful properties of HNNs</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Preprint. Under review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We make our code available at github.com/greydanus/hamiltonian-nn.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Any coordinates that describe the state of the system. Later we will use position and momentum (p, q).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>To see why energy is relative, imagine a cat that is at an elevation of 0 m in one reference frame and 1 m in another. Its potential energy (and total energy) will differ by a constant factor depending on frame of reference.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Choosing the "no torque" action at every timestep.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>Sam Greydanus would like to thank the <rs type="programName">Google AI Residency Program</rs> for providing extraordinary mentorship and resources. The authors would like to thank <rs type="person">Nic Ford</rs>, <rs type="person">Trevor Gale</rs>, <rs type="person">Rapha Gontijo Lopes</rs>, <rs type="person">Keren Gu</rs>, <rs type="person">Ben Caine</rs>, <rs type="person">Mark Woodward</rs>, <rs type="person">Stephan Hoyer</rs>, <rs type="person">Jascha Sohl-Dickstein</rs>, and many others for insightful conversations and support. Special thanks to <rs type="person">James</rs> and <rs type="person">Judy Greydanus</rs> for their feedback and support from beginning to end.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NVKStpC">
					<orgName type="program" subtype="full">Google AI Residency Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Supplementary Information for Tasks 1-3 Training details. We selected hyperparameters using a coarse grid search over learning rates {10 -1 , 10 -2 , 10 -3 }, layer widths {100, 200, 300}, activations {tanh, relu}, and batch size where relevant {100, 200}. The main objective of this work was not to produce state-of-the-art results, so the settings we chose were aimed simply at producing models that gave good qualitative performance on the tasks at hand. We used weight decay of 10 -4 on the first three tasks.</p><p>We trained all of these experiments on a desktop CPU.</p><p>The train/test split on Task 3. We partitioned the train and test sets on Task 3 in an unusual manner. The dataset provided by <ref type="bibr" target="#b35">[35]</ref> consisted of just a single trajectory from a real pendulum, as shown in the second panel of Figure <ref type="figure">2</ref>(c). We needed to evaluate our model's performance over a series of adjacent time steps in order to measure the energy MSE metric. For this reason, we were forced to use the first 4/5 of this trajectory for training (black vectors in Figure <ref type="figure">2(c</ref>)) and the last 1/5 for evaluation (red vectors).</p><p>The consequence of this train/test split is that our test set had a slightly different distribution from our training set. We found that the relative magnitudes of the test losses between the baseline and HNN models were informative. We did not perform this ungainly train/test split on the other two tasks in this section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Supplementary Information for Task 4: Two-body problem</head><p>Training details. We selected hyperparameters with a grid search as described in the previous section. Again, the main objective of this work was not to produce state-of-the-art results, so the settings we chose were aimed simply at producing models that gave good qualitative performance on the tasks at hand. We did not use weight decay on this task, though when we tried a weight decay of 10 -4 or results did not change significantly.</p><p>We trained this experiment on a desktop CPU. C Supplementary Information for Task 5: Pixel Pendulum Training details. We selected hyperparameters with a grid search as described in the previous section. We used a weight decay of 10 -5 on this experiment. We found that, unlike previous experiments, weight decay had a significant impact on results. We suspect that this is because the scale of the gradients on the weights of the HNN portion of the model were different from the scale of the gradients of the weights of the autoencoder portion of the model.</p><p>We trained this experiment on a desktop CPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pachocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00177</idno>
		<title level="m">Learning dexterous in-hand manipulation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural network potential-energy surfaces in chemistry: a tool for large-scale simulations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Behler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Chemistry Chemical Physics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="17930" to="17955" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning symmetries of classical integrable systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bondesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamacraft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04645</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00341</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning of accurate energy-conserving molecular force fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poltavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1603015</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Photons and atoms-introduction to quantum electrodynamics. Photons and Atoms-Introduction to Quantum Electrodynamics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cohen-Tannoudji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dupont-Roc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grynberg</surname></persName>
		</author>
		<editor>Claude Cohen-Tannoudji, Jacques Dupont-Roc, Gilbert Grynberg</editor>
		<imprint>
			<biblScope unit="page">486</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wiley-Vch</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997-02">February 1997. 1997</date>
			<biblScope unit="page">486</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end differentiable physics for learning and control</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Avila Belbute-Peres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7178" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-dimensional neural network potentials for organic reactions and an improved training algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marquetand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2187" to="2198" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Modern condensed matter physics</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Girvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2214" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">NeuroAnimator: fast neural network emulation and control of physics-based models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent world models facilitate policy evolution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2450" to="2462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning latent dynamics for planning from pixels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04551</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01203</idno>
		<title level="m">Relational inductive bias for physical construction in humans and machines</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discovering physical concepts with neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Metger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wilming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Renner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10300</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07088</idno>
		<title level="m">Deep invertible networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<title level="m">A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="421" to="436" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9605" to="9616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep lagrangian networks: Using physics as model prior for deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reversible recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9029" to="9040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Playing Atari with Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Invariant variation problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Noether</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transport Theory and Statistical Physics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="186" to="207" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pukrittayakamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malshe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Raff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Narulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bukkapatnum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Komanduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">134101</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A modern course in statistical physics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Reichl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>AAPT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Über die numerische auflösung von differentialgleichungen</title>
		<author>
			<persName><forename type="first">C</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="1895">1895</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast and accurate modeling of molecular atomization energies with machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">58301</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Commins</surname></persName>
		</author>
		<title level="m">Modern quantum mechanics</title>
		<imprint>
			<publisher>AAPT</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note>revised edition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hamiltonian fluid mechanics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of fluid mechanics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="256" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distilling free-form natural laws from experimental data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">5923</biblScope>
			<biblScope unit="page" from="81" to="85" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantumchemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ani-1: an extensible neural network potential with dft accuracy at force field computational cost</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Roitberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3192" to="3203" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Classical mechanics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>University Science Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerating eulerian fluid simulation with convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schlachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3424" to="3433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Machine learning of coarse-grained molecular dynamics force fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wehmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Charron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Fabritiis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Noe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clementi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACS Central Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual interaction networks: Learning a physics simulator from video</title>
		<author>
			<persName><forename type="first">N</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The tensormol-0.1 model chemistry: A neural network augmented with long-range physics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parkhill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2261" to="2269" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Evolving robot gaits in hardware: the hyperneat generative encoding vs. parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Zagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th European Conference on Artificial Life</title>
		<meeting>the 20th European Conference on Artificial Life</meeting>
		<imprint>
			<date type="published" when="2011-08">August 2011</date>
			<biblScope unit="page" from="890" to="897" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
