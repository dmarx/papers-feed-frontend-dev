# Apollo: Band-sequence Modeling for High-Quality Audio Restoration

## Abstract

## 

Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid-and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving lowfrequency information while accurately reconstructing high-quality midand high-frequency content. Inspired by recent advancements in highsample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-samplerate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at [https://github.com/JusperLee/Apollo](https://github.com/JusperLee/Apollo).

## I. INTRODUCTION

Audio restoration has gained widespread application across various scenarios, ranging from music playback to real-time communication systems. For instance, in restoring vintage music, audio restoration methods effectively rejuvenate classic music pieces eroded by time or constrained by outdated equipment [[1]](#b0), [[2]](#b1). Moreover, these methods are found to be extensively used in speech communication, particularly in telephone or internet calls, by repairing low-quality or distorted codec audio at the receiving end, thereby delivering a clearer and more natural auditory experience [[3]](#b2)- [[6]](#b5). In music playback, audio restoration mitigates the degradation caused by compression, ensuring that users enjoy high-fidelity audio [[3]](#b2), [[7]](#b6), [[8]](#b7). For generative models, such as those used in music generation and speech synthesis, the audio quality is crucial, and restoration methods can enhance data quality, thus significantly improving model performance [[9]](#b8), [[10]](#b9). Robust audio restoration methods have become indispensable components of modern audio processing systems.

Audio restoration involves predicting high-quality, undistorted audio from degraded or compressed inputs. Current audio restoration technologies primarily focus on vocal recovery [[3]](#b2)- [[5]](#b4). In traditional methods, a common technique is bandwidth extension [[4]](#b3), [[5]](#b4), which aims to reconstruct lost high-frequency information and improve the perceptual quality of highly compressed audio signals. High-frequency spectral extension enhances encoding efficiency and proves crucial in low-bitrate scenarios [[11]](#b10). However, in some cases, bandwidth extension can introduce high-frequency artifacts that may degrade the overall audio signal quality.

With the rapid advancement of deep learning, NN-based methods have gradually replaced traditional signal-processing methods. Recently, GANs [[12]](#b11) have demonstrated substantial potential in audio super-resolution and restoration tasks [[1]](#b0), [[13]](#b12), especially in achieving high-quality restoration. In audio codecs [[14]](#b13)- [[16]](#b15), GANs effectively balance perceptual audio quality with distortion, offering superior restoration performance compared to traditional methods. Audio degradation typically affects the mid-to-high-frequency bands, particularly when using lossy codecs such as MP3 or AAC [[17]](#b16), where high-frequency information is prone to compression artifacts. An ideal generator should retain the original audio's lowfrequency components and supplement smooth and delicate midto-high-frequency details, thereby achieving a more realistic audio restoration effect. The Gull codec [[16]](#b15) has successfully demonstrated the effectiveness of GANs in the audio codec, showing significant progress in the super-resolution reconstruction of music and speech during the decoding phase of lossy codecs.

Inspired by Gull, we propose the Apollo model, a generative model specifically designed for high-sampling-rate audio restoration tasks. Apollo supports restoring audio quality at different compression rates. It comprises three main modules: a frequency band split module, a frequency band sequence modeling module, and a frequency band reconstruction module. Unlike Gull, we employ Roformer [[18]](#b17) in the frequency band sequence modeling module to capture frequency features and use TCN to model temporal features, enabling more efficient audio restoration. Specifically, Apollo first divides the spectrogram into sub-band spectrograms with predefined bandwidths, extracts gain-shape representations for each sub-band spectrogram, and encodes them through a bottleneck layer. Subsequently, stacked frequency band-sequence modeling modules perform interleaved modeling across frequency bands and sequences. Finally, each subband feature is mapped through nonlinear layers to generate the estimated restored sub-band spectrogram. These modules' design ensures the preservation of low-frequency information while restoring high-quality mid and high-frequency components. Additionally, with causal convolution and causal Roformer, our model supports streaming processing, making it suitable for real-time audio restoration.

We evaluated Apollo on the MUSDB18-HQ [[19]](#b18) and MoisesDB [[20]](#b19) datasets, comparing it with state-of-the-art models such as SR-GAN [[1]](#b0). The experimental results showed that Apollo performed exceptionally well across various compression bitrates and music genres, particularly in complex scenarios involving a mixture of multiple instruments and vocals. Additionally, Apollo's efficiency in streaming audio applications has been validated, demonstrating its potential in real-time, high-quality audio restoration. 

## II. APOLLO

## A. Overall Pipeline

Fig. [1](#fig_0)(a) presents the proposed Apollo pipeline. Apollo operates in the time-frequency domain and comprises a band-split module, a band-sequence modeling module, and a band-reconstruction module. Specifically, given compressed or distorted audio S ∈ R 1×L , we first transfer S to its time-frequency domain representation X ∈ C F ×T using the Short-Time Fourier Transform (STFT), where L denotes the length of audio, F and T denote the number of frequency bins and frames, respectively. Then, the band-split module maps to subband embeddings Z ∈ R N ×T using gain-shape representations G ∈ R 3×M ×T for each sub-band, where N and M denote the number of channels in sub-band embeddings and gain-shape representations, respectively. Next, the band-sequence modeling module performs joint modeling of temporal and sub-band using a stacked architecture based on Roformer [[18]](#b17) and temporal convolutional network (TCN) [[21]](#b20), [[22]](#b21). Finally, the band-reconstruction module converts the output Q ∈ R N ×T of the band-sequence modeling module into the reconstructed complex-valued spectrogram Y ∈ C F ×T . It uses the inverse Short-Time Fourier Transform (iSTFT) to convert Y to a waveform S ∈ R 1×L .

## B. Band-split Module

As shown in Fig. [1](#fig_0)(b), given compressed or distorted audio spectrogram X, we first split its frequency dimension

$F into K sub-band spectrograms {X k ∈ C M k ×T |k ∈ [1, K]}.$Inspired by the Gull codec [[16]](#b15), we extract gain-shape representations G k ∈ R 3×M k ×T for each sub-band spectrogram:

$G k = Concat Re(X k ) ∥X k ∥2 , Im(X k ) ∥X k ∥2 , log (∥X k ∥2) ,(1)$where Re(X k ) and Im(X k ) denote the real and imaginary parts, respectively. ∥X k ∥2 represents the ℓ2-norm of X k , given by:

$∥X k ∥2 = Re(X k ) 2 + Im(X k ) 2(2)$log (∥X k ∥2) is the logarithm of the ℓ2-norm of X k . Concat refers to the concatenation of components. The gain-shape representation decouples the sub-band spectrogram's content and energy, allowing the reconstruction model to learn appropriate mappings that preserve the audio content. Subsequently, we map the gain-shape representations G into high-dimensional embeddings Z through a bottleneck layer, which consists of RMSNorm [[23]](#b22) and a 1D convolutional layer.

## C. Band-sequence Modeling Module

In Apollo, we employ stacked Band-sequence modeling modules (BS modules, Fig. [1(c](#fig_1))) to perform joint sub-band and temporal modeling with a stacking depth of B. Unlike BSRNN [[24]](#b23) and Gull [[16]](#b15), each BS module consists of a series of residual Roformers [[18]](#b17) and TCNs, which sequentially scan along the sub-band and time dimensions, and can increase the modeling capacity to improve the model performance. First, the residual Roformer is applied to the input Z along the frequency band dimension K to obtain Z ′ ∈ R N ×T , capturing global dependencies between sub-bands while preserving

TABLE I THE STRUCTURE OF THE STFT DISCRIMINATOR NETWORK.

## Layer Index

Layer Type Input Channels Output Channels Kernel Size Padding Stride Activation 

$1 SpectralNorm + Conv2d F F (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 2 SpectralNorm + Conv2d F F × 2 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 3 SpectralNorm + Conv2d F × 2 F × 4 (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 4 SpectralNorm + Conv2d F × 4 F × 8 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 5 SpectralNorm + Conv2d F × 8 F × 16 (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 6 SpectralNorm + Conv2d F × 16 F × 32 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 7 Conv2d F × 32 1 (3, 3) (1, 1)(1$
## D. Band-reconstruction Module

The output Q is passed through sub-band-specific fully connected (FC) layers to generate the estimated real and imaginary parts of the restored sub-band spectrograms (see Fig. [1(d)](#fig_1)). We utilize RMSNorm as the normalization layer within the fully connected layers and employ Gated Linear Units (GLUs) as the nonlinear activation function. Subsequently, the K reconstructed sub-band spectrograms are concatenated along the frequency dimension to form the final reconstructed complex-valued spectrogram Y. Finally, the reconstructed complex-valued spectrogram Y is converted back to the waveform domain S through the iSTFT.

## E. Training Objection

The proposed Apollo model is trained using a GAN framework to enhance the quality of audio restoration. Specifically, the discriminator network is inspired by the multi-resolution STFT discriminator, similar to the Gull codec [[16]](#b15). As described in Table [I](#), the discriminator input consists of the spectrogram's real and imaginary parts, which are stacked into a 3D tensor along the channel dimension. To ensure energy invariance in the input, the signal is normalized to have a unit ℓ2-norm before being passed into the discriminator. The discriminator is trained using the Least Squares GAN (LSGAN) loss [[25]](#b24), defined as:

$LD = I i=1 E A∼p data (Di(A) -1) 2 + I i=1 E Y∼p G (Di(Y)) 2 , (3)$where A ∈ C F ×T denotes the spectrogram of uncompressed audio and I = 5 denotes the number of discriminator.

The generator, Apollo, is optimized through a composite loss function, which includes the reconstruction loss, feature matching loss, and the adversarial loss from the discriminator. The reconstruction loss Lrec is based on the mean absolute error (MAE) between the magnitude spectrograms of the restored and target audio, evaluated over multiple STFT resolutions:

$Lrec = 1 W W w=1 ∥|STFTw(Y)| -|STFTw(A)|∥ 1 ∥|STFTw(T)|∥ 1 ,(4)$where STFTw denotes the STFT with window size w ∈ [32, [64,](#)[128,](#)[256,](#)[512,](#)[1024,](#)[2048]](#). This multi-resolution approach allows the model to capture fine and coarse details, leading to accurate restoration of audio signals across various frequency ranges. The feature matching loss is defined as the layer-wise normalized MAE between the hidden representations of the discriminator for both the reconstructed and target signals. These hidden representations, denoted as Hi,j for the reconstructed signal and Hi,j for the target signal, are obtained from the j-th layer of the i-th discriminator. The feature matching loss is computed as follows:

$LFM = 1 5 5 i=1 1 6 6 j=1 E Hi,j -sg[Hi,j] mean (|sg[Hi,j]|) . (5$$)$The reconstruction loss Lrec is calculated as the multi-resolution frequency MAE over several STFT window sizes, ensuring that both short-and long-term signal characteristics are restored effectively:

$Lrec = 1 W W w=1 ∥|STFTw(Y)| -|STFTw(A)|∥ 1 .(6)$The overall generator loss combines reconstruction, feature matching, and adversarial losses, expressed as:

$LG = αLrec + βLFM + γLGAN(7)$where α = 1, β = 1, and γ = 1 are hyperparameters used to balance the contributions of the individual loss components. This comprehensive loss formulation ensures that Apollo reconstructs not only accurate audio signals but also maintains perceptual quality and adversarial robustness by leveraging multi-resolution STFT losses and feature-matching mechanisms.

## III. EXPERIMENT CONFIGURATIONS

## A. Datasets

We trained and tested Apollo on the combined MUSDB18-HQ [[19]](#b18) and MoisesDB [[20]](#b19) datasets. By integrating these two datasets, we leveraged their rich diversity and comprehensive musical resources to evaluate Apollo's restoration performance across different music genres more thoroughly. During the data preprocessing stage, inspired by music separation techniques [[10]](#b9), [[26]](#b25), we employed a Source Activity Detector (SAD) to remove silent regions from the tracks, retaining only the significant portions for training. Throughout training, we implemented real-time data augmentation by randomly mixing tracks from different songs. Specifically, we randomly selected between 1 and 8 stems from 11 available tracks and extracted 3-second clips from each selected stem. These clips were then randomly scaled in energy within a range of [-10, 10] dB relative to their original levels. All selected stem clips were summed together to generate simulated music. Subsequently, we simulated dynamic bitrate scenarios by applying MP3 codecs with bitrates of [24000, 32000, 48000, 64000, 96000, 128000] to generate the compressed music. To ensure all samples were on the same scale, we rescaled both the target audio and the encoded audio based on the maximum absolute value. TABLE II DIFFERENT METHODS' SDR/SI-SNR/VISQOL SCORES FOR VARIOUS TYPES OF MUSIC, AS WELL AS THE NUMBER OF MODEL PARAMETERS AND GPU INFERENCE TIME. FOR THE GPU INFERENCE TIME TEST, A MUSIC SIGNAL WITH A SAMPLING RATE OF 44.1 KHZ AND A LENGTH OF 1 SECOND WAS USED. Model Vocal Single Stem Multi-Stems Multi-Stems+Vocal Overall Params (M) RTF (ms) SR-GAN [1] 10.62/9.19/2.72 13.88/12.52/3.28 14.92/14.16/3.41 16.87/15.54/3.76 14.07/12.85/3.29 322.53 34.55 Apollo (Ours) 13.99/12.58/3.44 16.56/15.99/4.08 17.52/17.15/4.41 18.51/18.26/4.54 16.64/16.00/4.12 16.54 53.23

## B. Hyperparameters

For the proposed Apollo model, the Short-Time Fourier Transform (STFT) window length was set to 20 ms with a hop size of 10 ms, using a Hanning window. The bandwidth for frequency band segmentation was set to 160 Hz, and the feature dimension N was set to 256. The Band Sequence modeling module was stacked B = 6 times. In the discriminator network, the STFT window sizes were configured with a multi-scale setup, including [[32,](#)[64,](#)[128,](#)[256,](#)[512,](#)[1024,](#)[2048]](#). For the optimizer, both the generator and discriminator utilize the AdamW optimizer [[27]](#b26). The generator's initial learning rate was set to 0.001, with a weight decay of 0.01, while the discriminator's initial learning rate was set to 0.0001, with the same weight decay of 0.01. The learning rate decayed by 0.98 every two epochs, and gradient clipping with a maximum norm of 5 was employed to prevent gradient explosion. Additionally, we implemented an early stopping mechanism to prevent overfitting: training was terminated if the validation loss did not decrease for 20 consecutive epochs. All models were trained on 8 Nvidia RTX 4090 GPUs.

## C. Evaluation metrics

In all experiments, we used the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) [[28]](#b27), Signal-to-Distortion Ratio (SDR) [[29]](#b28), and Virtual Speech Quality Objective Listener (VISQOL) [[30]](#b29) to evaluate the quality of the reconstructed audio. To assess the model's efficiency, we reported the time consumption per second of audio processed by Apollo and SR-GAN (Real-Time Factor, RTF). RTF is calculated by processing 1-second audio tracks sampled at 44.1 kHz on both CPU and GPU, and the average value is taken after running 1000 iterations. Additionally, we measured the model size by reporting the number of parameters using the open-source tool PyTorch-OpCounter 1 .

## IV. RESULTS

We evaluated the restoration performance of the Stochastic-Restoration-GAN (SR-GAN) [[1]](#b0) and Apollo models across various bitrates and music genres on the combined test set of MUSDB18-HQ and MoisesDB (with 5000 samples for each case). The test set 1 [https://github.com/Lyken17/pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter) encompasses a wide range of music genres, including vocals, single instruments, and mixed instruments, aiming to comprehensively assess each model's restoration capabilities.

Bitrate Impact Analysis. Fig. [2](#fig_2) compares the performance of the Apollo model and the Stochastic-Restoration-GAN (SR-GAN) at different bitrates (ranging from 24 kHz to 128 kHz). The experimental results demonstrated that Apollo consistently outperformed SR-GAN across all bitrates, particularly in addressing issues such as frequency band voids or reduced signal bandwidth, as reflected by SI-SNR and SDR scores. Additionally, Apollo significantly improved audio restoration quality as measured by VISQOL. Project page[foot_0](#foot_0) for Apollo's reconstructed audio given multiple MP3 bitrates.

Music Genre Impact Analysis. Table II further illustrates the performance of both models across different music genres. In audio scenarios involving vocals, single instruments, mixed instruments, and a combination of instruments with vocals, Apollo consistently surpasses SR-GAN, with its advantage being especially pronounced in complex scenarios with mixed instruments and vocals. This is attributed to Apollo's alternating band and sequence modeling design, which emphasizes capturing and restoring complex spectral information. Compared to SR-GAN, Apollo delivers higher user ratings (VISQOL) with comparable inference speed while maintaining a more compact model size. This is especially important for realtime communications and live audio restoration, where low latency is critical to the user experience.

## V. CONCLUSION

We propose Apollo, a novel method specifically designed for compressed audio restoration. Apollo significantly enhances audio quality in the frequency domain through band split, sequence modeling, and reconstruction modules. Empirical evaluations on the integrated MUSDB18-HQ and MoisesDB datasets validate Apollo's outstanding performance. Notably, Apollo achieves substantial improvements in music restoration while maintaining a smaller model size and high computational efficiency. The experimental results demonstrated that when addressing the complex acoustic characteristics of music, bandsplit, and band-sequence modeling more effectively captured and restored audio information lost during compression.

![Fig. 1. Overall pipeline of the model architecture of Apollo and its modules.]()

![Nonethe local characteristics of the frequency domain signals. Next, the TCN is applied along the time dimension T on Z ′ to generate the output Q ∈ R N ×T . Since the K sub-band features share the same feature dimension N , they all share a single TCN. The TCN consists of three convolutional blocks, each containing three convolutional layers. This design allows the TCN module to efficiently handle shortterm dependencies and local temporal dynamics in audio signals, enhancing the model's ability to capture and understand temporal domain features.]()

![Fig. 2. Apollo and SR-GAN's SDR, SI-SNR and ViSQOL result in comparison at different bitrates.]()

https://cslikai.cn/Apollo/

