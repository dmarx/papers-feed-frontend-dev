<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Apollo: Band-sequence Modeling for High-Quality Audio Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">AI Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Apollo: Band-sequence Modeling for High-Quality Audio Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">11061F32F795A27CF0FC072085A89B7F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Audio restoration</term>
					<term>audio superresolution</term>
					<term>bandwidth extension</term>
					<term>generative adversarial network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio restoration has become increasingly significant in modern society, not only due to the demand for high-quality auditory experiences enabled by advanced playback devices, but also because the growing capabilities of generative audio models necessitate high-fidelity audio. Typically, audio restoration is defined as a task of predicting undistorted audio from damaged input, often trained using a GAN framework to balance perception and distortion. Since audio degradation is primarily concentrated in mid-and high-frequency ranges, especially due to codecs, a key challenge lies in designing a generator capable of preserving lowfrequency information while accurately reconstructing high-quality midand high-frequency content. Inspired by recent advancements in highsample-rate music separation, speech enhancement, and audio codec models, we propose Apollo, a generative model designed for high-samplerate audio restoration. Apollo employs an explicit frequency band split module to model the relationships between different frequency bands, allowing for more coherent and higher-quality restored audio. Evaluated on the MUSDB18-HQ and MoisesDB datasets, Apollo consistently outperforms existing SR-GAN models across various bit rates and music genres, particularly excelling in complex scenarios involving mixtures of multiple instruments and vocals. Apollo significantly improves music restoration quality while maintaining computational efficiency. The source code for Apollo is publicly available at <ref type="url" target="https://github.com/JusperLee/Apollo">https://github.com/JusperLee/Apollo</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Audio restoration has gained widespread application across various scenarios, ranging from music playback to real-time communication systems. For instance, in restoring vintage music, audio restoration methods effectively rejuvenate classic music pieces eroded by time or constrained by outdated equipment <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Moreover, these methods are found to be extensively used in speech communication, particularly in telephone or internet calls, by repairing low-quality or distorted codec audio at the receiving end, thereby delivering a clearer and more natural auditory experience <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref>. In music playback, audio restoration mitigates the degradation caused by compression, ensuring that users enjoy high-fidelity audio <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. For generative models, such as those used in music generation and speech synthesis, the audio quality is crucial, and restoration methods can enhance data quality, thus significantly improving model performance <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Robust audio restoration methods have become indispensable components of modern audio processing systems.</p><p>Audio restoration involves predicting high-quality, undistorted audio from degraded or compressed inputs. Current audio restoration technologies primarily focus on vocal recovery <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. In traditional methods, a common technique is bandwidth extension <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which aims to reconstruct lost high-frequency information and improve the perceptual quality of highly compressed audio signals. High-frequency spectral extension enhances encoding efficiency and proves crucial in low-bitrate scenarios <ref type="bibr" target="#b10">[11]</ref>. However, in some cases, bandwidth extension can introduce high-frequency artifacts that may degrade the overall audio signal quality.</p><p>With the rapid advancement of deep learning, NN-based methods have gradually replaced traditional signal-processing methods. Recently, GANs <ref type="bibr" target="#b11">[12]</ref> have demonstrated substantial potential in audio super-resolution and restoration tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, especially in achieving high-quality restoration. In audio codecs <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, GANs effectively balance perceptual audio quality with distortion, offering superior restoration performance compared to traditional methods. Audio degradation typically affects the mid-to-high-frequency bands, particularly when using lossy codecs such as MP3 or AAC <ref type="bibr" target="#b16">[17]</ref>, where high-frequency information is prone to compression artifacts. An ideal generator should retain the original audio's lowfrequency components and supplement smooth and delicate midto-high-frequency details, thereby achieving a more realistic audio restoration effect. The Gull codec <ref type="bibr" target="#b15">[16]</ref> has successfully demonstrated the effectiveness of GANs in the audio codec, showing significant progress in the super-resolution reconstruction of music and speech during the decoding phase of lossy codecs.</p><p>Inspired by Gull, we propose the Apollo model, a generative model specifically designed for high-sampling-rate audio restoration tasks. Apollo supports restoring audio quality at different compression rates. It comprises three main modules: a frequency band split module, a frequency band sequence modeling module, and a frequency band reconstruction module. Unlike Gull, we employ Roformer <ref type="bibr" target="#b17">[18]</ref> in the frequency band sequence modeling module to capture frequency features and use TCN to model temporal features, enabling more efficient audio restoration. Specifically, Apollo first divides the spectrogram into sub-band spectrograms with predefined bandwidths, extracts gain-shape representations for each sub-band spectrogram, and encodes them through a bottleneck layer. Subsequently, stacked frequency band-sequence modeling modules perform interleaved modeling across frequency bands and sequences. Finally, each subband feature is mapped through nonlinear layers to generate the estimated restored sub-band spectrogram. These modules' design ensures the preservation of low-frequency information while restoring high-quality mid and high-frequency components. Additionally, with causal convolution and causal Roformer, our model supports streaming processing, making it suitable for real-time audio restoration.</p><p>We evaluated Apollo on the MUSDB18-HQ <ref type="bibr" target="#b18">[19]</ref> and MoisesDB <ref type="bibr" target="#b19">[20]</ref> datasets, comparing it with state-of-the-art models such as SR-GAN <ref type="bibr" target="#b0">[1]</ref>. The experimental results showed that Apollo performed exceptionally well across various compression bitrates and music genres, particularly in complex scenarios involving a mixture of multiple instruments and vocals. Additionally, Apollo's efficiency in streaming audio applications has been validated, demonstrating its potential in real-time, high-quality audio restoration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APOLLO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Pipeline</head><p>Fig. <ref type="figure" target="#fig_0">1</ref>(a) presents the proposed Apollo pipeline. Apollo operates in the time-frequency domain and comprises a band-split module, a band-sequence modeling module, and a band-reconstruction module. Specifically, given compressed or distorted audio S ∈ R 1×L , we first transfer S to its time-frequency domain representation X ∈ C F ×T using the Short-Time Fourier Transform (STFT), where L denotes the length of audio, F and T denote the number of frequency bins and frames, respectively. Then, the band-split module maps to subband embeddings Z ∈ R N ×T using gain-shape representations G ∈ R 3×M ×T for each sub-band, where N and M denote the number of channels in sub-band embeddings and gain-shape representations, respectively. Next, the band-sequence modeling module performs joint modeling of temporal and sub-band using a stacked architecture based on Roformer <ref type="bibr" target="#b17">[18]</ref> and temporal convolutional network (TCN) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Finally, the band-reconstruction module converts the output Q ∈ R N ×T of the band-sequence modeling module into the reconstructed complex-valued spectrogram Y ∈ C F ×T . It uses the inverse Short-Time Fourier Transform (iSTFT) to convert Y to a waveform S ∈ R 1×L .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Band-split Module</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), given compressed or distorted audio spectrogram X, we first split its frequency dimension</p><formula xml:id="formula_0">F into K sub-band spectrograms {X k ∈ C M k ×T |k ∈ [1, K]}.</formula><p>Inspired by the Gull codec <ref type="bibr" target="#b15">[16]</ref>, we extract gain-shape representations G k ∈ R 3×M k ×T for each sub-band spectrogram:</p><formula xml:id="formula_1">G k = Concat Re(X k ) ∥X k ∥2 , Im(X k ) ∥X k ∥2 , log (∥X k ∥2) ,<label>(1)</label></formula><p>where Re(X k ) and Im(X k ) denote the real and imaginary parts, respectively. ∥X k ∥2 represents the ℓ2-norm of X k , given by:</p><formula xml:id="formula_2">∥X k ∥2 = Re(X k ) 2 + Im(X k ) 2<label>(2)</label></formula><p>log (∥X k ∥2) is the logarithm of the ℓ2-norm of X k . Concat refers to the concatenation of components. The gain-shape representation decouples the sub-band spectrogram's content and energy, allowing the reconstruction model to learn appropriate mappings that preserve the audio content. Subsequently, we map the gain-shape representations G into high-dimensional embeddings Z through a bottleneck layer, which consists of RMSNorm <ref type="bibr" target="#b22">[23]</ref> and a 1D convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Band-sequence Modeling Module</head><p>In Apollo, we employ stacked Band-sequence modeling modules (BS modules, Fig. <ref type="figure" target="#fig_1">1(c</ref>)) to perform joint sub-band and temporal modeling with a stacking depth of B. Unlike BSRNN <ref type="bibr" target="#b23">[24]</ref> and Gull <ref type="bibr" target="#b15">[16]</ref>, each BS module consists of a series of residual Roformers <ref type="bibr" target="#b17">[18]</ref> and TCNs, which sequentially scan along the sub-band and time dimensions, and can increase the modeling capacity to improve the model performance. First, the residual Roformer is applied to the input Z along the frequency band dimension K to obtain Z ′ ∈ R N ×T , capturing global dependencies between sub-bands while preserving</p><p>TABLE I THE STRUCTURE OF THE STFT DISCRIMINATOR NETWORK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Index</head><p>Layer Type Input Channels Output Channels Kernel Size Padding Stride Activation </p><formula xml:id="formula_3">1 SpectralNorm + Conv2d F F (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 2 SpectralNorm + Conv2d F F × 2 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 3 SpectralNorm + Conv2d F × 2 F × 4 (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 4 SpectralNorm + Conv2d F × 4 F × 8 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 5 SpectralNorm + Conv2d F × 8 F × 16 (3, 3) (1, 1) (1, 1) LeakyReLU(0.2) 6 SpectralNorm + Conv2d F × 16 F × 32 (3, 3) (1, 1) (2, 2) LeakyReLU(0.2) 7 Conv2d F × 32 1 (3, 3) (1, 1)<label>(1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Band-reconstruction Module</head><p>The output Q is passed through sub-band-specific fully connected (FC) layers to generate the estimated real and imaginary parts of the restored sub-band spectrograms (see Fig. <ref type="figure" target="#fig_1">1(d)</ref>). We utilize RMSNorm as the normalization layer within the fully connected layers and employ Gated Linear Units (GLUs) as the nonlinear activation function. Subsequently, the K reconstructed sub-band spectrograms are concatenated along the frequency dimension to form the final reconstructed complex-valued spectrogram Y. Finally, the reconstructed complex-valued spectrogram Y is converted back to the waveform domain S through the iSTFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Objection</head><p>The proposed Apollo model is trained using a GAN framework to enhance the quality of audio restoration. Specifically, the discriminator network is inspired by the multi-resolution STFT discriminator, similar to the Gull codec <ref type="bibr" target="#b15">[16]</ref>. As described in Table <ref type="table">I</ref>, the discriminator input consists of the spectrogram's real and imaginary parts, which are stacked into a 3D tensor along the channel dimension. To ensure energy invariance in the input, the signal is normalized to have a unit ℓ2-norm before being passed into the discriminator. The discriminator is trained using the Least Squares GAN (LSGAN) loss <ref type="bibr" target="#b24">[25]</ref>, defined as:</p><formula xml:id="formula_4">LD = I i=1 E A∼p data (Di(A) -1) 2 + I i=1 E Y∼p G (Di(Y)) 2 , (3)</formula><p>where A ∈ C F ×T denotes the spectrogram of uncompressed audio and I = 5 denotes the number of discriminator.</p><p>The generator, Apollo, is optimized through a composite loss function, which includes the reconstruction loss, feature matching loss, and the adversarial loss from the discriminator. The reconstruction loss Lrec is based on the mean absolute error (MAE) between the magnitude spectrograms of the restored and target audio, evaluated over multiple STFT resolutions:</p><formula xml:id="formula_5">Lrec = 1 W W w=1 ∥|STFTw(Y)| -|STFTw(A)|∥ 1 ∥|STFTw(T)|∥ 1 ,<label>(4)</label></formula><p>where STFTw denotes the STFT with window size w ∈ [32, <ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. This multi-resolution approach allows the model to capture fine and coarse details, leading to accurate restoration of audio signals across various frequency ranges. The feature matching loss is defined as the layer-wise normalized MAE between the hidden representations of the discriminator for both the reconstructed and target signals. These hidden representations, denoted as Hi,j for the reconstructed signal and Hi,j for the target signal, are obtained from the j-th layer of the i-th discriminator. The feature matching loss is computed as follows:</p><formula xml:id="formula_6">LFM = 1 5 5 i=1 1 6 6 j=1 E Hi,j -sg[Hi,j] mean (|sg[Hi,j]|) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The reconstruction loss Lrec is calculated as the multi-resolution frequency MAE over several STFT window sizes, ensuring that both short-and long-term signal characteristics are restored effectively:</p><formula xml:id="formula_8">Lrec = 1 W W w=1 ∥|STFTw(Y)| -|STFTw(A)|∥ 1 .<label>(6)</label></formula><p>The overall generator loss combines reconstruction, feature matching, and adversarial losses, expressed as:</p><formula xml:id="formula_9">LG = αLrec + βLFM + γLGAN<label>(7)</label></formula><p>where α = 1, β = 1, and γ = 1 are hyperparameters used to balance the contributions of the individual loss components. This comprehensive loss formulation ensures that Apollo reconstructs not only accurate audio signals but also maintains perceptual quality and adversarial robustness by leveraging multi-resolution STFT losses and feature-matching mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT CONFIGURATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We trained and tested Apollo on the combined MUSDB18-HQ <ref type="bibr" target="#b18">[19]</ref> and MoisesDB <ref type="bibr" target="#b19">[20]</ref> datasets. By integrating these two datasets, we leveraged their rich diversity and comprehensive musical resources to evaluate Apollo's restoration performance across different music genres more thoroughly. During the data preprocessing stage, inspired by music separation techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we employed a Source Activity Detector (SAD) to remove silent regions from the tracks, retaining only the significant portions for training. Throughout training, we implemented real-time data augmentation by randomly mixing tracks from different songs. Specifically, we randomly selected between 1 and 8 stems from 11 available tracks and extracted 3-second clips from each selected stem. These clips were then randomly scaled in energy within a range of [-10, 10] dB relative to their original levels. All selected stem clips were summed together to generate simulated music. Subsequently, we simulated dynamic bitrate scenarios by applying MP3 codecs with bitrates of [24000, 32000, 48000, 64000, 96000, 128000] to generate the compressed music. To ensure all samples were on the same scale, we rescaled both the target audio and the encoded audio based on the maximum absolute value. TABLE II DIFFERENT METHODS' SDR/SI-SNR/VISQOL SCORES FOR VARIOUS TYPES OF MUSIC, AS WELL AS THE NUMBER OF MODEL PARAMETERS AND GPU INFERENCE TIME. FOR THE GPU INFERENCE TIME TEST, A MUSIC SIGNAL WITH A SAMPLING RATE OF 44.1 KHZ AND A LENGTH OF 1 SECOND WAS USED. Model Vocal Single Stem Multi-Stems Multi-Stems+Vocal Overall Params (M) RTF (ms) SR-GAN [1] 10.62/9.19/2.72 13.88/12.52/3.28 14.92/14.16/3.41 16.87/15.54/3.76 14.07/12.85/3.29 322.53 34.55 Apollo (Ours) 13.99/12.58/3.44 16.56/15.99/4.08 17.52/17.15/4.41 18.51/18.26/4.54 16.64/16.00/4.12 16.54 53.23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyperparameters</head><p>For the proposed Apollo model, the Short-Time Fourier Transform (STFT) window length was set to 20 ms with a hop size of 10 ms, using a Hanning window. The bandwidth for frequency band segmentation was set to 160 Hz, and the feature dimension N was set to 256. The Band Sequence modeling module was stacked B = 6 times. In the discriminator network, the STFT window sizes were configured with a multi-scale setup, including <ref type="bibr">[32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024,</ref><ref type="bibr">2048]</ref>. For the optimizer, both the generator and discriminator utilize the AdamW optimizer <ref type="bibr" target="#b26">[27]</ref>. The generator's initial learning rate was set to 0.001, with a weight decay of 0.01, while the discriminator's initial learning rate was set to 0.0001, with the same weight decay of 0.01. The learning rate decayed by 0.98 every two epochs, and gradient clipping with a maximum norm of 5 was employed to prevent gradient explosion. Additionally, we implemented an early stopping mechanism to prevent overfitting: training was terminated if the validation loss did not decrease for 20 consecutive epochs. All models were trained on 8 Nvidia RTX 4090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation metrics</head><p>In all experiments, we used the Scale-Invariant Signal-to-Noise Ratio (SI-SNR) <ref type="bibr" target="#b27">[28]</ref>, Signal-to-Distortion Ratio (SDR) <ref type="bibr" target="#b28">[29]</ref>, and Virtual Speech Quality Objective Listener (VISQOL) <ref type="bibr" target="#b29">[30]</ref> to evaluate the quality of the reconstructed audio. To assess the model's efficiency, we reported the time consumption per second of audio processed by Apollo and SR-GAN (Real-Time Factor, RTF). RTF is calculated by processing 1-second audio tracks sampled at 44.1 kHz on both CPU and GPU, and the average value is taken after running 1000 iterations. Additionally, we measured the model size by reporting the number of parameters using the open-source tool PyTorch-OpCounter 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS</head><p>We evaluated the restoration performance of the Stochastic-Restoration-GAN (SR-GAN) <ref type="bibr" target="#b0">[1]</ref> and Apollo models across various bitrates and music genres on the combined test set of MUSDB18-HQ and MoisesDB (with 5000 samples for each case). The test set 1 <ref type="url" target="https://github.com/Lyken17/pytorch-OpCounter">https://github.com/Lyken17/pytorch-OpCounter</ref> encompasses a wide range of music genres, including vocals, single instruments, and mixed instruments, aiming to comprehensively assess each model's restoration capabilities.</p><p>Bitrate Impact Analysis. Fig. <ref type="figure" target="#fig_2">2</ref> compares the performance of the Apollo model and the Stochastic-Restoration-GAN (SR-GAN) at different bitrates (ranging from 24 kHz to 128 kHz). The experimental results demonstrated that Apollo consistently outperformed SR-GAN across all bitrates, particularly in addressing issues such as frequency band voids or reduced signal bandwidth, as reflected by SI-SNR and SDR scores. Additionally, Apollo significantly improved audio restoration quality as measured by VISQOL. Project page<ref type="foot" target="#foot_0">foot_0</ref> for Apollo's reconstructed audio given multiple MP3 bitrates.</p><p>Music Genre Impact Analysis. Table II further illustrates the performance of both models across different music genres. In audio scenarios involving vocals, single instruments, mixed instruments, and a combination of instruments with vocals, Apollo consistently surpasses SR-GAN, with its advantage being especially pronounced in complex scenarios with mixed instruments and vocals. This is attributed to Apollo's alternating band and sequence modeling design, which emphasizes capturing and restoring complex spectral information. Compared to SR-GAN, Apollo delivers higher user ratings (VISQOL) with comparable inference speed while maintaining a more compact model size. This is especially important for realtime communications and live audio restoration, where low latency is critical to the user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We propose Apollo, a novel method specifically designed for compressed audio restoration. Apollo significantly enhances audio quality in the frequency domain through band split, sequence modeling, and reconstruction modules. Empirical evaluations on the integrated MUSDB18-HQ and MoisesDB datasets validate Apollo's outstanding performance. Notably, Apollo achieves substantial improvements in music restoration while maintaining a smaller model size and high computational efficiency. The experimental results demonstrated that when addressing the complex acoustic characteristics of music, bandsplit, and band-sequence modeling more effectively captured and restored audio information lost during compression.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall pipeline of the model architecture of Apollo and its modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>, 1 )</head><label>1</label><figDesc>Nonethe local characteristics of the frequency domain signals. Next, the TCN is applied along the time dimension T on Z ′ to generate the output Q ∈ R N ×T . Since the K sub-band features share the same feature dimension N , they all share a single TCN. The TCN consists of three convolutional blocks, each containing three convolutional layers. This design allows the TCN module to efficiently handle shortterm dependencies and local temporal dynamics in audio signals, enhancing the model's ability to capture and understand temporal domain features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Apollo and SR-GAN's SDR, SI-SNR and ViSQOL result in comparison at different bitrates.</figDesc><graphic coords="4,43.95,43.68,187.24,112.34" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://cslikai.cn/Apollo/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic restoration of heavily compressed musical audio using generative adversarial networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1349</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Voicefixer: Toward general speech restoration with neural vocoder</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13731</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting time-frequency patterns with lstm-rnns for low-bitrate audio restoration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1095" to="1107" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral band replication, a novel approach in audio coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liljeryd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kjorling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kunz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Audio Engineering Society Convention</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<date type="published" when="2002">2002</date>
			<publisher>Audio Engineering Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Speech coding: with code-excited linear prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bäckström</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An audio-visual speech separation model inspired by cortico-thalamo-cortical circuits</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Diffusion models for audio restoration</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Lemercier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Welker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Välimäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.09821</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Solving audio inverse problems with a diffusion model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Moliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Välimäki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06801</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The sound demixing challenge 2023-cinematic demixing track</title>
		<author>
			<persName><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fabbro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Audio bandwidth extension: application of psychoacoustics, signal processing and loudspeaker design</title>
		<author>
			<persName><forename type="first">E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Aarts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audiodec: An open-source streaming high-fidelity neural audio codec</title>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Highfidelity audio compression with improved rvqgan</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gull: A generative multifunctional audio codec</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.04947</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mp3 and aac explained</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brandenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio Engineering Society Conference: 17th International Conference: High-Quality Audio Coding</title>
		<imprint>
			<publisher>Audio Engineering Society</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Musdb18-hq-an uncompressed version of musdb18</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo</idno>
		<ptr target="doi.org/10.5281/zenodo" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3338373</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Moisesdb: A dataset for source separation beyond 4-stems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vogl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.15913</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An efficient encoder-decoder architecture with top-down attention for speech separation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.15200</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Music source separation with band-split rnn</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1893" to="1901" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Subnetwork-to-go: Elastic neural network with dynamic training and customizable inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6775" to="6779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sdr-half-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visqol: an objective speech quality model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
