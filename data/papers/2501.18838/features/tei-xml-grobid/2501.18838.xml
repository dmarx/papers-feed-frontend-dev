<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partially Rewriting a Transformer in Natural Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-31">31 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gonc</forename><surname>Â¸alo Paulo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nora</forename><surname>Belrose</surname></persName>
						</author>
						<title level="a" type="main">Partially Rewriting a Transformer in Natural Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-31">31 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">55A111368D6A793AC5DEC82AA861B3DF</idno>
					<idno type="arXiv">arXiv:2501.18838v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The greatest ambition of mechanistic interpretability is to completely rewrite deep neural networks in a format that is more amenable to human understanding, while preserving their behavior and performance. In this paper, we attempt to partially rewrite a large language model using simple natural language explanations. We first approximate one of the feedforward networks in the LLM with a wider MLP with sparsely activating neurons -a transcoder -and use an automated interpretability pipeline to generate explanations for these neurons. We then replace the first layer of this sparse MLP with an LLM-based simulator, which predicts the activation of each neuron given its explanation and the surrounding context. Finally, we measure the degree to which these modifications distort the model's final output. With our pipeline, the model's increase in loss is statistically similar to entirely replacing the sparse MLP output with the zero vector. We employ the same protocol, this time using a sparse autoencoder, on the residual stream of the same layer and obtain similar results. These results suggest that more detailed explanations are needed to improve performance substantially above the zero ablation baseline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While large language models (LLMs) have reached human level performance in many areas <ref type="bibr" target="#b10">(Guo et al., 2025)</ref>, we understand little about their internal representations. Early mechanistic interpretability research attempted to explain the activation patterns of individual neurons <ref type="bibr" target="#b16">(Olah et al., 2020;</ref><ref type="bibr" target="#b11">Gurnee et al., 2023;</ref><ref type="bibr">2024)</ref>, but research has found that most neurons are "polysemantic", activating in semantically diverse contexts <ref type="bibr" target="#b0">(Arora et al., 2018;</ref><ref type="bibr" target="#b7">Elhage et al., 2022)</ref>.</p><p>Sparse autoencoders (SAEs) were proposed to address polysemanticity <ref type="bibr" target="#b4">(Cunningham et al., 2023)</ref>. SAEs consist of two parts: an encoder that transforms activation vectors into a sparse, higher-dimensional latent space, and a decoder that projects the latents back into the original space. Both parts are trained jointly to minimize reconstruction error. Recently, a significant effort was made to scale SAE training to larger models, like GPT-4 <ref type="bibr" target="#b9">(Gao et al., 2024)</ref> and Claude 3 Sonnet <ref type="bibr" target="#b19">(Templeton et al., 2024)</ref>, and they have become an important interpretability tool for LLMs. <ref type="bibr" target="#b17">Paulo et al. (2024)</ref> took inspiration on <ref type="bibr">Bills et al. (2023)</ref> and built an automated pipeline for generating natural language explanations of SAE features and evaluating how good these explanations are, although rigorously measuring how interpretable an explanation is still a complicated and methodologically fraught task.</p><p>Recently <ref type="bibr" target="#b6">Dunefsky et al. (2024)</ref> proposed sparse transcoders as an alternative method for extracting interpretable features from LLMs. The architecture of the transcoder is identical to that of an SAE, but it is trained to predict the output of a feedforward network given its input. We can then entirely replace the original FFN with its transcoder approximation, thereby partially rewriting the model in terms of more interpretable primitives.</p><p>The idea of rewriting a neural net in a more interpretable form is not new. The "microscope AI" framework <ref type="bibr" target="#b13">(Hubinger, 2019)</ref> aims to analyze a neural network's learned representations to gain actionable insights for humans, rather than using the network directly. These insights would likely take the form of natural language explanations of the network's features and circuits. Microscope AI aims to reduce risks associated with model deployment while still benefiting from the model's knowledge. Imitative generalization is a proposal to extend this idea by jointly optimizing the network and its human-interpretable annotations to maximize their prior likelihood <ref type="bibr">(Barnes, 2021)</ref>.</p><p>In this work, we pursue the following idea: if the latents of a transcoder are interpretable enough, we can simulate their activations using natural language explanations. Specifically, we replace the encoder of the transcoder with an LLM prompted to predict the activation of each latent given its explanation and the textual context. We then patch this modified transcoder back into the model, hopefully yielding behavior nearly identical to the unpatched model. In the limit, we could use this to "rewrite" every feedforward layer in the model in terms of interpretable features and operations on those features.</p><p>With our pipeline, the model's increase in loss is only slightly smaller than when entirely replacing the sparse MLP output with the zero vector. Our results suggest that more detailed explanations are needed to improve performance substantially above the zero ablation baseline. There are many potential ways to improve the quality and specificity of explanations, and we hope this work inspires the community to invest more resources in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>We begin by training a sparse transcoder on the MLP of the sixth layer of Pythia 160M <ref type="bibr" target="#b2">(Biderman et al., 2023)</ref>. Our loss function is the mean squared error between the transcoder's output and the MLP output, with no auxiliary loss terms. Sparsity is continously enforced on the transcoder latents using the TopK activation function proposed by <ref type="bibr" target="#b9">Gao et al. (2024)</ref> with k = 32. We train over the first 8B tokens of Pythia's training corpus, the Pile <ref type="bibr" target="#b8">(Gao et al., 2020)</ref>, using the Adam optimizer <ref type="bibr" target="#b15">(Kingma, 2014)</ref>, a sequence length of 2049, and a batch size of 64 sequences. We have also added a linear "skip connection" to the transcoder, which we find improves its ability to approximate the original MLP at no cost to interpretability scores. That is, the transcoder takes the functional form</p><formula xml:id="formula_0">f (x) = W 2 TopK(W 1 x + b 1 ) + W skip x + b 2 (1)</formula><p>Both W 2 and W skip are zero-initialized, and b 2 is initialized to the empirical mean of the MLP outputs, so that the transcoder is a constant function at the beginning of training.</p><p>We leave a deeper analysis of the skip connection for future work. We also train a sparse autoencoder on the residual stream, with the same training conditions as the transcoder, but without a skip connection.</p><p>We use the automated interpretability pipeline released by <ref type="bibr" target="#b17">Paulo et al. (2024)</ref> to generate explanations and scores for transcoder and SAE latents. Then we modify the pipeline to do "single" token simulation, tasking an LLM to determine if a latent is active on the last token of a sequence, and by how much (Figure <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Quantile normalization</head><p>We found in early experiments that Llama produces highly uncalibrated predictions of feature activations: the marginal distribution of the predicted activations differs markedly from the marginal distribution of the true activations (Figure <ref type="figure" target="#fig_0">1</ref>). Patching these uncalibrated activations into the model yields very poor results. To alleviate this problem, we use quantile normalization, which monotonically transforms the model's predictions in such a way that their marginal distribution matches that of the true activations. This transformation is an optimal transport map under a variety of cost functions <ref type="bibr" target="#b18">(Santambrogio, 2015)</ref>.</p><p>We compute the quantile normalizer separately for each individual feature, using the empirical CDFs of the simulator's predicted activations and of the true activations. Since it is vastly more efficient to compute true activations directly from the transcoder, we compute the CDF for these on a</p><formula xml:id="formula_1">â¦ Figure 2.</formula><p>Partially rewriting an LLM. After training a Transcoder, or any type of SAE, we generate explanations for all the latents using the contexts where that latent is active. An LLM is tasked to summarize or otherwise find patterns in the activations and output a simple, single sentence explanation for that latent. These explanations are used by another instance of an LLM to predict wether the latent should be active in a given token. After some post-processing of those predictions, a reconstruction vector is calculated using the decoder directions of the latents that are considered to be active for that token.</p><p>much larger dataset of 10M tokens, while we are only able to use 10K contexts (with each context contributing a single token) for the predicted activations. Once the quantile normalizer has been computed, this transformation is then applied to all simulator predictions. We find that the quality of this transformation is strongly dependent on the sample size used for the predicted activations, and the loss of the partially rewritten model is sensitive to this distribution, see discussion in Appendix B.</p><p>One problem with estimating the quantile normalizer on an empirical sample is that, while the empirical CDF is an unbiased estimator for the true CDF, the empirical inverse CDF (or quantile function) is biased for the true inverse CDF. This means that a quantile normalizer fit on a modestly sized dataset of predicted and true activations may generalize poorly to unseen inputs. In a future draft of this paper we plan to experiment with bias-corrected estimators for the population quantiles <ref type="bibr" target="#b14">(Hyndman &amp; Fan, 1996)</ref>, which will hopefully improve the sample complexity. On the other hand, since we have three orders of magnitude more datapoints for the true activation quantiles than we do for the predicted activation CDF, the biased quantiles may matter less than the finite sample variance in the predicted activation statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>Simply replacing a single MLP with a transcoder increases the model's cross-entropy loss to that of an early Pythia checkpoint-namely one that was trained on only 25% of the data.<ref type="foot" target="#foot_1">foot_1</ref> Rewriting any part of the transcoder in natural language will necessarily degrade the model's performance even further. Consequently, we focus on rewriting a single MLP block of Pythia 160M, since rewriting all MLP blocks simultaneously would likely cause the model to become completely unusable. The same applies to the residual stream SAE: the performance of the model when adding a single SAE to the residual stream is close to using a checkpoint trained on only 10% of the original data.</p><p>For evaluation, we sample chunks of text from the Pile and gather latents from the transcoder evaluated on the last to- ken of each chunk. For each latent in each text chunk, we prompt Llama 3 Instruct 8B <ref type="bibr" target="#b5">(Dubey et al., 2024)</ref> to output a number from zero to ten indicating how strongly it predicts the latent should activate given its natural language explanation and the textual context. We record the probability that Llama assigns to each of the ten numbers, and compute the expected value. This expected prediction is then quantile normalized (Section 2.1) to produce the predicted activation for this latent.</p><p>This yields a vector of predicted activation values for each latent, and we apply the TopK activation function to this vector to ensure it has the expected level of sparsity. We evaluate over 10K different prompts for the transcoder and 1K different prompts for the SAE, measuring the crossentropy loss for next-token prediction.</p><p>Partial rewriting. We also experiment with mixing predicted and ground truth latent activations in varying proportions, allowing us to examine the effect of rewriting only part of the encoder. We do this in two different ways:</p><p>1. Select the top k most interpretable features according to our evaluation pipeline <ref type="bibr" target="#b17">(Paulo et al., 2024)</ref>. This is labeled "Top scoring" in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>2. Sample k features uniformly at random from the transcoder. This is labeled "Sampling" in Figure <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Figure <ref type="figure" target="#fig_1">3</ref> illustrates how cross-entropy loss increases as we replace more and more transcoder latents with their simulated counterparts. When we replace all latents with simulated counterparts, the cross-entropy loss is the same as that of a Pythia checkpoint trained on only 10-15% of the full training corpus. This result is similar to that of setting the output of the MLP to the zero vector. This means that a predictor that ignores the explanations and always predicts that every latent is inactive would achieve only slightly more loss than this setup, for any fraction of re-written transcoder and SAE. Randomly selecting which latents to substitute, instead of always substituting the best scoring latents, leads to a bigger performance hit than zeroing out the MLP. Not calibrating the predictions using quantile normalization leads to an even worse performance, equivalent to barely training the model at all (Figure <ref type="figure">A5</ref>).</p><p>Using the empirical distribution as the target distribution for quantile normalization significantly improves this performance and rewritten models perform better than zeroing them, see Figure . Simulating activations over 10K prompts requires individually prompting a model for 32768 latents, for a total of 327 million predictions, an expensive endeavor. We expect that, if performed on 100 thousand, the normalized distribution would match the empirical distribution, see discussion in Appendix B, and that the performance overwriting the model would be better even when using a larger calibration distribution and not the real distribution, but due to the computational costs we cannot claim that for sure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Explanations are not detailed enough</head><p>The poor performance of the model when using the uncalibrated predicted activations is mainly due to the low specificity of explanations. To see this, let's consider the case that the classifier only achieves a specificity of 99%.</p><p>The transcoder used in this work has 32768 latents, and if the LLM predictor only can only achieve a specificity of 99% that means, that on average, it predicts there are 320 active latents, which is 10 times more than the actual number (k = 32). Even with a specificity of 100%, where the model only predicts 32 latents to be active, it is unlikely that the top 32 predictions would be the correct ones. We observe than on average the current automatic latent explanation setup has a specificity of around 80%, a number much lower than what would be required to do this task.</p><p>By performing quantile normalization, a large chunk of incorrectly predicted activations, activations that should be zero but were given a non-zero value by the predictor, are set back to zero. This significantly increases the specificity, enough that some of the original model's performance is maintained, but at the same time this significantly decreases the sensitivity, as some of the correctly classified active latents are also set to zero. This makes it clear that the current pipeline is lacking, as it is not specific enough when its simulated activations are uncalibrated and is not sensitive enough when they are.</p><p>We find that detection scores <ref type="bibr">(Paulo et al., 2024, page 5</ref>) are predictive of the specificity and sensitivity of an explanation, with higher scoring latents corresponding to explanations that have higher specificity and sensitivity (Figure <ref type="figure" target="#fig_2">4</ref>). This is expected, as detection scoring corresponds to detecting whether a given latent is active on a given context, which is similar to our simulation task in this work. For the same reason, latents with higher fuzzing scores also have higher sensitivity and specificity (Figure <ref type="figure" target="#fig_5">A4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed a new methodology for rigorously evaluating the faithfulness of natural language explanations of sparse autoencoder transcoder latents, based on partially rewriting the base model using these explanations. We found that existing explanations are severely wanting: without quantile normalization they are insufficiently precise to enable even 20% of latents to be simulated while preserving the base model's performance. While normalization improves performance significantly, we are still unable to outperform the zero ablation baseline, where the entire MLP is replaced with the zero vector. This is mainly due to the fact that explanations are not specific enough, leading to a high number of false positives.</p><p>Our results highlight the fact that it is important for an explanation to correctly identify the contexts where a feature is not active, in addition to the feature's activation level in contexts where it is active. Future work on the interpretability of latents should take this into consideration.</p><p>To improve upon these results, new techniques are needed to make explanations more specific, for instance using contrast pairs of highly similar features to bring out additional details. This could potentially increase the sensitivity as well, which takes a big hit when using quantile normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Contributions and Acknowledgments</head><p>Nora and Gonc Â¸alo had the idea for the simulation experiments. Gonc Â¸alo executed the experiments and wrote the first draft of the article. Nora trained the transcoders, suggested the quantile normalization experiments, provided feedback and did significant revisions.</p><p>Partially Rewriting a Transformer in Natural Language</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulation prompt</head><p>You are an intelligent and meticulous linguistics researcher.</p><p>You will be given a certain explanation of a feature of text, such as "male pronouns" or "text with negative sentiment" and examples of text that contains this feature. Some explanations will be given a score from 0 to 1. The higher the score the better the explanation is, and you should be more certain of your response (positive or negative).</p><p>These features of text are normally identified by looking for specific words or patterns in the text.</p><p>There are many features associated with a single token, and sometimes the feature is related with the previous token or context.</p><p>Your job is to identify how much the last token, which is marked between &lt;&lt; and &gt;&gt;, represents the feature. You will output a integer between 0 and 9, where 0 corresponds to no relation to the explanation and 9 to a strong relation.</p><p>Most of the tokens should have no relation. The ones that are related, should more likely be given 1 than 2, 2 than 3, and so on. Only give a 9 if the description exactly matches the token.</p><p>You must return your response in a valid Python list. Do not return anything else besides a Python list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sample size</head><p>Activations of latents are very infrequent, and the empirical distribution of their activations on a small number of tokens can have a small number of non-zero values. If we use only 1K samples instead of 10K samples, the mismatch between the normalized and the empirical distribution 1 grows larger (Figure <ref type="figure" target="#fig_3">A1</ref>). We then expect that a larger number of samples would lead to a better convergence.</p><p>We observe that the fact that the normalized activation distribution and the empirical activation distribution not matching significantly worsens the performance of substituting the predicted activations as the performance over the 1K samples, using the distribution in Figure <ref type="figure" target="#fig_3">A1</ref>, is much worse ( Figure <ref type="figure" target="#fig_4">A2</ref>).</p><p>Using the empirical activation distribution over the prompts improves the result both in the 1K samples case and in the 10K samples case (Figure <ref type="figure" target="#fig_1">A3</ref>). We expect that, were we able to have done prediction over 100k samples, the results of substituting using the empirical distribution over the prompts or over the larger 10M token sample would be closer, and if the trend holds, substituting the predictions would be better than zeroing out the MLP.  . Using the empirical distribution over the prompt improves results. If instead of using the empirical distribution of activations computed over the larger 10M token sample, we use the empirical distribution of activations that we are predicting, the results of substituting the predictions improve significantly. We expect that, as the number of samples increases, these should converge, and that substituting predictions will perform better than zeroing out the component. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Distribution of predicted activations for all latents. On the left we compare the distribution of predicted activations before normalization, and on the right we show what the distribution looks like after quantile normalization. Before normalization, the predictor model systematically over-predicts high activation values by multiple orders of magnitude. Quantile normalization primarily has the effect of enforcing a prior in favor of features not being active.</figDesc><graphic coords="2,55.44,67.06,486.00,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Cross entropy loss increase for different fractions of transcoder and SAE substitution. We compute the CE loss over 10K prompts, for the transcoder (left) and SAE (right) respectively, by substituting parts of the encoder with natural language explanations. Bars in green show the average loss increase when choosing the top scoring latents for replacement. Bars in orange show the average loss increase when randomly selecting a subset of latents to replace. Bars in blue show the average loss increase caused by zeroing out a part of the transcoder. Bar heights represent the median value of the absolute difference, because the distribution is heavy-tailed, and error bars are 95% confidence intervals computed using bootstrapping. The interpretability score used for the selecting latents is detection scoring,(Paulo et al., 2024, page 5), computed over 100 positive and 100 negative samples. Over this set of prompts, Pythia had a cross entropy loss of 3.19 Â± 0.09 nats per token.</figDesc><graphic coords="4,55.44,67.06,486.01,203.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Detection score predicts sensitivity and specificity. Binning explanations by their scores makes it evident that high-scoring explanations are more specific and sensitive.</figDesc><graphic coords="5,55.44,67.06,486.00,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A1 .</head><label>A1</label><figDesc>Figure A1. Distribution of predicted activations for all latents over a smaller sample size If we use only 1K prompts as the predicted activations and use the 10M prompts as the target distribution, the mismatch with the empirical activation distribution is higher.</figDesc><graphic coords="9,55.44,67.06,486.00,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A2 .</head><label>A2</label><figDesc>Figure A2. Cross entropy loss increase for different fractions of transcode depends on the sample size. Using only 1K samples to compute the quantile normalization function leads to a much worse CE loss when performing substitution, due to the larger difference between the empirical and the normalized distribution (Appendix B)</figDesc><graphic coords="9,55.44,388.48,234.00,175.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A4 .</head><label>A4</label><figDesc>Figure A4. Fuzzing score predicts sensitivity and specificity Explanations with higher fuzzing scores lead to better predictions of the simulations</figDesc><graphic coords="10,55.44,442.66,486.00,194.40" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>EleutherAI. Correspondence to:Gonc Â¸alo Paulo &lt;gonc Â¸alo@eleuther.ai&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>The cross-entropy loss of Pythia 160M checkpoints on this set of prompts is not monotonic with training time, so a more precise estimate is not possible.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We thank <rs type="person">Alice Rigg</rs> for discussions and comments. Gonc Â¸alo and Nora are funded by a grant from <rs type="person">Open Philanthropy</rs>. We thank <rs type="institution">Coreweave</rs> for computing resources.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Code Availability</head><p>Code for these experiments is available the nl simulations branch of the sae-auto-interp GitHub repo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Mechanistic Interpretability. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imitative generalisation (aka &apos;learning the prior&apos;), 2021</title>
		<author>
			<persName><forename type="first">B</forename><surname>Barnes</surname></persName>
		</author>
		<ptr target="https://www.lesswrong.com/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1" />
		<imprint>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pythia: A suite for analyzing large language models across training and scaling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">G</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2397" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models can explain neurons in language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mossing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<ptr target="https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html" />
		<imprint/>
	</monogr>
	<note>Date accessed: 14.05. 2023), 2, 2023</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sharkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08600</idno>
		<title level="m">Sparse autoencoders find highly interpretable features in language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Dunefsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chlenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.11944</idno>
		<title level="m">Transcoders find interpretable llm feature circuits</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10652</idno>
		<title level="m">Toy models of superposition</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>La Tour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Troll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04093</idno>
		<title level="m">Scaling and evaluating sparse autoencoders</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Finding neurons in a haystack: Case studies with sparse probing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Troitskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.01610</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gurnee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Kheirkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hathaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12181</idno>
		<title level="m">Universal neurons in gpt2 language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">i9dQQK3gETCyqh2/ chris-olah-s-views-on-agi-safety</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hubinger</surname></persName>
		</author>
		<ptr target="https://www.lesswrong.com/posts/X2" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2025" to="2026" />
		</imprint>
	</monogr>
	<note>Chris olah&apos;s views on agi safety</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sample quantiles in statistical packages</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="365" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Zoom in: An introduction to circuits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00024.001</idno>
		<ptr target="http://dx.doi.org/10.23915/distill.00024.001" />
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<idno type="ISSN">2476-0757</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020-03">March 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatically interpreting millions of features in large language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paulo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Belrose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13928</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimal transport for applied mathematicians</title>
		<author>
			<persName><forename type="first">F</forename><surname>Santambrogio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">94</biblScope>
			<pubPlace>BirkÃ¤user, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Templeton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bricken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ameisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdougall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Macdiarmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sumers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jermyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<ptr target="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" />
		<title level="m">Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
