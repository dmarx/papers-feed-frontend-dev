<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fastfood: Approximate Kernel Expansions in Loglinear Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2018-10-01">October 1, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Viet</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName><surname>Le</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pky</addrLine>
									<postCode>94043 CA</postCode>
									<settlement>Mountain View</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Google Strategic Technologies</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pky</addrLine>
									<postCode>94043 CA</postCode>
									<settlement>Mountain View</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213 PA</postCode>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Google Strategic Technologies</orgName>
								<address>
									<addrLine>1600 Amphitheatre Pky</addrLine>
									<postCode>94043 CA</postCode>
									<settlement>Mountain View</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fastfood: Approximate Kernel Expansions in Loglinear Time</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-10-01">October 1, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">78BFF90857664AC234C067AD9E74747D</idno>
					<idno type="arXiv">arXiv:1408.3060v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that storing and computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks proposed by <ref type="bibr" target="#b36">Rahimi and Recht (2009)</ref> and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy.</p><p>Our method applies to any translation invariant and any dot-product kernel, such as the popular RBF kernels and polynomial kernels. We prove that the approximation is unbiased and has low variance. Experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Kernel methods have proven to be a highly successful technique for solving many problems in machine learning, ranging from classification and regression to sequence annotation and feature extraction <ref type="bibr" target="#b5">(Boser et al., 1992;</ref><ref type="bibr" target="#b8">Cortes and Vapnik, 1995;</ref><ref type="bibr" target="#b51">Vapnik et al., 1997;</ref><ref type="bibr" target="#b47">Taskar et al., 2004;</ref><ref type="bibr" target="#b39">Schölkopf et al., 1998)</ref>. At their heart lies the idea that inner products in high-dimensional feature spaces can be computed in implicit form via kernel function k: k(x, x ) = φ(x), φ(x ) .</p><p>(1)</p><p>Here φ : X → F is a feature map transporting elements of the observation space X into a possibly infinite-dimensional feature space F. This idea was first used by <ref type="bibr" target="#b1">Aizerman et al. (1964)</ref> to show nonlinear separation. There exists a rich body of literature on Reproducing Kernel Hilbert Spaces (RKHS) <ref type="bibr" target="#b2">(Aronszajn, 1944;</ref><ref type="bibr" target="#b52">Wahba, 1990;</ref><ref type="bibr" target="#b33">Micchelli, 1986)</ref> and one may show that estimators using norms in feature space as penalty are equivalent to estimators using smoothness in an RKHS <ref type="bibr" target="#b16">(Girosi, 1998;</ref><ref type="bibr">Smola et al., 1998a)</ref>. Furthermore, one may provide a Bayesian interpretation via Gaussian Processes. See e.g. <ref type="bibr" target="#b53">(Williams, 1998;</ref><ref type="bibr" target="#b34">Neal, 1994;</ref><ref type="bibr" target="#b30">MacKay, 2003)</ref> for details. More concretely, to evaluate the decision function f (x) on an example x, one typically employs the kernel trick as follows</p><formula xml:id="formula_0">f (x) = w, φ(x) = N i=1 α i φ(x i ), φ(x) = N i=1 α i k(x i , x)</formula><p>This has been viewed as a strength of kernel methods, especially in the days that datasets consisted of ten thousands of examples. This is because the Representer Theorem of <ref type="bibr" target="#b24">Kimeldorf and Wahba (1970)</ref> states that such a function expansion in terms of finitely many coefficients must exist under fairly benign conditions even whenever the space is infinite dimensional. Hence we can effectively perform optimization in infinite dimensional spaces. This trick that was also exploited by <ref type="bibr" target="#b39">Schölkopf et al. (1998)</ref> for evaluating PCA. Frequently the coefficient space is referred to as dual space. This arises from the fact that the coefficients are obtained by solving a dual optimization problem.</p><p>Unfortunately, on large amounts of data, this expansion becomes a significant liability for computational efficiency. For instance, <ref type="bibr" target="#b46">Steinwart and Christmann (2008)</ref> show that the number of nonzero α i (i.e., N , also known as the number of "support vectors") in many estimation problems can grow linearly in the size of the training set. As a consequence, as the dataset grows, the expense of evaluating f also grows. This property makes kernel methods expensive in many large scale problems: there the sample size m may well exceed billions of instances. The large scale solvers of <ref type="bibr" target="#b13">Fan et al. (2008)</ref> and <ref type="bibr" target="#b31">Matsushima et al. (2012)</ref> work in primal space to sidestep these problems, albeit at the cost of limiting themselves to linear kernels, a significantly less powerful function class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Numerous methods have been proposed to mitigate this issue. To compare computational cost of these methods we make the following assumptions:</p><p>• We have m observations and access to an O(m β ) with β ≥ 1 algorithm for solving the optimization problem at hand. In other words, the algorithm is linear or worse. This is a reasonable assumption -almost all data analysis algorithm need to inspect the data at least once to draw inference.</p><p>• Data has d dimensions. For simplicity we assume that it is dense with density rate ρ, i.e. on average O(ρd) coordinates are nonzero.</p><p>• The number of nontrivial basis functions is O(γm). This is well motivated by <ref type="bibr" target="#b46">Steinwart and Christmann (2008)</ref> and it also follows from the fact that e.g. in regularized risk minimization the subgradient of the loss function determines the value of the associated dual variable.</p><p>• We denote the number of (nonlinear) basis functions by n.</p><p>Reduced Set Expansions <ref type="bibr" target="#b7">Burges (1996)</ref> focused on compressing function expansions after the problem was solved by means of reduced-set expansions. That is, one first solves the full optimization problem at O(m β+1 ρd) cost and subsequently one minimizes the discrepancy between the full expansion and an expansion on a subset of basis functions. The exponent of m β+1 arises from the fact that we need to compute O(m) kernels O(m β ) times. Evaluation of the reduced function set costs at least O(nρd) operations per instance and O(nρd) storage, since each kernel function k(x i , •) requires storage of x i .</p><p>Low Rank Expansions Subsequent work by <ref type="bibr" target="#b42">Smola and Schölkopf (2000)</ref>; <ref type="bibr" target="#b14">Fine and Scheinberg (2001)</ref> and <ref type="bibr" target="#b54">Williams and Seeger (2001)</ref> aimed to reduce memory footprint and complexity by finding subspaces to expand functions. The key difference is that these algorithms reduce the function space before seeing labels. While this is suboptimal, experimental evidence shows that for well designed kernels the basis functions extracted in this fashion are essentially as good as reduced set expansions. This is to be expected. After all, the kernel encodes our prior belief in which function space is most likely to capture the relevant dependencies between covariates and labels. These projection-based algorithms generate an n-dimensional subspace:</p><p>• Compute the kernel matrix K nn on an n-dimensional subspace at O(n 2 ρd) cost.</p><p>• The matrix K nn is inverted at O(n 3 ) cost.</p><p>• For all observations one computes an explicit feature map by projecting data in RKHS onto the set of n basis vectors via φ</p><formula xml:id="formula_1">(x) = K n n -1 2 [k(x 1 , x), . . . , k(x n , x)]. That is, training proceeds at O(nρm β + n 2 m) cost.</formula><p>• Prediction costs O(nρd) computation and O(nρd) memory, as in reduced set methods, albeit with a different set of basis functions.</p><p>Note that these methods temporarily require O(n 2 ) storage during training, since we need to be able to multiply with the inverse covariance matrix efficiently. This allows for solutions to problems where m is in the order of millions and n is in the order of thousands: for n = 10 4 we need approximately 1GB of memory to store and invert the covariance matrix. Preprocessing can be parallelized efficiently. Obtaining a minimal set of observations to project on is even more difficult and only the recent work of <ref type="bibr" target="#b9">Das and Kempe (2011)</ref> provides usable performance guarantees for it.</p><p>Multipole Methods Fast multipole expansions <ref type="bibr" target="#b29">(Lee and Gray, 2009;</ref><ref type="bibr" target="#b19">Gray and Moore, 2003)</ref> offer one avenue for efficient function expansions whenever the dimensionality of the underlying space is relatively modest. However, for high dimensions they become computationally intractable in terms of space partitioning, due to the curse of dimensionality. Moreover, they are typically tuned for localized basis functions, specifically the Gaussian RBF kernel.</p><p>Random Subset Kernels A promising alternative to approximating an existing kernel function is to design new ones that are immediately compatible with scalable data analysis. A recent instance of such work is the algorithm of <ref type="bibr" target="#b11">Davies and Ghahramani (2014)</ref> who map observations x into set membership indicators s i (x), where i denotes the random partitioning chosen at iterate i and s ∈ N indicates the particular set.</p><p>While the paper suggests that the algorithm is scalable to large amounts of data, it suffers from essentially the same problem as other feature generation methods insofar as it needs to evaluate set membership for each of the partitions for all data, hence we have an O(knm) computational cost for n partitions into k sets on m observations. Even this estimate is slightly optimistic since we assume that computing the partitions is independent of the dimensionality of the data. In summary, while the function class is potentially promising, its computational cost considerably exceeds that of the other algorithms discussed below, hence we do not investigate it further.</p><p>Random Kitchen Sinks A promising alternative was proposed by <ref type="bibr" target="#b36">Rahimi and Recht (2009)</ref> under the moniker of Random Kitchen Sinks. In contrast to previous work the authors attempt to obtain an explicit function space expansion directly. This works for translation invariant kernel functions by performing the following operations:</p><p>• Generate a (Gaussian) random matrix M of size n × d.</p><p>• For each observation x compute M x and apply a nonlinearity ψ to each coordinate separately, i.e.</p><formula xml:id="formula_2">φ i (x) = ψ([M x] i ).</formula><p>The approach requires O(n × d) storage both at training and test time. Training costs O(m β nρd) operations and prediction on a new observation costs O(nρd). This is potentially much cheaper than reduced set kernel expansions. The experiments in <ref type="bibr" target="#b36">(Rahimi and Recht, 2009)</ref> showed that performance was very competitive with conventional RBF kernel approaches while providing dramatically simplified code. Note that explicit spectral finite-rank expansions offer potentially much faster rates of convergence, since the spectrum decays as fast as the eigenvalues of the associated regularization operator <ref type="bibr" target="#b55">(Williamson et al., 2001)</ref>. Nonetheless Random Kitchen Sinks are a very attractive alternative due to their simple construction and the flexility in synthesizing kernels with predefined smoothness properties.</p><p>Fastfood Our approach hews closely to random kitchen sinks. However, it succeeds at overcoming their key obstacle -the need to store and to multiply by a random matrix. This way, fastfood, accelerates Random Kitchen Sinks from O(nd) to O(n log d) time while only requiring O(n) rather than O(nd) storage. The speedup is most significant for large input dimensions, a common case in many large-scale applications. For instance, a tiny 32x32x3 image in the CIFAR-10 ( <ref type="bibr" target="#b27">Krizhevsky, 2009)</ref> already has 3072 dimensions, and non-linear function classes have shown to work well for MNIST <ref type="bibr" target="#b40">(Schölkopf and Smola, 2002)</ref> and CIFAR-10. Our approach relies on the fact that Hadamard matrices, when combined with Gaussian scaling matrices, behave very much like Gaussian random matrices. That means these two matrices can be used in place of Gaussian matrices in Random Kitchen Sinks and thereby speeding up the computation for a large range of kernel functions. The computational gain is achieved because unlike Gaussian random matrices, Hadamard matrices admit FFT-like multiplication and require no storage.</p><p>We prove that the Fastfood approximation is unbiased, has low variance, and concentrates almost at the same rate as Random Kitchen Sinks. Moreover, we extend the range of applications from radial basis functions k( x -x ) to any kernel that can be written as dot product k( x, x ). Extensive experiments with a wide range of datasets show that Fastfood achieves similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster with 1000x less memory. These improvements, especially in terms of memory usage, make it possible to use kernel methods even for embedded applications.</p><p>Our experiments also demonstrate that Fastfood, thanks to its speedup in training, achieves state-of-the-art accuracy on the CIFAR-10 dataset <ref type="bibr" target="#b27">(Krizhevsky, 2009)</ref> among permutation-invariant methods. Table <ref type="table">1</ref> summarizes the computational cost of the above algorithms.</p><p>Having an explicit function expansion is extremely beneficial from an optimization point of view. Recent advances in both online <ref type="bibr" target="#b38">(Ratliff et al., 2007)</ref> and batch <ref type="bibr" target="#b48">(Teo et al., 2010;</ref><ref type="bibr" target="#b6">Boyd et al., 2010)</ref> subgradient algorithms summarily rely on the ability to compute gradients in the feature space F explicitly. Algorithm CPU Training RAM Training CPU Test RAM Test Reduced set O(m β+1 ρd + mnρd) O(γmρd) O(nρd) O(nρd) Low rank O(m β nρd + mn 2 ) O(n 2 + nρd) O(nρd) O(nρd) Random Kitchen Sinks O(m β nρd) O(nd) O(nρd) O(nd) Fastfood O(m β n log d) O(n) O(n log d) O(n) Table 1: Computational cost for reduced rank expansions. Efficient algorithms achieve β = 1 and typical sparsity coefficients are ρ = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Kernels and Regularization</head><p>For concreteness and to allow for functional-analytic tools we need to introduce some machinery from regularization theory and functional analysis. The derivation is kept brief but we aim to be self-contained. A detailed overview can be found e.g. in the books of <ref type="bibr" target="#b40">Schölkopf and Smola (2002)</ref> and <ref type="bibr" target="#b52">Wahba (1990)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regularization Theory Basics</head><p>When solving a regularized risk minimization problem one needs to choose a penalty on the functions employed. This can be achieved e.g. via a simple norm penalty on the coefficients</p><formula xml:id="formula_3">f (x) = w, φ(x) with penalty Ω[w] = 1 2 w 2 2 .<label>(2)</label></formula><p>Alternatively we could impose a smoothness requirement which emphasizes simple functions over more complex ones via</p><formula xml:id="formula_4">Ω[w] = 1 2 f 2 H such as Ω[w] = 1 2 f 2 L 2 + ∇f 2 L 2</formula><p>One may show that the choice of feature map φ(x) and RKHS norm • H are connected. This is formalized in the reproducing property</p><formula xml:id="formula_5">f (x) = w, φ(x) F = f, k(x, •) H .<label>(3)</label></formula><p>In other words, inner products in feature space F can be viewed as inner products in the RKHS. An immediate consequence of the above is that k(x, x ) = k(x, •), k(x , •) H . It also means that whenever norms can be written via regularization operator P , we may find k as the Greens function of the operator. That is, whenever</p><formula xml:id="formula_6">f 2 H = P f 2 L 2 we have f (x) = f, k(x, •) H = f, P † P k(x, •) = f, δ x .<label>(4)</label></formula><p>That is, P † P k(x, •) as like a delta distribution on f ∈ H. This allows us to identify P † P from k and vice versa <ref type="bibr">(Smola et al., 1998a;</ref><ref type="bibr" target="#b16">Girosi, 1998;</ref><ref type="bibr" target="#b18">Girosi et al., 1995;</ref><ref type="bibr" target="#b17">Girosi and Anzellotti, 1993;</ref><ref type="bibr" target="#b52">Wahba, 1990)</ref>. Note, though, that this need not uniquely identify P , a property that we will be taking advantage of when expressing a given kernel in terms of global and local basis functions. For instance, any isometry U with U U = 1 generates an equivalent P = U P . In other words, there need not be a unique feature space representation that generates a given kernel (that said, all such representations are equivalent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mercer's Theorem and Feature Spaces</head><p>A key tool is the theorem of <ref type="bibr" target="#b32">Mercer (1909)</ref> which guarantees that kernels can be expressed as an inner product in some Hilbert space.</p><p>Theorem 1 (Mercer) Any kernel k : X × X → R satisfying Mercer's condition</p><formula xml:id="formula_7">k(x, x )f (x)f (x )dxdx ≥ 0 for all f ∈ L 2 (X ) (5) can be expanded into k(x, x ) = j λ j φ j (x)φ j (x ) with λ j ≥ 0 and φ i , φ j = δ ij .<label>(6)</label></formula><p>The key idea of <ref type="bibr" target="#b35">Rahimi and Recht (2008)</ref> is to use sampling to approximate the sum in (6). Note that for trace-class kernels, i.e. for kernels with finite j λ j we can normalize the sum to mimic a probability distribution, i.e. we have</p><formula xml:id="formula_8">k(x, x ) = λ 1 E λ φ λ (x)φ λ (x ) where p(λ) = λ -1 1 λ if λ ∈ {. . . λ j . . .} 0 otherwise<label>(7)</label></formula><p>Consequently the following approximation converges for n → ∞ to the true kernel</p><formula xml:id="formula_9">λ i ∼ p(λ) and k(x, x ) ≈ λ 1 n n i=1 φ λ i (x)φ λ i (x )<label>(8)</label></formula><p>Note that the basic connection between random basis functions was well established, e.g., by <ref type="bibr" target="#b34">Neal (1994)</ref> in proving that the Gaussian Process is a limit of an infinite number of basis functions. A related strategy can be found in the so-called 'empirical' kernel map <ref type="bibr" target="#b50">(Tsuda et al., 2002;</ref><ref type="bibr" target="#b40">Schölkopf and Smola, 2002)</ref> where kernels are computed via</p><formula xml:id="formula_10">k(x, x ) = 1 n n i=1 κ(x i , x)κ(x i , x )<label>(9)</label></formula><p>for x i often drawn from the same distribution as the training data. An explicit expression for this map is given e.g. in <ref type="bibr">(Smola et al., 1998b)</ref>. The expansion (8) is possible whenever the following conditions hold:</p><p>1. An inner product expansion of the form (6) is known for a given kernel k.</p><p>2. The basis functions φ j are sufficiently inexpensive to compute.</p><p>3. The norm λ 1 exists, i.e., k corresponds to a trace class operator <ref type="bibr" target="#b26">Kreyszig (1989)</ref>.</p><p>Although condition 2 is typically difficult to achieve, there exist special classes of expansions that are computationally attractive. Specifically, whenever the kernels are invariant under the action of a symmetry group, we can use the eigenfunctions of its representation to diagonalize the kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Kernels via Symmetry Groups</head><p>Of particular interest in our case are kernels with some form of group invariance since in these cases it is fairly straightforward to identify the basis functions φ i (x). The reason is that whenever k(x, x ) is invariant under a symmetry group transformation of its arguments, it means that we can find a matching eigensystem efficiently, simply by appealing to the functions that decompose according to the irreducible representation of the group.</p><p>Theorem 2 Assume that a kernel k : X 2 → R is invariant under the action of a symmetry group G, i.e. assume that k(x, x ) = k(g • x, g • x ) holds for all g ∈ G. In this case, the eigenfunctions φ i of k can be decomposed according to the irreducible representations of G on k(x, •). The eigenvalues within each such representation are identical.</p><p>For details see e.g. <ref type="bibr" target="#b3">Berg et al. (1984)</ref>. This means that knowledge of a group invariance dramatically simplifies the task of finding an eigensystem that satisfies the Mercer decomposition. Moreover, by construction unitary representations are orthonormal.</p><p>Fourier Basis To make matters more concrete, consider translation invariant kernels</p><formula xml:id="formula_11">k(x, x ) = k(x -x , 0). (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>The matching symmetry group is translation group with the Fourier basis admitting a unitary irreducible representation. Corresponding kernels can be expanded</p><formula xml:id="formula_13">k(x, x ) = z dz exp (i z, x ) exp -i z, x λ(z) = z dz exp i z, x -x λ(z). (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>This expansion is particularly simple since the translation group is Abelian. By construction the function λ(z) is obtained by applying the Fourier transform to k(x, 0) -in this case the above expansion is simply the inverse Fourier transform. We have</p><formula xml:id="formula_15">λ(z) = (2π) -d dx exp (-i x, z ) k(x, 0). (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>This is a well studied problem and for many kernels we may obtain explicit Fourier expansions. For instance, for Gaussians it is a Gaussian with the inverse covariance structure. For the Laplace kernel it yields the damped harmonic oscillator spectrum. That is, good choices of λ are</p><formula xml:id="formula_17">λ(z) = (2π) -d 2 σ d e -1 2σ 2 z 2 2 (Gaussian RBF Kernel) (13) λ(z) = l j=1 1 U d (z) (Matern Kernel) (14)</formula><p>Here the first follows from the fact that Fourier transforms of Gaussians are Gaussians and the second equality follows from the fact that the Fourier spectrum of Bessel functions can be expressed as multiple convolution of the unit sphere. For instance, this includes the Bernstein polynomials as special case for one-dimensional problems. For a detailed discussion of spectral properties for a broad range of kernels see e.g. <ref type="bibr" target="#b41">(Smola, 1998)</ref>.</p><p>Spherical Harmonics Kernels that are rotation invariant can be written as an expansion of spherical harmonics. <ref type="bibr">(Smola et al., 2001, Theorem 5)</ref> shows that dot-product kernels of the form k(x, x ) = κ( x, x ) can be expanded in terms of spherical harmonics. This provides necessary and sufficient conditions for certain families of kernels. Since <ref type="bibr" target="#b45">Smola et al. (2001)</ref> derive an incomplete characterization involving an unspecified radial contribution we give a detailed derivation below.</p><p>Theorem 3 For a kernel k(x, x ) = κ( x, x ) with x, x ∈ R d and with analytic κ, the eigenfunction expansion can be written as</p><formula xml:id="formula_18">k(x, x ) = n Ω d-1 N (d, n) λ n x n x n j Y d n,j x x Y d n,j x x (15) = n λ n x n x n L n,d x, x x x (16) = n N (d, n) Ω d-1 λ n x n x n S d L n,d x -1 x, z L n,d x -1 x , z dz<label>(17)</label></formula><p>Here Y d n,j are orthogonal polynomials of degree n on the d-dimensional sphere. Moreover, N (d, n) denotes the number of linearly independent homogeneous polynomials of degree n in d dimensions, and Ω d-1 denotes the volume of the d-1 dimensional unit ball. L n,d denotes the Legendre polynomial of degree n in d dimensions. Finally, λ n denotes the expansion coefficients of κ in terms of L n,d .</p><p>Proof Equality between the two expansions follows from the addition theorem of spherical harmonics of order n in d dimensions. Hence, we only need to show that for</p><formula xml:id="formula_19">x = x = 1 the expansion κ( x, x ) = n λ n L n,d ( x, x ) holds.</formula><p>First, observe, that such an expansion is always possible since the Legendre polynomials are orthonormal with respect to the measure induced by the d -1 dimensional unit sphere, i.e. with respect to (1-t 2 ) d-3</p><p>2 . See e.g. <ref type="bibr">(Hochstadt, 1961, Chapter 3)</ref> for details. Hence they form a complete basis for one-dimensional expansions of κ(ξ) in terms of L n,d (ξ). Since κ is analytic, we can extend the homogeneous polynomials radially by expanding according to (16). This proves the correctness.</p><p>To show that this expansion provides necessary and sufficient conditions for positive semidefiniteness, note that Y d l,n are orthogonal polynomials. Hence, if we had λ n &lt; 0 we could use any matching Y d l,n to falsify the conditions of Mercer's theorem. Finally, the last equality follows from the fact that</p><formula xml:id="formula_20">S d-1 Y d l,n Y d l ,n = δ l,l , i.e. the functions Y d l,n</formula><p>are orthogonal polynomials. Moreover, we use the series expansion of L n,d that also established equality between the first and second line.</p><p>The integral representation of ( <ref type="formula" target="#formula_18">17</ref>) may appear to be rather cumbersome. Quite counterintuitively, it holds the key to a computationally efficient expansion for kernels depending on x, x only. This is the case since we may sample from a spherically isotropic distribution of unit vectors z and compute Legendre polynomials accordingly. As we will see, computing inner products with spherically isotropic vectors can be accomplished very efficiently using a construction described in Section 4.</p><p>Corollary 4 Denote by λ n the coefficients obtained by a Legendre polynomial series expansion of κ( x, x ) and let N (d, n) = (d+n-1)! n!(d-1)! be the number of linearly independent homogeneous polynomials of degree n in d variables. Draw z i ∼ S d-1 uniformly from the unit sphere and draw n i from a spectral distribution with p</p><formula xml:id="formula_21">(n) ∝ λ n N (d, n). Then E m -1 m i=1 L n i ,d ( x, z i )L n i ,d ( x , z i ) = κ( x, x )<label>(18)</label></formula><p>In other words, provided that we are able to compute the Legendre polynomials L n,d efficiently, and provided that it is possible to draw from the spectral distribution of λ n N (d, n), we have an efficient means of computing dot-product kernels.</p><p>For kernels on the symmetric group that are invariant under group action, i.e. kernels satisfying k(x, x ) = k(g •x, g •x ) for permutations, expansions using Young Tableaux can be found in <ref type="bibr" target="#b22">(Huang et al., 2007)</ref>. A very detailed discussion of kernels on symmetry groups is given in <ref type="bibr">(Kondor, 2008, Section 4</ref>). However, efficient means of computing such kernels rapidly still remains an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explicit Templates</head><p>In some cases expanding into eigenfunctions of a symmetry group may be undesirable. For instance, the Fourier basis is decidedly nonlocal and function expansions using it may exhibit undesirable local deviations, effectively empirical versions of the well-known Gibbs phenomenon. That is, local changes in terms of observations can have far-reaching global effects on observations quite distant from the observed covariates.</p><p>This makes it desirable to expand estimates in terms of localized basis functions, such as Gaussians, Epanechikov kernels, B-splines or Bessel functions. It turns out that the latter is just as easily achievable as the more commonplace nonlocal basis function expansions. Likewise, in some cases the eigenfunctions are expensive to compute and it would be desirable to replace them with possibly less statistically efficient alternatives that offer cheap computation.</p><p>Consequently we generalize the above derivation to general nonlinear function classes dependent on matrix multiplication or distance computation with respect to spherically symmetric sets of instances. The key is that the feature map depends on x only via</p><formula xml:id="formula_22">φ z (x) := κ(x z, x , z ) for x, z ∈ R d (19)</formula><p>That is, the feature map depends on x and z only in terms of their norms and an inner product between both terms. Here the dominant cost of evaluating φ z (x) is the inner product x z. All other operations are O(1), provided that we computed x and z previously as a one-off operation. Eq. ( <ref type="formula">19</ref>) includes the squared distance as a special case:</p><formula xml:id="formula_23">x -z 2 = x 2 + z 2 -2x z and (20) κ(x z, x , z ) := κ( x -z 2 ) (<label>21</label></formula><formula xml:id="formula_24">)</formula><p>Here κ is suitably normalized, such as dzκ(z) = 1. In other words, we expand x in terms of how close the observations are to a set of well-defined anchored basis functions. It is clear that in this case</p><formula xml:id="formula_25">k(x, x ) := dµ(z)κ z (x -z)φ z (x -z) (22)</formula><p>is a kernel function since it can be expressed as an inner product. Moreover, provided that the basis functions φ z (x) are well bounded, we can use sampling from the (normalized) measure µ(z)</p><p>to obtain an approximate kernel expansion</p><formula xml:id="formula_26">k(x, x ) = 1 n n i=1 κ(x -z i )κ(x -z i ). (<label>23</label></formula><formula xml:id="formula_27">)</formula><p>Note that there is no need to obtain an explicit closed-form expansion in ( <ref type="formula">22</ref>). Instead, it suffices to show that this expansion is well-enough approximated by draws from µ(z).</p><p>Gaussian RBF Expansion For concreteness consider the following:</p><formula xml:id="formula_28">φ z (x) = exp - a 2 x 2 -2x z + z 2 and µ(z) := exp - b 2 z 2 (24)</formula><p>Integrating out z yields</p><formula xml:id="formula_29">k(x, x ) ∝ exp - a 2 b 2a + b x 2 + x 2 - a 2 4a + 2b x -x 2 . (<label>25</label></formula><formula xml:id="formula_30">)</formula><p>This is a locally weighted variant of the conventional Gaussian RBF kernel, e.g. as described by <ref type="bibr" target="#b20">Haussler (1999)</ref>. While this loses its translation invariance, one can easily verify that for b → 0 it converges to the conventional kernel. Note that the key operation in generating an explicit kernel expansion is to evaluate z i -x for all i. We will explore settings where this can be achieved for n locations z i that are approximately random at only O(n log d) cost, where d is the dimensionality of the data. Any subsequent scaling operation is O(n), hence negligible in terms of aggregate cost. Finally note that by dividing out the terms related only to x and x respectively we obtain a 'proper' Gaussian RBF kernel. That is, we use the following features:</p><formula xml:id="formula_31">φz (x) = exp - a 2 2a 2a + b x 2 -2x z + z 2 (26) and µ(z) = exp - b 2 z 2 . (<label>27</label></formula><formula xml:id="formula_32">)</formula><p>Weighting functions µ(z) that are more spread-out than a Gaussian will yield basis function expansions that are more adapted to heavy-tailed distributions. It is easy to see that such expansions can be obtained simply by specifying an algorithm to draw z rather than having to express the kernel k in closed form at all.</p><p>Polynomial Expansions One of the main inconveniences in computational evaluation of Corollary 4 is that we need to evaluate the associated Legendre polynomials L n,d (ξ) directly. This is costly since currently there are no known O(1) expansions for the associate Legendre polynomials, although approximate O(1) variants for the regular Legendre polynomials exist <ref type="bibr" target="#b4">(Bogaert et al., 2012)</ref>. This problem can be alleviated by considering the following form of polynomial kernels:</p><formula xml:id="formula_33">k(x, x ) = p c p |S d-1 | S d-1 x, v p x , v p dv<label>(28)</label></formula><p>In this case we only need the ability to draw from the uniform distribution over the unit sphere to compute a kernel. The price to be paid for this is that the effective basis function expansion is rather more complex. To compute it we use the following tools from the theory of special functions.</p><p>• For fixed d ∈ N 0 the associated kernel is a homogeneous polynomial of degree d in x and x respectively and it only depends on x , x and the cosine of the angle θ := x,x x x between both vectors. This follows from the fact that convex combinations of homogeneous polynomials remain homogeneous polynomials. Moreover, the dependence on lenghts and θ follows from the fact that the expression is rotation invariant.</p><p>• The following integral has a closed-form solution for b ∈ N and for even a.</p><p>1</p><formula xml:id="formula_34">-1 x a (1 -x 2 ) b 2 dx = Γ a+1 2 Γ b+3 2 Γ a+b+3 2 (29)</formula><p>For odd a the integral vanishes, which follows immediately from the dependence on x a and the symmetric domain of integration [-1, 1].</p><p>• The integral over the unit-sphere S d-1 ∈ R d can be decomposed via</p><formula xml:id="formula_35">S d-1 f (x)dx = 1 -1 S d-2 f x 1 , x 2 1 -x 2 1 dx 2 (1 -x 1 ) d-3 2 dx 1 (30)</formula><p>That is, we decompose x into its first coordinate x 1 and the remainder x 2 that lies on S d-2 with suitable rescaling by (1-x 1 )</p><p>d-3</p><p>2 . Note the exponent of d-3 2 that arises from the curvature of the unit sphere. See e.g. <ref type="bibr">(Hochstadt, 1961, Chapter 6)</ref> for details. While (28) offers a simple expansion for sampling, it is not immediately useful in terms of describing the kernel as a function of x, x . For this we need to solve the integral in (28). Without loss of generality we may assume that x = (x 1 , x 2 , 0, . . . 0) and that x = (1, 0, . . . 0) with x = x = 1. In this case a single summand of (28) becomes</p><formula xml:id="formula_36">S d-1 x, v p x , v p dv = S d-1 (v 1 x 1 + v 2 x 2 ) p v p 1 dv (31) = p i=0 p i x p-i 1 x i 2 1 -1 v 2p-i 1 (1 -v 2 1 ) i+d-3 2 dv 1 S d-2 v i 2 dv = p i=0 p i x p-i 1 x i 2 1 -1 v 2p-i 1 (1 -v 2 1 ) i+d-3 2 dv 1 1 -1 v i 2 (1 -v 2 2 ) d-4 2 dv 2 |S d-3 | = |S d-3 | p i=0 p i x p-i 1 x i 2 Γ 2p-i+1 2 Γ i+d-1 2 Γ 2p+d 2 Γ i+1 2 Γ d-2 2 Γ i+d-1 2 (32)</formula><p>Using the fact that x 1 = θ and x 2 = √ 1 -θ 2 we have the full expansion of (28) via</p><formula xml:id="formula_37">k(x, x ) = p x p x p c p |S d-3 | |S d-1 | p i=0 θ p-i 1 -θ 2 i 2 p i Γ 2p-i+1 2 Γ i+d-1 2 Γ 2p+d 2 Γ i+1 2 Γ d-2 2 Γ i+d-1 2</formula><p>The above form is quite different from commonly used inner-product kernels, such as an inhomogeneous polynomial ( x, x + d) p . That said, the computational savings are considerable and the expansion bears sufficient resemblance to warrant its use due to significantly faster evaluation.</p><p>4 Sampling Basis Functions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Random Kitchen Sinks</head><p>We now discuss computationally efficient strategies for approximating the function expansions introduced in the previous section, beginning with Random Kitchen Sinks of <ref type="bibr" target="#b35">Rahimi and Recht (2008)</ref>, as described in Section 3.2. Direct use for Gaussian RBF kernels yields the following algorithm to approximate kernel functions by explicit feature construction:</p><formula xml:id="formula_38">input Scale σ 2 , n, d Draw Z ∈ R n×d with iid entries Z ij ∼ N (0, σ -2 ).</formula><p>for all x do Compute empirical feature map via</p><formula xml:id="formula_39">φ j (x) = c √ n exp(i[Zx] j )</formula><p>end for</p><p>As discussed previously, and as shown by <ref type="bibr" target="#b36">Rahimi and Recht (2009)</ref>, the associated feature map converges in expectation to the Gaussian RBF kernel. Moreover, they also show that this convergence occurs with high probability and at the rate of independent empirical averages. While this allows one to use primal space methods, the approach remains limited by the fact that we need to store Z and, more importantly, we need to compute Zx for each x. That is, each observation costs O(n • d) operations. This seems wasteful, given that we are really only multiplying x with a 'random' matrix Z, hence it seems implausible to require a high degree of accuracy for Zx.</p><p>The above idea can be improved to extend matters beyond a Gaussian RBF kernel and to reduce the memory footprint in computationally expensive settings. We summarize this in the following two remarks:</p><p>Remark 5 (Reduced Memory Footprint) To avoid storing the Gaussian random matrix Z we recompute Z ij on the fly. Assume that we have access to a random number generator which takes samples from the uniform distribution ξ ∼ U [0, 1] as input and emits samples from a Gaussian, e.g. by using the inverse cumulative distribution function z = F -<ref type="foot" target="#foot_0">foot_0</ref> (ξ). Then we may replace the random number generator by a hash function via ξ ij = N -1 h(i, j) where N denotes the range of the hash, and subsequently</p><formula xml:id="formula_40">Z ij = F -1 (ξ ij ).</formula><p>Unfortunately this variant is computationally even more costly than Random Kitchen Sinks, its only benefit being the O(n) memory footprint relative to the O(nd) footprint for random kitchen sinks. To make progress, a more effective approximation of the Gaussian random matrix Z is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fastfood</head><p>For simplicity we begin with the Gaussian RBF case and extend it to more general spectral distributions subsequently. Without loss of generality assume that d = 2 l for some l ∈ N. 1 For the moment assume that d = n. The matrices that we consider instead of Z are parameterized by a product of diagonal matrices and the Hadamard matrix:</p><formula xml:id="formula_41">V := 1 σ √ d SHGΠHB.<label>(33)</label></formula><p>Here Π ∈ {0, 1} d×d is a permutation matrix and H is the Walsh-Hadamard matrix.<ref type="foot" target="#foot_2">foot_2</ref> S, G and B are all diagonal random matrices. More specifically, B has random {±1} entries on its main diagonal, G has random Gaussian entries, and S is a random scaling matrix. V is then used to compute the feature map. The coefficients for S, G, B are computed once and stored. On the other hand, the Walsh-Hadamard matrix is never computed explicitly. Instead we only multiply by it via the fast Hadamard transform, a variant of the FFT which allows us to compute H d x in O(d log d) time. The Hadamard matrices are defined as follows:</p><formula xml:id="formula_42">H 2 := 1 1 1 -1 and H 2d := H d H d H d -H d .</formula><p>When n &gt; d, we replicate (33) for n/d independent random matrices V i and stack them via</p><formula xml:id="formula_43">V T = [V 1 , V 2 , . . . V n/d</formula><p>] T until we have enough dimensions. The feature map for Fastfood is then defined as</p><formula xml:id="formula_44">φ j (x) = n -1 2 exp(i[V x] j ). (<label>34</label></formula><formula xml:id="formula_45">)</formula><p>Before proving that in expectation this transform yields a Gaussian random matrix, let us briefly verify the computational efficiency of the method.</p><p>Lemma 6 (Computational Efficiency) The features of ( <ref type="formula" target="#formula_44">34</ref> Note that the construction of V is analogous to that of <ref type="bibr" target="#b10">Dasgupta et al. (2011)</ref>. We will use these results in establishing a sufficiently high degree of decorrelation between rows of V . Also note that multiplying with a longer chain of Walsh-Hadamard matrices and permutations would yield a distribution closer to independent Gaussians. However, as we shall see, two matrices provide a sufficient amount of decorrelation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Basic Properties</head><p>Now that we showed that the above operation is fast, let us give some initial indication why it is also useful and how the remaining matrices S, G, B, Π are defined.</p><p>Binary scaling matrix B: This is a diagonal matrix with B ii ∈ {±1} drawn iid from the uniform distribution over {±1}. The initial HBd -1 2 acts as an isometry that densifies the input, as pioneered by <ref type="bibr" target="#b0">Ailon and Chazelle (2009)</ref>.</p><p>Permutation Π: It ensures that the rows of the two Walsh-Hadamard matrices are incoherent relative to each other. Π can be stored efficiently as a lookup table at O(d) cost and it can be generated by sorting random numbers.</p><p>Gaussian scaling matrix G: This is a diagonal matrix whose elements G ii ∼ N (0, 1) are drawn iid from a Gaussian. The next Walsh-Hadamard matrices H will allow us to 'recycle' n Gaussians to make the resulting matrix closer to an iid Gaussian. The goal of the preconditioning steps above is to guarantee that no single G ii can influence the output too much and hence provide near-independence.</p><p>Scaling matrix S: Note that the length of all rows of HGΠHB are constant as equation ( <ref type="formula" target="#formula_48">36</ref>) shows below. In the Gaussian case S ensures that the length distribution of the row of V are independent of each other. In the more general case, one may also adjust the capacity of the function class via a suitably chosen scaling matrix S. That is, large values in S ii correspond to high complexity basis functions whereas small S ii relate to simple functions with low total variation. For the RBF kernel we choose</p><formula xml:id="formula_46">S ii = s i G -1 2 Frob where p(s i ) ∝ r d-1 e -r 2 2 . (<label>35</label></formula><formula xml:id="formula_47">)</formula><p>Thus s i matches the radial part of a normal distribution and we rescale it using the Frobenius norm of G.</p><p>We now analyze the distribution of entries in V .</p><p>The rows of HGΠHB have the same length. To compute their length we take</p><formula xml:id="formula_48">l 2 := HGΠHB(HGΠHB) jj = [HG 2 H] jj d = i H 2 ij G 2 ii d = G 2 Frob d<label>(36)</label></formula><p>In this we used the fact that H H = d1 and moreover that</p><formula xml:id="formula_49">|H ij | = 1. Consequently, rescaling the entries by G -1 2 Frob d -1 2 yields rows of length 1.</formula><p>Any given row of HGΠHB is iid Gaussian. Each entry of the matrix</p><formula xml:id="formula_50">[HGΠHB] ij = B jj H T i GΠH j</formula><p>is zero-mean Gaussian as it consists of a sum of zero-mean independent Gaussian random variables. Sign changes retain Gaussianity. Also note that Var [HGΠHB] ij = d. B ensures that different entries in [HGΠHB] i• have 0 correlation. Hence they are iid Gaussian (checking first and second order moments suffices).</p><p>The rows of SHGΠHB are Gaussian. Rescaling the length of a Gaussian vector using (35) retains Gaussianity. Hence the rows of SHGΠHB are Gaussian, albeit not independent.</p><p>Lemma 7 The expected feature map recovers the Gaussian RBF kernel, i.e.,</p><formula xml:id="formula_51">E S,G,B,Π φ(x) φ(x ) = e -x-x 2 2σ 2</formula><p>.</p><p>Moreover, the same holds for V = 1 σ √ d HGΠHB. Proof We already proved that any given row in V is a random Gaussian vector with distribution N (0, σ -2 I d ), hence we can directly appeal to the construction of <ref type="bibr" target="#b35">Rahimi and Recht (2008)</ref>. This also holds for V . The main difference being that the rows in V are considerably more correlated. Note that by assembling several d × d blocks to obtain an n × d matrix this property is retained, since each block is drawn independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Changing the Spectrum</head><p>Changing the kernel from a Gaussian RBF to any other radial basis function kernel is straightforward. After all, HGΠHB provides a approximately spherically uniformly distributed random vectors of the same length. Rescaling each direction of projection separately costs only O(n) space and computation. Consequently we are free to choose different coefficients S ii rather than ( <ref type="formula" target="#formula_46">35</ref>). Instead, we may use</p><formula xml:id="formula_52">S ii ∼ c -1 r d-1 A -1 d-1 λ(r).</formula><p>Here c is a normalization constant and λ(r) is the radial part of the spectral density function of the regularization operator associated with the kernel.</p><p>A key advantage over a conventional kernel approach is that we are not constrained by the requirement that the spectral distributions be analytically computable. Even better, we only need to be able to sample from the distribution rather than compute its Fourier integral in closed form.</p><p>For concreteness consider the Matern kernel. Its spectral properties are discussed, e.g. by <ref type="bibr" target="#b40">Schölkopf and Smola (2002)</ref>. In a nutshell, given data in R d denote by ν := d 2 a dimension calibration and let t ∈ N be a fixed parameter determining the degree of the Matern kernel (which is usually determined experimentally). Moreover, denote by J ν (r) the Bessel function of the first kind of order ν. Then the kernel given by</p><formula xml:id="formula_53">k(x, x ) := x -x -tν J t ν ( x -x ) for n ∈ N<label>(37)</label></formula><p>has as its associated Fourier transform</p><formula xml:id="formula_54">Fk(ω) = n i=1 χ S d [ω].</formula><p>Here χ S d is the characteristic function on the unit ball in R d and denotes convolution. In words, the Fourier transform of k is the n-fold convolution of χ S d . Since convolutions of distributions arise from adding independent random variables this yields a simple algorithm for computing the Matern kernel:</p><p>for each S ii do Draw t iid samples ξ i uniformly from S d . Use S ii = t i=1 ξ i as scale. end for While this may appear costly, it only needs to be carried out once at initialization time and it allows us to sidestep computing the convolution entirely. After that we can store the coefficients S ii . Also note that this addresses a rather surprising problem with the Gaussian RBF kernelin high dimensional spaces draws from a Gaussian are strongly concentrated on the surface of a sphere. That is, we only probe the data with a fixed characteristic length. The Matern kernel, on the other hand, spreads its capacity over a much larger range of frequencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inner Product Kernels</head><p>We now put Theorem 3 and Corollary 4 to good use. Recall that the latter states that any dotproduct kernel can be obtained by taking expectations over draws from the degree of corresponding Legendre polynomial and over a random direction of reference, as established by the integral representation of (17).</p><p>It is understood that the challenging part is to draw vectors uniformly from the unit sphere. Note, though, that it is this very operation that Fastfood addresses by generating pseudo-Gaussian vectors. Hence the modified algorithm works as follows:</p><p>Initialization for j = 1 to n/d do Generate matrix block V j ← G -1 Frob d -1 2 HGΠHB implicitly as per (33). Draw degrees</p><formula xml:id="formula_55">n i from p(n) ∝ λ n N (d, n). end for Computation r ← x and t ← V x for i = 1 to n do ψ i ← L n i ,d (t i ) = r n i L n i ,d (t i /r) end for Note that the equality L n i ,d (t i ) = r n i L n i ,d (t i /r</formula><p>) follows from the fact that L n i ,d is a homogeneous polynomial of degree n i . The second representation may sometimes be more effective for reasons of numerical stability. As can be seen, this relies on access to efficient Legendre polynomial computation. Recent work of <ref type="bibr" target="#b4">Bogaert et al. (2012)</ref> shows that (quite surprisingly) this is possible in O(1) time for L n (t) regardless of the degree of the polynomial. Extending these guarantees to associated Legendre polynomials is unfortunately rather nontrivial. Hence, a direct expansion in terms of x, v d , as discussed previously, or brute force computation may well be more effective.</p><p>Remark 8 (Kernels on the Symmetric Group) We conclude our reasoning by providing an extension of the above argument to the symmetric group. Clearly, by treating permutation matrices Π ∈ C n as d × d dimensional vectors, we can use them as inputs to a dot-product kernel. Subsequently, taking inner products with random reference vectors of unit length yields kernels which are dependent on the matching between permutations only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>The next step is to show that the feature map is well behaved also in terms of decorrelation between rows of V . We focus on Gaussian RBF kernels in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Low Variance</head><p>When using random kitchen sinks, the variance of the feature map is at least O(1/n) since we draw n samples iid from the space of parameters. In the following we show that the variance of fastfood is comparable, i.e. it is also O(1/n), albeit with a dependence on the magnitude of the magnitude of the inputs of the feature map. This guarantee matches empirical evidence that both algorithms perform equally well as the exact kernel expansion.</p><p>For convenience, since the kernel values are real numbers, let us simplify terms and rewrite the inner product in terms of a sum of cosines. Trigonometric reformulation yields</p><formula xml:id="formula_56">1 n j φj (x)φ j (x ) = 1 n j cos [V (x -x )] j for V = d -1 2 HGΠHB.<label>(38)</label></formula><p>We begin the analysis with a general variance bound for square matrices V ∈ R d×d . The extension to n/d iid drawn stacked matrices is deferred to a subsequent corollary.</p><p>Theorem 9 Let v = x-x σ and let ψ j (v) = cos[V v] j denote the estimate of the kernel value arising from the jth pair of random features for each j ∈ {1 . . . d}. Then for each j we have</p><formula xml:id="formula_57">Var [ψ j (v)] = 1 2 1 -e -v 2 2 and Var   d j=1 ψ j (v)   ≤ d 2 1 -e -v 2 2 + dC( v )<label>(39)</label></formula><p>where C(α) = 6α 4 e -α 2 + α 2 3 depends on the scale of the argument of the kernel.</p><p>Proof Since for any random variable X j we can decompose Var ( X j ) = j,t Cov(X j , X t ) our goal is to compute</p><formula xml:id="formula_58">Cov(ψ(v), ψ(v)) = E ψ(v)ψ(v) -E [ψ(v)] E [ψ(v)] .</formula><p>We decompose V v into a sequence of terms w = d -1 2 HBv and u = Πw and z = HGu. Hence we have ψ j (v) = cos(z j ). Note that u = v since by construction d -1 2 H, B and Π are orthonormal matrices.</p><p>Gaussian Integral Now condition on the value of u. Then it follows that Cov(z j , z t |u) = ρ jt (u) v 2 = ρ(u) v 2 where ρ ∈ [-1, 1] is the correlation of z j and z t . By symmetry all ρ ij are identical.</p><p>Observe that the marginal distribution of each z j is N (0, v 2 ) since each element of H is ±1. Thus the joint distribution of z j and z t is a Gaussian with mean 0 and covariance</p><formula xml:id="formula_59">Cov [[z j , z t ]|u] = 1 ρ ρ 1 v 2 = L • L T where L = 1 0 ρ 1 -ρ 2 v is its Cholesky factor. Hence Cov(ψ j (v), ψ t (v)|u) = E g [cos([Lg] 1 ) cos([Lg] 2 )] -E g [cos(z j )]E g [cos(z t )]<label>(40)</label></formula><p>where g ∈ R 2 is drawn from N (0, 1). From the trigonometric identity</p><formula xml:id="formula_60">cos(α) cos(β) = 1 2 [cos(α -β) + cos(α + β)]</formula><p>it follows that we can rewrite</p><formula xml:id="formula_61">E g [cos([Lg] 1 ) cos([Lg] 2 )] = 1 2 E h [cos(a -h) + cos(a + h)] = 1 2 e -1 2 a 2 -+ e -1 2 a 2 +</formula><p>where h ∼ N (0, 1) and</p><formula xml:id="formula_62">a 2 ± = L [1, ±1] 2 = 2 v 2 (1 ± ρ).</formula><p>That is, after applying the addition theorem we explicitly computed the now one-dimensional Gaussian integrals. We compute the first moment analogously. Since by construction z j and z j have zero mean and variance v 2 we have that</p><formula xml:id="formula_63">E g [cos(z j )]E g [cos(z t )] = E h [cos( v h)] 2 = e -v 2</formula><p>Combining both terms we obtain that the covariance can be written as</p><formula xml:id="formula_64">Cov[ψ j (v), ψ t (v)|u] = e -v 2 cosh[ v 2 ρ] -1<label>(41)</label></formula><p>Taylor Expansion To prove the first claim note that here j = t, since we are computing the variance of a single feature. Correspondingly ρ(u) = 1. Plugging this into (41) and simplifying terms yields the first claim of (39).</p><p>To prove our second claim, observe that from the Taylor series of cosh with remainder in Lagrange form, it follows that there exists η ∈</p><formula xml:id="formula_65">[-v 2 |ρ|, v 2 |ρ|] such cosh( v 2 ρ) -1 = 1 2 v 4 ρ 2 + 1 6 sinh(η) v 6 ρ 3 ≤ 1 2 v 4 ρ 2 + 1 6 sinh( v 2 ) v 6 ρ 3 ≤ ρ 2 v 4 B( v ),</formula><p>where</p><formula xml:id="formula_66">B( v ) = 1 2 + sinh( v 2 ) v 2 6</formula><p>. Plugging this into (41) yields</p><formula xml:id="formula_67">Cov[ψ j (v), ψ t (v)|u] ≤ ρ 2 v 4 B( v ).</formula><p>Bounding E u [ρ 2 ] Note that the above is still conditioned on u. What remains is to bound</p><formula xml:id="formula_68">E u [ρ 2 ], which is small if E[ u 4</formula><p>4 ] is small. The latter is ensured by HB, which acts as a randomized preconditioner: Since G is diagonal and G ii ∼ N (0, 1) independently we have</p><formula xml:id="formula_69">Cov[z, z] = Cov[HGu, HGu] = H Cov[Gu, Gu]H = HE diag(u 2 1 , . . . , u 2 d ) H .</formula><p>Recall that H ij = H ji are elements of the Hadamard matrix. For ease of notation fix j = t and let T = {i ∈ [1..d] : H ji = H ti } be the set of columns where the j th and the t th row of the Hadamard matrix agree. Then</p><formula xml:id="formula_70">Cov(z j , z t |u) = d i=1 H ji H ti u 2 i = i∈T u 2 i - i / ∈T u 2 i = 2 i∈T u 2 i - d i=1 u 2 i = 2 i∈T u 2 i -v 2 .</formula><p>Now recall that u = Πw and that Π is a random permutation matrix. Therefore u i = w π(i) for a randomly chosen permutation π and thus the distribution of ρ v 2 and 2 i∈R w 2 i -v 2 where R is a randomly chosen subset of size d 2 in {1 . . . d} are the same. Let us fix (condition on)</p><formula xml:id="formula_71">w. Since 2E R i∈R w 2 i = v 2 we have that E R ρ 2 v 4 = 4E R   i∈R w 2 i 2   -v 4 . Now let δ i = 1 if i ∈ R and 0 otherwise. Note that E δ (δ i ) = 1 2 and if j = k then E δ (δ i δ k ) ≤ 1 4 as δ i are (mildly) negatively correlated. From w = v it follows that E R   i∈R w 2 i 2   = E δ   d i=1 δ i w 2 i 2   = E δ   i =k δ i δ k w 2 i w 2 k   + E δ i δ i w 4 i ≤ v 4 4 + w 4 4 2 .</formula><p>From the two equations above it follows that</p><formula xml:id="formula_72">E R ρ 2 v 4 ≤ 2 w 4 4 . (<label>42</label></formula><formula xml:id="formula_73">)</formula><p>Bounding the fourth moment of w Let b i = B ii be the independent ±1 random variables of B. Using the fact w</p><formula xml:id="formula_74">i = 1 √ d d t=1</formula><p>H it b t v t and that b i are independent with similar calculations to the above it follows that</p><formula xml:id="formula_75">E b w 4 i ≤ 6 d 2   v 4 i + t =j v 2 t v 2 j   and hence E b w 4 4 ≤ 6 d v<label>4</label></formula><p>2 which shows that 1 √ d HB acts as preconditioner that densifies the input. Putting it all together we have</p><formula xml:id="formula_76">j =t E u [Cov(ψ j (v), ψ t (v)|u)] ≤ d 2 e -v 2 B( v )E R [ρ 2 v 4 ] ≤ 12de -v 2 B( v ) v 4 = 6d v 4 e -v 2 + v 2 /3</formula><p>Combining the latter with the already proven first claim establishes the second claim.</p><p>Corollary 10 Denote by V, V Gauss-like matrices of the form</p><formula xml:id="formula_77">V = σ -1 d -1 2 HGΠHB and V = σ -1 d -1 2 SHGΠHB.<label>(43)</label></formula><p>Moreover, let C(α) = 6α 4 e -α 2 + α 2 3 be a scaling function. Then for the feature maps obtained by stacking n/d iid copies of either V or V we have</p><formula xml:id="formula_78">Var φ (x) φ (x ) ≤ 2 n 1 -e -v 2 2 + 1 n C( v ) where v = σ -1 (x -x ). (<label>44</label></formula><formula xml:id="formula_79">)</formula><p>Proof Since φ (x) φ (x ) is the average of n/d independent estimates, each arising from 2d features. Hence we can appeal to Theorem 9 for a single block, i.e. when n = d. The near-identical argument for V is omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Concentration</head><p>The following theorem shows that for a given error probability δ, the approximation error of a d × d block of Fastfood is at most logarithmically larger than the error of Random Kitchen Sinks. That is, it is only logarithmically weaker. We believe that this bound is pessimistic and could be further improved with considerable analytic effort. That said, the O(m -1 2 ) approximation guarantees to the kernel matrix are likely rather conservative when it comes to generalization performance, as we found in experiments. In other words, we found that the algorithm works much better in practice than in theory, as confirmed in Section 6. Nonetheless it is important to establish tail bounds, not to the least since this way improved guarantees for random kitchen sinks also immediately benefit fastfood.</p><p>Theorem 11 For all x, x ∈ R d let k(x, x ) = d j=1 cos(d -1 2 [HGΠHB(x -x )/σ] j )/d denote our estimate of the RBF kernel k(x, x ) that arises from a d × d block of Fastfood. Then we have that</p><formula xml:id="formula_80">P k(x, x ) -k(x, x ) ≥ 2σ -1 d -1 2 x -x log(2/δ) log(2d/δ) ≤ 2δ for all δ &gt; 0</formula><p>Theorem 11 demonstrates almost sub-Gaussian convergence Fastfood kernel for a fixed pair of points x, x . A standard -net argument then shows uniform convergence over any compact set of R d with bounded diameter <ref type="bibr">(Rahimi and Recht, 2008, Claim 1)</ref>. Also, the small error of the approximate kernel does not significantly perturb the solution returned by wide range of learning algorithms <ref type="bibr">(Rahimi and Recht, 2008, Appendix B)</ref> or affect their generalization error.</p><p>Our key tool is concentration of Lipschitz continuous functions under the Gaussian measure <ref type="bibr" target="#b28">Ledoux (1996)</ref>. We ensure that Fastfood construct has a small Lipschitz constant using the following lemma, which is due to <ref type="bibr" target="#b0">Ailon and Chazelle (2009)</ref>.</p><p>Lemma 12 <ref type="bibr" target="#b0">(Ailon and Chazelle, 2009)</ref> Let x ∈ R d and t &gt; 0. Let H, B ∈ R d×d denote the Hadamard and the binary random diagonal matrices respectively in our construction. Then for any δ &gt; 0 we have that</p><formula xml:id="formula_81">P HBx ∞ ≥ x 2 2 log 2d/δ ≤ δ<label>(45)</label></formula><p>In other words, with high probability, the largest elements of d -1 2 HBx are with high probability no larger than what one could expect if all terms were of even size as per the x 2 norm.</p><p>To use concentration of the Gaussian measure we need Lipschitz continuity. We refer to a function f : R d → R as Lipschitz continuous with constant L if for all x, y ∈ R d it holds that |f (x) -f (y)| ≤ L x -y 2 . Then the following holds <ref type="bibr" target="#b28">(Ledoux, 1996</ref>, Inequality 2.9):</p><p>Theorem 13 Assume that f : R d → R is Lipschitz continuous with constant L and let g ∼ N (0, 1) be drawn from a d-dimensional Normal distribution. Then we have Observe that Lemma 7 implies E G,Π,B [f (G, Π, B)] = k(v). Therefore it is sufficient to prove that f (G, Π, B) concentrates around its mean. We will accomplish this by showing that f is Lipschitz continuous as a function of G for most Π and B. For</p><formula xml:id="formula_82">P [|f (g) -E g [f (g)]| ≥ t] ≤ 2e -t 2 2L 2 . (<label>46</label></formula><formula xml:id="formula_83">a ∈ R d let h(a) = d -1 d j=1 cos(a j ). (<label>47</label></formula><formula xml:id="formula_84">)</formula><p>Using the fact that cosine is Lipschitz continuous with constant 1 we observe that for any pair of vectors a, b ∈ R d it holds that</p><formula xml:id="formula_85">|h(a) -h(b)| ≤ d -1 d j=1 | cos(a j ) -cos(b j )| ≤ d -1 a -b 1 ≤ d -1 2 a -b 2 . (<label>48</label></formula><formula xml:id="formula_86">)</formula><p>For any vector g ∈ R d let diag(g) ∈ R d×d denote the diagonal matrix whose diagonal is g. Observe that for any pair of vectors g, g ∈ R d we have that</p><formula xml:id="formula_87">Hdiag(g)u -Hdiag(g )u 2 ≤ H 2 diag(g -g )u 2</formula><p>Let G = Diag(g) in the Fastfood construct and recall the definition of function h as in (47).</p><p>Combining the above inequalities for any pair of vectors g, g ∈ R d yields the following bound</p><formula xml:id="formula_88">|h(HDiag(g)u)) -h(HDiag(g )u)| ≤ u ∞ g -g 2 .<label>(49)</label></formula><p>From u = Πd -1 2 HBv and Πw ∞ = w ∞ combined with Lemma 12 it follows that</p><formula xml:id="formula_89">u ∞ ≤ v 2 2 d log 2d δ<label>(50)</label></formula><p>holds with probability at least 1 -δ, where the probability is over the choice of B.<ref type="foot" target="#foot_3">foot_3</ref> Now condition on (50). From inequality (49) we have that the function</p><formula xml:id="formula_90">g → h(Hdiag(g)u) = f (diag(g), Π, B)</formula><p>is Lipschitz continuous with Lipschitz constant</p><formula xml:id="formula_91">L = v 2 2 d log 2d δ .<label>(51)</label></formula><p>Hence from Theorem 13 and from the independently chosen G jj ∼ N (0, 1) it follows that</p><formula xml:id="formula_92">P G |f (G, Π, B) -k(v)| ≥ 2L log 2/δ ≤ δ.<label>(52)</label></formula><p>Combining inequalities ( <ref type="formula" target="#formula_91">51</ref>) and ( <ref type="formula" target="#formula_92">52</ref>) with the union bound concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In the following we assess the performance of Random Kitchen Sinks and Fastfood. The results show that Fastfood performs as well as Random Kitchen Sinks in terms of accuracy. Fastfood, however, is orders of magnitude faster and exhibits a significantly lower memory footprint. For simplicity, we focus on penalized least squares regression since in this case we are able to compute exact solutions and are independent of any other optimization algorithms. We also benchmark Fastfood on <ref type="bibr">CIFAR-10 (Krizhevsky, 2009)</ref> and observe that it achieves state-of-the-art accuracy. This advocates for the use of non-linear expansions even when d is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Approximation quality</head><p>We begin by investigating how well our features can approximate the exact kernel computation as n increases. For that purpose, we uniformly sample 4000 vectors from [0, 1] 10 . We compare the exact kernel values to Random Kitchen Sinks and Fastfood.</p><p>The results are shown in Figure <ref type="figure" target="#fig_2">1</ref>. We used the absolute difference between the exact kernel and the approximation to quantify the error (the relative difference also exhibits similar behavior and is thus not shown due to space constraints). The results are presented as averages, averaging over 4000 samples. As can be seen, as n increases, both Random Kitchen Sinks and Fastfood converge quickly to the exact kernel values. Their performance is indistinguishable, as expected from the construction of the algorithm. Note, though, that fidelity in approximating k(x, x ) does not imply generalization performance (unless the bounds are very tight). To assess this, we carried out experiments on all regression datasets from the UCI repository <ref type="bibr" target="#b15">(Frank and Asuncion, 2010)</ref> that are not too tiny, i.e., that contained at least 4, 000 instances.</p><p>We investigate estimation accuracy via Gaussian process regression <ref type="bibr" target="#b37">(Rasmussen and Williams, 2006)</ref> using approximated kernel computation methods and we compare this to exact kernel computation whenever the latter is feasible. For completeness, we compare the following methods:</p><p>Exact RBF uses the exact Gaussian RBF kernel, that is k(x, x ) = exp -x -x 2 /2σ 2 . This is possible, albeit not practically desirable due to its excessive cost, on all but the largest datasets where the kernel matrix does not fit into memory.</p><p>Nystrom uses the Nystrom approximation of the kernel matrix <ref type="bibr" target="#b54">(Williams and Seeger, 2001)</ref>. These methods have received recent interest due to the improved approximation guarantees of <ref type="bibr" target="#b23">Jin et al. (2011)</ref> which indicate that approximation rates faster than O(n -1 2 ) are achievable. Hence, theoretically, the Nystrom method could have a significant accuracy advantage over Random Kitchen Sinks and Fastfood when using the same number of basis functions, albeit at exponentially higher cost of O(d) vs. O(log d) per function. We set n = 2, 048 to retain a computationally feasible feature projection.</p><p>Random Kitchen Sinks uses the the Gaussian random projection matrices of <ref type="bibr" target="#b35">Rahimi and Recht (2008)</ref>. As before, we use n = 2, 048 basis functions. Note that this is a rather difficult setting for Random Kitchen Sinks relative to the Nystrom decomposition, since the basis functions obtained in the latter are arguably better in terms of approximating the kernel. Hence, one would naively expect slightly inferior performance from Random Kitchen Sinks relative to direct Hilbert Space methods.</p><p>Fastfood (Hadamard features) uses the random matrix given by SHGΠHB, again with n = 2, 048 dimensions. Based on the above reasoning one would expect that the performance of the Hadamard features is even weaker than that of Random Kitchen Sinks since now the basis functions are no longer even independently drawn from each other.</p><p>FFT Fastfood (Fourier features) uses a variant of the above construction. Instead of combining two Hadamard matrices, a permutation and Gaussian scaling, we use a permutation in conjunction with a Fourier Transform matrix F : the random matrix given by V = ΠF B. The motivation is the Subsampled Random Fourier Transform, as described by <ref type="bibr" target="#b49">Tropp (2010)</ref>: by picking a random subset of columns from a (unitary) Fourier matrix, we end up with vectors that are almost spatially isotropic, albeit with slightly more dispersed lengths than in Fastfood. We use this heuristic for comparison purposes.</p><p>Exact Poly uses the exact polynomial kernel, that is k(x, x ) = ( z, x + 1) d , with d = 10. Similar to the case of Exact RBF, this method is only practical on small datasets.</p><p>Fastfood Poly uses the Fastfood trick via Spherical Harmonics to approximate the polynomial kernels.</p><p>The results of the comparison are given in Table <ref type="table" target="#tab_3">3</ref>. As can be seen, and contrary to the intuition above, there is virtually no difference between the exact kernel, the Nystrom approximation, Random Kitchen Sinks and Fastfood. In other words, Fastfood performs just as well as the exact method, while being substantially cheaper to compute. Somewhat surprisingly, the Fourier features work very well. This indicates that the concentration of measure effects impacting Gaussian RBF kernels may actually be counterproductive at their extreme. This is corroborated by the good performance observed with the Matern kernel. In Figure <ref type="figure" target="#fig_3">2</ref>, we show regression performance as a function of the number of basis functions n on the CPU dataset. As is evident, it is necessary to have a large n in order to learn highly nonlinear functions. Interestingly, although the Fourier features do not seem to approximate the Gaussian RBF kernel, they perform well compared to other variants and improve as n increases. This suggests that learning the kernel by direct spectral adjustment might be a useful application of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Speed of kernel computations</head><p>In the previous experiments, we observe that Fastfood is on par with exact kernel computation, the Nystrom method, and Random Kitchen Sinks. The key point, however, is to establish whether the algorithm offers computational savings. For this purpose we compare Random Kitchen Sinks using Eigen<ref type="foot" target="#foot_4">foot_4</ref> and our method using Spiral<ref type="foot" target="#foot_5">foot_5</ref> . Both are highly optimized numerical linear algebra libraries in C++. We are interested in the time it takes to go from raw features of a vector with dimension d to the label prediction of that vector. On a small problem with d = 1, 024 and n = 16, 384, performing prediction with Random Kitchen Sinks takes 0.07 seconds. Our method is around 24x faster, taking only 0.003 seconds to compute the label for one input vector. The speed gain is even more significant for larger problems, as is evident in Table <ref type="table" target="#tab_2">2</ref>. This confirms experimentally the O(n log d) vs. O(nd) runtime and the O(n) vs. O(nd) storage of Fastfood relative to Random Kitchen Sinks. In other words, the computational savings are substantial for large input dimensionality d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Random features for CIFAR-10</head><p>To understand the importance of nonlinear feature expansions for a practical application, we benchmarked Fastfood, Random Kitchen Sinks on the CIFAR-10 dataset <ref type="bibr" target="#b27">Krizhevsky (2009)</ref> which has 50,000 training images and 10,000 test images. Each image has 32x32 pixels and 3 channels (d = 3072). In our experiments, linear SVMs achieve 42.3% accuracy on the test set. Non-linear expansions improve the classification accuracy significantly. In particular, Fastfood FFT ("Fourier features") achieve 63.1% while Fastfood ("Hadamard features") and Random Kitchen Sinks achieve 62.4% with an expansion of n = 16, 384. These are also best known classification accuracies using permutation-invariant representations on this dataset. In terms of speed, Random Kitchen Sinks is 5x slower (in total training time) and 20x slower (in predicting a label given an image) compared to both Fastfood and and Fastfood FFT. This demonstrates that non-linear expansions are needed  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>) can be computed at O(n log d) cost using O(n) permanent storage for n ≥ d.ProofStoring the matrices S, G, B costs 3n entries and 3n operations for a multiplication. The permutation matrix Π costs n entries and n operations. The Hadamard matrix itself requires no storage since it is only implicitly represented. Furthermore, the fast Hadamard transforms costs O(n log d) operations to carry out since we have O(d log d) per block and n/d blocks. Computing the Fourier basis for n numbers is an O(n) operation. Hence the total CPU budget is O(n log d) and the storage is O(n).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><figDesc>Proof [Theorem 11] Since both k and k are shift invariant we set v = σ(x -x ) and write k(v) = k(x, x ) and k(v) = k(x, x ) to simplify the notation. Set u = Πd -1 2 HBv, and z = HGu and define f (G, Π, B) = d -1 d j=1 cos(z j ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Kernel approximation errors of different methods with respect to number of basis functions n.</figDesc><graphic coords="22,116.98,72.01,374.36,287.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Test RMSE on CPU dataset with respect to the number of basis functions. As number of basis functions increases, the quality of regression generally improves.</figDesc><graphic coords="24,140.38,71.99,327.58,256.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Speed and memory improvements of Fastfood relative to Random Kitchen Sinks</figDesc><table><row><cell>d</cell><cell>n Fastfood</cell><cell cols="2">RKS Speedup RAM</cell></row><row><cell cols="3">1, 024 16, 384 0.00058s 0.0139s</cell><cell>24x</cell><cell>256x</cell></row><row><cell cols="3">4, 096 32, 768 0.00137s 0.1222s</cell><cell>89x 1024x</cell></row><row><cell cols="3">8, 192 65, 536 0.00269s 0.5351</cell><cell>199x 2048x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test set RMSE of different kernel computation methods. We can see Fastfood methods perform comparably with Exact RBF, Nystrom, Random Kitchen Sinks (RKS) and Exact Polynomial (degree 10). m and d are the size of the training set the dimension of the input. Note that the problem size made it impossible to compute the exact solution for datasets of size 40,000 and up.</figDesc><table><row><cell>m d Exact Nystrom RKS Fastfood Fastfood Exact Fastfood Exact Fastfood</cell><cell>RBF RBF RBF FFT Poly RBF Matern Matern Poly</cell><cell>5, 822 85 0.231 0.232 0.266 0.266 0.271 0.264 0.234 0.235 0.256</cell><cell>4, 080 11 0.819 0.797 0.740 0.721 0.731 0.740 0.753 0.720 0.827</cell><cell></cell><cell>4, 700 21 0.059 0.058 0.054 0.052 0.055 0.054 0.053 0.052 0.061</cell><cell>6, 554 21 7.271 6.758 7.103 4.544 5.451 7.366 4.345 4.211 7.959</cell><cell>42, 800 384 n.a. 60.683 49.491 58.425 53.793 43.858 n.a. 14.868 n.a.</cell><cell></cell><cell>51, 686 27 n.a. 17.872 17.837 17.826 17.818 n.a. 17.846 n.a.</cell></row><row><cell>Dataset</cell><cell></cell><cell>Insurance</cell><cell>Wine</cell><cell>Quality</cell><cell>Parkinson</cell><cell>CPU</cell><cell>CT slices</cell><cell>(axial)</cell><cell>KEGG</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If this is not the case, we can trivially pad the vectors with zeros until d =</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>l holds.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>We conjecture that H can be replaced by any matrix T ∈ R d×d , such that T / √ d is orthonormal, maxij |Tij| = O(1), i.e. T is smooth, and T x can be computed in O(d log d) time. A natural candidate is the Discrete Cosine Transform (DCT).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>Note that in contrast to Theorem 9, the permutation matrix Π does not play a role in the proof of Theorem 11.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>http://eigen.tuxfamily.org/index.php?title=Main_Page</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>http://spiral.net</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>even when the raw data is high-dimensional, and that Fastfood is more practical for such problems.</p><p>In particular, in many cases, linear function classes are used because they provide fast training time, and especially test time, but not because they offer better accuracy. The results on CIFAR-10 demonstrate that Fastfood can overcome this obstacle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary</head><p>We demonstrated that it is possible to compute n nonlinear basis functions in O(n log d) time, a significant speedup over the best competitive algorithms. This means that kernel methods become more practical for problems that have large datasets and/or require real-time prediction. In fact, Fastfood can be used to run on cellphones because not only it is fast, but it also requires only a small amount of storage.</p><p>Note that our analysis is not limited to translation invariant kernels but it also includes inner product formulations. This means that for most practical kernels our tools offer an easy means of making kernel methods scalable beyond simple subspace decomposition strategies. Extending our work to other symmetry groups is subject to future research. Also note that fast multiplications with near-Gaussian matrices are a key building block of many randomized algorithms. It remains to be seen whether one could use the proposed methods as a substitute and reap significant computational savings.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The fast johnson-lindenstrauss transform and approximate nearest neighbors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="302" to="322" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theoretical foundations of the potential function method in pattern recognition learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rozonoér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autom. Remote Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="821" to="837" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">La théorie générale des noyaux réproduisants et ses applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Cambridge Philos. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="133" to="153" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Harmonic Analysis on Semigroups</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P R</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ressel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">O(1) computation of legendre polynomials and gausslegendre nodes and weights for parallel computing</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bogaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Michiels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fostier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="C101" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Conf. Computational Learning Theory</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</editor>
		<meeting>Annual Conf. Computational Learning Theory<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992-07">July 1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="123" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simplified support vector decision rules</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Machine Learning</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Saitta</surname></persName>
		</editor>
		<meeting>Intl. Conf. Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</editor>
		<meeting>the 28th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1057" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast locality-sensitive hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1073" to="1081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.4293</idno>
		<title level="m">The random forest kernel and other kernels for big data from random partitions</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R.-E</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008-08">August 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient SVM training using low-rank kernel representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="243" to="264" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An equivalence between sparse approximation and support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1455" to="1480" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rates of convergence for radial basis functions and neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anzellotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks for Speech and Vision</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="97" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rapid evaluation of multiple density models</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conference on Artificial Intelligence and Statistics</title>
		<meeting>Intl. Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Convolution kernels on discrete structures</title>
		<author>
			<persName><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
		<idno>UCS-CRL-99-10</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>UC Santa Cruz</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Special functions of mathematical physics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hochstadt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>Dover</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference for distributions on permutations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved bound for the nystrom&apos;s method and its application to kernel classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1111.2262" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A correspondence between Bayesian estimation on stochastic processes and smoothing by splines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="495" to="502" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Group theoretical methods in machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<ptr target="http://people.cs.uchicago.edu/~risi/papers/KondorThesis.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Introductory Functional Analysis with Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kreyszig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Isoperimetry and gaussian analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures on probability theory and statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="165" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast high-dimensional kernel summations using the monte carlo multipole method</title>
		<author>
			<persName><forename type="first">Dongryeol</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Information Theory, Inference, and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linear support vector machines via dual cached loops</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2339530" />
	</analytic>
	<monogr>
		<title level="m">The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type and their connection with the theory of integral equations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci., A</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="415" to="446" />
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interpolation of scattered data: distance matrices and conditionally positive definite functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Priors for infinite networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
		<idno>CRG-TR-94-1</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<date type="published" when="2007-03">March 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning with Kernels</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning with Kernels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">GMD Research Series</title>
		<imprint>
			<biblScope unit="issue">25</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universität Berlin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sparse greedy matrix approximation for machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="911" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">General cost functions for support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Ninth Australian Conf. on Neural Networks</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Downs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Frean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Gallagher</surname></persName>
		</editor>
		<meeting>of the Ninth Australian Conf. on Neural Networks<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="79" to="83" />
		</imprint>
		<respStmt>
			<orgName>University of Queensland</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The connection between regularization operators and support vector kernels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Regularization with dot-product kernels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Óvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="308" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Christmann</surname></persName>
		</author>
		<title level="m">Support Vector Machines. Information Science and Statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Max-margin Markov networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bundle methods for regularized risk minimization</title>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="311" to="365" />
			<date type="published" when="2010-01">January 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Improved analysis of the subsampled randomized hadamard transform</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<idno>CoRR, abs/1011.1595</idno>
		<ptr target="http://arxiv.org/abs/1011.1595" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Marginalized kernels for biological sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Suppl. 2):S268-S275</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Support vector method for function approximation, regression estimation, and signal processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 9</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="281" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spline Models for Observational Data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<date type="published" when="1990">1990</date>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Prediction with Gaussian processes: From linear regression to linear prediction and beyond</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<editor>M. I. Jordan</editor>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Academic</publisher>
			<biblScope unit="page" from="599" to="621" />
		</imprint>
	</monogr>
	<note>Learning and Inference in Graphical Models</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Using the Nystrom method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christoper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalization bounds for regularization networks and support vector machines via entropy numbers of compact operators</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2516" to="2532" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
