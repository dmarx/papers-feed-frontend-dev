{
  "arxivId": "2305.13169",
  "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data\n  Age, Domain Coverage, Quality, & Toxicity",
  "authors": "Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito",
  "abstract": "Pretraining is the preliminary and fundamental step in developing capable\nlanguage models (LM). Despite this, pretraining data design is critically\nunder-documented and often guided by empirically unsupported intuitions. To\naddress this, we pretrain 28 1.5B parameter decoder-only models, training on\ndata curated (1) at different times, (2) with varying toxicity and quality\nfilters, and (3) with different domain compositions. First, we quantify the\neffect of pretraining data age. A temporal shift between evaluation data and\npretraining data leads to performance degradation, which is not overcome by\nfinetuning. Second, we explore the effect of quality and toxicity filters,\nshowing a trade-off between performance on standard benchmarks and risk of\ntoxic generations. Our findings indicate there does not exist a\none-size-fits-all solution to filtering training data. We also find that the\neffects of different types of filtering are not predictable from text domain\ncharacteristics. Lastly, we empirically validate that the inclusion of\nheterogeneous data sources, like books and web, is broadly beneficial and\nwarrants greater prioritization. These findings constitute the largest set of\nexperiments to validate, quantify, and expose many undocumented intuitions\nabout text pretraining, which we hope will help support more informed\ndata-centric decisions in LM development.",
  "url": "https://arxiv.org/abs/2305.13169",
  "issue_number": 657,
  "issue_url": "https://github.com/dmarx/papers-feed/issues/657",
  "created_at": "2025-01-04T06:53:03.610654",
  "state": "open",
  "labels": [
    "paper",
    "rating:novote"
  ],
  "total_reading_time_seconds": 0,
  "last_read": null,
  "last_visited": "2024-12-30T20:20:50.556Z",
  "main_tex_file": null,
  "published_date": "2023-05-22T15:57:53Z",
  "arxiv_tags": [
    "cs.CL",
    "cs.LG"
  ]
}