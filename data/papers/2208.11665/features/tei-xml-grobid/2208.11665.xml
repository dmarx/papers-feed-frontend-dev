<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical exploration of the Manifold Hypothesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-12">February 12, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nick</forename><surname>Whiteley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Annie</forename><surname>Gray</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Rubin-Delanchy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematics</orgName>
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical exploration of the Manifold Hypothesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-12">February 12, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">542B176FA5AF7E58F00115EBCE6ADF34</idno>
					<idno type="arXiv">arXiv:2208.11665v4[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -the Latent Metric Model -via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The manifold hypothesis is a widely accepted tenet of Machine Learning which posits that <ref type="bibr" target="#b14">[16]</ref>: "...the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space".</p><p>This phenomenon has impacted a wide range of methods and algorithms. Presence of manifold structure is the premise of manifold estimation and testing <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b27">29]</ref>, nonlinear dimension reduction techniques <ref type="bibr" target="#b76">[78,</ref><ref type="bibr" target="#b87">89,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b96">98,</ref><ref type="bibr" target="#b93">95,</ref><ref type="bibr" target="#b57">59]</ref>, intrinsic dimension estimation <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b13">15]</ref>, and regression and classification techniques specially adapted to settings in which covariates are valued on manifolds <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b100">102,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b63">65]</ref>. Assumptions that data are concentrated near lowdimensional topological or geometric structures underpin clustering techniques and topological data analysis <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b64">66,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b15">17]</ref>. Some nonparametric techniques, such as nearest neighbour or tree-based regression methods, function without manifold structure necessarily being present, but benefit significantly when it is there, since their convergence rates depends on intrinsic rather than ambient dimension of covariates <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b45">47]</ref>. It has been proved that deep neural networks exhibit a similar property <ref type="bibr" target="#b62">[64]</ref>. More broadly, the presence of manifold structure has been suggested as a key factor in the success of deep learning methods <ref type="bibr" target="#b8">[10]</ref>. Assumptions that data lie on a low-dimensional manifold embedded in high-dimensional space are central to very recent practical and theoretical developments in generative modelling in Artificial Intelligence, especially diffusion models <ref type="bibr" target="#b80">[82,</ref><ref type="bibr" target="#b81">83,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b82">84,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b23">25]</ref>.</p><p>Why might manifold structure be present in data? In some situations, such as image analysis, an intuitive albeit heuristic explanation can be given in terms of the physical mechanism which generated the data (see e.g., Pless and Souvenir <ref type="bibr" target="#b73">[75]</ref> for a review of manifold estimation in this context). Figure <ref type="figure">1</ref> shows 24 grayscale images of a car, a subset of n = 75 images from <ref type="bibr" target="#b29">[31]</ref>, taken from angles 0, 5, 10, . . . , 355 degrees around the circumference of a circle. Each image is of resolution 384 × 288 pixels and so can be represented as a vector of length p = 110592. However, at least intuitively, we can account for the variation across the collection of images using far fewer dimensions, in terms of the position of the camera in the three-dimensional space of the world around us. Figure <ref type="figure">1</ref> shows the result of using principal component analysis (PCA) to reduce dimension, upon which we make the following observations. The first 20 principal components account for 91.5% of the total variance, suggesting that the data are concentrated somewhere in a low-dimensional linear subspace of R 110592 . The first three dimensions -the coordinates of the data with respect to the eigenvectors associated with the three largest eigenvalues -produce points around a loop which is somewhat irregular in shape but resembles the circle of camera positions, subject to deformation by bending and twisting. The points appear roughly equally spaced around the loop, like the camera positions which are equally spaced at intervals of 5 degrees around a circle.</p><p>Evidently reducing the dimension of these image data by PCA allows us to access some of the geometric structure of the data generating mechanism, but questions remain. We have chosen to plot the first three dimensions for ease of visualisation, is this a "good" choice? What might the other dimensions convey? What explains the precise shape of the loop and the spacing of the points along it, relative to the underlying circle of camera positions?</p><p>In other situations, embedded topological and geometric structure may appear in different forms and have different interpretations. Figure <ref type="figure" target="#fig_1">2</ref> shows two approaches to visualising expression levels of p = 5821 genes measured across n = 5000 individual cells from an adult planarians, a type of flatworm. In the field of single-cell transcriptomics -as set out in the 2018 Science paper <ref type="bibr" target="#b72">[74]</ref> -such data offer the possibility of discovering the cell lineage tree of an entire animal: the aim is to find out if the data reflect the tree-structured process by which stem cells differentiate into a variety of distinct cell types. These data were prepared using the Python package Scanpy <ref type="bibr" target="#b98">[100]</ref> following the methods of <ref type="bibr" target="#b72">[74]</ref>.</p><p>The left plot in figure <ref type="figure" target="#fig_1">2</ref> shows the result of dimension reduction from 5821 to 2 using PCA. The right plot shows the result of first reducing from 5821 to 14 dimensions using PCA, followed by reduction to 2 dimensions using t-SNE <ref type="bibr" target="#b93">[95]</ref>, a very popular nonlinear dimension reduction method which finds a lower dimensional representation of a data set by minimising a particular measure of distortion of pairwise distances. We used the default t-SNE parameter settings in scikit-learn. In both plots, the points are coloured by cell type, but neither PCA nor t-SNE have access to this information. Similarly to figure <ref type="figure">1</ref>, it is evident from figure <ref type="figure" target="#fig_1">2</ref> that performing some form of dimension reduction allows us to access structure underlying the data, albeit in the form of discrete cell types rather than the geometry of camera positions. In figure <ref type="figure">1</ref>, using only PCA to reduce dimension was enough to make this structure visible. However, in figure <ref type="figure" target="#fig_1">2</ref>, using only PCA and reducing to 2 dimensions, distinct cell types are not clearly separated, whereas PCA down to 14 dimensions followed by t-SNE seems to be more effective. The t-SNE visualisation hints at the presence of tree structure underlying the data, with some areas having branch-like arms originating at the central point cloud, but other lineages lack clarity or seem to be disconnected. Could we combine methods differently to obtain a clearer picture? These examples illustrate just some of the ways in which underlying structure can manifest itself in embedded topological and geometric patterns in data. Many other examples can be found: in genomics, where genotyping DNA sites has revealed striking geographic patterns <ref type="bibr" target="#b65">[67,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b21">23]</ref>; neuroscience, where simultaneous recordings from Grid cells have been shown to exhibit toroidal structure seemingly independent of behavioural tasks <ref type="bibr" target="#b26">[28]</ref>; as well as manifold structure in data from wireless sensor networks <ref type="bibr" target="#b66">[68]</ref>, visual speech recognition <ref type="bibr" target="#b9">[11]</ref>, drug discovery <ref type="bibr" target="#b75">[77]</ref>, RNA sequencing <ref type="bibr" target="#b60">[62]</ref>, and human motion synthesis <ref type="bibr" target="#b50">[52]</ref>.</p><p>In this work we put forward a perspective that embedded topological and geometric structure in data can be explained as a general statistical phenomenon, without reference to physical properties or other domain-specific details of the data generating mechanism.</p><p>Main contributions. Our first main contribution is to propose a simple and generic statistical model which produces hidden, low-dimensional manifold structure in high-dimensional data, thus providing a statistical justification for the manifold hypothesis.</p><p>Our second main contribution is to describe how this hidden manifold relates to a true latent domain defined by the model, explaining, for example, why the points in the right panel of figure <ref type="figure">1</ref> are not in a perfect circle, as the camera positions are, but still form a loop. More precisely, we give mild conditions under which the relationship between the manifold and the latent domain is a homeomorphism (a topological equivalence), and stronger conditions under which it becomes an isometry (a metric equivalence).</p><p>Our third main contribution is to show how to combine simple or well-known techniques, broadly relating to manifold-learning, to explore hypotheses and uncover information about the latent domain and broader data generating mechanism. Given data vectors Y 1 , . . . , Y n ∈ R p , we rationalise the following workflow: 2. Linear dimension reduction of Y 1 , . . . , Y n by PCA, resulting in an r-dimensional embedding, ζ 1 , . . . , ζ n .</p><p>3. Spherical projection of the embedding, setting ζ sp i := ζ i /∥ζ i ∥, i = 1, . . . , n 4. Nearest neighbour graph construction from ζ sp 1 , . . . , ζ sp n . 5. Analysis and visualisation of the nearest neighbour graph, e.g., shortest paths, minimum spanning tree, topology.</p><p>These recommended steps are supported by new theory and empirical evidence, providing a general-purpose mechanism for geometric data exploration under minimal assumptions. The remainder of this article is structured as follows. In Section 2 we introduce the Latent Metric Model, and the associated manifold M, which arises as a consequence of correlation over a latent domain Z. In section 3 we describe how this manifold structure hides in the data and how the manifold relates to Z. We establish a representation formula (proposition 1) uncovering the perhaps surprising fact that, under the Latent Metric Model, data are noisy, random projections of points in M. Standard statistical concepts, such as stationarity, give rise to striking geometric relationships between M and Z, such as isometry. In section 4 we develop theory and methodology supporting the workflow above, elucidating the benefits of applying PCA (theorem 1), proposing a new dimension selection method, and more. In Section 5 we demonstrate the workflow on real data, revisiting the image and transcriptomics data from section 1, as well as a temperature time series example. The key new feature of these analyses is that we can explore manifold hypotheses grounded in a statistical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Latent Metric Model</head><p>The Latent Metric Model (LMM) is constructed from three independent sources of randomness.</p><p>Latent Variables. Z 1 , . . . , Z n are independent and identically distributed random elements of a metric space (Z, d Z ), that is Z is a set, and d Z (•, •) is a distance function on Z. It is assumed that the metric space (Z, d Z ) is compact, and Z 1 , . . . , Z n are distributed according to a Borel probability measure µ supported on Z.</p><p>Random Functions. X 1 , . . . , X p are random R-valued functions, each with domain Z. That is, for each z ∈ Z and j = 1, . . . , p, X j (z) is an R-valued random variable. It is not assumed that X 1 , . . . , X p are identically distributed, but it is assumed that E[|X j (z)| 2 ] &lt; ∞, for all j = 1, . . . , p and z ∈ Z.</p><p>Noise. E ∈ R p×n is a matrix of random variables whose elements are each zero-mean and unitvariance. The columns of E are assumed independent and elements in distinct rows of E are assumed pairwise uncorrelated.</p><p>The data matrix Y ∈ R n×p is defined by:</p><formula xml:id="formula_0">Y ij := X j (Z i ) + σE ij (1)</formula><p>for some σ ≥ 0. It will sometimes be convenient to think of data vectors Y 1 , . . . ,</p><formula xml:id="formula_1">Y n ∈ R p such that [Y 1 | • • • |Y n ] ⊤ ≡ Y, so Y ij is the jth element of Y i . Similarly we shall write noise vectors [E 1 | • • • |E n ] ⊤ ≡ E.</formula><p>We call:</p><formula xml:id="formula_2">f (z, z ′ ) := 1 p p j=1 E[X j (z)X j (z ′ )]<label>(2)</label></formula><p>the mean correlation kernel associated with the LMM. The following assumption is taken to hold throughout the paper without further mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. For each</head><formula xml:id="formula_3">j = 1, . . . , p, E[X j (z)X j (z ′ )] is a continuous function of (z, z ′ ) ∈ Z × Z.</formula><p>Assumption A1 implies f (z, z ′ ) is continuous in z, z ′ , and by a generalisation of Mercer's theorem <ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b83">85,</ref><ref type="bibr">Thm 4.49]</ref> given in section A, when A1 holds there exists a countable collection of non-negative real numbers</p><formula xml:id="formula_4">(λ f k ) k≥1 , λ f 1 ≥ λ f 2 ≥ • • • , and a sequence of functions (u f k ) k≥1 which are orthonormal in L 2 (µ) such that f (z, z ′ ) = ∞ k=1 λ f k u f k (z)u f k (z ′ ) = ⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 ,<label>(3)</label></formula><p>where the series converges absolutely and uniformly in z, z ′ . Here ϕ is the "feature map":</p><formula xml:id="formula_5">ϕ(z) := (λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • ⊤ .<label>(4)</label></formula><p>We stress two points. First, the central purpose of the LMM is to explain and describe manifold structure in data as a general statistical phenomenon. The breadth of this objective necessitates a flexible modelling paradigm and, in particular, we do not make specific distributional or functional assumptions, such as Gaussian errors or that the kernel is squared exponential. The assumptions in this paper, which are always clearly indicated, involve more general concepts, such as continuity, smoothness or stationarity. Second, we stress the perspective here that f and ϕ are derived quantities, defined implicitly by the ingredients of the LMM, rather than model parameters or hyperparameters whose values need to be chosen. The inner product on the r.h.s. of ( <ref type="formula" target="#formula_4">3</ref>) is ⟨x,</p><formula xml:id="formula_6">x ′ ⟩ ℓ2 := ∞ k=1 x k x ′ k , between infinitely long vectors x = [x 1 x 2 • • • ] ⊤ belonging to ℓ 2 := {x ∈ R N : ∥x∥ ℓ2 &lt; ∞}, where ∥x∥ ℓ2 := ∞ k=1 |x k | 2 1/2 = ⟨x, x⟩ 1/2 ℓ2 . The image of ϕ, that is M := {ϕ(z); z ∈ Z} ,<label>(5)</label></formula><formula xml:id="formula_7">is a subset of ℓ 2 , because Z is compact and z → f (z, z) is continuous under A1, so sup z∈Z ∥ϕ(z)∥ 2 ℓ2 = sup z∈Z f (z, z) &lt; ∞.</formula><p>We denote by r the rank of f , that is the largest k ≥ 1 such that λ f k &gt; 0, with r := ∞ if λ f k &gt; 0 for all k ≥ 1. When r &lt; ∞ we abuse notation slightly by writing</p><formula xml:id="formula_8">ϕ(z) := (λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • (λ f r ) 1/2 u f r (z) ⊤ .<label>(6)</label></formula><p>Some special cases of the LMM are detailed in appendix B. It is shown there that in a setting where Z is a subset of Euclidean space and X 1 , . . . , X p are deterministic linear functions, the LMM reduces to the spiked-covariance model <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b67">69]</ref>, which is the de-facto standard model under which to study the theoretical properties of PCA in high dimensions. In the case that Z is a subset of Euclidean space and X 1 , . . . , X p are independent and identically distributed Gaussian processes, the LMM reduces to the Gaussian Process Latent Variable Model introduced by Lawrence <ref type="bibr" target="#b48">[50]</ref> and Lawrence and Hyvärinen <ref type="bibr" target="#b47">[49]</ref> to facilitate probabilistic nonlinear dimension reduction. Appendix A also examines the situation in which Z is a set containing finitely many points, in which case the LMM reduces to a form of finite mixture model.</p><p>The LMM deviates from the ubiquitous assumption in Statistics that data are independent and identically distributed; from the definitions above, the "noise-free" data vectors</p><formula xml:id="formula_9">Y i -σE i ≡ [X 1 (Z i ) • • • X p (Z i )] ⊤ , i = 1, . . . ,</formula><p>n, are exchangeable but not independent. We shall see that this dependence, combined with the latent structure of the LMM, are key to the emergence of manifold structure in high dimensions. Without such dependence, the behaviour of data can be explained by high dimension, low sample size (HDLSS) asymptotics, introduced in the seminal JRSSB paper of Hall et al. <ref type="bibr" target="#b32">[34]</ref>. HDLSS asymptotics are based on an argument that if Y i and Y j are i.i.d. random vectors whose elements satisfy suitable weak dependence and moment conditions, then p -1 ∥Y i -Y j ∥ 2 converges to a constant as p → ∞. In <ref type="bibr" target="#b32">[34]</ref> this leads to a conclusion that i.i.d. high-dimensional data vectors tend to lie deterministically at the vertices of a simplex. In section 3 we set out a different, much richer conclusion about the behaviour of p -1 ∥Y i -Y j ∥ 2 , arising from the LMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Connecting statistical and geometric properties of the LMM</head><p>In this section we explain how statistical properties of the LMM allow us to connect the geometry of the data vectors, Y 1 , . . . , Y n , which can be thought of as a point cloud in R p , to the structure of M, and in turn the latent metric space Z. This is important for two reasons. Firstly, it shows how manifold structure in data emerges from elementary statistical properties of the LMM, thus clarifying in what sense and why the Manifold Hypothesis holds. Secondly, it forms the basis for data analysis procedures we detail in section 4. We proceed in four main steps:</p><p>• Section 3.1 shows how inner-products between data vectors, say Y i , Y j , relate to inner products between ϕ(Z i ), ϕ(Z j ). Since ϕ(Z 1 ), . . . , ϕ(Z n ) are i.i.d. and valued in M, recall <ref type="bibr" target="#b3">(5)</ref>, this gives our first indication that the geometry of the point cloud Y 1 , . . . , Y n will reflect the shape of M.</p><p>• Section 3.2 shows that under a simple distinguishability assumption, the feature map ϕ is a homeomorphism. Informally, this means we can think of M as being equivalent to Z up to some continuous, invertible distortion such as bending, twisting or stretching. Formally, we can say M is a topological manifold.</p><p>• Section 3.3 shows that when Z is a subset of Euclidean space, conditions closely related to weak stationarity of the random function X j imply ϕ is an isometry. This means a very special form of geometric relationship holds between M and Z, in which distances between points in Z, say Z i and Z j , are faithfully represented by distances measured along the manifold M between ϕ(Z i ) and ϕ(Z j ), rather than by straight-line distances of the form ∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 .</p><p>• Section 3.4 shows that if the kernel is sufficiently smooth, most of the structure of M is captured in a low-dimensional subspace. This hints towards the potential effectiveness of PCA (step 2 in the workflow) for manifold exploration.</p><p>Remarkably, we shall draw the conclusions in the second and third points above without any explicit knowledge of the eigenvalues and eigenfunctions which appear in the definition of ϕ, and which thus define M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relating data inner products to feature map inner products</head><p>We have not made any assumptions about the functional form of z → X j (z), j = 1, . . . , p, in the LMM, other than A1. Nevertheless, the following proposition shows that a linear relationship holds between Y i -σE i and ϕ(Z i ).</p><p>Proposition 1. Under the LMM with r ∈ {1, 2, . . . , } ∪ {∞}, the matrix W ∈ R p×r with elements</p><formula xml:id="formula_10">W jk := 1 (pλ f k ) 1/2 Z X j (z)u f k (z)µ(dz)<label>(7)</label></formula><p>satisfies</p><formula xml:id="formula_11">Y i m.s. = p 1/2 Wϕ(Z i ) + σE i , i = 1, . . . , n, E[W ⊤ W] = I r ,<label>(8)</label></formula><p>where I r is the identity matrix with r rows and columns.</p><p>The qualification " m.s.</p><p>= " in <ref type="bibr" target="#b6">(8)</ref> indicates that the infinite summations constituting the matrix-vector product Wϕ(Z i ) in the case r = ∞ converge in the mean-square sense. The proof of proposition 1, in appendix C, entails a generalised form of Karhunen-Loève expansion of X 1 , . . . , X p .</p><p>The identities in (8) can be interpreted as meaning that p -1/2 Y i is a noisy, random projection of ϕ(Z i ). Indeed we can use <ref type="bibr" target="#b6">(8)</ref> together with the defining properties of the LMM in section 2 to describe the behaviour of the inner-product between Y i , Y j ∈ R p when the randomness in X 1 , . . . , X p and E is averaged out:</p><formula xml:id="formula_12">1 p E[⟨Y i , Y j ⟩|Z i , Z j ] = ⟨ϕ(Z i ), E[W ⊤ W]ϕ(Z j )⟩ ℓ2 + 0 + 0 + σ 2 1 p E[⟨E i , E j ⟩] = ⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 + σ 2 I[i = j].<label>(9)</label></formula><p>The quantity p -1 ⟨Y i , Y j ⟩ is an arithmetic mean of p random variables. If, conditionally on Z i , Z j , the summands in p -1 ⟨Y i , Y j ⟩ are weakly dependent and have moments bounded uniformly in p, then by a law of large numbers argument p -1 ⟨Y i , Y j ⟩ will be close to its conditional expectation <ref type="bibr" target="#b7">(9)</ref> with high probability when p is large (see proposition 6 in appendix C for details). Moreover, write W ≡ [W 1 | • • • |W p ] ⊤ and note from (7) that the only randomness in W j arises from X j . So if X 1 , . . . , X p were assumed weakly dependent, W 1 , . . . , W p would be too. Then, again assuming moments bounded uniformly in p, by a law of large numbers argument the sum p j=1 W j W ⊤ j will be close to its expectation with high probability when p is large, i.e.,</p><formula xml:id="formula_13">W ⊤ W = p j=1 W j W ⊤ j ≈ E[W ⊤ W] = I r .</formula><p>We therefore conclude that, subject to suitable weak dependence and moment conditions,</p><formula xml:id="formula_14">p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] → 0, as p → ∞,<label>(10)</label></formula><p>in probability. In this sense the geometry of the collection of high-dimensional data vectors Y 1 , . . . , Y n reflects that of ϕ(Z 1 ), . . . , ϕ(Z n ), subject to some distortion depending on the noise level σ. Moreover, if <ref type="bibr" target="#b8">(10)</ref> holds, then</p><formula xml:id="formula_15">|p -1 ∥Y i -Y j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ 2 ℓ2 -2σ 2 | → 0 as p → ∞.</formula><p>This stands in contrast to the behaviour established in <ref type="bibr" target="#b32">[34]</ref> that p -1 ∥Y i -Y j ∥ 2 → const. when Y i and Y j are independent and identically distributed.</p><p>In section 4.1, we shall complement the above reasoning with theorem 1 which shows that when the noise level σ is fixed, and n → ∞ and p/n → ∞ simultaneously, using PCA to reduce dimension of Y 1 , . . . , Y n allows ϕ(Z 1 ), . . . , ϕ(Z n ) to be recovered, up to an orthogonal transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relating distinguishability of latent variables to homeomorphism</head><p>A homeomorphism between two metric spaces is a mapping which is continuous, bijective and has a continuous inverse. If such a mapping exists the two metric spaces are said to be homeomorphic, or topologically equivalent. To develop some intuition, one can think about the case in which the metric spaces in question are subsets of the three dimensional Euclidean world around us. In this situation mappings which qualify as homeomorphisms include transformations of shape by bending, twisting, stretching and folding, but not cutting, puncturing or joining <ref type="bibr" target="#b7">[9]</ref>. Topological equivalence implies the two metric spaces in question must exhibit the same number of connected components, the same number of 1-dimensional loops and more generally the same number of kdimensional "holes" as each other. Detecting such features using data is the purpose of persistent homology methods within the field of Topological Data Analysis <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b15">17]</ref>. But there is more to a topological structure than its homology; for example, in the transcriptomics application (introduction and Section 5.2), the hypothesized underlying structure has interesting, 'tree-like', topology but no interesting homology.</p><p>We shall now see that, with only a little more structure added to the LMM, ϕ is homeomorphism between Z and M, where the distance on M is ∥ • -• ∥ ℓ2 . The first requirement, continuity of ϕ, means that d Z (z, z ′ ) → 0 implies ∥ϕ(z) -ϕ(z ′ )∥ ℓ2 → 0. This holds due to the identities:</p><formula xml:id="formula_16">∥ϕ(z) -ϕ(z ′ )∥ 2 ℓ2 = ∥ϕ(z)∥ 2 ℓ2 + ∥ϕ(z ′ )∥ 2 ℓ2 -2⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 = f (z, z) + f (z ′ , z ′ ) -2f (z, z ′ ),</formula><p>combined with continuity of f under A1. By its definition, ϕ : Z → M is automatically surjective, and if ϕ is one-to-one, its inverse is automatically continuous due to a general result in the theory of metric spaces <ref type="bibr" target="#b84">[86,</ref><ref type="bibr">Prop. 13.26]</ref> concerning the inverse of a continuous mapping with compact domain. The question of whether or not ϕ is a homeomorphism thus reduces to whether or not it is one-to-one. Consider the following assumption.</p><p>A2. For each z, z ′ ∈ Z such that z ̸ = z ′ , there exists ξ ∈ Z such that f (z, ξ) ̸ = f (z ′ , ξ).</p><p>Assumption A2 can be interpreted as a "distinguishability" condition, requiring that points in Z can be distinguished from each other via the kernel f , and furthermore: Proposition 2. ϕ : Z → M is a homeomorphism if and only if A2 holds.</p><p>The proof of proposition 2 is in appendix C. The term topological manifold conventionally means some set such that each point in that set has a neighbourhood which is homeomorphic to some subset of Euclidean space. We note that the relationship between M and Z is of a similar nature, except that M is globally rather than only locally homeomorphic to Z, and the metric space Z need not be Euclidean. Putting these differences aside, we shall simply call M a manifold from now on.</p><p>When M and Z are homeomorphic, they must have the same covering dimension-see <ref type="bibr" target="#b69">[71,</ref><ref type="bibr">Ch.3]</ref> for background-this is an abstract topological notion dimension, which generalises the usual notion of dimension of Euclidean space. In this sense, we can say that when Z is low-dimensional, M is low-dimensional too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relating stationarity to isometry</head><p>Weak stationarity of any one of the random functions X j in the LMM would mean that:</p><formula xml:id="formula_17">• E[X j (z)] is constant in z, and • E[(X j (z) -E[X j (z)])(X j (z ′ ) -E[X j (z ′ )])</formula><p>] is a function only of distance between z and z ′ . If all the random functions X 1 , . . . , X p were to have this property, it would follow from the definition of f in (2) that f (z, z ′ ) must also be a function only of distance between z and z ′ . We shall now see that this leads to an isometric relationship between M and Z. To define isometry, it's convenient to work in the following setting: A3. Z is a compact subset of R d , and there exists a continuous path in Z of finite length between any two points in Z.</p><p>The precise mathematical definition of a path and its length are given in appendix C.1. In the setting of A3, we denote by d geo Z (z, z ′ ) the shortest path length, or geodesic distance, in Z. This is the infimum of the lengths of all paths in Z with end-points z, z ′ (see appendix C.1 for details). If Z is convex, the shortest path between two points is a straight line and d geo Z (z, z ′ ) = ∥z-z ′ ∥ R d . For x, x ′ ∈ M, the shortest path length, or geodesic distance, in M is denoted d geo M (x, x ′ ) and defined analogously to d geo Z (z, z ′ ). Even when Z is convex, in general M is not convex and d geo M (x, x ′ ) is not equal to the straight-line distance ∥x -x ′ ∥ ℓ2 .</p><p>We shall say isometry holds between Z and M if</p><formula xml:id="formula_18">d geo M (ϕ(z), ϕ(z ′ )) = d geo Z (z, z ′ ), ∀z, z ′ ∈ Z.<label>(11)</label></formula><p>Compared to homeomorphism, this isometry condition imposes more of a constraint on the relationship between Z and M. One can interpret isometry as allowing ϕ to transform Z into M by bending, but not by stretching or compressing, since that would violate the equality of shortest path lengths.</p><p>The following proposition shows that isometry holds up to a scaling constant when, for z, z ′ close to each other, f (z, z ′ ) depends only on the Euclidean distance between z and z ′ . In contrast, weak-sense stationarity involves the more stringent requirement that such dependence holds for all z, z ′ . Define D := {(z, z); z ∈ Z} ⊂ Z × Z.</p><formula xml:id="formula_19">Proposition 3. Assume A2 and A3. If f (z, z ′ ) = g(∥z -z ′ ∥ 2 R d ) for all z, z</formula><p>′ in an open neighbourhood of D where g is twice continuously differentiable and g ′ (0) &lt; 0, then</p><formula xml:id="formula_20">d geo M (ϕ(z), ϕ(z ′ )) = -2g ′ (0)d geo Z (z, z ′ ).<label>(12)</label></formula><p>The following proposition complements proposition 3 by addressing the special case in which Z is a sphere.</p><formula xml:id="formula_21">Proposition 4. Assume A2. If Z = {z ∈ R d : ∥z∥ R d = 1} and f (z, z ′ ) = g(⟨z, z ′ ⟩ R d ) for all z, z ′</formula><p>in an open neighbourhood of D where g is twice continuously differentiable and g ′ (1) &gt; 0, then</p><formula xml:id="formula_22">d geo M (ϕ(z), ϕ(z ′ )) = g ′ (1)d geo Z (z, z ′ ).<label>(13)</label></formula><p>The proofs of propositions 3 and 4 are at the end of appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relating smoothness to concentration within a low-dimensional subspace</head><p>When the latent domain Z is a subset of R d , we will say that f is smooth if it can be expressed as the restriction of a smooth function on R d × R d to Z × Z. How smooth f is affects how much of the manifold M we can capture using only the first few coordinates. For some s &lt; r, consider the truncated map</p><formula xml:id="formula_23">ϕ s (z) := (λ f 1 ) 1/2 u f 1 (z) • • • (λ f s ) 1/2 u f s (z) 0 • • • ⊤ .</formula><p>The eigenvalues give us a measure of how well M s := ϕ s (Z) approximates M through the mean square error</p><formula xml:id="formula_24">E[∥ϕ(Z i ) -ϕ s (Z i )∥ 2 ℓ2 ] = k&gt;s λ f k E |u f k (Z i )| 2 = k&gt;s λ f k .</formula><p>The rate of decay of the eigenvalues is known to be related to the smoothness of the kernel <ref type="bibr" target="#b85">[87]</ref> and so, under smoothness assumptions, the first s coordinates of ϕ can provide a good approximation to M, even if r = ∞. Recalling the representation Y i m.s.</p><p>= p 1/2 Wϕ(Z i )+σE i from proposition 1, such smoothness also implies each vector Y i will be concentrated within the (at most) s-dimensional subspace of R p , when s ≤ p, spanned by the first s columns of W, since</p><formula xml:id="formula_25">E ∥Y i -p 1/2 Wϕ s (Z i )∥ 2 = E ∥p 1/2 Wϕ(Z i ) -p 1/2 Wϕ s (Z i ) + σE i ∥ 2 = pE[∥ϕ(Z i ) -ϕ s (Z i )∥ 2 ℓ2 ] + σ 2 E[∥E i ∥ 2 ] = p k&gt;s λ f k + pσ 2 ,</formula><p>where the independence of W, ϕ(Z i ), E i , the second equality in <ref type="bibr" target="#b6">(8)</ref> and the properties</p><formula xml:id="formula_26">E[E ij ] = 0, E[E 2 ij ] = 1 have been used.</formula><p>An LMM with smooth kernel can therefore produce a data matrix Y which is 'approximately low-rank', a common feature of real data <ref type="bibr" target="#b92">[94]</ref>, hinting that PCA may be a useful tool to help recover ϕ(Z 1 ), . . . , ϕ(Z n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">A visual example</head><p>To illustrate some of the concepts from section 3, we consider a case in which Z is a torus embedded in R 3 , satisfying A3 with d = 3. We take µ to be the uniform distribution on the torus, and Z 1 , . . . , Z 4000 simulated from µ are shown in figure <ref type="figure" target="#fig_2">3</ref>. The colouring of the points in this figure emphasises that the torus is the Cartesian product of two circles, and the locations on the torus can be parameterised in terms of angles around these two circles. We assume X 1 . . . , X p are i.i.d., zero-mean Gaussian processes with common covariance function exp(-∥z -z ′ ∥ 2 R 3 ) = f (z, z ′ ), which satisfies A1. Figure <ref type="figure">4</ref> shows numerical approximations to the first 1-3, 4-6 and 7-9 dimensions of ϕ(Z i ), i = 1, . . . , 4000 (these approximations were obtained Figure <ref type="figure">4</ref>: Torus example. Both the top and bottom rows show the first 9 dimensions of ϕ(Z i ), i = 1, . . . , 4000. In each row, points are coloured according to the coordinates of the underlying points Z 1 , . . . , Z n with respect to the two circles shown in figure <ref type="figure" target="#fig_2">3</ref>. Numerical scales are omitted to de-clutter the plots. using PCA, the details of which are given later in section 4.1). The only difference between the two rows of plots in figure <ref type="figure">4</ref> is the colouring of the points; the colouring in the top row is the colouring of the corresponding points in the middle plot in figure <ref type="figure" target="#fig_2">3</ref>, similarly the colouring in the bottom row matches that in the plot on the right of figure <ref type="figure" target="#fig_2">3</ref>. It is clear from figure <ref type="figure">4</ref> that the global shape of M, when viewed three dimensions at a time, is qualitatively different to the global shape of Z. However, assumption A2 holds, so by lemma 2, we know M is topologically equivalent to Z, and by proposition 3, ϕ isometry holds, up to a scaling factor of -g ′ (0) = √ 2 for the f in question. This tells us that shortest path lengths in M are equal to the corresponding path lengths Z, up to a factor of √ 2. Figure <ref type="figure" target="#fig_3">5</ref> shows comparison of these shortest path lengths, computed numerically from the points in figures 4 and 3 using a nearest neighbour graph as detailed in section 4.4. We see a close approximation to the theoretical scaling factor of √ 2, shown by the red line. Overall this example illustrates that if we are interested in discovering the topological or geometric structure of Z based on observations of M, we should not pay attention to the global shape of M that we perceive visually, because that depends on both Z and ϕ. However, when homeomorphism holds, we can in principle recover the abstract topological structure of Z and its homological features such as number of connected components, number of holes, etc., from M. Moreover, when isometry holds, at least up to a constant scaling factor, we can gain insight into the geometry of Z from shortest paths in M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, properties of the LMM are used to explain and justify the workflow outlined in section 1. Discussion of the step 1. is postponed until after discussion of step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear dimension reduction by PCA</head><p>Given data Y ∈ R n×p and s ≤ min{p, n}, let the columns of V Y ∈ R p×s be orthonormal eigenvectors associated with the s largest eigenvalues of Y ⊤ Y ∈ R p×p . The dimension-s PCA embedding is the collection of vectors ζ 1 , . . . , ζ n , defined by:</p><formula xml:id="formula_27">[ζ 1 | • • • |ζ n ] ⊤ := YV Y<label>(14)</label></formula><p>so for each</p><formula xml:id="formula_28">ζ i = V ⊤ Y Y i is is a vector in R s .</formula><p>These quantities are sometimes called principal component scores <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b78">80,</ref><ref type="bibr" target="#b35">37]</ref>. When performing PCA in practice, one often centers the data about their sample mean. For simplicity of presentation we do not consider such centering here, although we do not require population centering, that is we do not assume E[Y i ] = 0.</p><p>The following assumptions about the LMM are introduced to enable theoretical analysis of the PCA embedding.</p><p>A4. The random functions X 1 , X 2 , . . . are independent.</p><formula xml:id="formula_29">A5. sup j≥1 sup z∈Z E[|X j (z)| 4 ] &lt; ∞ and sup j≥1 sup i≥1 E[|E ij | 4 ] &lt; ∞.</formula><p>A6. For each p ≥ 1, the rank r of the mean correlation kernel f defined in (2) is finite, and r and 1/λ f r are bounded as p → ∞.</p><p>Theorem 1. Assume A4-A6 and let r be as therein. Let ζ 1 , . . . , ζ n be the dimension-r PCA embedding of Y ∈ R n×p under the LMM. Then there exists a random orthogonal matrix Q ∈R r×r depending on n and p such that</p><formula xml:id="formula_30">max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 ∈ O P 1 √ n + n p<label>(15)</label></formula><p>as n → ∞ and p/n → ∞ simultaneously, where ∥ • ∥ 2 is the Euclidean norm.</p><p>Theorem 1 is a corollary to a more detailed non-asymptotic concentration result for the PCA embedding, theorem 4, stated and proved in appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretation of theorem 1 and relation to the literature</head><p>Theorem 1 implies that for any ϵ &gt; 0, the probability that max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 &gt; ϵ converges to zero as n and p/n grow simultaneously. In that sense ϕ(Z 1 ), . . . , ϕ(Z n ) can be recovered from p -1/2 ζ 1 , . . . , p -1/2 ζ n , up to an orthogonal transformation, i.e., a transformation which preserves distances and inner-products. We see that computing the PCA embedding achieves a form of de-noising: each ζ i depends on all three sources of randomness in the LMM, but ϕ(Z i ) clearly depends only on the random latent variable Z i .</p><p>The result has positive implications for different forms of unsupervised learning, such as clustering, topological data analysis or manifold learning in the regime n, p/n → ∞. Viewed as sets, the point clouds {p -1/2 ζ i } i=1,...,n and {ϕ(Z i )} i=1,...,n converge to each other in Hausdorff distance, up to Q, implying convergence of topological summaries such as persistence diagrams <ref type="bibr" target="#b95">[97]</ref>, and so on. Broadly speaking, whatever we wish to estimate about M, if we can show the estimator would be consistent given ϕ(Z 1 ), . . . , ϕ(Z n ), there is a reasonable chance to show it is consistent given p -1/2 ζ 1 , . . . , p -1/2 ζ n . The LMM then gives us a way to translate such estimates into statements about the latent domain Z (see Section 3).</p><p>The behaviour of PCA and principal component scores in high-dimensions has been the subject of intensive theoretical study, e.g., <ref type="bibr" target="#b67">[69,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b101">103,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b102">104,</ref><ref type="bibr" target="#b78">80,</ref><ref type="bibr" target="#b79">81,</ref><ref type="bibr" target="#b35">37]</ref>. A central theme in these works is analysis of the eigenvectors of the sample "covariance" matrix n -1 Y ⊤ Y ∈ R p×p , which make up the columns of the matrix V Y appearing in <ref type="bibr" target="#b12">(14)</ref>. It is usually assumed that the data follow a spiked covariance model (a special case of the LMM in which Z is Euclidean, f is linear and X 1 , . . . , X p are deterministic -see section B for details), with consideration given to various scaling relationships involving p, n and other parameters. One of the key messages of <ref type="bibr" target="#b41">[43]</ref> is that the eigenvectors of the sample covariance matrix consistently estimate their population counterparts if and only if p/n → 0.</p><p>Prima facie, theorem 1 may therefore seem surprising, since it says the PCA embedding is consistent in a regime, p/n → ∞, where PCA was previously said to be inconsistent <ref type="bibr" target="#b41">[43]</ref>, for the reasons above. The explanation is that we can obtain a consistent embedding, YV Y , without requiring consistency of V Y . The proof of theorem 1 sets out in a crucially different direction: an elementary linear algebra argument (lemma 5 in section D) shows that</p><formula xml:id="formula_31">p -1/2 YV Y = U Y Λ 1/2 Y ,</formula><p>where the columns of U Y ∈ R n×r are orthonormal eigenvectors of p -1 YY ⊤ ∈ R n×n with associated eigenvalues on the diagonal of Λ Y . The n/p term in <ref type="bibr" target="#b13">(15)</ref> relates to the concentration behavior of the n×n matrix p -1 YY ⊤ about its conditional expectation: <ref type="formula" target="#formula_30">15</ref>) concerns approximations to certain integrals with respect to µ, based on the samples Z 1 , . . . , Z n , which arise when relating the rows of</p><formula xml:id="formula_32">E (p -1 YY ⊤ ) ij |Z i , Z j = p -1 E [⟨Y i , Y j ⟩|Z i , Z j ], c.f. (9). The 1/ √ n term in (</formula><formula xml:id="formula_33">U Y Λ 1/2 Y to ϕ(Z 1 ), . . . , ϕ(Z n ).</formula><p>The proof of theorem 4 relies heavily on matrix decomposition techniques used by Lyzinski et al. <ref type="bibr" target="#b56">[58]</ref> in the study of spectral embedding of random graphs under a random dot product model. The uniform nature of theorem 1 is directly inspired by the uniform consistency result of Lyzinski et al. <ref type="bibr" target="#b56">[58]</ref>[Thm. 15], which is an instance of convergence with respect to the 2 → ∞ matrix norm, studied in detail by Cape et al. <ref type="bibr" target="#b11">[13]</ref>. We note more generally that singular vector estimation under low-rank assumptions is an active area of research. As a recent example, Agterberg et al. <ref type="bibr" target="#b0">[2]</ref> obtained finite sample bounds and a Berry-Esseen type theorem for singular vectors under a model in which the signal is a deterministic low-rank matrix and heteroskedasticity and dependence is allowed in additive sub-Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of assumptions A4-A6</head><p>In the proof of theorem 4 and hence theorem 1, the independence assumption A4 and the moment assumption A9 are used when analysing p -1 YY ⊤ via matrix a polynomial moment concentration inequality from <ref type="bibr" target="#b68">[70]</ref>. The moment assumption A9 is not particularly restrictive. From a modelling point of view relaxing A4 to some form of weak dependence or mixing condition would be desirable, but the authors do not know of any suitable polynomial moment matrix concentration inequalities which are applicable in that situation.</p><p>Concerning assumption A6, that f has finite rank: recall from section 3.4 that the eigenvalues tend to tail off quickly when f is smooth, in which case assumption A6 might be taken to hold approximately. Here we will add that if f is polynomial <ref type="bibr" target="#b77">[79]</ref> or piecewise polynomial, or if Z has finitely many points, then f has strictly finite rank. As a result, assumption A6 is mild enough to include any function f which is obtainable from standard numerical or function approximation schemes (e.g. Taylor expansion, polynomial splines, etc).</p><p>The condition that r is bounded as p → ∞ in A6 can be understood as constraining the functional complexity of f as p grows. The condition that 1/λ f r is bounded as p → ∞ means that the additive noise whose scale is specified by the constant σ cannot overwhelm the "signal" in the LMM. If X 1 , X 2 , . . . are identically distributed then f , and hence r and λ f r , are automatically constant in p. However, the fact that theorem 1 involves choosing the dimension of the PCA embedding to be equal to r is less realistic, although a very common type of assumption in uniform consistency results for spectral embedding, e.g. <ref type="bibr" target="#b56">[58]</ref>. Truncated spectral embedding of graphs under a model with an infinite rank kernel was studied by <ref type="bibr" target="#b86">[88]</ref>, but their results concern a weaker, non-uniform measure of error than the one we consider here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Choosing the PCA dimension</head><p>Our model and theory motivate a new method for choosing the embedding dimension, r. Before proceeding, we should make clear that the hat notation in r is meant loosely: we seek a choice which achieves a good bias/variance trade-off in practice, and this may or may not coincide with the true rank of the kernel, r. Moreover, we do not claim that there is a 'best' choice: different tasks benefit from different choices. In particular, if using PCA for prediction purposes we simply recommend cross-validation, as is common practice. For more exploratory analyses, as conducted here, we propose the following approach instead.</p><p>Assuming n is even split the data Y into two,</p><formula xml:id="formula_34">Y (1) , Y (2) ∈ R n/2×p</formula><p>, and for each candidate dimension ρ, take the orthogonal projection of the rows of Y (1) onto the ρ principal eigenvectors of Y (1) ⊤ Y (1) -the resulting n/2 vectors are p-dimensional, just constrained to a ρ-dimensional subspace. Next, measure how much this projection step has brought the first half closer to the second, using Wasserstein distance. Select r to be the ρ achieving the smallest distance. The procedure is described precisely in algorithm 1.</p><p>To understand how r might relate to r, let us make a few simplifying assumptions. Suppose r &lt; ∞, so that (with exact equality)</p><formula xml:id="formula_35">Y i = p 1/2 Wϕ(Z i ) + σE i ,</formula><p>and that the second Wasserstein distance is used, that is</p><formula xml:id="formula_36">d ρ = W 2 (Y (1) Π ρ , Y (2) ) where W 2 2 (A, B) := min π 1 m ∥A i -B π(i) ∥ 2 2 , A, B ∈ R m×d ,</formula><p>where A i and B i are the ith rows of A and B, and where the minimum is over all permutations of the integers 1, . . . , m.</p><p>The second Wasserstein distance is particularly amenable to mathematical analysis because of the following property, which can be checked by direct calculation. If there exist Â1 , . . . , Âm such that for all i, j we have ⟨A i -Âi , B j ⟩ = 0, then</p><formula xml:id="formula_37">W 2 2 (A, B) = 1 m ∥A -Â∥ 2 F + W 2 2 ( Â, B).<label>(16)</label></formula><p>To see why algorithm 1 might reject overly values of ρ, suppose ρ &gt; r and consider the projection errors</p><formula xml:id="formula_38">E (1) = Y (1) (Π ρ -Π r ). With E (1) i</formula><p>denoting the ith row of E (1) and the superscript (k) indicating random objects associated with Y (k) , suppose</p><formula xml:id="formula_39">1 p ⟨ E (1) i , E<label>(2)</label></formula><p>j ⟩ ≈ 0, and</p><formula xml:id="formula_40">1 p ⟨ E (1) i , √ pWϕ(Z (2) j )⟩ ≈ 0, i, j = 1, . . . , n/2. (<label>17</label></formula><formula xml:id="formula_41">)</formula><p>Then, using <ref type="bibr" target="#b14">(16)</ref>,</p><formula xml:id="formula_42">1 p d 2 ρ ≈ 2 np ∥E (1) ∥ 2 F + 1 p d 2 r ,</formula><p>where ∥E (1) ∥ 2 F is non-decreasing in ρ -r, and it follows that we should expect d ρ &gt; d r when ρ is large relative to r.</p><p>Why should the approximations in <ref type="bibr" target="#b15">(17)</ref> hold? The first is the product of sample-splitting. If the E i are statistically independent, then E (2) is statistically independent of V (1) ρ . Combined with the fact that the elements of E</p><p>(2) i are mean-zero and unit-variance, we therefore expect the p -1 E (2) i to be approximately orthogonal to the subspace spanned by the columns of V</p><p>(1) ρ , when p is large relative to n. The second approximation seems to be reasonable as long as n and p are large, and we have confirmed this by simulation. Now consider the case ρ &lt; r. We have</p><formula xml:id="formula_43">d 2 ρ = W 2 2 (Y (1) Π ρ , Y (2) ) = 2 n ∥Y (2) (Π ρ -I p )∥ 2 F + W 2 2 (Y (1) Π ρ , Y (2) Π ρ ).</formula><p>By the Eckart-Young theorem,</p><formula xml:id="formula_44">∥Y (2) (Π ρ -I p )∥ 2 F ≥ k&gt;ρ λ k (Y (2)⊤ Y (2) )<label>(18)</label></formula><p>where λ k (Y (2)⊤ Y (2) ) is the kth largest eigenvalue of Y (2)⊤ Y (2) . The r.h.s. of ( <ref type="formula" target="#formula_44">18</ref>) is nondecreasing in r -ρ. To see that the term W 2 2 (Y (1) Π ρ , Y (2) Π ρ ) converges to zero as p/n, n → ∞, it is convenient to consider the case σ = 0. In this situation,</p><formula xml:id="formula_45">1 p W 2 2 (Y (1) Π ρ , Y (2) Π ρ ) ≤ 1 p W 2 2 (Y (1) , Y (2) ) = 1 p W 2 2 (Φ (1) W ⊤ , Φ (2) W ⊤ )</formula><p>where</p><formula xml:id="formula_46">Φ (k) = [ϕ(Z (k) 1 )| • • • |ϕ(Z (k) n/2 )] ⊤ ∈ R n/2×r</formula><p>. Appealing to the same arguments as in section 3.1, as p/n → ∞,</p><formula xml:id="formula_47">1 p W 2 2 (Φ (1) W ⊤ , Φ (2) W ⊤ ) is concentrated about W 2 2 (Φ (1) , Φ (2)</formula><p>), and the latter converges to zero as n → ∞ because the rows of Φ (1) and Φ (2) are i.i.d. random vectors in R r . It follows that we should expect d ρ &gt; d r when ρ is small relative to r, and so overall that d ρ will have a minimum near ρ = r.</p><p>A general rule we could draw from these arguments, and which we see in practice, is that to recommend substantial dimension reduction the algorithm wants to see a large p relative to n, and noise. Conversely, if the noise level is low or if n is large relative to p, then d ρ may keep decreasing with ρ, which we tend to interpret as contraindication against PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method comparison</head><p>We now explore the performance of Algorithm 1 in a few simulated examples. We consider four configurations, where each configuration refers to a choice of latent space Z and corresponding kernel f . In the first configuration, the latent space comprises six distinct elements. The latent spaces in the remaining configurations are different subsets of R 2 . In each configuration, we draw n = 500 points Z i uniformly on Z, and the resulting point sets are shown in figures 6a)1-4.</p><p>In the first configuration, we choose an arbitrary 6 × 6 positive-definite matrix to represent the kernel. In the second, f (x, y) = (x ⊤ y + 1) 2 , which has rank 6; in the third, f (x, y) = {cos(x (1)y (1) ) + cos(x (2) -y (2) ) + 2}, which has rank 5; and in the fourth, f (x, y) = exp(-∥x -y∥ 2 R 2 /2), which has infinite rank.</p><p>We simulate a 500 × 1000 data matrix Y in each configuration, where the p = 1000 random functions are independent, zero mean Gaussian processes with the same covariance kernel f , and the errors E ij are independent and standard normal.</p><p>In figures 6c)1-4 we show the Wasserstein error (log-scale), i.e., the distance computed in Algorithm 1, for different choices of dimension. Reassuringly, the optimum roughly coincides with the true rank of the kernel when finite (dashed black line, configurations 1-3) and at the same time it is interesting that a non-degenerate optimum is still found under infinite rank (configuration 4). If we lower the noise, the optimal dimension increases (figure <ref type="figure" target="#fig_17">17</ref>, Appendix), reflecting the afore-mentioned bias/variance trade-off.</p><p>For comparison, we also show the dimensions selected using the ladle <ref type="bibr" target="#b55">[57]</ref> and elbow methods <ref type="bibr" target="#b104">[106]</ref>, as implemented in the R packages 'dimension' (on github: <ref type="url" target="https://github.com/WenlanzZ">https://github.com/WenlanzZ</ref>) and 'igraph' (on The Comprehensive R Archive Network), respectively. The ladle and Wasserstein methods seem to make similar choices, although as implemented the ladle method is computationally costly, which has precluded more simulations or going beyond max(n, p) = 1000 to allow a more comprehensive comparison. We would advise against the elbow method for dimension selection under the LMM, as it appears to favour dangerously low dimensions.</p><p>In configurations 3 and 4, there is isometry between M and Z. As a result, we can aim to recover the path lengths in Z amongst Z 1 , . . . , Z n from p -1/2 ζ 1 , . . . , p -1/2 ζ n -see section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Project Y (1) onto the linear span of the columns of</p><formula xml:id="formula_48">V (1) ρ , that is compute: Y (1) Π ρ where Π ρ := V (1) ρ V (1)⊤ ρ . 5:</formula><p>Compute Wasserstein distance d ρ between Y (1) Π ρ and Y (2) as point sets in R p . 6: end for Output: selected dimension r = argmin {d ρ }.</p><p>details. This method yields infinite distances when the k-nearest neighbour graph isn't connected. Dealing with this issue in a systematic way is awkward, and we settled on the following solution. Picking ϵ as the 5% quantile of the R 2 Euclidean distance matrix between the Z i , we place an edge between any pair of points within distance ϵ, weighted by Euclidean distance, and approximate the geodesic distance between two points as the corresponding weighted graph distance. Any infinite distance remaining is replaced with the original Euclidean distance. The blue line in figures 6d)3-4 shows the entrywise mean square error between the estimated geodesic distance matrices of Z 1 , . . . , Z n and p -1/2 ζ 1 , . . . , p -1/2 ζ n , for different choices of r. The optimum roughly coincides with the dimensions selected by the ladle and Wasserstein methods.</p><p>Because of the isometric relationship between M and Z in configurations 3 and 4, we might also hope that the persistence diagrams of their Rips filtrations would be similar. The red line in figures 6d)3-4 shows the bottleneck distance between the persistence diagrams of the Rips filtrations of Z 1 , . . . , Z n and p -1/2 ζ 1 , . . . , p -1/2 ζ n , as implemented in the R package 'TDA', for different choices of r. In this metric, the optimal dimension (lowest bottleneck distance) is lower than that suggested by the ladle and Wasserstein methods, but we do not know to what extent this should be expected in general. The scales of the log-Wasserstein error, geodesic distance error, and bottleneck distance are not comparable and in figures 6d) we have recentered and rescaled the curves to make their maxima and minima agree.</p><p>In figures 6e)1-4 we show the persistence diagrams of the Rips filtrations of p -1/2 ζ 1 , . . . , p -1/2 ζ n computed on the basis of the dimension selected by the Wasserstein method (Algorithm 1), using the R package 'TDA'. Recall that in persistent homology the significance of a topological feature is quantified by its persistence, death minus birth, which is the vertical distance between the point (birth,death) to the diagonal x = y. Following Fasy et al. <ref type="bibr" target="#b24">[26]</ref> we draw a line parallel to x = y to separate the signal from the noise, picking y = x+0.2 by eye. In each figure, we report the number of connected components, β0 , and holes, β1 , estimated by this heuristic. The true corresponding values for Z are respectively (6,0), (1,8), (1,0), and (1,1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Spherical projection</head><p>When performing data analysis we may wish to consider the assumption that f belongs to one of the families of kernels in proposition 3 or 4, because of their stationarity interpretation, and because the associated isometry properties would justify use of the PCA embedding to recover geometric features of Z. However, all these kernels have the property that</p><formula xml:id="formula_49">p -1 p j=1 E[|X j (z)| 2 ] = f (z, z) = const.,<label>(19)</label></formula><p>which from a modelling point of view may be restrictive. We shall now show that the spherically projected PCA embedding has a model-based interpretation which allows this restriction to be loosened. Suppose we are given random functions X 1 , . . . , X p such that f (z, z) is constant in z ∈ Z. Without loss of generality assume this constant is 1. As usual let ϕ be the feature map associated  with f . Note that in this situation M is a subset of the unit hypersphere {x ∈ ℓ 2 : ∥x∥ ℓ2 = 1}. With α 1 , . . . , α n being i.i.d. random variables whose distribution is supported on a compact set A ⊂ R + , define the model:</p><formula xml:id="formula_50">H 0 H 1 0 2 4 Birth 0 1 2 3 4 Death 0 = 1 1 = 0 H 0 H 1 0 2 4 Birth Death 0 = 1 1 = 1 H 0 H 1</formula><formula xml:id="formula_51">Y ij = α i X j (Z i ) + σE ij .<label>(20)</label></formula><p>This can be viewed as a particular form of LMM with extended latent space Z ext := A × Z and extended random functions X ext j (α, z) := αX j (z). Its mean correlation kernel is:</p><formula xml:id="formula_52">f ext (α, z, α ′ , z ′ ) := 1 p p j=1 E X ext j (α, z)X ext j (α ′ , z ′ ) = α⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 α ′ ,</formula><p>the Mercer feature map of f ext satisfies: ϕ ext (α, z) = αϕ(z), and we have p</p><formula xml:id="formula_53">-1 p j=1 E[|X ext j (α, z)| 2 ] = α 2</formula><p>, allowing more flexibility than <ref type="bibr" target="#b17">(19)</ref>. Now suppose assumption A6 holds, let ζ 1 , . . . , ζ n be the dimension-r PCA embedding computed from Y under the extended LMM <ref type="bibr" target="#b18">(20)</ref> and consider the spherical projection</p><formula xml:id="formula_54">ζ sp i := ζ i /∥ζ i ∥ 2 . Using the identities ∥ϕ ext (α, z)∥ 2 = α∥ϕ(z)∥ 2 = αf (z, z) 1/2 =</formula><p>α, and applying the triangle inequality several times gives:</p><formula xml:id="formula_55">∥Qζ sp i -ϕ(Z i )∥ 2 = p -1/2 Qζ i p -1/2 ∥ζ i ∥ 2 - ϕ ext (α i , Z i ) ∥ϕ ext (α i , Z i )∥ 2 2 ≤ 2 ∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 α i -∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 ,</formula><p>where Q is any orthogonal matrix. Theorem 1 could therefore be applied to the LMM (20) to establish that for the particular Q in that theorem, ∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 → 0, which by the above inequality implies ∥Qζ sp i -ϕ(Z i )∥ 2 → 0. In summary, under the model <ref type="bibr" target="#b18">(20)</ref>, ϕ(Z 1 ), . . . , ϕ(Z n ) can be recovered from the spherically projected embedding ζ sp 1 , . . . , ζ sp n , up to an orthogonal transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Nearest neighbour graph construction</head><p>Constructing a nearest neighbour graph from the PCA embedding allows us to approximate topological and geometric features of M and hence Z. In keeping with the workflow set out in section 1, we focus on the spherically projected embedding ζ sp 1 , .</p><p>. . , ζ sp n but very similar considerations apply to the raw embedding ζ 1 , . . . , ζ n . Noting that ∥ζ sp i ∥ 2 = 1 for all i, we denote by d S (ζ sp i , ζ sp j ) := arccos(⟨ζ sp i , ζ sp j ⟩ 2 ) the circular arc distance on the unit hypersphere. There are two popular types of nearest neighbour graph: the ϵ-nn and k-nn graphs, both of which are undirected, weighted graphs with n vertices, identified with ζ sp 1 , . . . , ζ sp n . There is an edge between ζ sp i and ζ sp j in the ϵ-nn graph if d S (ζ sp i , ζ sp j ) ≤ ϵ, and in the k-nn graph if ζ sp i is one of the k-nearest (with respect to d S ) neighbours of ζ sp j or vice versa. In both types of graph, if there is an edge between ζ sp i and ζ sp j it is assigned weight d S (ζ sp i , ζ sp j ). A number of algorithms for identifying nearest neighbours exactly or approximately are available, for example in the Python library scikit-learn [72].</p><p>Recalling <ref type="bibr" target="#b27">(29)</ref>, nearest neighbour graph distances can be used to approximate shortest path lengths in M:</p><formula xml:id="formula_56">d geo M (ϕ(Z i ), ϕ(Z j )) ≈ D ij M := min x1,...,xm {d S (x 1 , x 2 ) + • • • + d S (x m-1 , x m )} ,</formula><p>where the minimum is over all paths in the nearest neighbour graph connecting x 1 = ζ sp i and x m = ζ sp j . If there are no such paths, D ij M = ∞ by convention. Various fast algorithms for computing shortest paths and shortest path lengths are available, for example in the Python library NetworkX <ref type="bibr" target="#b31">[33]</ref>.</p><p>The use of nearest neighbour graphs to approximate path lengths on manifolds is well studied and is the first step in the Isomap procedure <ref type="bibr" target="#b87">[89]</ref>. The theoretical accuracy of such approximations has been analysed by <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b91">93]</ref>. In particular <ref type="bibr" target="#b91">[93]</ref> note that the k-nn graph is often preferred in practice although its analysis is more complicated. They also note that choosing a single value for ϵ or k is a difficult problem in general. Where possible in the examples of section 5 we compute and analyse the nearest neighbour graph over a range of values for ϵ or k, rather than selecting one single value. This approach is similar in spirit to the computation of ϵ-nn graphs over a range of ϵ values in persistent homology techniques <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b15">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Examples</head><p>In the following three real data examples, we will assume the LMM holds and explore hypotheses about the latent domain, Z, feature map, ϕ, and the manifold underlying the data, M = ϕ(Z).</p><p>In all examples, we have access to background information. In some cases, such as the first hypothesis in the image and transcriptomics examples, the background points us towards generic hypotheses, such as 'M is a loop' or 'M is a tree'. In others, e.g. the second hypothesis in the image and transcriptomics examples, the information is used more explicitly to obtain trial values for Z, realisations z i of Z i , and to estimate parts of the model. Although we provide baseline comparisons against uniform models, the workflow we present is designed for exploratory rather than confirmatory analysis and should not be viewed as formal hypothesis testing.</p><p>The code and data used are available here: <ref type="url" target="https://github.com/anniegray52/explore_manifold_hyp">https://github.com/anniegray52/explore_ manifold_hyp</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Images</head><p>We return to the data set of images described in section 1. Recall there are n = 72 images, each consisting of p = 110592 grey-scale pixels, taken from angles 0, 5, 10, . . . , 355 degrees around the circumference of a circle. We will assume XY coordinates for the camera positions, cos(θ i ), sin(θ i ) for each angle θ i (converted to radians). In this context, we will first consider the hypothesis:</p><p>1. Z is a circle and ϕ is a homeomorphism. An informal implication is: The data lie close to a loop.</p><p>Finding the data consistent with the above, we will consider the stronger hypothesis:</p><p>2. Z is the circle of camera positions, z i = (cos(θ i ), sin(θ i )) are the (known) camera positions, and ϕ is a scaled isometry. An informal implication is: Distances along the loop correspond to distances along the circle between camera positions.</p><p>The first step of the workflow is to apply the dimension selection method. As per figure <ref type="figure" target="#fig_17">7a</ref>), this results in r = 11. The kernel density estimate in 7b) demonstrates the variation in the magnitudes of the dimension-r PCA embedding vectors ∥ζ 1 ∥ 2 , . . . , ∥ζ n ∥ 2 ; in all subsequent steps we instead work with the spherically projected embedding ζ sp 1 , . . . , ζ sp n as per (4.3). We now consider the first hypothesis, which would be mathematically justified by assumption A2, with Z (say) the unit circle on R 2 (d Z the Euclidean metric). Then indeed ϕ would be a homeomorphism and M would be topologically equivalent to a circle. Conveniently in this example, the presence of loops or holes in data point-clouds can be assessed using persistent homology techniques <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b15">17]</ref>. Figure <ref type="figure" target="#fig_17">7c</ref>) shows a persistence diagram computed from the spherically projected PCA embedding using the Python package Ripser.py <ref type="bibr" target="#b89">[91]</ref>. The blue dot on the horizontal dashed line is indicative of a single connected component with persists over a large range of length scales. The isolated single orange dot close to the horizontal dashed line is indicative of a single "loop" in the embedding, also persisting over a large range of length scales. This is consistent with the hypothesis.</p><p>We now consider the second hypothesis, which by proposition 4 would be mathematically justified by the mean correlation kernel being of the form f (z, z ′ ) = g(⟨z, z ′ ⟩) in some neighbourhood of z = z ′ (or equivalently ⟨z, z ′ ⟩ = 1), and g ′ (1) &gt; 0 (recall proposition 4). On the hypothesis assumption that z i = (cos(θ i ), sin(θ i )), figure 7d) shows f (z i , z j ) := ⟨ζ sp i , ζ sp j ⟩, which we regard as an estimator of f (z i , z j ), plotted as a function of θ i and θ j . The fairly constant width of the pronounced yellow/white diagonal stripe in this plot admits the interpretation that indeed f (z, z ′ ) = g(⟨z, z ′ ⟩) in a neighbourhood of z = z ′ . To examine this in more detail, figure <ref type="figure" target="#fig_17">7e</ref>) plots values of f (z i , z j ) against ⟨z i , z j ⟩ over all i, j = 1, . . . , n. The red dashed ellipse highlights that (1) i ). e) Estimated kernel as a function of latent inner product ⟨z i , z j ⟩, the red dashed ellipse highlights f (z i , z j ) in the region ⟨z i , z j ⟩ ≈ 1. f) Evidence of a linear relationship between shortest path lengths computed from the nearest neighbour graph G (y-axis), and from the latent positions (x-axis).</p><p>f (z i , z j ) is approximately an increasing function of ⟨z i , z j ⟩ in a neighbourhood of z i = z j , which is consistent with g ′ (1) &gt; 0. Informed by <ref type="bibr" target="#b11">(13)</ref> we thus anticipate there is isometry between M and Z, up a scaling factor of g ′ (1) 1/2 . To see if the data are consistent with such a relationship we compute the k-nn graph G as per section 4.4 with k = 2. This is the natural choice for k if M is topologically equivalent to a circle. Figure <ref type="figure" target="#fig_17">7f</ref>) shows shortest path lengths D ij M in G plotted against shortest path lengths around the circle, denoted D ij Z , over i, j = 1, . . . , n. The clear linear relationship is consistent with there being little deviation from isometry, up to a scaling factor, which by a straight line fit in figure <ref type="figure" target="#fig_17">7f</ref>) we can estimate: g ′ (1) ≈ 3.18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Single-cell transcriptomics</head><p>We now revisit the planarian single-cell transcriptomics example introduced in section 1. Recall that here we have p = 5821 dimensional gene expression data in n = 5000 cells from adult planarians, and we also know cell-type labels for each of these cells, indicated by the different colours in figure <ref type="figure" target="#fig_1">2</ref>. Adult planarians have a large number of pluripotent stem cells, known as neoblasts, that continuously differentiate into all adult cell types, resulting in a lineage tree that connects all the cells in the whole animal. We represent this lineage by a continuous tree (formally defined later) and suppose the cell types are named positions, c i , on this tree.</p><p>In this context, we will first consider the hypothesis:</p><p>1. Z is a continuous tree and ϕ is a homeomorphism. An informal implication is: The data lie close to a tree.</p><p>Finding the data consistent with the above, we will consider the stronger hypothesis:</p><p>2. Z is the lineage tree, z i = c i are the (known) cell types, and ϕ is a homeomorphism. Informally: the tree represents the lineage of the cell types.</p><p>In graph theory, a tree is an undirected graph in which any two vertices are connected by a unique path. We consider an analogue of this concept which reflects the continuous nature of cell differentiation. Inspired by definitions in <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b39">41]</ref>, we say a metric space Z is a continuous tree if for any z, z ′ ∈ Z there exists a homeomorphism ψ between [0, 1] and some subset of Z such that ψ(0) = z and ψ(1) = z ′ (this means a continuous path in Z exists between z and z ′ ), and all such homeomorphisms have the same image (this means the path is unique).</p><p>Following the workflow in section 1, dimension selection results in r = 14. We then calculate the dimension-r embedding and its spherical projection ζ sp 1 , . . . , ζ sp n . For the remainder of this section we refer to the latter as the PCA embedding.</p><p>We now consider the first hypothesis, which would be mathematically justified by assumption A2. Then M equipped with the ℓ 2 distance also qualifies as a continuous tree, as the composition of two homeomorphisms is a homeomorphism. To gain some preliminary insight into the structure of the PCA embedding, figure <ref type="figure" target="#fig_8">8a</ref>) shows, in red, a histogram of inner products between all distinct pairs of embedding points ζ sp i , ζ sp j . As a baseline comparison, we generated a random embedding consisting of the same number n = 5000 points uniformly distributed on the r-dimensional, unitradius hypershere. Figure <ref type="figure" target="#fig_8">8a</ref>) shows, in black, a histogram of inner-products between all distinct pairs of points in this random embedding. We see this black histogram is symmetrical and concentrated around 0. By contrast, the red histogram is not symmetrical and exhibits two peaks. The peak near an inner product value of 1 indicates a substantial proportion of pairs of points ζ sp i , ζ sp j which are much closer together than is observed in the random embedding. Many other pairs ζ sp i , ζ sp j have inner products between -0.5 and 0, indicating they are spread out on the hypersphere, but not in the same way that uniformly random points are spread out. On the basis of this preliminary check we see no reason to rule out tree structure in the PCA embedding. We now quantify how "tree-like" the PCA embedding is in two steps. First we compute the k-nn graph of the PCA embedding as per section 4.4, and its minimum spanning tree. The latter is obtained by removing edges from the k-nn graph until a tree is formed, in such a way that the total edge length of the tree is minimal. Various fast algorithms for computing minimum spanning trees are available, we used the Python library NetworkX <ref type="bibr" target="#b31">[33]</ref>. The second step is to compare shortest path lengths in the k-nn graph to those in the minimum spanning tree. The shortest path length between any pair of vertices in the minimum spanning tree can only be greater than or equal to the shortest path length between those vertices in the k-nn graph. The percentage increase in shortest path length, when averaged over all pairs of vertices, serves as a univariate statistic which quantifies how tree-like the k-nn graph is. If the k-nn graph were a tree, this statistic would be exactly zero.</p><p>Figure <ref type="figure" target="#fig_8">8b</ref>) shows the average percentage increase in shortest path length, as a function of k. The red line shows the results for the PCA embedding. The black line and error bars show the same quantity computed from repeated simulations of the random embedding, serving as a baseline for comparison. We see that across all values of k, the average percentage increase in shortest path length is much lower for the PCA embedding than for the random embedding. This indicates that the minimum spanning tree is a close approximation to the k-nn graph of the PCA embedding. To take a finer-grained look, figure <ref type="figure" target="#fig_8">8b</ref>) shows shortest path lengths in the minimum spanning tree, versus in the k-nn graph with k = 10, for all pairs of vertices. The blue "y = x" line indicates the lower bound on path length increase which would be achieved if the k-nn graph were a tree. Overall, the findings in figure <ref type="figure" target="#fig_8">8</ref> are consistent with the PCA embedding of the planaria data being tree-like.</p><p>We now consider the second hypothesis. The left plot in figure <ref type="figure" target="#fig_9">9</ref> shows a visualisation of the minimum spanning tree derived from the k-nn graph with k = 10, with vertices coloured by the known cell type labels. This visualisation was obtained using the Scaleable Force Directed Placement graph layout algorithm <ref type="bibr" target="#b38">[40]</ref>. From the colouring by cell type, we see that biologically similar types, such as the three types of muscle cell, appear in localised branches of the tree. We next construct a "class graph" which captures the relationships between cell types implied by the minimum spanning tree in figure <ref type="figure" target="#fig_9">9a</ref>). In this class graph, each vertex corresponds to a cell type, and the undirected edge weight between any two vertices in the class graph is defined to be the total number of edges in the minimum spanning tree between cells of those two types.</p><p>The class graph is shown in figure <ref type="figure" target="#fig_9">9b</ref>). The size of each node represents the total number of cells of that type. The thickness of the edges reflects their weights in the class graph, although for visual clarity we do not draw some edges with very low weights. Figure <ref type="figure" target="#fig_9">9</ref> elucidates cell development, tracing the lineage from stem cells to progenitors and differentiated cell populations: neurons, muscle cells, protonephridia, epidermis, and secretory cells.</p><p>The original paper <ref type="bibr" target="#b72">[74]</ref> provides a consolidated tree, which amalgamates various evidence types. The overall structure aligns with our nearest neighbour approach, with branches for individual known cell types, however, discrepancies exist in the form of minor variations in the differentiated cell populations. For example, cav-1+ neurons connect to ChAT neurons 1 rather than neural progenitors. Additionally, a more interconnected secretory cell network is found in the class graph. In the consolidated tree, two connections are added based on marker gene analysis (genes known to be present in specific cell types): muscle pharynx to muscle body and from epidermis to the epidermal lineage. In contrast, the nearest-neighbour approach employed here identifies these connections. Acknowledging these differences, we refrain from delving further into minor disparities, given the current paper's intended scope.</p><p>For visual clarity, we draw a single node grouping together all the neoblast 1-13 cell types. The subgraph of the class graph corresponding to these neoblast types is shown in the inset of figure <ref type="figure" target="#fig_9">9b</ref>), revealing a large number of neoblast 1 cells, linked by edges to most other neoblast cell types. This aligns with the results of the original authors, but contrasts with previous studies <ref type="bibr" target="#b94">[96]</ref>, <ref type="bibr" target="#b59">[61]</ref>, which suggested distinct fates for various neoblast types. These disparities might be due to the unique ability of specialised neoblast cells to maintain pluripotency <ref type="bibr" target="#b74">[76]</ref> or the sensitivity of the single-cell transcriptomic method, as in <ref type="bibr" target="#b72">[74]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Temperature time series</head><p>In this example the raw data are time series of average daily temperatures in n = 265 towns and cities, on p = 1450 days. The data originate from the Berkeley Earth project [1]. Our objective is to explore the relationship between temperature deviations and geographic locations of the towns and cities. To do so we take the ith data vector Y i to be the temperature time series for town or city i centered about its long-run average. Thus geometry of the data point-cloud Y 1 , . . . , Y n as specified by the inner products p -1 ⟨Y i , Y j ⟩ reflects the lag-zero cross-correlations amongst the time series.</p><p>In this context, we will first consider the hypothesis:</p><p>1. Z is a geographic region, z i = (latitute i , longitude i ) are the (known) geographical locations of towns or cities, ϕ is a scaled isometry. An informal implication is: geodesic distances in M reflect geographical distances.</p><p>On rejecting the above, we ask if we can at least entertain:</p><p>2. Z is a geographic region, z i = (latitute i , longitude i ) are the geographical locations of towns or cities, ϕ is a scaled isometry in certain subregions.</p><p>This will be found to hold approximately in e.g. central Europe. Following the workflow from section 1, figure <ref type="figure" target="#fig_10">10a</ref>) shows the results of dimension selection, figure <ref type="figure" target="#fig_10">10b</ref>) illustrates the variability of the magnitudes ∥ζ i ∥ 2 of the non-projected embedding vectors, and we work henceforth with the spherically projected embedding as per section 4.3.</p><p>We now consider the first hypothesis, which by proposition 4 would be mathematically justified if the temperatures on a given day were stationary processes over Earth (a 'sphere'). If isometry between Z and M were to hold up to a scaling factor, then the k-nearest neighbours of z i amongst {z j ; j ̸ = i} would correspond to the k-nearest neighbours of ϕ(z i ) amongst {ϕ(z j ); j ̸ = i}, with respect to d geo M . In order to see if the data are consistent with the hypothesis of isometry, we therefore compute the proportion of edges in common between the embedding k-nn graph G (as per section 4.4), and the geographic k-nn graph defined by the known locations z 1 , . . . , z n . Figure <ref type="figure" target="#fig_10">10c</ref>) shows this proportion as a function of k. As a baseline to help interpret these results, we sampled n points uniformly from the r-dimensional unit hypersphere, derived the k-nn graph from these points, then computed the proportion of edges in common with the geographic knn graph. This was repeated independently 100 times, and the resulting minimum, mean and maximum proportions of edges in common for each k are shown in red and black in figure <ref type="figure" target="#fig_10">10c</ref>). The correspondence between G and the geographic k-nn graph is much better than under this uniform model. However, we see that as k increases up to 50, the embedding k-nn graph has about 70% of edges in common with the geographic k-nn graph, but increasing k further up to about k = 130 does not increase this percentage further. This plateauing suggests isometry does not hold.</p><p>We now consider the second hypothesis. The plateauing leaves open the possibility that there may be coincidence between the embedding and geographic k-nn graphs in some localised areas of Z but not in others. Indeed isometry as in <ref type="bibr" target="#b9">(11)</ref> or <ref type="bibr" target="#b10">(12)</ref> requires equality of shortest path lengths for all z, z ′ ∈ Z. Figure <ref type="figure" target="#fig_11">11</ref> shows the locations of the towns and cities, and the edges in the embedding k-nn graph G, with k = 5 chosen so that according to figure <ref type="figure" target="#fig_10">10c</ref>) the embedding and geographic k-nn graphs have around 50% of edges in common. We see from figure <ref type="figure" target="#fig_11">11</ref> that in some regions, especially in central Europe, edges in the embedding k-nn graph generally correspond to geographic proximity, but elsewhere this correspondence does not hold. For example there are edges connecting Edinburgh, U.K., to cities in Norway which are not amongst its geographically nearest neighbours. Similarly, there are edges connecting Novorossiysk, Russia, to cities on the opposite shore of the Black Sea which are not amongst its geographically nearest neighbours. Conversely, geographic proximity does not always imply presence of an edge. For example, there are no edges between Baia Mare, Romania, and two geographically close cities directly to the east, on the other side of the Carpathian mountains. Plotting the k-nn graph G in this way shows presence or absence of edges, but it doesn't convey the weight of these edges in the k-nn graph G, which as per section 4.4, can approximate distances d geo M . Since the embedding is of dimension r = 36, it is challenging or perhaps impossible to construct a two-dimensional visualisation which faithfully conveys all aspects of its geometry. However, the visualisation task is much simpler if we choose some town or city, and then visualise the shortest paths in the embedding k-nn graph from that city to all other cities -the graph consisting of the union of all such paths is sometimes called a shortest path tree.</p><p>Figure <ref type="figure" target="#fig_12">12</ref> shows the shortest paths in G from Tallinn, Estonia, to all other towns and cities. Each such path is a sequence of towns or cities, and is visualised as a spline with knot points given by the locations of these towns and cities, with colour indicating length. Tallinn was chosen because of the different relationships between these shortest paths and geographic shortest paths which can be seen in different regions: the shortest paths in G which terminate at some towns and cities in central Europe, to the south-west of Tallinn, resemble geographic shortest paths, indicating a geometric relationship not far removed from isometry. By contrast, the red dots in figure <ref type="figure" target="#fig_12">12</ref> highlight the shortest path in G from Tallinn to Tripoli, Libya. This path passes through Sweden, Norway, the U.K., Ireland, France, Spain, back to France and then Italy. Clearly, this is not the geographically shortest path from Tallinn to Tripoli, indicating a strong deviation from isometry in these regions. Recalling from section 3.3 the relationship between weak stationarity and isometry, this deviation from isometry implies a pronounced lack of stationarity (with respect to geographic location) in these regions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Conventional interpretation of the Manifold Hypothesis as per the quote from <ref type="bibr" target="#b14">[16]</ref> in section 1 is that data vectors Y 1 , . . . , Y n ∈ R p are samples from some distribution supported on a manifold embedded in R p , perhaps subject to noise disturbances. Our analysis of the LMM in section 3 provides a more nuanced perspective: Y 1 , . . . , Y n are noisy, random projections of samples on a manifold M; the manifold itself is a high-dimensional distortion of some latent domain Z and arises due to correlation over Z. Under appropriate assumptions, M is homeomorphic or isometric to Z. This perspective leads us to a 'model-informed' workflow, involving PCA and nearest neighbour graph construction, in which we think about the data in relation to M, and then M in relation to Z, to explore hypotheses about the topological and geometric structure of Z.</p><p>In how much generality is this perspective applicable? Inspired by remarks of <ref type="bibr" target="#b92">[94]</ref> in the context of latent variable models of low-rank matrices, we note the basic structure of the LMM,</p><formula xml:id="formula_57">Y ij = X j (Z i ) + σE ij ,<label>(21)</label></formula><p>resembles a representation formula for exchangeable arrays due to Aldous <ref type="bibr" target="#b1">[3]</ref>: if Y is any infinite two-dimensional array of random variables such that permutations of its rows or columns do not alter the distribution of Y, then there exists a function h such that the following equality in distribution holds</p><formula xml:id="formula_58">Y ij d = h(ξ, Z i , X j , E ij ) (<label>22</label></formula><formula xml:id="formula_59">)</formula><p>where ξ and the Z i 's, X j 's and E ij 's are i.i.d. U[0, 1]-distributed random variables. Putting aside the fact that in the LMM the rows of Y are exchangeable but the columns need not be, the resemblance between ( <ref type="formula" target="#formula_57">21</ref>) and <ref type="bibr" target="#b20">(22)</ref> indicates that the LMM is rather general, albeit constrained to an additive form of error. The ability of PCA to extinguish noise, as characterised in theorem 1, seems closely tied to this additive structure. What are the limitations of the data analysis workflow we have proposed? This workflow is intentionally generic, and suitable for preliminary exploration of data and hypotheses about the data generating mechanism. It could serve as a first step before more detailed confirmatory analysis of a given data set, in order to quantify uncertainty, perform formal hypothesis testing, or fit a parametric model, and so forth, but it clearly does not include those functionalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supporting results for section 2</head><p>The following version of Mercer's theorem can be found in <ref type="bibr" target="#b83">[85,</ref><ref type="bibr">Thm 4.49]</ref>.</p><p>Theorem 2 (Mercer's theorem). Let Z be a compact metric space and let f : Z × Z → R, be a symmetric, positive semi-definite, continuous function. Let µ be a finite Borel measure supported on Z. Then there exists a countable collection of nonnegative real numbers (λ f k ) k≥1 , λ f 1 ≥ λ f 2 ≥ . . . and R-valued functions (u f k ) k≥1 which are orthonormal in L 2 (µ), such that:</p><formula xml:id="formula_60">f (z, z ′ ) = ∞ k=1 λ f k u f k (z)u f k (z ′ ), z, z ′ ∈ Z,</formula><p>where the convergence is absolute and uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Special cases of the LMM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spiked covariance model</head><p>The spiked covariance model <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b67">69]</ref> is the de facto standard model under which to study the theoretical properties of PCA, and is derived as follows. Let X ∈ R n×p be a matrix of random variables such that E[X ⊤ X] has rank r. Consider the eigendecomposition n -1 E[X ⊤ X] = VΛV ⊤ , where V ∈ R p×r , and define Z := XVΛ -1/2 . We have</p><formula xml:id="formula_61">E Z ⊤ Z = nI r , V ⊤ V = I r ,<label>(23)</label></formula><p>and X = ZΛ 1/2 V ⊤ , a.s., where the latter equality can be checked by verifying</p><formula xml:id="formula_62">E[∥X -ZΛ 1/2 V ⊤ ∥ 2 F ] = tr E[(X -ZΛ 1/2 V ⊤ ) ⊤ (X -ZΛ 1/2 V ⊤ )] = 0.</formula><p>The spiked covariance model takes the form:</p><formula xml:id="formula_63">Y = ZΛ 1/2 V ⊤ + σE,</formula><p>where the elements of E ∈ R n×p are usually assumed to be zero-mean, unit variance and uncorrelated. The rows of Z ∈ R n×r are called individual-specific random effects, and are usually assumed to be i.i.d. The following proposition shows that a spiked covariance model of precisely this form is a special case of the LMM.</p><p>Proposition 5. For any r &lt; ∞, let the rows of Z ∈ R n×r be i.i.d. random vectors such that the first equality in <ref type="bibr" target="#b21">(23)</ref> holds, let Λ = diag(λ 1 , . . . , λ r ) where λ 1 , . . . , λ r are any strictly positive real numbers and let</p><formula xml:id="formula_64">V = [v 1 | • • • |v p ] ⊤ ∈</formula><p>R p×r be any deterministic matrix such that the second equality in (23) holds. Then, if Y follows the Latent Metric Model specified by:</p><formula xml:id="formula_65">Z ⊂ R r , [Z 1 | • • • |Z n ] ⊤ := Z X j (z) := v j , Λ 1/2 z ,<label>(24)</label></formula><p>the mean correlation kernel associated with this Latent Metric Model is:</p><formula xml:id="formula_66">f (z, z ′ ) = 1 p ⟨z, Λz ′ ⟩ , which has rank r, λ f k = λ k /p, [u f 1 (z) • • • u f r (z)] ⊤ = z ∈ R r</formula><p>and the following identity holds:</p><formula xml:id="formula_67">Y = ZΛ 1/2 V ⊤ + σE.</formula><p>Proof. The claimed expression for f (z, z ′ ) holds by substituting the definition of X j (z) in ( <ref type="formula" target="#formula_65">24</ref>) into the definition f (z, z ′ ) := p -1 p j=1 E[X j (z)X j (z ′ )] and using the assumption of the proposition that</p><formula xml:id="formula_68">V ⊤ V = I r . The eigenfunctions [u f 1 (z) • • • u f r (z)] ⊤ = z ∈ R r</formula><p>are orthonormal due to the assumption E Z ⊤ Z = nI r and the i.i.d. nature of the rows of Z. The expression for Y in the statement holds by substituting <ref type="bibr" target="#b22">(24)</ref> into the definition of Y under the LMM, i.e.,</p><formula xml:id="formula_69">Y ij = X j (Z i ) + σE ij .</formula><p>The relationship between the spiked covariance model (SCM) and the LMM can thus be summarised as follows:</p><p>• the metric space (Z, d Z ) in the LMM generalizes the Euclidean domain of individual-specific random effects in the spiked covariance model;</p><p>• the eigenfunctions u f k , k ≥ 1, in the LMM generalise the linear dependence on individual-specific random effects in the SCM;</p><p>• the random functions X j , j = 1, . . . , p, in the LMM generalise the deterministic, linear functions v j , Λ 1/2 z , j = 1, . . . , p, which in light of <ref type="bibr" target="#b22">(24)</ref> are implicit in the SCM;</p><p>• the LMM allows for possibly infinite rank, generalising the finite-rank nature of the SCM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Process Latent Variable model</head><p>When Z is a subset of R d and X 1 , . . . , X p are independent and identically distributed Gaussian processes, the LMM reduces to the Gaussian Process Latent Variable model of Lawrence <ref type="bibr" target="#b48">[50]</ref>, Lawrence and Hyvärinen <ref type="bibr" target="#b47">[49]</ref>. In this case the elements of the matrix W are independent and identically distributed N (0, 1) and the aforementioned authors derive a likelihood function with these variables out. Assuming f belongs to a given parametric family, e.g., a radial basis function kernel, Lawrence and Hyvärinen <ref type="bibr" target="#b47">[49]</ref> proposed maximum a-posteriori estimation of Z 1 , . . . , Z n , parameters of the kernel and σ 2 using a gradient method. Titsias and Lawrence <ref type="bibr" target="#b88">[90]</ref> proposed alternative variational methods with enable model assessment. Lawrence <ref type="bibr" target="#b49">[51]</ref> derived a Gaussian Markov random field model related to a GPLVM through which Locally Linear Embedding <ref type="bibr" target="#b76">[78]</ref> has a statistical interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finite mixture model</head><p>Consider the case where Z has finitely many elements, say Z = {1, . . . , m}. For the following discussion it is not important that we take these elements to be the numbers 1, . . . , m, any m distinct abstract elements will do. In this situation the LMM is a form of finite mixture model with random mixture centres. Indeed we see from:</p><formula xml:id="formula_70">Y ij = X j (Z i ) + σE ij that [X 1 (z) • • • X p (z)</formula><p>] can be interpreted as the p-dimensional random centre of a mixture component labeled by z ∈ Z, and the latent variable Z i indicates which mixture component the ith row of the data matrix Y is drawn from. The simple form of the noise in the LMM constrains the generality of this mixture model: recall the elements of E are independent across columns; elements in the same column but distinct rows are uncorrelated; all elements are unit variance.</p><p>To make Z into a metric space we consider the discrete metric d Z (z, z ′ ) := 0 for z = z ′ , otherwise d Z (z, z ′ ) := 1. The kernel f is specified by the matrix F ∈ R m×m with entries</p><formula xml:id="formula_71">F kl := 1 p p j=1 E[X j (k)X j (l)], k, l ∈ {1, . . . , m}.</formula><p>In this situation A1 and A6 hold immediately, and r ≤ m. Topological equivalence of M and Z in this situation would mean that M consists of m distinct points {ϕ(1), . . . , ϕ(m)}, each associated with exactly one element of Z. If such topological equivalence were to hold then theorem 1 would tell us that the PCA embedding vectors will be clustered around the m distinct points {Q -1 ϕ(1), . . . ,</p><formula xml:id="formula_72">Q -1 ϕ(m)}, with specifically p -1/2 ζ i being close to Q -1 ϕ(Z i ).</formula><p>To verify topological equivalence it remains to check A2 holds. To this end, suppose that r = m, i.e. F is full rank. Then it is not possible that any two rows of F are identical. That is, for k, l ∈ {1, . . . m} such that k ̸ = l , there must exist some ξ ∈ {1, . . . , m} such that f (k, ξ) = F kξ ̸ = F lξ = f (l, ξ). Thus assumption A2 is satisfied and hence M is topologically equivalent to Z if r = m.</p><p>In practical terms, we therefore see that in order to organise the n rows of Y into m clusters, one can first reduce dimension to r = m by computing the PCA embedding and then apply some clustering technique to those embedding vectors. This two-step procedure of PCA followed by clustering, sometimes described as spectral clustering, is very popular in the practice of high-dimensional data analysis and is exactly what Yata and Aoshima <ref type="bibr" target="#b103">[105]</ref> recommend in the conclusion of their study of PCA embedding for mixture models in a regime where the number of samples is fixed and the dimension tends to infinity. It is already known that PCA, albeit under slightly different variations and assumptions, allows for "perfect clustering" in high-dimensional mixture models <ref type="bibr" target="#b54">[56,</ref><ref type="bibr" target="#b0">2]</ref>.  Left: maximum error max i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 , averaged over 50 independent realisations from the model, as a function of n and p. Right: the same error for p = 200, 1000, 15000, as a function of n.</p><p>To illustrate the behaviour of the LMM and PCA embedding in this context, we consider a case in which Z = {1, 2, 3} and µ is the uniform distribution on Z; for each j = 1, . . . , p, [X j (1) X j (2) X j (3)] ⊤ ∼ N (0, Σ) where Σ is full-rank; and the elements of E are independent and identically distributed N (0, 1) with σ = 1. Figure <ref type="figure" target="#fig_2">13</ref> shows the error max i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 , averaged over 50 independent realisations from the model. The plot on the left of the figure indicates that over the ranges considered, for fixed n the error decreases as p increases. Theorem 1 is not informative about the converse situation, when p is fixed and n increases: in this regime, the condition of theorem 4 involving a lower bound on n will eventually be satisfied, but the condition involving a lower bound on p/n will eventually be violated. We examine this in the right plot of figure <ref type="figure" target="#fig_2">13</ref>. We see that for fixed p, as n increases the error initially quickly decreases, but then the appears to very slowly increase n ≫ p. We conjecture the former and is related to the 1/ √ n term in <ref type="bibr" target="#b13">(15)</ref>.</p><p>Figure <ref type="figure" target="#fig_15">14</ref> illustrates how this error performance relates to the clustering of the PCA embedding vectors. When n is fixed, we see that as p increases the embedding vectors are increasingly tightly clustered around ϕ(1), ϕ(2), ϕ(3), in keeping with theorem 1. When p is fixed, we see that three clusters of embedding vectors are clearly discernible, but the clusters appear not to shrink as n grows.</p><p>Overall we conclude that, whilst theorem 1 shows that both n and p/n being large is sufficient to drive the error to zero, our numerical results suggest that for fixed p the error does not explode as n grows, and even when n ≫ p it may be that the PCA embedding still conveys the topological or geometric structure of M and hence Z. C Proofs and supporting material for section 3</p><p>Proof of Proposition 1. Define</p><formula xml:id="formula_73">W jk := Z X j (z)u f k (z)µ(dz),<label>(25)</label></formula><p>and note that</p><formula xml:id="formula_74">W jk = p 1/2 (λ f k ) 1/2 W jk ,<label>(26)</label></formula><p>where W jk is defined in <ref type="bibr" target="#b5">(7)</ref>. Pick any r 0 ≤ r and recall r ∈ {1, 2 . . . , } ∪ {∞} is the number of nonzero eigenvalues (λ f k ) k≥1 . We claim that, for any z ∈ Z, the following equality holds:</p><formula xml:id="formula_75">1 p p j=1 E   X j (z) - r0 k=1 u f k (z) W jk 2   = f (z, z) - r0 k=1 λ f k |u f k (z)| 2 .<label>(27)</label></formula><p>To verify the equality <ref type="bibr" target="#b25">(27)</ref>, observe:</p><formula xml:id="formula_76">1 p p j=1 E   X j (z) - r0 k=1 u f k (z) W jk 2   = 1 p p j=1 E |X j (z)| 2 - 2 p p j=1 E X j (z) r0 k=1 u f k (z) W jk + 1 p p j=1 r0 k=1 r0 ℓ=1 E W jk W jℓ u f k (z)u f ℓ (z) = f (z, z) -2 r0 k=1 u f k (z) Z f (z, z ′ )u f k (z ′ )µ(dz ′ ) + r0 k=1 r0 ℓ=1 u f k (z)u f ℓ (z) Z Z f (z ′ , z ′′ )u f k (z ′ )u f ℓ (z ′′ )µ(dz ′ )µ(dz ′′ ) = f (z, z) -2 r0 k=1 λ k |u f k (z)| 2 + r0 k=1 λ f k |u f k (z)| 2 = f (z, z) - r0 k=1 λ f k |u f k (z)| 2 ,</formula><p>where the second equality uses <ref type="bibr" target="#b23">(25)</ref> and</p><formula xml:id="formula_77">f (z, z ′ ) = p -1 p j=1 E[X j (z)X j (z ′ )],</formula><p>and the third equality uses the fact that (u f k , λ f k ) k≥1 , by definition, are L 2 (µ)-orthonormal eigenfunctions and eigenvalues of the integral operator associated with the kernel f and the measure µ. By Mercer's theorem (theorem 2) the r.h.s. of ( <ref type="formula" target="#formula_75">27</ref>) converges to zero as r 0 → r, uniformly in z. Each of the summands on the l.h.s. of ( <ref type="formula" target="#formula_75">27</ref>) is nonnegative, so they must also converge to zero uniformly in z. Using this uniform convergence and the fact that for any j = 1, . . . , p and i = 1, . . . , n, the pair of random variables X j and Z i are statistically independent, we have:</p><formula xml:id="formula_78">lim r0→r E   X j (Z i ) - r0 k=1 u f k (Z i ) W jk 2   = lim r0→r Z E   X j (z) - r0 k=1 u f k (z) W jk 2   µ(dz) ≤ lim r0→r sup z E   X j (z) - r0 k=1 u f k (z) W jk 2   = 0.<label>(28)</label></formula><p>Noting the identity <ref type="bibr" target="#b24">(26)</ref> and recalling ϕ</p><formula xml:id="formula_79">(z) = [(λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • ] ⊤</formula><p>, we find that (28) can equivalently be written:</p><formula xml:id="formula_80">X j (Z i ) m.s. = p 1/2 ⟨ϕ(Z i ), W j ⟩ ℓ2 ,</formula><p>where W j is the jth row of W. This completes the proof of the first identity in <ref type="bibr" target="#b6">(8)</ref>.</p><p>The second identity in <ref type="bibr" target="#b6">(8)</ref> follows from the fact that (u f k , λ f k ) k≥1 are orthonormal eigenfunctions/values:</p><formula xml:id="formula_81">p j=1 E[W jk W jℓ ] = 1 (λ f k λ f ℓ ) 1/2 Z Z u f k (z) 1 p p j=1 E [X j (z)X j (z ′ )] u f ℓ (z ′ )µ(dz ′ )µ(dz) = λ f ℓ (λ f k λ f ℓ ) 1/2 Z u f k (z)u f ℓ (z)µ(dz) = 1, k = ℓ 0, k ̸ = ℓ .</formula><p>We introduce the following assumption in order to prove proposition 6 below.</p><p>A7. For mixing coefficients φ satisfying k≥1 φ 1/2 (k) &lt; ∞ and all z, z ′ ∈ Z, the sequence {(X j (z), X j (z ′ )); j ≥ 1} is φ-mixing.</p><p>Assumption A9 is stated in section D. Proposition 6. Assume A9 and A7, and let q ≥ 1 and φ be as therein. Then there exists a constant C(φ) depending only on φ such that for any δ &gt; 0 and any i, j,</p><formula xml:id="formula_82">P p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] ≥ δ Z i , Z j ≤ 1 δ 2q 1 p q C(φ)M (q, σ)</formula><p>where</p><formula xml:id="formula_83">M (q, σ) := sup j≥1 sup z∈Z E |X j (z)| 4q + σ sup i,j≥1 E |E ij | 2q sup j≥1 sup z∈Z E |X j (z)| 2q + σ 2 sup i,j≥1 E |E ij | 4q .</formula><p>Proof. Fix any i, j and consider the decomposition:</p><formula xml:id="formula_84">p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] = 4 k=1 ∆ k</formula><p>where</p><formula xml:id="formula_85">∆ 1 := p -1 ⟨X(Z i ), X(Z j )⟩ -f (Z i , Z j ) ∆ 2 := p -1 σ ⟨X(Z i ), E j ⟩ ∆ 3 := p -1 σ ⟨X(Z j ), E i ⟩ ∆ 4 := p -1 σ 2 ⟨E i , E j ⟩ -σ 2 I[i = j] and X(z) := [X 1 (z) . . . X p (z)] ⊤ . Writing ∆ 1 as ∆ 1 = 1 p p k=1 ∆ 1,k , ∆ 1,k := X k (Z i )X k (Z j ) -E [ X k (Z i )X k (Z j )| Z i , Z j ] .</formula><p>we see that ∆ 1 is an arithmetic mean of p random variables, each of which is conditionally meanzero given Z i , Z j . For ∆ 2 , we have</p><formula xml:id="formula_86">∆ 2 = σ p p k=1 ∆ 2,k , ∆ 2,k := X k (Z i )E jk ,</formula><p>By definition of the LMM, the three collections of random variables, (Z 1 , . . . , Z n ), (X 1 , . . . , X p ) and (E 1 , . . . , E n ) are mutually independent, and the elements of each vector E j ∈ R p are mean-zero and independent. Therefore given Z i , Z j and X 1 , . . . , X p , ∆ 2 is an arithmetic mean of conditionally independent and conditionally mean-zero random variables. The same decomposition holds for ∆ 3 , with i, j interchanged. For ∆ 4 , we have</p><formula xml:id="formula_87">∆ 4 = σ 2 p 1≤k≤p ∆ 4,k , ∆ 4,k := E ik E jk -I[i = j].</formula><p>Recalling from the definition of the LMM that the elements of E are mean zero, unit-variance, uncorrelated across rows, and independent across columns, we see that ∆ 4 is a sum of p mean-zero and mutually independent random variables. The proof proceeds by using a moment inequality for mixing random variables <ref type="bibr" target="#b99">[101]</ref>[Lemma 1 <ref type="bibr">.7]</ref> to bound E[|∆ 1 | 2q |Z i , Z j ], and using the Marcinkiewicz-Zygmund inequality to bound</p><formula xml:id="formula_88">E[|∆ 2 | 2q |Z i , Z j , X 1 , . . . , X p ], E[|∆ 3 | 2q |Z i , Z j , X 1 , . . . , X p ] and E[|∆ 4 | 2q</formula><p>]. These bounds are then combined and Markov's inequality applied. The details are similar to <ref type="bibr" target="#b30">[32]</ref>[Proof of proposition 2], so are omitted.</p><p>Proof of Proposition 2. As explained above the statement of proposition 2, we only need to show that A2 holds if and only if ϕ is one-to-one. We first show that A2 implies ϕ is one-to-one. We prove the contrapositive to this statement. So suppose that ϕ is not one-to-one. Then there must exist z ̸ = z ′ ∈ Z such that ϕ(z) = ϕ(z ′ ). This implies that for any ξ in Z, f (z, ξ) = ⟨ϕ(z), ϕ(ξ)⟩ ℓ2 = ⟨ϕ(z ′ ), ϕ(ξ)⟩ ℓ2 = f (z ′ , ξ), which is the converse of A2.</p><p>In the other direction, suppose the converse of A2 holds, i.e., the exists z ̸ = z ′ such that f (z, ξ) = f (z ′ , ξ) for all ξ. By considering the cases ξ = z and ξ = z ′ we find f (z, z) = f (z, z ′ ) = f (z ′ , z ′ ). In turn, ∥ϕ(z) -ϕ(z ′ )∥ 2 ℓ2 = f (z, z) + f (z ′ , z ′ ) -2f (z, z ′ ) = 0, i.e., ϕ(z) = ϕ(z ′ ), and hence ϕ is not one-to-one.</p><p>C.1 Proofs and supporting material for section <ref type="bibr">3.3</ref> The purpose of this section is to state some definitions and intermediate results, building towards the proofs of propositions 3 and 4. Recall that the term "continuous path" was used in A3. From henceforth we just say "path" for short.</p><p>The following definitions are standard in metric geometry <ref type="bibr" target="#b10">[12]</ref>. For x, x ′ ∈ M, a path in M with end-points x, x ′ is a continuous function γ : [0, 1] → M such that γ 0 = x and γ 1 = x ′ , where M is equipped with the distance ∥ • -• ∥ ℓ2 . With n ≥ 1, a non-decreasing sequence t 0 , . . . , t n such that t 0 = 0 and t n = 1, is called a partition. Given a path γ and a partition P = (t 0 , . . . , t n ), define χ(γ, P) := n k=1 ∥γ t k -γ t k-1 ∥ ℓ2 . The length of γ is L(γ) := sup P χ(γ, P), where the supremum is over all possible partitions.</p><p>When Z is a subset of R d , a path η in Z with end-points z, z ′ is a continuous function η : [0, 1] → Z such that η 0 = z, η 1 = z ′ , and with χ(η, P)</p><formula xml:id="formula_89">:= n k=1 ∥η t k -η t k-1 ∥ R d the length of η is L(η) := sup P χ(η, P).</formula><p>The shortest path lengths, also known as geodesic distances, in M and Z are:</p><formula xml:id="formula_90">d geo M (x, x ′ ) := inf γ:γ0=x,γ1=x ′ L(γ) d geo Z (z, z ′ ) := inf η:η0=z,η1=z ′ L(η),<label>(29)</label></formula><p>where the infima are over all paths in respectively M and Z with the indicated end-points.</p><p>A8. Assume A3 holds and with d as therein, additionally assume there exists a closed ball Z ⊂ R d centered on the origin such that: Z ⊂ Z; the definition of f (z, z ′ ) can be extended from Z × Z to Z × Z; f is C 2 on Z × Z and the matrix H ξ ∈ R d×d with elements:</p><formula xml:id="formula_91">(H ξ ) ij := ∂ 2 f ∂z i ∂z ′ j (ξ,ξ)</formula><p>is positive-definite for all ξ ∈ Z.</p><p>The statement of the following theorem, from <ref type="bibr" target="#b97">[99]</ref>, is paraphrased slightly in order to match the assumptions of interest here. Theorem 3 ([99], Thm 1.). Assume A2 and A8. Then ϕ is a bi-Lipschitz homeomorphism between Z and M. Let x, x ′ be any two points in M, and let γ be any path in M of finite length, with end-points x, x ′ . Define η : [0, 1] → Z by η t := ϕ -1 (γ t ). Then η is a path in Z with L(η) &lt; ∞. For any ϵ &gt; 0 there exists a partition P ϵ such that for any partition P = (t 0 , . . . , t n ) satisfying P ϵ ⊆ P,</p><formula xml:id="formula_92">L(γ) - n k=1 η t k -η t k-1 , H ηt k-1 (η t k -η t k-1 ) 1/2 ≤ ϵ. (<label>30</label></formula><formula xml:id="formula_93">)</formula><p>Proof of proposition 3. Under the assumptions of the proposition, by direct calculation H ξ = -2g ′ (0)I d for all ξ ∈ Z and A8 holds. Fix any z, z ′ ∈ Z and let η be any finite length path in Z with these end-points. By theorem 3, ϕ is Lipschitz, so γ defined by γ t := ϕ(η t ) has finite length. Define x := ϕ(z), x ′ := ϕ(z ′ ). Applying theorem 3, we have from <ref type="bibr" target="#b28">(30)</ref> that for any ϵ &gt; 0 there exists a partition P ϵ such that for any P = (t 0 , . . . , t n ) satisfying P ϵ ⊆ P, L(γ) --2g ′ (0)χ(η, P) ≤ ϵ. <ref type="bibr" target="#b29">(31)</ref> Also, using the definition of path length L(η) and the triangle inequality, there exists P ϵ such that for any partition P satisfying P ϵ ⊆ P, we have:</p><formula xml:id="formula_94">|L(η) -χ(η, P)| ≤ ϵ. (<label>32</label></formula><formula xml:id="formula_95">)</formula><p>Choosing P to be the union of P ϵ and P ϵ , i.e., if τ ∈ P ϵ or P ϵ , then τ ∈ P, we find that ( <ref type="formula">31</ref>) and ( <ref type="formula" target="#formula_94">32</ref>) are satisfied simultaneously. Since ϵ was arbitrarily small, we find that L(γ) = -2g ′ (0)L(η). By theorem 3, ϕ is a bi-Lipschitz homeomorphism, so γt = ϕ(η t ) defines a bijection between the set of finite-length paths γ in Z with end-points ϕ(z), ϕ(z ′ ) and the set of finite length paths η in Z with end-points z, z ′ . Therefore by taking the infimum over η on both sides of L(γ) = -2g ′ (0)L(η) where γ is defined by γ t = ϕ(η t ) as above, we find that</p><formula xml:id="formula_96">d geo M (ϕ(z), ϕ(z ′ )) = -2g ′ (0)d geo Z (z, z ′ )<label>(33)</label></formula><p>as required.</p><p>Proof of proposition 4. For the Z in question, we have g(⟨z, z</p><formula xml:id="formula_97">′ ⟩ R d ) = g(1 -∥z -z ′ ∥ 2 R d /2</formula><p>). The proof is completed by applying proposition 3 and using the chain rule of differentiation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof and supporting results for theorem 1</head><p>Theorem 1 is a corollary to theorem 4. The proofs of both these theorems are in section D. The following assumption is a more detailed version of A5.</p><p>A9. For some q ≥ 1, sup</p><formula xml:id="formula_98">j≥1 sup z∈Z E[|X j (z)| 4q ] &lt; ∞ and sup j≥1 sup i≥1 E[|E ij | 4q ] &lt; ∞.</formula><p>Theorem 4. Assume A4, A6 and A9, and let q ≥ 1 and r &lt; ∞ be as therein. For min(p, n) ≥ r, let Y ∈ R n×p follow the LMM from section 2 and let ζ 1 , . . . , ζ n be the dimension-r PCA embedding.</p><p>Then there exists a random orthogonal matrix Q ∈R r×r depending on n and p such that for any δ ∈ (0, 1) and ϵ ∈ (0, 1], if</p><formula xml:id="formula_99">n ≥ c 1 σ 2 r 1/2 1 ∨ σ 2 r 1/2 ϵ 2 ∨ log r δ and p n ≥ c 2 (q) r δ 1/q ϵ 2 , then max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 ≤ ϵ</formula><p>with probability at least 1 -δ. Here c 1 and c 2 (q) are constants depending on the suprema in A9 and the quantity inf p≥1 λ f r which is strictly positive under A6; and ∥ • ∥ 2 is the Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Definitions and preliminaries</head><p>Throughout section D the probability measure µ in the LMM is considered fixed, (λ f k , u f k ) k≥1 are as in section 2, and assumption A6 is taken to hold, so that the rank of f is finite, i.e., r &lt; ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.1 Notation concerning vectors and matrices in general</head><p>We notationally index the eigenvalues of a generic symmetric matrix A in a non-increasing but otherwise arbitrary order λ 1 (A) ≥ λ 2 (A) ≥ • • • . For a vector x with elements x i , ∥x∥ ∞ := max i |x i | and ∥x∥ 2 := i |x i | 2 , and the spectral norm and Frobenius norm of matrices are denoted ∥ • ∥ 2 and ∥ • ∥ F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 Some matrices of interest</head><p>Let the matrix Φ ∈ R n×r be defined by</p><formula xml:id="formula_100">Φ := [ϕ(Z 1 )| • • • |ϕ(Z n )] ⊤ ,</formula><p>Let Λ Y ∈ R r×r be the diagonal matrix with diagonal elements the eigenvalues λ 1 (p -1 YY ⊤ ) . . . , λ r (p -1 YY ⊤ ), and let U Y ∈ R n×r have as its columns orthonormal eigenvectors associated with these eigenvalues. Since Φ ∈ R n×r and r ≤ min(p, n), the matrix ΦΦ ⊤ has rank at most r. Let Λ Φ ∈ R r×r be the diagonal matrix with diagonal elements which are the eigenvalues λ 1 (ΦΦ ⊤ ), . . . , λ r (ΦΦ ⊤ ), and let U Φ ∈ R n×r have as its columns orthonormal eigenvectors associated with these eigenvalues. Let F 1 ΣF ⊤ 2 denote the full singular value decomposition of U ⊤ Φ U Y and define the random orthogonal matrix</p><formula xml:id="formula_101">F ⋆ := F 1 F ⊤ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.3 Some events of interest</head><p>With U j denoting the jth column of U Φ , define:</p><formula xml:id="formula_102">A 1 (ϵ) := ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≤ ϵn A 2 (ϵ) := n i=1 B Y,i (ϵ) ∩ r i=1 B Φ,i (ϵ) A 3 (ϵ) := max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ ϵn 1/2 A rank := rank(YY ⊤ ) ≥ r ∩ rank(ΦΦ ⊤ ) = r B Y,i (ϵ) := λ f i (1 -ϵ) ≤ 1 n λ i (p -1 YY ⊤ ) ≤ λ f i (1 + ϵ) , 1 ≤ i ≤ r, 1 n λ i (p -1 YY ⊤ ) ≤ ϵλ f r , r + 1 ≤ i ≤ n. B Φ,i (ϵ) := (1 -ϵ)λ f i ≤ 1 n λ i (ΦΦ ⊤ ) ≤ (1 + ϵ)λ f i , 1 ≤ i ≤ r.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proofs of theorems 4 and 1</head><p>Proof of theorem 4. Let F 1 ΣF ⊤ 2 be the full singular value decomposition of U ⊤ Φ U Y and define the random orthogonal matrix F ⋆ := F 1 F ⊤ 2 . On the event A rank we have U Φ Λ Φ U ⊤ Φ = ΦΦ ⊤ , and applying lemma 4 we find there exists a random orthogonal matrix</p><formula xml:id="formula_103">Q such that U Φ Λ 1/2 Φ = Φ Q, hence [U Φ Λ 1/2 Φ F ⋆ ] i = ϕ(Z i ) ⊤ Q for all i = 1, . . . n, where Q := QF ⋆ is orthogonal and [•] i denotes the ith row of a matrix. Lemma 5 shows that [U Y Λ 1/2 Y ] i = p -1/2 ζ i .</formula><p>Combining these observations we have shown that on the event A rank ,</p><formula xml:id="formula_104">∥p -1/2 Qζ i -ϕ(Z i )∥ 2 = ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 , i = 1, . . . , n.<label>(34)</label></formula><p>Now fix any ϵ 1 &gt; 0, ϵ 2 ∈ (0, 1/2) and ϵ 3 &gt; 0. Note that the event A rank is a superset of A 2 (ϵ 2 ) and thus A 1 (ϵ 1 )∩A 2 (ϵ 2 )∩A 3 (ϵ 3 ) ⊆ A rank . Throughout the remainder of the proof of theorem 4 we shall establish various identities and inequalities involving random variables, random matrices, etc; all such identifies and inequalities to be understood as holding on the event A 1 (ϵ 1 )∩A 2 (ϵ 2 )∩A 3 (ϵ 3 ), although we shall avoid making this explicit in our notation in order to avoid repetition. For example, for two random matrices say A and B, we write "A = B" as shorthand for "A(ω) = B(ω) for all ω ∈ A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) ∩ A 3 (ϵ 3 )" and similarly for two random variables say X, Y , we write "X ≤ Y " as shorthand for "X(ω) ≤ Y (ω) for all ω ∈ A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) ∩ A 3 (ϵ 3 )".</p><p>Noting that on the event A rank , the matrices Λ -1/2 Y and Λ -1/2 Φ are well-defined, let us introduce:</p><formula xml:id="formula_105">C 1 := F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ C 2 := (U ⊤ Φ U Y -F ⋆ )Λ 1/2 Y C 3 := U Y -U Φ F ⋆ = U Y -U Φ U ⊤ Φ U Y + U Φ (U ⊤ Φ U Y -F ⋆ ) D 1 := U Φ C 1 D 2 := U Φ C 2 D 3 := (I -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y D 4 := -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y D 5 := (p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ )</formula><p>We now claim that:</p><formula xml:id="formula_106">U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + 5 i=1 D i ,<label>(35)</label></formula><p>which up to some notational differences, is the same decomposition used by Lyzinski et al. [58, Proof of Thm 18.] in the analysis of spectral methods for community detection in graphs. To verify the decomposition <ref type="bibr" target="#b33">(35)</ref>, observe:</p><formula xml:id="formula_107">U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ = U Y Λ 1/2 Y -U Φ F ⋆ Λ 1/2 Y + U Φ C 1 = (I n -U Φ U ⊤ Φ )U Y Λ 1/2 Y + U Φ C 2 + U Φ C 1 = (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )U Y Λ -1/2 Y<label>(36)</label></formula><formula xml:id="formula_108">+ U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -Φ ⊤ Φ)U Φ F ⋆ Λ -1/2 Y -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y + (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y + U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + (p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ) -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y + (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y + U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + D 5 + D 4 + D 3 + D 2 + D 1<label>(37)</label></formula><p>where <ref type="bibr" target="#b34">(36)</ref> </p><formula xml:id="formula_109">holds because U Y Λ 1/2 Y = p -1 Y ⊤ YU Y Λ -1/2 Y and U Φ U ⊤ Φ ΦΦ ⊤ = ΦΦ ⊤ .</formula><p>The proof proceeds by bounding the Frobenius norm of each matrix D i , i = 1, . . . , 5.. Using lemma 2,</p><formula xml:id="formula_110">∥D 1 ∥ F = ∥C 1 ∥ F ≤ r 1/2 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2 n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + nϵ 1 + σ 2 = r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) 2(1 -ϵ 2 ) 1/2 (λ f r ) 1/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + 1 .<label>(38)</label></formula><p>Using lemma 1,</p><formula xml:id="formula_111">∥D 2 ∥ F ≤ r 1/2 ∥C 2 ∥ 2 = r 1/2 n 1/2 [λ f 1 (1 + ϵ 2 )] 1/2 ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 . (<label>39</label></formula><formula xml:id="formula_112">)</formula><p>Again using lemma 1 and the fact that</p><formula xml:id="formula_113">U Y -U Φ U ⊤ Φ U Y = (U Y U ⊤ Y -U Φ U ⊤ Φ )U Y , ∥D 3 ∥ F ≤ 2r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥C 3 ∥ 2 ∥Λ -1/2 Y ∥ 2 ≤ 2r 1/2 (ϵ 1 n + σ 2 ) n 1/2 λ f r (1 -ϵ 2 ) 1/2 ∥U Y U ⊤ Y -U Φ U ⊤ Φ ∥ 2 + ∥U ⊤ Φ U Y -F ⋆ ∥ 2 ≤ 2r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -ϵ 2 ) 3/2 1 + ϵ 1 + n -1 σ 2 λ f r (1 -ϵ 2 )<label>(40)</label></formula><p>Directly:</p><formula xml:id="formula_114">∥D 4 ∥ F ≤ r 1/2 ∥D 4 ∥ 2 ≤ r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥Λ -1/2 Y ∥ 2 ≤ r 1/2 (ϵ 1 n + σ 2 ) n 1/2 λ f r (1 -ϵ 2 ) 1/2 = r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -ϵ 2 ) 1/2<label>(41)</label></formula><p>Using lemma 2,</p><formula xml:id="formula_115">∥D 5 ∥ F = ∥(p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ )∥ F ≤ r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ∥ F ≤ r 1/2 (ϵ 1 n + σ 2 ) ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 3/2 (λ f r ) 3/2 (1 -ϵ 2 ) 3/2 ≤ rn 2 (ϵ 1 + n -1 σ 2 ) 2n 3/2 (λ f r ) 3/2 (1 -ϵ 2 ) 3/2 (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + ϵ 1 + σ 2 n = rn 1/2 (ϵ 1 + n -1 σ 2 ) 2 2(λ f r ) 3/2 (1 -ϵ 2 ) 3/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + 1<label>(42)</label></formula><p>Having obtained the above bounds on ∥D i ∥ F , for i = 1, . . . , 5, we turn to the first term on the r.h.s. of <ref type="bibr" target="#b33">(35)</ref>. Writing [•] i to indicate the ith row of a matrix,</p><formula xml:id="formula_116">max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ ] i ∥ 2 (43) = max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ ] i ∥ 2 ≤ 1 n 1/2 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ ] i ∥ 2 ≤ r 1/2 n 1/2 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ )U j ∥ ∞ ≤ r 1/2 ϵ 3 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 . (<label>44</label></formula><formula xml:id="formula_117">)</formula><p>where U j is the jth column of U Φ .</p><p>Recall that at the start of the proof we fixed arbitrary values ϵ 1 &gt; 0, ϵ 2 ∈ (0, 1/2) and ϵ 3 &gt; 0. We now need to work with a specific numerical value for ϵ 2 , so let us take it to be 1/4. Elementary manipulations of the bounds ( <ref type="formula" target="#formula_110">38</ref>)-( <ref type="formula" target="#formula_115">42</ref>) then show that there exists c0 depending only on the constants c max λ , c min λ in lemma 7 such that</p><formula xml:id="formula_118">∥D 1 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n ϵ 1 + σ 2 n + 1 ∥D 2 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n 2 ∥D 3 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n 2 ϵ 1 + σ 2 n + 1 ∥D 4 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n ∥D 5 ∥ F ≤ c0 rn 1/2 ϵ 1 + σ 2 n 2 ϵ 1 + σ 2 n + 1 . Now assuming n ≥ 2σ 2 r 1/2<label>(45)</label></formula><p>i.e, n -1 σ 2 r 1/2 ≤ 1/2, and assuming</p><formula xml:id="formula_119">ϵ 1 r 1/2 ≤ 1/2<label>(46)</label></formula><p>we have</p><formula xml:id="formula_120">ϵ 1 + σ 2 n r 1/2 ≤ 1.</formula><p>Applying this inequality in the above bound on ∥D 5 ∥ F and allowing c0 to increase where necessary we obtain: max i=1,...,5</p><formula xml:id="formula_121">∥D i ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n</formula><p>Combining this estimate with <ref type="bibr" target="#b42">(44)</ref> and again allowing c0 to increase as needed,</p><formula xml:id="formula_122">max i=1,...,n ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 ≤ max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ ] i ∥ 2 + 5 i=1 ∥D i ∥ F ≤ r 1/2 c0 n 1/2 ϵ 1 + σ 2 n + r 1/2 c0 ϵ 3 .<label>(47)</label></formula><p>Now fix any ϵ ∈ (0, 1] and let us strengthen <ref type="bibr" target="#b43">(45)</ref> to</p><formula xml:id="formula_123">n ≥ 2σ 2 r 1/2 ∨ 9 ϵ 2 c2 0 rσ 4<label>(48)</label></formula><p>so that r 1/2 c0 n -1/2 σ 2 ≤ ϵ/3. Then setting ϵ 1 := ϵ/(3n 1/2 r 1/2 c0 ) (which satisfies (46) since c0 ≥ 1), ϵ 3 := ϵ/(3r 1/2 c0 ) and recalling that we have already chosen ϵ 2 := 1/4 we have as a consequence of (47),</p><formula xml:id="formula_124">P max i=1,...,n ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 ≤ ϵ ≥ 1 -P(A 1 (ϵ/[3n 1/2 r 1/2 c0 ]) c ) -P(A 2 (1/4) c ) -P(A 3 (ϵ/[3r 1/2 c0 ]) c ).</formula><p>Now fix any δ ∈ (0, 1). By lemma 9, proposition 7 and lemma 11, there exists constants c1 (q), c2 and c3 (q) (depending only on the constants c max λ , c min λ from lemma 7 and the constants c X (2q), c E (2q) from lemma 9) such that</p><formula xml:id="formula_125">p n ≥ c1 (q) 1/q r δ 1/q ϵ 2 ⇒ P(A 1 (ϵ/[3n 1/2 r 1/2 c0 ]) c ) ≤ δ 3 . n ≥ c2 σ 2 ∨ log r δ and p ≥ c2 δ 1/q ⇒ P(A 2 (1/4) c ) ≤ δ 3 . p n 1/q ≥ c3 (q) 1/q r 1+1/q δ 1/q ϵ 2 ⇒ P(A 3 (ϵ/[3r 1/2 c0 ]) c ) ≤ δ 3 .</formula><p>Combining these conditions with (48) and appropriately defining c 1 and c 2 gives the conditions in the statement of the theorem. Recalling (34), the proof is complete.</p><p>Proof of theorem 1. If A5 holds, then A9 holds with q = 1. We may then apply theorem 4 in the case q = 1, and in order for the lower bound conditions on n and p/n in the statement of theorem 4 to be satisfied for some given δ and ϵ, it is sufficient that:</p><formula xml:id="formula_126">n ≥ -č 1 log δ ϵ 2 and p n ≥ č2 ϵ 2 δ ,<label>(49)</label></formula><p>for suitable constants č1 &gt; 0 and č2 &gt; 0 depending on σ, c 1 , c 2 (q) and sup p≥1 r, noting the latter supremum is finite under A6.</p><p>To complete the proof we need to show that for any δ ∈ (0, 1) there exists ϵ 0 &gt; 0 and M &gt; 0 such that if (1/ √ n + n/p) -1 &gt; M , then:</p><formula xml:id="formula_127">P max i=1,...,n ∥p -1/2 Qζ i -ϕ(Z i )∥ 2 &gt; ϵ 0 1 √ n + n p &lt; δ.<label>(50)</label></formula><p>So to proceed, fix any δ ∈ (0, 1), define ϵ 0 := √ -č 1 log δ ∨ č2 /δ, M := ϵ 0 and ϵ := ϵ 0 (1/ √ n + n/p). Assume that (1/ √ n + n/p) -1 ≥ M and notice that in this situation ϵ ∈ (0, 1], which is a requirement of theorem 4. It follows from the definition of ϵ 0 that:</p><formula xml:id="formula_128">ϵ 2 0 ≥ -č 1 log δ ≥ -č 1 log δ 1 + n/ √ p 2 = -č 1 log δ n 1/ √ n + n/p 2 ,</formula><p>and rearranging then using the above definition of ϵ gives:</p><formula xml:id="formula_129">n ≥ -č 1 log δ ϵ 2 ,</formula><p>i.e., the first inequality in (49) holds. Similarly</p><formula xml:id="formula_130">ϵ 2 0 ≥ č2 δ ≥ č2 √ p/n + 1 2 δ = č2 p n 1/ √ n + n/p 2 δ hence p n ≥ č2 ϵ 2 δ ,</formula><p>i.e., the second inequality in (49) holds. Thus by theorem 4,</p><formula xml:id="formula_131">P max i=1,...,n ∥p -1/2 Qζ i -ϕ(Z i )∥ 2 &gt; 1 √ n + n p ϵ 0 &lt; δ.</formula><p>which is (50).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Matrix estimates</head><p>Lemma 1. Assume A6. Then for any ϵ 1 &gt; 0 and ϵ 2 ∈ (0, 1/2), on the event</p><formula xml:id="formula_132">A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 )</formula><p>we have</p><formula xml:id="formula_133">∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) and ∥U ⊤ Φ U Y -F ⋆ ∥ 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 .</formula><p>Proof. In outline, the proof follows Lyzinski et al. [58, Proof of Prop. 16], although we work with the spectral rather than Frobenius norm. On the event in the statement we have:</p><formula xml:id="formula_134">|λ r (ΦΦ ⊤ ) -λ r+1 (p -1 YY ⊤ )| ≥ nλ f r (1 -<label>2ϵ</label></formula><p>2 ) &gt; 0 and with σ i denoting the ith singular value of U ⊤ Φ U Y and σ i = cos(θ i ), the Davis-Kahan sin(θ) theorem gives:</p><formula xml:id="formula_135">∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 = max i | sin(θ i )| ≤ ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 |λ r (ΦΦ ⊤ ) -λ r+1 (p -1 YY ⊤ )| ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) .<label>(51)</label></formula><p>Therefore</p><formula xml:id="formula_136">∥U ⊤ Φ U Y -F ⋆ ∥ 2 = ∥F 1 ΣF ⊤ 2 -F 1 F ⊤ 2 ∥ 2 = ∥F 1 (Σ -I r )F ⊤ 2 ∥ 2 = ∥Σ -I r ∥ 2 = max i=1,...,r |1 -σ i | ≤ max i=1,...,r |1 -σ 2 i | = max i=1,...,r | sin(θ i )| 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2</formula><p>where for the first inequality uses ∥U ⊤ Φ U Y ∥ 2 ≤ 1 and the second inequality is from (51). Lemma 2. Assume A6. For any ϵ 1 &gt; 0, ϵ 2 ∈ (0, 1/2), on the event</p><formula xml:id="formula_137">A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) we have ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F ≤ r 1/2 n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + nϵ 1 + σ 2 , ∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2 , ∥F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F n(1 -ϵ 2 )λ f r .</formula><p>Proof. Using a decomposition idea from [58, proof of lemma 17], with</p><formula xml:id="formula_138">R := U Y -U Φ U ⊤ Φ U Y ,</formula><p>we have</p><formula xml:id="formula_139">F ⋆ Λ Y -Λ Φ F ⋆ = (F ⋆ -U ⊤ Φ U Y )Λ Y + U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R + U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y + Λ Φ (U ⊤ Φ U Y -F ⋆ ) hence ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ 2 ≤ ∥U ⊤ Φ U Y -F ⋆ ∥ 2 (∥Λ Y ∥ 2 + ∥Λ Φ ∥ 2 )<label>(52)</label></formula><formula xml:id="formula_140">+ ∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R∥ 2<label>(53)</label></formula><formula xml:id="formula_141">+ ∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y ∥ 2<label>(54)</label></formula><p>For the term on the r.h.s. of ( <ref type="formula" target="#formula_139">52</ref>), on the event in the statement of the present lemma and using lemma 1 we have:</p><formula xml:id="formula_142">∥U ⊤ Φ U Y -F ⋆ ∥ 2 (∥Λ Y ∥ 2 + ∥Λ Φ ∥ 2 ) ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 2nλ f 1 (1 + ϵ 2 ).</formula><p>For the term in (53</p><formula xml:id="formula_143">), using R = (U Y U ⊤ Y -U ⊤ Φ U Φ )U Y ,</formula><p>we have again on the event in the statement of the present lemma and using lemma 1,</p><formula xml:id="formula_144">∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R∥ 2 ≤ ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥R∥ 2 ≤ (∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 + σ 2 )∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 ≤ (ϵ 1 n + σ 2 ) ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) = n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 )</formula><p>.</p><p>For the term in <ref type="bibr" target="#b52">(54)</ref>,</p><formula xml:id="formula_145">∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y ∥ 2 ≤ ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )∥ 2 + σ 2 ∥U ⊤ Φ U Y ∥ 2 ≤ nϵ 1 + σ 2 .</formula><p>The bound on ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F given in the statement holds by combining the above spectral norm bounds.</p><p>For the bound on</p><formula xml:id="formula_146">∥F ⋆ Λ 1/2 Y -Λ 1/2</formula><p>Φ F ⋆ ∥ F we use the fact that the elements of</p><formula xml:id="formula_147">F ⋆ Λ 1/2 Y -Λ 1/2</formula><p>Φ F ⋆ can be written:</p><formula xml:id="formula_148">(F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ) ij = (F ⋆ ) ij λ j (p -1 YY ⊤ ) 1/2 -λ i (ΦΦ ⊤ ) 1/2 (F ⋆ ) ij = (F ⋆ ) ij [λ j (p -1 YY ⊤ ) -λ i (ΦΦ ⊤ )] λ j (p -1 YY ⊤ ) 1/2 + λ i (ΦΦ ⊤ ) 1/2 hence |(F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ) ij | ≤ |(F ⋆ Λ Y -Λ Φ F ⋆ ) ij | 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2</formula><p>, and so</p><formula xml:id="formula_149">∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The bound on ∥F</head><formula xml:id="formula_150">⋆ Λ -1/2 Y -Λ -1/2 Φ</formula><p>F ⋆ ∥ F in the statement is obtained in a similar manner using the fact that for any a, b &gt; 0, a -1/2 -b -1/2 = (b 1/2 -a 1/2 )/(a 1/2 b 1/2 ) .</p><p>Thus by computing the PCA embedding ζ 1 , . . . , ζ n and rescaling by p -1/2 , we are, in effect, computing the n rows of</p><formula xml:id="formula_151">U Y Λ 1/2 Y , where U Y Λ 1/2 Y (U Y Λ 1/2 Y ) ⊤ = U Y Λ Y U ⊤</formula><p>Y is a rank-r approximation to p -1 YY ⊤ . Lemma 6. Assume A6. Then p -1 E[YY ⊤ |Z 1 , . . . , Z n ] = ΦΦ ⊤ + σ 2 I n .</p><p>Proof. Let X ∈ R n×p be the matrix with entries X ij := X j (Z i ). According to the model specification in section 2, X and E are independent, and E[EE ⊤ ] = pI n . Thus: The existence of c max λ as required follows from the above inequalities combined with:</p><formula xml:id="formula_152">λ f 1 ≤ ∞ k=1 λ f k = ∞ k=1 λ f k E[|u f k (Z 1 )| 2 ] = E[f (Z 1 , Z 1 )] ≤ sup z f (z, z).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Matrix concentration results</head><p>The following matrix-valued version of the Bernstein inequality can be found in, e.g., [92, Thm 1.6. Lemma 8. Assume A6. For any t ≥ 0,</p><formula xml:id="formula_153">P ∥n -1 Φ ⊤ Φ -n -1 E[Φ ⊤ Φ]∥ 2 ≥ t ≤ 2r exp -t 2 n/2 (c max λ ) 2 + c max λ t/3 ,</formula><p>where c max λ is as in lemma 7.</p><p>Proof. Apply theorem 5 with</p><formula xml:id="formula_154">M i = 1 n ϕ(Z i )ϕ(Z i ) ⊤ -E[ 1 n ϕ(Z i )ϕ(Z i ) ⊤ ], ∥M i ∥ 2 ≤ 1 n ∥ϕ(Z i )ϕ(Z i ) ⊤ ∥ 2 + 1 n ∥E[ϕ(Z i )ϕ(Z i ) ⊤ ]∥ 2 = 1 n ∥ϕ(Z i )∥ 2 2 + 1 n λ f 1 = 1 n f (Z i , Z i ) + 1 n λ f 1 ≤ 1 n c max λ =: L and v(M) = E i M i i M i 2 = E i M i M i 2 ≤ 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 + 1 n ∥E[ϕ(Z 1 )ϕ(Z 1 ) ⊤ ] 2 ∥ 2 ≤ 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 + 1 n ∥E[ϕ(Z 1 )ϕ(Z 1 ) ⊤ ] 2 ∥ 2 = 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 2 + 1 n (λ f 1 ) 2 = 1 n E ∥ϕ(Z 1 )∥ 4 2 + 1 n (λ f 1 ) 2 ≤ 1 n (c max λ ) 2 .</formula><p>Lemma 9. Assume A6 and A9 with some q ≥ 1. Then for any t &gt; 0, P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t ≤ (16) q (2q -1) q n 2q t 2q 1 p q c X (2q) 1/2q + σ 2 c E (2q) 1/2q 2q where c X (q) := sup </p><formula xml:id="formula_155">YY ⊤ = p j=1 Y j Y ⊤ j .<label>(56)</label></formula><p>Observe that under the model of section 2, conditional on (Z 1 , . . . , Z n ) the summands in <ref type="bibr" target="#b54">(56)</ref> are independent and as per lemma 6, the conditional expectation of YY ⊤ given Z 1 , . . . , Z n is:</p><formula xml:id="formula_156">pΦΦ ⊤ + pσ 2 I n .</formula><p>The main tool we use from hereon is a direct combination of the matrix Chebyshev inequality <ref type="bibr" target="#b68">[70,</ref><ref type="bibr">Prop. 3</ref>.1] and the matrix polynomial Effron-Stein inequality <ref type="bibr" target="#b68">[70,</ref><ref type="bibr">Thm 4.2]</ref>, applied under the regular conditional distribution of (Y 1 , . . . , Y p ) given (Z 1 , . . . , Z n ). These inequalities taken together tell us that, for any q ≥ 1, the following holds almost surely:</p><formula xml:id="formula_157">P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t Z 1 , . . . , Z n ≤ 1 t 2q E ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2q</formula><p>S2q Z 1 , . . . , Z n ≤ 2 q (2q -1) q t 2q E ∥Σ∥ q Sq Z 1 , . . . , Z n .</p><p>Here ∥ • ∥ Sq is the Schatten q-norm and Σ ∈ R n×n is the variance proxy:</p><formula xml:id="formula_158">Σ := 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z 1 , . . . , Z n ,<label>(57)</label></formula><p>where, conditional on Z 1 , . . . , Z n , Ỹj is an independent copy of Y j . For brevity in the remainder of the proof we shall write Z ≡ (Z 1 , . . . , Z n ), and to avoid repetitive statements of "almost surely", every inequality involving conditional expectations is to be understood as holding in the almost sure sense. We estimate: Here (58) holds by the second claim of lemma 10; 59 holds by first claim of lemma 10 combined with the fact that x → x q is convex for x ≥ 0 (recall q ≥ 1); ≤ n i=1 E |X j (Z i ) + σE ij | 4q Z 1/2q ≤ 2 n i=1 E |X j (Z i )| 4q Z 1/2q + E |σE ij | 4q Z 1/2q ≤ 2n sup l≥1 sup z∈Z E |X l (z)| 4q 1/2q + σ 2 sup i≥1,l≥1 E |E il | 4q 1/2q</p><formula xml:id="formula_159">E ∥Σ∥ q Sq Z 1/q = 1 2p 2 E    p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z q Sq Z    1/q ≤ 1 2p 2 p j=1 E E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z q Sq Z 1/q (58) ≤ 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 q Sq Z 1/q (59) = 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2q S2q Z 1/q ≤ 1 2p 2 p j=1 2E Y j Y ⊤ j 2q S2q Z 1/2q 2<label>(60)</label></formula><p>, where the final inequality uses the facts that X j , Z and E are independent.</p><p>Combining the above estimates we find:</p><formula xml:id="formula_160">P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t</formula><p>Z 1 , . . . , Z n ≤ 2 q (2q -1) q t 2q 2 p q 4 q n 2q sup j≥1 sup z∈Z E |X j (z)| 4q 1/2q + σ 2 sup i≥1,j≥1 E |E ij | 4q 1/2q 2q = (16) q (2q -1) q n 2q t 2q 1 p q sup j≥1 sup z∈Z E |X j (z)| 4q 1/2q + σ 2 sup i≥1,j≥1 E |E ij | 4q 1/2q 2q , from which the result follows by the tower property of conditional expectation. Proposition 7. Assume A6 and A9 with some q ≥ 1. For any δ, ϵ ∈ (0, 1), if n ≥ 3σ 2 ϵc min λ ∨ log 1 δ + log(4r) 1 ϵ 2 2((c max λ ) 2 + ϵc max λ c min λ /9) (c min λ ) 2 /9 , Lemma 10. For any m 1 , m 2 ≥ 1 and any matrix norm ∥ • ∥ ⋆ on R m1×m2 , ∥ • ∥ ⋆ is convex. For any random A, B ∈ R m1×m2 and any 1 ≤ q &lt; ∞ such that E [∥A∥ q ⋆</p><formula xml:id="formula_161">] ∨ E [∥B∥ q ⋆ ] &lt; ∞, E [∥A + B∥ q ⋆ ] 1/q ≤ E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ] 1/q . ≤ E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ]</formula><p>1/q E ∥A + B∥ (q-1)( q q-1 ) ⋆</p><formula xml:id="formula_162">1-1 q = E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ] 1/q E [∥A + B∥ q ⋆ ] E [∥A + B∥ q ⋆ ] 1/q .</formula><p>The proof is completed by multiplying both sides by E [∥A + B∥ q ⋆ ] 1/q /E [∥A + B∥ q ⋆ ].</p><p>Lemma 11. Assume A6, and A9 with some q ≥ 1. Let U j denote the jth column of U Φ . Then there exists a constant b(q) depending only on q such that for any t &gt; 0, P max j=1,...,r</p><formula xml:id="formula_163">∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ t ≥ 1 - n 1+q r t 2q p q b(2q)2</formula><p>6q-1 max j=1,...,p sup z∈Z E |X j (z)| 4q + σ 4q max i=1,...,n,j=1,...,p E[|E ij | 4q ] .</p><p>Proof. The ith element of (p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j can be written in the form:</p><formula xml:id="formula_164">p -1 p k=1 ∆ ij (k)</formula><p>where</p><formula xml:id="formula_165">∆ ij (k) := Y (i) k Y ⊤ k U j -E Y (i) k Y ⊤ k U j Z 1 , . . . , Z n</formula><p>and for any i, j, the random variables ∆ ij (k), k = 1, . . . , p are conditionally independent and conditionally mean zero given Z 1 , . . . , Z n . Applying Markov's inequality, the Marcinkiewicz-Zygmund inequality and Minkowski's inequality, all conditionally on Z ≡ (Z 1 , . . . , Z n ), we have for any q ≥ 1 the following inequalities hold almost surely, Re-arranging the expression for ∆ ij (k), applying the Cauchy-Schwartz inequality and ∥U j ∥ 2 = 1, we estimate</p><formula xml:id="formula_166">P p -1 p k=1 ∆ ij (k) ≥ t Z ≤ 1 t 2q E   p -1 p k=1 ∆ ij (k)</formula><formula xml:id="formula_167">|∆ ij (k)| ≤ Y (i) k Y k -E Y (i) k Y k Z 2 ∥U j ∥ 2 ≤ |Y (i) k |∥Y k ∥ 2 + E |Y (i) k |∥Y k ∥ 2 Z</formula><p>and so </p><formula xml:id="formula_168">E |∆ ij (k)| 2q Z ≤ 2 2q E (|Y (i) k | 2 ∥Y k ∥ 2 2 ) q Z = 2 2q E n l=1 |Y (i) k | 2 |Y (l) k | 2 q Z ≤ 2 2q n l=1 E (|Y (i) k | 2 |Y (l) k | 2 ) q Z 1/q q ≤ 2 2q</formula><p>Combining the almost sure upper bounds ( <ref type="formula" target="#formula_169">64</ref>) and ( <ref type="formula">63</ref>), using the tower property of conditional expectation and then taking a union bound over i = 1, . . . , n and j = 1, . . . , r, we find:</p><formula xml:id="formula_170">P max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ t ≥ 1 - n 1+q r t 2q p q b(2q)2 6q-1 sup j≥1 sup z∈Z E |X j (z)| 4q + σ 4q sup i≥1,j≥1 E[|E ij | 4q ] ,</formula><p>which completes the proof.</p><p>E Supplementary figures for Section 4.2</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>24 of 72 Figure 1 :</head><label>721</label><figDesc>Figure 1: A collection of images reduced in dimension using PCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Planaria example. Left: first 2 dimensions of the PCA embedding. Right: representation of the data in 2 dimensions obtained by first reducing to 14 dimensions using PCA, then applying t-SNE.</figDesc><graphic coords="3,86.40,146.12,422.47,232.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Torus example. Left: grey wireframe of Z, a torus, with colour bars indicating coordinates with respect to two circles. Both the middle and right plots show the same n = 4000 points, Z 1 , . . . , Z 4000 , which are sampled uniformly on the torus, coloured by their coordinates with respect to each of the two circles.</figDesc><graphic coords="9,86.40,538.76,422.47,111.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Torus example. Blue: numerical shortest path lengths between points in M vs. between the corresponding points in Z. Red: theoretical scaling relationship √ 2.</figDesc><graphic coords="10,233.06,495.91,123.73,162.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>4 forAlgorithm 1 PCA dimension selection Input: data matrix Y ∈ R n×p .1: Split the data as Y (1) := Y 1:⌈n/2⌉,1:p , Y(2) := Y (⌈n/2⌉+1):n,1:p 2: for ρ ∈ {1, ..., min(n, p)} do3: Let V (1)ρ ∈ R p×ρ denote the matrix of orthogonal eigenvectors associated with the ρ largest eigenvalues of Y (1)⊤ Y(1)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: PCA dimension selection. Columns 1-4: different latent space/kernel configurations. 1-3 are finite rank, 4 infinite rank; configurations 3 and 4 are isometric. Row a: sampled positions (n = 500); b: first two principal components (p = 1000); c: the dimension selected by different methods, and the true rank when finite; d: error in geodesic distance and persistence diagram estimation (bottleneck distance) for the isometric configurations; e: persistence diagrams showing partial recovery of true topological features. Further details in main text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Images example. a) Wasserstein dimension selection; red vertical line indicates minimum at r = 11. b) Kernel density estimate for the magnitudes of the PCA embedding vectors. c) Persistence diagram shows evidence of a single "loop" in the embedding. d) Estimated kernel as a function of latent positions in angular form θ i = arctan(z (2) i /z</figDesc><graphic coords="19,86.40,86.40,422.49,302.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Single-cell transcriptomics example. a) histogram of inner products between distinct points in the PCA and random embeddings. b) average percentage increase in shortest path length in the minimum spanning tree compared to the k-nn graph, over different values of k. Results for the random embedding are shown in black, over 10 simulations with error bars indicated 2×standard error, c) comparing the shortest path lengths for samples in 10-nn graph and the MST.</figDesc><graphic coords="20,86.40,445.00,422.47,121.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Single-cell transcriptomics example. a) minimum spanning tree computed from the the spherically projected PCA embedding of the planaria data, colours indicate cell types. b) the class graph formed from the minimum spanning tree. All neoblast cell types are represented by a single dark grey node. The class subgraph consisting only of neoblast types is shown in the bottom right-hand corner inset.</figDesc><graphic coords="21,86.40,325.45,422.47,275.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Temperatures example. a) Wasserstein dimension selection; red line indicates minimum at r = 36. b) Kernel density estimate of the probability density of PC score magnitudes. c) The blue curve shows proportion of edges in common between embedding k-nn graph and geographic k-nn graph. The black line shows the mean proportion in common between the k-nn graph of a 100 uniformly random embeddings and the geographic k-nn graph. The red band indicates the range between maximum and minimum proportions across these 100 random embeddings.</figDesc><graphic coords="23,96.96,312.54,401.37,142.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Temperatures example. Locations of towns and cities are shown in red. The blue lines correspond to edges in the embedding k-nn graph G, with k = 5. The white circles highlight, from west to east: Edinburgh, U.K.; Baia Mare, Romania; and Novorossiysk, Russia.</figDesc><graphic coords="24,86.40,122.76,422.49,230.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Temperatures example. Shortest paths in the embedding k-nn graph G from Tallinn, Estonia, to all other towns and cities. Each shortest path is visualized as a spline, with knot points given by the geographic locations of its constituent towns and cities. The red dots highlight the shortest path from Tallinn to Tripoli, Libya.</figDesc><graphic coords="24,86.40,424.76,422.49,248.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>Figure 13: Mixture model example. Left: maximum errormax i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2, averaged over 50 independent realisations from the model, as a function of n and p. Right: the same error for p = 200, 1000, 15000, as a function of n.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Mixture model example. PCA embedding {p -1/2 ζ 1 , . . . , p -1/2 ζ n } (blue dots) and ϕ(1), ϕ(2), ϕ(3) (red dots). Top row: n fixed to 200 and p varying. Bottom row p fixed to 200 and n varying.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>2. Section D.1 contains definitions and notation used throughout section D. Various intermediate results used in the proof of theorem 4 are given in sections D.3-D.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>ELemma 7 .</head><label>7</label><figDesc>[YY ⊤ |Z 1 , . . . , Z n ] = E[XX ⊤ |Z 1 , . . . , Z n ] + σE[XE ⊤ |Z 1 , . . . , Z n ] + σE[EX ⊤ |Z 1 , . . . , Z n ] + σ 2 E[EE ⊤ |Z 1 , . . . , Z n ] = pΦΦ ⊤ + pσ 2 I n .Assume A6 and A9. Then there exists a constant c max λ &lt; ∞ depending only on the first supremum in A9, and a constant c min λ &gt; 0 such thatsup p≥1 sup z f (z, z) + λ f 1 ≤ c max λ , inf p≥1 λ f r ≥ c min λ .Proof. The existence of c min λ as required is an immediate consequence of A6. Using A9 and Jensen's inequality gives:sup z f (z, z) = sup z j (z)| 2 ] ≤ sup z j (z)| 4q ] 2/4q &lt; ∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>2 ] 5 (</head><label>25</label><figDesc>TheoremMatrix Bernstein inequality). Let M 1 , . . . , M n be independent random matrices with common dimensionsm 1 × m 2 satisfying E[M i ] = 0 and ∥M i ∥ 2 ≤ L for each 1 ≤ i ≤ n and some constant L. Let M := n i=1 M i and v(M) = max ∥E[MM ⊤ ]∥ 2 , ∥E[M ⊤ M]∥ 2 . Then for all t ≥ 0, P (∥M∥ 2 ≥ t) ≤ (m 1 + m 2 ) exp -t 2 /2 v(M) + Lt/3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>E</head><figDesc>|E ij | 2q .Proof. Let us write the matrix Y in terms of its columnsY ≡ [Y 1 | • • • |Y p ] so that:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>( 60 )=∥Y j ∥ 2 2 and λ k Y 1 Y ⊤ 1 = 0</head><label>6010</label><figDesc>holds by lemma 10 and the fact that Ỹj and Y j are equal in distribution.By definition of the Schatten-q norm, Yj Y ⊤ k Y j Y ⊤ j , where λ 1 Y j Y ⊤ jfor k = 2, . . . , n. Thus:(Z i ) + σE ij )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>EFigure 15 :</head><label>15</label><figDesc>Figure 15: First two coordinates of the data matrices corresponding to figure 6, showing much less structure than the principal components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>= 2</head><label>2</label><figDesc>2q n q max l=1,...,nE |X k (Z l ) + σE kl | 4q Z ≤ 2 6q-1 n q sup l≥1 sup z∈Z E |X l (z)| 4q + σ 4q sup l≥1, l≥1 E[|E l l| 4q ] .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proof. The convexity holds due to the fact that any norm must be absolutely homogeneous and satisfy the triangle inequality. For the second claim, sinceE [∥A∥ q ⋆ ] ∨ E [∥B∥ q ⋆ ] &lt; ∞ we have the preliminary estimate E [∥A + B∥ q ⋆ ] ≤ 2 q-1 (E [∥A∥ q ⋆ ] + E [∥B∥ q ⋆ ]) &lt; ∞. If E [∥A + B∥ q ⋆ ] = 0 then the desired inequality is trivial. So suppose E [∥A + B∥ q ⋆ ] &gt; 0.Using the triangle inequality for the norm and then Holder's inequality for the expectation,E [∥A + B∥ q ⋆ ] = E ∥A + B∥ ⋆ ∥A + B∥ q-1 ⋆ ≤ E (∥A∥ ⋆ + ∥B∥ ⋆ ) ∥A + B∥ q-1 ⋆ = E ∥A∥ ⋆ ∥A + B∥ q-1 ⋆ + E ∥B∥ ⋆ ∥A + B∥ q-1 ⋆</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Some linear algebra</head><p>Lemma 3. For any m 1 , m 2 ≥ 1, A ∈ R m2×m1 , q ≤ min{m 1 , m 2 } and strictly positive real numbers λ 1 , . . . , λ q , a) there exists U ∈ R m2×q such that U ⊤ U = I q and AA ⊤ U = UΛ, if and only if there exists V ∈ R m1×q such that V ⊤ V = I q and A ⊤ AV = VΛ, where Λ := diag(λ 1 , . . . , λ q ); b) when V with the properties stated in part a) exists, a choice of U which has the properties stated in part a) is</p><p>Proof. Assume the existence of V with the properties stated in part a). Taking U := AVΛ -1/2 we have</p><p>The implication in the other direction for part a) holds by interchanging A ⊤ and U with respectively A and V. We have thus proved parts a) and b) of the lemma. Part a) implies that the non-zero eigenvalues of A ⊤ A are equal to those of AA ⊤ , which establishes the claim of part c). Part d) follows from part c). Lemma 4. For any m 1 ≤ m 2 and A ∈ R m2×m1 such that A has rank m 1 , there exists an orthogonal matrix Q ∈ R m1×m1 such that UΛ 1/2 = AQ, where Λ = diag{λ 1 (AA ⊤ ), • • • , λ m1 (AA ⊤ )} and the columns of U ∈ R m2×m1 are orthonormal eigenvectors of AA ⊤ with eigenvalues λ 1 (AA ⊤ ), . . . , λ m1 (AA ⊤ ).</p><p>Proof. We have</p><p>We then find:</p><p>Consider the reduced singular value decomposition A = UΛ 1/2 V ⊤ where V ∈ R m1×m1 has orthonormal columns. Substituting into the r.h.s. of <ref type="bibr" target="#b53">(55)</ref>,</p><p>D.5 Some properties of the LMM Lemma 5. On the event that the rank of</p><p>Y , where the columns of U Y ∈ R n×r are orthonormal eigenvectors of p -1 YY ⊤ with associated eigenvalues on the diagonal of the diagonal matrix Λ Y ∈ R r×r .</p><p>Proof. Apply lemma 3, part b). and p ≥ 1 δ 1/q ϵ 2 2 1/q 16(2q -1) 9 (c min λ ) 2 c X (2q) 1/2q + σ 2 c E (2q) 1/2q 2 where c X , c E are as in lemma 9 and c max λ , c min λ are as in lemma 7, then</p><p>Proof. Throughout the proof we shall adopt the convention λ f i := 0 for all r + 1 ≤ i ≤ n and, in several places, we shall use the fact that λ i (ΦΦ ⊤ ) = 0 for r + 1 ≤ i ≤ n which holds since Φ ∈ R n×r .</p><p>Consider the following decomposition for any 1 ≤ i ≤ n:</p><p>Combining this decomposition with Weyl's inequality; the facts that for</p><p>and</p><p>Now fix any ϵ ∈ (0, 1). We have</p><p>where the second inequality holds by using λ f r ≤ λ f i for i = 1, . . . r, together with <ref type="bibr" target="#b60">(62)</ref> and the condition of the proposition n ≥ 3σ 2 /(ϵλ f r ); and the third inequality holds by applying lemma 8 and lemma 9 and using λ f r ≥ c min λ . The proof is completed by re-arranging each of the two following inequalities:   The curves are shifted and rescaled so that their maxima and minima agree.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and dependence</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Agterberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lubberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4618" to="4650" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representations for partially exchangeable arrays of random variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Aldous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="581" to="598" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Regression on manifolds: Estimation of the exterior derivative</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Aswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Tomlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="81" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Minimax rates for homology inference</title>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alesandro</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Sheehy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Mira</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<ptr target="https://users.math.msu.edu/users/iwenmark/Teaching/MTH995/Papers/MMod_BSLT00.pdf,2000.online" />
		<title level="m">Graph approximations to geodesics on embedded manifolds</title>
		<imprint>
			<date type="published" when="2022-03-14">14th March, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Local polynomial regression on unknown manifolds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes-Monograph Series</title>
		<imprint>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Topological equivalence</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bing</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2308625" />
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<idno type="ISSN">00029890</idno>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="4" to="7" />
			<date type="published" when="1960">1960. 19300972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Why deep learning works: A manifold disentanglement perspective</title>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Pratik Prabhanjan Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1997" to="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear manifold learning for visual speech recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Omohundro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="494" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A course in metric geometry</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Burago</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The two-to-infinity norm and singular subspace geometry with applications to high-dimensional statistics</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Cape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2405" to="2439" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Topology and data</title>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="308" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On local intrinsic dimension estimation and its applications</title>
		<author>
			<persName><forename type="first">Kevin M</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raviv</forename><surname>Raich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="650" to="663" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithms for manifold learning. Univ. of California at San Diego Tech</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Cayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1-17</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An introduction to topological data analysis: fundamental and practical aspects for data scientists</title>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Persistence-based clustering in Riemannian manifolds</title>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Chazal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><forename type="middle">Y</forename><surname>Oudot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Primoz</forename><surname>Skraba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local linear regression on manifolds and its geometric interpretation</title>
		<author>
			<persName><forename type="first">Ming-Yen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hau-Tieng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">504</biblScope>
			<biblScope unit="page" from="1421" to="1434" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving diffusion models for inverse problems using manifold constraints</title>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongsu</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dohoon</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25683" to="25696" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convergence of denoising diffusion models under the manifold hypothesis</title>
		<author>
			<persName><forename type="first">Valentin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bortoli</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Riemannian score-based generative modelling</title>
		<author>
			<persName><forename type="first">Emile</forename><surname>Valentin De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2406" to="2422" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Umap reveals cryptic population structure and phenotype heterogeneity in large genomic cohorts</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Diaz-Papkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Anderson-Trocmé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chief</forename><surname>Ben-Eghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Gravel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS genetics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1008432</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Persistent homology-a survey</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="257" to="282" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Manifold diffusion fields</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elhag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista Martin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2305.15586" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Confidence sets for persistence diagrams</title>
		<author>
			<persName><forename type="first">Terese</forename><surname>Brittany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Fasy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lecci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Rinaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaraman</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aarti</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="2301" to="2339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Testing the manifold hypothesis</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Fefferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Mitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hariharan</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="983" to="1049" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Toroidal topology of population activity in grid cells</title>
		<author>
			<persName><forename type="first">J</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Hermansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Pachitariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">A</forename><surname>Burak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">A</forename><surname>Baas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">May-Britt</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edvard I</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><surname>Moser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Manifold estimation and singular deconvolution under Hausdorff loss</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Christopher R Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Perone-Pacifico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Verdinelli</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="941" to="963" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Minimax manifold estimation</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Christopher R Genovese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabella</forename><surname>Perone-Pacifico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Verdinelli</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1263" to="1291" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Amsterdam library of object images</title>
		<author>
			<persName><forename type="first">Jan-Mark</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gertjan</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">Wm</forename><surname>Smeulders</surname></persName>
		</author>
		<ptr target="https://aloi.science.uva.nl" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2005-03">2005. March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with dot products recovers hidden tree structure</title>
		<author>
			<persName><forename type="first">Annie</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Modell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rubin-Delanchy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Whiteley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploring network structure, dynamics, and function using networkx</title>
		<author>
			<persName><forename type="first">Aric</forename><surname>Hagberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Swart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Chult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Los Alamos, NM (United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Los Alamos National Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geometric representation of high dimension, low sample size data</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Stephen Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Neeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="427" to="444" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yutong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieh-Hsin</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhta</forename><surname>Takida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshimitsu</forename><surname>Uesaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hsiang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.16424</idno>
		<title level="m">Manifold preserving guided diffusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Intrinsic dimensionality estimation of submanifolds in rd</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">When and why are principal component scores a good tool for visualizing high-dimensional data?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kristoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magne</forename><surname>Hellton</surname></persName>
		</author>
		<author>
			<persName><surname>Thoresen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="597" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stochastic neighbor embedding</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient, high-quality force-directed graph drawing</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematica journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="71" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<idno type="arXiv">arXiv:2303.07920</idno>
		<title level="m">Svante Janson. Real trees</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the distribution of the largest eigenvalue in principal components analysis</title>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="327" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On consistency and sparsity for principal components analysis in high dimensions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">486</biblScope>
			<biblScope unit="page" from="682" to="693" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PCA consistency in high dimension, low sample size context</title>
		<author>
			<persName><forename type="first">Sungkyu</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stephen Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6B</biblScope>
			<biblScope unit="page" from="4104" to="4130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Intrinsic dimension estimation using packing numbers</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Samory Kpotufe. k-NN regression adapts to local intrinsic dimension</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A tree-based regressor that adapts to intrinsic dimension</title>
		<author>
			<persName><forename type="first">Samory</forename><surname>Kpotufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1496" to="1515" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Correlation between genetic and geographic structure in europe</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Nothnagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Junge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Freitag-Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amke</forename><surname>Caliebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslava</forename><surname>Balascakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaume</forename><surname>Bertranpetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>Bindoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="1241" to="1248" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic non-linear principal component analysis with Gaussian process latent variable models</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for visualisation of high dimensional data</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A unifying probabilistic perspective for spectral dimensionality reduction: Insights and new models</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1609" to="1638" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Human motion synthesis by motion manifold learning and motion primitive segmentation</title>
		<author>
			<persName><forename type="first">Chan-Su</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Articulated Motion and Deformable Objects</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convergence and prediction of principal component scores in high-dimensional settings</title>
		<author>
			<persName><forename type="first">Seunggeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3605</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of intrinsic dimension</title>
		<author>
			<persName><forename type="first">Elizaveta</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bickel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Extrinsic Gaussian processes for regression and classification on manifolds</title>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niu</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pokman</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="887" to="906" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Optimality of spectral clustering in the gaussian mixture model</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Löffler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2506" to="2530" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Combining eigenvalues and variation of eigenvectors for order determination</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="875" to="887" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Community detection and classification in hierarchical stochastic blockmodels</title>
		<author>
			<persName><forename type="first">Vince</forename><surname>Lyzinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngser</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="26" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<title level="m">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type and their connection with the theory of integral equations</title>
		<author>
			<persName><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philos. Transactions Royal Soc</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="4" to="415" />
			<date type="published" when="1909">1909</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Decoding stem cells: An overview on planarian stem cell heterogeneity and lineage progression</title>
		<author>
			<persName><forename type="first">Dolores</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Cebrià</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomolecules</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1532</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Manifold learning-based methods for analyzing single-cell RNA-sequencing data</title>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">S</forename><surname>Kevin R Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smita</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Systems Biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="36" to="46" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Valuations, trees, and degenerations of hyperbolic structures</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">B</forename><surname>Shalen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="476" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adaptive approximation and generalization of deep neural network with intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">Ryumei</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Imaizumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7018" to="7055" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Intrinsic Gaussian processes on complex constrained domains</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pokman</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="603" to="627" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Finding the homology of submanifolds with high confidence from random samples</title>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="419" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Genes mirror geography within Europe</title>
		<author>
			<persName><forename type="first">John</forename><surname>Novembre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Bryc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Kutalik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Boyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Auton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">S</forename><surname>Indap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew R Nelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="issue">7218</biblScope>
			<biblScope unit="page" from="98" to="101" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Manifold learning algorithms for localization in wireless sensor networks</title>
		<author>
			<persName><forename type="first">Neal</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE international conference on acoustics, speech, and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">857</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Asymptotics of sample eigenstructure for a large dimensional spiked covariance model</title>
		<author>
			<persName><forename type="first">Debashis</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="1617" to="1642" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lester</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efron-Stein inequalities for random matrices</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="3431" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Dimension Theory of General Spaces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pears</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Score-based generative models detect manifolds</title>
		<author>
			<persName><forename type="first">Jakiw</forename><surname>Pidstrigach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="35852" to="35865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cell type atlas and lineage tree of a whole complex animal by single-cell transcriptomics</title>
		<author>
			<persName><forename type="first">Mireya</forename><surname>Plass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Solana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salah</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristotelis</forename><surname>Misios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Glažar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Obermayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Kocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Rajewsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6391</biblScope>
			<biblScope unit="page">1723</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A survey of manifold learning for images</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Souvenir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="83" to="94" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Planarian stem cells specify fate yet retain potency during the cell cycle</title>
		<author>
			<persName><forename type="first">Amelie</forename><forename type="middle">A</forename><surname>Raz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Wurtzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Reddien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Stem Cell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1307" to="1322" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction and mapping of compound libraries for drug discovery</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Reutlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gisbert</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Graphics and Modelling</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="108" to="117" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Manifold structure in graph embeddings</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rubin-Delanchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11687" to="11699" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Marron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.2679</idno>
		<title level="m">High dimensional principal component scores and data visualization</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongtu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Marron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1303.6171</idno>
		<title level="m">Surprising asymptotic conical structure in critical sample eigen-directions</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><surname>Stanczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Batzolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teo</forename><surname>Deveney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carola-Bibiane</forename><surname>Schönlieb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.12611</idno>
		<title level="m">Your diffusion model secretly knows the dimension of the data manifold</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Support vector machines</title>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Christmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Introduction to metric and topological spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Sutherland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">On the speed of uniform convergence in mercer&apos;s theorem</title>
		<author>
			<persName><forename type="first">Rustem</forename><surname>Takhanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">126718</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Universally consistent vertex classification for latent positions graphs</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Daniel L Sussman</surname></persName>
		</author>
		<author>
			<persName><surname>Priebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1406" to="1430" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Bayesian Gaussian process latent variable model</title>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">py: A lean persistent homology library for python</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Tralie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rann</forename><surname>Bar-On</surname></persName>
		</author>
		<author>
			<persName><surname>Ripser</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00925</idno>
		<ptr target="https://doi.org/10.21105/joss.00925" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">925</biblScope>
			<date type="published" when="2018-09">Sep 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">An introduction to matrix concentration inequalities</title>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="230" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokcen</forename><surname>Trosset</surname></persName>
		</author>
		<author>
			<persName><surname>Buyukbas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10858</idno>
		<title level="m">Rehabilitating Isomap: euclidean representation of geodesic structure</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Why are big data matrices approximately low rank?</title>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Udell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Townsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="144" to="160" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Single-cell analysis reveals functionally distinct classes within the planarian stem cell compartment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Josien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Van Wolfswinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><surname>Reddien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell stem cell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="326" to="339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Topological data analysis</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="501" to="532" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Learning a kernel matrix for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Kilian Q Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">106</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Matrix factorisation and the interpretation of geodesic distance</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Whiteley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Rubin-Delanchy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Scanpy: large-scale single-cell gene expression data analysis</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">On complete convergence for weighted sums of-mixing random variables</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Xuejun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Shuhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wenzhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Inequalities and Applications</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Bayesian manifold regression</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="876" to="905" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">PCA consistency for non-Gaussian data in high dimension, low sample size context</title>
		<author>
			<persName><forename type="first">Kazuyoshi</forename><surname>Yata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Aoshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics: Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="2634" to="2652" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Effective PCA for high-dimension, low-sample-size data with noise reduction via geometric representations</title>
		<author>
			<persName><forename type="first">Kazuyoshi</forename><surname>Yata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Aoshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of multivariate analysis</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="215" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Geometric consistency of principal component scores for high-dimensional mixture models and its application</title>
		<author>
			<persName><forename type="first">Kazuyoshi</forename><surname>Yata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Aoshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="899" to="921" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Automatic dimensionality selection from the scree plot via the use of profile likelihood</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
