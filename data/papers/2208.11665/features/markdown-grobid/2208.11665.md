# Statistical exploration of the Manifold Hypothesis

## Abstract

## 

The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -the Latent Metric Model -via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known, scaleable graph-analytic algorithms.

## Introduction

The manifold hypothesis is a widely accepted tenet of Machine Learning which posits that [[16]](#b14): "...the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space".

This phenomenon has impacted a wide range of methods and algorithms. Presence of manifold structure is the premise of manifold estimation and testing [[27,](#b25)[30,](#b28)[29]](#b27), nonlinear dimension reduction techniques [[78,](#b76)[89,](#b87)[38,](#b36)[6,](#b4)[98,](#b96)[95,](#b93)[59]](#b57), intrinsic dimension estimation [[45,](#b43)[54,](#b52)[36,](#b34)[15]](#b13), and regression and classification techniques specially adapted to settings in which covariates are valued on manifolds [[8,](#b6)[4,](#b2)[19,](#b17)[102,](#b100)[55,](#b53)[65]](#b63). Assumptions that data are concentrated near lowdimensional topological or geometric structures underpin clustering techniques and topological data analysis [[24,](#b22)[66,](#b64)[14,](#b12)[5,](#b3)[18,](#b16)[17]](#b15). Some nonparametric techniques, such as nearest neighbour or tree-based regression methods, function without manifold structure necessarily being present, but benefit significantly when it is there, since their convergence rates depends on intrinsic rather than ambient dimension of covariates [[46,](#b44)[47]](#b45). It has been proved that deep neural networks exhibit a similar property [[64]](#b62). More broadly, the presence of manifold structure has been suggested as a key factor in the success of deep learning methods [[10]](#b8). Assumptions that data lie on a low-dimensional manifold embedded in high-dimensional space are central to very recent practical and theoretical developments in generative modelling in Artificial Intelligence, especially diffusion models [[82,](#b80)[83,](#b81)[39,](#b37)[21,](#b19)[22,](#b20)[84,](#b82)[20,](#b18)[73,](#b71)[35,](#b33)[25]](#b23).

Why might manifold structure be present in data? In some situations, such as image analysis, an intuitive albeit heuristic explanation can be given in terms of the physical mechanism which generated the data (see e.g., Pless and Souvenir [[75]](#b73) for a review of manifold estimation in this context). Figure [1](#) shows 24 grayscale images of a car, a subset of n = 75 images from [[31]](#b29), taken from angles 0, 5, 10, . . . , 355 degrees around the circumference of a circle. Each image is of resolution 384 × 288 pixels and so can be represented as a vector of length p = 110592. However, at least intuitively, we can account for the variation across the collection of images using far fewer dimensions, in terms of the position of the camera in the three-dimensional space of the world around us. Figure [1](#) shows the result of using principal component analysis (PCA) to reduce dimension, upon which we make the following observations. The first 20 principal components account for 91.5% of the total variance, suggesting that the data are concentrated somewhere in a low-dimensional linear subspace of R 110592 . The first three dimensions -the coordinates of the data with respect to the eigenvectors associated with the three largest eigenvalues -produce points around a loop which is somewhat irregular in shape but resembles the circle of camera positions, subject to deformation by bending and twisting. The points appear roughly equally spaced around the loop, like the camera positions which are equally spaced at intervals of 5 degrees around a circle.

Evidently reducing the dimension of these image data by PCA allows us to access some of the geometric structure of the data generating mechanism, but questions remain. We have chosen to plot the first three dimensions for ease of visualisation, is this a "good" choice? What might the other dimensions convey? What explains the precise shape of the loop and the spacing of the points along it, relative to the underlying circle of camera positions?

In other situations, embedded topological and geometric structure may appear in different forms and have different interpretations. Figure [2](#fig_1) shows two approaches to visualising expression levels of p = 5821 genes measured across n = 5000 individual cells from an adult planarians, a type of flatworm. In the field of single-cell transcriptomics -as set out in the 2018 Science paper [[74]](#b72) -such data offer the possibility of discovering the cell lineage tree of an entire animal: the aim is to find out if the data reflect the tree-structured process by which stem cells differentiate into a variety of distinct cell types. These data were prepared using the Python package Scanpy [[100]](#b98) following the methods of [[74]](#b72).

The left plot in figure [2](#fig_1) shows the result of dimension reduction from 5821 to 2 using PCA. The right plot shows the result of first reducing from 5821 to 14 dimensions using PCA, followed by reduction to 2 dimensions using t-SNE [[95]](#b93), a very popular nonlinear dimension reduction method which finds a lower dimensional representation of a data set by minimising a particular measure of distortion of pairwise distances. We used the default t-SNE parameter settings in scikit-learn. In both plots, the points are coloured by cell type, but neither PCA nor t-SNE have access to this information. Similarly to figure [1](#), it is evident from figure [2](#fig_1) that performing some form of dimension reduction allows us to access structure underlying the data, albeit in the form of discrete cell types rather than the geometry of camera positions. In figure [1](#), using only PCA to reduce dimension was enough to make this structure visible. However, in figure [2](#fig_1), using only PCA and reducing to 2 dimensions, distinct cell types are not clearly separated, whereas PCA down to 14 dimensions followed by t-SNE seems to be more effective. The t-SNE visualisation hints at the presence of tree structure underlying the data, with some areas having branch-like arms originating at the central point cloud, but other lineages lack clarity or seem to be disconnected. Could we combine methods differently to obtain a clearer picture? These examples illustrate just some of the ways in which underlying structure can manifest itself in embedded topological and geometric patterns in data. Many other examples can be found: in genomics, where genotyping DNA sites has revealed striking geographic patterns [[67,](#b65)[48,](#b46)[23]](#b21); neuroscience, where simultaneous recordings from Grid cells have been shown to exhibit toroidal structure seemingly independent of behavioural tasks [[28]](#b26); as well as manifold structure in data from wireless sensor networks [[68]](#b66), visual speech recognition [[11]](#b9), drug discovery [[77]](#b75), RNA sequencing [[62]](#b60), and human motion synthesis [[52]](#b50).

In this work we put forward a perspective that embedded topological and geometric structure in data can be explained as a general statistical phenomenon, without reference to physical properties or other domain-specific details of the data generating mechanism.

Main contributions. Our first main contribution is to propose a simple and generic statistical model which produces hidden, low-dimensional manifold structure in high-dimensional data, thus providing a statistical justification for the manifold hypothesis.

Our second main contribution is to describe how this hidden manifold relates to a true latent domain defined by the model, explaining, for example, why the points in the right panel of figure [1](#) are not in a perfect circle, as the camera positions are, but still form a loop. More precisely, we give mild conditions under which the relationship between the manifold and the latent domain is a homeomorphism (a topological equivalence), and stronger conditions under which it becomes an isometry (a metric equivalence).

Our third main contribution is to show how to combine simple or well-known techniques, broadly relating to manifold-learning, to explore hypotheses and uncover information about the latent domain and broader data generating mechanism. Given data vectors Y 1 , . . . , Y n ∈ R p , we rationalise the following workflow: 2. Linear dimension reduction of Y 1 , . . . , Y n by PCA, resulting in an r-dimensional embedding, ζ 1 , . . . , ζ n .

3. Spherical projection of the embedding, setting ζ sp i := ζ i /∥ζ i ∥, i = 1, . . . , n 4. Nearest neighbour graph construction from ζ sp 1 , . . . , ζ sp n . 5. Analysis and visualisation of the nearest neighbour graph, e.g., shortest paths, minimum spanning tree, topology.

These recommended steps are supported by new theory and empirical evidence, providing a general-purpose mechanism for geometric data exploration under minimal assumptions. The remainder of this article is structured as follows. In Section 2 we introduce the Latent Metric Model, and the associated manifold M, which arises as a consequence of correlation over a latent domain Z. In section 3 we describe how this manifold structure hides in the data and how the manifold relates to Z. We establish a representation formula (proposition 1) uncovering the perhaps surprising fact that, under the Latent Metric Model, data are noisy, random projections of points in M. Standard statistical concepts, such as stationarity, give rise to striking geometric relationships between M and Z, such as isometry. In section 4 we develop theory and methodology supporting the workflow above, elucidating the benefits of applying PCA (theorem 1), proposing a new dimension selection method, and more. In Section 5 we demonstrate the workflow on real data, revisiting the image and transcriptomics data from section 1, as well as a temperature time series example. The key new feature of these analyses is that we can explore manifold hypotheses grounded in a statistical model.

## The Latent Metric Model

The Latent Metric Model (LMM) is constructed from three independent sources of randomness.

Latent Variables. Z 1 , . . . , Z n are independent and identically distributed random elements of a metric space (Z, d Z ), that is Z is a set, and d Z (•, •) is a distance function on Z. It is assumed that the metric space (Z, d Z ) is compact, and Z 1 , . . . , Z n are distributed according to a Borel probability measure µ supported on Z.

Random Functions. X 1 , . . . , X p are random R-valued functions, each with domain Z. That is, for each z ∈ Z and j = 1, . . . , p, X j (z) is an R-valued random variable. It is not assumed that X 1 , . . . , X p are identically distributed, but it is assumed that E[|X j (z)| 2 ] < ∞, for all j = 1, . . . , p and z ∈ Z.

Noise. E ∈ R p×n is a matrix of random variables whose elements are each zero-mean and unitvariance. The columns of E are assumed independent and elements in distinct rows of E are assumed pairwise uncorrelated.

The data matrix Y ∈ R n×p is defined by:

$Y ij := X j (Z i ) + σE ij (1)$for some σ ≥ 0. It will sometimes be convenient to think of data vectors Y 1 , . . . ,

$Y n ∈ R p such that [Y 1 | • • • |Y n ] ⊤ ≡ Y, so Y ij is the jth element of Y i . Similarly we shall write noise vectors [E 1 | • • • |E n ] ⊤ ≡ E.$We call:

$f (z, z ′ ) := 1 p p j=1 E[X j (z)X j (z ′ )](2)$the mean correlation kernel associated with the LMM. The following assumption is taken to hold throughout the paper without further mention.

## A1. For each

$j = 1, . . . , p, E[X j (z)X j (z ′ )] is a continuous function of (z, z ′ ) ∈ Z × Z.$Assumption A1 implies f (z, z ′ ) is continuous in z, z ′ , and by a generalisation of Mercer's theorem [[60,](#b58)[85,](#b83)[Thm 4.49]](#) given in section A, when A1 holds there exists a countable collection of non-negative real numbers

$(λ f k ) k≥1 , λ f 1 ≥ λ f 2 ≥ • • • , and a sequence of functions (u f k ) k≥1 which are orthonormal in L 2 (µ) such that f (z, z ′ ) = ∞ k=1 λ f k u f k (z)u f k (z ′ ) = ⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 ,(3)$where the series converges absolutely and uniformly in z, z ′ . Here ϕ is the "feature map":

$ϕ(z) := (λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • ⊤ .(4)$We stress two points. First, the central purpose of the LMM is to explain and describe manifold structure in data as a general statistical phenomenon. The breadth of this objective necessitates a flexible modelling paradigm and, in particular, we do not make specific distributional or functional assumptions, such as Gaussian errors or that the kernel is squared exponential. The assumptions in this paper, which are always clearly indicated, involve more general concepts, such as continuity, smoothness or stationarity. Second, we stress the perspective here that f and ϕ are derived quantities, defined implicitly by the ingredients of the LMM, rather than model parameters or hyperparameters whose values need to be chosen. The inner product on the r.h.s. of ( [3](#formula_4)) is ⟨x,

$x ′ ⟩ ℓ2 := ∞ k=1 x k x ′ k , between infinitely long vectors x = [x 1 x 2 • • • ] ⊤ belonging to ℓ 2 := {x ∈ R N : ∥x∥ ℓ2 < ∞}, where ∥x∥ ℓ2 := ∞ k=1 |x k | 2 1/2 = ⟨x, x⟩ 1/2 ℓ2 . The image of ϕ, that is M := {ϕ(z); z ∈ Z} ,(5)$$is a subset of ℓ 2 , because Z is compact and z → f (z, z) is continuous under A1, so sup z∈Z ∥ϕ(z)∥ 2 ℓ2 = sup z∈Z f (z, z) < ∞.$We denote by r the rank of f , that is the largest k ≥ 1 such that λ f k > 0, with r := ∞ if λ f k > 0 for all k ≥ 1. When r < ∞ we abuse notation slightly by writing

$ϕ(z) := (λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • (λ f r ) 1/2 u f r (z) ⊤ .(6)$Some special cases of the LMM are detailed in appendix B. It is shown there that in a setting where Z is a subset of Euclidean space and X 1 , . . . , X p are deterministic linear functions, the LMM reduces to the spiked-covariance model [[42,](#b40)[69]](#b67), which is the de-facto standard model under which to study the theoretical properties of PCA in high dimensions. In the case that Z is a subset of Euclidean space and X 1 , . . . , X p are independent and identically distributed Gaussian processes, the LMM reduces to the Gaussian Process Latent Variable Model introduced by Lawrence [[50]](#b48) and Lawrence and Hyvärinen [[49]](#b47) to facilitate probabilistic nonlinear dimension reduction. Appendix A also examines the situation in which Z is a set containing finitely many points, in which case the LMM reduces to a form of finite mixture model.

The LMM deviates from the ubiquitous assumption in Statistics that data are independent and identically distributed; from the definitions above, the "noise-free" data vectors

$Y i -σE i ≡ [X 1 (Z i ) • • • X p (Z i )] ⊤ , i = 1, . . . ,$n, are exchangeable but not independent. We shall see that this dependence, combined with the latent structure of the LMM, are key to the emergence of manifold structure in high dimensions. Without such dependence, the behaviour of data can be explained by high dimension, low sample size (HDLSS) asymptotics, introduced in the seminal JRSSB paper of Hall et al. [[34]](#b32). HDLSS asymptotics are based on an argument that if Y i and Y j are i.i.d. random vectors whose elements satisfy suitable weak dependence and moment conditions, then p -1 ∥Y i -Y j ∥ 2 converges to a constant as p → ∞. In [[34]](#b32) this leads to a conclusion that i.i.d. high-dimensional data vectors tend to lie deterministically at the vertices of a simplex. In section 3 we set out a different, much richer conclusion about the behaviour of p -1 ∥Y i -Y j ∥ 2 , arising from the LMM.

## Connecting statistical and geometric properties of the LMM

In this section we explain how statistical properties of the LMM allow us to connect the geometry of the data vectors, Y 1 , . . . , Y n , which can be thought of as a point cloud in R p , to the structure of M, and in turn the latent metric space Z. This is important for two reasons. Firstly, it shows how manifold structure in data emerges from elementary statistical properties of the LMM, thus clarifying in what sense and why the Manifold Hypothesis holds. Secondly, it forms the basis for data analysis procedures we detail in section 4. We proceed in four main steps:

• Section 3.1 shows how inner-products between data vectors, say Y i , Y j , relate to inner products between ϕ(Z i ), ϕ(Z j ). Since ϕ(Z 1 ), . . . , ϕ(Z n ) are i.i.d. and valued in M, recall [(5)](#b3), this gives our first indication that the geometry of the point cloud Y 1 , . . . , Y n will reflect the shape of M.

• Section 3.2 shows that under a simple distinguishability assumption, the feature map ϕ is a homeomorphism. Informally, this means we can think of M as being equivalent to Z up to some continuous, invertible distortion such as bending, twisting or stretching. Formally, we can say M is a topological manifold.

• Section 3.3 shows that when Z is a subset of Euclidean space, conditions closely related to weak stationarity of the random function X j imply ϕ is an isometry. This means a very special form of geometric relationship holds between M and Z, in which distances between points in Z, say Z i and Z j , are faithfully represented by distances measured along the manifold M between ϕ(Z i ) and ϕ(Z j ), rather than by straight-line distances of the form ∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 .

• Section 3.4 shows that if the kernel is sufficiently smooth, most of the structure of M is captured in a low-dimensional subspace. This hints towards the potential effectiveness of PCA (step 2 in the workflow) for manifold exploration.

Remarkably, we shall draw the conclusions in the second and third points above without any explicit knowledge of the eigenvalues and eigenfunctions which appear in the definition of ϕ, and which thus define M.

## Relating data inner products to feature map inner products

We have not made any assumptions about the functional form of z → X j (z), j = 1, . . . , p, in the LMM, other than A1. Nevertheless, the following proposition shows that a linear relationship holds between Y i -σE i and ϕ(Z i ).

Proposition 1. Under the LMM with r ∈ {1, 2, . . . , } ∪ {∞}, the matrix W ∈ R p×r with elements

$W jk := 1 (pλ f k ) 1/2 Z X j (z)u f k (z)µ(dz)(7)$satisfies

$Y i m.s. = p 1/2 Wϕ(Z i ) + σE i , i = 1, . . . , n, E[W ⊤ W] = I r ,(8)$where I r is the identity matrix with r rows and columns.

The qualification " m.s.

= " in [(8)](#b6) indicates that the infinite summations constituting the matrix-vector product Wϕ(Z i ) in the case r = ∞ converge in the mean-square sense. The proof of proposition 1, in appendix C, entails a generalised form of Karhunen-Loève expansion of X 1 , . . . , X p .

The identities in (8) can be interpreted as meaning that p -1/2 Y i is a noisy, random projection of ϕ(Z i ). Indeed we can use [(8)](#b6) together with the defining properties of the LMM in section 2 to describe the behaviour of the inner-product between Y i , Y j ∈ R p when the randomness in X 1 , . . . , X p and E is averaged out:

$1 p E[⟨Y i , Y j ⟩|Z i , Z j ] = ⟨ϕ(Z i ), E[W ⊤ W]ϕ(Z j )⟩ ℓ2 + 0 + 0 + σ 2 1 p E[⟨E i , E j ⟩] = ⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 + σ 2 I[i = j].(9)$The quantity p -1 ⟨Y i , Y j ⟩ is an arithmetic mean of p random variables. If, conditionally on Z i , Z j , the summands in p -1 ⟨Y i , Y j ⟩ are weakly dependent and have moments bounded uniformly in p, then by a law of large numbers argument p -1 ⟨Y i , Y j ⟩ will be close to its conditional expectation [(9)](#b7) with high probability when p is large (see proposition 6 in appendix C for details). Moreover, write W ≡ [W 1 | • • • |W p ] ⊤ and note from (7) that the only randomness in W j arises from X j . So if X 1 , . . . , X p were assumed weakly dependent, W 1 , . . . , W p would be too. Then, again assuming moments bounded uniformly in p, by a law of large numbers argument the sum p j=1 W j W ⊤ j will be close to its expectation with high probability when p is large, i.e.,

$W ⊤ W = p j=1 W j W ⊤ j ≈ E[W ⊤ W] = I r .$We therefore conclude that, subject to suitable weak dependence and moment conditions,

$p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] → 0, as p → ∞,(10)$in probability. In this sense the geometry of the collection of high-dimensional data vectors Y 1 , . . . , Y n reflects that of ϕ(Z 1 ), . . . , ϕ(Z n ), subject to some distortion depending on the noise level σ. Moreover, if [(10)](#b8) holds, then

$|p -1 ∥Y i -Y j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ 2 ℓ2 -2σ 2 | → 0 as p → ∞.$This stands in contrast to the behaviour established in [[34]](#b32) that p -1 ∥Y i -Y j ∥ 2 → const. when Y i and Y j are independent and identically distributed.

In section 4.1, we shall complement the above reasoning with theorem 1 which shows that when the noise level σ is fixed, and n → ∞ and p/n → ∞ simultaneously, using PCA to reduce dimension of Y 1 , . . . , Y n allows ϕ(Z 1 ), . . . , ϕ(Z n ) to be recovered, up to an orthogonal transformation.

## Relating distinguishability of latent variables to homeomorphism

A homeomorphism between two metric spaces is a mapping which is continuous, bijective and has a continuous inverse. If such a mapping exists the two metric spaces are said to be homeomorphic, or topologically equivalent. To develop some intuition, one can think about the case in which the metric spaces in question are subsets of the three dimensional Euclidean world around us. In this situation mappings which qualify as homeomorphisms include transformations of shape by bending, twisting, stretching and folding, but not cutting, puncturing or joining [[9]](#b7). Topological equivalence implies the two metric spaces in question must exhibit the same number of connected components, the same number of 1-dimensional loops and more generally the same number of kdimensional "holes" as each other. Detecting such features using data is the purpose of persistent homology methods within the field of Topological Data Analysis [[14,](#b12)[17]](#b15). But there is more to a topological structure than its homology; for example, in the transcriptomics application (introduction and Section 5.2), the hypothesized underlying structure has interesting, 'tree-like', topology but no interesting homology.

We shall now see that, with only a little more structure added to the LMM, ϕ is homeomorphism between Z and M, where the distance on M is ∥ • -• ∥ ℓ2 . The first requirement, continuity of ϕ, means that d Z (z, z ′ ) → 0 implies ∥ϕ(z) -ϕ(z ′ )∥ ℓ2 → 0. This holds due to the identities:

$∥ϕ(z) -ϕ(z ′ )∥ 2 ℓ2 = ∥ϕ(z)∥ 2 ℓ2 + ∥ϕ(z ′ )∥ 2 ℓ2 -2⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 = f (z, z) + f (z ′ , z ′ ) -2f (z, z ′ ),$combined with continuity of f under A1. By its definition, ϕ : Z → M is automatically surjective, and if ϕ is one-to-one, its inverse is automatically continuous due to a general result in the theory of metric spaces [[86,](#b84)[Prop. 13.26]](#) concerning the inverse of a continuous mapping with compact domain. The question of whether or not ϕ is a homeomorphism thus reduces to whether or not it is one-to-one. Consider the following assumption.

A2. For each z, z ′ ∈ Z such that z ̸ = z ′ , there exists ξ ∈ Z such that f (z, ξ) ̸ = f (z ′ , ξ).

Assumption A2 can be interpreted as a "distinguishability" condition, requiring that points in Z can be distinguished from each other via the kernel f , and furthermore: Proposition 2. ϕ : Z → M is a homeomorphism if and only if A2 holds.

The proof of proposition 2 is in appendix C. The term topological manifold conventionally means some set such that each point in that set has a neighbourhood which is homeomorphic to some subset of Euclidean space. We note that the relationship between M and Z is of a similar nature, except that M is globally rather than only locally homeomorphic to Z, and the metric space Z need not be Euclidean. Putting these differences aside, we shall simply call M a manifold from now on.

When M and Z are homeomorphic, they must have the same covering dimension-see [[71,](#b69)[Ch.3]](#) for background-this is an abstract topological notion dimension, which generalises the usual notion of dimension of Euclidean space. In this sense, we can say that when Z is low-dimensional, M is low-dimensional too.

## Relating stationarity to isometry

Weak stationarity of any one of the random functions X j in the LMM would mean that:

$• E[X j (z)] is constant in z, and • E[(X j (z) -E[X j (z)])(X j (z ′ ) -E[X j (z ′ )])$] is a function only of distance between z and z ′ . If all the random functions X 1 , . . . , X p were to have this property, it would follow from the definition of f in (2) that f (z, z ′ ) must also be a function only of distance between z and z ′ . We shall now see that this leads to an isometric relationship between M and Z. To define isometry, it's convenient to work in the following setting: A3. Z is a compact subset of R d , and there exists a continuous path in Z of finite length between any two points in Z.

The precise mathematical definition of a path and its length are given in appendix C.1. In the setting of A3, we denote by d geo Z (z, z ′ ) the shortest path length, or geodesic distance, in Z. This is the infimum of the lengths of all paths in Z with end-points z, z ′ (see appendix C.1 for details). If Z is convex, the shortest path between two points is a straight line and d geo Z (z, z ′ ) = ∥z-z ′ ∥ R d . For x, x ′ ∈ M, the shortest path length, or geodesic distance, in M is denoted d geo M (x, x ′ ) and defined analogously to d geo Z (z, z ′ ). Even when Z is convex, in general M is not convex and d geo M (x, x ′ ) is not equal to the straight-line distance ∥x -x ′ ∥ ℓ2 .

We shall say isometry holds between Z and M if

$d geo M (ϕ(z), ϕ(z ′ )) = d geo Z (z, z ′ ), ∀z, z ′ ∈ Z.(11)$Compared to homeomorphism, this isometry condition imposes more of a constraint on the relationship between Z and M. One can interpret isometry as allowing ϕ to transform Z into M by bending, but not by stretching or compressing, since that would violate the equality of shortest path lengths.

The following proposition shows that isometry holds up to a scaling constant when, for z, z ′ close to each other, f (z, z ′ ) depends only on the Euclidean distance between z and z ′ . In contrast, weak-sense stationarity involves the more stringent requirement that such dependence holds for all z, z ′ . Define D := {(z, z); z ∈ Z} ⊂ Z × Z.

$Proposition 3. Assume A2 and A3. If f (z, z ′ ) = g(∥z -z ′ ∥ 2 R d ) for all z, z$′ in an open neighbourhood of D where g is twice continuously differentiable and g ′ (0) < 0, then

$d geo M (ϕ(z), ϕ(z ′ )) = -2g ′ (0)d geo Z (z, z ′ ).(12)$The following proposition complements proposition 3 by addressing the special case in which Z is a sphere.

$Proposition 4. Assume A2. If Z = {z ∈ R d : ∥z∥ R d = 1} and f (z, z ′ ) = g(⟨z, z ′ ⟩ R d ) for all z, z ′$in an open neighbourhood of D where g is twice continuously differentiable and g ′ (1) > 0, then

$d geo M (ϕ(z), ϕ(z ′ )) = g ′ (1)d geo Z (z, z ′ ).(13)$The proofs of propositions 3 and 4 are at the end of appendix C.1.

## Relating smoothness to concentration within a low-dimensional subspace

When the latent domain Z is a subset of R d , we will say that f is smooth if it can be expressed as the restriction of a smooth function on R d × R d to Z × Z. How smooth f is affects how much of the manifold M we can capture using only the first few coordinates. For some s < r, consider the truncated map

$ϕ s (z) := (λ f 1 ) 1/2 u f 1 (z) • • • (λ f s ) 1/2 u f s (z) 0 • • • ⊤ .$The eigenvalues give us a measure of how well M s := ϕ s (Z) approximates M through the mean square error

$E[∥ϕ(Z i ) -ϕ s (Z i )∥ 2 ℓ2 ] = k>s λ f k E |u f k (Z i )| 2 = k>s λ f k .$The rate of decay of the eigenvalues is known to be related to the smoothness of the kernel [[87]](#b85) and so, under smoothness assumptions, the first s coordinates of ϕ can provide a good approximation to M, even if r = ∞. Recalling the representation Y i m.s.

= p 1/2 Wϕ(Z i )+σE i from proposition 1, such smoothness also implies each vector Y i will be concentrated within the (at most) s-dimensional subspace of R p , when s ≤ p, spanned by the first s columns of W, since

$E ∥Y i -p 1/2 Wϕ s (Z i )∥ 2 = E ∥p 1/2 Wϕ(Z i ) -p 1/2 Wϕ s (Z i ) + σE i ∥ 2 = pE[∥ϕ(Z i ) -ϕ s (Z i )∥ 2 ℓ2 ] + σ 2 E[∥E i ∥ 2 ] = p k>s λ f k + pσ 2 ,$where the independence of W, ϕ(Z i ), E i , the second equality in [(8)](#b6) and the properties

$E[E ij ] = 0, E[E 2 ij ] = 1 have been used.$An LMM with smooth kernel can therefore produce a data matrix Y which is 'approximately low-rank', a common feature of real data [[94]](#b92), hinting that PCA may be a useful tool to help recover ϕ(Z 1 ), . . . , ϕ(Z n ).

## A visual example

To illustrate some of the concepts from section 3, we consider a case in which Z is a torus embedded in R 3 , satisfying A3 with d = 3. We take µ to be the uniform distribution on the torus, and Z 1 , . . . , Z 4000 simulated from µ are shown in figure [3](#fig_2). The colouring of the points in this figure emphasises that the torus is the Cartesian product of two circles, and the locations on the torus can be parameterised in terms of angles around these two circles. We assume X 1 . . . , X p are i.i.d., zero-mean Gaussian processes with common covariance function exp(-∥z -z ′ ∥ 2 R 3 ) = f (z, z ′ ), which satisfies A1. Figure [4](#) shows numerical approximations to the first 1-3, 4-6 and 7-9 dimensions of ϕ(Z i ), i = 1, . . . , 4000 (these approximations were obtained Figure [4](#): Torus example. Both the top and bottom rows show the first 9 dimensions of ϕ(Z i ), i = 1, . . . , 4000. In each row, points are coloured according to the coordinates of the underlying points Z 1 , . . . , Z n with respect to the two circles shown in figure [3](#fig_2). Numerical scales are omitted to de-clutter the plots. using PCA, the details of which are given later in section 4.1). The only difference between the two rows of plots in figure [4](#) is the colouring of the points; the colouring in the top row is the colouring of the corresponding points in the middle plot in figure [3](#fig_2), similarly the colouring in the bottom row matches that in the plot on the right of figure [3](#fig_2). It is clear from figure [4](#) that the global shape of M, when viewed three dimensions at a time, is qualitatively different to the global shape of Z. However, assumption A2 holds, so by lemma 2, we know M is topologically equivalent to Z, and by proposition 3, ϕ isometry holds, up to a scaling factor of -g ′ (0) = √ 2 for the f in question. This tells us that shortest path lengths in M are equal to the corresponding path lengths Z, up to a factor of √ 2. Figure [5](#fig_3) shows comparison of these shortest path lengths, computed numerically from the points in figures 4 and 3 using a nearest neighbour graph as detailed in section 4.4. We see a close approximation to the theoretical scaling factor of √ 2, shown by the red line. Overall this example illustrates that if we are interested in discovering the topological or geometric structure of Z based on observations of M, we should not pay attention to the global shape of M that we perceive visually, because that depends on both Z and ϕ. However, when homeomorphism holds, we can in principle recover the abstract topological structure of Z and its homological features such as number of connected components, number of holes, etc., from M. Moreover, when isometry holds, at least up to a constant scaling factor, we can gain insight into the geometry of Z from shortest paths in M.

## Methodology

In this section, properties of the LMM are used to explain and justify the workflow outlined in section 1. Discussion of the step 1. is postponed until after discussion of step 2.

## Linear dimension reduction by PCA

Given data Y ∈ R n×p and s ≤ min{p, n}, let the columns of V Y ∈ R p×s be orthonormal eigenvectors associated with the s largest eigenvalues of Y ⊤ Y ∈ R p×p . The dimension-s PCA embedding is the collection of vectors ζ 1 , . . . , ζ n , defined by:

$[ζ 1 | • • • |ζ n ] ⊤ := YV Y(14)$so for each

$ζ i = V ⊤ Y Y i is is a vector in R s .$These quantities are sometimes called principal component scores [[53,](#b51)[80,](#b78)[37]](#b35). When performing PCA in practice, one often centers the data about their sample mean. For simplicity of presentation we do not consider such centering here, although we do not require population centering, that is we do not assume E[Y i ] = 0.

The following assumptions about the LMM are introduced to enable theoretical analysis of the PCA embedding.

A4. The random functions X 1 , X 2 , . . . are independent.

$A5. sup j≥1 sup z∈Z E[|X j (z)| 4 ] < ∞ and sup j≥1 sup i≥1 E[|E ij | 4 ] < ∞.$A6. For each p ≥ 1, the rank r of the mean correlation kernel f defined in (2) is finite, and r and 1/λ f r are bounded as p → ∞.

Theorem 1. Assume A4-A6 and let r be as therein. Let ζ 1 , . . . , ζ n be the dimension-r PCA embedding of Y ∈ R n×p under the LMM. Then there exists a random orthogonal matrix Q ∈R r×r depending on n and p such that

$max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 ∈ O P 1 √ n + n p(15)$as n → ∞ and p/n → ∞ simultaneously, where ∥ • ∥ 2 is the Euclidean norm.

Theorem 1 is a corollary to a more detailed non-asymptotic concentration result for the PCA embedding, theorem 4, stated and proved in appendix D.

## Interpretation of theorem 1 and relation to the literature

Theorem 1 implies that for any ϵ > 0, the probability that max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 > ϵ converges to zero as n and p/n grow simultaneously. In that sense ϕ(Z 1 ), . . . , ϕ(Z n ) can be recovered from p -1/2 ζ 1 , . . . , p -1/2 ζ n , up to an orthogonal transformation, i.e., a transformation which preserves distances and inner-products. We see that computing the PCA embedding achieves a form of de-noising: each ζ i depends on all three sources of randomness in the LMM, but ϕ(Z i ) clearly depends only on the random latent variable Z i .

The result has positive implications for different forms of unsupervised learning, such as clustering, topological data analysis or manifold learning in the regime n, p/n → ∞. Viewed as sets, the point clouds {p -1/2 ζ i } i=1,...,n and {ϕ(Z i )} i=1,...,n converge to each other in Hausdorff distance, up to Q, implying convergence of topological summaries such as persistence diagrams [[97]](#b95), and so on. Broadly speaking, whatever we wish to estimate about M, if we can show the estimator would be consistent given ϕ(Z 1 ), . . . , ϕ(Z n ), there is a reasonable chance to show it is consistent given p -1/2 ζ 1 , . . . , p -1/2 ζ n . The LMM then gives us a way to translate such estimates into statements about the latent domain Z (see Section 3).

The behaviour of PCA and principal component scores in high-dimensions has been the subject of intensive theoretical study, e.g., [[69,](#b67)[43,](#b41)[44,](#b42)[103,](#b101)[53,](#b51)[104,](#b102)[80,](#b78)[81,](#b79)[37]](#b35). A central theme in these works is analysis of the eigenvectors of the sample "covariance" matrix n -1 Y ⊤ Y ∈ R p×p , which make up the columns of the matrix V Y appearing in [(14)](#b12). It is usually assumed that the data follow a spiked covariance model (a special case of the LMM in which Z is Euclidean, f is linear and X 1 , . . . , X p are deterministic -see section B for details), with consideration given to various scaling relationships involving p, n and other parameters. One of the key messages of [[43]](#b41) is that the eigenvectors of the sample covariance matrix consistently estimate their population counterparts if and only if p/n → 0.

Prima facie, theorem 1 may therefore seem surprising, since it says the PCA embedding is consistent in a regime, p/n → ∞, where PCA was previously said to be inconsistent [[43]](#b41), for the reasons above. The explanation is that we can obtain a consistent embedding, YV Y , without requiring consistency of V Y . The proof of theorem 1 sets out in a crucially different direction: an elementary linear algebra argument (lemma 5 in section D) shows that

$p -1/2 YV Y = U Y Λ 1/2 Y ,$where the columns of U Y ∈ R n×r are orthonormal eigenvectors of p -1 YY ⊤ ∈ R n×n with associated eigenvalues on the diagonal of Λ Y . The n/p term in [(15)](#b13) relates to the concentration behavior of the n×n matrix p -1 YY ⊤ about its conditional expectation: [15](#formula_30)) concerns approximations to certain integrals with respect to µ, based on the samples Z 1 , . . . , Z n , which arise when relating the rows of

$E (p -1 YY ⊤ ) ij |Z i , Z j = p -1 E [⟨Y i , Y j ⟩|Z i , Z j ], c.f. (9). The 1/ √ n term in ($$U Y Λ 1/2 Y to ϕ(Z 1 ), . . . , ϕ(Z n ).$The proof of theorem 4 relies heavily on matrix decomposition techniques used by Lyzinski et al. [[58]](#b56) in the study of spectral embedding of random graphs under a random dot product model. The uniform nature of theorem 1 is directly inspired by the uniform consistency result of Lyzinski et al. [[58]](#b56)[Thm. 15], which is an instance of convergence with respect to the 2 → ∞ matrix norm, studied in detail by Cape et al. [[13]](#b11). We note more generally that singular vector estimation under low-rank assumptions is an active area of research. As a recent example, Agterberg et al. [[2]](#b0) obtained finite sample bounds and a Berry-Esseen type theorem for singular vectors under a model in which the signal is a deterministic low-rank matrix and heteroskedasticity and dependence is allowed in additive sub-Gaussian noise.

## Discussion of assumptions A4-A6

In the proof of theorem 4 and hence theorem 1, the independence assumption A4 and the moment assumption A9 are used when analysing p -1 YY ⊤ via matrix a polynomial moment concentration inequality from [[70]](#b68). The moment assumption A9 is not particularly restrictive. From a modelling point of view relaxing A4 to some form of weak dependence or mixing condition would be desirable, but the authors do not know of any suitable polynomial moment matrix concentration inequalities which are applicable in that situation.

Concerning assumption A6, that f has finite rank: recall from section 3.4 that the eigenvalues tend to tail off quickly when f is smooth, in which case assumption A6 might be taken to hold approximately. Here we will add that if f is polynomial [[79]](#b77) or piecewise polynomial, or if Z has finitely many points, then f has strictly finite rank. As a result, assumption A6 is mild enough to include any function f which is obtainable from standard numerical or function approximation schemes (e.g. Taylor expansion, polynomial splines, etc).

The condition that r is bounded as p → ∞ in A6 can be understood as constraining the functional complexity of f as p grows. The condition that 1/λ f r is bounded as p → ∞ means that the additive noise whose scale is specified by the constant σ cannot overwhelm the "signal" in the LMM. If X 1 , X 2 , . . . are identically distributed then f , and hence r and λ f r , are automatically constant in p. However, the fact that theorem 1 involves choosing the dimension of the PCA embedding to be equal to r is less realistic, although a very common type of assumption in uniform consistency results for spectral embedding, e.g. [[58]](#b56). Truncated spectral embedding of graphs under a model with an infinite rank kernel was studied by [[88]](#b86), but their results concern a weaker, non-uniform measure of error than the one we consider here.

## Choosing the PCA dimension

Our model and theory motivate a new method for choosing the embedding dimension, r. Before proceeding, we should make clear that the hat notation in r is meant loosely: we seek a choice which achieves a good bias/variance trade-off in practice, and this may or may not coincide with the true rank of the kernel, r. Moreover, we do not claim that there is a 'best' choice: different tasks benefit from different choices. In particular, if using PCA for prediction purposes we simply recommend cross-validation, as is common practice. For more exploratory analyses, as conducted here, we propose the following approach instead.

Assuming n is even split the data Y into two,

$Y (1) , Y (2) ∈ R n/2×p$, and for each candidate dimension ρ, take the orthogonal projection of the rows of Y (1) onto the ρ principal eigenvectors of Y (1) ⊤ Y (1) -the resulting n/2 vectors are p-dimensional, just constrained to a ρ-dimensional subspace. Next, measure how much this projection step has brought the first half closer to the second, using Wasserstein distance. Select r to be the ρ achieving the smallest distance. The procedure is described precisely in algorithm 1.

To understand how r might relate to r, let us make a few simplifying assumptions. Suppose r < ∞, so that (with exact equality)

$Y i = p 1/2 Wϕ(Z i ) + σE i ,$and that the second Wasserstein distance is used, that is

$d ρ = W 2 (Y (1) Π ρ , Y (2) ) where W 2 2 (A, B) := min π 1 m ∥A i -B π(i) ∥ 2 2 , A, B ∈ R m×d ,$where A i and B i are the ith rows of A and B, and where the minimum is over all permutations of the integers 1, . . . , m.

The second Wasserstein distance is particularly amenable to mathematical analysis because of the following property, which can be checked by direct calculation. If there exist Â1 , . . . , Âm such that for all i, j we have ⟨A i -Âi , B j ⟩ = 0, then

$W 2 2 (A, B) = 1 m ∥A -Â∥ 2 F + W 2 2 ( Â, B).(16)$To see why algorithm 1 might reject overly values of ρ, suppose ρ > r and consider the projection errors

$E (1) = Y (1) (Π ρ -Π r ). With E (1) i$denoting the ith row of E (1) and the superscript (k) indicating random objects associated with Y (k) , suppose

$1 p ⟨ E (1) i , E(2)$j ⟩ ≈ 0, and

$1 p ⟨ E (1) i , √ pWϕ(Z (2) j )⟩ ≈ 0, i, j = 1, . . . , n/2. (17$$)$Then, using [(16)](#b14),

$1 p d 2 ρ ≈ 2 np ∥E (1) ∥ 2 F + 1 p d 2 r ,$where ∥E (1) ∥ 2 F is non-decreasing in ρ -r, and it follows that we should expect d ρ > d r when ρ is large relative to r.

Why should the approximations in [(17)](#b15) hold? The first is the product of sample-splitting. If the E i are statistically independent, then E (2) is statistically independent of V (1) ρ . Combined with the fact that the elements of E

(2) i are mean-zero and unit-variance, we therefore expect the p -1 E (2) i to be approximately orthogonal to the subspace spanned by the columns of V

(1) ρ , when p is large relative to n. The second approximation seems to be reasonable as long as n and p are large, and we have confirmed this by simulation. Now consider the case ρ < r. We have

$d 2 ρ = W 2 2 (Y (1) Π ρ , Y (2) ) = 2 n ∥Y (2) (Π ρ -I p )∥ 2 F + W 2 2 (Y (1) Π ρ , Y (2) Π ρ ).$By the Eckart-Young theorem,

$∥Y (2) (Π ρ -I p )∥ 2 F ≥ k>ρ λ k (Y (2)⊤ Y (2) )(18)$where λ k (Y (2)⊤ Y (2) ) is the kth largest eigenvalue of Y (2)⊤ Y (2) . The r.h.s. of ( [18](#formula_44)) is nondecreasing in r -ρ. To see that the term W 2 2 (Y (1) Π ρ , Y (2) Π ρ ) converges to zero as p/n, n → ∞, it is convenient to consider the case σ = 0. In this situation,

$1 p W 2 2 (Y (1) Π ρ , Y (2) Π ρ ) ≤ 1 p W 2 2 (Y (1) , Y (2) ) = 1 p W 2 2 (Φ (1) W ⊤ , Φ (2) W ⊤ )$where

$Φ (k) = [ϕ(Z (k) 1 )| • • • |ϕ(Z (k) n/2 )] ⊤ ∈ R n/2×r$. Appealing to the same arguments as in section 3.1, as p/n → ∞,

$1 p W 2 2 (Φ (1) W ⊤ , Φ (2) W ⊤ ) is concentrated about W 2 2 (Φ (1) , Φ (2)$), and the latter converges to zero as n → ∞ because the rows of Φ (1) and Φ (2) are i.i.d. random vectors in R r . It follows that we should expect d ρ > d r when ρ is small relative to r, and so overall that d ρ will have a minimum near ρ = r.

A general rule we could draw from these arguments, and which we see in practice, is that to recommend substantial dimension reduction the algorithm wants to see a large p relative to n, and noise. Conversely, if the noise level is low or if n is large relative to p, then d ρ may keep decreasing with ρ, which we tend to interpret as contraindication against PCA.

## Method comparison

We now explore the performance of Algorithm 1 in a few simulated examples. We consider four configurations, where each configuration refers to a choice of latent space Z and corresponding kernel f . In the first configuration, the latent space comprises six distinct elements. The latent spaces in the remaining configurations are different subsets of R 2 . In each configuration, we draw n = 500 points Z i uniformly on Z, and the resulting point sets are shown in figures 6a)1-4.

In the first configuration, we choose an arbitrary 6 × 6 positive-definite matrix to represent the kernel. In the second, f (x, y) = (x ⊤ y + 1) 2 , which has rank 6; in the third, f (x, y) = {cos(x (1)y (1) ) + cos(x (2) -y (2) ) + 2}, which has rank 5; and in the fourth, f (x, y) = exp(-∥x -y∥ 2 R 2 /2), which has infinite rank.

We simulate a 500 × 1000 data matrix Y in each configuration, where the p = 1000 random functions are independent, zero mean Gaussian processes with the same covariance kernel f , and the errors E ij are independent and standard normal.

In figures 6c)1-4 we show the Wasserstein error (log-scale), i.e., the distance computed in Algorithm 1, for different choices of dimension. Reassuringly, the optimum roughly coincides with the true rank of the kernel when finite (dashed black line, configurations 1-3) and at the same time it is interesting that a non-degenerate optimum is still found under infinite rank (configuration 4). If we lower the noise, the optimal dimension increases (figure [17](#fig_17), Appendix), reflecting the afore-mentioned bias/variance trade-off.

For comparison, we also show the dimensions selected using the ladle [[57]](#b55) and elbow methods [[106]](#b104), as implemented in the R packages 'dimension' (on github: [https://github.com/WenlanzZ](https://github.com/WenlanzZ)) and 'igraph' (on The Comprehensive R Archive Network), respectively. The ladle and Wasserstein methods seem to make similar choices, although as implemented the ladle method is computationally costly, which has precluded more simulations or going beyond max(n, p) = 1000 to allow a more comprehensive comparison. We would advise against the elbow method for dimension selection under the LMM, as it appears to favour dangerously low dimensions.

In configurations 3 and 4, there is isometry between M and Z. As a result, we can aim to recover the path lengths in Z amongst Z 1 , . . . , Z n from p -1/2 ζ 1 , . . . , p -1/2 ζ n -see section 4. 

## 4:

Project Y (1) onto the linear span of the columns of

$V (1) ρ , that is compute: Y (1) Π ρ where Π ρ := V (1) ρ V (1)⊤ ρ . 5:$Compute Wasserstein distance d ρ between Y (1) Π ρ and Y (2) as point sets in R p . 6: end for Output: selected dimension r = argmin {d ρ }.

details. This method yields infinite distances when the k-nearest neighbour graph isn't connected. Dealing with this issue in a systematic way is awkward, and we settled on the following solution. Picking ϵ as the 5% quantile of the R 2 Euclidean distance matrix between the Z i , we place an edge between any pair of points within distance ϵ, weighted by Euclidean distance, and approximate the geodesic distance between two points as the corresponding weighted graph distance. Any infinite distance remaining is replaced with the original Euclidean distance. The blue line in figures 6d)3-4 shows the entrywise mean square error between the estimated geodesic distance matrices of Z 1 , . . . , Z n and p -1/2 ζ 1 , . . . , p -1/2 ζ n , for different choices of r. The optimum roughly coincides with the dimensions selected by the ladle and Wasserstein methods.

Because of the isometric relationship between M and Z in configurations 3 and 4, we might also hope that the persistence diagrams of their Rips filtrations would be similar. The red line in figures 6d)3-4 shows the bottleneck distance between the persistence diagrams of the Rips filtrations of Z 1 , . . . , Z n and p -1/2 ζ 1 , . . . , p -1/2 ζ n , as implemented in the R package 'TDA', for different choices of r. In this metric, the optimal dimension (lowest bottleneck distance) is lower than that suggested by the ladle and Wasserstein methods, but we do not know to what extent this should be expected in general. The scales of the log-Wasserstein error, geodesic distance error, and bottleneck distance are not comparable and in figures 6d) we have recentered and rescaled the curves to make their maxima and minima agree.

In figures 6e)1-4 we show the persistence diagrams of the Rips filtrations of p -1/2 ζ 1 , . . . , p -1/2 ζ n computed on the basis of the dimension selected by the Wasserstein method (Algorithm 1), using the R package 'TDA'. Recall that in persistent homology the significance of a topological feature is quantified by its persistence, death minus birth, which is the vertical distance between the point (birth,death) to the diagonal x = y. Following Fasy et al. [[26]](#b24) we draw a line parallel to x = y to separate the signal from the noise, picking y = x+0.2 by eye. In each figure, we report the number of connected components, β0 , and holes, β1 , estimated by this heuristic. The true corresponding values for Z are respectively (6,0), (1,8), (1,0), and (1,1).

## Spherical projection

When performing data analysis we may wish to consider the assumption that f belongs to one of the families of kernels in proposition 3 or 4, because of their stationarity interpretation, and because the associated isometry properties would justify use of the PCA embedding to recover geometric features of Z. However, all these kernels have the property that

$p -1 p j=1 E[|X j (z)| 2 ] = f (z, z) = const.,(19)$which from a modelling point of view may be restrictive. We shall now show that the spherically projected PCA embedding has a model-based interpretation which allows this restriction to be loosened. Suppose we are given random functions X 1 , . . . , X p such that f (z, z) is constant in z ∈ Z. Without loss of generality assume this constant is 1. As usual let ϕ be the feature map associated  with f . Note that in this situation M is a subset of the unit hypersphere {x ∈ ℓ 2 : ∥x∥ ℓ2 = 1}. With α 1 , . . . , α n being i.i.d. random variables whose distribution is supported on a compact set A ⊂ R + , define the model:

$H 0 H 1 0 2 4 Birth 0 1 2 3 4 Death 0 = 1 1 = 0 H 0 H 1 0 2 4 Birth Death 0 = 1 1 = 1 H 0 H 1$$Y ij = α i X j (Z i ) + σE ij .(20)$This can be viewed as a particular form of LMM with extended latent space Z ext := A × Z and extended random functions X ext j (α, z) := αX j (z). Its mean correlation kernel is:

$f ext (α, z, α ′ , z ′ ) := 1 p p j=1 E X ext j (α, z)X ext j (α ′ , z ′ ) = α⟨ϕ(z), ϕ(z ′ )⟩ ℓ2 α ′ ,$the Mercer feature map of f ext satisfies: ϕ ext (α, z) = αϕ(z), and we have p

$-1 p j=1 E[|X ext j (α, z)| 2 ] = α 2$, allowing more flexibility than [(19)](#b17). Now suppose assumption A6 holds, let ζ 1 , . . . , ζ n be the dimension-r PCA embedding computed from Y under the extended LMM [(20)](#b18) and consider the spherical projection

$ζ sp i := ζ i /∥ζ i ∥ 2 . Using the identities ∥ϕ ext (α, z)∥ 2 = α∥ϕ(z)∥ 2 = αf (z, z) 1/2 =$α, and applying the triangle inequality several times gives:

$∥Qζ sp i -ϕ(Z i )∥ 2 = p -1/2 Qζ i p -1/2 ∥ζ i ∥ 2 - ϕ ext (α i , Z i ) ∥ϕ ext (α i , Z i )∥ 2 2 ≤ 2 ∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 α i -∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 ,$where Q is any orthogonal matrix. Theorem 1 could therefore be applied to the LMM (20) to establish that for the particular Q in that theorem, ∥p -1/2 Qζ i -ϕ ext (α i , Z i )∥ 2 → 0, which by the above inequality implies ∥Qζ sp i -ϕ(Z i )∥ 2 → 0. In summary, under the model [(20)](#b18), ϕ(Z 1 ), . . . , ϕ(Z n ) can be recovered from the spherically projected embedding ζ sp 1 , . . . , ζ sp n , up to an orthogonal transformation.

## Nearest neighbour graph construction

Constructing a nearest neighbour graph from the PCA embedding allows us to approximate topological and geometric features of M and hence Z. In keeping with the workflow set out in section 1, we focus on the spherically projected embedding ζ sp 1 , .

. . , ζ sp n but very similar considerations apply to the raw embedding ζ 1 , . . . , ζ n . Noting that ∥ζ sp i ∥ 2 = 1 for all i, we denote by d S (ζ sp i , ζ sp j ) := arccos(⟨ζ sp i , ζ sp j ⟩ 2 ) the circular arc distance on the unit hypersphere. There are two popular types of nearest neighbour graph: the ϵ-nn and k-nn graphs, both of which are undirected, weighted graphs with n vertices, identified with ζ sp 1 , . . . , ζ sp n . There is an edge between ζ sp i and ζ sp j in the ϵ-nn graph if d S (ζ sp i , ζ sp j ) ≤ ϵ, and in the k-nn graph if ζ sp i is one of the k-nearest (with respect to d S ) neighbours of ζ sp j or vice versa. In both types of graph, if there is an edge between ζ sp i and ζ sp j it is assigned weight d S (ζ sp i , ζ sp j ). A number of algorithms for identifying nearest neighbours exactly or approximately are available, for example in the Python library scikit-learn [72].

Recalling [(29)](#b27), nearest neighbour graph distances can be used to approximate shortest path lengths in M:

$d geo M (ϕ(Z i ), ϕ(Z j )) ≈ D ij M := min x1,...,xm {d S (x 1 , x 2 ) + • • • + d S (x m-1 , x m )} ,$where the minimum is over all paths in the nearest neighbour graph connecting x 1 = ζ sp i and x m = ζ sp j . If there are no such paths, D ij M = ∞ by convention. Various fast algorithms for computing shortest paths and shortest path lengths are available, for example in the Python library NetworkX [[33]](#b31).

The use of nearest neighbour graphs to approximate path lengths on manifolds is well studied and is the first step in the Isomap procedure [[89]](#b87). The theoretical accuracy of such approximations has been analysed by [[7,](#b5)[93]](#b91). In particular [[93]](#b91) note that the k-nn graph is often preferred in practice although its analysis is more complicated. They also note that choosing a single value for ϵ or k is a difficult problem in general. Where possible in the examples of section 5 we compute and analyse the nearest neighbour graph over a range of values for ϵ or k, rather than selecting one single value. This approach is similar in spirit to the computation of ϵ-nn graphs over a range of ϵ values in persistent homology techniques [[14,](#b12)[17]](#b15).

## Examples

In the following three real data examples, we will assume the LMM holds and explore hypotheses about the latent domain, Z, feature map, ϕ, and the manifold underlying the data, M = ϕ(Z).

In all examples, we have access to background information. In some cases, such as the first hypothesis in the image and transcriptomics examples, the background points us towards generic hypotheses, such as 'M is a loop' or 'M is a tree'. In others, e.g. the second hypothesis in the image and transcriptomics examples, the information is used more explicitly to obtain trial values for Z, realisations z i of Z i , and to estimate parts of the model. Although we provide baseline comparisons against uniform models, the workflow we present is designed for exploratory rather than confirmatory analysis and should not be viewed as formal hypothesis testing.

The code and data used are available here: [https://github.com/anniegray52/explore_ manifold_hyp](https://github.com/anniegray52/explore_manifold_hyp)

## Images

We return to the data set of images described in section 1. Recall there are n = 72 images, each consisting of p = 110592 grey-scale pixels, taken from angles 0, 5, 10, . . . , 355 degrees around the circumference of a circle. We will assume XY coordinates for the camera positions, cos(θ i ), sin(θ i ) for each angle θ i (converted to radians). In this context, we will first consider the hypothesis:

1. Z is a circle and ϕ is a homeomorphism. An informal implication is: The data lie close to a loop.

Finding the data consistent with the above, we will consider the stronger hypothesis:

2. Z is the circle of camera positions, z i = (cos(θ i ), sin(θ i )) are the (known) camera positions, and ϕ is a scaled isometry. An informal implication is: Distances along the loop correspond to distances along the circle between camera positions.

The first step of the workflow is to apply the dimension selection method. As per figure [7a](#fig_17)), this results in r = 11. The kernel density estimate in 7b) demonstrates the variation in the magnitudes of the dimension-r PCA embedding vectors ∥ζ 1 ∥ 2 , . . . , ∥ζ n ∥ 2 ; in all subsequent steps we instead work with the spherically projected embedding ζ sp 1 , . . . , ζ sp n as per (4.3). We now consider the first hypothesis, which would be mathematically justified by assumption A2, with Z (say) the unit circle on R 2 (d Z the Euclidean metric). Then indeed ϕ would be a homeomorphism and M would be topologically equivalent to a circle. Conveniently in this example, the presence of loops or holes in data point-clouds can be assessed using persistent homology techniques [[14,](#b12)[17]](#b15). Figure [7c](#fig_17)) shows a persistence diagram computed from the spherically projected PCA embedding using the Python package Ripser.py [[91]](#b89). The blue dot on the horizontal dashed line is indicative of a single connected component with persists over a large range of length scales. The isolated single orange dot close to the horizontal dashed line is indicative of a single "loop" in the embedding, also persisting over a large range of length scales. This is consistent with the hypothesis.

We now consider the second hypothesis, which by proposition 4 would be mathematically justified by the mean correlation kernel being of the form f (z, z ′ ) = g(⟨z, z ′ ⟩) in some neighbourhood of z = z ′ (or equivalently ⟨z, z ′ ⟩ = 1), and g ′ (1) > 0 (recall proposition 4). On the hypothesis assumption that z i = (cos(θ i ), sin(θ i )), figure 7d) shows f (z i , z j ) := ⟨ζ sp i , ζ sp j ⟩, which we regard as an estimator of f (z i , z j ), plotted as a function of θ i and θ j . The fairly constant width of the pronounced yellow/white diagonal stripe in this plot admits the interpretation that indeed f (z, z ′ ) = g(⟨z, z ′ ⟩) in a neighbourhood of z = z ′ . To examine this in more detail, figure [7e](#fig_17)) plots values of f (z i , z j ) against ⟨z i , z j ⟩ over all i, j = 1, . . . , n. The red dashed ellipse highlights that (1) i ). e) Estimated kernel as a function of latent inner product ⟨z i , z j ⟩, the red dashed ellipse highlights f (z i , z j ) in the region ⟨z i , z j ⟩ ≈ 1. f) Evidence of a linear relationship between shortest path lengths computed from the nearest neighbour graph G (y-axis), and from the latent positions (x-axis).

f (z i , z j ) is approximately an increasing function of ⟨z i , z j ⟩ in a neighbourhood of z i = z j , which is consistent with g ′ (1) > 0. Informed by [(13)](#b11) we thus anticipate there is isometry between M and Z, up a scaling factor of g ′ (1) 1/2 . To see if the data are consistent with such a relationship we compute the k-nn graph G as per section 4.4 with k = 2. This is the natural choice for k if M is topologically equivalent to a circle. Figure [7f](#fig_17)) shows shortest path lengths D ij M in G plotted against shortest path lengths around the circle, denoted D ij Z , over i, j = 1, . . . , n. The clear linear relationship is consistent with there being little deviation from isometry, up to a scaling factor, which by a straight line fit in figure [7f](#fig_17)) we can estimate: g ′ (1) ≈ 3.18.

## Single-cell transcriptomics

We now revisit the planarian single-cell transcriptomics example introduced in section 1. Recall that here we have p = 5821 dimensional gene expression data in n = 5000 cells from adult planarians, and we also know cell-type labels for each of these cells, indicated by the different colours in figure [2](#fig_1). Adult planarians have a large number of pluripotent stem cells, known as neoblasts, that continuously differentiate into all adult cell types, resulting in a lineage tree that connects all the cells in the whole animal. We represent this lineage by a continuous tree (formally defined later) and suppose the cell types are named positions, c i , on this tree.

In this context, we will first consider the hypothesis:

1. Z is a continuous tree and ϕ is a homeomorphism. An informal implication is: The data lie close to a tree.

Finding the data consistent with the above, we will consider the stronger hypothesis:

2. Z is the lineage tree, z i = c i are the (known) cell types, and ϕ is a homeomorphism. Informally: the tree represents the lineage of the cell types.

In graph theory, a tree is an undirected graph in which any two vertices are connected by a unique path. We consider an analogue of this concept which reflects the continuous nature of cell differentiation. Inspired by definitions in [[63,](#b61)[41]](#b39), we say a metric space Z is a continuous tree if for any z, z ′ ∈ Z there exists a homeomorphism ψ between [0, 1] and some subset of Z such that ψ(0) = z and ψ(1) = z ′ (this means a continuous path in Z exists between z and z ′ ), and all such homeomorphisms have the same image (this means the path is unique).

Following the workflow in section 1, dimension selection results in r = 14. We then calculate the dimension-r embedding and its spherical projection ζ sp 1 , . . . , ζ sp n . For the remainder of this section we refer to the latter as the PCA embedding.

We now consider the first hypothesis, which would be mathematically justified by assumption A2. Then M equipped with the ℓ 2 distance also qualifies as a continuous tree, as the composition of two homeomorphisms is a homeomorphism. To gain some preliminary insight into the structure of the PCA embedding, figure [8a](#fig_8)) shows, in red, a histogram of inner products between all distinct pairs of embedding points ζ sp i , ζ sp j . As a baseline comparison, we generated a random embedding consisting of the same number n = 5000 points uniformly distributed on the r-dimensional, unitradius hypershere. Figure [8a](#fig_8)) shows, in black, a histogram of inner-products between all distinct pairs of points in this random embedding. We see this black histogram is symmetrical and concentrated around 0. By contrast, the red histogram is not symmetrical and exhibits two peaks. The peak near an inner product value of 1 indicates a substantial proportion of pairs of points ζ sp i , ζ sp j which are much closer together than is observed in the random embedding. Many other pairs ζ sp i , ζ sp j have inner products between -0.5 and 0, indicating they are spread out on the hypersphere, but not in the same way that uniformly random points are spread out. On the basis of this preliminary check we see no reason to rule out tree structure in the PCA embedding. We now quantify how "tree-like" the PCA embedding is in two steps. First we compute the k-nn graph of the PCA embedding as per section 4.4, and its minimum spanning tree. The latter is obtained by removing edges from the k-nn graph until a tree is formed, in such a way that the total edge length of the tree is minimal. Various fast algorithms for computing minimum spanning trees are available, we used the Python library NetworkX [[33]](#b31). The second step is to compare shortest path lengths in the k-nn graph to those in the minimum spanning tree. The shortest path length between any pair of vertices in the minimum spanning tree can only be greater than or equal to the shortest path length between those vertices in the k-nn graph. The percentage increase in shortest path length, when averaged over all pairs of vertices, serves as a univariate statistic which quantifies how tree-like the k-nn graph is. If the k-nn graph were a tree, this statistic would be exactly zero.

Figure [8b](#fig_8)) shows the average percentage increase in shortest path length, as a function of k. The red line shows the results for the PCA embedding. The black line and error bars show the same quantity computed from repeated simulations of the random embedding, serving as a baseline for comparison. We see that across all values of k, the average percentage increase in shortest path length is much lower for the PCA embedding than for the random embedding. This indicates that the minimum spanning tree is a close approximation to the k-nn graph of the PCA embedding. To take a finer-grained look, figure [8b](#fig_8)) shows shortest path lengths in the minimum spanning tree, versus in the k-nn graph with k = 10, for all pairs of vertices. The blue "y = x" line indicates the lower bound on path length increase which would be achieved if the k-nn graph were a tree. Overall, the findings in figure [8](#fig_8) are consistent with the PCA embedding of the planaria data being tree-like.

We now consider the second hypothesis. The left plot in figure [9](#fig_9) shows a visualisation of the minimum spanning tree derived from the k-nn graph with k = 10, with vertices coloured by the known cell type labels. This visualisation was obtained using the Scaleable Force Directed Placement graph layout algorithm [[40]](#b38). From the colouring by cell type, we see that biologically similar types, such as the three types of muscle cell, appear in localised branches of the tree. We next construct a "class graph" which captures the relationships between cell types implied by the minimum spanning tree in figure [9a](#fig_9)). In this class graph, each vertex corresponds to a cell type, and the undirected edge weight between any two vertices in the class graph is defined to be the total number of edges in the minimum spanning tree between cells of those two types.

The class graph is shown in figure [9b](#fig_9)). The size of each node represents the total number of cells of that type. The thickness of the edges reflects their weights in the class graph, although for visual clarity we do not draw some edges with very low weights. Figure [9](#fig_9) elucidates cell development, tracing the lineage from stem cells to progenitors and differentiated cell populations: neurons, muscle cells, protonephridia, epidermis, and secretory cells.

The original paper [[74]](#b72) provides a consolidated tree, which amalgamates various evidence types. The overall structure aligns with our nearest neighbour approach, with branches for individual known cell types, however, discrepancies exist in the form of minor variations in the differentiated cell populations. For example, cav-1+ neurons connect to ChAT neurons 1 rather than neural progenitors. Additionally, a more interconnected secretory cell network is found in the class graph. In the consolidated tree, two connections are added based on marker gene analysis (genes known to be present in specific cell types): muscle pharynx to muscle body and from epidermis to the epidermal lineage. In contrast, the nearest-neighbour approach employed here identifies these connections. Acknowledging these differences, we refrain from delving further into minor disparities, given the current paper's intended scope.

For visual clarity, we draw a single node grouping together all the neoblast 1-13 cell types. The subgraph of the class graph corresponding to these neoblast types is shown in the inset of figure [9b](#fig_9)), revealing a large number of neoblast 1 cells, linked by edges to most other neoblast cell types. This aligns with the results of the original authors, but contrasts with previous studies [[96]](#b94), [[61]](#b59), which suggested distinct fates for various neoblast types. These disparities might be due to the unique ability of specialised neoblast cells to maintain pluripotency [[76]](#b74) or the sensitivity of the single-cell transcriptomic method, as in [[74]](#b72).

## Temperature time series

In this example the raw data are time series of average daily temperatures in n = 265 towns and cities, on p = 1450 days. The data originate from the Berkeley Earth project [1]. Our objective is to explore the relationship between temperature deviations and geographic locations of the towns and cities. To do so we take the ith data vector Y i to be the temperature time series for town or city i centered about its long-run average. Thus geometry of the data point-cloud Y 1 , . . . , Y n as specified by the inner products p -1 ⟨Y i , Y j ⟩ reflects the lag-zero cross-correlations amongst the time series.

In this context, we will first consider the hypothesis:

1. Z is a geographic region, z i = (latitute i , longitude i ) are the (known) geographical locations of towns or cities, ϕ is a scaled isometry. An informal implication is: geodesic distances in M reflect geographical distances.

On rejecting the above, we ask if we can at least entertain:

2. Z is a geographic region, z i = (latitute i , longitude i ) are the geographical locations of towns or cities, ϕ is a scaled isometry in certain subregions.

This will be found to hold approximately in e.g. central Europe. Following the workflow from section 1, figure [10a](#fig_10)) shows the results of dimension selection, figure [10b](#fig_10)) illustrates the variability of the magnitudes ∥ζ i ∥ 2 of the non-projected embedding vectors, and we work henceforth with the spherically projected embedding as per section 4.3.

We now consider the first hypothesis, which by proposition 4 would be mathematically justified if the temperatures on a given day were stationary processes over Earth (a 'sphere'). If isometry between Z and M were to hold up to a scaling factor, then the k-nearest neighbours of z i amongst {z j ; j ̸ = i} would correspond to the k-nearest neighbours of ϕ(z i ) amongst {ϕ(z j ); j ̸ = i}, with respect to d geo M . In order to see if the data are consistent with the hypothesis of isometry, we therefore compute the proportion of edges in common between the embedding k-nn graph G (as per section 4.4), and the geographic k-nn graph defined by the known locations z 1 , . . . , z n . Figure [10c](#fig_10)) shows this proportion as a function of k. As a baseline to help interpret these results, we sampled n points uniformly from the r-dimensional unit hypersphere, derived the k-nn graph from these points, then computed the proportion of edges in common with the geographic knn graph. This was repeated independently 100 times, and the resulting minimum, mean and maximum proportions of edges in common for each k are shown in red and black in figure [10c](#fig_10)). The correspondence between G and the geographic k-nn graph is much better than under this uniform model. However, we see that as k increases up to 50, the embedding k-nn graph has about 70% of edges in common with the geographic k-nn graph, but increasing k further up to about k = 130 does not increase this percentage further. This plateauing suggests isometry does not hold.

We now consider the second hypothesis. The plateauing leaves open the possibility that there may be coincidence between the embedding and geographic k-nn graphs in some localised areas of Z but not in others. Indeed isometry as in [(11)](#b9) or [(12)](#b10) requires equality of shortest path lengths for all z, z ′ ∈ Z. Figure [11](#fig_11) shows the locations of the towns and cities, and the edges in the embedding k-nn graph G, with k = 5 chosen so that according to figure [10c](#fig_10)) the embedding and geographic k-nn graphs have around 50% of edges in common. We see from figure [11](#fig_11) that in some regions, especially in central Europe, edges in the embedding k-nn graph generally correspond to geographic proximity, but elsewhere this correspondence does not hold. For example there are edges connecting Edinburgh, U.K., to cities in Norway which are not amongst its geographically nearest neighbours. Similarly, there are edges connecting Novorossiysk, Russia, to cities on the opposite shore of the Black Sea which are not amongst its geographically nearest neighbours. Conversely, geographic proximity does not always imply presence of an edge. For example, there are no edges between Baia Mare, Romania, and two geographically close cities directly to the east, on the other side of the Carpathian mountains. Plotting the k-nn graph G in this way shows presence or absence of edges, but it doesn't convey the weight of these edges in the k-nn graph G, which as per section 4.4, can approximate distances d geo M . Since the embedding is of dimension r = 36, it is challenging or perhaps impossible to construct a two-dimensional visualisation which faithfully conveys all aspects of its geometry. However, the visualisation task is much simpler if we choose some town or city, and then visualise the shortest paths in the embedding k-nn graph from that city to all other cities -the graph consisting of the union of all such paths is sometimes called a shortest path tree.

Figure [12](#fig_12) shows the shortest paths in G from Tallinn, Estonia, to all other towns and cities. Each such path is a sequence of towns or cities, and is visualised as a spline with knot points given by the locations of these towns and cities, with colour indicating length. Tallinn was chosen because of the different relationships between these shortest paths and geographic shortest paths which can be seen in different regions: the shortest paths in G which terminate at some towns and cities in central Europe, to the south-west of Tallinn, resemble geographic shortest paths, indicating a geometric relationship not far removed from isometry. By contrast, the red dots in figure [12](#fig_12) highlight the shortest path in G from Tallinn to Tripoli, Libya. This path passes through Sweden, Norway, the U.K., Ireland, France, Spain, back to France and then Italy. Clearly, this is not the geographically shortest path from Tallinn to Tripoli, indicating a strong deviation from isometry in these regions. Recalling from section 3.3 the relationship between weak stationarity and isometry, this deviation from isometry implies a pronounced lack of stationarity (with respect to geographic location) in these regions.  

## Conclusion

Conventional interpretation of the Manifold Hypothesis as per the quote from [[16]](#b14) in section 1 is that data vectors Y 1 , . . . , Y n ∈ R p are samples from some distribution supported on a manifold embedded in R p , perhaps subject to noise disturbances. Our analysis of the LMM in section 3 provides a more nuanced perspective: Y 1 , . . . , Y n are noisy, random projections of samples on a manifold M; the manifold itself is a high-dimensional distortion of some latent domain Z and arises due to correlation over Z. Under appropriate assumptions, M is homeomorphic or isometric to Z. This perspective leads us to a 'model-informed' workflow, involving PCA and nearest neighbour graph construction, in which we think about the data in relation to M, and then M in relation to Z, to explore hypotheses about the topological and geometric structure of Z.

In how much generality is this perspective applicable? Inspired by remarks of [[94]](#b92) in the context of latent variable models of low-rank matrices, we note the basic structure of the LMM,

$Y ij = X j (Z i ) + σE ij ,(21)$resembles a representation formula for exchangeable arrays due to Aldous [[3]](#b1): if Y is any infinite two-dimensional array of random variables such that permutations of its rows or columns do not alter the distribution of Y, then there exists a function h such that the following equality in distribution holds

$Y ij d = h(ξ, Z i , X j , E ij ) (22$$)$where ξ and the Z i 's, X j 's and E ij 's are i.i.d. U[0, 1]-distributed random variables. Putting aside the fact that in the LMM the rows of Y are exchangeable but the columns need not be, the resemblance between ( [21](#formula_57)) and [(22)](#b20) indicates that the LMM is rather general, albeit constrained to an additive form of error. The ability of PCA to extinguish noise, as characterised in theorem 1, seems closely tied to this additive structure. What are the limitations of the data analysis workflow we have proposed? This workflow is intentionally generic, and suitable for preliminary exploration of data and hypotheses about the data generating mechanism. It could serve as a first step before more detailed confirmatory analysis of a given data set, in order to quantify uncertainty, perform formal hypothesis testing, or fit a parametric model, and so forth, but it clearly does not include those functionalities.

## A Supporting results for section 2

The following version of Mercer's theorem can be found in [[85,](#b83)[Thm 4.49]](#).

Theorem 2 (Mercer's theorem). Let Z be a compact metric space and let f : Z × Z → R, be a symmetric, positive semi-definite, continuous function. Let µ be a finite Borel measure supported on Z. Then there exists a countable collection of nonnegative real numbers (λ f k ) k≥1 , λ f 1 ≥ λ f 2 ≥ . . . and R-valued functions (u f k ) k≥1 which are orthonormal in L 2 (µ), such that:

$f (z, z ′ ) = ∞ k=1 λ f k u f k (z)u f k (z ′ ), z, z ′ ∈ Z,$where the convergence is absolute and uniform.

## B Special cases of the LMM

## Spiked covariance model

The spiked covariance model [[42,](#b40)[69]](#b67) is the de facto standard model under which to study the theoretical properties of PCA, and is derived as follows. Let X ∈ R n×p be a matrix of random variables such that E[X ⊤ X] has rank r. Consider the eigendecomposition n -1 E[X ⊤ X] = VΛV ⊤ , where V ∈ R p×r , and define Z := XVΛ -1/2 . We have

$E Z ⊤ Z = nI r , V ⊤ V = I r ,(23)$and X = ZΛ 1/2 V ⊤ , a.s., where the latter equality can be checked by verifying

$E[∥X -ZΛ 1/2 V ⊤ ∥ 2 F ] = tr E[(X -ZΛ 1/2 V ⊤ ) ⊤ (X -ZΛ 1/2 V ⊤ )] = 0.$The spiked covariance model takes the form:

$Y = ZΛ 1/2 V ⊤ + σE,$where the elements of E ∈ R n×p are usually assumed to be zero-mean, unit variance and uncorrelated. The rows of Z ∈ R n×r are called individual-specific random effects, and are usually assumed to be i.i.d. The following proposition shows that a spiked covariance model of precisely this form is a special case of the LMM.

Proposition 5. For any r < ∞, let the rows of Z ∈ R n×r be i.i.d. random vectors such that the first equality in [(23)](#b21) holds, let Λ = diag(λ 1 , . . . , λ r ) where λ 1 , . . . , λ r are any strictly positive real numbers and let

$V = [v 1 | • • • |v p ] ⊤ ∈$R p×r be any deterministic matrix such that the second equality in (23) holds. Then, if Y follows the Latent Metric Model specified by:

$Z ⊂ R r , [Z 1 | • • • |Z n ] ⊤ := Z X j (z) := v j , Λ 1/2 z ,(24)$the mean correlation kernel associated with this Latent Metric Model is:

$f (z, z ′ ) = 1 p ⟨z, Λz ′ ⟩ , which has rank r, λ f k = λ k /p, [u f 1 (z) • • • u f r (z)] ⊤ = z ∈ R r$and the following identity holds:

$Y = ZΛ 1/2 V ⊤ + σE.$Proof. The claimed expression for f (z, z ′ ) holds by substituting the definition of X j (z) in ( [24](#formula_65)) into the definition f (z, z ′ ) := p -1 p j=1 E[X j (z)X j (z ′ )] and using the assumption of the proposition that

$V ⊤ V = I r . The eigenfunctions [u f 1 (z) • • • u f r (z)] ⊤ = z ∈ R r$are orthonormal due to the assumption E Z ⊤ Z = nI r and the i.i.d. nature of the rows of Z. The expression for Y in the statement holds by substituting [(24)](#b22) into the definition of Y under the LMM, i.e.,

$Y ij = X j (Z i ) + σE ij .$The relationship between the spiked covariance model (SCM) and the LMM can thus be summarised as follows:

• the metric space (Z, d Z ) in the LMM generalizes the Euclidean domain of individual-specific random effects in the spiked covariance model;

• the eigenfunctions u f k , k ≥ 1, in the LMM generalise the linear dependence on individual-specific random effects in the SCM;

• the random functions X j , j = 1, . . . , p, in the LMM generalise the deterministic, linear functions v j , Λ 1/2 z , j = 1, . . . , p, which in light of [(24)](#b22) are implicit in the SCM;

• the LMM allows for possibly infinite rank, generalising the finite-rank nature of the SCM.

## Gaussian Process Latent Variable model

When Z is a subset of R d and X 1 , . . . , X p are independent and identically distributed Gaussian processes, the LMM reduces to the Gaussian Process Latent Variable model of Lawrence [[50]](#b48), Lawrence and Hyvärinen [[49]](#b47). In this case the elements of the matrix W are independent and identically distributed N (0, 1) and the aforementioned authors derive a likelihood function with these variables out. Assuming f belongs to a given parametric family, e.g., a radial basis function kernel, Lawrence and Hyvärinen [[49]](#b47) proposed maximum a-posteriori estimation of Z 1 , . . . , Z n , parameters of the kernel and σ 2 using a gradient method. Titsias and Lawrence [[90]](#b88) proposed alternative variational methods with enable model assessment. Lawrence [[51]](#b49) derived a Gaussian Markov random field model related to a GPLVM through which Locally Linear Embedding [[78]](#b76) has a statistical interpretation.

## Finite mixture model

Consider the case where Z has finitely many elements, say Z = {1, . . . , m}. For the following discussion it is not important that we take these elements to be the numbers 1, . . . , m, any m distinct abstract elements will do. In this situation the LMM is a form of finite mixture model with random mixture centres. Indeed we see from:

$Y ij = X j (Z i ) + σE ij that [X 1 (z) • • • X p (z)$] can be interpreted as the p-dimensional random centre of a mixture component labeled by z ∈ Z, and the latent variable Z i indicates which mixture component the ith row of the data matrix Y is drawn from. The simple form of the noise in the LMM constrains the generality of this mixture model: recall the elements of E are independent across columns; elements in the same column but distinct rows are uncorrelated; all elements are unit variance.

To make Z into a metric space we consider the discrete metric d Z (z, z ′ ) := 0 for z = z ′ , otherwise d Z (z, z ′ ) := 1. The kernel f is specified by the matrix F ∈ R m×m with entries

$F kl := 1 p p j=1 E[X j (k)X j (l)], k, l ∈ {1, . . . , m}.$In this situation A1 and A6 hold immediately, and r ≤ m. Topological equivalence of M and Z in this situation would mean that M consists of m distinct points {ϕ(1), . . . , ϕ(m)}, each associated with exactly one element of Z. If such topological equivalence were to hold then theorem 1 would tell us that the PCA embedding vectors will be clustered around the m distinct points {Q -1 ϕ(1), . . . ,

$Q -1 ϕ(m)}, with specifically p -1/2 ζ i being close to Q -1 ϕ(Z i ).$To verify topological equivalence it remains to check A2 holds. To this end, suppose that r = m, i.e. F is full rank. Then it is not possible that any two rows of F are identical. That is, for k, l ∈ {1, . . . m} such that k ̸ = l , there must exist some ξ ∈ {1, . . . , m} such that f (k, ξ) = F kξ ̸ = F lξ = f (l, ξ). Thus assumption A2 is satisfied and hence M is topologically equivalent to Z if r = m.

In practical terms, we therefore see that in order to organise the n rows of Y into m clusters, one can first reduce dimension to r = m by computing the PCA embedding and then apply some clustering technique to those embedding vectors. This two-step procedure of PCA followed by clustering, sometimes described as spectral clustering, is very popular in the practice of high-dimensional data analysis and is exactly what Yata and Aoshima [[105]](#b103) recommend in the conclusion of their study of PCA embedding for mixture models in a regime where the number of samples is fixed and the dimension tends to infinity. It is already known that PCA, albeit under slightly different variations and assumptions, allows for "perfect clustering" in high-dimensional mixture models [[56,](#b54)[2]](#b0).  Left: maximum error max i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 , averaged over 50 independent realisations from the model, as a function of n and p. Right: the same error for p = 200, 1000, 15000, as a function of n.

To illustrate the behaviour of the LMM and PCA embedding in this context, we consider a case in which Z = {1, 2, 3} and µ is the uniform distribution on Z; for each j = 1, . . . , p, [X j (1) X j (2) X j (3)] ⊤ ∼ N (0, Σ) where Σ is full-rank; and the elements of E are independent and identically distributed N (0, 1) with σ = 1. Figure [13](#fig_2) shows the error max i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2 , averaged over 50 independent realisations from the model. The plot on the left of the figure indicates that over the ranges considered, for fixed n the error decreases as p increases. Theorem 1 is not informative about the converse situation, when p is fixed and n increases: in this regime, the condition of theorem 4 involving a lower bound on n will eventually be satisfied, but the condition involving a lower bound on p/n will eventually be violated. We examine this in the right plot of figure [13](#fig_2). We see that for fixed p, as n increases the error initially quickly decreases, but then the appears to very slowly increase n ≫ p. We conjecture the former and is related to the 1/ √ n term in [(15)](#b13).

Figure [14](#fig_15) illustrates how this error performance relates to the clustering of the PCA embedding vectors. When n is fixed, we see that as p increases the embedding vectors are increasingly tightly clustered around ϕ(1), ϕ(2), ϕ(3), in keeping with theorem 1. When p is fixed, we see that three clusters of embedding vectors are clearly discernible, but the clusters appear not to shrink as n grows.

Overall we conclude that, whilst theorem 1 shows that both n and p/n being large is sufficient to drive the error to zero, our numerical results suggest that for fixed p the error does not explode as n grows, and even when n ≫ p it may be that the PCA embedding still conveys the topological or geometric structure of M and hence Z. C Proofs and supporting material for section 3

Proof of Proposition 1. Define

$W jk := Z X j (z)u f k (z)µ(dz),(25)$and note that

$W jk = p 1/2 (λ f k ) 1/2 W jk ,(26)$where W jk is defined in [(7)](#b5). Pick any r 0 ≤ r and recall r ∈ {1, 2 . . . , } ∪ {∞} is the number of nonzero eigenvalues (λ f k ) k≥1 . We claim that, for any z ∈ Z, the following equality holds:

$1 p p j=1 E   X j (z) - r0 k=1 u f k (z) W jk 2   = f (z, z) - r0 k=1 λ f k |u f k (z)| 2 .(27)$To verify the equality [(27)](#b25), observe:

$1 p p j=1 E   X j (z) - r0 k=1 u f k (z) W jk 2   = 1 p p j=1 E |X j (z)| 2 - 2 p p j=1 E X j (z) r0 k=1 u f k (z) W jk + 1 p p j=1 r0 k=1 r0 ℓ=1 E W jk W jℓ u f k (z)u f ℓ (z) = f (z, z) -2 r0 k=1 u f k (z) Z f (z, z ′ )u f k (z ′ )µ(dz ′ ) + r0 k=1 r0 ℓ=1 u f k (z)u f ℓ (z) Z Z f (z ′ , z ′′ )u f k (z ′ )u f ℓ (z ′′ )µ(dz ′ )µ(dz ′′ ) = f (z, z) -2 r0 k=1 λ k |u f k (z)| 2 + r0 k=1 λ f k |u f k (z)| 2 = f (z, z) - r0 k=1 λ f k |u f k (z)| 2 ,$where the second equality uses [(25)](#b23) and

$f (z, z ′ ) = p -1 p j=1 E[X j (z)X j (z ′ )],$and the third equality uses the fact that (u f k , λ f k ) k≥1 , by definition, are L 2 (µ)-orthonormal eigenfunctions and eigenvalues of the integral operator associated with the kernel f and the measure µ. By Mercer's theorem (theorem 2) the r.h.s. of ( [27](#formula_75)) converges to zero as r 0 → r, uniformly in z. Each of the summands on the l.h.s. of ( [27](#formula_75)) is nonnegative, so they must also converge to zero uniformly in z. Using this uniform convergence and the fact that for any j = 1, . . . , p and i = 1, . . . , n, the pair of random variables X j and Z i are statistically independent, we have:

$lim r0→r E   X j (Z i ) - r0 k=1 u f k (Z i ) W jk 2   = lim r0→r Z E   X j (z) - r0 k=1 u f k (z) W jk 2   µ(dz) ≤ lim r0→r sup z E   X j (z) - r0 k=1 u f k (z) W jk 2   = 0.(28)$Noting the identity [(26)](#b24) and recalling ϕ

$(z) = [(λ f 1 ) 1/2 u f 1 (z) (λ f 2 ) 1/2 u f 2 (z) • • • ] ⊤$, we find that (28) can equivalently be written:

$X j (Z i ) m.s. = p 1/2 ⟨ϕ(Z i ), W j ⟩ ℓ2 ,$where W j is the jth row of W. This completes the proof of the first identity in [(8)](#b6).

The second identity in [(8)](#b6) follows from the fact that (u f k , λ f k ) k≥1 are orthonormal eigenfunctions/values:

$p j=1 E[W jk W jℓ ] = 1 (λ f k λ f ℓ ) 1/2 Z Z u f k (z) 1 p p j=1 E [X j (z)X j (z ′ )] u f ℓ (z ′ )µ(dz ′ )µ(dz) = λ f ℓ (λ f k λ f ℓ ) 1/2 Z u f k (z)u f ℓ (z)µ(dz) = 1, k = ℓ 0, k ̸ = ℓ .$We introduce the following assumption in order to prove proposition 6 below.

A7. For mixing coefficients φ satisfying k≥1 φ 1/2 (k) < ∞ and all z, z ′ ∈ Z, the sequence {(X j (z), X j (z ′ )); j ≥ 1} is φ-mixing.

Assumption A9 is stated in section D. Proposition 6. Assume A9 and A7, and let q ≥ 1 and φ be as therein. Then there exists a constant C(φ) depending only on φ such that for any δ > 0 and any i, j,

$P p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] ≥ δ Z i , Z j ≤ 1 δ 2q 1 p q C(φ)M (q, σ)$where

$M (q, σ) := sup j≥1 sup z∈Z E |X j (z)| 4q + σ sup i,j≥1 E |E ij | 2q sup j≥1 sup z∈Z E |X j (z)| 2q + σ 2 sup i,j≥1 E |E ij | 4q .$Proof. Fix any i, j and consider the decomposition:

$p -1 ⟨Y i , Y j ⟩ -⟨ϕ(Z i ), ϕ(Z j )⟩ ℓ2 -σ 2 I[i = j] = 4 k=1 ∆ k$where

$∆ 1 := p -1 ⟨X(Z i ), X(Z j )⟩ -f (Z i , Z j ) ∆ 2 := p -1 σ ⟨X(Z i ), E j ⟩ ∆ 3 := p -1 σ ⟨X(Z j ), E i ⟩ ∆ 4 := p -1 σ 2 ⟨E i , E j ⟩ -σ 2 I[i = j] and X(z) := [X 1 (z) . . . X p (z)] ⊤ . Writing ∆ 1 as ∆ 1 = 1 p p k=1 ∆ 1,k , ∆ 1,k := X k (Z i )X k (Z j ) -E [ X k (Z i )X k (Z j )| Z i , Z j ] .$we see that ∆ 1 is an arithmetic mean of p random variables, each of which is conditionally meanzero given Z i , Z j . For ∆ 2 , we have

$∆ 2 = σ p p k=1 ∆ 2,k , ∆ 2,k := X k (Z i )E jk ,$By definition of the LMM, the three collections of random variables, (Z 1 , . . . , Z n ), (X 1 , . . . , X p ) and (E 1 , . . . , E n ) are mutually independent, and the elements of each vector E j ∈ R p are mean-zero and independent. Therefore given Z i , Z j and X 1 , . . . , X p , ∆ 2 is an arithmetic mean of conditionally independent and conditionally mean-zero random variables. The same decomposition holds for ∆ 3 , with i, j interchanged. For ∆ 4 , we have

$∆ 4 = σ 2 p 1≤k≤p ∆ 4,k , ∆ 4,k := E ik E jk -I[i = j].$Recalling from the definition of the LMM that the elements of E are mean zero, unit-variance, uncorrelated across rows, and independent across columns, we see that ∆ 4 is a sum of p mean-zero and mutually independent random variables. The proof proceeds by using a moment inequality for mixing random variables [[101]](#b99)[Lemma 1 [.7]](#) to bound E[|∆ 1 | 2q |Z i , Z j ], and using the Marcinkiewicz-Zygmund inequality to bound

$E[|∆ 2 | 2q |Z i , Z j , X 1 , . . . , X p ], E[|∆ 3 | 2q |Z i , Z j , X 1 , . . . , X p ] and E[|∆ 4 | 2q$]. These bounds are then combined and Markov's inequality applied. The details are similar to [[32]](#b30)[Proof of proposition 2], so are omitted.

Proof of Proposition 2. As explained above the statement of proposition 2, we only need to show that A2 holds if and only if ϕ is one-to-one. We first show that A2 implies ϕ is one-to-one. We prove the contrapositive to this statement. So suppose that ϕ is not one-to-one. Then there must exist z ̸ = z ′ ∈ Z such that ϕ(z) = ϕ(z ′ ). This implies that for any ξ in Z, f (z, ξ) = ⟨ϕ(z), ϕ(ξ)⟩ ℓ2 = ⟨ϕ(z ′ ), ϕ(ξ)⟩ ℓ2 = f (z ′ , ξ), which is the converse of A2.

In the other direction, suppose the converse of A2 holds, i.e., the exists z ̸ = z ′ such that f (z, ξ) = f (z ′ , ξ) for all ξ. By considering the cases ξ = z and ξ = z ′ we find f (z, z) = f (z, z ′ ) = f (z ′ , z ′ ). In turn, ∥ϕ(z) -ϕ(z ′ )∥ 2 ℓ2 = f (z, z) + f (z ′ , z ′ ) -2f (z, z ′ ) = 0, i.e., ϕ(z) = ϕ(z ′ ), and hence ϕ is not one-to-one.

C.1 Proofs and supporting material for section [3.3](#) The purpose of this section is to state some definitions and intermediate results, building towards the proofs of propositions 3 and 4. Recall that the term "continuous path" was used in A3. From henceforth we just say "path" for short.

The following definitions are standard in metric geometry [[12]](#b10). For x, x ′ ∈ M, a path in M with end-points x, x ′ is a continuous function γ : [0, 1] → M such that γ 0 = x and γ 1 = x ′ , where M is equipped with the distance ∥ • -• ∥ ℓ2 . With n ≥ 1, a non-decreasing sequence t 0 , . . . , t n such that t 0 = 0 and t n = 1, is called a partition. Given a path γ and a partition P = (t 0 , . . . , t n ), define χ(γ, P) := n k=1 ∥γ t k -γ t k-1 ∥ ℓ2 . The length of γ is L(γ) := sup P χ(γ, P), where the supremum is over all possible partitions.

When Z is a subset of R d , a path η in Z with end-points z, z ′ is a continuous function η : [0, 1] → Z such that η 0 = z, η 1 = z ′ , and with χ(η, P)

$:= n k=1 ∥η t k -η t k-1 ∥ R d the length of η is L(η) := sup P χ(η, P).$The shortest path lengths, also known as geodesic distances, in M and Z are:

$d geo M (x, x ′ ) := inf γ:γ0=x,γ1=x ′ L(γ) d geo Z (z, z ′ ) := inf η:η0=z,η1=z ′ L(η),(29)$where the infima are over all paths in respectively M and Z with the indicated end-points.

A8. Assume A3 holds and with d as therein, additionally assume there exists a closed ball Z ⊂ R d centered on the origin such that: Z ⊂ Z; the definition of f (z, z ′ ) can be extended from Z × Z to Z × Z; f is C 2 on Z × Z and the matrix H ξ ∈ R d×d with elements:

$(H ξ ) ij := ∂ 2 f ∂z i ∂z ′ j (ξ,ξ)$is positive-definite for all ξ ∈ Z.

The statement of the following theorem, from [[99]](#b97), is paraphrased slightly in order to match the assumptions of interest here. Theorem 3 ([99], Thm 1.). Assume A2 and A8. Then ϕ is a bi-Lipschitz homeomorphism between Z and M. Let x, x ′ be any two points in M, and let γ be any path in M of finite length, with end-points x, x ′ . Define η : [0, 1] → Z by η t := ϕ -1 (γ t ). Then η is a path in Z with L(η) < ∞. For any ϵ > 0 there exists a partition P ϵ such that for any partition P = (t 0 , . . . , t n ) satisfying P ϵ ⊆ P,

$L(γ) - n k=1 η t k -η t k-1 , H ηt k-1 (η t k -η t k-1 ) 1/2 ≤ ϵ. (30$$)$Proof of proposition 3. Under the assumptions of the proposition, by direct calculation H ξ = -2g ′ (0)I d for all ξ ∈ Z and A8 holds. Fix any z, z ′ ∈ Z and let η be any finite length path in Z with these end-points. By theorem 3, ϕ is Lipschitz, so γ defined by γ t := ϕ(η t ) has finite length. Define x := ϕ(z), x ′ := ϕ(z ′ ). Applying theorem 3, we have from [(30)](#b28) that for any ϵ > 0 there exists a partition P ϵ such that for any P = (t 0 , . . . , t n ) satisfying P ϵ ⊆ P, L(γ) --2g ′ (0)χ(η, P) ≤ ϵ. [(31)](#b29) Also, using the definition of path length L(η) and the triangle inequality, there exists P ϵ such that for any partition P satisfying P ϵ ⊆ P, we have:

$|L(η) -χ(η, P)| ≤ ϵ. (32$$)$Choosing P to be the union of P ϵ and P ϵ , i.e., if τ ∈ P ϵ or P ϵ , then τ ∈ P, we find that ( [31](#)) and ( [32](#formula_94)) are satisfied simultaneously. Since ϵ was arbitrarily small, we find that L(γ) = -2g ′ (0)L(η). By theorem 3, ϕ is a bi-Lipschitz homeomorphism, so γt = ϕ(η t ) defines a bijection between the set of finite-length paths γ in Z with end-points ϕ(z), ϕ(z ′ ) and the set of finite length paths η in Z with end-points z, z ′ . Therefore by taking the infimum over η on both sides of L(γ) = -2g ′ (0)L(η) where γ is defined by γ t = ϕ(η t ) as above, we find that

$d geo M (ϕ(z), ϕ(z ′ )) = -2g ′ (0)d geo Z (z, z ′ )(33)$as required.

Proof of proposition 4. For the Z in question, we have g(⟨z, z

$′ ⟩ R d ) = g(1 -∥z -z ′ ∥ 2 R d /2$). The proof is completed by applying proposition 3 and using the chain rule of differentiation.

## D Proof and supporting results for theorem 1

Theorem 1 is a corollary to theorem 4. The proofs of both these theorems are in section D. The following assumption is a more detailed version of A5.

A9. For some q ≥ 1, sup

$j≥1 sup z∈Z E[|X j (z)| 4q ] < ∞ and sup j≥1 sup i≥1 E[|E ij | 4q ] < ∞.$Theorem 4. Assume A4, A6 and A9, and let q ≥ 1 and r < ∞ be as therein. For min(p, n) ≥ r, let Y ∈ R n×p follow the LMM from section 2 and let ζ 1 , . . . , ζ n be the dimension-r PCA embedding.

Then there exists a random orthogonal matrix Q ∈R r×r depending on n and p such that for any δ ∈ (0, 1) and ϵ ∈ (0, 1], if

$n ≥ c 1 σ 2 r 1/2 1 ∨ σ 2 r 1/2 ϵ 2 ∨ log r δ and p n ≥ c 2 (q) r δ 1/q ϵ 2 , then max i=1,...,n p -1/2 Qζ i -ϕ(Z i ) 2 ≤ ϵ$with probability at least 1 -δ. Here c 1 and c 2 (q) are constants depending on the suprema in A9 and the quantity inf p≥1 λ f r which is strictly positive under A6; and ∥ • ∥ 2 is the Euclidean norm.

## D.1 Definitions and preliminaries

Throughout section D the probability measure µ in the LMM is considered fixed, (λ f k , u f k ) k≥1 are as in section 2, and assumption A6 is taken to hold, so that the rank of f is finite, i.e., r < ∞.

## D.1.1 Notation concerning vectors and matrices in general

We notationally index the eigenvalues of a generic symmetric matrix A in a non-increasing but otherwise arbitrary order λ 1 (A) ≥ λ 2 (A) ≥ • • • . For a vector x with elements x i , ∥x∥ ∞ := max i |x i | and ∥x∥ 2 := i |x i | 2 , and the spectral norm and Frobenius norm of matrices are denoted ∥ • ∥ 2 and ∥ • ∥ F .

## D.1.2 Some matrices of interest

Let the matrix Φ ∈ R n×r be defined by

$Φ := [ϕ(Z 1 )| • • • |ϕ(Z n )] ⊤ ,$Let Λ Y ∈ R r×r be the diagonal matrix with diagonal elements the eigenvalues λ 1 (p -1 YY ⊤ ) . . . , λ r (p -1 YY ⊤ ), and let U Y ∈ R n×r have as its columns orthonormal eigenvectors associated with these eigenvalues. Since Φ ∈ R n×r and r ≤ min(p, n), the matrix ΦΦ ⊤ has rank at most r. Let Λ Φ ∈ R r×r be the diagonal matrix with diagonal elements which are the eigenvalues λ 1 (ΦΦ ⊤ ), . . . , λ r (ΦΦ ⊤ ), and let U Φ ∈ R n×r have as its columns orthonormal eigenvectors associated with these eigenvalues. Let F 1 ΣF ⊤ 2 denote the full singular value decomposition of U ⊤ Φ U Y and define the random orthogonal matrix

$F ⋆ := F 1 F ⊤ 2 .$
## D.1.3 Some events of interest

With U j denoting the jth column of U Φ , define:

$A 1 (ϵ) := ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≤ ϵn A 2 (ϵ) := n i=1 B Y,i (ϵ) ∩ r i=1 B Φ,i (ϵ) A 3 (ϵ) := max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ ϵn 1/2 A rank := rank(YY ⊤ ) ≥ r ∩ rank(ΦΦ ⊤ ) = r B Y,i (ϵ) := λ f i (1 -ϵ) ≤ 1 n λ i (p -1 YY ⊤ ) ≤ λ f i (1 + ϵ) , 1 ≤ i ≤ r, 1 n λ i (p -1 YY ⊤ ) ≤ ϵλ f r , r + 1 ≤ i ≤ n. B Φ,i (ϵ) := (1 -ϵ)λ f i ≤ 1 n λ i (ΦΦ ⊤ ) ≤ (1 + ϵ)λ f i , 1 ≤ i ≤ r.$
## D.2 Proofs of theorems 4 and 1

Proof of theorem 4. Let F 1 ΣF ⊤ 2 be the full singular value decomposition of U ⊤ Φ U Y and define the random orthogonal matrix F ⋆ := F 1 F ⊤ 2 . On the event A rank we have U Φ Λ Φ U ⊤ Φ = ΦΦ ⊤ , and applying lemma 4 we find there exists a random orthogonal matrix

$Q such that U Φ Λ 1/2 Φ = Φ Q, hence [U Φ Λ 1/2 Φ F ⋆ ] i = ϕ(Z i ) ⊤ Q for all i = 1, . . . n, where Q := QF ⋆ is orthogonal and [•] i denotes the ith row of a matrix. Lemma 5 shows that [U Y Λ 1/2 Y ] i = p -1/2 ζ i .$Combining these observations we have shown that on the event A rank ,

$∥p -1/2 Qζ i -ϕ(Z i )∥ 2 = ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 , i = 1, . . . , n.(34)$Now fix any ϵ 1 > 0, ϵ 2 ∈ (0, 1/2) and ϵ 3 > 0. Note that the event A rank is a superset of A 2 (ϵ 2 ) and thus A 1 (ϵ 1 )∩A 2 (ϵ 2 )∩A 3 (ϵ 3 ) ⊆ A rank . Throughout the remainder of the proof of theorem 4 we shall establish various identities and inequalities involving random variables, random matrices, etc; all such identifies and inequalities to be understood as holding on the event A 1 (ϵ 1 )∩A 2 (ϵ 2 )∩A 3 (ϵ 3 ), although we shall avoid making this explicit in our notation in order to avoid repetition. For example, for two random matrices say A and B, we write "A = B" as shorthand for "A(ω) = B(ω) for all ω ∈ A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) ∩ A 3 (ϵ 3 )" and similarly for two random variables say X, Y , we write "X ≤ Y " as shorthand for "X(ω) ≤ Y (ω) for all ω ∈ A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) ∩ A 3 (ϵ 3 )".

Noting that on the event A rank , the matrices Λ -1/2 Y and Λ -1/2 Φ are well-defined, let us introduce:

$C 1 := F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ C 2 := (U ⊤ Φ U Y -F ⋆ )Λ 1/2 Y C 3 := U Y -U Φ F ⋆ = U Y -U Φ U ⊤ Φ U Y + U Φ (U ⊤ Φ U Y -F ⋆ ) D 1 := U Φ C 1 D 2 := U Φ C 2 D 3 := (I -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y D 4 := -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y D 5 := (p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ )$We now claim that:

$U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + 5 i=1 D i ,(35)$which up to some notational differences, is the same decomposition used by Lyzinski et al. [58, Proof of Thm 18.] in the analysis of spectral methods for community detection in graphs. To verify the decomposition [(35)](#b33), observe:

$U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ = U Y Λ 1/2 Y -U Φ F ⋆ Λ 1/2 Y + U Φ C 1 = (I n -U Φ U ⊤ Φ )U Y Λ 1/2 Y + U Φ C 2 + U Φ C 1 = (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )U Y Λ -1/2 Y(36)$$+ U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -Φ ⊤ Φ)U Φ F ⋆ Λ -1/2 Y -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y + (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y + U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + (p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ) -U Φ U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ F ⋆ Λ -1/2 Y + (I n -U Φ U ⊤ Φ )(p -1 YY ⊤ -ΦΦ ⊤ )C 3 Λ -1/2 Y + U Φ C 2 + U Φ C 1 = (p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ + D 5 + D 4 + D 3 + D 2 + D 1(37)$where [(36)](#b34)

$holds because U Y Λ 1/2 Y = p -1 Y ⊤ YU Y Λ -1/2 Y and U Φ U ⊤ Φ ΦΦ ⊤ = ΦΦ ⊤ .$The proof proceeds by bounding the Frobenius norm of each matrix D i , i = 1, . . . , 5.. Using lemma 2,

$∥D 1 ∥ F = ∥C 1 ∥ F ≤ r 1/2 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2 n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + nϵ 1 + σ 2 = r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) 2(1 -ϵ 2 ) 1/2 (λ f r ) 1/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + 1 .(38)$Using lemma 1,

$∥D 2 ∥ F ≤ r 1/2 ∥C 2 ∥ 2 = r 1/2 n 1/2 [λ f 1 (1 + ϵ 2 )] 1/2 ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 . (39$$)$Again using lemma 1 and the fact that

$U Y -U Φ U ⊤ Φ U Y = (U Y U ⊤ Y -U Φ U ⊤ Φ )U Y , ∥D 3 ∥ F ≤ 2r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥C 3 ∥ 2 ∥Λ -1/2 Y ∥ 2 ≤ 2r 1/2 (ϵ 1 n + σ 2 ) n 1/2 λ f r (1 -ϵ 2 ) 1/2 ∥U Y U ⊤ Y -U Φ U ⊤ Φ ∥ 2 + ∥U ⊤ Φ U Y -F ⋆ ∥ 2 ≤ 2r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -ϵ 2 ) 3/2 1 + ϵ 1 + n -1 σ 2 λ f r (1 -ϵ 2 )(40)$Directly:

$∥D 4 ∥ F ≤ r 1/2 ∥D 4 ∥ 2 ≤ r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥Λ -1/2 Y ∥ 2 ≤ r 1/2 (ϵ 1 n + σ 2 ) n 1/2 λ f r (1 -ϵ 2 ) 1/2 = r 1/2 n 1/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -ϵ 2 ) 1/2(41)$Using lemma 2,

$∥D 5 ∥ F = ∥(p -1 YY ⊤ -ΦΦ ⊤ )U Φ (F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ )∥ F ≤ r 1/2 ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ∥ F ≤ r 1/2 (ϵ 1 n + σ 2 ) ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 3/2 (λ f r ) 3/2 (1 -ϵ 2 ) 3/2 ≤ rn 2 (ϵ 1 + n -1 σ 2 ) 2n 3/2 (λ f r ) 3/2 (1 -ϵ 2 ) 3/2 (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + ϵ 1 + σ 2 n = rn 1/2 (ϵ 1 + n -1 σ 2 ) 2 2(λ f r ) 3/2 (1 -ϵ 2 ) 3/2 (ϵ 1 + n -1 σ 2 ) λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + 1(42)$Having obtained the above bounds on ∥D i ∥ F , for i = 1, . . . , 5, we turn to the first term on the r.h.s. of [(35)](#b33). Writing [•] i to indicate the ith row of a matrix,

$max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ ] i ∥ 2 (43) = max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ ] i ∥ 2 ≤ 1 n 1/2 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ ] i ∥ 2 ≤ r 1/2 n 1/2 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ )U j ∥ ∞ ≤ r 1/2 ϵ 3 (λ f r ) 1/2 (1 -ϵ 2 ) 1/2 . (44$$)$where U j is the jth column of U Φ .

Recall that at the start of the proof we fixed arbitrary values ϵ 1 > 0, ϵ 2 ∈ (0, 1/2) and ϵ 3 > 0. We now need to work with a specific numerical value for ϵ 2 , so let us take it to be 1/4. Elementary manipulations of the bounds ( [38](#formula_110))-( [42](#formula_115)) then show that there exists c0 depending only on the constants c max λ , c min λ in lemma 7 such that

$∥D 1 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n ϵ 1 + σ 2 n + 1 ∥D 2 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n 2 ∥D 3 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n 2 ϵ 1 + σ 2 n + 1 ∥D 4 ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n ∥D 5 ∥ F ≤ c0 rn 1/2 ϵ 1 + σ 2 n 2 ϵ 1 + σ 2 n + 1 . Now assuming n ≥ 2σ 2 r 1/2(45)$i.e, n -1 σ 2 r 1/2 ≤ 1/2, and assuming

$ϵ 1 r 1/2 ≤ 1/2(46)$we have

$ϵ 1 + σ 2 n r 1/2 ≤ 1.$Applying this inequality in the above bound on ∥D 5 ∥ F and allowing c0 to increase where necessary we obtain: max i=1,...,5

$∥D i ∥ F ≤ c0 r 1/2 n 1/2 ϵ 1 + σ 2 n$Combining this estimate with [(44)](#b42) and again allowing c0 to increase as needed,

$max i=1,...,n ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 ≤ max i=1,...,n ∥[(p -1 YY ⊤ -ΦΦ ⊤ )U Φ Λ -1/2 Φ F ⋆ ] i ∥ 2 + 5 i=1 ∥D i ∥ F ≤ r 1/2 c0 n 1/2 ϵ 1 + σ 2 n + r 1/2 c0 ϵ 3 .(47)$Now fix any ϵ ∈ (0, 1] and let us strengthen [(45)](#b43) to

$n ≥ 2σ 2 r 1/2 ∨ 9 ϵ 2 c2 0 rσ 4(48)$so that r 1/2 c0 n -1/2 σ 2 ≤ ϵ/3. Then setting ϵ 1 := ϵ/(3n 1/2 r 1/2 c0 ) (which satisfies (46) since c0 ≥ 1), ϵ 3 := ϵ/(3r 1/2 c0 ) and recalling that we have already chosen ϵ 2 := 1/4 we have as a consequence of (47),

$P max i=1,...,n ∥[U Y Λ 1/2 Y -U Φ Λ 1/2 Φ F ⋆ ] i ∥ 2 ≤ ϵ ≥ 1 -P(A 1 (ϵ/[3n 1/2 r 1/2 c0 ]) c ) -P(A 2 (1/4) c ) -P(A 3 (ϵ/[3r 1/2 c0 ]) c ).$Now fix any δ ∈ (0, 1). By lemma 9, proposition 7 and lemma 11, there exists constants c1 (q), c2 and c3 (q) (depending only on the constants c max λ , c min λ from lemma 7 and the constants c X (2q), c E (2q) from lemma 9) such that

$p n ≥ c1 (q) 1/q r δ 1/q ϵ 2 ⇒ P(A 1 (ϵ/[3n 1/2 r 1/2 c0 ]) c ) ≤ δ 3 . n ≥ c2 σ 2 ∨ log r δ and p ≥ c2 δ 1/q ⇒ P(A 2 (1/4) c ) ≤ δ 3 . p n 1/q ≥ c3 (q) 1/q r 1+1/q δ 1/q ϵ 2 ⇒ P(A 3 (ϵ/[3r 1/2 c0 ]) c ) ≤ δ 3 .$Combining these conditions with (48) and appropriately defining c 1 and c 2 gives the conditions in the statement of the theorem. Recalling (34), the proof is complete.

Proof of theorem 1. If A5 holds, then A9 holds with q = 1. We may then apply theorem 4 in the case q = 1, and in order for the lower bound conditions on n and p/n in the statement of theorem 4 to be satisfied for some given δ and ϵ, it is sufficient that:

$n ≥ -č 1 log δ ϵ 2 and p n ≥ č2 ϵ 2 δ ,(49)$for suitable constants č1 > 0 and č2 > 0 depending on σ, c 1 , c 2 (q) and sup p≥1 r, noting the latter supremum is finite under A6.

To complete the proof we need to show that for any δ ∈ (0, 1) there exists ϵ 0 > 0 and M > 0 such that if (1/ √ n + n/p) -1 > M , then:

$P max i=1,...,n ∥p -1/2 Qζ i -ϕ(Z i )∥ 2 > ϵ 0 1 √ n + n p < δ.(50)$So to proceed, fix any δ ∈ (0, 1), define ϵ 0 := √ -č 1 log δ ∨ č2 /δ, M := ϵ 0 and ϵ := ϵ 0 (1/ √ n + n/p). Assume that (1/ √ n + n/p) -1 ≥ M and notice that in this situation ϵ ∈ (0, 1], which is a requirement of theorem 4. It follows from the definition of ϵ 0 that:

$ϵ 2 0 ≥ -č 1 log δ ≥ -č 1 log δ 1 + n/ √ p 2 = -č 1 log δ n 1/ √ n + n/p 2 ,$and rearranging then using the above definition of ϵ gives:

$n ≥ -č 1 log δ ϵ 2 ,$i.e., the first inequality in (49) holds. Similarly

$ϵ 2 0 ≥ č2 δ ≥ č2 √ p/n + 1 2 δ = č2 p n 1/ √ n + n/p 2 δ hence p n ≥ č2 ϵ 2 δ ,$i.e., the second inequality in (49) holds. Thus by theorem 4,

$P max i=1,...,n ∥p -1/2 Qζ i -ϕ(Z i )∥ 2 > 1 √ n + n p ϵ 0 < δ.$which is (50).

## D.3 Matrix estimates

Lemma 1. Assume A6. Then for any ϵ 1 > 0 and ϵ 2 ∈ (0, 1/2), on the event

$A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 )$we have

$∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) and ∥U ⊤ Φ U Y -F ⋆ ∥ 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 .$Proof. In outline, the proof follows Lyzinski et al. [58, Proof of Prop. 16], although we work with the spectral rather than Frobenius norm. On the event in the statement we have:

$|λ r (ΦΦ ⊤ ) -λ r+1 (p -1 YY ⊤ )| ≥ nλ f r (1 -2ϵ$2 ) > 0 and with σ i denoting the ith singular value of U ⊤ Φ U Y and σ i = cos(θ i ), the Davis-Kahan sin(θ) theorem gives:

$∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 = max i | sin(θ i )| ≤ ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 |λ r (ΦΦ ⊤ ) -λ r+1 (p -1 YY ⊤ )| ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) .(51)$Therefore

$∥U ⊤ Φ U Y -F ⋆ ∥ 2 = ∥F 1 ΣF ⊤ 2 -F 1 F ⊤ 2 ∥ 2 = ∥F 1 (Σ -I r )F ⊤ 2 ∥ 2 = ∥Σ -I r ∥ 2 = max i=1,...,r |1 -σ i | ≤ max i=1,...,r |1 -σ 2 i | = max i=1,...,r | sin(θ i )| 2 ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2$where for the first inequality uses ∥U ⊤ Φ U Y ∥ 2 ≤ 1 and the second inequality is from (51). Lemma 2. Assume A6. For any ϵ 1 > 0, ϵ 2 ∈ (0, 1/2), on the event

$A 1 (ϵ 1 ) ∩ A 2 (ϵ 2 ) we have ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F ≤ r 1/2 n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 ) 1 + 2 λ f 1 λ f r 1 + ϵ 2 1 -2ϵ 2 + nϵ 1 + σ 2 , ∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2 , ∥F ⋆ Λ -1/2 Y -Λ -1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F n(1 -ϵ 2 )λ f r .$Proof. Using a decomposition idea from [58, proof of lemma 17], with

$R := U Y -U Φ U ⊤ Φ U Y ,$we have

$F ⋆ Λ Y -Λ Φ F ⋆ = (F ⋆ -U ⊤ Φ U Y )Λ Y + U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R + U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y + Λ Φ (U ⊤ Φ U Y -F ⋆ ) hence ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ 2 ≤ ∥U ⊤ Φ U Y -F ⋆ ∥ 2 (∥Λ Y ∥ 2 + ∥Λ Φ ∥ 2 )(52)$$+ ∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R∥ 2(53)$$+ ∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y ∥ 2(54)$For the term on the r.h.s. of ( [52](#formula_139)), on the event in the statement of the present lemma and using lemma 1 we have:

$∥U ⊤ Φ U Y -F ⋆ ∥ 2 (∥Λ Y ∥ 2 + ∥Λ Φ ∥ 2 ) ≤ ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) 2 2nλ f 1 (1 + ϵ 2 ).$For the term in (53

$), using R = (U Y U ⊤ Y -U ⊤ Φ U Φ )U Y ,$we have again on the event in the statement of the present lemma and using lemma 1,

$∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )R∥ 2 ≤ ∥p -1 YY ⊤ -ΦΦ ⊤ ∥ 2 ∥R∥ 2 ≤ (∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 + σ 2 )∥U Y U ⊤ Y -U ⊤ Φ U Φ ∥ 2 ≤ (ϵ 1 n + σ 2 ) ϵ 1 + n -1 σ 2 λ f r (1 -2ϵ 2 ) = n (ϵ 1 + n -1 σ 2 ) 2 λ f r (1 -2ϵ 2 )$.

For the term in [(54)](#b52),

$∥U ⊤ Φ (p -1 YY ⊤ -ΦΦ ⊤ )U Φ U ⊤ Φ U Y ∥ 2 ≤ ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )∥ 2 + σ 2 ∥U ⊤ Φ U Y ∥ 2 ≤ nϵ 1 + σ 2 .$The bound on ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F given in the statement holds by combining the above spectral norm bounds.

For the bound on

$∥F ⋆ Λ 1/2 Y -Λ 1/2$Φ F ⋆ ∥ F we use the fact that the elements of

$F ⋆ Λ 1/2 Y -Λ 1/2$Φ F ⋆ can be written:

$(F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ) ij = (F ⋆ ) ij λ j (p -1 YY ⊤ ) 1/2 -λ i (ΦΦ ⊤ ) 1/2 (F ⋆ ) ij = (F ⋆ ) ij [λ j (p -1 YY ⊤ ) -λ i (ΦΦ ⊤ )] λ j (p -1 YY ⊤ ) 1/2 + λ i (ΦΦ ⊤ ) 1/2 hence |(F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ) ij | ≤ |(F ⋆ Λ Y -Λ Φ F ⋆ ) ij | 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2$, and so

$∥F ⋆ Λ 1/2 Y -Λ 1/2 Φ F ⋆ ∥ F ≤ ∥F ⋆ Λ Y -Λ Φ F ⋆ ∥ F 2n 1/2 (1 -ϵ 2 ) 1/2 (λ f r ) 1/2$.

## The bound on ∥F

$⋆ Λ -1/2 Y -Λ -1/2 Φ$F ⋆ ∥ F in the statement is obtained in a similar manner using the fact that for any a, b > 0, a -1/2 -b -1/2 = (b 1/2 -a 1/2 )/(a 1/2 b 1/2 ) .

Thus by computing the PCA embedding ζ 1 , . . . , ζ n and rescaling by p -1/2 , we are, in effect, computing the n rows of

$U Y Λ 1/2 Y , where U Y Λ 1/2 Y (U Y Λ 1/2 Y ) ⊤ = U Y Λ Y U ⊤$Y is a rank-r approximation to p -1 YY ⊤ . Lemma 6. Assume A6. Then p -1 E[YY ⊤ |Z 1 , . . . , Z n ] = ΦΦ ⊤ + σ 2 I n .

Proof. Let X ∈ R n×p be the matrix with entries X ij := X j (Z i ). According to the model specification in section 2, X and E are independent, and E[EE ⊤ ] = pI n . Thus: The existence of c max λ as required follows from the above inequalities combined with:

$λ f 1 ≤ ∞ k=1 λ f k = ∞ k=1 λ f k E[|u f k (Z 1 )| 2 ] = E[f (Z 1 , Z 1 )] ≤ sup z f (z, z).$
## D.6 Matrix concentration results

The following matrix-valued version of the Bernstein inequality can be found in, e.g., [92, Thm 1.6. Lemma 8. Assume A6. For any t ≥ 0,

$P ∥n -1 Φ ⊤ Φ -n -1 E[Φ ⊤ Φ]∥ 2 ≥ t ≤ 2r exp -t 2 n/2 (c max λ ) 2 + c max λ t/3 ,$where c max λ is as in lemma 7.

Proof. Apply theorem 5 with

$M i = 1 n ϕ(Z i )ϕ(Z i ) ⊤ -E[ 1 n ϕ(Z i )ϕ(Z i ) ⊤ ], ∥M i ∥ 2 ≤ 1 n ∥ϕ(Z i )ϕ(Z i ) ⊤ ∥ 2 + 1 n ∥E[ϕ(Z i )ϕ(Z i ) ⊤ ]∥ 2 = 1 n ∥ϕ(Z i )∥ 2 2 + 1 n λ f 1 = 1 n f (Z i , Z i ) + 1 n λ f 1 ≤ 1 n c max λ =: L and v(M) = E i M i i M i 2 = E i M i M i 2 ≤ 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 + 1 n ∥E[ϕ(Z 1 )ϕ(Z 1 ) ⊤ ] 2 ∥ 2 ≤ 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 + 1 n ∥E[ϕ(Z 1 )ϕ(Z 1 ) ⊤ ] 2 ∥ 2 = 1 n E ϕ(Z 1 )ϕ(Z 1 ) ⊤ 2 2 + 1 n (λ f 1 ) 2 = 1 n E ∥ϕ(Z 1 )∥ 4 2 + 1 n (λ f 1 ) 2 ≤ 1 n (c max λ ) 2 .$Lemma 9. Assume A6 and A9 with some q ≥ 1. Then for any t > 0, P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t ≤ (16) q (2q -1) q n 2q t 2q 1 p q c X (2q) 1/2q + σ 2 c E (2q) 1/2q 2q where c X (q) := sup 

$YY ⊤ = p j=1 Y j Y ⊤ j .(56)$Observe that under the model of section 2, conditional on (Z 1 , . . . , Z n ) the summands in [(56)](#b54) are independent and as per lemma 6, the conditional expectation of YY ⊤ given Z 1 , . . . , Z n is:

$pΦΦ ⊤ + pσ 2 I n .$The main tool we use from hereon is a direct combination of the matrix Chebyshev inequality [[70,](#b68)[Prop. 3](#).1] and the matrix polynomial Effron-Stein inequality [[70,](#b68)[Thm 4.2]](#), applied under the regular conditional distribution of (Y 1 , . . . , Y p ) given (Z 1 , . . . , Z n ). These inequalities taken together tell us that, for any q ≥ 1, the following holds almost surely:

$P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t Z 1 , . . . , Z n ≤ 1 t 2q E ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2q$S2q Z 1 , . . . , Z n ≤ 2 q (2q -1) q t 2q E ∥Σ∥ q Sq Z 1 , . . . , Z n .

Here ∥ • ∥ Sq is the Schatten q-norm and Σ ∈ R n×n is the variance proxy:

$Σ := 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z 1 , . . . , Z n ,(57)$where, conditional on Z 1 , . . . , Z n , Ỹj is an independent copy of Y j . For brevity in the remainder of the proof we shall write Z ≡ (Z 1 , . . . , Z n ), and to avoid repetitive statements of "almost surely", every inequality involving conditional expectations is to be understood as holding in the almost sure sense. We estimate: Here (58) holds by the second claim of lemma 10; 59 holds by first claim of lemma 10 combined with the fact that x → x q is convex for x ≥ 0 (recall q ≥ 1); ≤ n i=1 E |X j (Z i ) + σE ij | 4q Z 1/2q ≤ 2 n i=1 E |X j (Z i )| 4q Z 1/2q + E |σE ij | 4q Z 1/2q ≤ 2n sup l≥1 sup z∈Z E |X l (z)| 4q 1/2q + σ 2 sup i≥1,l≥1 E |E il | 4q 1/2q

$E ∥Σ∥ q Sq Z 1/q = 1 2p 2 E    p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z q Sq Z    1/q ≤ 1 2p 2 p j=1 E E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 Y j , Z q Sq Z 1/q (58) ≤ 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2 q Sq Z 1/q (59) = 1 2p 2 p j=1 E Y j Y ⊤ j -Ỹj Ỹ ⊤ j 2q S2q Z 1/q ≤ 1 2p 2 p j=1 2E Y j Y ⊤ j 2q S2q Z 1/2q 2(60)$, where the final inequality uses the facts that X j , Z and E are independent.

Combining the above estimates we find:

$P ∥p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n ∥ 2 ≥ t$Z 1 , . . . , Z n ≤ 2 q (2q -1) q t 2q 2 p q 4 q n 2q sup j≥1 sup z∈Z E |X j (z)| 4q 1/2q + σ 2 sup i≥1,j≥1 E |E ij | 4q 1/2q 2q = (16) q (2q -1) q n 2q t 2q 1 p q sup j≥1 sup z∈Z E |X j (z)| 4q 1/2q + σ 2 sup i≥1,j≥1 E |E ij | 4q 1/2q 2q , from which the result follows by the tower property of conditional expectation. Proposition 7. Assume A6 and A9 with some q ≥ 1. For any δ, ϵ ∈ (0, 1), if n ≥ 3σ 2 ϵc min λ ∨ log 1 δ + log(4r) 1 ϵ 2 2((c max λ ) 2 + ϵc max λ c min λ /9) (c min λ ) 2 /9 , Lemma 10. For any m 1 , m 2 ≥ 1 and any matrix norm ∥ • ∥ ⋆ on R m1×m2 , ∥ • ∥ ⋆ is convex. For any random A, B ∈ R m1×m2 and any 1 ≤ q < ∞ such that E [∥A∥ q ⋆

$] ∨ E [∥B∥ q ⋆ ] < ∞, E [∥A + B∥ q ⋆ ] 1/q ≤ E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ] 1/q . ≤ E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ]$1/q E ∥A + B∥ (q-1)( q q-1 ) ⋆

$1-1 q = E [∥A∥ q ⋆ ] 1/q + E [∥B∥ q ⋆ ] 1/q E [∥A + B∥ q ⋆ ] E [∥A + B∥ q ⋆ ] 1/q .$The proof is completed by multiplying both sides by E [∥A + B∥ q ⋆ ] 1/q /E [∥A + B∥ q ⋆ ].

Lemma 11. Assume A6, and A9 with some q ≥ 1. Let U j denote the jth column of U Φ . Then there exists a constant b(q) depending only on q such that for any t > 0, P max j=1,...,r

$∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ t ≥ 1 - n 1+q r t 2q p q b(2q)2$6q-1 max j=1,...,p sup z∈Z E |X j (z)| 4q + σ 4q max i=1,...,n,j=1,...,p E[|E ij | 4q ] .

Proof. The ith element of (p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j can be written in the form:

$p -1 p k=1 ∆ ij (k)$where

$∆ ij (k) := Y (i) k Y ⊤ k U j -E Y (i) k Y ⊤ k U j Z 1 , . . . , Z n$and for any i, j, the random variables ∆ ij (k), k = 1, . . . , p are conditionally independent and conditionally mean zero given Z 1 , . . . , Z n . Applying Markov's inequality, the Marcinkiewicz-Zygmund inequality and Minkowski's inequality, all conditionally on Z ≡ (Z 1 , . . . , Z n ), we have for any q ≥ 1 the following inequalities hold almost surely, Re-arranging the expression for ∆ ij (k), applying the Cauchy-Schwartz inequality and ∥U j ∥ 2 = 1, we estimate

$P p -1 p k=1 ∆ ij (k) ≥ t Z ≤ 1 t 2q E   p -1 p k=1 ∆ ij (k)$$|∆ ij (k)| ≤ Y (i) k Y k -E Y (i) k Y k Z 2 ∥U j ∥ 2 ≤ |Y (i) k |∥Y k ∥ 2 + E |Y (i) k |∥Y k ∥ 2 Z$and so 

$E |∆ ij (k)| 2q Z ≤ 2 2q E (|Y (i) k | 2 ∥Y k ∥ 2 2 ) q Z = 2 2q E n l=1 |Y (i) k | 2 |Y (l) k | 2 q Z ≤ 2 2q n l=1 E (|Y (i) k | 2 |Y (l) k | 2 ) q Z 1/q q ≤ 2 2q$Combining the almost sure upper bounds ( [64](#formula_169)) and ( [63](#)), using the tower property of conditional expectation and then taking a union bound over i = 1, . . . , n and j = 1, . . . , r, we find:

$P max j=1,...,r ∥(p -1 YY ⊤ -ΦΦ ⊤ -σ 2 I n )U j ∥ ∞ ≤ t ≥ 1 - n 1+q r t 2q p q b(2q)2 6q-1 sup j≥1 sup z∈Z E |X j (z)| 4q + σ 4q sup i≥1,j≥1 E[|E ij | 4q ] ,$which completes the proof.

E Supplementary figures for Section 4.2

![Figure 1: A collection of images reduced in dimension using PCA.]()

![Figure 2: Planaria example. Left: first 2 dimensions of the PCA embedding. Right: representation of the data in 2 dimensions obtained by first reducing to 14 dimensions using PCA, then applying t-SNE.]()

![Figure 3: Torus example. Left: grey wireframe of Z, a torus, with colour bars indicating coordinates with respect to two circles. Both the middle and right plots show the same n = 4000 points, Z 1 , . . . , Z 4000 , which are sampled uniformly on the torus, coloured by their coordinates with respect to each of the two circles.]()

![Figure 5: Torus example. Blue: numerical shortest path lengths between points in M vs. between the corresponding points in Z. Red: theoretical scaling relationship √ 2.]()

![4 forAlgorithm 1 PCA dimension selection Input: data matrix Y ∈ R n×p .1: Split the data as Y (1) := Y 1:⌈n/2⌉,1:p , Y(2) := Y (⌈n/2⌉+1):n,1:p 2: for ρ ∈ {1, ..., min(n, p)} do3: Let V (1)ρ ∈ R p×ρ denote the matrix of orthogonal eigenvectors associated with the ρ largest eigenvalues of Y (1)⊤ Y(1)]()

![Figure 6: PCA dimension selection. Columns 1-4: different latent space/kernel configurations. 1-3 are finite rank, 4 infinite rank; configurations 3 and 4 are isometric. Row a: sampled positions (n = 500); b: first two principal components (p = 1000); c: the dimension selected by different methods, and the true rank when finite; d: error in geodesic distance and persistence diagram estimation (bottleneck distance) for the isometric configurations; e: persistence diagrams showing partial recovery of true topological features. Further details in main text.]()

![Figure 7: Images example. a) Wasserstein dimension selection; red vertical line indicates minimum at r = 11. b) Kernel density estimate for the magnitudes of the PCA embedding vectors. c) Persistence diagram shows evidence of a single "loop" in the embedding. d) Estimated kernel as a function of latent positions in angular form θ i = arctan(z (2) i /z]()

![Figure 8: Single-cell transcriptomics example. a) histogram of inner products between distinct points in the PCA and random embeddings. b) average percentage increase in shortest path length in the minimum spanning tree compared to the k-nn graph, over different values of k. Results for the random embedding are shown in black, over 10 simulations with error bars indicated 2×standard error, c) comparing the shortest path lengths for samples in 10-nn graph and the MST.]()

![Figure 9: Single-cell transcriptomics example. a) minimum spanning tree computed from the the spherically projected PCA embedding of the planaria data, colours indicate cell types. b) the class graph formed from the minimum spanning tree. All neoblast cell types are represented by a single dark grey node. The class subgraph consisting only of neoblast types is shown in the bottom right-hand corner inset.]()

![Figure 10: Temperatures example. a) Wasserstein dimension selection; red line indicates minimum at r = 36. b) Kernel density estimate of the probability density of PC score magnitudes. c) The blue curve shows proportion of edges in common between embedding k-nn graph and geographic k-nn graph. The black line shows the mean proportion in common between the k-nn graph of a 100 uniformly random embeddings and the geographic k-nn graph. The red band indicates the range between maximum and minimum proportions across these 100 random embeddings.]()

![Figure 11: Temperatures example. Locations of towns and cities are shown in red. The blue lines correspond to edges in the embedding k-nn graph G, with k = 5. The white circles highlight, from west to east: Edinburgh, U.K.; Baia Mare, Romania; and Novorossiysk, Russia.]()

![Figure 12: Temperatures example. Shortest paths in the embedding k-nn graph G from Tallinn, Estonia, to all other towns and cities. Each shortest path is visualized as a spline, with knot points given by the geographic locations of its constituent towns and cities. The red dots highlight the shortest path from Tallinn to Tripoli, Libya.]()

![Figure 13: Mixture model example. Left: maximum errormax i̸ =j p -1/2 ∥ζ i -ζ j ∥ 2 -∥ϕ(Z i ) -ϕ(Z j )∥ ℓ2, averaged over 50 independent realisations from the model, as a function of n and p. Right: the same error for p = 200, 1000, 15000, as a function of n.]()

![Figure 14: Mixture model example. PCA embedding {p -1/2 ζ 1 , . . . , p -1/2 ζ n } (blue dots) and ϕ(1), ϕ(2), ϕ(3) (red dots). Top row: n fixed to 200 and p varying. Bottom row p fixed to 200 and n varying.]()

![2. Section D.1 contains definitions and notation used throughout section D. Various intermediate results used in the proof of theorem 4 are given in sections D.3-D.6.]()

![[YY ⊤ |Z 1 , . . . , Z n ] = E[XX ⊤ |Z 1 , . . . , Z n ] + σE[XE ⊤ |Z 1 , . . . , Z n ] + σE[EX ⊤ |Z 1 , . . . , Z n ] + σ 2 E[EE ⊤ |Z 1 , . . . , Z n ] = pΦΦ ⊤ + pσ 2 I n .Assume A6 and A9. Then there exists a constant c max λ < ∞ depending only on the first supremum in A9, and a constant c min λ > 0 such thatsup p≥1 sup z f (z, z) + λ f 1 ≤ c max λ , inf p≥1 λ f r ≥ c min λ .Proof. The existence of c min λ as required is an immediate consequence of A6. Using A9 and Jensen's inequality gives:sup z f (z, z) = sup z j (z)| 2 ] ≤ sup z j (z)| 4q ] 2/4q < ∞.]()

![TheoremMatrix Bernstein inequality). Let M 1 , . . . , M n be independent random matrices with common dimensionsm 1 × m 2 satisfying E[M i ] = 0 and ∥M i ∥ 2 ≤ L for each 1 ≤ i ≤ n and some constant L. Let M := n i=1 M i and v(M) = max ∥E[MM ⊤ ]∥ 2 , ∥E[M ⊤ M]∥ 2 . Then for all t ≥ 0, P (∥M∥ 2 ≥ t) ≤ (m 1 + m 2 ) exp -t 2 /2 v(M) + Lt/3.]()

![|E ij | 2q .Proof. Let us write the matrix Y in terms of its columnsY ≡ [Y 1 | • • • |Y p ] so that:]()

![holds by lemma 10 and the fact that Ỹj and Y j are equal in distribution.By definition of the Schatten-q norm, Yj Y ⊤ k Y j Y ⊤ j , where λ 1 Y j Y ⊤ jfor k = 2, . . . , n. Thus:(Z i ) + σE ij )]()

![Figure 15: First two coordinates of the data matrices corresponding to figure 6, showing much less structure than the principal components.]()

![2q n q max l=1,...,nE |X k (Z l ) + σE kl | 4q Z ≤ 2 6q-1 n q sup l≥1 sup z∈Z E |X l (z)| 4q + σ 4q sup l≥1, l≥1 E[|E l l| 4q ] .]()

Proof. The convexity holds due to the fact that any norm must be absolutely homogeneous and satisfy the triangle inequality. For the second claim, sinceE [∥A∥ q ⋆ ] ∨ E [∥B∥ q ⋆ ] < ∞ we have the preliminary estimate E [∥A + B∥ q ⋆ ] ≤ 2 q-1 (E [∥A∥ q ⋆ ] + E [∥B∥ q ⋆ ]) < ∞. If E [∥A + B∥ q ⋆ ] = 0 then the desired inequality is trivial. So suppose E [∥A + B∥ q ⋆ ] > 0.Using the triangle inequality for the norm and then Holder's inequality for the expectation,E [∥A + B∥ q ⋆ ] = E ∥A + B∥ ⋆ ∥A + B∥ q-1 ⋆ ≤ E (∥A∥ ⋆ + ∥B∥ ⋆ ) ∥A + B∥ q-1 ⋆ = E ∥A∥ ⋆ ∥A + B∥ q-1 ⋆ + E ∥B∥ ⋆ ∥A + B∥ q-1 ⋆

