- **Offline Reinforcement Learning (RL)**: Learning to act from observational data without active interaction; significant for practical applications (e.g., robotics, healthcare).

- **Extrapolation Error**: Major challenge in offline RL; occurs when the value of under-represented state-action pairs is over-estimated, leading to poor policy performance.

- **Corrective Feedback Loop**: Missing in offline settings; in online RL, value over-estimation is corrected through exploration, while under-estimation is corrected through exploitation.

- **Policy Constraints**: Strategies to mitigate extrapolation error include:
  - Constraints on policy updates to prevent deviations from the dataset.
  - Pessimism bias to counteract value over-estimation.
  - Use of large, diverse datasets to improve state space coverage.

- **Tandem Learning Paradigm**: Introduces an 'active' agent (interacts with the environment) and a 'passive' agent (learns from the active agent's data). Key for analyzing passive learning difficulties.

- **Tandem Effect**: Passive learners fail to learn adequately from data sufficient for active learners; observed across various environments and architectures.

- **Contributing Factors to Tandem Effect**:
  - **Bootstrapping (B)**: Amplifies mis-estimation from poorly estimated values.
  - **Data Distribution (D)**: Insufficient coverage of actions leads to mis-estimation.
  - **Function Approximation (F)**: Non-linear approximators may wrongly extrapolate values for underrepresented actions.

- **Empirical Findings**: 
  - Active and passive Q-networks produce similar values for common actions but diverge significantly for less frequent actions.
  - Over-estimation of state-action values can grow during training, particularly in passive agents.

- **Tandem DQN Implementation**: 
  - Active and passive agents trained on identical sequences; total training budget of 200 iterations, each with 1M steps.
  - Evaluation of both agents after each training iteration for 500K steps.

- **Dataset Characteristics**: 
  - Impact of dataset size, diversity, and stochasticity of the data-generating policy on offline learning success.
  - Small amounts of self-generated data can enhance training data distribution.

- **Hypothesis on Control Learning**: Robust learning may require interactivity not just for data gathering but also to counteract the tendency of function approximators to exploit gaps in fixed data distributions.