<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-27">27 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Livathinos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Auer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maksym</forename><surname>Lysak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Nassar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michele</forename><surname>Dolfi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Panagiotis</forename><surname>Vagenas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cesar</forename><surname>Berrospi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Omenetti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kasper</forename><surname>Dinkla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yusik</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shubham</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Teixeira De Lima</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Valery</forename><surname>Weber</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucas</forename><surname>Morin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ingmar</forename><surname>Meijer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viktor</forename><surname>Kuropiatnyk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">W J</forename><surname>Staar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<settlement>Rüschlikon</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-27">27 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">163CA14D6704C38BCB0BF2BDB00D3CC9</idno>
					<idno type="arXiv">arXiv:2501.17887v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Docling, an easy-to-use, self-contained, MITlicensed, open-source toolkit for document conversion, that can parse several types of popular document formats into a unified, richly structured representation. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. Docling is released as a Python package and can be used as a Python API or as a CLI tool. Docling's modular architecture and efficient document representation make it easy to implement extensions, new features, models, and customizations. Docling has been already integrated in other popular open-source frameworks (e.g., LangChain, LlamaIndex, spaCy), making it a natural fit for the processing of documents and the development of high-end applications. The open-source community has fully engaged in using, promoting, and developing for Docling, which gathered 10k stars on GitHub in less than a month and was reported as the No. 1 trending repository in GitHub worldwide in November 2024.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Converting documents back into a unified machineprocessable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which often discards structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs, Office documents, and scanned document images has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, SaaS offerings on hyperscalers <ref type="bibr" target="#b2">(Auer et al. 2022</ref>) and most recently, multimodal vision-language models. Typically, they incur a cost (e.g., for licensing or LLM inference) and cannot be run easily on local hardware. Meanwhile, only a handful of different open-source tools cover PDF, MS Word, MS PowerPoint, Images, or HTML conversion, leaving a significant feature and quality gap to proprietary solutions.</p><p>With Docling, we recently open-sourced a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition that we developed and presented in the recent past <ref type="bibr" target="#b10">(Livathinos et al. 2021;</ref><ref type="bibr" target="#b15">Pfitzmann et al. 2022;</ref><ref type="bibr" target="#b11">Lysak et al. 2023)</ref>. Docling is designed as a simple, self-contained Python library with permissive MIT license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models. Since its launch in July 2024, Docling has attracted considerable attention in the AI developer community and ranks top on GitHub's monthly trending repositories with more than 10,000 stars at the time of writing. On October 16, 2024, Docling reached a major milestone with version 2, introducing several new features and concepts, which we outline in this updated technical report, along with details on its architecture, conversion speed benchmarks, and comparisons to other open-source assets.</p><p>The following list summarizes the features currently available on Docling:</p><p>• Parses common document formats (PDF, Images, MS Office formats, HTML) and exports to Markdown, JSON, and HTML.</p><p>• Applies advanced AI for document understanding, including detailed page layout, OCR, reading order, figure extraction, and table structure recognition. • Establishes a unified DoclingDocument data model for rich document representation and operations. • Provides fully local execution capabilities making it suitable for sensitive data and air-gapped environments. • Has an ecosystem of plug-and-play integrations with prominent generative AI development frameworks, including LangChain and LlamaIndex. • Can leverage hardware accelerators such as GPUs. 2 State of the Art Document conversion is a well-established field with numerous solutions already available on the market. These solutions can be categorized along several key dimensions, including open vs. closed source, permissive vs. restrictive licensing, Web APIs vs. local code deployment, susceptibility &lt;/&gt; Simple Pipeline Parse Markup Format Build Enrich PDF Pipeline Assemble Document Layout Analysis Parse PDF pages Table Structure OCR Use Build Assemble Document Export JSON, Markdown, HTML, Figures, … {;} ## … Chunking for RAG Docling Document to hallucinations, conversion quality, time-to-solution, and compute resource requirements.</p><p>The most popular conversion tools today leverage visionlanguage models (VLMs), which process page images to text and markup directly. Among proprietary solutions, prominent examples include GPT-4o (OpenAI), Claude (Anthropic), and Gemini (Google). In the open-source domain, LLaVA-based models, such as LLaVA-next, are noteworthy. However, all generative AI-based models face two significant challenges. First, they are prone to hallucinations, i.e., their output may contain false information which is not present in the source document -a critical issue when faithful transcription of document content is required. Second, these models demand substantial computational resources, making the conversion process expensive. Consequently, VLM-based tools are typically offered as SaaS, with compute-intensive operations performed remotely in the cloud.</p><p>A second category of solutions prioritizes on-premises deployment, either as Web APIs or as libraries. Examples include Adobe Acrobat, Grobid, Marker, MinerU, Unstructured, and others. These solutions often rely on multiple specialized models, such as OCR, layout analysis, and table recognition models. Docling falls into this category, leveraging modular, task-specific models which recover document structures and features only. All text content is taken from the programmatic PDF or transcribed through OCR methods. This design ensures faithful conversion, without the risk of generating false content. However, it necessitates maintaining a diverse set of models for different document components, such as formulas or figures.</p><p>Within this category, Docling distinguishes itself through its permissive MIT license, allowing organizations to integrate Docling into their solutions without incurring licensing fees or adopting restrictive licenses (e.g., GPL). Addi-tionally, Docling offers highly accurate, resource-efficient, and fast models, making it well-suited for integration with many standard frameworks.</p><p>In summary, Docling stands out as a cost-effective, accurate and transparent open-source library with a permissive license, offering a reliable and flexible solution for document conversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design and Architecture</head><p>Docling is designed in a modular fashion with extensibility in mind, and it builds on three main concepts: pipelines, parser backends, and the DoclingDocument data model as its centerpiece (see Figure <ref type="figure" target="#fig_0">1</ref>). Pipelines and parser backends share the responsibility of constructing and enriching a DoclingDocument representation from any supported input format. The DoclingDocument data model with its APIs enable inspection, export, and downstream processing for various applications, such as RAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Docling Document</head><p>Docling v2 introduces a unified document representation, DoclingDocument, as a Pydantic data model that can express various common document features, such as:</p><p>• Text, Tables, Pictures, Captions, Lists, and more.</p><p>• Document hierarchy with sections and groups.</p><p>• Disambiguation between main body and headers, footers (furniture).</p><p>• Layout information (i.e., bounding boxes) for all items, if available.</p><p>• Provenance information (i.e., page numbers, document origin).</p><p>With this data model, Docling enables representing document content in a unified manner, i.e., regardless of the source document format.</p><p>Besides specifying the data model, the DoclingDocument class defines APIs encompassing document construction, inspection, and export. Using the respective methods, users can incrementally build a DoclingDocument, traverse its contents in reading order, or export to commonly used formats. Docling supports lossless serialization to (and deserialization from) JSON, and lossy export formats such as Markdown and HTML, which, unlike JSON, cannot retain all available meta information.</p><p>A DoclingDocument can additionally be passed to a chunker class, an abstraction that returns a stream of chunks, each of which captures some part of the document as a string accompanied by respective metadata. To enable both flexibility for downstream applications and out-of-the-box utility, Docling defines a chunker class hierarchy, providing a base type as well as specific subclasses. By using the base chunker type, downstream applications can leverage popular frameworks like LangChain or LlamaIndex, which provide a high degree of flexibility in the chunking approach. Users can therefore plug in any built-in, self-defined, or third-party chunker implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parser Backends</head><p>Document formats can be broadly categorized into two types:</p><p>1. Low-level formats, like PDF files or scanned images.</p><p>These formats primarily encode the visual representation of the document, containing instructions for rendering text cells and lines or defining image pixels. Most semantics of the represented content are typically lost and need to be recovered through specialized AI methods, such as OCR, layout analysis, or table structure recognition.</p><p>2. Markup-based formats, including MS Office, HTML, Markdown, and others. These formats preserve the semantics of the content (e.g., sections, lists, tables, and figures) and are comparatively inexpensive to parse.</p><p>Docling implements several parser backends to read and interpret different formats and it routes their output to a fitting processing pipeline. For PDFs Docling provides backends which: a) retrieve all text content and their geometric properties, b) render the visual representation of each page as it would appear in a PDF viewer. For markup-based formats, the respective backends carry the responsibility of creating a DoclingDocument representation directly. For some formats, such as PowerPoint slides, element locations and page provenance are available, whereas in other formats (for example, MS Word or HTML), this information is unknown unless rendered in a Word viewer or a browser. The DoclingDocument data model handles both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDF Backends</head><p>While several open-source PDF parsing Python libraries are available, in practice we ran into various limitations, among which are restrictive licensing (e.g., pymupdf (pym 2024)), poor speed, or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) (PyPDFium <ref type="bibr" target="#b17">Team 2024;</ref><ref type="bibr" target="#b16">pypdf Maintainers 2024)</ref>.</p><p>We therefore developed a custom-built PDF parser, which is based on the low-level library qpdf ( <ref type="bibr" target="#b3">Berkenbilt 2024)</ref>. Our PDF parser is made available in a separate package named docling-parse and acts as the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium (PyPDFium <ref type="bibr" target="#b17">Team 2024)</ref>.</p><p>Other Backends Markup-based formats like HTML, Markdown, or Microsoft Office (Word, PowerPoint, Excel) as well as plain formats like AsciiDoc can be transformed directly to a DoclingDocument representation with the help of several third-party format parsing libraries.</p><p>For HTML documents we utilize BeautifulSoup <ref type="bibr">(Richardson 2004</ref><ref type="bibr">(Richardson -2024))</ref>, for Markdown we use the Marko library <ref type="bibr">(Ming 2019</ref><ref type="bibr">(Ming -2024))</ref>, and for Office XML-based formats (Word, PowerPoint, Excel) we implement custom extensions on top of the python-docx (Canny and contributors 2013-2024a), python-pptx <ref type="bibr">(Canny and</ref><ref type="bibr">contributors 2013-2024b)</ref>, and openpyxl (Eric Gazoni 2010-2024) libraries, respectively. During parsing, we identify and extract common document elements (e.g., title, headings, paragraphs, tables, lists, figures, and code) and reflect the correct hierarchy level if possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pipelines</head><p>Pipelines in Docling serve as an orchestration layer which iterates through documents, gathers the extracted data from a parser backend, and applies a chain of models to: a) build up the DoclingDocument representation and b) enrich this representation further (e.g., classify images).</p><p>Docling provides two standard pipelines. The Standard-PdfPipeline leverages several state-of-the-art AI models to reconstruct a high-quality DoclingDocument representation from PDF or image input, as described in section 4. The SimplePipeline handles all markup-based formats (Office, HTML, AsciiDoc) and may apply further enrichment models as well.</p><p>Pipelines can be fully customized by sub-classing from an abstract base class or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To create and use a custom model pipeline, you can provide a custom pipeline class as an argument to the main document conversion API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PDF Conversion Pipeline</head><p>The capability to recover detailed structure and content from PDF and image files is one of Docling's defining features. In this section, we outline the underlying methods and models that drive the system.</p><p>Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Any image format input is wrapped in a PDF container on the fly, and proceeds through the pipeline as a scanned PDF document. Then, the standard PDF pipeline applies a sequence of AI models independently on every page of the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which eventually assembles the DoclingDocument representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AI Models</head><p>As part of Docling, we release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object detector for page elements <ref type="bibr" target="#b15">(Pfitzmann et al. 2022</ref>). The second model is TableFormer <ref type="bibr" target="#b13">(Nassar et al. 2022;</ref><ref type="bibr" target="#b11">Lysak et al. 2023)</ref>, a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on Hugging Face) and a separate Python package for the inference code (doclingibm-models).</p><p>Layout Analysis Model Our layout analysis model is an object detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR <ref type="bibr" target="#b24">(Zhao et al. 2023</ref>) and re-trained on DocLayNet <ref type="bibr" target="#b15">(Pfitzmann et al. 2022)</ref>, our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the Hugging Face transformers <ref type="bibr" target="#b22">(Wolf et al. 2020</ref>) library and the Safetensors file format. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures, or tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure</head><p>Recognition The TableFormer model <ref type="bibr" target="#b13">(Nassar et al. 2022)</ref>, first published in 2022 and since refined with a custom structure token language <ref type="bibr" target="#b11">(Lysak et al. 2023)</ref>, is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, Table-Former handles many characteristics of tables like partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy on both column-heading and row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch (Ansel et al. 2024). The PDF pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells during a post-processing step, to avoid expensive re-transcription of the table image-crop, which also makes the TableFormer model language agnostic.</p><p>OCR Docling utilizes OCR to convert scanned PDFs and extract content from bitmaps images embedded in a page. Currently, we provide integration with EasyOCR (eas 2024), a popular third-party OCR library with support for many languages, and Tesseract as a widely available alternative. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (see section 5), making it the biggest compute expense in the pipeline.</p><p>Assembly In the final pipeline stage, Docling assembles all prediction results produced on each page into the Do-clingDocument representation, as defined in the auxiliary Python package docling-core. The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as correcting the reading order or matching figures with captions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance</head><p>In this section, we characterize the conversion speed of PDF documents with Docling in a given resource budget for different scenarios and establish reference numbers.</p><p>Further, we compare the conversion speed to three popular contenders in the open-source space, namely unstructured.io (Unstructured.io <ref type="bibr" target="#b17">Team 2024)</ref>, Marker (Paruchuri 2024), and MinerU <ref type="bibr" target="#b21">(Wang et al. 2024)</ref>. All aforementioned solutions can universally convert PDF documents to Markdown or similar representations and offer a library-style interface to run the document processing entirely locally. We exclude SaaS offerings and remote services for document conversion from this comparison, since the latter do not provide any possibility to control the system resources they run on, rendering any speed comparison invalid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Dataset</head><p>To enable a meaningful benchmark, we composed a test set of 89 PDF files covering a large variety of styles, features, content, and length (see Figure <ref type="figure" target="#fig_1">2</ref>). This dataset is based to a large extend on our DocLayNet <ref type="bibr" target="#b15">(Pfitzmann et al. 2022</ref>) dataset and augmented with additional samples from CCpdf <ref type="bibr" target="#b20">(Turski et al. 2023)</ref> to increase the variety. Overall, it includes 4008 pages, 56246 text items, 1842 tables and 4676 pictures. As such, it is large enough to provide variety without requiring excessively long benchmarking times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System Configurations</head><p>We schedule our benchmark experiments each on two different systems to create reference numbers:</p><p>• AWS EC2 VM (g6.xlarge), 8 virtual cores (AMD EPYC 7R13, x86), 32 GB RAM, Nvidia L4 GPU (24 GB VRAM), on Ubuntu 22.04 with Nvidia CUDA 12.4 drivers • MacBook Pro M3 Max (ARM), 64GB RAM, on macOS 14.7</p><p>All experiments on the AWS EC2 VM are carried out once with GPU acceleration enabled and once purely on the x86 CPU, resulting in three total system configurations which we refer to as M3 Max SoC, L4 GPU, and x86 CPU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Benchmarking Methodology</head><p>We implemented several measures to enable a fair and reproducible benchmark across all tested assets. Specifically, the experimental setup accounts for the following factors:</p><p>• All assets are installed in the latest available versions, in a clean Python environment, and configured to use the state-of-the-art processing options and models, where applicable. We selectively disabled non-essential functionalities achieve a compatible feature-set across all compared libraries.</p><p>• When running experiments on CPU, we inform all assets of the desired CPU thread budget of 8 threads, via the OMP NUM THREADS environment variable and any accepted configuration options. The L4 GPU on our AWS EC2 VM is hidden.</p><p>• When running experiments on the L4 GPU, we enable CUDA acceleration in all accepted configuration options, ensure the GPU is visible and all required runtimes for AI inference are installed with CUDA support.</p><p>Table <ref type="table" target="#tab_3">1</ref> provides an overview of the versions and configuration options we considered for each asset. ordered by number of pages in a document, on all system configurations. Every dot represents one document. Log/log scale is used to even the spacing, since both number of pages and conversion times have long-tail distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime Characteristics</head><p>To analyze Docling's runtime characteristics, we begin by exploring the relationship between document length (in pages) and conversion time. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, this relationship is not strictly linear, as documents differ in their frequency of tables and bitmap elements (i.e., scanned content). This requires OCR or table structure recognition models to engage dynamically when layout analysis has detected such elements.</p><p>By breaking down the runtimes to a page level, we receive a more intuitive measure for the conversion speed (see also Figure <ref type="figure">4</ref>). Processing a page in our benchmark dataset requires between 0.6 sec (5 th percentile) and 16.3 sec (95 th percentile), with a median of 0.79 sec on the x86 CPU. On the M3 Max SoC, it achieves 0.26/0.32/6.48 seconds per page (.05/median/.95), and on the Nvidia L4 GPU it achieves 57/114/2081 milliseconds per page (.05/median/.95). The large range between 5 and 95 percentiles results from the highly different complexity of content across pages (i.e., almost empty pages vs. full-page tables).</p><p>Disabling OCR saves 60% of runtime on the x86 CPU and the M3 Max SoC, and 50% on the L4 GPU. Turning off table structure recognition saves 16% of runtime on the x86 CPU and the M3 Max SoC, and 24% on the L4 GPU. Disabling both OCR and table structure recognition saves around 75% of runtime on all system configurations. Profiling Docling's AI Pipeline We analyzed the contributions of Docling's PDF backend and all AI models in the PDF pipeline to the total conversion time. The results are shown in Figure <ref type="figure">4</ref>. On average, processing a page took 481 ms on the L4 GPU, 3.1 s on the x86 CPU and 1.26 s on the M3 Max SoC.</p><p>It is evident that applying OCR is the most expensive operation. In our benchmark dataset, OCR engages in 578 pages. On average, transcribing a page with EasyOCR took 1.6 s on the L4 GPU, 13 s on the x86 CPU and 5 s on the M3 Max SoC. The layout model spent 44 ms on the L4 GPU, 633 ms on the x86 CPU and 271 ms on the M3 Max SoC on average for each page, making it the cheapest of the AI models, while TableFormer (fast flavour) spent 400 ms on the L4 GPU, 1.74 s on the x86 CPU and 704 ms on the M3 Max SoC on average per table. Regarding the total time spent converting our benchmark dataset, TableFormer had less impact than other AI models, since tables appeared on only 28% of all pages (see Figure <ref type="figure">4</ref>).</p><p>On the L4 GPU, we observe a speedup of 8x (OCR), 14x (Layout model) and 4.3x (Table <ref type="table">structure</ref>) compared to the x86 CPU and a speedup of 3x (OCR), 6x (Layout model) and 1.7x (Table <ref type="table">structure</ref>) compared to the M3 Max CPU of our MacBook Pro. This shows that there is no equal benefit for all AI models from the GPU acceleration and there might be potential for optimization.</p><p>The time spent in parsing a PDF page through our docling-parse backend is substantially lower in comparison to the AI models. On average, parsing a PDF page took 81 ms on the x86 CPU and 44 ms on the M3 Max SoC (there is no GPU support).</p><p>Comparison to Other Tools We compare the average times to convert a page between Docling, Marker, MinerU, and Unstructured on the system configurations outlined in section 5.2. Results are shown in Figure <ref type="figure">5</ref>.</p><p>Without GPU support, Docling leads with 3.1 sec/page (x86 CPU) and 1.27 sec/page (M3 Max SoC), followed closely by MinerU (3.3 sec/page on x86 CPU) and Unstructured (4.2 sec/page on x86 CPU, 2.7 sec/page on M3 Max SoC), while Marker needs over 16 sec/page (x86 CPU) and 4.2 sec/page (M3 Mac SoC). MinerU, despite several efforts to configure its environment, did not finish any run on our MacBook Pro M3 Max. With CUDA acceleration on the Nvidia L4 GPU, the picture changes and MinerU takes the lead over the contenders with 0.21 sec/page, compared to 0.49 sec/page with Docling and 0.86 sec/page with Marker. Unstructured does not profit from GPU acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applications</head><p>Docling's document extraction capabilities make it naturally suitable for workflows like generative AI applications (e.g., RAG), data preparation for foundation model training, and fine-tuning, as well as information extraction.</p><p>As far as RAG is concerned, users can leverage existing Docling extensions for popular frameworks like LlamaIndex and then harness framework capabilities for RAG components like embedding models, vector stores, etc. These Docling extensions typically provide two modes of operation: one using a lossy export, e.g., to Markdown, and one using lossless serialization via JSON. The former provides a simple starting point, upon which any text-based chunking method may be applied (e.g., also drawing from the framework library), while the latter, which uses a swappable Docling chunker type, can be the more powerful one, as it can provide document-native RAG grounding via rich metadata such as the page number and the bounding box of the supporting context. For usage outside of these frameworks, users can still employ Docling chunkers to accelerate and simplify the development of their custom pipelines. Besides strict RAG pipelines for Q&amp;A, Docling can naturally be utilized in the context of broader agentic workflows for which it can provide document-based knowledge for agents to decide and act on.</p><p>Moreover, Docling-enabled pipelines can generate ground truth data out of documents. Such domain-specific knowledge can make significant impact when infused to foundation model training and fine-tuning.</p><p>Last but not least, Docling can be used as a backbone for information extraction tasks. Users who seek to create structured representations out of unstructured documents can leverage Docling, which maps various document formats to the unified DoclingDocument format, as well as its strong table understanding capabilities that can help better analyze semi-structured document parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ecosystem</head><p>Docling is quickly evolving into a mainstream package for document conversion. The support for PDF, MS Office formats, Images, HTML, and more makes it a universal choice for downstream applications. Users appreciate the intuitiveness of the library, the high-quality, richly structured conversion output, as well as the permissive MIT license, and the possibility of running entirely locally on commodity hardware.</p><p>Among the integrations created by the Docling team and the growing community, a few are worth mentioning as depicted in Figure <ref type="figure" target="#fig_4">6</ref>. For popular generative AI application patterns, we provide native integration within LangChain <ref type="bibr" target="#b6">(Chase 2022)</ref> and LlamaIndex <ref type="bibr" target="#b9">(Liu 2022)</ref> for reading documents and chunking. Processing and transforming documents at scale for building large-scale multi-modal training datasets are enabled by the integration in the open IBM data-prep-kit <ref type="bibr" target="#b23">(Wood et al. 2024)</ref>. Agentic workloads can leverage the integration with the Bee framework (IBM <ref type="bibr">Research 2024)</ref>. For the fine-tuning of language models, Docling is integrated in InstructLab <ref type="bibr" target="#b19">(Sudalairaj et al. 2024)</ref>, where it supports the enhancement of the knowledge taxonomy. Docling is also available and officially maintained as a system package in the Red Hat ® Enterprise Linux ® AI (RHEL AI) distribution, which seamlessly allows to develop, test, and run the Granite family of large language models for enterprise applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work and Contributions</head><p>Docling's modular architecture allows an easy extension of the model library and pipelines. In the future, we plan to extend Docling with several additional models, such as a figure-classifier model, an equation-recognition model and a code-recognition model. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Furthermore, we will focus on building an opensource quality evaluation framework for the tasks performed by Docling, such as layout analysis, table structure recognition, reading order detection, text transcription, etc. This will allow transparent quality comparisons based on publicly available benchmarks such as DP-Bench (Zhong 2020), Om-nidDocBench <ref type="bibr" target="#b14">(Ouyang et al. 2024)</ref> and others. Results will be published in a future update of this technical report. The codebase of Docling is open for use under the MIT license agreement and its roadmap is outlined in the discussions section<ref type="foot" target="#foot_0">foot_0</ref> of our GitHub repository. We encourage everyone to propose improvements and make contributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sketch of Docling's pipelines and usage model. Both PDF pipeline and simple pipeline build up a DoclingDocument representation, which can be further enriched. Downstream applications can utilize Docling's API to inspect, export, or chunk the document for various purposes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dataset categories and sample counts for documents and pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Distribution of conversion times for all documents, ordered by number of pages in a document, on all system configurations. Every dot represents one document. Log/log scale is used to even the spacing, since both number of pages and conversion times have long-tail distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Contributions of PDF backend and AI models to the conversion time of a page (in seconds per page). Lower is better.Left: Ranges of time contributions for each model to pages it was applied on (i.e., OCR was applied only on pages with bitmaps, table structure was applied only on pages with tables). Right: Average time contribution to a page in the benchmark dataset (factoring in zero-time contribution for OCR and table structure models on pages without bitmaps or tables) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ecosystem of Docling integrations contributed by the Docling team or the broader community. Docling is already used for RAG, model fine-tuning, large-scale datasets creation, information extraction and agentic workflows.</figDesc><graphic coords="7,414.44,359.83,51.57,51.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Versions and configuration options considered for each tested asset. * denotes the default setting.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Asset Version OCR</cell><cell>Layout</cell><cell>Tables</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Docling 2.5.2</cell><cell>EasyOCR * default</cell><cell>TableFormer (fast) *</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Marker 0.3.10</cell><cell>Surya *</cell><cell>default</cell><cell>default</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MinerU 0.9.3</cell><cell>auto *</cell><cell>doclayout yolo rapid table *</cell></row><row><cell></cell><cell></cell><cell cols="3">Unstructured 0.16.5</cell><cell>hi res with table structure</cell></row><row><cell cols="5">Documents per Category</cell></row><row><cell cols="2">Science Spec sheets 4</cell><cell>9</cell><cell>5</cell><cell>Law and Regulations Manuals Patents 4 8</cell></row><row><cell>Annual Reports</cell><cell>20</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>39</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CCPdf (misc)</cell></row><row><cell></cell><cell cols="4">Pages per Category</cell></row><row><cell cols="3">Patents Science Spec sheets 24 151 132</cell><cell cols="2">989</cell><cell>68 Manuals</cell><cell>Law and Regulations</cell></row><row><cell>Annual Reports</cell><cell>1554</cell><cell></cell><cell></cell><cell>1090</cell><cell>CCPdf (misc)</cell></row><row><cell cols="5">Categories Annual Reports CCPdf (misc) Law and Regulations Manuals</cell><cell>Patents Science Spec sheets</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/DS4SD/docling/discussions/categories/ roadmap</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://github.com/pymupdf/PyMuPDF" />
		<title level="m">EasyOCR: Ready-to-use OCR with 80+ supported languages</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Ramis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Staar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 15th International Conference on Cloud Computing (CLOUD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="363" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">QPDF: A Content-Preserving PDF Document Transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berkenbilt</surname></persName>
		</author>
		<ptr target="https://github.com/qpdf/qpdf" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">2013-2024a. python-docx: Create and update Microsoft Word .docx files with Python</title>
		<author>
			<persName><forename type="first">S</forename><surname>Canny</surname></persName>
		</author>
		<ptr target="https://python-docx.readthedocs.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Canny</surname></persName>
		</author>
		<ptr target="https://python-pptx.readthedocs.io/" />
		<title level="m">-2024b. python-pptx: Python library for creating and updating PowerPoint</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LangChain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chase</surname></persName>
		</author>
		<ptr target="https://github.com/langchain-ai/langchain" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Gazoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
		<ptr target="https://openpyxl.readthedocs.io/" />
		<title level="m">Python library to read/write Excel 2010 xlsx/xlsm files</title>
		<imprint>
			<date type="published" when="2010">2010-2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://github.com/i-am-bee/bee-agent-framework" />
		<title level="m">Bee Agent Framework</title>
		<imprint>
			<publisher>IBM Research</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://github.com/jerryjliu/llamaindex" />
		<title level="m">LlamaIndex</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust PDF Document Conversion using Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Livathinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berrospi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lysak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuropiatnyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dinkla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Staar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15137" to="15145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimized Table Tokenization for Table Structure Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lysak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Livathinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Staar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition -ICDAR 2023: 17th International Conference</title>
		<meeting><address><addrLine>San José, CA, USA; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2023-08-21">2023. August 21-26, 2023</date>
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Ming</surname></persName>
		</author>
		<ptr target="https://github.com/frostming/marko" />
		<title level="m">A markdown parser with high extensibility</title>
		<imprint>
			<date type="published" when="2019">2019-2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tableformer: Table structure understanding with transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Livathinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lysak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Staar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4614" to="4623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.07626</idno>
		<ptr target="https://github.com/VikParuchuri/marker" />
		<title level="m">OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>Paruchuri, V. 2024. Marker: Convert PDF to Markdown Quickly with High Accuracy</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">DocLayNet: a large human-annotated dataset for document-layout segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pfitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Staar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3743" to="3751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Maintainers</forename><surname>Pypdf</surname></persName>
		</author>
		<ptr target="https://github.com/py-pdf/pypdf" />
		<title level="m">pypdf: A Pure-Python PDF Library</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Pypdfium</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/pypdfium2-team/pypdfium2" />
		<title level="m">PyPDFium2: Python bindings for PDFium</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Richardson</surname></persName>
		</author>
		<ptr target="https://www.crummy.com/software/BeautifulSoup/" />
		<title level="m">Beautiful Soup: A Python library for parsing HTML and XML</title>
		<imprint>
			<date type="published" when="2004">2004-2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sudalairaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhandwaldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01081</idno>
		<title level="m">LAB: Large-Scale Alignment for ChatBots</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stanisławek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kaczmarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dyda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Graliński</surname></persName>
		</author>
		<ptr target="https://unstructured.io" />
	</analytic>
	<monogr>
		<title level="m">Unstructured.io: Open-Source Pre-Processing Tools for Unstructured Data</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Kise</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2024" to="2035" />
		</imprint>
	</monogr>
	<note>Document Analysis and Recognition -ICDAR 2023</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.18839</idno>
		<title level="m">MinerU: An Open-Source Solution for Precise Document Content Extraction</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">HuggingFace&apos;s Transformers: Stateof-the-art Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lublinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roytman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Emami-Gohari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koyfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Surendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsuzuku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zerfos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daijavad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.18164</idno>
		<title level="m">Data-Prep-Kit: getting your data ready for LLM application development</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08069</idno>
		<title level="m">DETRs Beat YOLOs on Real-time Object Detection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Image-based table recognition: data, model, and evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10683</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
