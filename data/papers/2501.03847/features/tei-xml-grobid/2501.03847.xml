<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zekai</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qifeng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">RUI YAN</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">PENG LI</orgName>
								<orgName type="institution" key="instit1">JIAHAO LU</orgName>
								<orgName type="institution" key="instit2">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">ZHIYANG DOU</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">CHENYANG SI</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">ZHEN DONG</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory">CHENG LIN</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution" key="instit1">ZIWEI LIU</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution" key="instit1">WENPING WANG</orgName>
								<orgName type="institution" key="instit2">Texas A&amp;M University</orgName>
								<address>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E375478A49BE8A89A9F4E87A89A3D57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process-such as camera manipulation or content editing-remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation. Codes and more results are available at <ref type="url" target="https://igl-hkust.github.io/das/">https://igl-hkust.github.io/das/</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The development of diffusion generative models <ref type="bibr" target="#b1">[Blattmann et al. 2023;</ref><ref type="bibr" target="#b3">Brooks et al. 2024;</ref><ref type="bibr" target="#b22">Ho et al. 2020;</ref><ref type="bibr" target="#b33">Lin et al. 2024;</ref><ref type="bibr" target="#b53">Rombach et al. 2022;</ref><ref type="bibr">Zheng et al. 2024b</ref>] enables high-quality video generation from text prompts or a starting image. Recent emerging models, e.g. Sora <ref type="bibr" target="#b3">[Brooks et al. 2024]</ref>, CogVideo-X <ref type="bibr">[Yang et al. 2024b</ref>], Keling <ref type="bibr" target="#b31">[Kuaishou 2024</ref>], and Hunyuan <ref type="bibr" target="#b29">[Kong et al. 2024]</ref>, have shown impressive video generation ability with strong temporal consistency and appealing visual effects, which becomes a promising tool for artists to create stunning videos using just few images or text arXiv:2501.03847v2 [cs.CV] 9 Jan 2025 prompts. These advancements show strong potential to revolutionize the advertising, film, robotics, and game industries, becoming fundamental elements for various generative AI-based applications.</p><p>A major challenge in video generation lies in achieving versatile and precise control to align seamlessly with users' creative visions. While recent methods have introduced strategies to integrate control into the video generation process <ref type="bibr" target="#b16">[Guo et al. 2024;</ref><ref type="bibr">He et al. 2024b,a;</ref><ref type="bibr" target="#b24">Huang et al. 2023;</ref><ref type="bibr">Ma et al. 2024b,a;</ref><ref type="bibr" target="#b40">Namekata et al. 2024;</ref><ref type="bibr" target="#b46">Polyak et al. 2024;</ref><ref type="bibr">Wang et al. 2024f,c;</ref><ref type="bibr" target="#b75">Yuan et al. 2024]</ref>, they predominantly focus on specific control types, relying on specialized architectures that lack adaptability to emerging control requirements. Furthermore, these approaches are generally limited to high-level adjustments-such as camera movements or maintaining identity-falling short when it comes to enabling fine-grained modifications, like precisely raising an avatar's left hand.</p><p>We argue that achieving versatile and precise video generation control fundamentally requires 3D control signals in the diffusion model. Videos are 2D renderings of dynamic 3D content. In a traditional Computer Graphics (CG)-based video-making pipeline, we can effectively control all aspects of a video in detail by manipulating the underlying 3D representations, such as meshes or particles. However, existing video control methods solely apply 2D control signals on rendered pixels, lacking the 3D awareness in the video generation process and thus struggling to achieve versatile and finegrained controls. Thus, to this end, we present a novel 3D-aware video diffusion method, called Diffusion as Shader (DaS) in this paper, which utilizes 3D control signals to enable diverse and precise control tasks within a unified architecture.</p><p>Specifically, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (a), DaS is an image-to-video diffusion model that takes a 3D tracking video as the 3D control signals for various control tasks. The 3D tracking video contains the motion trajectories of 3D points whose colors are defined by their coordinates in the camera coordinate system of the first frame. In this way, the 3D tracking video represents the underlying 3D motion of this video. The video diffusion model acts like a shader to compute shaded appearances on the dynamic 3D points to generate the video. Thus, we call our model Diffusion as Shader.</p><p>Using 3D tracking videos as control signals offers a significant advantage over depth videos with enhanced temporal consistency. While a straightforward approach to incorporating 3D control into video diffusion models involves using depth maps as control signals, depth maps only define the structural properties of the underlying 3D content without explicitly linking frames across time. In contrast, 3D tracking videos provide a consistent association between frames, as identical 3D points maintain the same colors across the video. These color anchors ensure consistent appearances for the same 3D points, thereby significantly improving temporal coherence in the generated videos. Our experiments demonstrate that even when a 3D region temporarily disappears and later reappears, DaS effectively preserves the appearance consistency of that region, thanks to the temporal consistency enabled by the tracking video.</p><p>By leveraging 3D tracking videos, DaS enables versatile video generation controls, encompassing but not limited to the following video control tasks.</p><p>(1) Animating meshes to videos. Using advanced 3D tools like Blender, we can design animated 3D meshes based on predefined templates. These animated meshes are transformed into 3D tracking videos to guide high-quality video generation (Figure <ref type="figure" target="#fig_0">1</ref>  (3) Camera control. To enable precise camera control, depth maps are estimated to extract 3D points <ref type="bibr" target="#b2">[Bochkovskii et al. 2024</ref>]. These 3D points are then projected onto a specified camera path to create a 3D tracking video, which guides the generation of videos with customized camera movements (Figure <ref type="figure" target="#fig_0">1 (d)</ref>). (4) Object manipulation. By integrating object segmentation techniques <ref type="bibr" target="#b28">[Kirillov et al. 2023</ref>] with a monocular depth estimator <ref type="bibr" target="#b2">[Bochkovskii et al. 2024]</ref>, the 3D points of specific objects can be extracted and manipulated. These modified 3D points are used to construct a 3D tracking video, which guides the creation of videos for object manipulation (Figure <ref type="figure" target="#fig_0">1 (e)</ref>).</p><p>Due to the 3D awareness of DaS, DaS is data-efficient. Finetuning with less than 10k videos on 8 H800 GPUs for 3 days already gives the powerful control ability to DaS, which is demonstrated by various control tasks. We compare DaS with baseline methods on camera control <ref type="bibr">[He et al. 2024b;</ref><ref type="bibr">Wang et al. 2024c</ref>] and motion transfer <ref type="bibr">[Geyer et al. 2023a</ref>], which demonstrates that DaS achieves significantly improved performances in these two controlling tasks than baselines. For the remaining two tasks, i.e. mesh-to-video and object manipulation, we provide extensive qualitative results to show the superior generation quality of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Video diffusion</head><p>In recent years, the success of diffusion models in image generation <ref type="bibr" target="#b22">[Ho et al. 2020;</ref><ref type="bibr">Peebles and Xie 2023a;</ref><ref type="bibr" target="#b53">Rombach et al. 2022]</ref> has sparked interest in exploring video generation <ref type="bibr" target="#b1">[Blattmann et al. 2023;</ref><ref type="bibr" target="#b3">Brooks et al. 2024;</ref><ref type="bibr">Chen et al. 2023b</ref><ref type="bibr">Chen et al. , 2024b;;</ref><ref type="bibr" target="#b17">Guo et al. 2023;</ref><ref type="bibr" target="#b20">He et al. 2022;</ref><ref type="bibr" target="#b23">Ho et al. 2022;</ref><ref type="bibr" target="#b29">Kong et al. 2024;</ref><ref type="bibr" target="#b31">Kuaishou 2024;</ref><ref type="bibr" target="#b33">Lin et al. 2024;</ref><ref type="bibr" target="#b69">Xing et al. 2024;</ref><ref type="bibr">Yang et al. 2024b;</ref><ref type="bibr">Zheng et al. 2024b]</ref>. VDM <ref type="bibr" target="#b23">[Ho et al. 2022]</ref> is the first work to explore the feasibility of diffusion in the field of video generation. SVD <ref type="bibr" target="#b1">[Blattmann et al. 2023</ref>] introduces a unified strategy for training a robust video generation model. Sora <ref type="bibr" target="#b3">[Brooks et al. 2024]</ref>, through training on extensive video data, suggests that scaling video generation models is a promising path towards building general-purpose simulators of the physical world. CogVideo-X <ref type="bibr">[Yang et al. 2024b]</ref>, VideoCrafter <ref type="bibr">[Chen et al. 2023b</ref><ref type="bibr">[Chen et al. , 2024b]]</ref>, DynamiCrafter <ref type="bibr" target="#b69">[Xing et al. 2024]</ref>, Keling <ref type="bibr" target="#b31">[Kuaishou 2024</ref>], and Hunyuan <ref type="bibr" target="#b29">[Kong et al. 2024</ref>] have demonstrated impressive video generation performance with strong temporal consistency.</p><p>Controllable video generation. Existing works still lack an effective way to control the generation process. There are many works <ref type="bibr" target="#b16">[Guo et al. 2024;</ref><ref type="bibr">He et al. 2024b,a;</ref><ref type="bibr" target="#b24">Huang et al. 2023;</ref><ref type="bibr">Ma et al. 2024b,a,a;</ref><ref type="bibr" target="#b40">Namekata et al. 2024;</ref><ref type="bibr" target="#b46">Polyak et al. 2024;</ref><ref type="bibr" target="#b49">Qiu et al. 2024;</ref><ref type="bibr">Wang et al. 2024f,c;</ref><ref type="bibr" target="#b74">Yu et al. 2024;</ref><ref type="bibr" target="#b75">Yuan et al. 2024</ref>] that introduce a specific control signal in the video generation process which can only achieve one control type like identity preserving, camera control, and motion transfer. Our method is more versatile in various video control types by using a 3D-aware video generation with 3D tracking videos as conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Controlled video generation</head><p>We review the following 4 types of controlled video generation.</p><p>Animating meshes to videos. Animating meshes to videos aims to texture meshes. Several works <ref type="bibr" target="#b4">[Cai et al. 2024;</ref><ref type="bibr" target="#b5">Cao et al. 2023;</ref><ref type="bibr" target="#b52">Richardson et al. 2023;</ref><ref type="bibr" target="#b61">Wang et al. 2023</ref>] have demonstrated the feasibility of mesh texturization using powerful diffusion models. TexFusion <ref type="bibr" target="#b5">[Cao et al. 2023</ref>] applies the diffusion model's denoiser on a set of 2D renders of the 3D object, optimizing an intermediate neural color field to output final RGB textures. TEXTure <ref type="bibr" target="#b52">[Richardson et al. 2023</ref>] introduces a dynamic trimap representation and a novel diffusion sampling process, leveraging this trimap to generate seamless textures from various views. G-Rendering <ref type="bibr" target="#b4">[Cai et al. 2024</ref>] takes a dynamic mesh as input. To preserve consistency, G-Rendering employs UV-guided noise initialization and correspondence-aware blending of both pre-and post-attention features. Following G-Rendering, our method also targets dynamic meshes, utilizing a diffusion model as a shader to incorporate realistic texture information. Unlike G-Rendering, which preserves consistency at the noise and attention levels, our approach leverages 3D tracking videos as supplementary information, integrating them into the diffusion model to ensure both temporal and spatial consistency.</p><p>Camera control. Camera control <ref type="bibr" target="#b0">[Bahmani et al. 2024;</ref><ref type="bibr" target="#b13">Geng et al. 2024;</ref><ref type="bibr">He et al. 2024b;</ref><ref type="bibr">Wang et al. 2024e,c;</ref><ref type="bibr">Xiao et al. 2024a;</ref><ref type="bibr">Yang et al. 2024a;</ref><ref type="bibr" target="#b74">Yu et al. 2024;</ref><ref type="bibr">Zheng et al. 2024a</ref>] is an important capability for enhancing the realism of generated videos and increasing user engagement by allowing customized viewpoints. Recently, many efforts have been made to introduce camera control in video generation. MotionCtrl <ref type="bibr">[Wang et al. 2024c</ref>] incorporates a flexible motion controller for video generation, which can independently or jointly control camera motion and object motion in generated videos. CameraCtrl <ref type="bibr">[He et al. 2024b</ref>] adopts Pl√ºcker embeddings <ref type="bibr" target="#b55">[Sitzmann et al. 2021]</ref> as the primary form of camera parameters, enabling the ViewCrafter <ref type="bibr" target="#b74">[Yu et al. 2024</ref>] employs a point-based representation for free-view rendering, enabling precise camera control. AC3D <ref type="bibr" target="#b0">[Bahmani et al. 2024</ref>] optimizes pose conditioning schedules during training and testing to accelerate convergence and restricts the injection of camera conditioning to specific positions, reducing interference with other meaningful video features. CPA <ref type="bibr">[Wang et al. 2024e</ref>] incorporates a Sparse Motion Encoding Module to embed the camera pose information and integrating the embedded motion information via temporal attention. Our method aims to use 3D tracking videos as an intermediary to achieve precise and consistent camera control.</p><p>Motion transfer. Motion transfer <ref type="bibr">[Esser et al. 2023;</ref><ref type="bibr" target="#b13">Geng et al. 2024;</ref><ref type="bibr">Geyer et al. 2023a;</ref><ref type="bibr" target="#b38">Meral et al. 2024;</ref><ref type="bibr" target="#b43">Park et al. 2024;</ref><ref type="bibr" target="#b47">Pondaven et al. 2024;</ref><ref type="bibr">Wang et al. 2024d,c;</ref><ref type="bibr" target="#b72">Yatim et al. 2024]</ref> aims to synthesize novel videos by following the motion of the original one. Gen-1 <ref type="bibr">[Esser et al. 2023</ref>] employs depth estimation results <ref type="bibr" target="#b2">[Bochkovskii et al. 2024;</ref><ref type="bibr" target="#b35">Lu et al. 2024;</ref><ref type="bibr" target="#b51">Ranftl et al. 2020]</ref> to guide the motion. TokenFlow <ref type="bibr">[Geyer et al. 2023a</ref>] achieves consistent motion transfer by enforcing consistency in the diffusion feature space. MotionCtrl <ref type="bibr">[Wang et al. 2024c</ref>] also achieves motion transfer by incorporating a motion controller. DiTFlow <ref type="bibr" target="#b47">[Pondaven et al. 2024]</ref> proposes Attention Motion Flow as guidance for motion transfer on DiTs [Peebles and Xie 2023a]. Motion Prompting <ref type="bibr" target="#b13">[Geng et al. 2024</ref>] utilizes 2D motions as prompts to realize impressive motion transfer. Unlike these approaches, our method employs 3D tracking as guidance for motion transfer, enabling a more comprehensive capture of each object's motion and the relationships between them within the video. This ensures accurate and globally consistent geometric and temporal consistency.</p><p>Object manipulation. Object manipulation refers to versatile object movement control for image-to-video generation. Different from camera control, which focuses on changes in perspective, object manipulation emphasizes the movement of the objects themselves. Currently, mainstream methods <ref type="bibr">[Chen et al. 2023a;</ref><ref type="bibr" target="#b13">Geng et al. 2024;</ref><ref type="bibr" target="#b25">Jain et al. 2024;</ref><ref type="bibr" target="#b32">Li et al. 2024;</ref><ref type="bibr">Ma et al. 2024b;</ref><ref type="bibr" target="#b39">Mou et al. 2024;</ref><ref type="bibr" target="#b49">Qiu et al. 2024;</ref><ref type="bibr" target="#b57">Teng et al. 2023;</ref><ref type="bibr">Wang et al. 2024f,c;</ref><ref type="bibr">Yang et al. 2024a;</ref><ref type="bibr" target="#b73">Yin et al. 2023</ref>] typically achieve object manipulation by utilizing directed trajectories or modeling the relationships between bounding boxes with specific semantic meanings. However, these methods primarily rely on 2D guidance to represent the spatial movement of target objects, which often fails to accurately capture user intent and frequently results in distorted outputs. ObjCtrl-2.5D <ref type="bibr" target="#b65">[Wang et al. 2024a</ref>] tries to address this limitation by extending 2D trajectories with depth information, creating a single 3D trajectory as the control signal. Better than the single 3D trajectory, our method leverages 3D tracking videos, which offer greater details and more effectively represent the motion relationships between foreground and background for more precise and realistic object manipulation.</p><p>Concurrent works. Recently, several works <ref type="bibr">[Feng et al. 2024a;</ref><ref type="bibr" target="#b13">Geng et al. 2024;</ref><ref type="bibr" target="#b26">Jeong et al. 2024;</ref><ref type="bibr" target="#b30">Koroglu et al. 2024;</ref><ref type="bibr">Lei et al. 2024;</ref><ref type="bibr" target="#b42">Niu et al. 2024;</ref><ref type="bibr" target="#b54">Shi et al. 2024;</ref><ref type="bibr" target="#b77">Zhang et al. 2024</ref>] have explored utilizing motion as control signals. These approaches can be broadly categorized into two groups: 2D motion-based and 3D motion-based methods. <ref type="bibr" target="#b30">[Koroglu et al. 2024;</ref><ref type="bibr">Lei et al. 2024;</ref><ref type="bibr" target="#b54">Shi et al. 2024</ref>] leverage 2D optical flow to condition motion, while <ref type="bibr" target="#b13">[Geng et al. 2024;</ref><ref type="bibr" target="#b26">Jeong et al. 2024;</ref><ref type="bibr" target="#b42">Niu et al. 2024</ref>] utilize 2D tracks, which are sparser than optical flow, to track or control video motion. <ref type="bibr" target="#b77">[Zhang et al. 2024</ref>] learns to generate 3D coordinates in the video diffusion model, which 3D awareness. <ref type="bibr">[Feng et al. 2024a</ref>] lifts videos into 3D space and extracts the motion of 3D points, enabling a more accurate capture of spatial relationships between objects and supporting tasks such as object manipulation and camera control. Our method, DaS, also leverages recent tracking methods <ref type="bibr">[Xiao et al. 2024b;</ref><ref type="bibr" target="#b79">Zhang et al. 2025</ref>] to construct videos. However, we extend the applicability by unifying a broader range of control tasks, including mesh-to-video generation and motion transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD 3.1 Overview</head><p>DaS is an image-to-video (I2V) diffusion generative model, which applies both an input image and a 3D tracking video as conditions for controllable video generation. In the following, we first review the backend I2V video diffusion model in Sec. 3.2. Then, we discuss the definition of the 3D tracking video and how to inject the 3D tracking video into the generation process as a condition in Sec. 3.3. Finally, in Sec. 3.4, we discuss how to apply DaS in various types of video generation control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Backend video diffusion model</head><p>DaS is finetuned from the CogVideoX <ref type="bibr">[Yang et al. 2024b</ref>] model that is a transformer-based video diffusion model <ref type="bibr">[Peebles and Xie 2023a]</ref> operating on a latent space. Specifically, as shown in Figure <ref type="figure" target="#fig_2">2 (d)</ref>, we adopt the I2V CogVideoX model as the base model, which takes an image I ‚àà R ùêª √óùëä √ó3 as input and generate a video V ‚àà R ùëá √óùêª √óùëä √ó3 . The generated video V has ùëá frames with the same image size of width ùëä height ùêª as the input image. The input image I is first padded with zeros to get an input condition video with the same size ùëá √ó ùêª √ó ùëä √ó 3 as the target video. Then, a VAE encoder is applied to the padded condition video to get a latent vector of size ùëá 4 √ó ùêª 8 √ó ùëä 8 √ó 16, which is concatenated with a noise of the same size. A diffusion transformer (DiT) <ref type="bibr">[Peebles and Xie 2023b]</ref> is iteratively used to denoise the noise latent for a predefined number of steps and the output denoised latent is processed by a VAE decoder to get the video V. In the following, we discuss how to add a 3D tracking video as an additional condition on this base model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Finetuning with 3D tracking videos</head><p>We add a 3D tracking video as an additional condition to our video diffusion model. As shown in Figure <ref type="figure" target="#fig_2">2 (a,</ref> <ref type="figure">b</ref>), the 3D tracking video is rendered from a set of moving 3D points {p ùëñ (ùë°) ‚àà R 3 }, where ùë° = 1, ...,ùëá means the frame index in the video. The colors of these points are determined by their coordinates in the first frame, where we normalize the coordinates into [0, 1] 3 and convert the coordinates into RGB colors {c ùëñ }. Note we adopt the reciprocal of z-coordinate in the normalization. These colors remain the same for different timesteps ùë°. Then, to get a specific ùë°-th frame of the tracking video, we project these 3D points onto the ùë°-th camera to render this frame. In Sec. 3.4, we will discuss how to get these moving 3D points and the camera poses of different frames for different control tasks. Next, we first introduce the architecture to utilize the 3D tracking video as a condition for video generation.</p><p>Injecting 3D tracking control. We follow a similar design as the ControlNet <ref type="bibr" target="#b65">[Chen et al. 2024a;</ref><ref type="bibr" target="#b76">Zhang et al. 2023]</ref> in DaS to add the 3D tracking video as the additional condition. As shown in Figure <ref type="figure" target="#fig_2">2</ref> (d), we apply the pretrained VAE encoder to encode the 3D tracking video to get the latent vector. Then, we make a trainable copy of the pretrained denoising DiT, called condition DiT, to process the latent vector of the 3D tracking video. The denoising DiT contains 42 blocks and we copy the first 18 blocks as the condition DiT. In the condition DiT, we extract the output feature of each DiT block, process it with a zero-initialized linear layer, and add the feature to the corresponding feature map of the denoising DiT. We finetune the condition DiT with the diffusion losses while freezing the pretrained denoising DiT.</p><p>Finetuning details. To train the DaS model, we construct a training dataset containing both real-world videos and synthetic rendered videos. The real-world videos are from MiraData <ref type="bibr" target="#b27">[Ju et al. 2024</ref>] while we use the meshes and motion sequences from Mixamo to render synthetic videos. All videos are center-cropped and resized to 720 √ó 480 resolution with 49 frames. We only finetune the copied condition DiT while freezing all the original denoising DiT. To construct the 3D tracking video for the rendered videos, since we have access to the ground-truth 3D meshes and camera poses for the synthetic videos, we construct our 3D tracking videos directly using these dense ground-truth 3D points, which results in dense 3D point tracking. For real-world videos, we adopt SpatialTracker <ref type="bibr">[Xiao et al. 2024b</ref>] to detect 3D points and their trajectories in the 3D space. Specifically, for each real-world video, we detect 4,900 3D evenly distributed points and track their trajectories. For training, we employ a learning rate of 1 √ó 10 -4 using the AdamW optimizer. We train the model for 2000 steps using the gradient accumulation strategy to get an effective batch size of 64. The training takes 3 days on 8 H800 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Video generation control</head><p>In this section, we describe how to utilize DaS for the following controllable video generation.</p><p>3.4.1 Object manipulation. DaS can generate a video to manipulate a specific object. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (a), given an image, we estimate the depth map using Depth Pro <ref type="bibr" target="#b2">[Bochkovskii et al. 2024]</ref> or MoGE <ref type="bibr">[Wang et al. 2024b</ref>] and segment out the object using SAM <ref type="bibr" target="#b28">[Kirillov et al. 2023]</ref>. Then, we are able to manipulate the point cloud of the object to construct a 3D tracking video for object manipulation video generation.</p><p>3.4.2 Animating meshes to videos. DaS enables the creation of visually appealing, high-quality videos from simple animated meshes. While many Computer Graphics (CG) software tools provide basic 3D models and motion templates to generate animated meshes, these outputs are often simplistic and lack the detailed appearance and geometry needed for high-quality animations. Starting with these simple animated meshes, as shown in Figure <ref type="figure" target="#fig_3">3</ref> (b), we generate an initial visually appealing frame using a depth-to-image FLUX model [Labs 2024]. We then produce 3D tracking videos from the animated meshes, which, when combined with the generated first frame, guide DaS to transform the basic meshes into visually rich and appealing videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Camera control.</head><p>Previous approaches <ref type="bibr">[He et al. 2024b;</ref><ref type="bibr">Wang et al. 2024c</ref>] rely on camera or ray embeddings as conditions to control the camera trajectory in video generation. However, these embeddings lack true 3D awareness, leaving the diffusion models to infer the scene's 3D structure and simulate camera movement.</p><p>In contrast, DaS significantly enhances 3D awareness by incorporating 3D tracking videos for precise camera control. To generate videos with a specific camera trajectory, as shown in Figure <ref type="figure" target="#fig_3">3</ref> (c), we first estimate the depth map of the initial frame using Depth Pro <ref type="bibr" target="#b2">[Bochkovskii et al. 2024</ref>] and convert it into colored 3D points. These points are then projected onto the given camera trajectory, constructing a 3D tracking video that enables DaS to control camera movements with high 3D accuracy. Then, using SpatialTracker <ref type="bibr">[Xiao et al. 2024b</ref>], we generate a 3D tracking video from the source video to serve as control signals.</p><p>Finally, the DaS model generates the target video by combining the edited first frame with the 3D tracking video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct experiments on five tasks, including camera control, motion transfer, mesh-to-video generation, and object manipulation to demonstrate the versatility of DaS in controlling the video generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Camera control</head><p>Baseline methods. To evaluate the ability to control camera motions of generated videos, we select two representative methodologies, MotionCtrl <ref type="bibr">[Wang et al. 2024c]</ref> and CameraCtrl <ref type="bibr">[He et al. 2024b]</ref> as baseline methods, both of which allow camera trajectories as input and use camera or ray embeddings for camera control. Metrics. To measure the accuracy of the camera trajectories of generated videos, we evaluate the consistency between the estimated camera poses from the generated videos and the input ground-truth camera poses using rotation errors and translation errors. Specifically, for each frame of a generated video, we reconstruct its relative pose given the first frame using <ref type="bibr">SIFT [Ng and Henikoff 2003]</ref>. Then, we get the normalized quaternion and translation vectors for the rotation and translation. Finally, we calculate the cosine similarity between the estimated camera poses with the given camera poses.</p><formula xml:id="formula_0">RotErr = arccos 1 ùëá -1 ùëá ‚àëÔ∏Å ùëñ=2 ‚ü® q ùëñ gen , q ùëñ gt ‚ü© , TransErr = arccos 1 ùëá -1 ùëá ‚àëÔ∏Å ùëñ=2 ‚ü® t ùëñ gen , t ùëñ gt ‚ü© ,</formula><p>where ùëá is the number of frames, q ùëñ and t ùëñ are the normalized quaternion and translation vector of the ùëñ-th frame, and ‚ü®‚Ä¢, ‚Ä¢‚ü© means the dot product between two vectors.</p><p>Results. We compare against baseline methods on 100 random trajectories from RealEstate10K <ref type="bibr" target="#b81">[Zhou et al. 2018]</ref>. But since most of the random trajectories only contain small movements, we further test the models on larger fixed movements (moving left, right, up, down, spiral) as shown in Figure <ref type="figure" target="#fig_5">4</ref>. As shown in Table <ref type="table" target="#tab_0">1</ref>, our method outperforms the baseline methods, which demonstrates that our method achieves stable and accurate control of the camera poses of the generated videos. The main reason is that due to the utilization of the 3D tracking videos, our method is fully 3D-aware to enable accurate spatial inference in the video generation process. In comparison, baseline methods <ref type="bibr">[He et al. 2024b;</ref><ref type="bibr">Wang et al. 2024c]</ref>   <ref type="bibr">et al. 2024c]</ref>, CameraCtrl <ref type="bibr">[He et al. 2024b</ref>], and our method. "TransErr" and "RotErr" are the angle differences between the estimated translation and rotation and the ground-truth ones in degree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Motion transfer</head><p>Baseline methods. We compare DaS with two famous motion transfer methods, TokenFlow <ref type="bibr">[Geyer et al. 2023b]</ref> and CCEdit <ref type="bibr">[Feng et al. 2024b</ref>]. TokenFlow represents video motions with the feature consistency across different timesteps extracted by a diffusion model. Then, the feature consistency is propagated to several keyframes generated by a text prompt for video generation. For TokenFlow, we adopt the Stable Diffusion 2.1 <ref type="bibr" target="#b53">[Rombach et al. 2022</ref>] model for the motion transfer task. CCEdit adopts depth maps as conditions to control the video motion and transfers the motion using a new repainted frame to generate a video.</p><p>Metrics. Since all methods generate the transferred videos based on text prompts, we aim to evaluate the alignment between the generated videos and the text prompts, as well as the video coherence, using the CLIP <ref type="bibr" target="#b50">[Radford et al. 2021]</ref>. Specifically, for video-text alignment, we extract multiple frames from the video and compare them with the corresponding text prompts by calculating the CLIP score <ref type="bibr" target="#b21">[Hessel et al. 2022]</ref> for each frame. This score reflects</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours TokenFlow</head><p>CCEdit "An old man with short gray hair and a short-sleeve navy shirt is looking towards a distant pagoda."</p><p>"An old-fashioned car is parked on the beach, illuminated by the soft glow of the sun." Source Fig. <ref type="figure">5</ref>. Qualitative comparison on motion transfer between our method, CCEdit <ref type="bibr">[Feng et al. 2024b</ref>], and TokenFlow <ref type="bibr">[Geyer et al. 2023b</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Tex-Ali ‚Üë Tem-Con ‚Üë CCEdit 16.9 0.932 Tokenflow 31.9 0.956 Ours 32.6 0.971</p><p>Table <ref type="table">2</ref>. CLIP scores for motion transfer of CCEdit <ref type="bibr">[Feng et al. 2024b]</ref>,</p><p>TokenFlow <ref type="bibr">[Geyer et al. 2023b</ref>], and our method. "Text-Ali" is the semantic CLIP consistency between generated videos and the given text prompts. "Tem-Con" is the temporal CLIP consistency between neighboring frames.</p><p>the alignment between image content and textual descriptions. For temporal consistency, we extract normalized CLIP features from adjacent video frames and compute the cosine similarity between the adjacent features.</p><p>Results. As shown in Table <ref type="table">2</ref>, our method demonstrates outstanding performance in both text alignment and frame consistency, surpassing two baseline methods. Furthermore, Figure <ref type="figure">5</ref> presents the qualitative comparison of our method, CCEdit, and TokenFlow. It shows that CCEdit produces frames of low quality and struggles to maintain temporal coherence. TokenFlow produces semantically consistent frames but has difficulty producing coherent videos. In contrast, our method accurately transfers the video motion with strong temporal coherence as shown in Figure <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Animating meshes to videos</head><p>Qualitative comparison. We compare our method against a stateof-the-art human image animation method CHAMP <ref type="bibr" target="#b82">[Zhu et al. 2024]</ref> on the mesh-to-video task. Champ takes a human image and a motion sequence as input and generates a corresponding human video.  <ref type="figure">8</ref>. We also generate different styles of videos from the same animated 3D meshes as shown in Figure <ref type="figure">8</ref>. Compared to CHAMP, our method demonstrates better consistency in the 3D structure and texture details of the avatar on different motion sequences and across different styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Object manipulation</head><p>Qualitative results. For the object manipulation, we adopt the SAM <ref type="bibr" target="#b28">[Kirillov et al. 2023</ref>] and depth estimation models <ref type="bibr" target="#b2">[Bochkovskii et al. 2024;</ref><ref type="bibr">Wang et al. 2024b</ref>] to get the object points. Then, we evaluate two kinds of manipulation, i.e. translation and rotation. The results are shown in Figure <ref type="figure">9</ref>, which demonstrate that DaS achieves accurate object manipulation to produce photorealistic videos with strong multiview consistency for these objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Transferred "An animated red car moves from left to right, with a deserted city in the background."</p><p>"A green alien is generating ancient cityscapes displayed on a computer screen."</p><p>"A herd of bird-deer in a towering, wooded forest."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Transferred "An anime girl with a white hat and tanned skin sits by the edge of a tranquil mountain lake." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis</head><p>We conduct analysis on the choice of 3D control signals, i.e. depth maps or 3D tracking videos, and the number of 3D tracking points.</p><p>To achieve this, we randomly selected 50 videos from the validation split of the DAVIS <ref type="bibr" target="#b48">[Pont-Tuset et al. 2017]</ref> and MiraData <ref type="bibr" target="#b27">[Ju et al. 2024</ref>] video dataset. We extract the first-frame images as the input image and apply different models to re-generate these videos. To evaluate the quality of the generated videos, we compute PSNR, SSIM <ref type="bibr" target="#b64">[Wang et al. 2004]</ref>, LPIPS <ref type="bibr" target="#b78">[Zhang et al. 2018]</ref>, and FVD <ref type="bibr" target="#b58">[Unterthiner et al. 2019</ref>] between the generated videos and the groundtruth videos.</p><p>4.5.1 Depth maps vs. 3D tracking videos. To illustrate the effectiveness of our 3D tracking videos, we compare DaS with a baseline using depth maps as conditions instead of 3D tracking videos. Specifically, the baseline adopts the same architecture as DaS but replaces the 3D tracking video with a depth map video. We adopt the Depth Pro <ref type="bibr" target="#b2">[Bochkovskii et al. 2024]</ref> to generate the video depth video for this baseline method. As shown in Table <ref type="table" target="#tab_1">3</ref>, our model outperforms this baseline in all metrics, demonstrating that the 3D tracking videos provide a better signal for the diffusion model to recover groud-truth videos than the depth map conditions. Figure <ref type="figure" target="#fig_0">10</ref> shows the generated videos, which demonstrate that our method produces more consistent videos with the ground truth. The main reason is that the 3D tracking videos effectively associate different frames of a video while the depth maps only provide some cues of the scene structures without constraining the motion of the video.</p><p>4.5.2 Point density. In Table <ref type="table" target="#tab_1">3</ref>, we further present an ablation study with varying numbers of 3D tracking points as control signals. The number of 3D tracking points ranges from 900 (30√ó30) to 8100 (90√ó90). Though the generated videos with 4900 tracking points perform slightly better than the other ones, the visual qualities of 2500, 4900, and 8100 tracking points are very similar to each other. Since tracking too many points with SpatialTracker <ref type="bibr">[Xiao et al. 2024b</ref>] would be slow, we choose 4900 as our default setting in all our other experiments using 3D point tracking.</p><p>4.5.3 Runtime. In the inference stage, we employ the DDIM <ref type="bibr" target="#b56">[Song et al. 2020</ref>] sampler with 50 steps, classifier-free guidance of magnitude 7.0, which costs about 2.5 minutes to generate 49 frames on a H800 GPU at a resolution of 480√ó720.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS AND CONCLUSIONS</head><p>Limitations and future works. Though DaS achieves control over the video generation process in most cases, it still suffers from multiple failure cases mainly caused by incorrect 3Dtracking videos. The first failure case is that the input image should be compatible with the 3D tracking videos. Otherwise, the generated videos would be implausible as shown in Figure <ref type="figure" target="#fig_8">11</ref> (top). Another failure case is that for regions without 3D tracking points, the generated contents may be out-of-control and produce some unnatural results (Figure <ref type="figure" target="#fig_8">11</ref> (bottom)). For future works, we currently rely on provided animated meshes or existing videos to get high-quality 3D tracking videos and a promising direction is to learn to generate these 3D tracking videos with a new diffusion model. Conclusions. In this paper, we introduce Diffusion as Shader (DaS) for controllable video generation. The key idea of DaS is to adopt the 3D tracking videos as 3D control signals for video generation. The 3D tracking videos are constructed from colored dynamic 3D points which represent the underlying 3D motion of the video. Then, diffusion models are applied to generate a video following the motion of the 3D tracking video. We demonstrate that the 3D tracking videos not only improve the temporal consistency of the generated videos but also enable versatile control of the video content, including mesh-to-video generation, camera control, motion transfer, and object manipulation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diffusion as Shader (DaS) is (a) a 3D-aware video diffusion method enabling versatile video control tasks including (b) animating meshes to video generation, (c) motion transfer, (d) camera control, and (e) object manipulation.</figDesc><graphic coords="1,54.67,434.54,57.30,53.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>(b)).(2) Motion transfer. Starting with an input video, we employ a 3D tracker[Xiao et al. 2024b]  to generate a corresponding 3D tracking video. Next, the depth-to-image Flux model [Labs 2024] is used to modify the style or content of the first frame. Based on the updated first frame and the 3D tracking video, DaS generates a new video that replicates the motion patterns of the original while reflecting the new style or content (Figure 1 (c)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of DaS. (a) We colorize dynamic 3D points according to their coordinates to get (b) a 3D tracking video. (c) The input image and the 3D tracking video are processed by (d) a transformer-based latent diffusion with a variational autoencoder (VAE). The 3D tracking video is processed by a trainable copy of the denoising DiT and zero linear layers are used to inject the condition features from 3D tracking videos into the denoising process.</figDesc><graphic coords="4,356.19,170.81,110.70,54.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. 3D tracking video generation in (a) object manipulation, (b) animating mesh to video generation, (c) camera control, and (d) motion transfer.</figDesc><graphic coords="5,347.28,199.96,51.41,87.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 4 . 4</head><label>44</label><figDesc>Motion transfer. As shown in Figure 3 (d), DaS also facilitates creating a new video by transferring motion from an existing source video. First, we estimate the depth map of the source video's first frame and apply the depth-to-image FLUX model [Labs 2024] to repaint the frame into a target appearance guided by text prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative results of DaS on the camera control task. We show 4 trajectories (left, right, up, down) with large movements.</figDesc><graphic coords="6,477.93,266.03,78.91,52.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .Fig. 8 .Fig. 9 .Fig. 10 .</head><label>678910</label><figDesc>Fig. 6. Qualitative results on motion transfer of our method.</figDesc><graphic coords="8,148.14,382.76,78.44,52.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc>is seen kiteboarding on a vibrant turquoise sea, expertly balancing atop a blue and white kiteboard."A woman in a blue blouse and a wide-brimmed hat is standing beside a sleek, modern electric bike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Failure cases. (Top) Incompatible tracking video. When a tracking video that does not correspond to the structures of the input image is provided, DaS will generate a video with a scene transition to a compatible new scene. (Bottom) Out of tracking range. For regions without 3D tracking points, the tracking video fails to constrain these regions and DaS may generate some uncontrolled content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>only adopt implicit camera or ray embeddings for camera control.Quantitative results on camera control of MotionCtrl [Wang</figDesc><table><row><cell>Method</cell><cell cols="2">Small Movement</cell><cell cols="2">Large Movement</cell></row><row><cell></cell><cell cols="4">TransErr ‚Üì RotErr ‚Üì TransErr ‚Üì RotErr ‚Üì</cell></row><row><cell cols="2">MotionCtrl 44.23</cell><cell>8.92</cell><cell>67.05</cell><cell>39.86</cell></row><row><cell cols="2">CameraCtrl 42.31</cell><cell>7.82</cell><cell>66.76</cell><cell>29.70</cell></row><row><cell>Ours</cell><cell>27.85</cell><cell>5.97</cell><cell>37.17</cell><cell>10.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Analysis of applying different 3D control signals for image to video generation. We evaluate PSNR, SSIM, LPIPS, and FVD of generated videos on the validation set of the DAVIS and MiraData datasets. "Depth" means using depth maps as the 3D control signals. "Tracking" means using 3D tracking videos as the control signals. #Tracks means the number of 3D points used in the 3D tracking video. et al. 2023] mesh. We use the same input image but the SMPL mesh for CHAMP and generate the corresponding animation videos for qualitative comparison as shown in Figure</figDesc><table><row><cell>‚úì</cell><cell>-</cell><cell>18.08</cell><cell>0.573</cell><cell>0.312 645.1</cell></row><row><cell>‚úì</cell><cell>900</cell><cell>18.52</cell><cell>0.586</cell><cell>0.337 765.3</cell></row><row><cell>‚úì</cell><cell>2500</cell><cell>19.17</cell><cell>0.632</cell><cell>0.263 566.4</cell></row><row><cell>‚úì</cell><cell>4900</cell><cell cols="3">19.27 0.658 0.261 551.3</cell></row><row><cell>‚úì</cell><cell>8100</cell><cell>19.11</cell><cell>0.649</cell><cell>0.262 599.0</cell></row></table><note><p>The motion sequence is represented by an animated SMPL[Loper   Depth  </p><p>Tracking #Tracks PSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì FVD ‚Üì</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.18673</idno>
		<title level="m">AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stable video diffusion: Scaling latent video diffusion models to large datasets</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumith</forename><surname>Kulal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mendelevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yam</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zion</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Letts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.15127</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Depth pro: Sharp monocular metric depth in less than a second</title>
		<author>
			<persName><forename type="first">Aleksei</forename><surname>Bochkovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ama√´l</forename><surname>Delaunoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.02073</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<ptr target="https://openai.com/research/video-generation-models-as-world-simulators" />
		<title level="m">Video generation models as world simulators</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative rendering: Controllable 4d-guided video generation with 2d diffusion models</title>
		<author>
			<persName><forename type="first">Shengqu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matheus</forename><surname>Gadelha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Hao Paul</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuanfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="7611" to="7620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texfusion: Synthesizing 3d textures with text-guided image diffusion models</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangxue</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4169" to="4181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">2023b. Videocrafter1: Open diffusion models for high-quality video generation</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoshu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaofang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.19512</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2024b. Videocrafter2: Overcoming data limitations for high-quality video diffusion models</title>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="7310" to="7320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">2024a. PIXART-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation</title>
		<author>
			<persName><forename type="first">Junsong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="74" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Chieh</forename><surname>Tsai-Shien Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yu</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14404</idno>
		<title level="m">Motion-conditioned diffusion model for controllable video synthesis</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jonathan Granskog, and Anastasis Germanidis. 2023. Structure and content-guided video synthesis with diffusion models</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnathan</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parmida</forename><surname>Atighehchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="7346" to="7356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16496[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2309.16496" />
		<title level="m">CCEdit: Creative and Controllable Video Editing via Diffusion Models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Wanquan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengqi</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.17765</idno>
		<title level="m">Siyu Zhou, and Qian He. 2024a. I2VControl: Disentangled and Unified Video Motion Synthesis Control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Lopez-Guevara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.02700</idno>
		<title level="m">Motion Prompting: Controlling Video Generation with Motion Trajectories</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.10373</idno>
		<title level="m">Consistent diffusion features for consistent video editing</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Michal</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.10373[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2307.10373" />
		<title level="m">Shai Bagon, and Tali Dekel. 2023b. TokenFlow: Consistent Diffusion Features for Consistent Video Editing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparsectrl: Adding sparse controls to text-to-video diffusion models</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="330" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Animatediff: Animate your personalized textto-image diffusion models without specific tuning</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.04725</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">2024b. Cameractrl: Enabling camera control for text-to-video generation</title>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Hao He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ceyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.02101</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Xuanhua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quande</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengju</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.15275</idno>
		<title level="m">2024a. Id-animator: Zero-shot identity-preserving human video generation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Latent video diffusion models for high-fidelity long video generation</title>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13221</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2104.08718" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video diffusion models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8633" to="8646" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Hsin-Ping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02919</idno>
		<title level="m">Fine-grained controllable video generation via object appearance and context</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Peekaboo: Interactive video generation via masked-diffusion</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Nasery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harkirat</forename><surname>Behl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8079" to="8088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hyeonho</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Hao Paul</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.06016</idno>
		<title level="m">Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.06358[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2407.06358" />
		<title level="m">MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segment anything</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhila</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4015" to="4026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rox</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03603</idno>
		<title level="m">Hunyuan-Video: A Systematic Framework For Large Video Generative Models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Mathis</forename><surname>Koroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Caselles-Dupr√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><forename type="middle">Jeanneret</forename><surname>Sanmiguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.10501</idno>
		<title level="m">OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Kuaishou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.10836</idno>
		<ptr target="https://github.com/black-forest-labs/flux" />
		<title level="m">AnimateAnything: Consistent and Controllable Animation for Video Generation</title>
		<imprint>
			<date type="published" when="2024">2024. 2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Black Forest Labs. 2024. FLUX</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image conductor: Precision control for interactive video synthesis</title>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.15339</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.00131</idno>
		<title level="m">Open-Sora Plan: Open-Source Large Video Generation Model</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminal Graphics Papers: Pushing the Boundaries</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="851" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Align3R: Aligned Monocular Depth Estimation for Dynamic Videos</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.03079</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Trailblazer: Trajectory control for diffusion-based video generation</title>
		<author>
			<persName><forename type="first">Wan-Duo Kurt</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">2024a. Follow-your-click: Opendomain regional image animation via short prompts</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.08268</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Tuna</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salih</forename><surname>Meral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hidir</forename><surname>Yesiltepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Dunlop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.05275</idno>
		<title level="m">MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Chong</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingdeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.13865</idno>
		<title level="m">Remake a Video with Motion and Content Control</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sg-i2v: Self-guided trajectory control in image-to-video generation</title>
		<author>
			<persName><forename type="first">Koichi</forename><surname>Namekata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Lindell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.04989</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SIFT: Predicting amino acid changes that affect protein function</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pauline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3812" to="3814" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model</title>
		<author>
			<persName><forename type="first">Muyao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Spectral motion alignment for video motion transfer using diffusion models</title>
		<author>
			<persName><forename type="first">Geon Yeong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonho</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><forename type="middle">Wan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2403.15249</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2023a. Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.09748</idno>
		<ptr target="https://arxiv.org/abs/2212.09748" />
		<title level="m">Scalable Diffusion Models with Transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13720</idno>
		<title level="m">Movie gen: A cast of media foundation models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pondaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Pizzati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.07776</idno>
		<title level="m">Video Motion Transfer with Diffusion Transformers</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbel√°ez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 DAVIS Challenge on Video Object Segmentation</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.16863</idno>
		<title level="m">Freetraj: Tuning-free trajectory control in video diffusion models</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2103.00020" />
		<title level="m">Learning Transferable Visual Models From Natural Language Supervision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer</title>
		<author>
			<persName><forename type="first">Ren√©</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1623" to="1637" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Metzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2023 conference proceedings</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Texture: Text-guided texturing of 3d shapes</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Motioni2v: Consistent and controllable image-to-video generation with explicit motion modeling</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu-Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Chun</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><surname>Hongwei Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Light field networks: Neural scene representations with single-evaluation rendering</title>
		<author>
			<persName><forename type="first">Semon</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Rezchikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredo</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19313" to="19325" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<title level="m">Denoising diffusion implicit models</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Drag-a-video: Non-rigid video editing with point-based interaction</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.02936</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01717[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/1812.01717" />
		<title level="m">Towards Accurate Generative Models of Video: A New Metric &amp; Challenges</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01566</idno>
		<title level="m">Generating rich and controllable motions for video synthesis</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision</title>
		<author>
			<persName><forename type="first">Ruicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassie</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.19115[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/2410.19115" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Tianfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menelaos</forename><surname>Kanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.08523</idno>
		<title level="m">Breathing new life into 3d assets with generative repainting</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Videocomposer: Compositional video synthesis with motion controllability</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangjie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiuniu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">CPA: Camera-pose-awareness Diffusion Transformer for Video Generation</title>
		<author>
			<persName><forename type="first">Yuelei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01429</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><surname>Simoncelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2003.819861</idno>
		<ptr target="https://doi.org/10.1109/TIP.2003.819861" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.07721</idno>
		<title level="m">D: Training-free Object Control with Camera Poses</title>
		<imprint>
			<date type="published" when="2024-05">2024a. ObjCtrl-2.5. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">2024c. Motionctrl: A unified and flexible motion controller for video generation</title>
		<author>
			<persName><forename type="first">Zhouxia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">SpatialTracker: Tracking Any 2D Pixels in 3D Space</title>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="20406" to="20417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.19324</idno>
		<title level="m">Jianlou Si, and Xingang Pan. 2024a. Trajectory Attention for Fine-grained Video Motion Control</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dynamicrafter: Animating open-domain images with video diffusion priors</title>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">2024a. Direct-a-video: Customized video generation with user-directed camera movement and object motion</title>
		<author>
			<persName><forename type="first">Shiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2024 Conference Papers</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2408.06072</idno>
		<title level="m">Guanyu Feng, et al. 2024b. Cogvideox: Text-to-video diffusion models with an expert transformer</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Spacetime diffusion features for zero-shot text-driven motion transfer</title>
		<author>
			<persName><forename type="first">Danah</forename><surname>Yatim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bar-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="8466" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08089</idno>
		<title level="m">Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><forename type="first">Wangbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2409.02048</idno>
		<title level="m">Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Shenghai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfa</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2411.17440</idno>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Identity-Preserving Text-to-Video Generation by Frequency Decomposition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Adding conditional control to text-to-image diffusion models</title>
		<author>
			<persName><forename type="first">Lvmin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anyi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3836" to="3847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.01821</idno>
		<title level="m">World-consistent Video Diffusion with Explicit 3D Modeling</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03924[cs.CV</idno>
		<ptr target="https://arxiv.org/abs/1801.03924" />
		<title level="m">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Lei Qingzhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoquan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno>arxiv:2501.03220</idno>
		<title level="m">ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking</title>
		<imprint>
			<date type="published" when="2025">2025. 2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">2024a. CamI2V: Camera-Controlled Image-to-Video Diffusion Model</title>
		<author>
			<persName><forename type="first">Guangcong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15957</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Stereo Magnification: Learning View Synthesis using Multiplane Images</title>
		<author>
			<persName><forename type="first">Zangwei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianji</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenggui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<ptr target="https://github.com/hpcaitech/Open-SoraTinghuiZhou" />
		<editor>Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>2024b. Open-Sora: Democratizing Efficient Video Production for All In SIG-GRAPH</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">Shenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junming</forename><surname>Leo Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.14781[cs.CV]</idno>
		<title level="m">Champ: Controllable and Consistent Human Image Animation with 3D Parametric Guidance</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
