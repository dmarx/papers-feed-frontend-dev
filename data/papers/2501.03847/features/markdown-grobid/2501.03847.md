# Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control

## Abstract

## 

Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process-such as camera manipulation or content editing-remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation. Codes and more results are available at [https://igl-hkust.github.io/das/](https://igl-hkust.github.io/das/).

## INTRODUCTION

The development of diffusion generative models [[Blattmann et al. 2023;](#b1)[Brooks et al. 2024;](#b3)[Ho et al. 2020;](#b22)[Lin et al. 2024;](#b33)[Rombach et al. 2022;](#b53)[Zheng et al. 2024b](#)] enables high-quality video generation from text prompts or a starting image. Recent emerging models, e.g. Sora [[Brooks et al. 2024]](#b3), CogVideo-X [[Yang et al. 2024b](#)], Keling [[Kuaishou 2024](#b31)], and Hunyuan [[Kong et al. 2024]](#b29), have shown impressive video generation ability with strong temporal consistency and appealing visual effects, which becomes a promising tool for artists to create stunning videos using just few images or text arXiv:2501.03847v2 [cs.CV] 9 Jan 2025 prompts. These advancements show strong potential to revolutionize the advertising, film, robotics, and game industries, becoming fundamental elements for various generative AI-based applications.

A major challenge in video generation lies in achieving versatile and precise control to align seamlessly with users' creative visions. While recent methods have introduced strategies to integrate control into the video generation process [[Guo et al. 2024;](#b16)[He et al. 2024b,a;](#)[Huang et al. 2023;](#b24)[Ma et al. 2024b,a;](#)[Namekata et al. 2024;](#b40)[Polyak et al. 2024;](#b46)[Wang et al. 2024f,c;](#)[Yuan et al. 2024]](#b75), they predominantly focus on specific control types, relying on specialized architectures that lack adaptability to emerging control requirements. Furthermore, these approaches are generally limited to high-level adjustments-such as camera movements or maintaining identity-falling short when it comes to enabling fine-grained modifications, like precisely raising an avatar's left hand.

We argue that achieving versatile and precise video generation control fundamentally requires 3D control signals in the diffusion model. Videos are 2D renderings of dynamic 3D content. In a traditional Computer Graphics (CG)-based video-making pipeline, we can effectively control all aspects of a video in detail by manipulating the underlying 3D representations, such as meshes or particles. However, existing video control methods solely apply 2D control signals on rendered pixels, lacking the 3D awareness in the video generation process and thus struggling to achieve versatile and finegrained controls. Thus, to this end, we present a novel 3D-aware video diffusion method, called Diffusion as Shader (DaS) in this paper, which utilizes 3D control signals to enable diverse and precise control tasks within a unified architecture.

Specifically, as shown in Figure [1](#fig_0) (a), DaS is an image-to-video diffusion model that takes a 3D tracking video as the 3D control signals for various control tasks. The 3D tracking video contains the motion trajectories of 3D points whose colors are defined by their coordinates in the camera coordinate system of the first frame. In this way, the 3D tracking video represents the underlying 3D motion of this video. The video diffusion model acts like a shader to compute shaded appearances on the dynamic 3D points to generate the video. Thus, we call our model Diffusion as Shader.

Using 3D tracking videos as control signals offers a significant advantage over depth videos with enhanced temporal consistency. While a straightforward approach to incorporating 3D control into video diffusion models involves using depth maps as control signals, depth maps only define the structural properties of the underlying 3D content without explicitly linking frames across time. In contrast, 3D tracking videos provide a consistent association between frames, as identical 3D points maintain the same colors across the video. These color anchors ensure consistent appearances for the same 3D points, thereby significantly improving temporal coherence in the generated videos. Our experiments demonstrate that even when a 3D region temporarily disappears and later reappears, DaS effectively preserves the appearance consistency of that region, thanks to the temporal consistency enabled by the tracking video.

By leveraging 3D tracking videos, DaS enables versatile video generation controls, encompassing but not limited to the following video control tasks.

(1) Animating meshes to videos. Using advanced 3D tools like Blender, we can design animated 3D meshes based on predefined templates. These animated meshes are transformed into 3D tracking videos to guide high-quality video generation (Figure [1](#fig_0)  (3) Camera control. To enable precise camera control, depth maps are estimated to extract 3D points [[Bochkovskii et al. 2024](#b2)]. These 3D points are then projected onto a specified camera path to create a 3D tracking video, which guides the generation of videos with customized camera movements (Figure [1 (d)](#fig_0)). (4) Object manipulation. By integrating object segmentation techniques [[Kirillov et al. 2023](#b28)] with a monocular depth estimator [[Bochkovskii et al. 2024]](#b2), the 3D points of specific objects can be extracted and manipulated. These modified 3D points are used to construct a 3D tracking video, which guides the creation of videos for object manipulation (Figure [1 (e)](#fig_0)).

Due to the 3D awareness of DaS, DaS is data-efficient. Finetuning with less than 10k videos on 8 H800 GPUs for 3 days already gives the powerful control ability to DaS, which is demonstrated by various control tasks. We compare DaS with baseline methods on camera control [[He et al. 2024b;](#)[Wang et al. 2024c](#)] and motion transfer [[Geyer et al. 2023a](#)], which demonstrates that DaS achieves significantly improved performances in these two controlling tasks than baselines. For the remaining two tasks, i.e. mesh-to-video and object manipulation, we provide extensive qualitative results to show the superior generation quality of our method.

## RELATED WORK 2.1 Video diffusion

In recent years, the success of diffusion models in image generation [[Ho et al. 2020;](#b22)[Peebles and Xie 2023a;](#)[Rombach et al. 2022]](#b53) has sparked interest in exploring video generation [[Blattmann et al. 2023;](#b1)[Brooks et al. 2024;](#b3)[Chen et al. 2023b](#)[Chen et al. , 2024b;;](#)[Guo et al. 2023;](#b17)[He et al. 2022;](#b20)[Ho et al. 2022;](#b23)[Kong et al. 2024;](#b29)[Kuaishou 2024;](#b31)[Lin et al. 2024;](#b33)[Xing et al. 2024;](#b69)[Yang et al. 2024b;](#)[Zheng et al. 2024b]](#). VDM [[Ho et al. 2022]](#b23) is the first work to explore the feasibility of diffusion in the field of video generation. SVD [[Blattmann et al. 2023](#b1)] introduces a unified strategy for training a robust video generation model. Sora [[Brooks et al. 2024]](#b3), through training on extensive video data, suggests that scaling video generation models is a promising path towards building general-purpose simulators of the physical world. CogVideo-X [[Yang et al. 2024b]](#), VideoCrafter [[Chen et al. 2023b](#)[[Chen et al. , 2024b]]](#), DynamiCrafter [[Xing et al. 2024]](#b69), Keling [[Kuaishou 2024](#b31)], and Hunyuan [[Kong et al. 2024](#b29)] have demonstrated impressive video generation performance with strong temporal consistency.

Controllable video generation. Existing works still lack an effective way to control the generation process. There are many works [[Guo et al. 2024;](#b16)[He et al. 2024b,a;](#)[Huang et al. 2023;](#b24)[Ma et al. 2024b,a,a;](#)[Namekata et al. 2024;](#b40)[Polyak et al. 2024;](#b46)[Qiu et al. 2024;](#b49)[Wang et al. 2024f,c;](#)[Yu et al. 2024;](#b74)[Yuan et al. 2024](#b75)] that introduce a specific control signal in the video generation process which can only achieve one control type like identity preserving, camera control, and motion transfer. Our method is more versatile in various video control types by using a 3D-aware video generation with 3D tracking videos as conditions.

## Controlled video generation

We review the following 4 types of controlled video generation.

Animating meshes to videos. Animating meshes to videos aims to texture meshes. Several works [[Cai et al. 2024;](#b4)[Cao et al. 2023;](#b5)[Richardson et al. 2023;](#b52)[Wang et al. 2023](#b61)] have demonstrated the feasibility of mesh texturization using powerful diffusion models. TexFusion [[Cao et al. 2023](#b5)] applies the diffusion model's denoiser on a set of 2D renders of the 3D object, optimizing an intermediate neural color field to output final RGB textures. TEXTure [[Richardson et al. 2023](#b52)] introduces a dynamic trimap representation and a novel diffusion sampling process, leveraging this trimap to generate seamless textures from various views. G-Rendering [[Cai et al. 2024](#b4)] takes a dynamic mesh as input. To preserve consistency, G-Rendering employs UV-guided noise initialization and correspondence-aware blending of both pre-and post-attention features. Following G-Rendering, our method also targets dynamic meshes, utilizing a diffusion model as a shader to incorporate realistic texture information. Unlike G-Rendering, which preserves consistency at the noise and attention levels, our approach leverages 3D tracking videos as supplementary information, integrating them into the diffusion model to ensure both temporal and spatial consistency.

Camera control. Camera control [[Bahmani et al. 2024;](#b0)[Geng et al. 2024;](#b13)[He et al. 2024b;](#)[Wang et al. 2024e,c;](#)[Xiao et al. 2024a;](#)[Yang et al. 2024a;](#)[Yu et al. 2024;](#b74)[Zheng et al. 2024a](#)] is an important capability for enhancing the realism of generated videos and increasing user engagement by allowing customized viewpoints. Recently, many efforts have been made to introduce camera control in video generation. MotionCtrl [[Wang et al. 2024c](#)] incorporates a flexible motion controller for video generation, which can independently or jointly control camera motion and object motion in generated videos. CameraCtrl [[He et al. 2024b](#)] adopts Pl√ºcker embeddings [[Sitzmann et al. 2021]](#b55) as the primary form of camera parameters, enabling the ViewCrafter [[Yu et al. 2024](#b74)] employs a point-based representation for free-view rendering, enabling precise camera control. AC3D [[Bahmani et al. 2024](#b0)] optimizes pose conditioning schedules during training and testing to accelerate convergence and restricts the injection of camera conditioning to specific positions, reducing interference with other meaningful video features. CPA [[Wang et al. 2024e](#)] incorporates a Sparse Motion Encoding Module to embed the camera pose information and integrating the embedded motion information via temporal attention. Our method aims to use 3D tracking videos as an intermediary to achieve precise and consistent camera control.

Motion transfer. Motion transfer [[Esser et al. 2023;](#)[Geng et al. 2024;](#b13)[Geyer et al. 2023a;](#)[Meral et al. 2024;](#b38)[Park et al. 2024;](#b43)[Pondaven et al. 2024;](#b47)[Wang et al. 2024d,c;](#)[Yatim et al. 2024]](#b72) aims to synthesize novel videos by following the motion of the original one. Gen-1 [[Esser et al. 2023](#)] employs depth estimation results [[Bochkovskii et al. 2024;](#b2)[Lu et al. 2024;](#b35)[Ranftl et al. 2020]](#b51) to guide the motion. TokenFlow [[Geyer et al. 2023a](#)] achieves consistent motion transfer by enforcing consistency in the diffusion feature space. MotionCtrl [[Wang et al. 2024c](#)] also achieves motion transfer by incorporating a motion controller. DiTFlow [[Pondaven et al. 2024]](#b47) proposes Attention Motion Flow as guidance for motion transfer on DiTs [Peebles and Xie 2023a]. Motion Prompting [[Geng et al. 2024](#b13)] utilizes 2D motions as prompts to realize impressive motion transfer. Unlike these approaches, our method employs 3D tracking as guidance for motion transfer, enabling a more comprehensive capture of each object's motion and the relationships between them within the video. This ensures accurate and globally consistent geometric and temporal consistency.

Object manipulation. Object manipulation refers to versatile object movement control for image-to-video generation. Different from camera control, which focuses on changes in perspective, object manipulation emphasizes the movement of the objects themselves. Currently, mainstream methods [[Chen et al. 2023a;](#)[Geng et al. 2024;](#b13)[Jain et al. 2024;](#b25)[Li et al. 2024;](#b32)[Ma et al. 2024b;](#)[Mou et al. 2024;](#b39)[Qiu et al. 2024;](#b49)[Teng et al. 2023;](#b57)[Wang et al. 2024f,c;](#)[Yang et al. 2024a;](#)[Yin et al. 2023](#b73)] typically achieve object manipulation by utilizing directed trajectories or modeling the relationships between bounding boxes with specific semantic meanings. However, these methods primarily rely on 2D guidance to represent the spatial movement of target objects, which often fails to accurately capture user intent and frequently results in distorted outputs. ObjCtrl-2.5D [[Wang et al. 2024a](#b65)] tries to address this limitation by extending 2D trajectories with depth information, creating a single 3D trajectory as the control signal. Better than the single 3D trajectory, our method leverages 3D tracking videos, which offer greater details and more effectively represent the motion relationships between foreground and background for more precise and realistic object manipulation.

Concurrent works. Recently, several works [[Feng et al. 2024a;](#)[Geng et al. 2024;](#b13)[Jeong et al. 2024;](#b26)[Koroglu et al. 2024;](#b30)[Lei et al. 2024;](#)[Niu et al. 2024;](#b42)[Shi et al. 2024;](#b54)[Zhang et al. 2024](#b77)] have explored utilizing motion as control signals. These approaches can be broadly categorized into two groups: 2D motion-based and 3D motion-based methods. [[Koroglu et al. 2024;](#b30)[Lei et al. 2024;](#)[Shi et al. 2024](#b54)] leverage 2D optical flow to condition motion, while [[Geng et al. 2024;](#b13)[Jeong et al. 2024;](#b26)[Niu et al. 2024](#b42)] utilize 2D tracks, which are sparser than optical flow, to track or control video motion. [[Zhang et al. 2024](#b77)] learns to generate 3D coordinates in the video diffusion model, which 3D awareness. [[Feng et al. 2024a](#)] lifts videos into 3D space and extracts the motion of 3D points, enabling a more accurate capture of spatial relationships between objects and supporting tasks such as object manipulation and camera control. Our method, DaS, also leverages recent tracking methods [[Xiao et al. 2024b;](#)[Zhang et al. 2025](#b79)] to construct videos. However, we extend the applicability by unifying a broader range of control tasks, including mesh-to-video generation and motion transfer.

## METHOD 3.1 Overview

DaS is an image-to-video (I2V) diffusion generative model, which applies both an input image and a 3D tracking video as conditions for controllable video generation. In the following, we first review the backend I2V video diffusion model in Sec. 3.2. Then, we discuss the definition of the 3D tracking video and how to inject the 3D tracking video into the generation process as a condition in Sec. 3.3. Finally, in Sec. 3.4, we discuss how to apply DaS in various types of video generation control.

## Backend video diffusion model

DaS is finetuned from the CogVideoX [[Yang et al. 2024b](#)] model that is a transformer-based video diffusion model [[Peebles and Xie 2023a]](#) operating on a latent space. Specifically, as shown in Figure [2 (d)](#fig_2), we adopt the I2V CogVideoX model as the base model, which takes an image I ‚àà R ùêª √óùëä √ó3 as input and generate a video V ‚àà R ùëá √óùêª √óùëä √ó3 . The generated video V has ùëá frames with the same image size of width ùëä height ùêª as the input image. The input image I is first padded with zeros to get an input condition video with the same size ùëá √ó ùêª √ó ùëä √ó 3 as the target video. Then, a VAE encoder is applied to the padded condition video to get a latent vector of size ùëá 4 √ó ùêª 8 √ó ùëä 8 √ó 16, which is concatenated with a noise of the same size. A diffusion transformer (DiT) [[Peebles and Xie 2023b]](#) is iteratively used to denoise the noise latent for a predefined number of steps and the output denoised latent is processed by a VAE decoder to get the video V. In the following, we discuss how to add a 3D tracking video as an additional condition on this base model.

## Finetuning with 3D tracking videos

We add a 3D tracking video as an additional condition to our video diffusion model. As shown in Figure [2 (a,](#fig_2)[b](#)), the 3D tracking video is rendered from a set of moving 3D points {p ùëñ (ùë°) ‚àà R 3 }, where ùë° = 1, ...,ùëá means the frame index in the video. The colors of these points are determined by their coordinates in the first frame, where we normalize the coordinates into [0, 1] 3 and convert the coordinates into RGB colors {c ùëñ }. Note we adopt the reciprocal of z-coordinate in the normalization. These colors remain the same for different timesteps ùë°. Then, to get a specific ùë°-th frame of the tracking video, we project these 3D points onto the ùë°-th camera to render this frame. In Sec. 3.4, we will discuss how to get these moving 3D points and the camera poses of different frames for different control tasks. Next, we first introduce the architecture to utilize the 3D tracking video as a condition for video generation.

Injecting 3D tracking control. We follow a similar design as the ControlNet [[Chen et al. 2024a;](#b65)[Zhang et al. 2023]](#b76) in DaS to add the 3D tracking video as the additional condition. As shown in Figure [2](#fig_2) (d), we apply the pretrained VAE encoder to encode the 3D tracking video to get the latent vector. Then, we make a trainable copy of the pretrained denoising DiT, called condition DiT, to process the latent vector of the 3D tracking video. The denoising DiT contains 42 blocks and we copy the first 18 blocks as the condition DiT. In the condition DiT, we extract the output feature of each DiT block, process it with a zero-initialized linear layer, and add the feature to the corresponding feature map of the denoising DiT. We finetune the condition DiT with the diffusion losses while freezing the pretrained denoising DiT.

Finetuning details. To train the DaS model, we construct a training dataset containing both real-world videos and synthetic rendered videos. The real-world videos are from MiraData [[Ju et al. 2024](#b27)] while we use the meshes and motion sequences from Mixamo to render synthetic videos. All videos are center-cropped and resized to 720 √ó 480 resolution with 49 frames. We only finetune the copied condition DiT while freezing all the original denoising DiT. To construct the 3D tracking video for the rendered videos, since we have access to the ground-truth 3D meshes and camera poses for the synthetic videos, we construct our 3D tracking videos directly using these dense ground-truth 3D points, which results in dense 3D point tracking. For real-world videos, we adopt SpatialTracker [[Xiao et al. 2024b](#)] to detect 3D points and their trajectories in the 3D space. Specifically, for each real-world video, we detect 4,900 3D evenly distributed points and track their trajectories. For training, we employ a learning rate of 1 √ó 10 -4 using the AdamW optimizer. We train the model for 2000 steps using the gradient accumulation strategy to get an effective batch size of 64. The training takes 3 days on 8 H800 GPUs.

## Video generation control

In this section, we describe how to utilize DaS for the following controllable video generation.

3.4.1 Object manipulation. DaS can generate a video to manipulate a specific object. As shown in Figure [3](#fig_3) (a), given an image, we estimate the depth map using Depth Pro [[Bochkovskii et al. 2024]](#b2) or MoGE [[Wang et al. 2024b](#)] and segment out the object using SAM [[Kirillov et al. 2023]](#b28). Then, we are able to manipulate the point cloud of the object to construct a 3D tracking video for object manipulation video generation.

3.4.2 Animating meshes to videos. DaS enables the creation of visually appealing, high-quality videos from simple animated meshes. While many Computer Graphics (CG) software tools provide basic 3D models and motion templates to generate animated meshes, these outputs are often simplistic and lack the detailed appearance and geometry needed for high-quality animations. Starting with these simple animated meshes, as shown in Figure [3](#fig_3) (b), we generate an initial visually appealing frame using a depth-to-image FLUX model [Labs 2024]. We then produce 3D tracking videos from the animated meshes, which, when combined with the generated first frame, guide DaS to transform the basic meshes into visually rich and appealing videos.

## Camera control.

Previous approaches [[He et al. 2024b;](#)[Wang et al. 2024c](#)] rely on camera or ray embeddings as conditions to control the camera trajectory in video generation. However, these embeddings lack true 3D awareness, leaving the diffusion models to infer the scene's 3D structure and simulate camera movement.

In contrast, DaS significantly enhances 3D awareness by incorporating 3D tracking videos for precise camera control. To generate videos with a specific camera trajectory, as shown in Figure [3](#fig_3) (c), we first estimate the depth map of the initial frame using Depth Pro [[Bochkovskii et al. 2024](#b2)] and convert it into colored 3D points. These points are then projected onto the given camera trajectory, constructing a 3D tracking video that enables DaS to control camera movements with high 3D accuracy. Then, using SpatialTracker [[Xiao et al. 2024b](#)], we generate a 3D tracking video from the source video to serve as control signals.

Finally, the DaS model generates the target video by combining the edited first frame with the 3D tracking video.

## EXPERIMENTS

We conduct experiments on five tasks, including camera control, motion transfer, mesh-to-video generation, and object manipulation to demonstrate the versatility of DaS in controlling the video generation process.

## Camera control

Baseline methods. To evaluate the ability to control camera motions of generated videos, we select two representative methodologies, MotionCtrl [[Wang et al. 2024c]](#) and CameraCtrl [[He et al. 2024b]](#) as baseline methods, both of which allow camera trajectories as input and use camera or ray embeddings for camera control. Metrics. To measure the accuracy of the camera trajectories of generated videos, we evaluate the consistency between the estimated camera poses from the generated videos and the input ground-truth camera poses using rotation errors and translation errors. Specifically, for each frame of a generated video, we reconstruct its relative pose given the first frame using [SIFT [Ng and Henikoff 2003]](#). Then, we get the normalized quaternion and translation vectors for the rotation and translation. Finally, we calculate the cosine similarity between the estimated camera poses with the given camera poses.

$RotErr = arccos 1 ùëá -1 ùëá ‚àëÔ∏Å ùëñ=2 ‚ü® q ùëñ gen , q ùëñ gt ‚ü© , TransErr = arccos 1 ùëá -1 ùëá ‚àëÔ∏Å ùëñ=2 ‚ü® t ùëñ gen , t ùëñ gt ‚ü© ,$where ùëá is the number of frames, q ùëñ and t ùëñ are the normalized quaternion and translation vector of the ùëñ-th frame, and ‚ü®‚Ä¢, ‚Ä¢‚ü© means the dot product between two vectors.

Results. We compare against baseline methods on 100 random trajectories from RealEstate10K [[Zhou et al. 2018]](#b81). But since most of the random trajectories only contain small movements, we further test the models on larger fixed movements (moving left, right, up, down, spiral) as shown in Figure [4](#fig_5). As shown in Table [1](#tab_0), our method outperforms the baseline methods, which demonstrates that our method achieves stable and accurate control of the camera poses of the generated videos. The main reason is that due to the utilization of the 3D tracking videos, our method is fully 3D-aware to enable accurate spatial inference in the video generation process. In comparison, baseline methods [[He et al. 2024b;](#)[Wang et al. 2024c]](#)[et al. 2024c]](#), CameraCtrl [[He et al. 2024b](#)], and our method. "TransErr" and "RotErr" are the angle differences between the estimated translation and rotation and the ground-truth ones in degree.

## Motion transfer

Baseline methods. We compare DaS with two famous motion transfer methods, TokenFlow [[Geyer et al. 2023b]](#) and CCEdit [[Feng et al. 2024b](#)]. TokenFlow represents video motions with the feature consistency across different timesteps extracted by a diffusion model. Then, the feature consistency is propagated to several keyframes generated by a text prompt for video generation. For TokenFlow, we adopt the Stable Diffusion 2.1 [[Rombach et al. 2022](#b53)] model for the motion transfer task. CCEdit adopts depth maps as conditions to control the video motion and transfers the motion using a new repainted frame to generate a video.

Metrics. Since all methods generate the transferred videos based on text prompts, we aim to evaluate the alignment between the generated videos and the text prompts, as well as the video coherence, using the CLIP [[Radford et al. 2021]](#b50). Specifically, for video-text alignment, we extract multiple frames from the video and compare them with the corresponding text prompts by calculating the CLIP score [[Hessel et al. 2022]](#b21) for each frame. This score reflects

## Ours TokenFlow

CCEdit "An old man with short gray hair and a short-sleeve navy shirt is looking towards a distant pagoda."

"An old-fashioned car is parked on the beach, illuminated by the soft glow of the sun." Source Fig. [5](#). Qualitative comparison on motion transfer between our method, CCEdit [[Feng et al. 2024b](#)], and TokenFlow [[Geyer et al. 2023b](#)].

## Method

Tex-Ali ‚Üë Tem-Con ‚Üë CCEdit 16.9 0.932 Tokenflow 31.9 0.956 Ours 32.6 0.971

Table [2](#). CLIP scores for motion transfer of CCEdit [[Feng et al. 2024b]](#),

TokenFlow [[Geyer et al. 2023b](#)], and our method. "Text-Ali" is the semantic CLIP consistency between generated videos and the given text prompts. "Tem-Con" is the temporal CLIP consistency between neighboring frames.

the alignment between image content and textual descriptions. For temporal consistency, we extract normalized CLIP features from adjacent video frames and compute the cosine similarity between the adjacent features.

Results. As shown in Table [2](#), our method demonstrates outstanding performance in both text alignment and frame consistency, surpassing two baseline methods. Furthermore, Figure [5](#) presents the qualitative comparison of our method, CCEdit, and TokenFlow. It shows that CCEdit produces frames of low quality and struggles to maintain temporal coherence. TokenFlow produces semantically consistent frames but has difficulty producing coherent videos. In contrast, our method accurately transfers the video motion with strong temporal coherence as shown in Figure [6](#).

## Animating meshes to videos

Qualitative comparison. We compare our method against a stateof-the-art human image animation method CHAMP [[Zhu et al. 2024]](#b82) on the mesh-to-video task. Champ takes a human image and a motion sequence as input and generates a corresponding human video.  [8](#). We also generate different styles of videos from the same animated 3D meshes as shown in Figure [8](#). Compared to CHAMP, our method demonstrates better consistency in the 3D structure and texture details of the avatar on different motion sequences and across different styles.

## Object manipulation

Qualitative results. For the object manipulation, we adopt the SAM [[Kirillov et al. 2023](#b28)] and depth estimation models [[Bochkovskii et al. 2024;](#b2)[Wang et al. 2024b](#)] to get the object points. Then, we evaluate two kinds of manipulation, i.e. translation and rotation. The results are shown in Figure [9](#), which demonstrate that DaS achieves accurate object manipulation to produce photorealistic videos with strong multiview consistency for these objects.

## Source

Transferred "An animated red car moves from left to right, with a deserted city in the background."

"A green alien is generating ancient cityscapes displayed on a computer screen."

"A herd of bird-deer in a towering, wooded forest."

## Source

Transferred "An anime girl with a white hat and tanned skin sits by the edge of a tranquil mountain lake." 

## Analysis

We conduct analysis on the choice of 3D control signals, i.e. depth maps or 3D tracking videos, and the number of 3D tracking points.

To achieve this, we randomly selected 50 videos from the validation split of the DAVIS [[Pont-Tuset et al. 2017]](#b48) and MiraData [[Ju et al. 2024](#b27)] video dataset. We extract the first-frame images as the input image and apply different models to re-generate these videos. To evaluate the quality of the generated videos, we compute PSNR, SSIM [[Wang et al. 2004]](#b64), LPIPS [[Zhang et al. 2018]](#b78), and FVD [[Unterthiner et al. 2019](#b58)] between the generated videos and the groundtruth videos.

4.5.1 Depth maps vs. 3D tracking videos. To illustrate the effectiveness of our 3D tracking videos, we compare DaS with a baseline using depth maps as conditions instead of 3D tracking videos. Specifically, the baseline adopts the same architecture as DaS but replaces the 3D tracking video with a depth map video. We adopt the Depth Pro [[Bochkovskii et al. 2024]](#b2) to generate the video depth video for this baseline method. As shown in Table [3](#tab_1), our model outperforms this baseline in all metrics, demonstrating that the 3D tracking videos provide a better signal for the diffusion model to recover groud-truth videos than the depth map conditions. Figure [10](#fig_0) shows the generated videos, which demonstrate that our method produces more consistent videos with the ground truth. The main reason is that the 3D tracking videos effectively associate different frames of a video while the depth maps only provide some cues of the scene structures without constraining the motion of the video.

4.5.2 Point density. In Table [3](#tab_1), we further present an ablation study with varying numbers of 3D tracking points as control signals. The number of 3D tracking points ranges from 900 (30√ó30) to 8100 (90√ó90). Though the generated videos with 4900 tracking points perform slightly better than the other ones, the visual qualities of 2500, 4900, and 8100 tracking points are very similar to each other. Since tracking too many points with SpatialTracker [[Xiao et al. 2024b](#)] would be slow, we choose 4900 as our default setting in all our other experiments using 3D point tracking.

4.5.3 Runtime. In the inference stage, we employ the DDIM [[Song et al. 2020](#b56)] sampler with 50 steps, classifier-free guidance of magnitude 7.0, which costs about 2.5 minutes to generate 49 frames on a H800 GPU at a resolution of 480√ó720.  

## LIMITATIONS AND CONCLUSIONS

Limitations and future works. Though DaS achieves control over the video generation process in most cases, it still suffers from multiple failure cases mainly caused by incorrect 3Dtracking videos. The first failure case is that the input image should be compatible with the 3D tracking videos. Otherwise, the generated videos would be implausible as shown in Figure [11](#fig_8) (top). Another failure case is that for regions without 3D tracking points, the generated contents may be out-of-control and produce some unnatural results (Figure [11](#fig_8) (bottom)). For future works, we currently rely on provided animated meshes or existing videos to get high-quality 3D tracking videos and a promising direction is to learn to generate these 3D tracking videos with a new diffusion model. Conclusions. In this paper, we introduce Diffusion as Shader (DaS) for controllable video generation. The key idea of DaS is to adopt the 3D tracking videos as 3D control signals for video generation. The 3D tracking videos are constructed from colored dynamic 3D points which represent the underlying 3D motion of the video. Then, diffusion models are applied to generate a video following the motion of the 3D tracking video. We demonstrate that the 3D tracking videos not only improve the temporal consistency of the generated videos but also enable versatile control of the video content, including mesh-to-video generation, camera control, motion transfer, and object manipulation.

![Fig. 1. Diffusion as Shader (DaS) is (a) a 3D-aware video diffusion method enabling versatile video control tasks including (b) animating meshes to video generation, (c) motion transfer, (d) camera control, and (e) object manipulation.]()

![(b)).(2) Motion transfer. Starting with an input video, we employ a 3D tracker[Xiao et al. 2024b] to generate a corresponding 3D tracking video. Next, the depth-to-image Flux model [Labs 2024] is used to modify the style or content of the first frame. Based on the updated first frame and the 3D tracking video, DaS generates a new video that replicates the motion patterns of the original while reflecting the new style or content (Figure 1 (c)).]()

![Fig. 2. Architecture of DaS. (a) We colorize dynamic 3D points according to their coordinates to get (b) a 3D tracking video. (c) The input image and the 3D tracking video are processed by (d) a transformer-based latent diffusion with a variational autoencoder (VAE). The 3D tracking video is processed by a trainable copy of the denoising DiT and zero linear layers are used to inject the condition features from 3D tracking videos into the denoising process.]()

![Fig. 3. 3D tracking video generation in (a) object manipulation, (b) animating mesh to video generation, (c) camera control, and (d) motion transfer.]()

![Motion transfer. As shown in Figure 3 (d), DaS also facilitates creating a new video by transferring motion from an existing source video. First, we estimate the depth map of the source video's first frame and apply the depth-to-image FLUX model [Labs 2024] to repaint the frame into a target appearance guided by text prompts.]()

![Fig. 4. Qualitative results of DaS on the camera control task. We show 4 trajectories (left, right, up, down) with large movements.]()

![Fig. 6. Qualitative results on motion transfer of our method.]()

![is seen kiteboarding on a vibrant turquoise sea, expertly balancing atop a blue and white kiteboard."A woman in a blue blouse and a wide-brimmed hat is standing beside a sleek, modern electric bike.]()

![Fig. 11. Failure cases. (Top) Incompatible tracking video. When a tracking video that does not correspond to the structures of the input image is provided, DaS will generate a video with a scene transition to a compatible new scene. (Bottom) Out of tracking range. For regions without 3D tracking points, the tracking video fails to constrain these regions and DaS may generate some uncontrolled content.]()

![only adopt implicit camera or ray embeddings for camera control.Quantitative results on camera control of MotionCtrl [Wang]()

![Analysis of applying different 3D control signals for image to video generation. We evaluate PSNR, SSIM, LPIPS, and FVD of generated videos on the validation set of the DAVIS and MiraData datasets. "Depth" means using depth maps as the 3D control signals. "Tracking" means using 3D tracking videos as the control signals. #Tracks means the number of 3D points used in the 3D tracking video. et al. 2023] mesh. We use the same input image but the SMPL mesh for CHAMP and generate the corresponding animation videos for qualitative comparison as shown in Figure]()

