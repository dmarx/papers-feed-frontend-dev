<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MODELLING LONG RANGE DEPENDENCIES IN N D: FROM TASK-SPECIFIC TO A GENERAL PURPOSE CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-04-16">16 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Knigge</surname></persName>
							<email>d.m.knigge@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan-Jakob</forename><surname>Sonke</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Netherlands Cancer Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MODELLING LONG RANGE DEPENDENCIES IN N D: FROM TASK-SPECIFIC TO A GENERAL PURPOSE CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-16">16 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">B9F975C76B6FDD8D9A740759EB7961F0</idno>
					<idno type="arXiv">arXiv:2301.10540v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-28T01:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes. Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential (1D), visual (2D) and point-cloud (3D) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered. 1 * Equal contribution. 1 Our code is publicly available at github.com/david-knigge/ccnn.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The vast popularity of Convolutional Neural Networks <ref type="bibr" target="#b25">(LeCun et al., 1998</ref>) (CNNs) is a result of their high performance and efficiency, which has led them to achieve state-of-the-art in applications across sequential <ref type="bibr" target="#b0">(Abdel-Hamid et al., 2014;</ref><ref type="bibr">Van Den Oord et al., 2016)</ref>, visual <ref type="bibr" target="#b23">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b42">Simonyan &amp; Zisserman, 2014)</ref> and high-dimensional data <ref type="bibr" target="#b40">(Schütt et al., 2017;</ref><ref type="bibr" target="#b59">Wu et al., 2019)</ref>. Nevertheless, a major limitation of CNNs -and other neural networks-is that their architectures must be tailored to particular applications in order to consider the length, resolution and dimensionality of the input data. This has led to a plethora of task-specific architectures <ref type="bibr">(Oord et al., 2016;</ref><ref type="bibr" target="#b1">Bai et al., 2018;</ref><ref type="bibr" target="#b42">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b47">Szegedy et al., 2015;</ref><ref type="bibr" target="#b38">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b14">He et al., 2016;</ref><ref type="bibr">Qi et al., 2017;</ref><ref type="bibr" target="#b59">Wu et al., 2019)</ref> which (i) hampers the selection of the most appropriate architecture for a particular task, and (ii) obscures the transfer and generalization of insights across applications. In this work, we tackle the need for problem-specific CNN architectures and propose a generic CNN architecture that can be used independent of the length, resolution and dimensionality of the data.</p><p>CNN architectures are data dependent. Current CNN architectures are task-specific because they are tied to the length, resolution, and dimensionality of the input. The length of the data varies from task to task, e.g. audio fragments may span milliseconds to minutes. This requires carefully chosen  Figure <ref type="figure">2</ref>: Continuous convolutional kernels: the key to a unified CNN architecture. The continuous parameterization of convolutional kernels used in this work consists of a small kernel network ϕ kernel that receives coordinates as input and outputs the value of the convolutional kernel at that position (2a). By changing the dimensionality of the coordinates x i , the same kernel network can render convolutional kernels for sequential (2b), visual (2c), and higher dimensional data (2d).</p><p>strides and pooling to capture relevant dependencies across the entire input <ref type="bibr">(Van Den Oord et al., 2016;</ref><ref type="bibr" target="#b26">Lee et al., 2017)</ref>. In addition, physical signals, e.g., audio, images, are continuous in nature.</p><p>As such, their semantic meaning is independent of the resolution at which they are sampled, e.g., the same audio may be expressed at different resolutions. Nevertheless, current CNN architectures are resolution-bound, and thus different resolutions require different CNNs. These limitations aggravate when considering multi-dimensional data. Each input dimension can be defined at different lengths and resolutions, e.g., video, rectangular images, and each data modality brings its own conventions for each of these properties, e.g., the resolution of a second of audio (16kHz) <ref type="bibr" target="#b54">(Warden, 2018)</ref> strongly contrasts with that of images (32 × 32) <ref type="bibr" target="#b22">(Krizhevsky et al., 2009)</ref>.</p><p>Towards a unified CNN architecture. As discussed in Sec. 3, the core component that makes CNNs data-dependent are their discrete convolutional kernels. Convolutional kernels are implemented via a one-to-one mapping between kernel values and model parameters (Fig. <ref type="figure" target="#fig_0">1</ref> left), which (i) binds them to the input resolution and length, and (ii) makes them ill suited to model long-range dependencies. The latter results from the large number of parameters needed to construct large convolutional kernels. This is why standard CNNs favour using local kernels in combination with task-dependent depths and pooling layers to model long-range dependencies, at the cost of making them task-dependent.</p><p>The need for a continuous parameterization. To overcome task-dependent architectures, it is crucial to define a kernel parameterization that decouples parameter count from kernel size. Following <ref type="bibr" target="#b40">Schütt et al. (2017)</ref>; <ref type="bibr">Romero et al. (2022b)</ref>, we use a small neural network to define a continuous mapping from positions to the value of the kernel at those positions. The resulting Continuous Convolutional Kernels (Fig. <ref type="figure">2</ref>), allow for the construction of convolutional kernels of arbitrary size in a parameter efficient manner. Consequently, the same convolutional layers -and thus the same CNN-can be used regardless of the input length, resolution and dimensionality. We leverage this formulation to construct the Continuous Convolutional Neural Network (CCNN): a single CNN architecture that can be applied regardless of the input length, resolution and dimensionality.</p><p>Empirical results. To showcase the proposed CCNN, we deploy the same CCNN for several tasks on sequential (1D), visual (2D) and point-cloud (3D) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered. Importantly, the continuous parameterization of our CCNN allows it to handle irregularly sampled data natively. As a result, the CCNN is not restricted to grid data, e.g., 3D voxels, and can be used on point-clouds directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>• We propose the Continuous Convolutional Neural Network: a general purpose CNN architecture able to process data of arbitrary resolution, dimensionality and length without structural changes. • We study the layers of CNNs, and demonstrate that the ability to model long-term dependencies on N D without the need of input dependent downsampling and depth values is necessary and sufficient for the construction of a general purpose CNN architecture. • In order to model long-term dependencies on N D without input dependent downsampling and depth values, we utilize and improve the Continuous Kernel Convolutions of <ref type="bibr">Romero et al. (2022b)</ref>.</p><p>Our proposed improvements allow the proposed Continuous CNN to achieve good empirical results on the tasks considered in 1D, 2D and 3D without structural changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>An extended section with extended comparisons to related works is provided in Appx. A. General purpose architectures. To the best of our knowledge, the only existing method aiming for a general purpose architecture is the Perceiver <ref type="bibr" target="#b16">(Jaegle et al., 2021)</ref>, which uses a Transformer to lift restrictions regarding data characteristics and modalities. However, (i) it must map inputs -regardless of their size-to a small latent representation to reduce the quadratic complexity of self-attention, (ii) decouples the depth of the network from its parameter count via recurrence, which requires tuning the number of unrolling steps per task, and (iii) must use absolute positional encodings to encode the data structure, which break the translation equivariance of the self-attention operation <ref type="bibr" target="#b35">(Romero &amp; Cordonnier, 2020)</ref>. In contrast, CCNNs provide a general purpose convolutional method that: (i) scales much more favorably than self-attention, (ii) does not require a constant small latent representation, (iii) does not require task-dependent depths, and (iv) preserves translation equivariance.</p><p>Long range dependencies in N D. Based on our analysis (Sec. 3), any architecture able to model long range dependencies in N D without the need of input-dependent pooling or depth could be used as a general purpose architecture. To our best knowledge, the only existing convolutional methods able to construct global convolutional kernels are CKConvs <ref type="bibr">(Romero et al., 2022b;</ref><ref type="bibr">a)</ref> and state-spaces <ref type="bibr" target="#b11">(Gu et al., 2021;</ref><ref type="bibr">2022)</ref>. However, state-spaces rely on complex dynamical systems that are not easily defined in N D -aside from the combination of 1D systems, equivalent to representing N D kernels as combinations of N independent 1D kernels- <ref type="bibr" target="#b30">Nguyen et al. (2022)</ref>. Consequently, we select CKConvs as the building block of our approach given their advantages in terms of expressivity and simplicity.</p><p>Convolutional kernels as neural networks. Modelling convolution kernels with small neural networks that map kernel positions to kernel values showed promising results for small kernels <ref type="bibr" target="#b18">(Jia et al., 2016;</ref><ref type="bibr" target="#b40">Schütt et al., 2017;</ref><ref type="bibr" target="#b59">Wu et al., 2019)</ref>. Subsequently, <ref type="bibr">Romero et al. (2022b)</ref> realized that this parameterization decouples the size of the convolutional kernel from the number of parameters required to construct it, and thus can be used to construct arbitrarily large convolutional kernels in a parameter efficient manner. In this work, we show that this parameterization allows for the construction of a single CNN that can be used regardless of the input length, resolution and dimensionality. <ref type="bibr">Romero et al. (2022b)</ref> realized that the piece-wise MLPs used so far to parameterize convolutional kernels were unable to model complex long range dependencies due to their spectral bias <ref type="bibr" target="#b48">(Tancik et al., 2020)</ref>, and showed that implicit neural representations -specifically SIRENs <ref type="bibr" target="#b43">(Sitzmann et al., 2020)</ref>-could be used to solve the issue. Subsequently, <ref type="bibr">Romero et al. (2022a)</ref> parameterized their convolutional kernels with Multiplicative Anisotropic Gabor Nets (MAGNets), which provided them control over frequencies admitted in the convolutional kernel, thus preventing aliasing and aiding generalization across resolutions. In our work, we use FlexConvs parameterized by MAGNets. However, we observe that neural networks used to parameterize convolutional kernels are not correctly initialized for that purpose, and propose an initialization that solves the issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FROM DATA-DEPENDENT TO DATA-INDEPENDENT CNN ARCHITECTURES</head><p>In this section, we study the components of CNN architectures and pinpoint the changes required in order to construct a CNN architecture independent of input lengths, resolutions and dimensionalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">POINTWISE OPERATIONS: LINEAR LAYERS, DROPOUT, POINTWISE NONLINEARITIES AND RESIDUAL CONNECTIONS</head><p>Pointwise operations are operations applied to each spatial element of the input separately (Fig. <ref type="figure" target="#fig_2">3</ref>), e.g., pointwise linear layers, dropout and pointwise nonlinearities. As such pointwise operations do not depend on the input shape and model the same function regardless of input length, resolution and dimensionality. Learnable parameters of pointwise operations, e.g., in pointwise linear layers and Parametric ReLU <ref type="bibr" target="#b13">(He et al., 2015)</ref>, are shared over the spatial domain; the same set of parameters is applied to inputs of any spatial shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>Based on the previous observations, we can conclude that pointwise operations can be used without changes in order to construct a general-purpose CNN architecture.</p><p>3.2 GLOBAL OPERATIONS: NORMALIZATION LAYERS AND GLOBAL POOLING Global operations aggregate all spatial elements of the input for their processing, e.g., normalization layers, global pooling. As such, common global operations do not depend on the specific shape of the input signal and their effect is equivalent regardless of the input length, resolution and dimensionality.</p><p>It is important to note, however, that common global CNN operations only define learnable parameters along the channel axes e.g., the scale and mean parameters of a normalization layer have shape w ∈ R N in . Nevertheless, if one were to define a global operation for which spatial positions are also assigned weights, the shape of the parameters would be dependent on the input length, resolution and dimensionalty, and thus the previous statement would no longer hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>The previous observations indicate that global operations that only define channel-wise learnable parameters can be used without changes in order to construct a unified CNN architecture. Luckily, this is the case for all common global operations used in CNN architectures.<ref type="foot" target="#foot_0">foot_0</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LOCAL OPERATIONS: CONVOLUTIONAL LAYERS AND SUBSAMPLING</head><p>Local operations are operations that rely on spatial portions of the input and are applied across its spatial dimensions, e.g., convolution, subsampling. In practice, the neighborhoods on which these operations are applied are hyperparameters, e.g., 3 × 3 kernels or pooling over 2 × 2 regions, and are selected based on the input size. Unfortunately, if the resolution of the input changes then the portion of the input that the operation considers changes and the effect of the operation changes (Fig. <ref type="figure" target="#fig_2">3</ref>).</p><p>Similarly, if the length of the input changes, then larger neighborhoods are required in order to model dependencies across the input. These effects exacerbate if one considers multi-dimensional inputs, as different dimensions might require different neighborhood sizes. Consequently, we can say that local operations depend on the resolution, length and dimensionality of the input.</p><p>A possible solution would be to adjust the size of the operations proportional to resolution and length changes. Unfortunately, if the local operation defines learnable parameters over its spatial dimensions -as in (discrete) convolutional kernels (Sec. 3.3.1)-, then increasing the size of the convolutional kernel is tied to a proportional increase in the number of parameters required to construct it. This, in turn changes the learning dynamics of the network and easily becomes prohibitive.</p><p>The need for a continuous parameterization. A better solution results from using a parameterization in which the number of parameters is independent from the size of the kernel. By doing so, the same parameterization can be used independently from the input length, resolution and dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">FROM DATA-DEPENDENT TO DATA-INDEPENDENT CONVOLUTIONAL LAYERS</head><p>Conventional CNNs implement a discrete version of the convolution operation defined as:</p><formula xml:id="formula_0">(f * k) o (x) = R D f (x -x)k o (x)dx, o ∈ [1, ..., N out ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">f ∶ R D →R N in is a D-dimensional input signal with N in channels, and k ∶ R D →R N in ×Nout is a set of N out convolutional kernels.</formula><p>For the types of data CNNs are commonly used for, e.g. images, the signal f is generally sampled on a discrete grid of equidistant points, and thus it can be described as a function f ∶ Z D → R N in . In practice, the signal f is non-zero only on a finite subset of the grid Ω(f ) ⊂ Z D with limits given by the range of sampling, e.g., height and width of an image. Accordingly, the convolutional kernel k is defined over the same grid of coordinates, of which generally a subset Ω(k) ⊂ Ω(f ) maps to nonzero values. Since both Ω(k) and Ω(f ) are discrete and finite, the convolution can be implemented as the inner product of function and kernel values at each position:</p><formula xml:id="formula_2">(f * k) o (x) = x∈Ω(k) f (x -x)k o (x), x ∈ Ω(f ).<label>(2)</label></formula><p>Discrete convolutional kernels tie convolutional layers to data characteristics. Conventionally, the convolutional kernel k is implemented through a discrete set of randomly initialized weights W, Finally, we apply the convolution between the generated convolutional kernel and the input (4c).</p><p>of which each entry w i ∈ R N in ×Nout corresponds to a point x i ∈ Ω(k). Consequently, an increased kernel size is reflected in a larger set Ω(k), and thus in a correspondingly larger weight matrix W.</p><p>This directly ties a model's parameter count to its kernel size.</p><p>In order to model the long-range dependencies needed to extract high-level features in a parameterefficient way, we must then resort to pooling operations and the stacking of layers that implicitly increase the receptive field of the kernel. This in turn, makes CNNs effective only on inputs of a certain size. For example, if we apply a network created to model long-range dependencies over images of size 256 × 256 to images of size 32 × 32, then intermediary pooling layers would make the spatial extent of the feature maps collapse long before all convolutional layers are applied. Similarly, if we use a network designed to model long-range dependencies over images of size 32 × 32 to images of size 256 × 256, then the network will not be able to model long-range dependencies in the input.</p><p>Additionally, the definition of the kernel k through a discrete set of weights W ties the model to a given input resolution. Yet, for many applications, the input f is a discretization of an underlying continuous function. Therefore, we would like our model to provide the same responses regardless of the resolution at which f is provided. As discrete convolutional kernels live in a discrete domain, they cannot be easily represented at other resolutions. In fact, one can show that discrete CNNs do not generalize to unseen resolutions <ref type="bibr">Romero et al. (2022a)</ref>; <ref type="bibr" target="#b30">Nguyen et al. (2022)</ref>. Consequently, it is not possible to reliably apply trained discrete CNNs across resolutions.</p><p>In addition, note that in Eq. 2, the same discrete convolutional kernel k can be used at every location x ∈ Ω(f ) only because the input signal f is defined over an equidistant grid, and the values of the discrete kernel k(x) align with the features f (x), ∀x ∈ Ω(f ). This is not the case for irregular data. Consequently, discrete kernels are ill-suited to handle irregular data. With weights fixed to relative positions, an infinite number of weights would be needed to cover any continuous domain.</p><p>These limitations suggest a better approach to model convolutional kernels: using the model weights to parameterize k as a continuous function over the data domain R d .</p><p>A data independent parameterization. To obtain a formulation for a CNN applicable to arbitrary resolutions and sizes, we require a parameterization for convolutional layers that is invariant to the set Ω(f ) over which f is sampled. In other words, we must find a parameterization with which the kernel can be modelled over the underlying continuous domain of the input signal, i.e., R d . Moreover, to avoid models with different parameter count for different resolutions, it is necessary that the parameterization of the kernel decouples its parameter count from the size of the kernel.</p><p>Such a parameterization is provided by Continuous Kernel Convolutions (CKConvs) <ref type="bibr">(Romero et al., 2022b;</ref><ref type="bibr">a)</ref>. CKConvs provide a continuous parameterization for convolutional kernels by using a small neural network ϕ Kernel ∶ R D → R Nout×N in as a kernel generator network. This network maps coordinates in the domain of the kernel x i ∈ R D to the values of the convolutional kernel at that position: k(x) ∈ R Nout×N in (Fig. <ref type="figure" target="#fig_0">1</ref>). A Continuous Kernel Convolution is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Since the parameter count of the kernel generator network is independent from the number of points in the neighborhood that determines the size of the kernel, CKConvs allow for construction of arbitrarily large kernels without increasing the parameter count of the layer. <ref type="bibr">Romero et al. (2022b)</ref> shows that large kernels allow CNNs to model long range spatial dependencies at every layer, thus removing the need for downsampling and stacking of layers to increase receptive fields. This in turn allows us to build an architecture which does not contain resolution-, dimensionality-, and size-dependent layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>The previous observations indicate that local operations equipped with existing parameterizations are tied to the length, resolution and dimensionality of the input. Nevertheless, this limitation can be lifted if an alternative parameterization is used that detaches the neighborhood of action of the operation from the length, resolution and dimensionality of the input. Starting point. In principle, the CNN architectures with CKConv <ref type="bibr">(Romero et al., 2022b)</ref> and FlexConv <ref type="bibr">(Romero et al., 2022a)</ref> introduced previously fulfill the requirements posited in Sec. 3. Nevertheless, as depicted in these papers, these architectures still must be tailored to specific applications and domains in order to perform well, e.g., TCN <ref type="bibr" target="#b1">(Bai et al., 2018)</ref> and ResNet <ref type="bibr" target="#b14">(He et al., 2016)</ref> backbones for sequential and visual tasks, respectively in <ref type="bibr">Romero et al. (2022a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A GENERAL PURPOSE CNN ARCHITECTURE</head><p>In order to construct a single performant general purpose architecture that works well across all tasks considered, we start from a FlexNet <ref type="bibr">(Romero et al., 2022a)</ref>, and propose several structural changes (Sec. 4.1). The resulting CCNN architecture is shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODIFICATIONS AND IMPROVEMENTS</head><p>Proper initialization of the kernel generator network ϕ Kernel . First, we analize the kernel generator network ϕ Kernel , and observe that it is not initialized properly in previous works <ref type="bibr" target="#b40">(Schütt et al., 2017;</ref><ref type="bibr" target="#b59">Wu et al., 2019;</ref><ref type="bibr">Romero et al., 2022b;</ref><ref type="bibr">a)</ref> for the purpose of parameterizing convolutional kernels.</p><p>Recall that, in order to ensure training stability it is desirable to retain a constant (unitary) variance throughout the activations of a neural network <ref type="bibr" target="#b10">(Glorot &amp; Bengio, 2010)</ref>. Hence, the discrete weights W conventionally used to construct a convolutional kernel are initialized to have variance inversely proportional to the number of elements over which the convolution is computed, i.e., the number of pixels Ω(k) times the number of channels N in . For instance, He initialization <ref type="bibr" target="#b13">(He et al., 2015)</ref> initializes W s.t. Var(W)=g 2 (N in Ω(k) ), with a gain g that depends on the nonlinearity used. In related works, the networks used to parameterize convolutional kernels are themselves initialized to preserve a constant (unitary) variance throughout the network. Consequently, when used as a kernel generator network, standard initializations lead the generated kernel to have unitary variance, i.e., Var(k)=1. This in turn, make CNNs using neural networks to parameterize their convolutional kernels experience a layer-wise growth in the variance of the feature maps proportional to N in ⋅ Ω(k) . This growth is of particular importance for kernel generator networks that generate large convolutional kernels, i.e., with large Ω(k) . For instance, we observe that the logits of CNNs with CKConvs <ref type="bibr">(Romero et al., 2022b)</ref> and FlexConvs <ref type="bibr">(Romero et al., 2022a)</ref> lie in the order of 1e 19 upon initialization: an undersirable property that might lead to unstable training and a need for low learning rates.</p><p>To address this issue, we must ensure that the variance at the output of the kernel generator network is inversely proportional to N in ⋅ Ω(k) . Inspired by <ref type="bibr" target="#b3">Chang et al. (2020)</ref>, we therefore re-weight the last linear layer of the kernel generator network ϕ Kernel by g 2 N in ⋅ Ω(k) . With this modification, we observe that the variance of the generated convolutional kernels satisfies the desired constrains, and consequently, the logits of our CCNN show unitary variance upon initialization (Fig. <ref type="figure" target="#fig_5">6</ref>).</p><p>Depthwise Separable Continuous Convolutions. Separable convolutions have long been used to improve the parameter and computational efficiency of CNNs <ref type="bibr" target="#b34">(Rigamonti et al., 2013;</ref><ref type="bibr" target="#b41">Sifre &amp; Mallat, 2014)</ref>. Recent architectures have leveraged separability, and found CNNs with separable kernels to perform better than CNNs with conventional convolutions <ref type="bibr" target="#b21">(Knigge et al., 2021;</ref><ref type="bibr" target="#b27">Liu et al., 2022)</ref>. This phenomenon results from the separation of spatial and channel dimensions, which allows for wider networks without additional computational and parameter complexity.</p><p>Based on these observations, we construct a depth-wise separable version of FlexConv <ref type="bibr">(Romero et al., 2022a)</ref>, in which a channel-wise convolution is computed with a kernel generated by a kernel generator network ϕ Kernel ∶ R D → R N in , followed by a pointwise linear layer from N in to N out dimensions. Separable FlexConvs allow for the construction of a much wider CCNN -from 30 to 140 hidden channels-with the same parameter and computation complexity.</p><p>An improved residual block. Residual connections <ref type="bibr" target="#b14">(He et al., 2016)</ref> provide training stability and improved performance. A residual block R(f )=ψ(f ) + f is defined as the sum between the input and a so-called residual connection ψ composed of multiple layers: convolutional, normalization, etc.</p><p>Figure <ref type="figure">7</ref>: Residual blocks used in FlexCCNNs <ref type="bibr">(Romero et al., 2022a)</ref>, S4 <ref type="bibr" target="#b12">(Gu et al., 2022)</ref> and CCNNs (ours).</p><p>Recent studies found improvements over the residual block of <ref type="bibr" target="#b14">He et al. (2016)</ref> by changing the nonlinearities as well as the position and type of normalization layers within the blocks <ref type="bibr" target="#b61">(Xiong et al., 2020;</ref><ref type="bibr" target="#b27">Liu et al., 2022)</ref>. Based on these advances and empirical evidence, we modify the residual blocks of <ref type="bibr">Romero et al. (2022a)</ref> with residual blocks similar to those of S4 <ref type="bibr" target="#b12">(Gu et al., 2022)</ref> complemented with a nonlinearity at the end of the block. A comparison of our residual block and those in <ref type="bibr" target="#b12">Gu et al. (2022)</ref> and <ref type="bibr">Romero et al. (2022a)</ref> is given in Fig. <ref type="figure">7</ref>.</p><p>L 2 regularization of continuous kernels. Weight decay penalizes the L 2 norm of learned kernel values k by adding the norm of the weights W as an additional loss term <ref type="bibr" target="#b24">(Krogh &amp; Hertz, 1991)</ref>. Since continuous kernels are not directly parameterized by weights W, applying L 2 regularization on the parameters of ϕ Kernel directly would not have the intended effect. Consequently, in order to produce the same effect, we amend the L 2 regularization term to penalize the generated convolution kernel instead. Given ϕ ∶,l</p><p>Kernel the set of generated kernel for all channels at a layer l, L obj the objective loss function, and λ the regularizing parameter, we define a regularized loss as:</p><formula xml:id="formula_3">L = L obj + L reg = L obj + λ ⋅ 1 2 L l=1 ϕ ∶,l Kernel 2 .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We aim to define an architecture that can be applied regardless of the specific data characteristics. To this end, we construct a CCNN and validate it on sequential (1D), visual (2D) and point-cloud (3D) datasets -see Appx. B for a detailed description of each dataset-. We show that the same CCNN obtains state-of-the-art results on several sequential tasks, competitive performance on image tasks, and surpasses the Perceiver on point-cloud processing. In addition, we show that learned CCNNs generalize from regular to irregular data by transferring a trained CCNN from voxels to point-clouds.</p><p>Architecture specifications. To study the scalability of our method, we create two CCNNs with differing numbers of hidden layers sizes: CCNN 4,140 (4 blocks, 140 channels, 200K params) and CCNN 6,380 (6 blocks, 380 channels, 2M params). The kernel network ϕ Kernel is chosen to be a 3-layer MAGNet <ref type="bibr">(Romero et al., 2022a)</ref>, with 32 hidden units for CCNN 4,140 , and 64 hidden units for the larger CCNN 6,380 . In each task, the input dimension of the kernel network ϕ Kernel corresponds to the dimensionality of the data -1 for sequences, 2 for images, and 3 for point-clouds-. Additional details regarding hyperparameters, training regimes and experimental settings are given in Appx. C. An empirical assessment of the computational efficiency of our architectures is given in Appx D.1.</p><p>CCNNs on sequential datasets. In 1D we consider Sequential MNIST, Permuted <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, Sequential CIFAR10 <ref type="bibr" target="#b22">(Krizhevsky et al., 2009)</ref>, the Long Range Arena <ref type="bibr" target="#b49">(Tay et al., 2021)</ref> and the Speech Commands dataset <ref type="bibr" target="#b54">(Warden, 2018)</ref>. Our results (Tabs. 1, 2), demonstrate that CCNNs obtain state-of-the-art across several tasks, surpassing the performance of tailored architectures such as Recurrent Neural Networks and Transformers. The performance of CCNNs is explained by their ability to model long term dependencies with higher capabilities than tailored models. Interestingly, we observe that on tasks defined over flattened 2-dimensional signals, e.g., sMNIST, sCIFAR10, CCNNs learn to construct very large periodic kernels that model flattened local dependencies in 2D (Fig. <ref type="figure" target="#fig_11">12</ref>). This showcases the ability of CCNNs to model meaningful long range dependencies.</p><p>Additionally, we evaluate the capacity of CCNNs to classify speech <ref type="bibr" target="#b54">(Warden, 2018)</ref> both from prepossessed (Speech-MFCC) and raw signals (Speech-Raw). Our results (Tab. 1) demonstrate state-of-the-art results on both preprocessed and raw signals -of length 16000-, thus demonstrating the ability of CCNNs to process very heterogeneous data types with a unified architecture.</p><p>Generalization across resolutions. CCNNs are defined on a continuous space. Consequently, it is possible to train a CCNN at one resolution and deploy it at other resolutions -a feat not achievable with discrete models <ref type="bibr">(Romero et al., 2022a;</ref><ref type="bibr" target="#b30">Nguyen et al., 2022)</ref>-. To showcase this ability, we train CCNNs on Speech-Raw, and test them on a subsampled version of the dataset (Speech-0.5x). CCNNs not only generalize, but outperform existing models on zero-shot prediction over resolution changes.</p><p>CCNNs on image datasets. In 2D, we consider the CIFAR10, CIFAR100 <ref type="bibr" target="#b22">(Krizhevsky et al., 2009)</ref> and the STL10 datasets <ref type="bibr" target="#b4">(Coates et al., 2011)</ref>. CCNNs outperform existing continuous convolutional vision architectures <ref type="bibr" target="#b39">(Ruthotto &amp; Haber, 2020;</ref><ref type="bibr" target="#b51">Tomen et al., 2021;</ref><ref type="bibr">Romero et al., 2022b;</ref><ref type="bibr">a)</ref> and are competitive with existing large scale discrete models, while being (much) smaller (Tab. 1).</p><p>The importance of modelling long range dependencies in N D. In principle, all tasks can be treated as sequential tasks ignoring the N D structure -as done in S4 <ref type="bibr" target="#b12">(Gu et al., 2022)</ref> for images due to the complexity of defining state spaces on 2D-, but this sacrifices structural information. Contrarily, CCNNs can be easily defined on multidimensional spaces simply by changing the dimension of the input coordinates of the kernel generator networks. We observe that by considering the 2D structure of the Image and Pathfinder tasks of the LRA benchmark, much better results can be obtained (Tab. 2, right). In PathFinder with 2D images, the CCNN 6,380 obtains an accuracy of 96.00, outperforming the previous state-of-the-art by almost 10% points and performing remarkably better than on flattened images. Additionally, we observe that models trained on the original 2D data converge faster than their sequential counterparts (Fig. <ref type="figure" target="#fig_7">9</ref>). Finally, we remark that discrete 2D CNNs with small convolutional kernels, e.g., ResNet-18 <ref type="bibr" target="#b14">(He et al., 2016)</ref>, are unable to solve Pathfinder due to the lack of fine-grained global context resulting from intermediate pooling layers. This was also seen by <ref type="bibr" target="#b12">Gu et al. (2022)</ref>.</p><p>CCNNs on point-cloud datasets. An additional advantage that comes from the continuous nature of CCNNs, is that -contrary to discrete CNNs-CCNNs can be seamlessly applied on irregular data, e.g., point-clouds. To assess the performance of CCNNs in 3D, we consider the ModelNet40 dataset <ref type="bibr" target="#b60">(Wu et al., 2015)</ref>, which consists of 3D meshes of objects often treated as point-clouds <ref type="bibr" target="#b59">(Wu et al., 2019)</ref>. Our results show that CCNNs are able to achieve a decent overall classification accuracy in comparison to point-cloud specific architectures with a relatively low parameter count (200 K). In particular, CCNNs outperform the Perceiver <ref type="bibr" target="#b16">(Jaegle et al., 2021)</ref> while being much smaller (Fig. <ref type="figure" target="#fig_6">8</ref>).</p><p>From regular to irregular data. Interestingly, the continuous nature of CCNNs allows us to use CCNNs trained on regular data to process irregular data. We showcase this ability by training a CCNN 4,140 on a version of ModelNet10 <ref type="bibr" target="#b60">(Wu et al., 2015)</ref> voxelized onto a grid of 40×40×40 voxels   -Fig. <ref type="figure" target="#fig_9">11b</ref> shows an example of the point-cloud and voxelized datasets-. After trained, we deploy the CCNN on the original (point-cloud) and voxelized test sets and observe respective test accuracies of 90.99 and 90.64. This result shows that CCNNs learn representations that reflect the continuous nature of the data, showcasing CCNNs as a truly general-purpose architecture even able to generalize across different data representations -a feat hardly obtainable by existing architectures-.</p><p>Large CCNNs overfit on point-clouds, but tiny CCNNs perform surprisingly well. In contrast to 1D and 2D, we observe that larger CCNNs perform worse than smaller ones on 3D point-clouds (Fig. <ref type="figure" target="#fig_6">8</ref>).</p><p>We hypothesize that this is a result of the sparsity of the task. Continuous convolutional kernels define a kernel function over the entire domain even if the number of kernel values sampled is sparse. As a result, sparse supervision makes it hard for the network to learn a kernel function that generalizes to unseen data, as sparsity exposes the kernel function to aliasing -high frequencies between sampled points-. To corroborate this hypothesis, we train extremely small CCNNs -with 6K, 14K and 48K parameters-on ModelNet40. We observe that these models achieve surprising results and even outperform the 200K and 2M CCNNs (Fig. <ref type="figure" target="#fig_6">8</ref>). These results supports our previous hypothesis, but also show that the continuous kernel paradigm can be used to construct extremely small, performant CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS AND FUTURE WORK</head><p>Computational efficiency. Although convolutions with large convolutional kernels scale better than self-attention, e.g., Perceiver <ref type="bibr" target="#b16">(Jaegle et al., 2021)</ref>, they can still be expensive. Luckily, the Fourier convolution can be used to strongly reduce their cost, e.g., <ref type="bibr">Romero et al. (2022b;</ref><ref type="bibr">a)</ref>; <ref type="bibr" target="#b12">Gu et al. (2022)</ref>.</p><p>Nevertheless, the Fourier convolutions by themselves are not sufficient to scale CCNNs to very large inputs. This problem is exacerbated if irregular data is considered, as it (i) prevents the usage Fourier convolutions, and (ii) requires rendering a different convolutional kernel for each spatial position of each sample in the batch. An important avenue for future research may look into reducing computational requirements, either via separability or self-adjusting architectures, e.g., downsampling <ref type="bibr" target="#b33">(Riad et al., 2022)</ref>, for regular data, and "gridifying" techniques for irregular data.</p><p>Larger models on point-cloud data. Although CCNNs performs remarkably well on point-clouds with extremely small models, the current CCNN formulation is unable to achieve better performance with larger models. We hypothesize that this is due to increasing aliasing in the learned kernel generator networks. Although anti-aliasing techniques for kernel generator networks exist <ref type="bibr">(Romero et al., 2022a)</ref>, this assume an underlying grid and a corresponding Nyquist frequency. An additional avenue for future research may look into the aliasing issue on irregular data either by generalizations of <ref type="bibr">Romero et al. (2022a)</ref>, or properly-defined smoothing techniques.</p><p>Cross-modal training and data fusion. CCNNs present a potential solution for cross-modal training -currently challenging due to dissimilar per-modality architectures-. Additionally, the continuous properties of CCNNs allow them to be trained on a wide range of data sources, e.g., multiple resolutions, regular, irregular data, etc, which is interesting for several machine learning application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX MODELLING LONG RANGE DEPENDENCIES IN N D: FROM TASK-SPECIFIC TO A GENERAL PURPOSE CNN</head><p>A EXTENDED RELATED WORK</p><p>In this section, we provide a more extensive treatment of similar methods related to continuous reparameterizations of convolutional kernels and their limitations in use for general purpose architectures.</p><p>Rethinking convolutional kernels. Several previous works investigate a reformulation of CNNs based on modifications to the classical convolutional layer. These works provide formulations of continuous convolutional kernels pursuing several different motivations. Several works apply specifically to point-cloud data -for which discrete kernels are not appropriate-by interpolating a set of weights to obtain a definition over the continuous input space <ref type="bibr" target="#b15">(Hua et al., 2018;</ref><ref type="bibr" target="#b50">Thomas et al., 2019)</ref> or expressing the kernels in a polynomial <ref type="bibr" target="#b62">(Xu et al., 2018)</ref> or neural network basis <ref type="bibr" target="#b18">(Jia et al., 2016;</ref><ref type="bibr" target="#b40">Schütt et al., 2017;</ref><ref type="bibr" target="#b53">Wang et al., 2018;</ref><ref type="bibr" target="#b59">Wu et al., 2019)</ref>. These architectures often rely on sophisticated data augmentation and feature engineering schemes, perform pooling or downsampling operations, and have not been shown to perform well on regular data, e.g. images <ref type="bibr" target="#b59">(Wu et al., 2019)</ref>.</p><p>Other work focuses on continuous kernel formulations to address specific architectural considerations, for example to increase receptive fields efficiently <ref type="bibr" target="#b46">(Su &amp; Wen, 2021)</ref>, or to learn more appropriate kernel geometries <ref type="bibr" target="#b5">(Dai et al., 2017;</ref><ref type="bibr" target="#b17">Jeon &amp; Kim, 2017)</ref>. In these examples, a fixed set of weights is interpolated over the kernel domain, which implicitly defines a low-resolution kernel that impedes modeling long term dependencies in a dense manner <ref type="bibr">(Romero et al., 2022a)</ref>.</p><p>A different line of work incorporating continuous kernel definitions focuses on exploiting desirable properties of spatially structured kernels for implementing convolutions equivariant to rototranslations <ref type="bibr" target="#b41">(Sifre &amp; Mallat, 2014;</ref><ref type="bibr" target="#b58">Worrall et al., 2017;</ref><ref type="bibr" target="#b56">Weiler et al., 2018;</ref><ref type="bibr" target="#b55">Weiler &amp; Cesa, 2019;</ref><ref type="bibr" target="#b2">Bekkers, 2019;</ref><ref type="bibr" target="#b8">Finzi et al., 2020)</ref>, or dilation-translation transformations <ref type="bibr" target="#b44">(Sosnovik et al., 2019;</ref><ref type="bibr" target="#b57">Worrall &amp; Welling, 2019;</ref><ref type="bibr" target="#b45">Sosnovik et al., 2021)</ref>. Although these formulations also in principle give us handles for a continuous definition of convolutional kernels over the spatial domain, we choose not to restrict the kernel functions learned in our models to be equivariant, as this limits applicability to settings in which such priors are not warranted.</p><p>Note that graph convolutions (Kipf &amp; Welling, 2016) may similarly be viewed as providing a convolution operation invariant to arbitrary permutation symmetries on the input. Since this operation disregards positional information, it essentially shares the kernel function over the domain. In practice, the bias to permutation invariance needlessly impedes network expressivity for data where such assumptions do not hold, such as sequences, images or point-clouds.</p><p>Continuous Kernel Convolutions <ref type="bibr">(Romero et al., 2022b</ref>) may be seen as a special case of neural message passing <ref type="bibr" target="#b9">(Gilmer et al., 2017)</ref> on a fully connected graph over the input, with messages conditioned on relative positions of nodes. In the case of FlexConv <ref type="bibr">(Romero et al., 2022a)</ref>, graph connectivity is dynamic, conditioned on distance. B DATASET DESCRIPTION Sequential and Permuted MNIST. The MNIST dataset LeCun et al. (1998) consists of 70K grayscale 28×28 handwritten digits divided into training validation and test sets of 60K and 10K samples, respectively. For validation purposes, the training dataset is further divided into training and validation sets of 55K and 5K samples, respectively.</p><p>The sequential MNIST dataset (sMNIST) presents MNIST images as a sequence of 784 pixels for digit classification. Consequently, good predictions require the model to preserve long-term dependencies up to 784 steps in the past. The permuted MNIST dataset (pMNIST) incorporates an additional level of difficulty by permuting the order of all sMNIST sequences with a random permutation. Resultantly, models can no longer rely on local information for the construction of their features and the importance of modelling long-term dependencies becomes more pronounced.</p><p>CIFAR10, CIFAR100 and Sequential CIFAR10. The CIFAR10 dataset <ref type="bibr" target="#b22">Krizhevsky et al. (2009)</ref> consists of 60K real-world 32×32 RGB images uniformly drawn from 10 classes divided into training and test sets of 50K and 10K samples, respectively. The CIFAR100 dataset <ref type="bibr" target="#b22">Krizhevsky et al. (2009)</ref> is similar to the CIFAR10 dataset, with the difference that the images are now uniformly drawn from 100 different classes. For validation purposes, the training dataset of both CIFAR10 and CIFAR100 are further divided into training and validation sets of 45K and 5K samples, respectively.</p><p>Analogously to the sMNIST dataset, the sequential CIFAR10 (sCIFAR10) dataset presents CIFAR10 images as a sequence of 1024 pixels for image classification. This dataset is more difficult than sMNIST, as (i) larger memory horizons are required to successfully solve the task, and (ii) more complex structures and intra-class variations are present in the images. The Image task corresponds to the sequential CIFAR10 dataset with the only difference that the CIFAR10 images are treated as gray-scale images. The Pathfinder and Path-X tasks are binary tasks in which binary images are provided and the model must predict whether the two points in the images are connected with a line or not -see Fig. <ref type="figure" target="#fig_8">10</ref> for an example-. The difference between both datasets is their resolution. Whereas Pathfinder has images of size 32×32, Path-X has images of size 128×128. It is important to mention that these tasks are so difficult that even if treated as 2D signals, CNNs without global receptive fields cant solve them <ref type="bibr" target="#b12">(Gu et al., 2022)</ref>.</p><p>STL-10. The STL-10 dataset <ref type="bibr" target="#b4">(Coates et al., 2011)</ref> is a subset of the ImageNet dataset <ref type="bibr" target="#b23">(Krizhevsky et al., 2012)</ref> consisting of 13,000 96×96 real-world RGB images uniformly drawn from 10 classes divided into training and test sets of 5K and 8K images, respectively. For validation purposes, the training dataset is further divided into training and validation sets of 4,500 and 500 samples, respectively.</p><p>ModelNet40. The ModelNet40 dataset <ref type="bibr" target="#b60">(Wu et al., 2015)</ref> contains 12,311 3D meshes of objects belonging to 40 classes. We use the official split with 9,843 training samples and 2,468 validation samples. In contrast with previous works which sample 1,024 points from these meshes, (e.g. Wu et al. ( <ref type="formula">2019</ref>)), we sample 512 points uniformly from the faces of each mesh, along with the normal vectors at each of these positions. These position and normal vectors serve as input to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ModelNet10</head><p>Voxelized. ModelNet10 is a subset of ModelNet40 containing orientation-aligned samples from 10 classes of objects. We voxelize ModelNet10 based on a subsampling of 4096 points from the meshes. First, we normalize these points to fall within the interval [-1, 1]. Next, we bin these points into a grid of 40 × 40 × 40 voxels. All nonzero voxels get assigned their location as feature value. Afterwards, to align with our point-cloud configuration, we mask out the all but 512 nonzero voxels. Throughout the network, we only apply convolutions at the locations of each nonzero voxel (effectively masking out activations at all other voxel locations). Like with the point-cloud configuration, to limit computational complexity, we limit the convolutions to integrate over the closest 256 points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 HYPERPARAMETERS AND TRAINING DETAILS</head><p>Optimizer and learning rate scheduler. All our models are optimized with AdamW <ref type="bibr" target="#b29">(Loshchilov &amp; Hutter, 2017)</ref> in combination with a cosine annealing learning rate scheduler <ref type="bibr" target="#b28">Loshchilov &amp; Hutter (2016)</ref> and a linear learning rate warm-up stage of 10 epochs.</p><p>Best hyperparameters found. We perform hyperparameter search on the learning rate, dropout rate, weight decay, and ω 0 of our CCNNs for each task considered. <ref type="foot" target="#foot_2">3</ref> The best hyperparameters found are reported in Tables <ref type="table">3</ref> and <ref type="table">4</ref>.</p><p>Parameter efficiency experiments on ModelNet40.</p><p>To further assess parameter efficiency of the CCNN on ModelNet40, we run a number of experiments with smaller model sizes. Results for the following models are shown in Fig. 8: CCNN 4,16 with 4 residual blocks, a channel size of 16 and a kernel network hidden size of 8, with 6,545 total parameters. CCNN 4,32 with 4 residual blocks, a channel size of 32 and a kernel network hidden size of 8, with 14,401 total parameters. CCNN 4,48 with 4 residual blocks, a channel size of 48 and a kernel network hidden size of 8, with 26,353 total parameters. We use a learning rate of 2e -2 , no weight decay, and an ω 0 of 50. D ADDITIONAL EXPERIMENTS AND DETAILS D.1 COMPUTATIONAL EFFICIENCY D.1.1 EFFICIENCY OF CONTINUOUS CONVOLUTIONAL KERNELS</p><p>Experimental setup. We experimentally asses the computational complexity of our model. We measure the time of a single forward/backward pass for the two architectures used in our experiments:  The vertical axis indexes the channels, and the horizontal axis the length of the kernels in a single layer. Interstingly, we observe a clear periodic pattern of 32 steps along the spatial dimension across all layers -a period that corresponds to the width of the underlying 32×32 CIFAR10 images-. This illustrates that CCNNs in fact learn to represent 2D structures on the flattened 1D space on which sequential CIFAR10 is defined. Despite these important capabilities, it is important to note that modelling N D signals as flattened 1D signals poses a unnecessary burden to the model, and modelling the signal in the original 2D space leads to faster convergence and better accuracy (Fig. <ref type="figure" target="#fig_7">9</ref>). We compare to (i) discrete convolutional networks which match in architecture to ours, with global depthwise separable convolutional kernels; Global CNN 4,140 , and Global CNN 6,380 , (ii) more traditional CNN architectures with downsampling after each block (pooling of size 2), local depthwise separable kernels (kernels of size 5), and a depth tuned to the input size such that the receptive field of the final residual block covers the entire input. We take depth = ⌈d⌉ with d given by: input length pooling size d ≥ kernel size. These architectures are denoted Local CNN 4,140 and Local CNN 6,380 . For input lengths of 1024, 4096, 8192 and 16000 this results in depths of 8, 10, 11 and 12 blocks respectively.</p><p>We measure training time (forward and backward pass) for sequence data of different lengths between 1024 -the length of sCIFAR10-, and 16000 -the length of Speech-Raw-, the largest sequence length in our experiments. We measure the time of training over 1000 batches of size 8 on a RTX 3090.</p><p>Results. Results are summarized in Tab 5. Note that for the Global CNN 6,380 , we were unable to complete the experiment on data of lengths exceeding 8192 because of insufficient memory during the backward pass. This highlights another benefit of the continuous convolutional layer: its relative memory efficiency in computing weight updates, which results from the fact that gradients for individual kernel values can be discarded. As only the kernel network weights need to be updated, gradient values for the kernel weights do not need to be stored during the backward pass.</p><p>Our results show that our approach requires additional computation compared to more traditional discrete CNNs. We note that although the use of a kernel network to infer kernel values brings computational overhead, this overhead is not a limiting factor in practice. The global convolutional kernels used in our network architectures have an impact on computational efficiency, but the use of convolutions in the Fourier domain help address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1.2 EFFICIENCY OF CCNNS WITH REGARD TO OTHER GLOBAL METHODS</head><p>Experimental Setup. Next, we benchmark the computational efficiency of CCNNs compared to other input-global methods: S4 <ref type="bibr" target="#b12">(Gu et al., 2022)</ref> and LSSL <ref type="bibr" target="#b11">(Gu et al., 2021)</ref>.</p><p>We reproduce the efficiency benchmark in <ref type="bibr" target="#b12">Gu et al. (2022)</ref>. That is, we provide the runtime for a single layer of forward and backward pass for different hidden dimensionalities, measured for different input lengths. The kernel network used in these experiments is a MAGNet with 3 layers and a hidden dimensionality of 32. Again, we assume a "worst-case" setting in which the learned kernel size equals the input length, i.e. the layer performs convolutions with global kernels. Note that in this setting, we perform convolutions in the spatial domain. We average runtime over 10000 steps. Experiments are performed on a RTX 3090.</p><p>Results. Results are summarized in Tab. 6. We show that, up to a sequence length of 16000, runtimes in comparison with S4 are marginally faster, but remain in the same order of magnitude. Differences in memory utilization become more pronounced for increases in hidden dimensionality and sequence length, but remain a constant factor. Next, note that in both memory and speed, we show that a CCNN </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Discrete and continuous convolutional kernels. Discrete convolutional kernels assign a weight w i out of a discrete set of weights W to a relative offset xx. This ties the kernel to the length, resolution and dimensionality of the input, limiting the general applicability of the CNN architectures. Instead, our Continuous Convolutional Neural Network parameterizes kernel values as a continuous function ϕ Kernel over the input domain R d , which decouples it from data characteristics.</figDesc><graphic coords="1,108.00,596.94,395.98,96.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>(a) Kernel network. (b) D=1, sequences. (c) D=2, images. (d) D=3, volumes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Operation types: global, pointwise and local. Local operations are resolution dependent. Transferring a local operation from (3a) to a lower resolution (3b) leads to an increased receptive field.</figDesc><graphic coords="3,258.72,129.68,62.44,59.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Applying a Continuous Kernel Convolution. Given a pixel-position in the input image x ∈ Ω(f ), we obtain relative offsets to surrounding pixels {x -x} x∈Ω(f ) (4a). Next, we pass each relative position to the kernel generator network in order to generate the kernel: {ϕ Kernel (x -x)} x∈Ω(f ) (4b).Finally, we apply the convolution between the generated convolutional kernel and the input (4c).</figDesc><graphic coords="5,232.05,90.05,170.67,121.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The CCNN architecture.</figDesc><graphic coords="6,340.40,210.01,158.41,134.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Histogram of the output of a CCNN with and without the proposed kernel initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Parameter count versus performance on ModelNet40. Very small CCNN models obtain remarkable results.</figDesc><graphic coords="9,116.26,189.71,171.07,114.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of the CCNN 6,380 on the LRA Image task, where samples are either input as sequence or with the original image structure.</figDesc><graphic coords="9,314.94,183.71,180.58,120.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Positive and negative samples from the Path-X dataset</figDesc><graphic coords="15,315.39,340.21,91.08,91.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: An example of a point-cloud sampled from ModelNet10 (a), and a corresponding voxel representation of the same sample (b).C EXPERIMENTAL DETAILS C.1 GENERAL REMARKSCode repository and logging. Our code is written in PyTorch. We utilize wandb (Biewald, 2020) hydra<ref type="bibr" target="#b63">(Yadan, 2019)</ref> and pytorch-lightning<ref type="bibr" target="#b6">(Falcon et al., 2019)</ref> for logging and code structuring. Our experiments are performed on NVIDIA TITAN RTX, A6000 and A100 GPUs, depending on the size of the datasets and inputs considered. Our code is publicly available at url hidden for the sake of the double blind review process.Normalized relative positions. The kernel network ϕ Kernel can, in principle, receive arbitrary coordinates as input. However, considering unitary step-wise relative positions, i.e., 0, 1, 2, ... , N, can be problematic from a numerical stability perspective as N may grow very large, e.g., N=16000 for the Speech Commands dataset. Consequently, based on insights from the Implicit Neural Representations, e.g., Sitzmann et al. (2020);<ref type="bibr" target="#b7">Fathony et al. (2021)</ref>, we normalize the coordinates such that they lie in the space [-1, 1] D for D-dimensional kernels. To this end, we map largest unitary positions seen during training [0, N ] to a uniform linear space in [-1, 1]. Note that any possible relative kernel positions outside of the trained kernel domain which may be encountered during inference are masked out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Learned (causal) convolutional kernels in a CCNN 4,140 trained on sequential CIFAR10.The vertical axis indexes the channels, and the horizontal axis the length of the kernels in a single layer. Interstingly, we observe a clear periodic pattern of 32 steps along the spatial dimension across all layers -a period that corresponds to the width of the underlying 32×32 CIFAR10 images-. This illustrates that CCNNs in fact learn to represent 2D structures on the flattened 1D space on which sequential CIFAR10 is defined. Despite these important capabilities, it is important to note that modelling N D signals as flattened 1D signals poses a unnecessary burden to the model, and modelling the signal in the original 2D space leads to faster convergence and better accuracy (Fig.9).</figDesc><graphic coords="18,108.00,450.05,395.98,146.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on sequence, image and point-cloud datasets. × -unable to apply, either due to the model's inability to handle the specified data, or to computational complexity. N.A. -not available.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SEQUENCE</cell><cell></cell><cell></cell><cell></cell><cell>IMAGE</cell><cell></cell><cell>POINT-CLOUD</cell></row><row><cell></cell><cell>SIZE</cell><cell>SMNIST</cell><cell>PMNIST</cell><cell>SCIFAR10</cell><cell cols="6">SPEECH-MFCC SPEECH-RAW SPEECH-0.5X CIFAR10 CIFAR100 STL10</cell><cell>ModelNet40</cell></row><row><cell>Transformer</cell><cell>500K</cell><cell>98.90</cell><cell>97.90</cell><cell>62.20</cell><cell>90.75</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>LSSL</cell><cell>7.8M</cell><cell>99.53</cell><cell>98.76</cell><cell>84.65</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>S4</cell><cell>7.8M</cell><cell>99.63</cell><cell>98.70</cell><cell>91.13</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>LSSL</cell><cell>300k</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell><cell>93.58</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>S4</cell><cell>300k</cell><cell>N.A.</cell><cell>N.A.</cell><cell>N.A.</cell><cell>93.96</cell><cell>98.32</cell><cell>96.30</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>CKCNN-Seq</cell><cell>98K</cell><cell>99.31</cell><cell>98.00</cell><cell>62.25</cell><cell>95.30</cell><cell>71.66</cell><cell>65.96</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>CKCNN-Seq-Big</cell><cell>1M</cell><cell>99.32</cell><cell>98.54</cell><cell>63.74</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>FlexTCN-6</cell><cell>375K</cell><cell>99.62</cell><cell>98.63</cell><cell>80.82</cell><cell>97.67</cell><cell>91.73</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>ResNet-44</cell><cell>660K</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>92.90</cell><cell>71.15</cell><cell>NA</cell><cell>×</cell></row><row><cell>ResNet-18</cell><cell>11.2M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>94.92</cell><cell>77.50</cell><cell>81.04</cell><cell>×</cell></row><row><cell>ViT</cell><cell>6.3M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>90.92</cell><cell>66.54</cell><cell>N.A.</cell><cell>×</cell></row><row><cell>Swin-T/1</cell><cell>27.5M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>94.46</cell><cell>78.07</cell><cell>N.A.</cell><cell>×</cell></row><row><cell>Parabolic CNN</cell><cell>502K</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>88.5</cell><cell>64.8</cell><cell>77.0</cell><cell>×</cell></row><row><cell>Hamiltonian CNN</cell><cell>264K</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>89.3</cell><cell>64.9</cell><cell>78.3</cell><cell>×</cell></row><row><cell>CKCNN</cell><cell>630K</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>86.8</cell><cell>N.A.</cell><cell>N.A.</cell><cell>×</cell></row><row><cell>FlexNet-6</cell><cell>670K</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>92.2</cell><cell>N.A.</cell><cell>N.A.</cell><cell>×</cell></row><row><cell>SpiderCNN</cell><cell>N.A.</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>77.97</cell><cell>N.A.</cell><cell>N.A.</cell><cell>92.4</cell></row><row><cell>PointConv</cell><cell>N.A.</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>89.13</cell><cell>N.A.</cell><cell>N.A.</cell><cell>92.5</cell></row><row><cell>DGCNN</cell><cell>1.84M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>89.0</cell></row><row><cell>PointNet</cell><cell>3.48M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>89.2</cell></row><row><cell>PointNet++</cell><cell>1.99M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>90.0</cell></row><row><cell>RepSurf-U</cell><cell>1.48M</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>×</cell><cell>94.7</cell></row><row><cell>CCNN4,140</cell><cell>200K</cell><cell>99.72</cell><cell>98.82</cell><cell>90.30</cell><cell>95.01</cell><cell>98.34</cell><cell>96.22</cell><cell>92.78</cell><cell>66.86</cell><cell>81.80</cell><cell>84.44</cell></row><row><cell>CCNN6,380</cell><cell>2M</cell><cell>99.72</cell><cell>98.84</cell><cell>93.08</cell><cell>97.98</cell><cell>98.44</cell><cell>96.44</cell><cell>95.20</cell><cell>73.16</cell><cell>83.00</cell><cell>85.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on the Long Range Arena benchmark. × -unable to apply due to the model's inability to handle the specified data.</figDesc><table><row><cell></cell><cell cols="7">LISTOPS TEXT IMAGE PATHFINDER AVG. 2DIMAGE 2DPATHFINDER</cell></row><row><cell>Transformer</cell><cell>36.37</cell><cell>64.27</cell><cell>42.44</cell><cell>71.40</cell><cell>53.66</cell><cell>×</cell><cell>×</cell></row><row><cell>Reformer</cell><cell>37.27</cell><cell>56.10</cell><cell>38.07</cell><cell>68.50</cell><cell>50.56</cell><cell>×</cell><cell>×</cell></row><row><cell>BigBird</cell><cell>36.05</cell><cell>64.02</cell><cell>40.83</cell><cell>74.87</cell><cell>57.17</cell><cell>×</cell><cell>×</cell></row><row><cell>Linear Trans.</cell><cell>16.13</cell><cell>65.90</cell><cell>42.34</cell><cell>75.30</cell><cell>50.46</cell><cell>×</cell><cell>×</cell></row><row><cell>Performer</cell><cell>18.01</cell><cell>65.40</cell><cell>42.77</cell><cell>77.05</cell><cell>51.18</cell><cell>×</cell><cell>×</cell></row><row><cell>FNet</cell><cell>35.33</cell><cell>65.11</cell><cell>38.67</cell><cell>77.80</cell><cell>54.52</cell><cell>×</cell><cell>×</cell></row><row><cell>Nystromförmer</cell><cell>37.15</cell><cell>65.52</cell><cell>41.58</cell><cell>70.94</cell><cell>57.46</cell><cell>×</cell><cell>×</cell></row><row><cell>Luna-256</cell><cell>37.25</cell><cell>64.57</cell><cell>47.38</cell><cell>77.72</cell><cell>59.37</cell><cell>×</cell><cell>×</cell></row><row><cell>S4</cell><cell>58.35</cell><cell>76.02</cell><cell>87.26</cell><cell>86.05</cell><cell>80.48</cell><cell>×</cell><cell>×</cell></row><row><cell>CCNN4,140</cell><cell>44.85</cell><cell>83.59</cell><cell>87.62</cell><cell>91.36</cell><cell>76.86</cell><cell>89.48</cell><cell>94.80</cell></row><row><cell>CCNN6,380</cell><cell>43.60</cell><cell>84.08</cell><cell>88.90</cell><cell>91.51</cell><cell>77.02</cell><cell>91.12</cell><cell>96.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Training time (ms). × -unavailable due to memory issues. n -number of blocks tuned per input length.(4 blocks, 140 channels) and CCNN 6,380 (6 blocks, 380 channels). It is important to note that we assume the "worst-case" scenario for CCNNs in which the learned kernel size equals the input length, i.e., the model performs convolutions with global kernels at each layer. FlexConvs(Romero et al., 2022a)  learn the kernel size during training and -as shown exemplarily in Fig12-having global kernels at each layer is not something that we observe in practice.</figDesc><table><row><cell>SIZE</cell><cell cols="4">1024 4096 8192 16000</cell></row><row><cell>Local CNN n,140</cell><cell>24</cell><cell>32</cell><cell>43</cell><cell>65</cell></row><row><cell>Global CNN 4,140</cell><cell>13</cell><cell>25</cell><cell>53</cell><cell>122</cell></row><row><cell>CCNN 4,140</cell><cell>52</cell><cell>56</cell><cell>77</cell><cell>145</cell></row><row><cell>Local CNN n,380</cell><cell>24</cell><cell>38</cell><cell>93</cell><cell>178</cell></row><row><cell>Global CNN 6,380</cell><cell>31</cell><cell>102</cell><cell>220</cell><cell>×</cell></row><row><cell>CCNN 6,380</cell><cell>71</cell><cell>136</cell><cell>255</cell><cell>537</cell></row><row><cell>CCNN 4,140</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 :</head><label>8</label><figDesc>Test accuracy of models with FlexCNNBlock (F-CCNN 4,140 , F-CCNN 6,380 ), models with S4Block (S4-CCNN 4,140 , S4-CCNN 6,380 ) and models with CCNNBlock (CCNN 4,140 , CCNN 6,380 ).</figDesc><table><row><cell></cell><cell cols="3">SPEECHCOMMANDS-MFCC SCIFAR10 CIFAR10</cell></row><row><cell>F-CCNN 4,140</cell><cell>94.38</cell><cell>85.54</cell><cell>86.34</cell></row><row><cell>S4-CCNN 4,140</cell><cell>73.60</cell><cell>53.25</cell><cell>60.06</cell></row><row><cell>CCNN 4,140</cell><cell>95.01</cell><cell>90.30</cell><cell>92.78</cell></row><row><cell>F-CCNN 6,380</cell><cell>94.92</cell><cell>84.68</cell><cell>92.24</cell></row><row><cell>S4-CCNN 6,380</cell><cell>60.82</cell><cell>51.50</cell><cell>67.01</cell></row><row><cell>CCNN 6,380</cell><cell>97.98</cell><cell>93.08</cell><cell>95.20</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Global discrete convolutional kernels could be interpreted as global operations for which parameters along the spatial dimensions of the input are defined. Consequently, these require special treatment (Sec.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>3.3).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>ω0 serves as a prior on the variance of the data that is fitted with several types of implicit neural representations, e.g., SIRENs Sitzmann et al. (2020), MFNsFathony et al. (2021), etc.   </p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2023    <ref type="formula">2022</ref>) (Fig. <ref type="figure">7</ref>).</p><p>To this end, we run experiments on a select number of datasets covering sequence and image data with a version of our architecture that includes the FlexCNNBlock (F-CCNN 4,140 with 4 blocks, 140 channels, 233K params and F-CCNN 6,380 with 6 blocks, 380 channels, 2.24M params), and a version of our architecture that includes the S4Block (S4-CCNN 4,140 with 4 blocks, 140 channels, 200K params and S4-CCNN 6,380 with 6 blocks, 380 channels, 2M params). We compare performance against the two architectures used throughout the experiments in Sec. 5, which uses the CCNNBlock (CCNN 4,140 with 4 blocks, 140 channels, 200K params and CCNN 6,380 with 6 blocks, 380 channels, 2M params). To isolate the impact of the residual block architecture, we replace the FlexConv layers in the original formulation of the FlexCNNBlock with our proposed depthwise separable implementation SepFlexConv. Note that we do not parameter-match these models, which results in the architectures with FlexCNNBlocks having more parameters compared to ones with the CCNNBlock, due to the FlexCNNBlock having two convolutional layers instead of the one convolutional layer and one pointwise linear layer of the CCNNBlock. Training regimes and hyperparameters for all architectures are as per Sec. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. Results are summarized in table 8. First, note that without the nonlinearity added add the end of the block, the S4Block performs poorly over all tasks. Next, as shown, the CCNNBlock formulation improves performance of the architecture compared to FlexCNNBlock, in some cases by a significant margin. Note that architectures with the S4Block and CCNNBlock contain fewer convolutional layers and parameters compared to architecturally matched ones with FlexCNNBlocks, indicating higher parameter-efficiency as well as computational efficiency of CCNNBlocks (e.g. 8h 55m runtime for F-CCNN 6,380 vs. 8h 12m runtime for CCNN 6,380 on sCIFAR10). Because of the shown performance benefits, we use the CCNNBlock throughout our experiments.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">Ossama</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">B-spline cnns on lie groups</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12057</idno>
		<ptr target="https://www.wandb.com/" />
	</analytic>
	<monogr>
		<title level="m">Lukas Biewald. Experiment tracking with weights and biases</title>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Software available from wandb.com</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Principled weight initialization for hypernetworks</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lampros</forename><surname>Flokas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1lma24tPB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><forename type="first">William</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiplicative filter networks</title>
		<author>
			<persName><forename type="first">Rizal</forename><surname>Fathony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OmtmcPkkhT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3165" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=uYLFoz1vlAC" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointwise convolutional neural networks</title>
		<author>
			<persName><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Khoi</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="984" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4651" to="4664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Active convolution: Learning the shape of convolution for image classification</title>
		<author>
			<persName><forename type="first">Yunho</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4201" to="4209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6696" to="6707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>David M Knigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><surname>Bekkers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13059</idno>
		<title level="m">Exploiting redundancy: Separable group convolutional networks on lie groups</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Raw waveform-based audio classification using sample-level cnn architectures</title>
		<author>
			<persName><forename type="first">Jongpil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taejun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00866</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">W</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preey</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Baccus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.06583</idno>
		<title level="m">S4nd: Modeling images and videos as multidimensional signals using state spaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning strides in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Riad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.01653</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning separable filters</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2754" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Group equivariant stand-alone self-attention for vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><surname>Cordonnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00977</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Flexconv: Continuous kernel convolutions with differentiable kernel sizes</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert-Jan</forename><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Mikolaj Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=3jooF27-0Wy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">CKConv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Mikolaj Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=8FhxBtXSl0" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="352" to="364" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">Kristof</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huziel</forename><surname>Enoc Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Rigid-motion scattering for texture classification</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1687</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7462" to="7473" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><surname>Szmaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11093</idno>
		<title level="m">Scale-equivariant steerable networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Disco: accurate discrete scale convolutions</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Moskalev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02733</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Bing</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.11943</idno>
		<title level="m">Log-polar space convolution for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratul</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Fridovich-Keil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithin</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7537" to="7547" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qVyeW-grC2k" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Franc ¸ois Goulette</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep continuous networks</title>
		<author>
			<persName><forename type="first">Nergis</forename><surname>Tomen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia-Laura</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10324" to="10335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSW</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">General e (2)-equivariant steerable cnns</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Daniel E Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Hydra -a framework for elegantly configuring complex applications</title>
		<author>
			<persName><forename type="first">Omry</forename><surname>Yadan</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/hydra" />
	</analytic>
	<monogr>
		<title level="j">Github</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
