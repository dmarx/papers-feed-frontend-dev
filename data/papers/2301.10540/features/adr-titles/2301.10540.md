- Decision to develop a Continuous Convolutional Neural Network (CCNN) architecture
- Choice of continuous convolutional kernels for modeling long-range dependencies
- Decision to unify CNN architecture across different data types (1D, 2D, 3D)
- Selection of kernel parameterization method (e.g., using small neural networks)
- Decision to avoid task-specific downsampling and depth adjustments
- Choice of pointwise operations as invariant to input shape
- Decision to utilize global operations that only define channel-wise parameters
- Selection of FlexConvs parameterized by MAGNets for improved expressivity
- Decision to implement a specific initialization method for kernel parameterization
- Choice to showcase empirical results across multiple tasks to validate CCNN performance
- Decision to make the code publicly available for community use and collaboration
- Choice of related works for comparison and justification of CCNN advantages
- Decision to focus on the generalizability of the architecture across various input modalities
- Choice to analyze the limitations of existing CNN architectures in handling diverse data types
- Decision to emphasize the importance of continuous parameterization in CNN design