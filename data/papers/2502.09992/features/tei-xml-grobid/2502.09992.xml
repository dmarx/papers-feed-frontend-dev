<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Language Diffusion Models</title>
				<funder>
					<orgName type="full">Ministry of Education</orgName>
				</funder>
				<funder ref="#_aGgSCNB">
					<orgName type="full">Major Innovation &amp; Planning Interdisciplinary Platform for the &quot;Double-First Class&quot; Initiative, Renmin University of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder>
					<orgName type="full">Ant Group Research Fund</orgName>
				</funder>
				<funder ref="#_2zBJ35b">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_rsE88pH">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_QMWvBge">
					<orgName type="full">Research Funds of Renmin University of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-18">18 Feb 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shen</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fengqi</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zebin</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName><forename type="first">†</forename><forename type="middle">Xiaolu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingyang</forename><surname>Ou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">Large Language Diffusion Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-18">18 Feb 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">FEDC3BF2D4A5F92CDE3DEB96F488ED07</idno>
					<idno type="arXiv">arXiv:2502.09992v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-28T01:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Autoregressive models (ARMs) are widely regarded as the cornerstone of large language models (LLMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised finetuning (SFT) paradigm. LLaDA models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, LLaDA demonstrates strong scalability, outperforming our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to ARMs, challenging the assumption that key LLM capabilities discussed above are inherently tied to ARMs. Project page and codes: <ref type="url" target="https://ml-gsai.github.io/LLaDA-demo/">https: //ml-gsai.github.io/LLaDA-demo/</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>What is now proved was once only imagined. placeholder,placeholder,placeho-William Blake Large language models (LLMs) <ref type="bibr" target="#b106">(Zhao et al., 2023)</ref> fall entirely within the framework of generative modeling. Specifically, LLMs aim to capture the true but unknown language * Equal contribution † Work done during an internship at Ant Group ‡ Project leaders 1 Gaoling School of Artificial Intelligence, Renmin University of China; Beijing Key Laboratory of Big Data Management and Analysis Methods 2 Ant Group.</p><p>¶ Correspondence to: Chongxuan Li &lt;chongxuanli@ruc.edu.cn&gt;.</p><p>Preprint.</p><formula xml:id="formula_0">00/8 7UXWKIXO4$ $5&amp;&amp; *60. 0DWK +XPDQ(YDO 0%33 &amp;00/8 &amp;(YDO * H Q H U D O 7 D V N V 0 D W K H P D W L F V &amp; R G H &amp; K L Q H V H //D'$%%DVH //D0$%%DVH //D0$%%DVH Figure 1</formula><p>. Zero/Few-Shot Benchmarks. We scale LLaDA to an unprecedented size of 8B parameters from scratch, achieving competitive performance with strong LLMs <ref type="bibr" target="#b27">(Dubey et al., 2024)</ref>.</p><p>distribution p data (•) by optimizing a model distribution p θ (•) through maximum likelihood estimation, or equivalently KL divergence minimization between the two distributions:</p><formula xml:id="formula_1">max θ E pdata(x) log p θ (x) ⇔ min θ KL(p data (x)||p θ (x))</formula><p>Generative modeling principles</p><p>.</p><p>The predominant approach relies on the autoregressive modeling (ARM)-commonly referred to as the next-token prediction paradigm-to define the model distribution:</p><formula xml:id="formula_3">p θ (x) = p θ (x 1 ) L i=2 p θ (x i | x 1 , . . . , x i-1 ) Autoregressive formulation , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where x is a sequence of length L, and x i is the i-th token.</p><p>This paradigm has proven remarkably effective <ref type="bibr" target="#b72">(Radford, 2018;</ref><ref type="bibr" target="#b73">Radford et al., 2019;</ref><ref type="bibr" target="#b13">Brown, 2020;</ref><ref type="bibr" target="#b66">OpenAI, 2022)</ref> and has become the foundation of current LLMs. Despite its widespread adoption, a fundamental question remains unanswered: Is the autoregressive paradigm the only viable path to achieving the intelligence exhibited by LLMs?</p><p>We argue that the answer is not a simple "yes". The key insight overlooked previously is: it is the generative modeling principles (i.e., Eq. ( <ref type="formula" target="#formula_2">1</ref>)), rather than the autoregressive formulation (i.e., Eq. ( <ref type="formula" target="#formula_3">2</ref>)) itself, that fundamentally underpin the essential properties of LLMs, as detailed below. However, certain inherent limitations of LLMs can be directly traced to their autoregressive nature.</p><p>In particular, we argue that scalability is primarily a consequence of the interplay between Transformers <ref type="bibr" target="#b92">(Vaswani, 2017)</ref>, model and data size, and Fisher consistency 1 <ref type="bibr" target="#b28">(Fisher, 1922)</ref> induced by the generative principles in Eq. ( <ref type="formula" target="#formula_2">1</ref>), rather than a unique result of ARM. The success of diffusion transformers <ref type="bibr" target="#b7">(Bao et al., 2023;</ref><ref type="bibr" target="#b71">Peebles &amp; Xie, 2023)</ref> on visual data <ref type="bibr" target="#b12">(Brooks et al., 2024)</ref> supports this claim.</p><p>Furthermore, the capabilities of instruction-following and in-context learning <ref type="bibr" target="#b13">(Brown, 2020)</ref> appear to be intrinsic properties of all proper conditional generative models on structurally consistent linguistic tasks, rather than exclusive advantages of ARMs. In addition, while ARMs can be interpreted as a lossless data compressor <ref type="bibr" target="#b23">(Deletang et al.;</ref><ref type="bibr">Huang et al., 2024b)</ref>, any sufficiently expressive probabilistic model can achieve similar capabilities <ref type="bibr" target="#b80">(Shannon, 1948)</ref>.</p><p>Nevertheless, the autoregressive nature of LLMs presents notable challenges. For example, sequential token-by-token generation incurs high computational costs, and the leftto-right modeling limits effectiveness in reversal reasoning tasks <ref type="bibr" target="#b9">(Berglund et al., 2023)</ref>. These inherent limitations constrain LLMs in handling longer and more complex tasks.</p><p>Motivated by these insights, we introduce LLaDA, a Large Language Diffusion with mAsking, to investigate whether the capabilities exhibited by LLMs can emerge from generative modeling principles beyond ARMs in Eq. ( <ref type="formula" target="#formula_3">2</ref>), thereby addressing the fundamental question posed earlier. In contrast to traditional ARMs, LLaDA leverages a masked diffusion model (MDM) <ref type="bibr">(Austin et al., 2021a;</ref><ref type="bibr">Lou et al., 2023;</ref><ref type="bibr" target="#b82">Shi et al., 2024;</ref><ref type="bibr" target="#b78">Sahoo et al., 2024;</ref><ref type="bibr" target="#b68">Ou et al., 2024)</ref>, which incorporates a discrete random masking process and trains a mask predictor to approximate its reverse process. This design enables LLaDA to construct a model distribution with bidirectional dependencies and optimize a lower bound of its log-likelihood, offering an unexplored and principled alternative to existing LLMs.</p><p>The core of LLaDA is a mask predictor, a parametric model p θ (•|x t ) that takes x t as input and predicts all masked tokens (denoted M) simultaneously. It is trained using a crossentropy loss computed only on the masked tokens:</p><formula xml:id="formula_5">L(θ) ≜ -E t,x0,xt 1 t L i=1 1[x i t = M] log p θ (x i 0 |x t ) ,<label>(3)</label></formula><p>where x 0 is sampled from the training data, t is sampled uniformly from [0, 1], and x t is sampled from the forward process. The indicator function 1[•] ensures that the loss is computed only for masked tokens.</p><p>Once trained, we can simulate a reverse process (see Sec. induced at t = 0. Notably, the loss in Eq. ( <ref type="formula" target="#formula_5">3</ref>) has been proven to be an upper bound on the negative log-likelihood of the model distribution <ref type="bibr" target="#b82">(Shi et al., 2024;</ref><ref type="bibr" target="#b68">Ou et al., 2024)</ref>:</p><formula xml:id="formula_6">-E pdata(x0) [log p θ (x 0 )] ≤ L(θ),<label>(4)</label></formula><p>making it a principled objective for generative modeling.</p><p>Notably, LLaDA employs a masking ratio that varies randomly between 0 and 1 while masked language models (Devlin, 2018) use a fixed ratio. The subtly differences have significant implications, especially at scale: as shown in Eq. ( <ref type="formula" target="#formula_6">4</ref>), LLaDA is a principled generative model with the potential to perform in-context learning naturally, akin to LLMs. Moreover, its generative perspective ensures Fisher consistency <ref type="bibr" target="#b28">(Fisher, 1922)</ref> in extreme cases, suggesting strong scalability with large data and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pre-training</head><p>LLaDA employs a Transformer <ref type="bibr" target="#b92">(Vaswani, 2017)</ref> as the mask predictor, whose architecture is similar to existing LLMs. However, LLaDA does not use a causal mask, as its formulation allows it to see the entire input for predictions.</p><p>We trained two variants of LLaDA with different sizes: 1 billion (B) and 8B. We summarize the model architecture of LLaDA 8B and LLaMA3 8B <ref type="bibr" target="#b27">(Dubey et al., 2024)</ref> here and details are provided in Appendix B.2. We have ensured consistency in most hyperparameters while making several necessary modifications. We use vanilla multi-head attention instead of grouped query attention <ref type="bibr" target="#b1">(Ainslie et al., 2023)</ref> for simplicity, as LLaDA is incompatible with KV caching, resulting in a different number of key and value heads. Consequently, the attention layer has more parameters, and we reduce the FFN dimension to maintain a comparable model size. Additionally, the vocabulary size differs slightly due to a tokenizer <ref type="bibr" target="#b13">(Brown, 2020)</ref> adapted on our data.</p><p>The LLaDA model is pre-trained on a dataset comprising 2.3 trillion (T) tokens, adhering to a data protocol that aligns closely with existing large language models (LLMs) <ref type="bibr" target="#b90">(Touvron et al., 2023;</ref><ref type="bibr" target="#b20">Chu et al., 2024)</ref>, without the incorporation of any special techniques. The data are derived from online corpora, with low-quality content filtered through manually designed rules and LLM-based approaches. Beyond general text, the dataset encompasses high-quality code, math, and multilingual data. The mixing of data sources and domains is guided by scaled-down ARMs. The pre-training process utilizes a fixed sequence length of 4096 tokens, incurring a total computational cost of 0.13 million H800 GPU hours, similar to ARMs of the same scale and dataset size.</p><p>For a training sequence x 0 , we randomly sample t ∈ [0, 1], mask each token independently with the same probability t to obtain x t (see Fig. <ref type="figure" target="#fig_0">2</ref> (a)) and estimate Eq. (3) via the Monte Carlo method for stochastic gradient descent training. In addition, following <ref type="bibr" target="#b65">Nie et al. (2024)</ref>, to enhance the ability of LLaDA to handle variable-length data, we set 1% of the pre-training data to a random length that is uniformly sampled from the range <ref type="bibr">[1,</ref><ref type="bibr">4096]</ref>.</p><p>We adopted the Warmup-Stable-Decay <ref type="bibr" target="#b47">(Hu et al., 2024)</ref> learning rate scheduler to monitor the training progress without interrupting continuous training. Specifically, we linearly increased the learning rate from 0 to 4 × 10 -4 over the first 2000 iterations and maintained it at 4 × 10 -4 . After processing 1.2T tokens, we decayed the learning rate to 1 × 10 -4 and held it constant for the next 0.8T tokens to ensure stable training. Finally, we linearly reduced the learning rate from 1 × 10 -4 to 1 × 10 -5 for the last 0.3T tokens. Furthermore, we utilized the AdamW optimizer <ref type="bibr" target="#b60">(Loshchilov, 2017)</ref> with a weight decay of 0.1, a batch size of 1280, and a local batch size of 4 per GPU. The 8B experiment was executed once, without any hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Supervised Fine-Tuning</head><p>We enhance the capability of LLaDA to follow instructions by supervised fine-tuning (SFT) with paired data (p 0 , r 0 ), where p 0 is the prompt and r 0 denotes the response. This is the simplest and most basic post-training method for LLMs. Technically, this requires to model the conditional distribution p θ (r 0 |p 0 ) instead of p θ (x 0 ) in pre-training.</p><p>The implementation is similar to pre-training. As shown in Fig. <ref type="figure" target="#fig_0">2</ref> (b), we leave the prompt unchanged and mask the tokens in the response independently, as done for x 0 . Then, we feed both the prompt and the masked response r t to the pre-trained mask predictor to compute the loss for SFT:</p><formula xml:id="formula_7">-E t,p0,r0,rt   1 t L ′ i=1 1[r i t = M] log p θ (r i 0 |p 0 , r t )   ,<label>(5)</label></formula><p>where L ′ denotes a dynamic length specified later, and all other notations remain the same as before.</p><p>Note that this approach is fully compatible with pre-training.</p><p>Essentially, the concatenation of p 0 and r 0 can be treated as clean pre-training data x 0 , while the concatenation of p 0 and r t serves as the masked version x t . The process is identical to pre-training, with the only difference being that all masked tokens happen to appear in the r 0 portion.</p><p>The LLaDA 8B model undergoes SFT on a dataset comprising 4.5 million pairs. Consistent with the pre-training process, both data preparation and training follow the SFT protocols utilized in existing LLMs <ref type="bibr" target="#b20">(Chu et al., 2024;</ref><ref type="bibr" target="#b100">Yang et al., 2024)</ref>, without introducing any additional techniques to optimize LLaDA's performance. The dataset spans multiple domains, including code, mathematics, instructionfollowing, and structured data understanding. We append |EOS| tokens to the end of short pairs in each mini-batch to ensure equal lengths across all data. We treat |EOS| as a normal token during training and remove it during sampling, enabling LLaDA to control the response length automatically. Please refer to Appendix B.1 for more details.</p><p>We train for 3 epochs on the SFT data using a similar schedule to the pre-training phase. The learning rate is linearly increased from 0 to 2.5 × 10 -5 over the first 50 iterations and then kept constant. During the final 10% of iterations, it is linearly reduced to 2.5 × 10 -6 . Additionally, we set the weight decay to 0.1, the global batch size to 256, and the local batch size to 2 per GPU. The SFT experiment was executed once, without any hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Inference</head><p>As a generative model, LLaDA is capable of both sampling new text and evaluating the likelihood of candidate text.</p><p>We begin with the sampling. As illustrated in Fig. <ref type="figure" target="#fig_0">2 (c</ref>), given a prompt p 0 , we discretize the reverse process to sample from the model distribution p θ (r 0 |p 0 ), starting from a fully masked response. The total number of sampling steps is a hyperparameter, which naturally provides LLaDA with a trade-off between efficiency and sample quality, as analyzed in Sec. 3.3. We employ uniformly distributed timesteps by default. In addition, the generation length is also treated as a hyperparameter, specifying the length of the fully masked sentence at the beginning of the sampling process. As detailed in Appendix B.4, since both pre-training and SFT are conducted using datasets with variable lengths, the final results are insensitive to this length hyperparameter.</p><p>At an intermediate step from time t ∈ (0, 1] to s ∈ [0, t), we feed both p 0 and r t into the mask predictor and predict all masked tokens simultaneously. Subsequently, we remask s t of the predicted tokens in expectation to obtain r s , ensuring that the transition of the reverse process aligns with the forward process for accurate sampling <ref type="bibr">(Austin et al., 2021a)</ref>.</p><p>In principle, the remasking strategy should be purely random. However, inspired by the annealing tricks of sampling in LLMs <ref type="bibr" target="#b44">(Holtzman et al., 2019;</ref><ref type="bibr" target="#b13">Brown, 2020)</ref>, we explore two deterministic yet effective remasking strategies. Specifically, similarly to <ref type="bibr" target="#b15">Chang et al. (2022)</ref>, we remask the s t of predicted tokens with the lowest confidence based on the predictions, called low-confidence remasking. Additionally, for LLaDA after SFT, we can divide the sequence into several blocks and generate them from left to right, called semi-autoregressive remasking. Within each block, we apply the reverse process to perform sampling. We provide more details and ablation studies in Appendix. B.3.</p><p>For conditional likelihood evaluation, we can naturally utilize the upper bound in Eq. ( <ref type="formula" target="#formula_7">5</ref>). However, we find that the following equivalent form <ref type="bibr" target="#b68">(Ou et al., 2024)</ref> exhibits lower variance and is more stable for evaluation:</p><formula xml:id="formula_8">-E l,r0,r l L l L i=1 1[r i l = M] log p θ (r i 0 |p 0 , r l ) , (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>where l is uniformly sampled from {1, 2, . . . , L}, and r l is obtained by uniformly sampling l tokens from r 0 without replacement for masking. In addition, we employ the unsupervised classifier-free guidance <ref type="bibr" target="#b65">(Nie et al., 2024)</ref>. We refer the readers to more details in Appendix A.2.</p><p>We present the training, sampling, and likelihood evaluation algorithms, along with theoretical details, in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We evaluate the scalability, instruction-following, and incontext learning capabilities of LLaDA on standard benchmarks, followed by analyses and case studies on more controlled datasets to provide a comprehensive assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Scalability of LLaDA on Language Tasks</head><p>We first investigate the scalability of LLaDA on downstream tasks in comparison with the ARM baselines we constructed. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across increasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks. Specifically, at the 1B scale, we ensured that LLaDA and ARM shared the same architecture, data, and all other configurations. At larger scales, we also report results for LLaDA and ARM models of slightly different sizes trained on the same data due to resource limit, as detailed in Appendix B.2. We use the computational cost as a unified scaling metric. For evaluation, we focused on six standard and diverse tasks.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, LLaDA demonstrates impressive scalability, with its overall trend being highly competitive with that of ARM. Notably, in tasks such as MMLU and GSM8K, LLaDA exhibits even stronger scalability. Even on tasks like PIQA, where performance lags, LLaDA narrows the gap with ARMs at larger scales. To account for the significant influence of outliers, we opted not to fit quantitative scaling curves, avoiding potential misinterpretation. Nevertheless, the results clearly demonstrate the scalability of LLaDA. <ref type="bibr" target="#b65">Nie et al. (2024)</ref> suggests that MDM requires 16 times more computation than ARM to achieve the same likelihood. However, there are key differences that make the conclusions of this study more broadly applicable. In particular, likelihood is a relatively indirect metric for downstream task performance, and diffusion optimizes a bound of the likelihood, making it not directly comparable to ARM. Additionally, we extended the scaling range from 10 18 ∼ 10 20 in <ref type="bibr" target="#b65">Nie et al. (2024)</ref> to 10 20 ∼ 10 23 in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Benchmark Results</head><p>To comprehensively evaluate the in-context learning and instruction-following capabilities of LLaDA 8B, we conducted detailed comparisons with existing LLMs <ref type="bibr" target="#b90">(Touvron et al., 2023;</ref><ref type="bibr" target="#b27">Dubey et al., 2024;</ref><ref type="bibr" target="#b20">Chu et al., 2024;</ref><ref type="bibr" target="#b100">Yang et al., 2024;</ref><ref type="bibr" target="#b10">Bi et al., 2024;</ref><ref type="bibr" target="#b50">Jiang et al., 2023)</ref> of similar scale. The selection of tasks and evaluation protocols was aligned with existing studies, encompassing 15 popular benchmarks in general tasks, mathematics, code, and Chinese. Further details are provided in Appendix B.5. For a more direct comparison, we re-evaluated representative LLMs <ref type="bibr" target="#b90">(Touvron et al., 2023;</ref><ref type="bibr" target="#b27">Dubey et al., 2024)</ref> in our implementation.</p><p>As shown in Tab. 1, after pretraining on 2.3T tokens, LLaDA 8B demonstrates remarkable performance, surpassing LLaMA2 7B on nearly all tasks, and is overall competitive with LLaMA3 8B. LLaDA shows advantages in math and Chinese tasks. We conjecture that the strengths stem from the same factors as its relatively weaker performance in some tasks-differences in data quality and distribution, largely due to the closed-source situation of LLM datasets.</p><p>Notably, we have carefully ruled out the possibility of data leakage by taking GSM8K as an example. First, as shown in Fig. <ref type="figure" target="#fig_1">3</ref>, LLaDA outperformed ARM baselines regarding GSM8K. Moreover, the conclusion remains on a fully unseen GSM8K-like task <ref type="bibr" target="#b103">(Ye et al., 2024)</ref> in Appendix B.7. Further, Tab. 2 compares the performance of LLaDA 8B Instruct with existing LLMs. We observed that SFT improved LLaDA's performance on most downstream tasks. A few metrics, such as MMLU, showed declines, and we conjecture may be due to the suboptimal quality of the SFT data. Overall, since we did not perform alignment with reinforcement learning (RL), our results are slightly behind LLaMA3 8B Instruct, though the gaps in many metrics remain small. Notably, even with only SFT, LLaDA demonstrates impres-   Overall, despite the lack of data transparency, we have made every effort to adopt standardized procedures and introduce diverse tasks, we believe they sufficiently demonstrate the extraordinary capabilities of LLaDA, which is the only competitive non-autoregressive model to our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reversal Reasoning and Analyses</head><p>To quantify the reversal reasoning ability of models <ref type="bibr" target="#b9">(Berglund et al., 2023)</ref>, we follow the protocol established in Allen-Zhu &amp; Li (2023). Specifically, we construct a dataset of 496 famous Chinese poem sentence pairs. Given a sentence from a poem, models are tasked with generating the subsequent line (forward) or the preceding line (reversal) without additional fine-tuning. Examples can be found in Appendix B.8. This setting provides a straightforward and more realistic evaluation compared to previous studies <ref type="bibr" target="#b65">(Nie et al., 2024;</ref><ref type="bibr" target="#b53">Kitouni et al., 2024)</ref>.</p><p>As shown in Tab. 3, LLaDA effectively addresses the reversal curse <ref type="bibr" target="#b9">(Berglund et al., 2023)</ref>, demonstrating consistent zero-shot performance across both forward and reversal tasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a significant gap between the two. The results on forward generation confirm that both ARMs are strong, benefiting from significantly larger datasets and greater computational resources than LLaDA. However, LLaDA outperforms both by a large margin in the reversal task.</p><p>We emphasize that we did not design anything special for reversal tasks. Intuitively, LLaDA treats tokens uniformly without inductive bias, leading to balanced performance. See more details in Appendix A.2.</p><p>We also analyze the effect of remasking strategies and sampling steps, as detailed in Appendix B.3 and Appendix B.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Case Studies</head><p>We present samples generated by LLaDA 8B Instruct in Tab. 4, showcasing its instruction-following capabilities. First, the</p><p>table illustrates LLaDA's ability to generate coherent, fluent, and extended text in a non-autoregressive manner. Second, it highlights the model's multi-turn dialogue capability, effectively retaining conversation history and producing contextually appropriate responses across multiple languages. Such chat capabilities of LLaDA are impressive, as it departs from conventional ARMs for the first time, to the best of our knowledge. See more case studies on remasking and reasoning tasks in Appendix B.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Diffusion models <ref type="bibr" target="#b84">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b42">Ho et al., 2020;</ref><ref type="bibr" target="#b85">Song et al., 2020)</ref> have excelled in visual domains but remain unverified for LLMs despite extensive efforts.</p><p>A simple approach is to continuousize text data and apply diffusion models directly <ref type="bibr" target="#b56">(Li et al., 2022;</ref><ref type="bibr" target="#b31">Gong et al., 2022;</ref><ref type="bibr" target="#b37">Han et al., 2022;</ref><ref type="bibr" target="#b86">Strudel et al., 2022;</ref><ref type="bibr" target="#b18">Chen et al., 2022;</ref><ref type="bibr" target="#b26">Dieleman et al., 2022;</ref><ref type="bibr" target="#b77">Richemond et al., 2022;</ref><ref type="bibr" target="#b97">Wu et al., 2023;</ref><ref type="bibr" target="#b63">Mahabadi et al., 2024;</ref><ref type="bibr">Ye et al., 2023b)</ref>. Alternatively, some methods model continuous parameters of discrete distributions instead <ref type="bibr">(Lou &amp; Ermon, 2023;</ref><ref type="bibr" target="#b34">Graves et al., 2023;</ref><ref type="bibr" target="#b58">Lin et al., 2023;</ref><ref type="bibr" target="#b99">Xue et al., 2024)</ref>. However, scalability remains a challenge, as a 1B parameter model requires 64 times the compute of an ARM to achieve comparable performance <ref type="bibr" target="#b35">(Gulrajani &amp; Hashimoto, 2024)</ref>.</p><p>Another approach replaces continuous diffusion with discrete processes featuring new forward and reverse dynamics <ref type="bibr">(Austin et al., 2021a)</ref>, leading to numerous variants <ref type="bibr">(Hoogeboom et al.,</ref> 2021b;a; He et al., 2022; Campbell et al., 2022; Meng et al., 2022; Reid et al., 2022; Sun et al., 2022; Kitouni et al., 2023; Zheng et al., 2023; Chen et al., 2023; Ye et al., 2023a; Gat et al., 2024; Zheng et al., 2024; Sahoo et al., 2024; Shi et al., 2024). Notably, Lou et al. (2023) showed that masked diffusion, as a special case of discrete diffusion, achieves perplexity comparable to or surpassing ARMs at GPT-2 scale. Ou et al. (2024) established fundamental theoretical results, which motivated our model design, training, and inference (see Appendix A). Nie et al. (2024) explored how MDM can be leveraged for language tasks such as question answering at GPT-2 scale. Gong et al. (2024) fine-tune ARMs in the MDM formulation. However, improvements are confined to certain metrics, and it remains unclear whether this approach can yield a foundation model comparable to strong LLMs under a comprehensive evaluation.</p><p>In comparison, this study scales MDM to an unprecedented size of 8B parameters from scratch, achieving performance comparable to leading LLMs such as LLaMA 3.</p><p>Additionally, a parallel line of work on image generation <ref type="bibr" target="#b15">(Chang et al., 2022;</ref><ref type="bibr">2023)</ref> aligns well with the application of MDMs to text data. Moreover, MDMs have also shown promise in domains such as protein generation <ref type="bibr">(Wang et al., 2024b;</ref><ref type="bibr" target="#b80">c)</ref>, where they have achieved promising results. Notably, <ref type="bibr" target="#b54">Kou et al. (2024)</ref>; <ref type="bibr" target="#b98">Xu et al. (2025)</ref> demonstrate the potential of using distillation to accelerate MDMs sampling, further enhancing their efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussion</head><p>In the middle of difficulty lies opportunity. placeholder,placeholder,placeh-Albert Einstein</p><p>We introduce LLaDA, a principled and previously unexplored approach to large language modeling based on diffusion models. LLaDA demonstrates strong capabilities in scalability, in-context learning, and instruction-following, achieving performance comparable to strong LLMs. In addition, LLaDA offers unique advantages such as bidirectional modeling and enhanced robustness, effectively addressing several inherent limitations of existing LLMs. Our findings not only establish diffusion models as a viable and promising alternative but also challenge the prevailing assumption that these essential capabilities are inherently tied to ARMs.</p><p>While promising, the full potential of diffusion models re-mains to be fully explored. Several limitations of this work present significant opportunities for future research.</p><p>Due to computational constraints, direct comparisons between LLaDA and ARMs-such as training on identical datasets-were restricted to a computational budget of less than 10 23 FLOPs. To allocate resources for training the largest possible LLaDA model and showcasing its potential, we were unable to scale the ARM baseline to the same extent. Moreover, no specialized attention mechanisms or position embeddings were designed for LLaDA, nor were any system-level architectural optimizations applied.</p><p>On the inference side, our exploration of guidance mechanisms <ref type="bibr" target="#b25">(Dhariwal &amp; Nichol, 2021;</ref><ref type="bibr" target="#b41">Ho &amp; Salimans, 2022)</ref> remains preliminary, and LLaDA currently exhibits sensitivity to inference hyperparameters. Furthermore, LLaDA has yet to undergo alignment with reinforcement learning <ref type="bibr" target="#b69">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b74">Rafailov et al., 2024)</ref>, which is crucial for improving its performance and alignment with human intent.</p><p>Looking forward, the scale of LLaDA is still smaller than leading counterparts <ref type="bibr" target="#b0">(Achiam et al., 2023;</ref><ref type="bibr" target="#b27">Dubey et al., 2024;</ref><ref type="bibr">Google, 2024;</ref><ref type="bibr" target="#b4">Anthropic, 2024;</ref><ref type="bibr" target="#b100">Yang et al., 2024;</ref><ref type="bibr" target="#b59">Liu et al., 2024)</ref>, highlighting the need for further scaling to fully assess its capabilities. In addition, LLaDA's ability to handle multi-modal data remains unexplored. The impact of LLaDA on prompt tuning techniques <ref type="bibr" target="#b96">(Wei et al., 2022)</ref> and its integration into agent-based systems <ref type="bibr" target="#b70">(Park et al., 2023;</ref><ref type="bibr">Wang et al., 2024a</ref>) is yet to be fully understood. Finally, a systematic investigation into post-training for LLaDA could facilitate the development of O1-like systems <ref type="bibr" target="#b67">(OpenAI, 2024;</ref><ref type="bibr" target="#b36">Guo et al., 2025)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This work challenges the prevailing dominance of autoregressive modeling in LLMs by proposing a diffusion-based alternative. Our findings open new avenues for exploring alternative probabilistic paradigms in natural language processing, with potential applications in conversational AI, code generation, and complex reasoning tasks.</p><p>However, diffusion-based models, like traditional LLMs, raise similar societal concerns. These include the environmental impact of large-scale training, the potential misuse for generating harmful or misleading content, and the amplification of biases present in training data. Addressing these challenges is critical to ensuring the responsible development and deployment of diffusion-based language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large Language Diffusion Models</head><p>Algorithm 1 Pre-training of LLaDA Require: mask predictor p θ , data distribution p data 1: repeat 2:</p><p>x 0 ∼ p data , t ∼ U(0, 1] # with a probability of 1%, the sequence length of x 0 follows U <ref type="bibr">[1,</ref><ref type="bibr">4096]</ref> 3:</p><p>x t ∼ q t|0 (x t |x 0 ) # q t|0 is defined in Eq. ( <ref type="formula" target="#formula_13">7</ref>) p 0 , r 0 ∼ p data , t ∼ U(0, 1] # please refer to Appendix B.1 for details on the SFT data processing.</p><formula xml:id="formula_10">4: Calculate L = -1 t * L L i=1 1[x i t = M] log p θ (x i 0 |x t ) # L</formula><p>3:</p><formula xml:id="formula_11">r t ∼ q t|0 (r t |r 0 )</formula><p># q t|0 is defined in Eq. ( <ref type="formula" target="#formula_13">7</ref>) Masked diffusion models (MDMs) <ref type="bibr">(Austin et al., 2021a;</ref><ref type="bibr">Lou et al., 2023;</ref><ref type="bibr" target="#b68">Ou et al., 2024)</ref> define the model distribution p θ (x 0 ) in a manner distinct from autoregressive models.</p><formula xml:id="formula_12">4: Calculate L = -1 t * L ′ L ′ i=1 1[r i t = M] log p θ (r i 0 |p 0 , r t ) # L ′ is</formula><p>These models introduce a forward process {x t } indexed by a time t ∈ [0, 1]. This process gradually and independently masks all tokens in the sequence x 0 . At time t = 0, the data point x 0 is fully observed with no masks, while for t ∈ (0, 1],</p><p>x t represents latent variables with varying mask ratios in expectation.</p><p>Formally, the conditional distribution of x t given x 0 is defined by a fully factorized form:</p><formula xml:id="formula_13">q t|0 (x t |x 0 ) = L i=1 q t|0 (x i t |x i 0 ),<label>(7)</label></formula><p>where the conditional distribution for each token is given by:</p><formula xml:id="formula_14">q t|0 (x i t |x i 0 ) = 1 -t, x i t = x i 0 , t, x i t = M.<label>(8)</label></formula><p>Here, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of Algorithm 4 Reverse Process of LLaDA Require: mask predictor p θ , prompt p 0 , answer length L, sampling steps N 1: Set r 1 is a fully masked sequence of length L.</p><p>2: for t ← 1 down to 1 N step 1 N do 3:</p><formula xml:id="formula_15">s = t -1 N 4:</formula><p>r 0 = arg max r0 p θ (r 0 |p 0 , r t ) # we employ greedy sampling when predicting masked tokens 5:</p><p>for i ← 1 to L do 6:</p><p>if r t ̸ = M then 7:</p><formula xml:id="formula_16">r i 0 =</formula><p>r i t 8: else 9: With probability s t , r i 0 is set to M 10: end if 11: end for 12:</p><p>r s = r 0 13: end for 14: Return r 0 being masked increasing linearly as t progresses from 0 to 1. At t = 1, all tokens are guaranteed to be masked, meaning that x 1 follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability is analogous to but distinct from, the noise schedule in continuous diffusion models <ref type="bibr" target="#b84">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b42">Ho et al., 2020;</ref><ref type="bibr" target="#b85">Song et al., 2020)</ref>. This linearity is motivated by the assumption that the information in the text is proportional to the number of tokens on average, making it reasonable to lose information linearly during the forward process.</p><p>The forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all tokens <ref type="bibr">(Austin et al., 2021a)</ref>. The reverse process, from time t = 1 to 0, generates new data from sequences of fully masked tokens. The conditional distribution for the reverse process, for 0 ≤ s &lt; t ≤ 1, is factorized as:</p><formula xml:id="formula_17">q s|t (x s |x t ) = L i=1 q s|t (x i s |x t ),<label>(9)</label></formula><p>where the conditional distribution for each token is:</p><formula xml:id="formula_18">q s|t (x i s |x t ) =          1, x i t ̸ = M, x i s = x i t , s t , x i t = M, x i s = M, t-s t q 0|t (x i s |x t ), x i t = M, x i s ̸ = M, 0, otherwise.<label>(10)</label></formula><p>Thus, the key function to estimate is the conditional distribution q 0|t (x i s |x t ), which predicts the original token if it is masked in the input x t . This is analogous to the data prediction form in continuous diffusion models.</p><p>As proven in <ref type="bibr" target="#b68">(Ou et al., 2024)</ref>, an equivalent yet time-free parameterization can be derived as:</p><formula xml:id="formula_19">q 0|t (x i s |x t ) = p data (x i 0 |x UM t ), ∀i such that x i t = M,<label>(11)</label></formula><p>where x UM t denotes the collection of unmasked tokens in x t , which is identical to the corresponding tokens in the original data x 0 since unmasked tokens are solely determined by x 0 and are independent of time t. Intuitively, this implies that estimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is time-invariant. Consequently, the time t need not be provided as input to the parametric model.</p><p>Although the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the mask predictor, a parametric model p θ (•|x t ) (e.g., a Transformer without a causal mask), which takes x t for any t as input and predict all masked tokens simultaneously. Then, we define the model distribution p θ (x 0 ) as follows: starting with x 1 as a sequence of fully masked tokens, we simulate an approximate reverse process parameterized by p θ (•|x t ) from t = 1 to 0. The marginal distribution induced at t = 0 then represents the model distribution p θ (x 0 ). In addition, for n-turn dialogues (p 0 0 , r 0 0 , p 1 0 , r 1 0 , . . . , p n-1 0 , r n-1 0</p><p>), we treat it as n single-turn dialogue pairs, i.e., (p 0 0 , r 0 0 ), (p 0 0 r 0 0 p 1 0 , r 1 0 ), . . . , (p 0 0 r 0 0 p 1 0 r 1 0 . . . p n-1 0 , r n-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0</head><p>) and randomly sample one. This data partitioning strategy not only equips LLaDA with multi-turn dialogue capabilities but also aligns with the padding strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Architectures and Scaling Experiments</head><p>This section provides detailed information about the models we trained and the scalability experiments described in Sec. 3.1.</p><p>Firstly, for efficiency, we trained an ARM and a MDM, both with 1.5B parameters and identical architectures. Additionally, we scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive model with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison. These four models are utilized in the scalability analysis in Sec. 3.1. We adopted a Transformer architecture similar to LLaMA <ref type="bibr" target="#b90">(Touvron et al., 2023;</ref><ref type="bibr" target="#b27">Dubey et al., 2024)</ref> for the ARMs and MDMs we trained. Specifically, we employ RMSNorm <ref type="bibr" target="#b105">(Zhang &amp; Sennrich, 2019)</ref> to stabilize training, used SwiGLU <ref type="bibr" target="#b81">(Shazeer, 2020)</ref> as the activation function to enhance non-linearity, and integrate RoPE <ref type="bibr" target="#b87">(Su et al., 2024)</ref> for more expressive positional encoding. Tab. 5 provides an overview of the model architectures.</p><p>Additionally, we employ the widely used 6N D formulation <ref type="bibr" target="#b51">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b43">Hoffmann et al., 2022)</ref> to calculate the training FLOPs in Fig. <ref type="figure" target="#fig_1">3</ref>, where N represents the number of non-embedding parameters, and D denotes the total number of training tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details and Ablation on Remasking</head><p>This section first explains the inference methods in detail, followed by ablation studies on various remasking strategies.</p><p>For instruct model, since our training includes a large number of |EOS| tokens, directly applying the lowest confidence remasking strategy would result in an excessively high and unnatural occurrence of |EOS| tokens in the generated sentences. Therefore, we divide the entire generation length into several blocks. Each block is generated from left to right, and within each block, we apply the reverse process to perform sampling. However, for the base model, since |EOS| tokens are not padded during training, using the semi-autoregressive remasking strategy makes little difference in the results. For efficiency, we set the answer length to 512 and the number of sampling steps to 256 in our ablation studies. For the semi-autoregressive remasking strategy, we set the block length to 32. To simplify the evaluation, we use GSM8K accuracy as the performance metric. As shown in Table <ref type="table" target="#tab_7">6</ref>, for the base model, applying the lowest confidence remasking strategy significantly improves performance, whereas the semi-autoregressive remasking strategy does not yield any noticeable benefits. In contrast, for the instruct model, directly applying the lowest confidence remasking strategy leads to a substantial performance drop. However, combining the lowest confidence remasking strategy with the semi-autoregressive remasking strategy results in the best performance for the instruct model.</p><p>We provide the specific remasking strategies and hyperparameters used for each benchmark in Appendix B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Ablation on Generated Length</head><p>In this section, we conduct ablation studies on the generated length. For simplicity, we use GSM8K as the evaluation metric.</p><p>To ensure efficiency and fairness, for each setting, we set the number of sampling steps to half of the generated length, ensuring that in each sampling step, two tokens are transferred from the mask to the text.</p><p>For the LLaDA 8B Base model, we employ the lowest-confidence remasking strategy (i.e., Algorithm 5). For the LLaDA 8B Instruct model, we adopt semi-autoregressive sampling with a block length of 32.</p><p>As reported in Tab. 7, both the Base model and the Instruct model exhibit minimal sensitivity to the length hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Standard Benchmarks and Evaluation</head><p>In this section, we briefly introduce the benchmarks used in our study and describe the details of our evaluation process.</p><p>Following standard LLM <ref type="bibr" target="#b20">(Chu et al., 2024;</ref><ref type="bibr" target="#b100">Yang et al., 2024)</ref> evaluation practices, we assess LLaDA across four dimensions:</p><p>General ability: MMLU <ref type="bibr" target="#b39">(Hendrycks et al., 2020)</ref>, BBH <ref type="bibr" target="#b89">(Suzgun et al., 2022)</ref>, ARC-C <ref type="bibr" target="#b21">(Clark et al., 2018)</ref>, Hellaswag <ref type="bibr" target="#b104">(Zellers et al., 2019)</ref>, TruthfulQA <ref type="bibr" target="#b57">(Lin et al., 2021)</ref>, WinoGrande <ref type="bibr" target="#b79">(Sakaguchi et al., 2021)</ref> and PIQA <ref type="bibr" target="#b11">(Bisk et al., 2020)</ref>.</p><p>Math and science ability: GSM8K <ref type="bibr" target="#b22">(Cobbe et al., 2021)</ref>, Math <ref type="bibr" target="#b40">(Hendrycks et al., 2021)</ref> and GPQA <ref type="bibr" target="#b76">(Rein et al., 2023)</ref>.</p><p>Code generation: HumanEval <ref type="bibr" target="#b17">(Chen et al., 2021)</ref>, HumanEval-FIM <ref type="bibr" target="#b8">(Bavarian et al., 2022)</ref> and MBPP <ref type="bibr">(Austin et al., 2021b)</ref>.</p><p>Chinese understanding: CMMLU <ref type="bibr" target="#b55">(Li et al., 2023)</ref> and C-Eval <ref type="bibr">(Huang et al., 2024a)</ref>.</p><p>For all the aforementioned benchmarks, we follow the widely adopted evaluation process <ref type="bibr">(Gao et al., 2024)</ref> used in LLM assessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain benchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate's Table <ref type="table">8</ref>. Inference Configuration for LLaDA 8B Instruct. We apply the low-confidence remasking strategy across all datasets. A block length smaller than the answer length indicates the use of the semi-autoregressive remasking strategy; otherwise, it is not applied. The shorter answer lengths for MMLU and HellaSwag result from their requirement to generate only a single token (i.e., A, B, C, or D) and two special end tokens (i.e., end of the model response and end of the sentence).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A Conceptual Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio t ∼ U [0, 1]. (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from t = 1 (fully masked) to t = 0 (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across increasing computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 4 .</head><label>4</label><figDesc>Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.Sampling ProcessUserLily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours? LLaDA Multi-round Dialogue Case User Could you please share with me what the initial two lines of the renowned poem 'The Road Not Taken'? LLaDA Certainly! The initial two lines of the renowned poem "The Road Not Taken" by Robert Frost are: "Two roads diverged in a yellow wood, And sorry I could not travel both." These lines set the stage for the poem, introducing the idea of a speaker facing a choice between two paths. User Please help me translate into Chinese. LLaDA Sure! The first two lines of "The Road Not Taken" by Robert Frost can be translated into Chinese as: "两条路分岔在黄色的树林中，遗憾我不能同时走" User Please translate into German. LLaDA Sure! The first two lines of "The Road Not Taken" by Robert Frost can be translated into German as: "Zwei Wege trennten sich im gelben Wald, und leider konnte ich nicht beide tun." User Please also write a poem about life choices. I require 5 sentences, and the first word of each sentence starts with C as the first letter. LLaDA Certainly! Here's a poem about life choices, with each sentence starting with the letter "C": Choices cascade through life's journey, Creating connections with each decision, Careful consideration leads to better paths, Courageous choices shape our destiny, Choosing wisely guides us through the years.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>is the sequence length of x 0 5: Calculate ∇ θ L and run optimizer. 6: until Converged 7: Return p θ Algorithm 2 Supervised Fine-Tuning of LLaDA Require: mask predictor p θ , pair data distribution p data 1: repeat 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>the sequence length of r 0 5: Calculate ∇ θ L and run optimizer. 6: until Converged 7: Return p θ Algorithm 3 Conditional Log-likelihood Evaluation of LLaDA Require: mask predictor p θ , prompt p 0 , response r 0 , the number of Monte Carlo estimations n mc 1: log likelihood = 0 2: for i ← 1 to n mc do 3: l ∼ {1, 2, . . . , L} # L is the sequence length of r 0 4: Obtain r l by uniformly sampling l tokens from r 0 without replacement for masking 5: log likelihood = log likelihood + L l L i=1 1[r i l = M] log p θ (r i 0 |p 0 , r l ) 6: end for 7: log likelihood = log likelihood/n mc 8: Return log likelihood A. Formulation of Masked Diffusion Models A.1. Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Benchmark Results of Pre-trained LLMs. * indicates that LLaDA 8B Base, LLaMA2 7B Base, and LLaMA3 8B Base are evaluated under the same protocol, detailed in Appendix B.5. Results indicated by † and ¶ are sourced from Chu et al. (2024); Yang et al. (2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for evaluation. "-" indicates unknown data.LLaDA 8B * LLaMA3 8B * LLaMA2 7B * Qwen2 7B † Qwen2.5 7B † Mistral 7B † Deepseek 7B ¶</figDesc><table><row><cell>Model</cell><cell>Diffusion</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell></row><row><cell>Training tokens</cell><cell>2.3T</cell><cell>15T</cell><cell>2T</cell><cell>7T</cell><cell>18T</cell><cell>-</cell><cell>2T</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">General Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMLU</cell><cell>65.9 (5)</cell><cell>65.4 (5)</cell><cell>45.9 (5)</cell><cell>70.3 (5)</cell><cell>74.2 (5)</cell><cell>64.2 (5)</cell><cell>48.2 (5)</cell></row><row><cell>BBH</cell><cell>49.8 (3)</cell><cell>57.6 (3)</cell><cell>37.3 (3)</cell><cell>62.3 (3)</cell><cell>70.4 (3)</cell><cell>56.1 (3)</cell><cell>39.5 (3)</cell></row><row><cell>ARC-C</cell><cell>47.9 (0)</cell><cell>53.1 (0)</cell><cell>46.3 (0)</cell><cell>60.6 (25)</cell><cell>63.7 (25)</cell><cell>60.0 (25)</cell><cell>48.1 (0)</cell></row><row><cell>Hellaswag</cell><cell>72.5 (0)</cell><cell>79.1 (0)</cell><cell>76.0 (0)</cell><cell>80.7 (10)</cell><cell>80.2 (10)</cell><cell>83.3 (10)</cell><cell>75.4 (0)</cell></row><row><cell>TruthfulQA</cell><cell>46.4 (0)</cell><cell>44.0 (0)</cell><cell>39.0 (0)</cell><cell>54.2 (0)</cell><cell>56.4 (0)</cell><cell>42.2 (0)</cell><cell>-</cell></row><row><cell>WinoGrande</cell><cell>74.8 (5)</cell><cell>77.3 (5)</cell><cell>72.5 (5)</cell><cell>77.0 (5)</cell><cell>75.9 (5)</cell><cell>78.4 (5)</cell><cell>70.5 (0)</cell></row><row><cell>PIQA</cell><cell>74.4 (0)</cell><cell>80.6 (0)</cell><cell>79.1 (0)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>79.2 (0)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mathematics &amp; Science</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSM8K</cell><cell>70.7 (4)</cell><cell>53.1 (4)</cell><cell>14.3 (4)</cell><cell>80.2 (4)</cell><cell>85.4 (4)</cell><cell>36.2 (4)</cell><cell>17.4 (8)</cell></row><row><cell>Math</cell><cell>27.3 (4)</cell><cell>15.1 (4)</cell><cell>3.2 (4)</cell><cell>43.5 (4)</cell><cell>49.8 (4)</cell><cell>10.2 (4)</cell><cell>6.0 (4)</cell></row><row><cell>GPQA</cell><cell>26.1 (5)</cell><cell>25.9 (5)</cell><cell>25.7 (5)</cell><cell>30.8 (5)</cell><cell>36.4 (5)</cell><cell>24.7 (5)</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Code</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HumanEval</cell><cell>33.5 (0)</cell><cell>34.2 (0)</cell><cell>12.8 (0)</cell><cell>51.2 (0)</cell><cell>57.9 (0)</cell><cell>29.3 (0)</cell><cell>26.2 (0)</cell></row><row><cell>HumanEval-FIM</cell><cell>73.8 (2)</cell><cell>73.3 (2)</cell><cell>26.9 (2)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MBPP</cell><cell>38.2 (4)</cell><cell>47.4 (4)</cell><cell>18.4 (4)</cell><cell>64.2 (0)</cell><cell>74.9 (0)</cell><cell>51.1 (0)</cell><cell>39.0 (3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chinese</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CMMLU</cell><cell>69.9 (5)</cell><cell>50.7 (5)</cell><cell>32.5 (5)</cell><cell>83.9 (5)</cell><cell>-</cell><cell>-</cell><cell>47.2 (5)</cell></row><row><cell>C-Eval</cell><cell>70.5 (5)</cell><cell>51.7 (5)</cell><cell>34.0 (5)</cell><cell>83.2 (5)</cell><cell>-</cell><cell>-</cell><cell>45.0 (5)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure while other models have extra reinforcement learning (RL) alignment.Qwen2 7B † Qwen2.5 7B † Gemma2 9B † Deepseek 7B ¶</figDesc><table><row><cell>Model</cell><cell>Diffusion</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell><cell>AR</cell></row><row><cell>Training tokens</cell><cell>2.3T</cell><cell>15T</cell><cell>2T</cell><cell>7T</cell><cell>18T</cell><cell>8T</cell><cell>2T</cell></row><row><cell>Post-training</cell><cell>SFT</cell><cell>SFT+RL</cell><cell>SFT+RL</cell><cell>SFT+RL</cell><cell>SFT+RL</cell><cell>SFT+RL</cell><cell>SFT+RL</cell></row><row><cell>Alignment pairs</cell><cell>4.5M</cell><cell>-</cell><cell>-</cell><cell>0.5M + -</cell><cell>1M + 0.15M</cell><cell>-</cell><cell>1.5M + -</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">General Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMLU</cell><cell>65.5 (5)</cell><cell>68.4 (5)</cell><cell>44.1 (5)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.4 (0)</cell></row><row><cell>MMLU-pro</cell><cell>37.0 (0)</cell><cell>41.9 (0)</cell><cell>4.6 (0)</cell><cell>44.1 (5)</cell><cell>56.3 (5)</cell><cell>52.1 (5)</cell><cell>-</cell></row><row><cell>Hellaswag</cell><cell>74.6 (0)</cell><cell>75.5 (0)</cell><cell>51.5 (0)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>68.5 (-)</cell></row><row><cell>ARC-C</cell><cell>88.5 (0)</cell><cell>82.4 (0)</cell><cell>57.3 (0)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>49.4 (-)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Mathematics &amp; Science</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GSM8K</cell><cell>78.6 (4)</cell><cell>78.3 (4)</cell><cell>29.0 (4)</cell><cell>85.7 (0)</cell><cell>91.6 (0)</cell><cell>76.7 (0)</cell><cell>63.0 (0)</cell></row><row><cell>Math</cell><cell>26.6 (0)</cell><cell>29.6 (0)</cell><cell>3.8 (0)</cell><cell>52.9 (0)</cell><cell>75.5 (0)</cell><cell>44.3 (0)</cell><cell>15.8 (0)</cell></row><row><cell>GPQA</cell><cell>31.8 (5)</cell><cell>31.9 (5)</cell><cell>28.4 (5)</cell><cell>34.3 (0)</cell><cell>36.4 (0)</cell><cell>32.8 (0)</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Code</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HumanEval</cell><cell>47.6 (0)</cell><cell>59.8 (0)</cell><cell>16.5 (0)</cell><cell>79.9 (0)</cell><cell>84.8 (0)</cell><cell>68.9 (0)</cell><cell>48.2 (-)</cell></row><row><cell>MBPP</cell><cell>34.2 (4)</cell><cell>57.6 (4)</cell><cell>20.6 (4)</cell><cell>67.2 (0)</cell><cell>79.2 (0)</cell><cell>74.9 (0)</cell><cell>35.2 (-)</cell></row></table><note><p>* indicates that LLaDA 8B Instruct, LLaMA2 7B Instruct, and LLaMA3 8B Instruct are evaluated under the same protocol, detailed in Appendix B.5. Results indicated by † and ¶ are sourced from Yang et al. (2024) and Bi et al. (2024) respectively. The numbers in parentheses represent the number of shots used for in-context learning. "-" indicates unknown data. LLaDA 8B * LLaMA3 8B * LLaMA2 7B *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison in the Poem Completion Task.</figDesc><table><row><cell></cell><cell cols="2">Forward Reversal</cell></row><row><cell>GPT-4o (2024-08-06)</cell><cell>82.7</cell><cell>34.3</cell></row><row><cell>Qwen2.5 7B Instruct</cell><cell>75.9</cell><cell>38.0</cell></row><row><cell>LLaDA 8B Instruct</cell><cell>48.8</cell><cell>42.4</cell></row><row><cell cols="3">sive instruction-following abilities, as detailed in Sec. 3.4.</cell></row><row><cell cols="3">We leave RL-based alignment for future work.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Model Architecture. We provide the configurations of our 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA and the 8B LLaMA3 models.A dynamic sequence length is employed during supervised SFT, where |EOS| tokens are appended to the end of short pairs to ensure equal sequence lengths across all data in each mini-batch. Notably, the padding |EOS| tokens are treated as part of the response, i.e., masked and included in the training objective. The |EOS| tokens are removed from the generated outputs during sampling. This strategy is crucial for LLaDA and ensures that the model learns to control the length of its responses by generating |EOS|, enabling the response length to align effectively with the given prompt.</figDesc><table><row><cell></cell><cell cols="5">Our ARM Baseline 1B LLaDA 1B Our ARM Baseline 7B LLaDA 8B LLaMA3 8B</cell></row><row><cell>Layers</cell><cell>22</cell><cell>22</cell><cell>28</cell><cell>32</cell><cell>32</cell></row><row><cell>Model dimension</cell><cell>2048</cell><cell>2048</cell><cell>4096</cell><cell>4096</cell><cell>4096</cell></row><row><cell>Attention heads</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell>Vocabulary size</cell><cell>126,464</cell><cell>126,464</cell><cell>126,464</cell><cell>126,464</cell><cell>128,000</cell></row><row><cell>FFN dimension</cell><cell>5634</cell><cell>5634</cell><cell>13,440</cell><cell>12,288</cell><cell>14,336</cell></row><row><cell>Key/Value heads</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>32</cell><cell>8</cell></row><row><cell>Total parameters</cell><cell>1.49 B</cell><cell>1.49 B</cell><cell>6.83 B</cell><cell>8.02 B</cell><cell>8.03 B</cell></row><row><cell>Non-embedding parameters</cell><cell>0.97 B</cell><cell>0.97 B</cell><cell>5.80 B</cell><cell>6.98 B</cell><cell>6.98 B</cell></row><row><cell>B. Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>B.1. Details of SFT Data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Ablation on Remasking. We use GSM8K accuracy as the evaluation metric. The lowest-confidence &amp; semi-autoregressive remasking strategy refers to generating each block from left to right, while applying the lowest-confidence remasking strategy within each block.</figDesc><table><row><cell>LLaDA 8B Base LLaDA 8B Instruct</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Ablation on Generated Length. We use the GSM8K accuracy as the evaluation metric and set the number of sampling steps to half of the generated length.</figDesc><table><row><cell cols="3">Length LLaDA 8B Base LLaDA 8B Instruct</cell></row><row><cell>256</cell><cell>62.5</cell><cell>75.3</cell></row><row><cell>512</cell><cell>64.7</cell><cell>73.8</cell></row><row><cell>1024</cell><cell>65.9</cell><cell>75.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Here, we focus on the approach of LLaDA. A rigorous formulation of MDM is provided in Appendix A for interested readers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">92470118</rs>); <rs type="funder">Beijing Natural Science Foundation</rs> (No. <rs type="grantNumber">L247030</rs>); <rs type="programName">Beijing Nova Program</rs> (No. <rs type="grantNumber">20220484044</rs>); <rs type="funder">Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China</rs>; the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>, the <rs type="funder">Research Funds of Renmin University of China</rs> (<rs type="grantNumber">22XNKJ13</rs>); <rs type="funder">Ant Group Research Fund</rs>. The work was partially done at the <rs type="institution">Engineering Research Center of Next-Generation Intelligent Search and Recommendation</rs>, <rs type="funder">Ministry of Education</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rsE88pH">
					<idno type="grant-number">92470118</idno>
				</org>
				<org type="funding" xml:id="_2zBJ35b">
					<idno type="grant-number">L247030</idno>
					<orgName type="program" subtype="full">Beijing Nova Program</orgName>
				</org>
				<org type="funding" xml:id="_aGgSCNB">
					<idno type="grant-number">20220484044</idno>
				</org>
				<org type="funding" xml:id="_QMWvBge">
					<idno type="grant-number">22XNKJ13</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Algorithm 5 Low-confidence Remasking Strategy of LLaDA Require: mask predictor p θ , prompt p 0 , answer length L, sampling steps N 1: Set r 1 is a fully masked sequence of length L.</p><p>2: for t ← 1 down to 1 N step 1 N do 3:</p><p>for i ← 1 to L do 5:</p><p>if r i t ̸ = M then 6:</p><p>else 8:</p><p>r i 0 = arg max r i 0 p θ (r i 0 |p 0 , r t )</p><p>9: n un = ⌊L(1 -s)⌋ # the number of unmasked tokens is n un in timestep s 13:</p><p>for i ← 1 to L do 14:</p><p>r i 0 = M # the n un positions with the least confidence are selected for remasking.</p><p>16:</p><p>end if 17:</p><p>end for 18:</p><p>r s = r 0 19: end for 20: Return r 0 Formally, the mask predictor is trained using a cross-entropy loss with masking:</p><p>where x 0 is sampled from the training data, t is sampled uniformly from [0, 1], and x t is sampled from q t|0 (x t |x 0 ). The indicator function 1 <ref type="bibr">[•]</ref> ensures that the cross-entropy loss is computed only for masked tokens. In <ref type="bibr" target="#b68">Ou et al. (2024)</ref>, it has been proven that the loss function L(θ) is an upper bound on the negative log-likelihood of the model distribution:</p><p>-E x0∼p data(x 0 ) [log p θ (x 0 )] ≤ L(θ).</p><p>In summary, this principled approach trains a generative model by progressively masking tokens during a forward process and learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood estimation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Inference</head><p>The cross-entropy loss in Eq. ( <ref type="formula">12</ref>) has several equivalent forms <ref type="bibr" target="#b68">(Ou et al., 2024)</ref>. The first one is given by</p><p>where l is uniformly sampled from {1, 2, . . . , L}, and x l is obtained by uniformly sampling l tokens from x 0 without replacement for masking. Despite masking exactly l tokens is different from masking each token independently with probability t, these two masking methods lead to equivalent results in expectation <ref type="bibr" target="#b68">(Ou et al., 2024)</ref>.</p><p>While Eq. ( <ref type="formula">12</ref>) and Eq. ( <ref type="formula">14</ref>) share the same expectation, their variances differ. Intuitively, in Eq. ( <ref type="formula">12</ref>), we expect x t to have a fraction of t tokens masked. However, the randomness of the forward process (i.e., Eq. ( <ref type="formula">7</ref>)) often causes deviations, especially when x t contains few tokens. In contrast, in Eq. ( <ref type="formula">14</ref>), the fraction of masked tokens in x l is deterministically l L . While a theoretical analysis depends on the data distribution, empirical results show that Eq. ( <ref type="formula">12</ref>) requires over 1000 Monte Carlo estimates for stable results, whereas Eq. ( <ref type="formula">14</ref>) achieves stability with only 128 estimates. In addition, we can simply modify Eq. ( <ref type="formula">14</ref>) to its conditional version (i.e., Eq. ( <ref type="formula">6</ref>)) based on Eq. ( <ref type="formula">5</ref>). Any-order autoregressive models (AO-ARM) <ref type="bibr" target="#b91">(Uria et al., 2014;</ref><ref type="bibr">Hoogeboom et al., 2021a;</ref><ref type="bibr" target="#b83">Shih et al., 2022)</ref> characterize the joint distribution autoregressively for all possible orders π of the L variables. To learn such a distribution, an AO-ARM utilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent variables. During training, the expected negative log-likelihood over the uniform distribution of all orders U π is minimized:</p><p>Intuitively, x π(&lt;i) 0 can be understood as a masked token x t with index in π(≥ i) being masked. It can be further proved that Eq. ( <ref type="formula">15</ref>) is equivalent to Eq. ( <ref type="formula">12</ref>). This connection explains the bidirectional reasoning capabilities of LLaDA, even though it was never used explicitly in the inference procedure. <ref type="bibr" target="#b65">Nie et al. (2024)</ref> introduce unsupervised classifier-free guidance, a plug-and-play technique that balances alignment with prompts and text diversity. Specifically, unsupervised classifier-free guidance employs the following modified mask predictor for inference:</p><p>where m is a mask sequence of the same length as p 0 and w is a hyperparameter that controls the strength of p 0 . We employ unsupervised classifier-free guidance in the downstream tasks, as detailed in Appendix B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Algorithms</head><p>In this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised fine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is provided in Algorithm 3. Finally, we present the sampling algorithms in Algorithm 4 and Algorithm 5, which correspond to the original reverse process (i.e., random remasking) and the low-confidence remasking strategy <ref type="bibr" target="#b15">(Chang et al., 2022)</ref> of LLaDA, respectively.</p><p>Furthermore, we apply a semi-autoregressive remasking strategy for the instruct model. Specifically, the sequence is divided into several blocks and generated from left to right. Within each block, we apply the random remasking or the low-confidence remasking strategy for sampling. For clarity and better visualization, we illustrate the semi-autoregressive sampling process in Fig. <ref type="figure">4</ref>.</p><p>Our semi-autoregressive remasking strategy is inspired by the prior work of <ref type="bibr" target="#b3">Anonymous (2025)</ref>. However, it is important to note that <ref type="bibr" target="#b3">Anonymous (2025)</ref>  conditional likelihood. The candidate with the highest likelihood is then selected as the model's final answer, and accuracy is used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt, and performance is evaluated using metrics such as exact match and other relevant criteria.</p><p>For the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the instruct model, we evaluate all benchmarks using conditional generation. For datasets evaluated with conditional likelihood estimation, we adopt the widely used lm-evaluation-harness <ref type="bibr">(Gao et al., 2024)</ref> framework. However, for datasets evaluated with conditional generation, we rely on an internal library for evaluation, as lm-evaluation-harness lacks implementations for certain key metrics (e.g., HumanEval-FIM).</p><p>For benchmarks evaluated using conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. ( <ref type="formula">6</ref>) for conditional likelihood estimation. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a single Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo samples are adequate to produce stable results. For ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, we report the results with the optimal unsupervised classifier-free guidance scale searched in {0, 0.5, 1, 1.5, 2}.</p><p>For benchmarks evaluated using conditional generation, we apply the lowest confidence remasking strategy to the base model, setting both the response length and the number of sampling steps to 1024. For the instruct model, we set the sampling steps equal to the answer length and provide the inference hyperparameters in Table <ref type="table">8</ref>. Due to computational resource constraints, we do not use the unsupervised classifier-free guidance for these benchmarks evaluated using conditional generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Analysis of Sampling Steps</head><p>In this section, we analyze the impact of sampling steps. For simplicity, we use the LLaDA 8B Base model and evaluate it on the GSM8K and HumanEval datasets. Additionally, we fix the answer length at 1024. As shown in Fig. <ref type="figure">5</ref>, as the number of sampling steps increases to match the answer length, the model's accuracy improves accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7. Evaluation on iGSM Dataset</head><p>To further assess the mathematical capabilities of LLaDA, we test its performance on iGSM <ref type="bibr" target="#b103">(Ye et al., 2024)</ref>, an infinite, synthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems (i.e., the number of solution steps). For evaluation consistency, we append "#### $answer" to the final solution, adhering to Since there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow <ref type="bibr" target="#b103">(Ye et al., 2024</ref>) and provide a system prompt along with four-shot question-answer pairs for each problem.</p><p>(Prompt) You're an expert at solving elementary math problems involving addition, subtraction, and multiplication. You solve all the problems in a uniform format. All calculations are done modulo 5. For example, 4 + 4 equals 3, 2 + 4 equals 1, 3 + 3 + 3 equals 4, 3 * 3 equals 4, and 2 * 2 equals 4. When providing your solution, please end with '#### x.' where x is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution format. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each school may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries. We can specify quantities, such as ẗhe number of dance studios at each Lakeshore High. Ässume that every entity with the same name has an identical configuration; for example, each Lakeshore High contains the same number of dance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at Lakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore High is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).</p><p>For solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in Table <ref type="table">9</ref>. As shown in the table, LLaDA Base demonstrates significant and consistent advantages over LLaMA3 on unseen mathematical problems, aligning with the results in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8. Poem Completion Tasks</head><p>In this section, we present examples from our poem completion dataset as follows.</p><p>Example 1: Prompt: 窈窕淑女的下一句是什么？直接输出句子即可。 Answer: 君子好逑。 Large Language Diffusion Models Table <ref type="table">10</ref>. Visualization of the Semi-Autoregressive Sampling Process. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>What are the benefits of regular exercise for physical and mental health? LLaDA Table <ref type="table">11</ref>. Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User</head><p>I want to have a pet, but I don't know whether to get a cat or a dog. Do you have any suggestions? LLaDA User I'm very busy at work, so it seems like a cat would be a more suitable option. Do you have any suggestions for getting a cat? LLaDA Example 2: Prompt: 不拘一格降人才的上一句是什么？直接输出句子即可。 Answer: 我劝天公重抖擞。</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.9. More Case Studies</head><p>In this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 10 illustrates the sampling process of semi-autoregressive remasking, while Tab. 11 depicts the sampling process for multi-turn dialogues with random remasking. Additionally, Tab. 12 and Tab. 13 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 14 presents examples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Daily life</head><p>User What's a piece of advice you've received that you've found helpful? LLaDA One helpful piece of advice I've received is to always be open to learning and growth. I've found that being willing to explore new ideas and experiences has helped me become more resilient and adaptable. By embracing a growth mindset, I've been able to identify opportunities for improvement and development, both personally and professionally. Natalia sold three times as many items in June as she did in May. How many clips did Natalia sell altogether in these three months? LLaDA Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May, so she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in these three months. The final result is 144. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Daily life</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>User '但闻人语响'的上一句是什么? GPT-4o "但闻人语响"的上一句是"无可奈何花落去",这两句出自唐代词人李清照的(武陵 春，春晚: 无可奈何花落去，但闻人语响。这首词表达了作者在春天时节的孤独与 感伤，借花落和人语声的描写，反映出词人内心的无亲与失落。 LLaDA "但闻人语响"的上一句是"空山不见人"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question2</head><p>User '夜静春山空'的上一句是什么? GPT-4o "夜静春山空"的上一句是"空山新雨后".这两句出自唐代诗人王维的(山居秋 膜):空山新雨后，夜静春山空。这首诗描绘了春天山中雨后的寂静与空灵， 表现出自然的宁静与和谐美。 LLaDA "夜静春山空"的上一句是"人闲桂花落"</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Aleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altenschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anadkat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training generalized multi-query transformer models from multi-head checkpoints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zemlyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lebron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><surname>Gqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Em-pirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Em-pirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4895" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Physics of Language Models: Part 3.2, Knowledge Manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv e-prints, abs/2309.14402</idno>
		<ptr target="http://arxiv.org/abs/2309.14402" />
		<imprint>
			<date type="published" when="2023-09">September 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpolating autoregressive and discrete denoising diffusion language models</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tyEyYT267x" />
	</analytic>
	<monogr>
		<title level="m">The Thirteenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Claude 3.5 sonnet</title>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/news/claude-3-5-sonnet" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17981" to="17993" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">All are worth words: A vit backbone for diffusion models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="22669" to="22679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.14255</idno>
		<title level="m">Efficient training of language models to fill in the middle</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The reversal curse: Llms trained on&quot; a is b&quot; fail to learn</title>
		<author>
			<persName><forename type="first">L</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balesni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deepseek llm: Scaling open-source language models with longtermism</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.02954</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Video generation models as world simulators</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Depue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://openai.com/research/video-generation-models-as-world-simulators" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A continuous time framework for discrete denoising models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Bortoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="28266" to="28279" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Masked generative image transformer</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Maskgit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11315" to="11325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.00704</idno>
		<title level="m">Text-to-image generation via masked generative transformers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Analog bits: Generating discrete data using diffusion models with selfconditioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.04202</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast sampling via de-randomization for discrete diffusion models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.09193</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.10759</idno>
		<title level="m">Qwen2-audio technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457</idno>
		<title level="m">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14168</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language modeling is compression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deletang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ruoss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Duquenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Catt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grau-Moya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Wenliang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aitchison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Orseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sartran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roshannai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Durkan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15089</idno>
		<title level="m">Continuous diffusion for categorical data</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jauhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Al-Dahle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Letman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.21783</idno>
		<title level="m">The llama 3 herd of models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the mathematical foundations of theoretical statistics</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical transactions of the Royal Society of London. Series A</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page" from="309" to="368" />
			<date type="published" when="1922">1922</date>
		</imprint>
	</monogr>
	<note>containing papers of a mathematical or physical character</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Noac'h</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ociepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schoelkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Skowron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://zenodo.org/records/12608602" />
		<imprint>
			<biblScope unit="page" from="7" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.15595</idno>
		<title level="m">Discrete flow matching</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><surname>Diffuseq</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.08933</idno>
		<title level="m">Sequence to sequence text generation with diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scaling diffusion language models via adaptation from autoregressive models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.17891</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Our next-generation model: Gemini 1.5, 2024</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07037</idno>
		<title level="m">Bayesian flow networks</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Likelihood-based diffusion language models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ssd-lm: Semiautoregressive simplex-based diffusion language model for text generation and modular control</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17432</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Diffusionbert: Improving generative masked language models with diffusion models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.15029</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03300</idno>
		<title level="m">Measuring massive multitask language understanding</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the math dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03874</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<title level="m">The curious case of neural text degeneration</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02037</idno>
		<title level="m">Autoregressive diffusion models</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12454" to="12465" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.06395</idno>
		<title level="m">Unveiling the potential of small language models with scalable training strategies</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.09937</idno>
		<title level="m">Compression represents intelligence linearly</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Kitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hensman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitra</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2312.05253</idno>
		<title level="m">Disk: A diffusion model for structured knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The factorization curse: Which tokens you predict underlie the reversal curse and more</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kitouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05183</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Cllms</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.00835</idno>
		<title level="m">Consistency large language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><surname>Cmmlu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09212</idno>
		<title level="m">Measuring massive multitask language understanding in chinese</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Diffusion-lm improves controllable text generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4328" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><surname>Truthfulqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07958</idno>
		<title level="m">Measuring how models mimic human falsehoods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="21051" to="21064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2412.19437</idno>
		<title level="m">Deepseek-v3 technical report</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Reflected diffusion models</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Discrete diffusion language modeling by estimating the ratios of the data distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.16834</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Tess: Text-to-text selfconditioned simplex diffusion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ivison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Concrete score matching: Generalized score matching for discrete data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="34532" to="34545" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Scaling up masked diffusion models on text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.18514</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimizing Language Models for Dialogue</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><surname>Chatgpt</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/chatgpt/" />
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<date type="published" when="2022-11">November 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Learning to reason with llms</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/index/learning-to-reason-with-llms/" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Your absorbing discrete diffusion secretly models the conditional distributions of clean data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.03736</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Generative agents: Interactive simulacra of human behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th annual acm symposium on user interface software and technology</title>
		<meeting>the 36th annual acm symposium on user interface software and technology</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scalable diffusion models with transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Direct preference optimization: Your language model is secretly a reward model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Discrete diffusion via edit-based reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><surname>Diffuser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Rein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Gpqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.12022</idno>
		<title level="m">A graduate-level google-proof q&amp;a benchmark</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Categorical sdes with simplex diffusion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arriola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schiff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marroquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.07524</idno>
		<title level="m">Simple and effective masked diffusion language models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Winogrande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Simplified and generalized masked diffusion for discrete data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04329</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Training and inference on any-order autoregressive models the right way</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Self-conditioned embedding diffusion for text generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.04236</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Roformer: Enhanced transformer with rotary position embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">568</biblScope>
			<biblScope unit="page">127063</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.16750</idno>
		<title level="m">Scorebased continuous-time discrete diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">M</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.09261</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">Llama 2: Open foundation and finetuned chat models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A deep and tractable density estimator</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">A survey on large language model based autonomous agents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">186345</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18567</idno>
		<title level="m">Diffusion language models are versatile protein learners</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Dplm-2: A multimodal diffusion protein language model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.13782</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24824" to="24837" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Ar-diffusion: Auto-regressive diffusion model for text generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.05415</idno>
		<title level="m">Show-o turbo: Towards accelerated unified multimodal understanding and generation</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.15766</idno>
		<title level="m">Unifying bayesian flow networks and diffusion models through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiu</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2412.15115</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Z. Qwen2.5 technical report</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Diffusion language models can perform many tasks with scaling and instruction-finetuning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.12219</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Dinoiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.10025</idno>
		<title level="m">Diffused conditional sequence learning by manipulating noises</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2407.20311" />
		<imprint>
			<date type="published" when="2024">abs/2407.20311, July 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hellaswag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07830</idno>
		<title level="m">Can a machine really finish your sentence? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.02908" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A reparameterized discrete diffusion model for text generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<idno>ArXiv, abs/2302.05737</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
