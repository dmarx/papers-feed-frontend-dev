- Choice of diffusion model over autoregressive models
- Use of a vanilla Transformer architecture
- Implementation of a forward data masking process
- Optimization of a likelihood bound for generative modeling
- Decision to scale LLaDA to 8 billion parameters
- Selection of pre-training dataset and filtering methods
- Adoption of a variable masking ratio during training
- Use of the Warmup-Stable-Decay learning rate scheduler
- Choice of AdamW optimizer with specific hyperparameters
- Design of supervised fine-tuning (SFT) process
- Decision to include multiple domains in the SFT dataset
- Implementation of remasking strategies during inference
- Trade-off considerations between sampling efficiency and quality
- Handling of variable-length data in training and inference
- Approach to managing computational costs during training
- Decisions regarding the evaluation of model performance against baselines