- Choice of stochastic variational inference method
- Selection of reparameterization trick for variational inference
- Design of the recognition model architecture
- Decision to use neural networks for the recognition model
- Choice of optimization algorithm (e.g., SGD, Adagrad)
- Determination of minibatch size for training
- Selection of prior distribution for latent variables
- Decision on the form of the generative model
- Choice of loss function for model training
- Assumptions about the dataset (i.i.d., continuous latent variables)
- Handling of intractable posterior distributions
- Strategy for estimating the variational lower bound
- Decision to use KL divergence as a regularization term
- Choice of auxiliary noise distribution in the reparameterization
- Approach to sampling from the approximate posterior
- Design of the overall training loop and convergence criteria
- Consideration of computational efficiency in large datasets
- Decision to extend the method to online or non-stationary settings
- Assumptions regarding the differentiability of the model components
- Strategy for evaluating model performance and validation