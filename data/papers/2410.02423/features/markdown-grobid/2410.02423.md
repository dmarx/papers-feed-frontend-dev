# PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING

## Abstract

## 

In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pretrained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the datafidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.

## INTRODUCTION

Image restoration aims to recover an unknown image x ∈ R d from a degraded observation y ∈ R m y = noisy [(Hx)](#), where H : R d → R m is a (linear) degradation operator and noisy describes the underlying noise model. Since the problem is usually ill-posed and high dimensional, its treatment is challenging. We assume that the image x is sampled from a random variable X ∈ R d with a density p X , and the observation y from a random variable Y ∈ R m with a density p Y . Then the maximum a posteriori estimator: searches for the value with the highest probability of the posterior arg max

$x∈R d log p X|Y =y (x) = arg max x∈R d log p Y |X=x (y) + log p X (x) ,(1)$where the first term of the right hand side is the fidelity to the data and the second term represents the prior distribution of the image. Since p X is generally unknown, and in the absence of training data, it is standard to instead consider a regularized optimization problem of the form:

$arg min x∈R d {F (x) + R(x)} ,(2)$where F (x) := -log p Y |X=x (y) and R : R d → R usually enforces some assumptions on the solution and ensures the existence of a (unique) minimizer. The regularized minimization problem in (2) can be efficiently handled using proximal splitting methods [(Boyd et al., 2011)](#b8).

Plug-and-Play (PnP) [(Venkatakrishnan et al., 2013)](#b53) methods build upon the insight that the proximal step on the regularization term is effectively a denoising operation. They can be performed with nonlearned denoisers such as BM3D [(Chan et al., 2016)](#b9), however recently these were outperformed by neural network-based approaches [(Meinhardt et al., 2017;](#b37)[Zhang et al., 2017;](#b58)[2021)](#). While PnP methods have demonstrated considerable effectiveness, recent advances in generative models offer a more sophisticated framework for learning priors directly from data, surpassing the limitations of hand-crafted or neural denoisers.

In particular, generative models are used as regularizers in [Bora et al. (2017)](#b7); [Asim et al. (2020)](#b2); [Altekrüger et al. (2023)](#b0); [Wei et al. (2022)](#b55). Most of these references learn an invertible transport map T between a Gaussian latent distribution and the data distribution, using the change of variables formula to obtain the estimated prior p X as an explicit function of the known latent density and the map T . In particular the normalizing flow network with the Real-NVP [(Dinh et al., 2017)](#b17) architecture is by default invertible and has an easy-to-evaluate log determinant of the Jacobian of T . However, this architecture yields rather poor results on large images. Therefore, recent work [(Ben-Hamu et al., 2024;](#)[Zhang et al., 2024;](#b60)[Pokle et al., 2024;](#b39)[Chung et al., 2023;](#)[Song et al., 2023)](#b45) focuses more on learning diffusion models [(Song et al., 2021;](#)[Ho et al., 2020;](#b22)[Sohl-Dickstein et al., 2015)](#b43) or Flow Matching models [(Lipman et al., 2023;](#)[Liu et al., 2023)](#), which do not have these architectural constraints and scale better to large images. The main idea is to establish a path between the latent and the target distribution, which can be optimized in a simulation-free way.

In this paper, we focus on Flow Matching models, which learn a velocity field v : [0, 1]×R d -→ R d going from the latent to the data distribution. Once the velocity field v is learned, sampling from the target distribution can be done by solving an ODE (Chen et al., 2018)

$∂ t f (t, x) = v t (f (t, x)), f (0, x) = x,$where x is sampled from the latent distribution.

However, using Flow Matching to regularize inverse problems such as image restoration tasks is nontrivial. Indeed, when directly solving the MAP problem (1) using the change of variables formula, one faces numerical challenges due to the backpropagation through the ODE. We circumvent this problem by integrating the implicit Flow Matching prior into a custom denoiser that we plug into a Forward-Backward algorithm.

Contributions. In this paper, we combine PnP methods with Flow Matching and propose a PnP Flow Matching algorithm. Our contributions are as follows:

• Inspired by optimal transport Flow Matching, we design a time-dependent denoiser based on a pre-trained velocity field v learned through Flow Matching. • This denoiser is integrated into an adapted Forward-Backward Splitting PnP framework, which cycles through a gradient step on the data-fidelity term, an interpolation step to reproject iterates onto flow trajectories, and a denoising step. • Our algorithm is simple to implement, requires no backpropagation through the network, and is highly efficient in both memory usage and execution time compared to existing Flow Matching-based methods. • We show that, on two datasets and across denoising, deblurring, inpainting, and superresolution tasks, our method consistently outperforms state-of-the-art Flow Matchingbased and PnP methods in terms of both PSNR and SSIM metrics. The code with all benchmark methods is available at: [https://github.com/annegnx/PnP-Flow](https://github.com/annegnx/PnP-Flow).

## BACKGROUND

We next provide background on both Plug-and-Play algorithms and Flow Matching models.

## PLUG AND PLAY

PnP algorithms were introduced as extensions of proximal splitting methods like Forward-Backward Splitting (FBS) or the alternating direction method of multipliers (ADMM). These are first-order optimization algorithms for solving problem (2), alternating between a proximal step on the regularization and a gradient and/or proximal step on the data-fidelity function.

The proximal operator prox γR : R d → R d , γ > 0 is defined by prox γR (y) := arg min x∈R d 1 2 ∥y -x∥ 2 + γR(x) . In this paper, we assume that the data-fidelity F is differentiable with Lipschitz continuous gradient latent data ground truth y G radient Step Interpolation Step PnP Step (a) t = 0 (b) t = 0.56 (c) t = 0.89 Figure 1: Our method on a 2D denoising task (σ = 1.5) with Gaussian distributions. An OT Flow Matching model is trained to sample from P 1 = N (m, s 2 Id), with m = 7 and s = 0.5. At each time step, it performs a standard gradient step on the datafit, followed by a projection onto flow trajectories at time t, and finally applies the time-dependent denoiser D t . and we focus on the FBS algorithm, corresponding to Algorithm 1. The convergence of FBS to a minimizer of F + R is guaranteed as long as the stepsize is chosen as γ ∈ (0, 1/Lip(∇F )), where Lip(∇F ) is the Lipschitz constant of ∇F (Combettes [& Wajs, 2005)](#b15).

Algorithm 1: FBS

$Initialization: x (0) ∈ R d , γ > 0 for r = 0, 1, . . . do z (r+1) = x (r) -γ∇F (x (r) ) x (r+1) = prox γR (z (r+1) )$Algorithm 2: PnP-FBS Most of these PnP-FBS algorithms do not converge [(Zhang et al., 2017;](#b58)[Sommerhoff et al., 2019)](#b44) and convergence guarantees usually assume non-expansiveness of the denoiser [(Pesquet et al., 2021;](#b38)[Ryu et al., 2019;](#b42)[Hurault et al., 2022b](#)). Yet, constraining the Lipschitz constant of a neural network, e.g., through weight clipping, spectral normalization or averaged operator constructions harms in general its expressiveness [(Ryu et al., 2019)](#b42).

$Initialization: x (0) ∈ R d , γ > 0 for r = 0, 1, . . . do z (r+1) = x (r) -γ∇F (x (r) ) x (r+1) = D(z (r+1) ) ▷ PnP$
## FLOW MATCHING

Let P 2 (R d ) denote the subspace of probability measures on R d having a finite second moment. Let P 0 denote the latent probability measure and P 1 the target probability measure (i.e., the data distribution). We denote • ♯ the push-forward operation. We denote Γ(P 0 , P 1 ) the set of couplings π ∈ P(R d × R d ) having marginals P i ∈ P 2 (R d ), i = 0, 1, and Γ o (P 0 , P 1 ) the set of optimal couplings minimizing the Wasserstein-2 distance [(Villani, 2008)](#b54). We now start with any coupling π ∈ Γ(P 0 , P 1 ).

The core idea behind Flow Matching is to define a specific target probability path t → P t , t ∈ [0, 1], between P 0 and P 1 . Let P t := (e t ) ♯ π, where the map e t (x 0 , x 1 ) := (1 -t)x 0 + tx 1 interpolates between (a latent sample) x 0 and (a data sample) x 1 . The path P t can be shown to be an absolutely continuous curve, so there exists a Borel vector field v : [0, 1] × R d → R d such that the curve satisfies the continuity equation

$∂ t P t + ∇ • (P t v t ) = 0 (CE)$in the sense of distributions [(Ambrosio et al., 2008)](#b1). In addition, there exists a solution f :

$[0, 1] × R d → R d of the ODE ∂ t f (t, x) = v t (f (t, x)), f (0, x) = x (3)$such that P t = f (t, •) ♯ P 0 . If f is known, then a sample x 1 from the target distribution P 1 can be drawn by first sampling x 0 ∼ P 0 and then defining

$x 1 = f (1, x 0 ).$The goal of Flow Matching is to learn the velocity field of the flow ODE (3). This learning process consists in minimizing the loss function

$L FM (θ) := E t∼U [0,1],x∼Pt ∥v θ t (x) -v t (x)∥ 2 ,$where v θ is parametrized by a neural network with weights θ. Unfortunately, in practice, the true velocity field v t (x) is not available. However, [Lipman et al. (2023)](#) showed that minimizing L FM (θ) is equivalent to minimizing the Conditional Flow Matching (CFM) loss: L CFM (θ) = L FM (θ)+const, where

$L CFM (θ) := E t∼U [0,1],(x0,x1)∼π ∥v θ t (e t (x 0 , x 1 )) -(x 1 -x 0 )∥ 2 . (4$$)$Minimizing this loss only requires sampling from the coupling π. For example, if π = P 0 × P 1 is the independent coupling, we only need samples from the latent and target distributions. If instead, π ∈ Γ o (P 0 , P 1 ) is an optimal coupling, then we can use a standard optimal transport solver to (approximately) sample from π, as proposed in [Pooladian et al. (2023)](#); [Tong et al. (2024)](#b52).

Straight-line flows Let v : [0, 1] × R d -→ R and let f be a solution to the flow ODE associated with v. Given (X 0 , X 1 ) ∼ π, we call (f, v) a straight-line Flow Matching pair connecting X 0 and X 1 if X t = f (t, X 0 ) almost surely, where X t is defined as X t := e t ((X 0 , X 1 )) = tX 1 + (1 -t)X 0 [(Liu et al., 2023;](#)[Pooladian et al., 2023)](#). Note that this directly implies

$v t (f (t, X 0 )) = ∂ t f (t, X 0 ) = ∂ t X t = X 1 -X 0 almost surely.$We will focus on such straight-line flows later in the paper. Straight or nearly straight paths are preferred since they represent the shortest distance between two points and can be simulated with fewer steps of an ODE solver. This has been explored in many recent works that want to speed up sampling, such as [Kornilov et al. (2024)](#b29); [Lee et al. (2023)](#); [Yang et al. (2024)](#b56). A particular case of straight-line pair is given by OT couplings [(Pooladian et al., 2023;](#)[Tong et al., 2024)](#b52). Indeed, for π ∈ Γ o (P 0 , P 1 ) with Monge map T : R d → R d such that X 1 = T (X 0 ), there exists a velocity field v for which the pair (P t , v t ) is a solution of the continuity equation (CE), and such that for all

$x ∈ R d v t (f (t, x)) = T (x) -x,(5)$where f is the solution of the ODE (3), see [Ambrosio et al. (2008)](#b1); [Pooladian et al. (2023)](#); [Chemseddine et al. (2024)](#b10) for the precise conditions. By integrating (5

$), it is clear that (f, v) is a straight-line Flow Matching for the coupling (X 0 , T (X 0 )), since X t = (1-t)X 0 +tX 1 = (1-t)X 0 +tT (X 0 ) = X 0 + t 0 v s (f (s, X 0 ))ds = f (t, X 0 ).$
## PNP MEETS FLOW MATCHING

3.1 DENOISING OPERATOR FROM FLOW MATCHING Let X 0 ∼ P 0 and X 1 ∼ P 1 with joint distribution (X 0 , X 1 ) ∼ π. Assume we have access to a pre-trained velocity field v θ . Then we define, for t ∈ [0, 1], the following time-dependent denoiser

$D t := Id + (1 -t)v θ t ,(6)$and we propose to use it within a PnP framework.

To motivate the choice of the denoiser in ( [6](#formula_14)), recall that for a fixed time t ∈ [0, 1], the minimizer v * t of the CFM loss (4) over all possible vector fields reads

$v * t (x) = E[X 1 -X 0 |X t = x],$where X t := e t (X 0 , X 1 ) = (1 -t)X 0 + tX 1 [(Benton et al., 2024;](#b4)[Liu et al., 2023)](#). Assume we are in the ideal case where v θ t = v * t . Then it follows that, for any

$x ∈ R d and t ∈ [0, 1], D t (x) = x + (1 -t)v * t (x) = E (X0,X1)∼π [X t + (1 -t)(X 1 -X 0 )|X t = x] = E (X0,X1)∼π [X 1 |X t = x] .$Hence, the operator D t can be understood at the best approximation of X 1 given the knowledge of X t , which is also used in [Pokle et al. (2024)](#b39) and [Zhang et al. (2024)](#b60).

Just as standard PnP denoisers minimize the MSE loss between noisy and clean samples, the operator

$D t is the minimizer of L 2 -problem min g E (X0,X1)∼π [||X 1 -g(X t )|| 2 ],$projecting any noisy point taken along the path onto the target distribution. In particular, the following proposition holds, demonstrating that the best denoising performance is achieved with straight-line flows. Proposition 1. Assume v := v θ is continuous and assume that, given v, the Flow ODE (3) has a unique solution f . Then the denoising loss

$E (X0,X1)∼π [∥D t (X t ) -X 1 ∥ 2 ] is equal to 0 for all t ∈ [0, 1], if and only if the couple (f, v) is a straight-line Flow Matching pair between X 0 and X 1 .$The proof can be found in Appendix A.1.

As stated in Section 2.2, a special case of straight-line flows is given by OT Flow Matching for which we recover D t (X t ) = X 1 . Indeed, in this case, D t reduces to T (X 0 ) = X 1 , with T the Monge map between P 0 and P 1 . We next discuss whether diffusion models induce straight-line flows. Remark 2 (Flow Matching versus diffusion models). Contrary to OT Flow Matching, diffusion models, which are also flow ODE methods, do not generally induce straight paths. Indeed, during diffusion training, the target probability path takes the form Xt = α t X 0 + β t X 1 with α t ∈ (0, 1) and β t = 1 -α 2 t [(Song et al., 2021;](#)[Liu et al., 2023)](#), which clearly does not match the desired straight path X t = (1 -t)X 0 + tX 1 . The non-straightness of the flow generated by diffusion models is illustrated in [Liu et al. (2023)](#); Liu (2022, Figure [5](#)). On the other hand, a non-straight path obtained with Flow Matching (for instance, using an independent coupling π) can be rectified to a straighter one following the procedure described in [Liu et al. (2023)](#).

## PNP FLOW MATCHING ALGORITHM

In the previous section, we built a denoiser D t (defined in ( [6](#formula_14))). We now want to plug it in a Forward-Backward Splitting algorithm in order to solve inverse problems. Yet, our algorithm differs from the classical PnP-FBS (Algorithm 2) in two key aspects. First, the iterations of the our algorithm depend on time because of the definition of D t . Second, we introduce an intermediate reprojection step between the gradient step on the data-fidelity term and the denoising step. More precisely, at each time t ∈ [0, 1], given the current iterate x, our algorithm does the following updates: 1. Gradient step: a gradient step on the data-fidelity term, mapping x to z = x -γ∇F (x) for a given learning rate γ > 0. 2. Interpolation step: In a standard Forward-Backward scheme, the denoiser operator is applied right after the gradient step. Yet, as discussed earlier, our operator D t was specifically designed to correctly denoise inputs drawn from the straight path X t = (1 -t)X 0 + tX 1 . If the output z from the gradient step at time t does not lie in the support of X t , there is a high chance that the denoising will not be effective, hence the need to "reproject" it along the flow paths before applying D t . To achieve this, we perform a linear interpolation on z, as illustrated in Figure [2](#fig_1): at time t, we define z = (1 -t)ε + tz, where ε is a noise sample drawn from P 0 . Note that while ε is sampled from P 0 , it is not necessarily coupled to z ∼ P 1 via π. If it were, D t would map z directly back to z, annihilating the denoising effect. 3. PnP Denoising step: the operator D t is applied to the output z of the interpolation step, regularizing the current image by pushing it towards the distribution of X 1 .

The resulting discrete-time algorithm is given in Algorithm 3. Figure [1](#) illustrates the three steps of the algorithm on a denoising problem with a Gaussian prior.

## Algorithm 3: PnP Flow Matching

Input: Pre-trained network v θ by Flow Matching, time sequence (t n ) n either finite with

$t n = n/N , N ∈ N or infinite with lim n→+∞ t n = 1, adaptive stepsizes (γ n ) n . Initialize: x 0 ∈ R d . for n = 0, 1, . . . , do z n = x n -γ n ∇F (x n ). ▷ Gradient step on the data-fidelity term zn = (1 -t n )ε + t n z n , ε ∼ P 0 ▷ Linear interpolation x n+1 = D tn (z n ) ▷ PnP step with denoiser (6) return x n+1$Remark 3 (Averaging in the denoising step). Instead of drawing one noise realization, we can also average over multiple samples ε ∼ P 0 in the last step of the algorithm:

$x n+1 := E ε∼P0 [D tn (z n (ε))] with zn (ε) := (1 -t n )ε + t n z n .$The algorithm's output is then deterministic. In practice, averaging over a few realizations slightly improves the numerical results.

Time dependent learning rate Using a constant learning rate independent of time can give too much importance to the data fit. For example, if γ t = 1 for all t ∈ [0, 1] on a simple denoising task, the algorithm will return the noisy sample y since D 1 = Id. To prevent this, γ t should decrease with t to balance the contributions of the datafit and the denoiser. We set γ t = (1 -t) α with α ∈ (0, 1] for the remainder of the paper. This choice yields good numerical results in our experiments, but alternative values for γ t could also be explored.

Convergence Assuming that the sequence produced by the algorithm is bounded in the infinite time regime, we have the following convergence result. Proposition 4. Assume that F : R d → R is continuously differentiable and that the learned vector field

$v : [0, 1]×R d → R d is continuous. Let the time sequence (t n ) n∈N satisfy ∞ n=0 (1-t n ) < +∞ and let γ n := 1 -t n , n ∈ N. If the sequence (x n ) n∈N obtained by Algorithm 3 is bounded, then it converges.$The proof can be found in Appendix A.2.

## RELATED WORK

Our approach combining PnP with Flow Matching relates to several existing methods.

Pre-trained Flow Matching methods Using pre-trained Flow Matching models for regularizing image inverse problems has been the focus of several recent works.

OT-ODE [(Pokle et al., 2024)](#b39) assumes a Gaussian latent distribution and uses Tweedie's formula to derive a new velocity field ṽt (x, y

$) = E[X 1 -X 0 | X t = x, Y = y] from the original velocity field v t (x) = E[X 1 -X 0 | X t = x],$all without requiring retraining. In practice, they sample from the posterior distribution X 1 | Y by solving the ODE with the new velocity field using an Euler scheme.

In [Zhang et al. (2024)](#b60), the authors introduce Flow-Priors, a method to tackle the MAP problem by approximating it as a sequence of time-dependent MAP subproblems. Using Tweedie's formula, they show that for t < 1, the gradient of the distribution P t of X t can be computed in closed form, allowing for the use of gradient descent to optimize these subproblems. However, the closed-form expression for the gradient relies on the assumption of an independent coupling π and a Gaussian latent distribution. Besides, their method requires computing Tr ∇v θ , which is expensive.

In [Ben-Hamu et al. (2024)](#), an implicit regularization approach called D-Flow is considered: instead of minimizing the Gaussian data-fidelity function x → ∥Hx -y∥ 2 , they minimize the latent loss z → ∥H(f (1, z)) -y∥ 2 , where f is a solution to the flow ODE given the pre-trained network. The two problems are theoretically equivalent since f (1, •) is invertible. However, since the latent loss is not convex, first-or second-order optimization methods may not find the global minimizer. Interestingly, this is beneficial because the true minimizer of the original problem is simply the pseudo-inverse, which may not be desirable. The authors optimize the latent loss by backpropagating through the ODE solution with a few Euler steps, though this remains computationally expensive.

PnP diffusion methods While we present the first PnP method based on Flow Matching, related works combine diffusion models with the PnP framework. To our knowledge, the following are the only methods using PnP with pre-trained generative models instead of standard denoisers.

In [Zhu et al. (2023)](#b61), a Half Quadratic Splitting algorithm is used, alternating between a proximal step on the data-fidelity term and a proximal step on the regularization. Following the PnP strategy, the proximal step for the regularization term is replaced with a denoising step using a pre-trained diffusion model. The denoiser they use is reminiscent of the one we use, where the velocity is replaced by the gradient of the score function. Their method also includes an interpolation step with random noise, mapping the estimated image at each iteration back to the diffusion path.

Finally, conditional image restoration is explored in [(Graikos et al., 2022)](#), which uses a more relaxed definition of "plug and play". They integrate a pre-trained diffusion model under different conditions, leading to a variational objective similar to methods like [Mardani et al. (2024)](#).

## NUMERICAL EXPERIMENTS

## BASELINES

We benchmark our method against three state-of-the-art Flow Matching-based restoration methods: OT-ODE [(Pokle et al., 2024](#b39)[), D-Flow (Ben-Hamu et al., 2024)](#) and Flow Priors [(Zhang et al., 2024)](#b60). As no official implementations were publicly available for these methods, we developed our own based on the descriptions provided in their respective publications. We have made every effort to ensure faithful implementations, included in the code attached to this paper. We also benchmark our method against PnP-Diff [(Zhu et al., 2023)](#b61), a PnP algorithm based on diffusion models. Additionally, we compare our approach with the state-of-the-art PnP-FBS [(Hurault et al., 2022a)](#).

## EXPERIMENTAL SETUP

Datasets We evaluate all methods on two datasets: CelebA [(Yang et al., 2015)](#b57), with images resized to 128×128, and AFHQ-Cat, a subset of the Animal FacesHQ dataset [(Choi et al., 2020)](#b13) focused on the cat class, with images resized to 256 × 256. All images are normalized to the range [-1, 1]. For CelebA, we use the standard training, validation, and test splits. For AFHQ-Cat, as no validation split is provided, we randomly select 32 images from the test set to create a validation set.

Models For each dataset, we trained a Flow Matching model from scratch using the Mini Batch OT Flow Matching approach [(Tong et al., 2024)](#b52), as this choice of coupling usually leads to straight paths. We used a standard Gaussian as the latent distribution and a U-Net [(Ronneberger et al., 2015)](#b41) taken from [Huang et al. (2021)](#b23); [Ho et al. (2020)](#b22) as the model. The training parameters were a learning rate of 10 -4 , 200 epochs with a batch size of 128 for CelebA, and 400 epochs with a batch size of 64 for AFHQ-Cat. We train the denoising network for the PnP method PnP-GS [(Hurault et al., 2022a)](#) employing the same U-Net architecture. For PnP-Diff, because training a diffusion model with the same U-Net architecture as Flow-Matching yielded poor results due to insufficient number parameters, we used the pre-trained model from [Choi et al. (2021)](#), implemented in the DeepInv library[foot_0](#foot_0)[(Tachella et al., 2023)](#b49). Note that the pre-trained diffusion model was trained on the FFHQ dataset [(Karras et al., 2019)](#b28), making the comparison indirect, but we had no alternative.

Settings for the experiments We evaluate the methods using 100 test images across five restoration problems: denoising with Gaussian noise (σ = 0.2); deblurring using a 61×61 Gaussian kernel (σ b = 1.0 for CelebA, σ b = 3.0 for AFHQ-Cat); super-resolution (2× downsampling for CelebA, 4× for AFHQ-Cat); box-inpainting with a centered s × s mask (s = 40 for CelebA, s = 80 for AFHQ-Cat); and random pixel inpainting (70% masked pixels). For deblurring, super-resolution, and box-inpainting, we add Gaussian noise with σ = 0.05, and for random inpainting σ = 0.01.

## Hyper-parameters

We optimize the hyper-parameters for each method using a grid search on the validation set, selecting the configuration that yields the highest Peak Signal-to-Noise Ratio (PSNR). The optimal values identified for each dataset and problem scenario are detailed in Appendix A.8. Our proposed method has two hyper-parameters: the exponent α in the learning rate schedule γ n = (1 -t n ) α , and the number of uniformly spaced time steps was set to N = 100 for most experiments. We averaged the results of the denoising step over 5 realizations of the interpolation step.

## MAIN RESULTS

We report benchmark results for all methods across several restoration tasks, measuring average PSNR and Structural Similarity (SSIM) on 100 test images. To ensure reproducibility, all experiments were seeded. Results are presented in Table [1](#tab_1) for CelebA and Table [2](#tab_2) for AFHQ-Cat. "N/A" indicates cases where the method is inapplicable; for instance, PnP-GS [(Hurault et al., 2022a)](#) is not designed for generative tasks like box inpainting, and PnP-Diff relies on a diffusion model trained on another dataset, making its evaluation on box inpainting inappropriate.

The tables show that our method consistently ranks first or second in both reconstruction metrics across all tasks and datasets. More importantly, it demonstrates stability across tasks, unlike other methods. For example, D-Flow and Flow-Priors perform well in box inpainting but struggle with denoising, while PnP-GS, PnP-Diff, and OT-ODE excel in denoising and deblurring but perform worse on pseudo-generative tasks.

In terms of visual quality (Fig. [3](#), Fig. [4](#fig_2), and Appendix A.7), our method produces realistic, artifactfree images, though sometimes slightly over-smoothed. While D-Flow generates realistic images, it occasionally suffers from hallucinations (e.g., eye color shifts in CelebA denoising tasks). Flow-Priors introduces noise and artifacts, while OT-ODE captures textures well but struggles with image generation. Finally, Appendix A.6 shows the progression of the reconstruction given by our method PnP-Flow with respect to time.   Computation time and memory All experiments in this section are conducted on a single NVIDIA RTX 6000 Ada Generation with 48GB RAM. We measure the averaged time per image to complete a deblurring task on the CelebA dataset. We also compute the peak GPU memory load per image. The results are averaged over 100 images (25 batches of 4 images each). We use the same settings as those used for reporting performance metrics on CelebA. Results are in Table [3](#tab_3).

Sensitivity to initialization Notably, our algorithm does not rely on a good initialization. By starting the algorithm at time t 0 = 0, the linear interpolation step initially outputs a pure noise, which is then given to the denoiser. As a result, the algorithm's performance is independent of the initialization. In Appendix A.4, we stress that this is not the case for other methods.

Choice of the latent distribution Our algorithm can be used with any latent distribution. This is in the spirit of Flow Matching models [(Lipman et al., 2023)](#), which do not rely on a Gaussian latent as opposed to diffusion models. In particular, recently there has been a trend of modelling categorical data with Flow Matching [(Boll et al., 2024;](#b6)[Stark et al., 2024;](#)[Davis et al., 2024)](#b16). Our method does not rely on a Gaussian latent, contrary to the other Flow Matching restoration methods.

As an example to illustrate the performance of our approach in a non Gaussian case, in Appendix A.3 we conduct an experiment with a Dirichlet latent distribution, inspired by [(Stark et al., 2024)](#).

Adaptability to any straight-line flows Our denoiser is rooted in the straight-line flow framework, which motivates our use of OT Flow Matching. However, other choices of FM models are possible. Notably, Rectified Flows [(Liu et al., 2023)](#) are of particular interest, as the method described allows for the straightening of any flow model. In Appendix A.5, we show how our method performs similarly to what we observed in Section 5.3 using pre-trained Rectified Flows.

## CONCLUSION

We introduced PnP-Flow Matching, and compared it to common Flow Matching and PnP methods.

A great strength of our method is its versatility: it requires few hyperparameters, uses minimal memory and computational resources, delivers very good performance across various inverse problems, and supports different latent distributions as well as flexible initialization. Regarding limitations, our reconstructions seem to be more on the smooth side, which relates to the denoising operation as an minimum mean squared estimator. This however is a common tradeoff in regularization of inverse problems [(Blau & Michaeli, 2018)](#b5). Next, it would be fruitful to use our method for other types of measurement noise such as Poisson noise [(Hurault et al., 2023)](#b26) and to make use of different latent distributions [(Boll et al., 2024;](#b6)[Gat et al., 2024;](#b18)[Stark et al., 2024)](#) to model categorical distributions.

In particular, Dirichlet distributions are common in the field of biological/molecular data, which is a data domain to be investigated.

## A APPENDIX

A.1 PROOF OF PROPOSITION 1

Proof. We have by the definition of the denoiser (6) (it was also observed in [Lee et al. (2023)](#))

$E (X0,X1)∼π [∥D t (X t ) -X 1 ∥ 2 ] = (1 -t) 2 E (X0,X1)∼π [∥v t (X t ) -(X 1 -X 0 )∥ 2 ].$If we assume that the denoising loss is zero, this yields for all t ∈ [0, 1) that v t (X t ) = X 1 -X 0 almost surely. By continuity this also follows for t = 1. By the same arguments as in [(Liu et al., 2023, Theorem 3.6](#) iii)+ iv)) we have that both t → f (t, X 0 ) and t → X t are solution to the flow ODE initialized at X 0 , since

$X t = X 0 + tv t (X t ) = X 0 + t 0 v s (X s )ds a.s.$where we used that v is constant with time. This implies that ∂ t X t = v t (X t ) almost surely. Together with the uniqueness of the ODE solution, it follows that f (t, X 0 ) = X t almost surely.

On the other hand, if (f, v) is a straight-line Flow Matching pair connecting X 0 and X 1 , then we obtain that

$v t (X t ) = v t (f (t, X 0 )) = ∂ t f (t, X 0 ) = ∂ t X t = X 1 -X 0 a.s.$Therefore the denoising loss is zero.

## A.2 PROOF OF PROPOSITION 4

We provide the proof for Proposition 4 here below.

Proof. By the algorithm and definition of D tn , we have

$x n+1 = D tn (u n ) = u n + (1 -t n )v θ tn (u n )$, where together with the definition of γ n ,

$u n := (1 -t n )ε + t n (x n -γ n ∇F (x n )) = t n x n + (1 -t n ) (ε -t n ∇F (x n )) .$Hence we obtain

$∥x n+1 -x n ∥ = (1 -t n )∥ε -x n -t n ∇F (x n ) + (1 -t n )v θ tn (u n )∥.$By assumption on F and v and since (x n ) n is bounded, the expression in the norm is bounded as well, say by M > 0. Then, by assumption on t n , we conclude

$∞ n=0 ∥x n+1 -x n ∥ = M ∞ n=0 (1 -t n ) < ∞,$so that (x n ) n is a Cauchy sequence and converges.

## A.3 NUMERICAL RESULTS USING A LATENT DIRICHLET DISTRIBUTION

One of the main practical advantages of Flow Matching over diffusion is that one can choose a latent distribution with is not Gaussian. Motivated by DNA design and other discrete data, there has been growing interest in using a Dirichlet latent distribution in Flow Matching [(Stark et al., 2024;](#)[Boll et al., 2024;](#b6)[Davis et al., 2024)](#b16). Here, we want to show that applying PnP-Flow with a Flow Matching model trained on a Dirichlet latent distribution still yields good reconstructions. For this, we use the MNIST dataset [(LeCun et al., 1998)](#b30), rescaling each image to lie on the simplex, and train a Flow Matching model. We then apply this model to inpainting and super-resolution tasks and present the results. Importantly, the goal of this experiment is not to achieve state-of-the-art performance, but to illustrate that our algorithm generalizes to different latent distributions.

For this, we train a Dirichlet Flow Matching model for 200 epochs with a standard OT Flow Matching loss and a Dirichlet distribution with parameters (1, ..., 1) ∈ R 784 . This latent distribution is also used in [Stark et al. (2024)](#) and amounts to the uniform distribution on the 784-dimensional simplex.

Note that the interpolation step now involves Dirichlet noise instead of Gaussian, as this noise is drawn according to P 0 . We reconstruct our images with γ = 1 and 300 steps. Remarkably, almost no modifications for the algorithm are needed. In particular, the generated images x almost lie perfectly on the simplex without any normalization. We show the images in Figure [5](#). We compare to [(Ben-Hamu et al., 2024)](#), where we adapt the regularization in the algorithm so that the optimized variable z lies on the simplex, i.e., we use the loss

$L(z) = ∥y -H(f (1, z))∥ 2 + λ∥( i,j z i,j ) -1)∥ 2 ,$for a single latent image z, where f (1, z) is realized doing 5 Euler steps using the mid point rule and λ is a regularization constant.

As one can see, the PnP Flow outperforms D-Flow on all tasks. Note that the other Flow Matching baselines [(Pokle et al., 2024;](#b39)[Zhang et al., 2024)](#b60) are not usable since their algorithm heavily depends on Gaussian paths.

Figure [5](#): Dirichlet Flow Matching experiment on Simplex-MNIST, for denoising (1st row), superresolution (2nd row), box-inpainting (3rd row). We measure the reconstruction error as the mean L2 distance (called MSE) between ground truth and reconstruction averaged over the 16 images.

## A.4 SENSITIVITY TO INITIALIZATION

Interestingly, our method is inherently independent of the algorithm's initialization. For the image restoration problems considered in this work, initialization is not so much of a concern because a reasonable starting point for the solution x is given by H ⊤ y, where y is the observation and H is the degradation operator. However, for more complex problems where H ⊤ y is far from resembling a natural image, such as in CT reconstruction [(Guo et al., 2016)](#b20) or phase retrieval [(Manekar et al., 2020)](#b35), this property becomes much more relevant.

In contrast, competing methods are more sensitive to initialization and cannot be started from any value. As recommended in the paper, our implementation of OT-ODE is initialized with t 0 y + (1t 0 )ϵ where ϵ ∼ N (0, I d ) and t 0 is the initialization time. The latent variable in D-Flow is initialized as αT -1 (H ⊤ y) + (1 -α)ϵ, where T -1 is the reverse flow, ϵ ∼ N (0, I d ) is a random Gaussian noise, and α ∈ (0, 1) is a blending coefficient. Flow-Priors, on the other hand, is initialized with random noise and, like our method, does not depend on a "good" initialization.

In Figure [6](#fig_3), we illustrate the impact of changing the standard initialization for all methods to a black image on a Gaussian deblurring task, comparing the robustness of each approach to poor initialization.

Clean Init OT-ODE D-Flow Flow-Priors PnP-Flow  [4](#tab_5) and qualitative results on paintbrush inpainting are displayed in Figure [7](#). Here, we provide additional visual results, where we take the same ground truth image for the different inverse problems, see Fig. [9](#) and Fig. [10](#) for CelebA dataset and Fig. [11](#) and Fig. [12](#fig_1) for AFHQ-cat dataset.

Clean Degraded OT-ODE Flow-Priors PnP-Flow Figure 7: Comparison of restoration methods on the CelebA-HQ dataset for paintbrush inpainting using Rectified Flow model. Noisy t = 0.0 t = 0.1 t = 0.2 t = 0.3 t = 0.4 PSNR: 11.23 8.23 13.98 18.01 21.02 23.13 t = 0.5 t = 0.6 t = 0.7 t = 0.8 t = 0.9 t = 1.0 25.80 27.35 29.23 31.42 34.40 36.78 For random inpainting, the PnP algorithm is the Half Quadratic Splitting with the parameters given in the paper [(Hurault et al., 2022a)](#). For denoising, we only apply the trained denoiser once, feeding it with the true noise level σ. For all other methods, we use Proximal Gradient Descent (PGD), as prescribed in [Hurault et al. (2022b)](#). Note that we do not constrain the Lipschitz constant of the denoiser. For the PGD case, we tune three hyper-parameters: the learning rate γ ∈ {0.99, 2.0} in the gradient step (which is related to the regularization parameter λ in the paper), the inertia parameter α ∈ {0.3, 0.5, 0.8, 1.0} which coresponds to the relaxed denoiser D α σ = αD σ + (1 -α)Id, and the factor σ f ∈ {1., [1.2, 1.5, 1.8, 2., 3., 4., 5](#).} so that the denoiser gets as noise map σ f × σ (where σ is the true noise level). We also considered the number of iterations as a hyperparameter that we tuned on the validation set (with maximum number of iterations fixed to 100).

For PnP-Diff, we tuned two parameters: the regularization parameter λ ∈ {1.0, 5.0, 10.0, 100.0, 1000.0} and the blending parameter ζ ∈ {0.1, 0.3, 0.5, 1.0}. The number of iterations was fixed to 100. For D-Flow, we adjusted the blending parameter for initialization α ∈ {0.1, 0.3, 0.5} and the regularization parameter λ ∈ {0.1, 0.01, 0.001}. We observed that the PSNR of the reconstruction did not consistently increase across iterations (the method does not always converge); thus, we finetuned the value of the last iteration while keeping it below 20 for computational efficiency. The number of iterations for the inner LBFGS optimization was set to 20, and the number of Euler step when solving the ODE to 5, as recommended in the paper [Ben-Hamu et al. (2024)](#).

For OT-ODE, we tuned the initial time t 0 ∈ {0.1, 0.2, 0.3, 0.4} and the type of learning step γ (either √ t or constant). The number of iterations was set to 100.

For Flow-Priors, we considered -as described in the paper-the two hyper-parameters η ∈ {10 -3 , 10 -2 , 10 -1 } and λ ∈ {10 2 , 10 3 , 10 4 , 10 4 } which respectively correspond to the step size for the gradient descent and the guidance weight (weight put on the data likelihood). The number of iterations was set to 100 and the number of iner iterations to K = 1.

Finally, for our method PnP-Flow, we adjusted the exponent in the learning rate α ∈ {0.01, 0.1, 0.3, 0.5, 0.8, 1.0} and the number of time steps N ∈ {100, 200, 500}. When increasing N beyond 100 resulted in less than a 0.2 dB improvement in PSNR, we set N = 100 for computational efficiency. We average the output of the denoising step in Algorithm 3 over 5 realizations of the interpolation step. To speed up the algorithm, this number can be reduced to 1 with only a minor impact on performance. 

![step Observing that computing prox γR corresponds to solving a Gaussian denoising problem with regularization γR, Venkatakrishnan et al. (2013) proposed to replace it with an off-the-shelf denoiser D : R d -→ R d , which can be independently designed or learned (see Algorithm 2). Many PnP-FBS methods, such as Meinhardt et al. (2017); Zhang et al. (2017; 2021); Sun et al. (2019); Terris et al. (2020); Hertrich et al. (2021); Hurault et al. (2022a); Tan et al. (2024) use neural network denoisers.]()

![Figure 2: Illustration of the interpolation step.]()

![Figure4: Comparison of image restoration methods on the AFHQ-Cat dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), random pixel inpainting (4th row), box-inpainting (5th row). N/A means "method not applicable".]()

![Figure 6: Comparison of restoration methods on the CelebA dataset, for a Gaussian deblurring task and for two different initializations: default initialization recommended for each method (1st row), initialization set to the zero image (2rd row).]()

![Figure 8: Results for random inpainting using PnP-Flow across different iterations (time steps) with corresponding PSNR values. As expected, in the early iterations, the output resembles a natural face but diverges from the noisy observation. PSNR improves progressively with each iteration.]()

![Figure Comparison of image restoration methods on the CelebA dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).]()

![Comparisons of state-of-the-art methods on different inverse problems on the dataset CelebA. Results are averaged across 100 test images.]()

![Comparisons of state-of-the-art methods on different inverse problems on the dataset AFHQ-Cat. Results are averaged across 100 test images.]()

![Time and memory metrics per image on the deblurring task on CelebA (128 × 128).]()

![Comparisons of state-of-the-art methods on different inverse problems on the dataset CelebA-HQ. Results are averaged across 100 test images.]()

![Hyper-parameters used for all methods on the CelebA dataset. The values were selected based on the highest PSNR on the validation split.Denoising Deblurring Super-res. Rand. inpaint. Box inpaint.]()

https://github.com/deepinv/deepinv

https://github.com/gnobitab/RectifiedFlow

https://github.com/samuro95/GSPnP

