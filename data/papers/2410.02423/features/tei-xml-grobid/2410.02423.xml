<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING</title>
				<funder ref="#_S3CWzdB #_QaMxFwg">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-03">3 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ségolène</forename><surname>Martin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><surname>Gagneux</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ENS de Lyon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Université</orgName>
								<address>
									<addrLine>Claude Bernard Lyon 1</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Hagemann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Steidl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technische Universität Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Inria, LIP</orgName>
								<address>
									<postCode>5668</postCode>
									<country>UMR</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PNP-FLOW: PLUG-AND-PLAY IMAGE RESTORATION WITH FLOW MATCHING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-03">3 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">F1BCF6268856451FF3713C5591E811B7</idno>
					<idno type="arXiv">arXiv:2410.02423v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm for solving imaging inverse problems. PnP methods leverage the strength of pretrained denoisers, often deep neural networks, by integrating them in optimization schemes. While they achieve state-of-the-art performance on various inverse problems in imaging, PnP approaches face inherent limitations on more generative tasks like inpainting. On the other hand, generative models such as Flow Matching pushed the boundary in image sampling yet lack a clear method for efficient use in image restoration. We propose to combine the PnP framework with Flow Matching (FM) by defining a time-dependent denoiser using a pre-trained FM model. Our algorithm alternates between gradient descent steps on the datafidelity term, reprojections onto the learned FM path, and denoising. Notably, our method is computationally efficient and memory-friendly, as it avoids backpropagation through ODEs and trace computations. We evaluate its performance on denoising, super-resolution, deblurring, and inpainting tasks, demonstrating superior results compared to existing PnP algorithms and Flow Matching based state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Image restoration aims to recover an unknown image x ∈ R d from a degraded observation y ∈ R m y = noisy <ref type="bibr">(Hx)</ref>, where H : R d → R m is a (linear) degradation operator and noisy describes the underlying noise model. Since the problem is usually ill-posed and high dimensional, its treatment is challenging. We assume that the image x is sampled from a random variable X ∈ R d with a density p X , and the observation y from a random variable Y ∈ R m with a density p Y . Then the maximum a posteriori estimator: searches for the value with the highest probability of the posterior arg max</p><formula xml:id="formula_0">x∈R d log p X|Y =y (x) = arg max x∈R d log p Y |X=x (y) + log p X (x) ,<label>(1)</label></formula><p>where the first term of the right hand side is the fidelity to the data and the second term represents the prior distribution of the image. Since p X is generally unknown, and in the absence of training data, it is standard to instead consider a regularized optimization problem of the form:</p><formula xml:id="formula_1">arg min x∈R d {F (x) + R(x)} ,<label>(2)</label></formula><p>where F (x) := -log p Y |X=x (y) and R : R d → R usually enforces some assumptions on the solution and ensures the existence of a (unique) minimizer. The regularized minimization problem in (2) can be efficiently handled using proximal splitting methods <ref type="bibr" target="#b8">(Boyd et al., 2011)</ref>.</p><p>Plug-and-Play (PnP) <ref type="bibr" target="#b53">(Venkatakrishnan et al., 2013)</ref> methods build upon the insight that the proximal step on the regularization term is effectively a denoising operation. They can be performed with nonlearned denoisers such as BM3D <ref type="bibr" target="#b9">(Chan et al., 2016)</ref>, however recently these were outperformed by neural network-based approaches <ref type="bibr" target="#b37">(Meinhardt et al., 2017;</ref><ref type="bibr" target="#b58">Zhang et al., 2017;</ref><ref type="bibr">2021)</ref>. While PnP methods have demonstrated considerable effectiveness, recent advances in generative models offer a more sophisticated framework for learning priors directly from data, surpassing the limitations of hand-crafted or neural denoisers.</p><p>In particular, generative models are used as regularizers in <ref type="bibr" target="#b7">Bora et al. (2017)</ref>; <ref type="bibr" target="#b2">Asim et al. (2020)</ref>; <ref type="bibr" target="#b0">Altekrüger et al. (2023)</ref>; <ref type="bibr" target="#b55">Wei et al. (2022)</ref>. Most of these references learn an invertible transport map T between a Gaussian latent distribution and the data distribution, using the change of variables formula to obtain the estimated prior p X as an explicit function of the known latent density and the map T . In particular the normalizing flow network with the Real-NVP <ref type="bibr" target="#b17">(Dinh et al., 2017)</ref> architecture is by default invertible and has an easy-to-evaluate log determinant of the Jacobian of T . However, this architecture yields rather poor results on large images. Therefore, recent work <ref type="bibr">(Ben-Hamu et al., 2024;</ref><ref type="bibr" target="#b60">Zhang et al., 2024;</ref><ref type="bibr" target="#b39">Pokle et al., 2024;</ref><ref type="bibr">Chung et al., 2023;</ref><ref type="bibr" target="#b45">Song et al., 2023)</ref> focuses more on learning diffusion models <ref type="bibr">(Song et al., 2021;</ref><ref type="bibr" target="#b22">Ho et al., 2020;</ref><ref type="bibr" target="#b43">Sohl-Dickstein et al., 2015)</ref> or Flow Matching models <ref type="bibr">(Lipman et al., 2023;</ref><ref type="bibr">Liu et al., 2023)</ref>, which do not have these architectural constraints and scale better to large images. The main idea is to establish a path between the latent and the target distribution, which can be optimized in a simulation-free way.</p><p>In this paper, we focus on Flow Matching models, which learn a velocity field v : [0, 1]×R d -→ R d going from the latent to the data distribution. Once the velocity field v is learned, sampling from the target distribution can be done by solving an ODE (Chen et al., 2018)</p><formula xml:id="formula_2">∂ t f (t, x) = v t (f (t, x)), f (0, x) = x,</formula><p>where x is sampled from the latent distribution.</p><p>However, using Flow Matching to regularize inverse problems such as image restoration tasks is nontrivial. Indeed, when directly solving the MAP problem (1) using the change of variables formula, one faces numerical challenges due to the backpropagation through the ODE. We circumvent this problem by integrating the implicit Flow Matching prior into a custom denoiser that we plug into a Forward-Backward algorithm.</p><p>Contributions. In this paper, we combine PnP methods with Flow Matching and propose a PnP Flow Matching algorithm. Our contributions are as follows:</p><p>• Inspired by optimal transport Flow Matching, we design a time-dependent denoiser based on a pre-trained velocity field v learned through Flow Matching. • This denoiser is integrated into an adapted Forward-Backward Splitting PnP framework, which cycles through a gradient step on the data-fidelity term, an interpolation step to reproject iterates onto flow trajectories, and a denoising step. • Our algorithm is simple to implement, requires no backpropagation through the network, and is highly efficient in both memory usage and execution time compared to existing Flow Matching-based methods. • We show that, on two datasets and across denoising, deblurring, inpainting, and superresolution tasks, our method consistently outperforms state-of-the-art Flow Matchingbased and PnP methods in terms of both PSNR and SSIM metrics. The code with all benchmark methods is available at: <ref type="url" target="https://github.com/annegnx/PnP-Flow">https://github.com/annegnx/PnP-Flow</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We next provide background on both Plug-and-Play algorithms and Flow Matching models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PLUG AND PLAY</head><p>PnP algorithms were introduced as extensions of proximal splitting methods like Forward-Backward Splitting (FBS) or the alternating direction method of multipliers (ADMM). These are first-order optimization algorithms for solving problem (2), alternating between a proximal step on the regularization and a gradient and/or proximal step on the data-fidelity function.</p><p>The proximal operator prox γR : R d → R d , γ &gt; 0 is defined by prox γR (y) := arg min x∈R d 1 2 ∥y -x∥ 2 + γR(x) . In this paper, we assume that the data-fidelity F is differentiable with Lipschitz continuous gradient latent data ground truth y G radient Step Interpolation Step PnP Step (a) t = 0 (b) t = 0.56 (c) t = 0.89 Figure 1: Our method on a 2D denoising task (σ = 1.5) with Gaussian distributions. An OT Flow Matching model is trained to sample from P 1 = N (m, s 2 Id), with m = 7 and s = 0.5. At each time step, it performs a standard gradient step on the datafit, followed by a projection onto flow trajectories at time t, and finally applies the time-dependent denoiser D t . and we focus on the FBS algorithm, corresponding to Algorithm 1. The convergence of FBS to a minimizer of F + R is guaranteed as long as the stepsize is chosen as γ ∈ (0, 1/Lip(∇F )), where Lip(∇F ) is the Lipschitz constant of ∇F (Combettes <ref type="bibr" target="#b15">&amp; Wajs, 2005)</ref>.</p><p>Algorithm 1: FBS</p><formula xml:id="formula_3">Initialization: x (0) ∈ R d , γ &gt; 0 for r = 0, 1, . . . do z (r+1) = x (r) -γ∇F (x (r) ) x (r+1) = prox γR (z (r+1) )</formula><p>Algorithm 2: PnP-FBS Most of these PnP-FBS algorithms do not converge <ref type="bibr" target="#b58">(Zhang et al., 2017;</ref><ref type="bibr" target="#b44">Sommerhoff et al., 2019)</ref> and convergence guarantees usually assume non-expansiveness of the denoiser <ref type="bibr" target="#b38">(Pesquet et al., 2021;</ref><ref type="bibr" target="#b42">Ryu et al., 2019;</ref><ref type="bibr">Hurault et al., 2022b</ref>). Yet, constraining the Lipschitz constant of a neural network, e.g., through weight clipping, spectral normalization or averaged operator constructions harms in general its expressiveness <ref type="bibr" target="#b42">(Ryu et al., 2019)</ref>.</p><formula xml:id="formula_4">Initialization: x (0) ∈ R d , γ &gt; 0 for r = 0, 1, . . . do z (r+1) = x (r) -γ∇F (x (r) ) x (r+1) = D(z (r+1) ) ▷ PnP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FLOW MATCHING</head><p>Let P 2 (R d ) denote the subspace of probability measures on R d having a finite second moment. Let P 0 denote the latent probability measure and P 1 the target probability measure (i.e., the data distribution). We denote • ♯ the push-forward operation. We denote Γ(P 0 , P 1 ) the set of couplings π ∈ P(R d × R d ) having marginals P i ∈ P 2 (R d ), i = 0, 1, and Γ o (P 0 , P 1 ) the set of optimal couplings minimizing the Wasserstein-2 distance <ref type="bibr" target="#b54">(Villani, 2008)</ref>. We now start with any coupling π ∈ Γ(P 0 , P 1 ).</p><p>The core idea behind Flow Matching is to define a specific target probability path t → P t , t ∈ [0, 1], between P 0 and P 1 . Let P t := (e t ) ♯ π, where the map e t (x 0 , x 1 ) := (1 -t)x 0 + tx 1 interpolates between (a latent sample) x 0 and (a data sample) x 1 . The path P t can be shown to be an absolutely continuous curve, so there exists a Borel vector field v : [0, 1] × R d → R d such that the curve satisfies the continuity equation</p><formula xml:id="formula_5">∂ t P t + ∇ • (P t v t ) = 0 (CE)</formula><p>in the sense of distributions <ref type="bibr" target="#b1">(Ambrosio et al., 2008)</ref>. In addition, there exists a solution f :</p><formula xml:id="formula_6">[0, 1] × R d → R d of the ODE ∂ t f (t, x) = v t (f (t, x)), f (0, x) = x (3)</formula><p>such that P t = f (t, •) ♯ P 0 . If f is known, then a sample x 1 from the target distribution P 1 can be drawn by first sampling x 0 ∼ P 0 and then defining</p><formula xml:id="formula_7">x 1 = f (1, x 0 ).</formula><p>The goal of Flow Matching is to learn the velocity field of the flow ODE (3). This learning process consists in minimizing the loss function</p><formula xml:id="formula_8">L FM (θ) := E t∼U [0,1],x∼Pt ∥v θ t (x) -v t (x)∥ 2 ,</formula><p>where v θ is parametrized by a neural network with weights θ. Unfortunately, in practice, the true velocity field v t (x) is not available. However, <ref type="bibr">Lipman et al. (2023)</ref> showed that minimizing L FM (θ) is equivalent to minimizing the Conditional Flow Matching (CFM) loss: L CFM (θ) = L FM (θ)+const, where</p><formula xml:id="formula_9">L CFM (θ) := E t∼U [0,1],(x0,x1)∼π ∥v θ t (e t (x 0 , x 1 )) -(x 1 -x 0 )∥ 2 . (<label>4</label></formula><formula xml:id="formula_10">)</formula><p>Minimizing this loss only requires sampling from the coupling π. For example, if π = P 0 × P 1 is the independent coupling, we only need samples from the latent and target distributions. If instead, π ∈ Γ o (P 0 , P 1 ) is an optimal coupling, then we can use a standard optimal transport solver to (approximately) sample from π, as proposed in <ref type="bibr">Pooladian et al. (2023)</ref>; <ref type="bibr" target="#b52">Tong et al. (2024)</ref>.</p><p>Straight-line flows Let v : [0, 1] × R d -→ R and let f be a solution to the flow ODE associated with v. Given (X 0 , X 1 ) ∼ π, we call (f, v) a straight-line Flow Matching pair connecting X 0 and X 1 if X t = f (t, X 0 ) almost surely, where X t is defined as X t := e t ((X 0 , X 1 )) = tX 1 + (1 -t)X 0 <ref type="bibr">(Liu et al., 2023;</ref><ref type="bibr">Pooladian et al., 2023)</ref>. Note that this directly implies</p><formula xml:id="formula_11">v t (f (t, X 0 )) = ∂ t f (t, X 0 ) = ∂ t X t = X 1 -X 0 almost surely.</formula><p>We will focus on such straight-line flows later in the paper. Straight or nearly straight paths are preferred since they represent the shortest distance between two points and can be simulated with fewer steps of an ODE solver. This has been explored in many recent works that want to speed up sampling, such as <ref type="bibr" target="#b29">Kornilov et al. (2024)</ref>; <ref type="bibr">Lee et al. (2023)</ref>; <ref type="bibr" target="#b56">Yang et al. (2024)</ref>. A particular case of straight-line pair is given by OT couplings <ref type="bibr">(Pooladian et al., 2023;</ref><ref type="bibr" target="#b52">Tong et al., 2024)</ref>. Indeed, for π ∈ Γ o (P 0 , P 1 ) with Monge map T : R d → R d such that X 1 = T (X 0 ), there exists a velocity field v for which the pair (P t , v t ) is a solution of the continuity equation (CE), and such that for all</p><formula xml:id="formula_12">x ∈ R d v t (f (t, x)) = T (x) -x,<label>(5)</label></formula><p>where f is the solution of the ODE (3), see <ref type="bibr" target="#b1">Ambrosio et al. (2008)</ref>; <ref type="bibr">Pooladian et al. (2023)</ref>; <ref type="bibr" target="#b10">Chemseddine et al. (2024)</ref> for the precise conditions. By integrating (5</p><formula xml:id="formula_13">), it is clear that (f, v) is a straight-line Flow Matching for the coupling (X 0 , T (X 0 )), since X t = (1-t)X 0 +tX 1 = (1-t)X 0 +tT (X 0 ) = X 0 + t 0 v s (f (s, X 0 ))ds = f (t, X 0 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PNP MEETS FLOW MATCHING</head><p>3.1 DENOISING OPERATOR FROM FLOW MATCHING Let X 0 ∼ P 0 and X 1 ∼ P 1 with joint distribution (X 0 , X 1 ) ∼ π. Assume we have access to a pre-trained velocity field v θ . Then we define, for t ∈ [0, 1], the following time-dependent denoiser</p><formula xml:id="formula_14">D t := Id + (1 -t)v θ t ,<label>(6)</label></formula><p>and we propose to use it within a PnP framework.</p><p>To motivate the choice of the denoiser in ( <ref type="formula" target="#formula_14">6</ref>), recall that for a fixed time t ∈ [0, 1], the minimizer v * t of the CFM loss (4) over all possible vector fields reads</p><formula xml:id="formula_15">v * t (x) = E[X 1 -X 0 |X t = x],</formula><p>where X t := e t (X 0 , X 1 ) = (1 -t)X 0 + tX 1 <ref type="bibr" target="#b4">(Benton et al., 2024;</ref><ref type="bibr">Liu et al., 2023)</ref>. Assume we are in the ideal case where v θ t = v * t . Then it follows that, for any</p><formula xml:id="formula_16">x ∈ R d and t ∈ [0, 1], D t (x) = x + (1 -t)v * t (x) = E (X0,X1)∼π [X t + (1 -t)(X 1 -X 0 )|X t = x] = E (X0,X1)∼π [X 1 |X t = x] .</formula><p>Hence, the operator D t can be understood at the best approximation of X 1 given the knowledge of X t , which is also used in <ref type="bibr" target="#b39">Pokle et al. (2024)</ref> and <ref type="bibr" target="#b60">Zhang et al. (2024)</ref>.</p><p>Just as standard PnP denoisers minimize the MSE loss between noisy and clean samples, the operator</p><formula xml:id="formula_17">D t is the minimizer of L 2 -problem min g E (X0,X1)∼π [||X 1 -g(X t )|| 2 ],</formula><p>projecting any noisy point taken along the path onto the target distribution. In particular, the following proposition holds, demonstrating that the best denoising performance is achieved with straight-line flows. Proposition 1. Assume v := v θ is continuous and assume that, given v, the Flow ODE (3) has a unique solution f . Then the denoising loss</p><formula xml:id="formula_18">E (X0,X1)∼π [∥D t (X t ) -X 1 ∥ 2 ] is equal to 0 for all t ∈ [0, 1], if and only if the couple (f, v) is a straight-line Flow Matching pair between X 0 and X 1 .</formula><p>The proof can be found in Appendix A.1.</p><p>As stated in Section 2.2, a special case of straight-line flows is given by OT Flow Matching for which we recover D t (X t ) = X 1 . Indeed, in this case, D t reduces to T (X 0 ) = X 1 , with T the Monge map between P 0 and P 1 . We next discuss whether diffusion models induce straight-line flows. Remark 2 (Flow Matching versus diffusion models). Contrary to OT Flow Matching, diffusion models, which are also flow ODE methods, do not generally induce straight paths. Indeed, during diffusion training, the target probability path takes the form Xt = α t X 0 + β t X 1 with α t ∈ (0, 1) and β t = 1 -α 2 t <ref type="bibr">(Song et al., 2021;</ref><ref type="bibr">Liu et al., 2023)</ref>, which clearly does not match the desired straight path X t = (1 -t)X 0 + tX 1 . The non-straightness of the flow generated by diffusion models is illustrated in <ref type="bibr">Liu et al. (2023)</ref>; Liu (2022, Figure <ref type="figure">5</ref>). On the other hand, a non-straight path obtained with Flow Matching (for instance, using an independent coupling π) can be rectified to a straighter one following the procedure described in <ref type="bibr">Liu et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PNP FLOW MATCHING ALGORITHM</head><p>In the previous section, we built a denoiser D t (defined in ( <ref type="formula" target="#formula_14">6</ref>)). We now want to plug it in a Forward-Backward Splitting algorithm in order to solve inverse problems. Yet, our algorithm differs from the classical PnP-FBS (Algorithm 2) in two key aspects. First, the iterations of the our algorithm depend on time because of the definition of D t . Second, we introduce an intermediate reprojection step between the gradient step on the data-fidelity term and the denoising step. More precisely, at each time t ∈ [0, 1], given the current iterate x, our algorithm does the following updates: 1. Gradient step: a gradient step on the data-fidelity term, mapping x to z = x -γ∇F (x) for a given learning rate γ &gt; 0. 2. Interpolation step: In a standard Forward-Backward scheme, the denoiser operator is applied right after the gradient step. Yet, as discussed earlier, our operator D t was specifically designed to correctly denoise inputs drawn from the straight path X t = (1 -t)X 0 + tX 1 . If the output z from the gradient step at time t does not lie in the support of X t , there is a high chance that the denoising will not be effective, hence the need to "reproject" it along the flow paths before applying D t . To achieve this, we perform a linear interpolation on z, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>: at time t, we define z = (1 -t)ε + tz, where ε is a noise sample drawn from P 0 . Note that while ε is sampled from P 0 , it is not necessarily coupled to z ∼ P 1 via π. If it were, D t would map z directly back to z, annihilating the denoising effect. 3. PnP Denoising step: the operator D t is applied to the output z of the interpolation step, regularizing the current image by pushing it towards the distribution of X 1 .</p><p>The resulting discrete-time algorithm is given in Algorithm 3. Figure <ref type="figure">1</ref> illustrates the three steps of the algorithm on a denoising problem with a Gaussian prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3: PnP Flow Matching</head><p>Input: Pre-trained network v θ by Flow Matching, time sequence (t n ) n either finite with</p><formula xml:id="formula_19">t n = n/N , N ∈ N or infinite with lim n→+∞ t n = 1, adaptive stepsizes (γ n ) n . Initialize: x 0 ∈ R d . for n = 0, 1, . . . , do z n = x n -γ n ∇F (x n ). ▷ Gradient step on the data-fidelity term zn = (1 -t n )ε + t n z n , ε ∼ P 0 ▷ Linear interpolation x n+1 = D tn (z n ) ▷ PnP step with denoiser (6) return x n+1</formula><p>Remark 3 (Averaging in the denoising step). Instead of drawing one noise realization, we can also average over multiple samples ε ∼ P 0 in the last step of the algorithm:</p><formula xml:id="formula_20">x n+1 := E ε∼P0 [D tn (z n (ε))] with zn (ε) := (1 -t n )ε + t n z n .</formula><p>The algorithm's output is then deterministic. In practice, averaging over a few realizations slightly improves the numerical results.</p><p>Time dependent learning rate Using a constant learning rate independent of time can give too much importance to the data fit. For example, if γ t = 1 for all t ∈ [0, 1] on a simple denoising task, the algorithm will return the noisy sample y since D 1 = Id. To prevent this, γ t should decrease with t to balance the contributions of the datafit and the denoiser. We set γ t = (1 -t) α with α ∈ (0, 1] for the remainder of the paper. This choice yields good numerical results in our experiments, but alternative values for γ t could also be explored.</p><p>Convergence Assuming that the sequence produced by the algorithm is bounded in the infinite time regime, we have the following convergence result. Proposition 4. Assume that F : R d → R is continuously differentiable and that the learned vector field</p><formula xml:id="formula_21">v : [0, 1]×R d → R d is continuous. Let the time sequence (t n ) n∈N satisfy ∞ n=0 (1-t n ) &lt; +∞ and let γ n := 1 -t n , n ∈ N. If the sequence (x n ) n∈N obtained by Algorithm 3 is bounded, then it converges.</formula><p>The proof can be found in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Our approach combining PnP with Flow Matching relates to several existing methods.</p><p>Pre-trained Flow Matching methods Using pre-trained Flow Matching models for regularizing image inverse problems has been the focus of several recent works.</p><p>OT-ODE <ref type="bibr" target="#b39">(Pokle et al., 2024)</ref> assumes a Gaussian latent distribution and uses Tweedie's formula to derive a new velocity field ṽt (x, y</p><formula xml:id="formula_22">) = E[X 1 -X 0 | X t = x, Y = y] from the original velocity field v t (x) = E[X 1 -X 0 | X t = x],</formula><p>all without requiring retraining. In practice, they sample from the posterior distribution X 1 | Y by solving the ODE with the new velocity field using an Euler scheme.</p><p>In <ref type="bibr" target="#b60">Zhang et al. (2024)</ref>, the authors introduce Flow-Priors, a method to tackle the MAP problem by approximating it as a sequence of time-dependent MAP subproblems. Using Tweedie's formula, they show that for t &lt; 1, the gradient of the distribution P t of X t can be computed in closed form, allowing for the use of gradient descent to optimize these subproblems. However, the closed-form expression for the gradient relies on the assumption of an independent coupling π and a Gaussian latent distribution. Besides, their method requires computing Tr ∇v θ , which is expensive.</p><p>In <ref type="bibr">Ben-Hamu et al. (2024)</ref>, an implicit regularization approach called D-Flow is considered: instead of minimizing the Gaussian data-fidelity function x → ∥Hx -y∥ 2 , they minimize the latent loss z → ∥H(f (1, z)) -y∥ 2 , where f is a solution to the flow ODE given the pre-trained network. The two problems are theoretically equivalent since f (1, •) is invertible. However, since the latent loss is not convex, first-or second-order optimization methods may not find the global minimizer. Interestingly, this is beneficial because the true minimizer of the original problem is simply the pseudo-inverse, which may not be desirable. The authors optimize the latent loss by backpropagating through the ODE solution with a few Euler steps, though this remains computationally expensive.</p><p>PnP diffusion methods While we present the first PnP method based on Flow Matching, related works combine diffusion models with the PnP framework. To our knowledge, the following are the only methods using PnP with pre-trained generative models instead of standard denoisers.</p><p>In <ref type="bibr" target="#b61">Zhu et al. (2023)</ref>, a Half Quadratic Splitting algorithm is used, alternating between a proximal step on the data-fidelity term and a proximal step on the regularization. Following the PnP strategy, the proximal step for the regularization term is replaced with a denoising step using a pre-trained diffusion model. The denoiser they use is reminiscent of the one we use, where the velocity is replaced by the gradient of the score function. Their method also includes an interpolation step with random noise, mapping the estimated image at each iteration back to the diffusion path.</p><p>Finally, conditional image restoration is explored in <ref type="bibr">(Graikos et al., 2022)</ref>, which uses a more relaxed definition of "plug and play". They integrate a pre-trained diffusion model under different conditions, leading to a variational objective similar to methods like <ref type="bibr">Mardani et al. (2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NUMERICAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">BASELINES</head><p>We benchmark our method against three state-of-the-art Flow Matching-based restoration methods: OT-ODE <ref type="bibr" target="#b39">(Pokle et al., 2024</ref><ref type="bibr">), D-Flow (Ben-Hamu et al., 2024)</ref> and Flow Priors <ref type="bibr" target="#b60">(Zhang et al., 2024)</ref>. As no official implementations were publicly available for these methods, we developed our own based on the descriptions provided in their respective publications. We have made every effort to ensure faithful implementations, included in the code attached to this paper. We also benchmark our method against PnP-Diff <ref type="bibr" target="#b61">(Zhu et al., 2023)</ref>, a PnP algorithm based on diffusion models. Additionally, we compare our approach with the state-of-the-art PnP-FBS <ref type="bibr">(Hurault et al., 2022a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">EXPERIMENTAL SETUP</head><p>Datasets We evaluate all methods on two datasets: CelebA <ref type="bibr" target="#b57">(Yang et al., 2015)</ref>, with images resized to 128×128, and AFHQ-Cat, a subset of the Animal FacesHQ dataset <ref type="bibr" target="#b13">(Choi et al., 2020)</ref> focused on the cat class, with images resized to 256 × 256. All images are normalized to the range [-1, 1]. For CelebA, we use the standard training, validation, and test splits. For AFHQ-Cat, as no validation split is provided, we randomly select 32 images from the test set to create a validation set.</p><p>Models For each dataset, we trained a Flow Matching model from scratch using the Mini Batch OT Flow Matching approach <ref type="bibr" target="#b52">(Tong et al., 2024)</ref>, as this choice of coupling usually leads to straight paths. We used a standard Gaussian as the latent distribution and a U-Net <ref type="bibr" target="#b41">(Ronneberger et al., 2015)</ref> taken from <ref type="bibr" target="#b23">Huang et al. (2021)</ref>; <ref type="bibr" target="#b22">Ho et al. (2020)</ref> as the model. The training parameters were a learning rate of 10 -4 , 200 epochs with a batch size of 128 for CelebA, and 400 epochs with a batch size of 64 for AFHQ-Cat. We train the denoising network for the PnP method PnP-GS <ref type="bibr">(Hurault et al., 2022a)</ref> employing the same U-Net architecture. For PnP-Diff, because training a diffusion model with the same U-Net architecture as Flow-Matching yielded poor results due to insufficient number parameters, we used the pre-trained model from <ref type="bibr">Choi et al. (2021)</ref>, implemented in the DeepInv library<ref type="foot" target="#foot_0">foot_0</ref>  <ref type="bibr" target="#b49">(Tachella et al., 2023)</ref>. Note that the pre-trained diffusion model was trained on the FFHQ dataset <ref type="bibr" target="#b28">(Karras et al., 2019)</ref>, making the comparison indirect, but we had no alternative.</p><p>Settings for the experiments We evaluate the methods using 100 test images across five restoration problems: denoising with Gaussian noise (σ = 0.2); deblurring using a 61×61 Gaussian kernel (σ b = 1.0 for CelebA, σ b = 3.0 for AFHQ-Cat); super-resolution (2× downsampling for CelebA, 4× for AFHQ-Cat); box-inpainting with a centered s × s mask (s = 40 for CelebA, s = 80 for AFHQ-Cat); and random pixel inpainting (70% masked pixels). For deblurring, super-resolution, and box-inpainting, we add Gaussian noise with σ = 0.05, and for random inpainting σ = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyper-parameters</head><p>We optimize the hyper-parameters for each method using a grid search on the validation set, selecting the configuration that yields the highest Peak Signal-to-Noise Ratio (PSNR). The optimal values identified for each dataset and problem scenario are detailed in Appendix A.8. Our proposed method has two hyper-parameters: the exponent α in the learning rate schedule γ n = (1 -t n ) α , and the number of uniformly spaced time steps was set to N = 100 for most experiments. We averaged the results of the denoising step over 5 realizations of the interpolation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">MAIN RESULTS</head><p>We report benchmark results for all methods across several restoration tasks, measuring average PSNR and Structural Similarity (SSIM) on 100 test images. To ensure reproducibility, all experiments were seeded. Results are presented in Table <ref type="table" target="#tab_1">1</ref> for CelebA and Table <ref type="table" target="#tab_2">2</ref> for AFHQ-Cat. "N/A" indicates cases where the method is inapplicable; for instance, PnP-GS <ref type="bibr">(Hurault et al., 2022a)</ref> is not designed for generative tasks like box inpainting, and PnP-Diff relies on a diffusion model trained on another dataset, making its evaluation on box inpainting inappropriate.</p><p>The tables show that our method consistently ranks first or second in both reconstruction metrics across all tasks and datasets. More importantly, it demonstrates stability across tasks, unlike other methods. For example, D-Flow and Flow-Priors perform well in box inpainting but struggle with denoising, while PnP-GS, PnP-Diff, and OT-ODE excel in denoising and deblurring but perform worse on pseudo-generative tasks.</p><p>In terms of visual quality (Fig. <ref type="figure">3</ref>, Fig. <ref type="figure" target="#fig_2">4</ref>, and Appendix A.7), our method produces realistic, artifactfree images, though sometimes slightly over-smoothed. While D-Flow generates realistic images, it occasionally suffers from hallucinations (e.g., eye color shifts in CelebA denoising tasks). Flow-Priors introduces noise and artifacts, while OT-ODE captures textures well but struggles with image generation. Finally, Appendix A.6 shows the progression of the reconstruction given by our method PnP-Flow with respect to time.   Computation time and memory All experiments in this section are conducted on a single NVIDIA RTX 6000 Ada Generation with 48GB RAM. We measure the averaged time per image to complete a deblurring task on the CelebA dataset. We also compute the peak GPU memory load per image. The results are averaged over 100 images (25 batches of 4 images each). We use the same settings as those used for reporting performance metrics on CelebA. Results are in Table <ref type="table" target="#tab_3">3</ref>.</p><p>Sensitivity to initialization Notably, our algorithm does not rely on a good initialization. By starting the algorithm at time t 0 = 0, the linear interpolation step initially outputs a pure noise, which is then given to the denoiser. As a result, the algorithm's performance is independent of the initialization. In Appendix A.4, we stress that this is not the case for other methods.</p><p>Choice of the latent distribution Our algorithm can be used with any latent distribution. This is in the spirit of Flow Matching models <ref type="bibr">(Lipman et al., 2023)</ref>, which do not rely on a Gaussian latent as opposed to diffusion models. In particular, recently there has been a trend of modelling categorical data with Flow Matching <ref type="bibr" target="#b6">(Boll et al., 2024;</ref><ref type="bibr">Stark et al., 2024;</ref><ref type="bibr" target="#b16">Davis et al., 2024)</ref>. Our method does not rely on a Gaussian latent, contrary to the other Flow Matching restoration methods.</p><p>As an example to illustrate the performance of our approach in a non Gaussian case, in Appendix A.3 we conduct an experiment with a Dirichlet latent distribution, inspired by <ref type="bibr">(Stark et al., 2024)</ref>.</p><p>Adaptability to any straight-line flows Our denoiser is rooted in the straight-line flow framework, which motivates our use of OT Flow Matching. However, other choices of FM models are possible. Notably, Rectified Flows <ref type="bibr">(Liu et al., 2023)</ref> are of particular interest, as the method described allows for the straightening of any flow model. In Appendix A.5, we show how our method performs similarly to what we observed in Section 5.3 using pre-trained Rectified Flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We introduced PnP-Flow Matching, and compared it to common Flow Matching and PnP methods.</p><p>A great strength of our method is its versatility: it requires few hyperparameters, uses minimal memory and computational resources, delivers very good performance across various inverse problems, and supports different latent distributions as well as flexible initialization. Regarding limitations, our reconstructions seem to be more on the smooth side, which relates to the denoising operation as an minimum mean squared estimator. This however is a common tradeoff in regularization of inverse problems <ref type="bibr" target="#b5">(Blau &amp; Michaeli, 2018)</ref>. Next, it would be fruitful to use our method for other types of measurement noise such as Poisson noise <ref type="bibr" target="#b26">(Hurault et al., 2023)</ref> and to make use of different latent distributions <ref type="bibr" target="#b6">(Boll et al., 2024;</ref><ref type="bibr" target="#b18">Gat et al., 2024;</ref><ref type="bibr">Stark et al., 2024)</ref> to model categorical distributions.</p><p>In particular, Dirichlet distributions are common in the field of biological/molecular data, which is a data domain to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 PROOF OF PROPOSITION 1</p><p>Proof. We have by the definition of the denoiser (6) (it was also observed in <ref type="bibr">Lee et al. (2023)</ref>)</p><formula xml:id="formula_23">E (X0,X1)∼π [∥D t (X t ) -X 1 ∥ 2 ] = (1 -t) 2 E (X0,X1)∼π [∥v t (X t ) -(X 1 -X 0 )∥ 2 ].</formula><p>If we assume that the denoising loss is zero, this yields for all t ∈ [0, 1) that v t (X t ) = X 1 -X 0 almost surely. By continuity this also follows for t = 1. By the same arguments as in <ref type="bibr">(Liu et al., 2023, Theorem 3.6</ref> iii)+ iv)) we have that both t → f (t, X 0 ) and t → X t are solution to the flow ODE initialized at X 0 , since</p><formula xml:id="formula_24">X t = X 0 + tv t (X t ) = X 0 + t 0 v s (X s )ds a.s.</formula><p>where we used that v is constant with time. This implies that ∂ t X t = v t (X t ) almost surely. Together with the uniqueness of the ODE solution, it follows that f (t, X 0 ) = X t almost surely.</p><p>On the other hand, if (f, v) is a straight-line Flow Matching pair connecting X 0 and X 1 , then we obtain that</p><formula xml:id="formula_25">v t (X t ) = v t (f (t, X 0 )) = ∂ t f (t, X 0 ) = ∂ t X t = X 1 -X 0 a.s.</formula><p>Therefore the denoising loss is zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PROOF OF PROPOSITION 4</head><p>We provide the proof for Proposition 4 here below.</p><p>Proof. By the algorithm and definition of D tn , we have</p><formula xml:id="formula_26">x n+1 = D tn (u n ) = u n + (1 -t n )v θ tn (u n )</formula><p>, where together with the definition of γ n ,</p><formula xml:id="formula_27">u n := (1 -t n )ε + t n (x n -γ n ∇F (x n )) = t n x n + (1 -t n ) (ε -t n ∇F (x n )) .</formula><p>Hence we obtain</p><formula xml:id="formula_28">∥x n+1 -x n ∥ = (1 -t n )∥ε -x n -t n ∇F (x n ) + (1 -t n )v θ tn (u n )∥.</formula><p>By assumption on F and v and since (x n ) n is bounded, the expression in the norm is bounded as well, say by M &gt; 0. Then, by assumption on t n , we conclude</p><formula xml:id="formula_29">∞ n=0 ∥x n+1 -x n ∥ = M ∞ n=0 (1 -t n ) &lt; ∞,</formula><p>so that (x n ) n is a Cauchy sequence and converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 NUMERICAL RESULTS USING A LATENT DIRICHLET DISTRIBUTION</head><p>One of the main practical advantages of Flow Matching over diffusion is that one can choose a latent distribution with is not Gaussian. Motivated by DNA design and other discrete data, there has been growing interest in using a Dirichlet latent distribution in Flow Matching <ref type="bibr">(Stark et al., 2024;</ref><ref type="bibr" target="#b6">Boll et al., 2024;</ref><ref type="bibr" target="#b16">Davis et al., 2024)</ref>. Here, we want to show that applying PnP-Flow with a Flow Matching model trained on a Dirichlet latent distribution still yields good reconstructions. For this, we use the MNIST dataset <ref type="bibr" target="#b30">(LeCun et al., 1998)</ref>, rescaling each image to lie on the simplex, and train a Flow Matching model. We then apply this model to inpainting and super-resolution tasks and present the results. Importantly, the goal of this experiment is not to achieve state-of-the-art performance, but to illustrate that our algorithm generalizes to different latent distributions.</p><p>For this, we train a Dirichlet Flow Matching model for 200 epochs with a standard OT Flow Matching loss and a Dirichlet distribution with parameters (1, ..., 1) ∈ R 784 . This latent distribution is also used in <ref type="bibr">Stark et al. (2024)</ref> and amounts to the uniform distribution on the 784-dimensional simplex.</p><p>Note that the interpolation step now involves Dirichlet noise instead of Gaussian, as this noise is drawn according to P 0 . We reconstruct our images with γ = 1 and 300 steps. Remarkably, almost no modifications for the algorithm are needed. In particular, the generated images x almost lie perfectly on the simplex without any normalization. We show the images in Figure <ref type="figure">5</ref>. We compare to <ref type="bibr">(Ben-Hamu et al., 2024)</ref>, where we adapt the regularization in the algorithm so that the optimized variable z lies on the simplex, i.e., we use the loss</p><formula xml:id="formula_30">L(z) = ∥y -H(f (1, z))∥ 2 + λ∥( i,j z i,j ) -1)∥ 2 ,</formula><p>for a single latent image z, where f (1, z) is realized doing 5 Euler steps using the mid point rule and λ is a regularization constant.</p><p>As one can see, the PnP Flow outperforms D-Flow on all tasks. Note that the other Flow Matching baselines <ref type="bibr" target="#b39">(Pokle et al., 2024;</ref><ref type="bibr" target="#b60">Zhang et al., 2024)</ref> are not usable since their algorithm heavily depends on Gaussian paths.</p><p>Figure <ref type="figure">5</ref>: Dirichlet Flow Matching experiment on Simplex-MNIST, for denoising (1st row), superresolution (2nd row), box-inpainting (3rd row). We measure the reconstruction error as the mean L2 distance (called MSE) between ground truth and reconstruction averaged over the 16 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 SENSITIVITY TO INITIALIZATION</head><p>Interestingly, our method is inherently independent of the algorithm's initialization. For the image restoration problems considered in this work, initialization is not so much of a concern because a reasonable starting point for the solution x is given by H ⊤ y, where y is the observation and H is the degradation operator. However, for more complex problems where H ⊤ y is far from resembling a natural image, such as in CT reconstruction <ref type="bibr" target="#b20">(Guo et al., 2016)</ref> or phase retrieval <ref type="bibr" target="#b35">(Manekar et al., 2020)</ref>, this property becomes much more relevant.</p><p>In contrast, competing methods are more sensitive to initialization and cannot be started from any value. As recommended in the paper, our implementation of OT-ODE is initialized with t 0 y + (1t 0 )ϵ where ϵ ∼ N (0, I d ) and t 0 is the initialization time. The latent variable in D-Flow is initialized as αT -1 (H ⊤ y) + (1 -α)ϵ, where T -1 is the reverse flow, ϵ ∼ N (0, I d ) is a random Gaussian noise, and α ∈ (0, 1) is a blending coefficient. Flow-Priors, on the other hand, is initialized with random noise and, like our method, does not depend on a "good" initialization.</p><p>In Figure <ref type="figure" target="#fig_3">6</ref>, we illustrate the impact of changing the standard initialization for all methods to a black image on a Gaussian deblurring task, comparing the robustness of each approach to poor initialization.</p><p>Clean Init OT-ODE D-Flow Flow-Priors PnP-Flow  <ref type="table" target="#tab_5">4</ref> and qualitative results on paintbrush inpainting are displayed in Figure <ref type="figure">7</ref>. Here, we provide additional visual results, where we take the same ground truth image for the different inverse problems, see Fig. <ref type="figure">9</ref> and Fig. <ref type="figure">10</ref> for CelebA dataset and Fig. <ref type="figure">11</ref> and Fig. <ref type="figure" target="#fig_1">12</ref> for AFHQ-cat dataset.</p><p>Clean Degraded OT-ODE Flow-Priors PnP-Flow Figure 7: Comparison of restoration methods on the CelebA-HQ dataset for paintbrush inpainting using Rectified Flow model. Noisy t = 0.0 t = 0.1 t = 0.2 t = 0.3 t = 0.4 PSNR: 11.23 8.23 13.98 18.01 21.02 23.13 t = 0.5 t = 0.6 t = 0.7 t = 0.8 t = 0.9 t = 1.0 25.80 27.35 29.23 31.42 34.40 36.78 For random inpainting, the PnP algorithm is the Half Quadratic Splitting with the parameters given in the paper <ref type="bibr">(Hurault et al., 2022a)</ref>. For denoising, we only apply the trained denoiser once, feeding it with the true noise level σ. For all other methods, we use Proximal Gradient Descent (PGD), as prescribed in <ref type="bibr">Hurault et al. (2022b)</ref>. Note that we do not constrain the Lipschitz constant of the denoiser. For the PGD case, we tune three hyper-parameters: the learning rate γ ∈ {0.99, 2.0} in the gradient step (which is related to the regularization parameter λ in the paper), the inertia parameter α ∈ {0.3, 0.5, 0.8, 1.0} which coresponds to the relaxed denoiser D α σ = αD σ + (1 -α)Id, and the factor σ f ∈ {1., <ref type="bibr">1.2, 1.5, 1.8, 2., 3., 4., 5</ref>.} so that the denoiser gets as noise map σ f × σ (where σ is the true noise level). We also considered the number of iterations as a hyperparameter that we tuned on the validation set (with maximum number of iterations fixed to 100).</p><p>For PnP-Diff, we tuned two parameters: the regularization parameter λ ∈ {1.0, 5.0, 10.0, 100.0, 1000.0} and the blending parameter ζ ∈ {0.1, 0.3, 0.5, 1.0}. The number of iterations was fixed to 100. For D-Flow, we adjusted the blending parameter for initialization α ∈ {0.1, 0.3, 0.5} and the regularization parameter λ ∈ {0.1, 0.01, 0.001}. We observed that the PSNR of the reconstruction did not consistently increase across iterations (the method does not always converge); thus, we finetuned the value of the last iteration while keeping it below 20 for computational efficiency. The number of iterations for the inner LBFGS optimization was set to 20, and the number of Euler step when solving the ODE to 5, as recommended in the paper <ref type="bibr">Ben-Hamu et al. (2024)</ref>.</p><p>For OT-ODE, we tuned the initial time t 0 ∈ {0.1, 0.2, 0.3, 0.4} and the type of learning step γ (either √ t or constant). The number of iterations was set to 100.</p><p>For Flow-Priors, we considered -as described in the paper-the two hyper-parameters η ∈ {10 -3 , 10 -2 , 10 -1 } and λ ∈ {10 2 , 10 3 , 10 4 , 10 4 } which respectively correspond to the step size for the gradient descent and the guidance weight (weight put on the data likelihood). The number of iterations was set to 100 and the number of iner iterations to K = 1.</p><p>Finally, for our method PnP-Flow, we adjusted the exponent in the learning rate α ∈ {0.01, 0.1, 0.3, 0.5, 0.8, 1.0} and the number of time steps N ∈ {100, 200, 500}. When increasing N beyond 100 resulted in less than a 0.2 dB improvement in PSNR, we set N = 100 for computational efficiency. We average the output of the denoising step in Algorithm 3 over 5 realizations of the interpolation step. To speed up the algorithm, this number can be reduced to 1 with only a minor impact on performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>step Observing that computing prox γR corresponds to solving a Gaussian denoising problem with regularization γR, Venkatakrishnan et al. (2013) proposed to replace it with an off-the-shelf denoiser D : R d -→ R d , which can be independently designed or learned (see Algorithm 2). Many PnP-FBS methods, such as Meinhardt et al. (2017); Zhang et al. (2017; 2021); Sun et al. (2019); Terris et al. (2020); Hertrich et al. (2021); Hurault et al. (2022a); Tan et al. (2024) use neural network denoisers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the interpolation step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Comparison of image restoration methods on the AFHQ-Cat dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), random pixel inpainting (4th row), box-inpainting (5th row). N/A means "method not applicable".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of restoration methods on the CelebA dataset, for a Gaussian deblurring task and for two different initializations: default initialization recommended for each method (1st row), initialization set to the zero image (2rd row).</figDesc><graphic coords="18,109.58,195.18,62.70,62.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Results for random inpainting using PnP-Flow across different iterations (time steps) with corresponding PSNR values. As expected, in the early iterations, the output resembles a natural face but diverges from the noisy observation. PSNR improves progressively with each iteration.</figDesc><graphic coords="19,108.00,388.06,65.85,65.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure</head><figDesc>Figure Comparison of image restoration methods on the CelebA dataset for the following tasks : denoising (1st row), Gaussian deblurring (2nd row), super-resolution (3rd row), free-form inpainting (4th row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of state-of-the-art methods on different inverse problems on the dataset CelebA. Results are averaged across 100 test images.</figDesc><table><row><cell></cell><cell>Denoising</cell><cell cols="2">Deblurring</cell><cell>Super-res.</cell><cell>Rand. inpaint.</cell><cell>Box inpaint.</cell></row><row><cell>Method</cell><cell>σ = 0.2</cell><cell cols="2">σ = 0.05, σb = 1.0</cell><cell>σ = 0.05, ×2</cell><cell>σ = 0.01, 70%</cell><cell>σ = 0.05, 40 × 40</cell></row><row><cell></cell><cell cols="2">PSNR SSIM PSNR</cell><cell>SSIM</cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR</cell><cell>SSIM</cell></row><row><cell>Degraded</cell><cell cols="2">20.00 0.348 27.67</cell><cell>0.740</cell><cell cols="3">7.527 0.012 11.82 0.197 22.12</cell><cell>0.742</cell></row><row><cell>PnP-Diff</cell><cell cols="2">31.00 0.883 32.49</cell><cell>0.911</cell><cell cols="2">31.20 0.893 31.43 0.917</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PnP-GS</cell><cell cols="2">32.45 0.908 33.65</cell><cell>0.924</cell><cell cols="2">30.69 0.889 28.45 0.848</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>OT-ODE</cell><cell cols="2">30.50 0.867 32.63</cell><cell>0.915</cell><cell cols="3">31.05 0.902 28.36 0.865 28.84</cell><cell>0.914</cell></row><row><cell>D-Flow</cell><cell cols="2">26.42 0.651 31.07</cell><cell>0.877</cell><cell cols="3">30.75 0.866 33.07 0.938 29.70</cell><cell>0.893</cell></row><row><cell>Flow-Priors</cell><cell cols="2">29.26 0.766 31.40</cell><cell>0.856</cell><cell cols="3">28.35 0.717 32.33 0.945 29.40</cell><cell>0.858</cell></row><row><cell cols="3">PnP-Flow (ours) 32.45 0.911 34.51</cell><cell>0.940</cell><cell cols="3">31.49 0.907 33.54 0.953 30.59</cell><cell>0.943</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of state-of-the-art methods on different inverse problems on the dataset AFHQ-Cat. Results are averaged across 100 test images.</figDesc><table><row><cell></cell><cell>Denoising</cell><cell cols="2">Deblurring</cell><cell>Super-res.</cell><cell>Rand. inpaint.</cell><cell>Box inpaint.</cell></row><row><cell>Method</cell><cell>σ = 0.2</cell><cell cols="2">σ = 0.05, σb = 3.0</cell><cell>σ = 0.05, ×4</cell><cell>σ = 0.01, 70%</cell><cell>σ = 0.05, 80 × 80</cell></row><row><cell></cell><cell cols="2">PSNR SSIM PSNR</cell><cell>SSIM</cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR</cell><cell>SSIM</cell></row><row><cell>Degraded</cell><cell cols="2">20.00 0.319 23.77</cell><cell>0.514</cell><cell cols="3">10.74 0.042 13.35 0.234 21.50</cell><cell>0.744</cell></row><row><cell>PnP-Diff</cell><cell cols="2">30.27 0.835 27.97</cell><cell>0.764</cell><cell cols="2">23.22 0.601 31.08 0.882</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>PnP-GS</cell><cell cols="2">32.34 0.895 27.33</cell><cell>0.749</cell><cell cols="2">21.86 0.619 29.61 0.855</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>OT-ODE</cell><cell cols="2">29.90 0.831 26.43</cell><cell>0.709</cell><cell cols="3">25.17 0.711 28.84 0.838 23.88</cell><cell>0.874</cell></row><row><cell>D-Flow</cell><cell cols="2">26.22 0.620 27.49</cell><cell>0.740</cell><cell cols="3">24.10 0.595 31.37 0.888 26.69</cell><cell>0.833</cell></row><row><cell>Flow-Priors</cell><cell cols="2">29.32 0.768 25.78</cell><cell>0.692</cell><cell cols="3">23.34 0.573 31.76 0.909 25.85</cell><cell>0.822</cell></row><row><cell cols="3">PnP-Flow (ours) 31.65 0.876 27.62</cell><cell>0.763</cell><cell cols="3">26.75 0.774 32.98 0.930 26.87</cell><cell>0.904</cell></row><row><cell cols="2">5.4 PRACTICAL ASPECTS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Time and memory metrics per image on the deblurring task on CelebA (128 × 128).</figDesc><table><row><cell>Method</cell><cell>Computation time</cell><cell>GPU peak memory load</cell></row><row><cell>OT-ODE</cell><cell>1.50s</cell><cell>0.65GB</cell></row><row><cell>Flow-Priors</cell><cell>16.01s</cell><cell>2.96GB</cell></row><row><cell>D-Flow</cell><cell>32.19s</cell><cell>5.91GB</cell></row><row><cell>PnP-Flow (ours)</cell><cell>3.40s</cell><cell>0.10GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparisons of state-of-the-art methods on different inverse problems on the dataset CelebA-HQ. Results are averaged across 100 test images.</figDesc><table><row><cell></cell><cell>Denoising</cell><cell cols="2">Deblurring</cell><cell cols="2">Super-res.</cell><cell>Rand. inpaint.</cell><cell>Box inpaint.</cell></row><row><cell>Method</cell><cell>σ = 0.2</cell><cell cols="2">σ = 0.05, σb = 3.0</cell><cell cols="2">σ = 0.05, ×4</cell><cell>σ = 0.01, 70%</cell><cell>σ = 0.05, 80 × 80</cell></row><row><cell></cell><cell cols="2">PSNR SSIM PSNR</cell><cell>SSIM</cell><cell cols="3">PSNR SSIM PSNR SSIM PSNR</cell><cell>SSIM</cell></row><row><cell>Degraded</cell><cell cols="2">20.00 0.277 23.94</cell><cell>0.514</cell><cell>9.98</cell><cell cols="2">0.040 12.28 0.197 22.14</cell><cell>0.726</cell></row><row><cell>OT-ODE</cell><cell cols="2">29.38 0.731 25.47</cell><cell>0.589</cell><cell cols="3">24.24 0.578 26.59 0.639 26.68</cell><cell>0.683</cell></row><row><cell>Flow-Priors</cell><cell cols="2">25.73 0.524 28.09</cell><cell>0.804</cell><cell cols="3">23.64 0.494 33.26 0.935 29.89</cell><cell>0.817</cell></row><row><cell cols="3">PnP-Flow (ours) 32.65 0.891 28.57</cell><cell>0.815</cell><cell cols="3">25.92 0.762 33.60 0.935 31.11</cell><cell>0.928</cell></row><row><cell cols="7">A.6 PROGRESSION OF THE PNP-FLOW RECONSTRUCTION WITH TIME</cell></row><row><cell cols="7">Figure 8 presents the progression of the reconstruction outputed by the PnP-Flow with respect to</cell></row><row><cell>time.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">A.7 ADDITIONAL VISUAL RESULTS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyper-parameters used for all methods on the CelebA dataset. The values were selected based on the highest PSNR on the validation split.Denoising Deblurring Super-res. Rand. inpaint. Box inpaint.</figDesc><table><row><cell>PnP-Diff</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ζ (blending)</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell>N/A</cell></row><row><cell>λ (regularization)</cell><cell>1.0</cell><cell>1000.0</cell><cell>100.0</cell><cell>1.0</cell><cell>N/A</cell></row><row><cell>PnP-GS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>γ (learning rate)</cell><cell>-</cell><cell>2.0</cell><cell>2.0</cell><cell>-</cell><cell>N/A</cell></row><row><cell>α (inertia param.)</cell><cell>1.0</cell><cell>0.3</cell><cell>1.0</cell><cell>-</cell><cell>N/A</cell></row><row><cell>σ f (factor for noise input)</cell><cell>1.0</cell><cell>1.8</cell><cell>3.0</cell><cell>-</cell><cell>N/A</cell></row><row><cell>n iter (number of iter.)</cell><cell>1</cell><cell>35</cell><cell>20</cell><cell>23</cell><cell>N/A</cell></row><row><cell>OT-ODE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>t 0 (initial time) γ</cell><cell>0.3 √ t</cell><cell>0.4 √ t</cell><cell>0.1 constant</cell><cell>0.1 constant</cell><cell>0.1 √ t</cell></row><row><cell>Flow-Priors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>λ (regularization)</cell><cell>100</cell><cell>1,000</cell><cell>10,000</cell><cell>10,000</cell><cell>10,000</cell></row><row><cell>η (learning rate)</cell><cell>0.01</cell><cell>0.01</cell><cell>0.1</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>D-Flow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>λ (regularization)</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>α (blending)</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>n iter (number of iter.)</cell><cell>3</cell><cell>7</cell><cell>10</cell><cell>20</cell><cell>9</cell></row><row><cell>PnP-Flow</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>α (learning rate factor)</cell><cell>0.8</cell><cell>0.01</cell><cell>0.3</cell><cell>0.01</cell><cell>0.5</cell></row><row><cell>N (Number of time steps)</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/deepinv/deepinv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/gnobitab/RectifiedFlow</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/samuro95/GSPnP</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements <rs type="person">Ségolène Martin</rs>'s work is carried out in the framework of the <rs type="programName">DFG funded Cluster of Excellence EXC 2046 MATH+</rs> (project <rs type="projectName">ID</rs>: <rs type="grantNumber">AA5-8</rs>). The funding period of the project is from January 2024 until December 2025.</p></div>
			</div>
			<div type="funding">
<div><p><rs type="programName">Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_S3CWzdB">
					<idno type="grant-number">AA5-8</idno>
					<orgName type="project" subtype="full">ID</orgName>
					<orgName type="program" subtype="full">DFG funded Cluster of Excellence EXC 2046 MATH+</orgName>
				</org>
				<org type="funding" xml:id="_QaMxFwg">
					<orgName type="program" subtype="full">Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow Clean Degraded PnP-Diff PnP-GS OT-ODE D-Flow Flow-Priors PnP-Flow</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ethics Statement Our method makes use of pre-trained generative models, such that the general concerns of such models apply. In particular, these generative models carry inherent biases and can potentially be misused. However, our work is foundational and we believe there are many relevant applications, such as medical imaging or scientific applications which outweigh the negative usages. Furthermore our work involves only small Flow Matching models, which carry very low risk of being used for malicious purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility Statement</head><p>We implemented all the baselines and release the code in the supplementary material. In Appendix A.8 we state the hyper-parameter search procedure and the found values. Further, all our theoretical results state the precise assumptions and contain full proofs.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">PatchNR: learning from very few images by patch normalizing flow regularization</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Altekrüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hertrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">64006</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gradient Flows in Metric Spaces and in the Space of Probability Measures</title>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Ambrosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Gigli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Savaré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lectures in Mathematics ETH Zürich</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Birkhäuser, 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invertible generative models for inverse problems: mitigating representation error and dataset bias</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">D-Flow: Differentiating through flows for controlled generation</title>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Error bounds for flow matching methods</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Deligiannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generative assignment flows for representing and learning joint distributions of discrete data</title>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Boll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gonzalez-Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Petra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Schnörr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.04527</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compressed sensing using generative models</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="537" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plug-and-play admm for image restoration: Fixed-point convergence and applications</title>
		<author>
			<persName><forename type="first">Xiran</forename><surname>Stanley H Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><forename type="middle">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Elgendy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="98" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jannis</forename><surname>Chemseddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hagemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wald</surname></persName>
		</author>
		<idno>arXiv:2024</idno>
		<title level="m">Conditional Wasserstein distances with applications in Bayesian OT flow matching</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ilvr: Conditioning method for denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjune</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8188" to="8197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion posterior sampling for general noisy inverse problems</title>
		<author>
			<persName><forename type="first">Hyungjin</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongsol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Thompson Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">Louis</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Signal recovery by proximal forward-backward splitting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><surname>Combettes</surname></persName>
		</author>
		<author>
			<persName><surname>Valérie R Wajs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1168" to="1200" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Petrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>İsmail İlkan Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avishek Joey</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Bose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14664</idno>
		<title level="m">Fisher flow matching for generative modeling over discrete data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Itai</forename><surname>Gat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neta</forename><surname>Shaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.15595</idno>
		<title level="m">Discrete flow matching</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diffusion models as plug-and-play priors</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Graikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nebojsa</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Danielle</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Iterative image reconstruction for limited-angle CT using optimized initial image</title>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongliang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linghong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5836410</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional proximal neural networks and plug-and-play algorithms</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hertrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Neumayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Steidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">631</biblScope>
			<biblScope unit="page" from="203" to="234" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusionbased generative models and score matching</title>
		<author>
			<persName><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22863" to="22876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gradient step denoiser for convergent plug-and-play</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hurault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Leclaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proximal denoiser for convergent plugand-play optimization with nonconvex regularization</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hurault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Leclaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>b</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convergent bregman plug-and-play image restoration for poisson inverse problems</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hurault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulugbek</forename><surname>Kamilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Leclaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Naumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="27251" to="27280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kornilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gasnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Korotin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13117</idno>
		<title level="m">Optimal flow matching: Learning straight trajectories in just one step</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimizing trajectory curvature of ODE-based generative models</title>
		<author>
			<persName><forename type="first">Sangyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flow matching for generative modeling</title>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14577</idno>
		<title level="m">Rectified flow: A marginal preserving approach to optimal transport</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flow straight and fast: Learning to generate and transfer data with rectified flow</title>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning initialized phase retrieval</title>
		<author>
			<persName><forename type="first">Raunak</forename><surname>Manekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Tayal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ju</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS) Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A variational perspective on solving inverse problems with diffusion models</title>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning proximal operators: Using denoising networks for regularizing inverse imaging problems</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1799" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning maximally monotone operators for image recovery</title>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Pesquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Repetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Terris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Wiaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1206" to="1237" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training-free linear image inverses via flows</title>
		<author>
			<persName><forename type="first">Ashwini</forename><surname>Pokle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Muckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Karrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multisample flow matching: Straightening flows with minibatch couplings</title>
		<author>
			<persName><forename type="first">Aram-Alexandre</forename><surname>Pooladian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Domingo-Enrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MIC-CAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Plug-andplay methods provably converge with properly trained denoisers</title>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Energy dissipation with plug-and-play priors</title>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Sommerhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pseudoinverse-guided diffusion models for inverse problems</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morteza</forename><surname>Mardani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dirichlet flow matching with applications to DNA sequence design</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICLR)</title>
		<imprint>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An online plug-and-play algorithm for regularized image reconstruction</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendt</forename><surname>Wohlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulugbek</forename><forename type="middle">S</forename><surname>Kamilov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="408" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">DeepInverse: A deep learning framework for inverse problems in imaging</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Tachella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Hurault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Terris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Provably convergent plug-and-play quasi-Newton methods</title>
		<author>
			<persName><forename type="first">Subhadip</forename><surname>Hong Ye Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carola-Bibiane</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Schönlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="785" to="819" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Building firmly nonexpansive convolutional neural networks</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Terris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Repetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Pesquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Wiaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving and generalizing flow-based generative models with minibatch optimal transport</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Malkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Huguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarrid</forename><surname>Rector-Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Plug-and-play priors for model based reconstruction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singanallur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendt</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><surname>Wohlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="945" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Optimal Transport -Old and New</title>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep unfolding with normalizing flow priors for inverse problems</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Van Gorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizeth</forename><surname>Gonzalez-Carabarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonina</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Ruud</surname></persName>
		</author>
		<author>
			<persName><surname>Van Sloun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2962" to="2971" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Ling</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minkai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.02398</idno>
		<title level="m">Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Beyond a Gaussian denoiser: Residual learning of deep CNN for image denoising</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Plug-and-play image restoration with deep denoiser prior</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6360" to="6376" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Flow priors for linear inverse problems via iterative corrupted trajectory matching</title>
		<author>
			<persName><forename type="first">Yasi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingshan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Leong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18816</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Denoising diffusion models for plug-and-play image restoration</title>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1219" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
