<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
				<funder>
					<orgName type="full">Emmy Noether grant of the Deutsche Forschungsgemeinschaft</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2009-07-22">22 Jul 2009</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jean</forename><surname>Hausser</surname></persName>
							<email>jean.hausser@unibas.ch</email>
						</author>
						<author>
							<persName><forename type="first">Korbinian</forename><surname>Strimmer</surname></persName>
							<email>strimmer@uni-leipzig.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bioinformatics, Biozentrum</orgName>
								<orgName type="institution">University of Basel</orgName>
								<address>
									<addrLine>Klingel- bergstr. 50/70</addrLine>
									<postCode>CH-4056</postCode>
									<settlement>Basel</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute for Medical Informatics, Statistics and Epidemiol- ogy</orgName>
								<orgName type="institution">University of Leipzig</orgName>
								<address>
									<addrLine>Härtelstr. 16-18</addrLine>
									<postCode>D-04107</postCode>
									<settlement>Leipzig</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-07-22">22 Jul 2009</date>
						</imprint>
					</monogr>
					<idno type="MD5">B58972C3E063B5BDF8F0D379BDFF0E44</idno>
					<idno type="arXiv">arXiv:0811.3579v3[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Entropy</term>
					<term>shrinkage estimation</term>
					<term>James-Stein estimator</term>
					<term>&quot;small n</term>
					<term>large p&quot; setting</term>
					<term>mutual information</term>
					<term>gene association network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a procedure for effective estimation of entropy and mutual information from small-sample data, and apply it to the problem of inferring highdimensional gene association networks. Specifically, we develop a James-Stein-type shrinkage estimator, resulting in a procedure that is highly efficient statistically as well as computationally. Despite its simplicity, we show that it outperforms eight other entropy estimation procedures across a diverse range of sampling scenarios and data-generating models, even in cases of severe undersampling. We illustrate the approach by analyzing E. coli gene expression data and computing an entropybased gene-association network from gene expression data. A computer program is available that implements the proposed shrinkage estimator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entropy is a fundamental quantity in statistics and machine learning. It has a large number of applications, for example in astronomy, cryptography, signal processing, statistics, physics, image analysis neuroscience, network theory, and bioinformaticssee, for example, <ref type="bibr" target="#b38">Stinson (2006)</ref>, <ref type="bibr" target="#b44">Yeo and Burge (2004)</ref>, <ref type="bibr" target="#b21">MacKay (2003)</ref> and <ref type="bibr" target="#b39">Strong et al. (1998)</ref>. Here we focus on estimating entropy from small-sample data, with applications in genomics and gene network inference in mind <ref type="bibr" target="#b22">(Margolin et al., 2006;</ref><ref type="bibr" target="#b24">Meyer et al., 2007)</ref>.</p><p>To define the Shannon entropy, consider a categorical random variable with alphabet size p and associated cell probabilities θ 1 , . . . , θ p with θ k &gt; 0 and ∑ k θ k =<ref type="foot" target="#foot_0">foot_0</ref>. Throughout the article, we assume that p is fixed and known. In this setting, the Shannon entropy in natural units is given by 1</p><formula xml:id="formula_0">H = - p ∑ k=1 θ k log(θ k ).</formula><p>(1)</p><p>In practice, the underlying probability mass function are unknown, hence H and θ k need to be estimated from observed cell counts y k ≥ 0.</p><p>A particularly simple and widely used estimator of entropy is the maximum likelihood (ML) estimator</p><formula xml:id="formula_1">ĤML = - p ∑ k=1 θML k log( θML k )</formula><p>constructed by plugging the ML frequency estimates</p><formula xml:id="formula_2">θML k = y k n<label>(2)</label></formula><p>into Eq. 1, with n = ∑ p k=1 y k being the total number of counts. In situations with n p, that is, when the dimension is low and when there are many observation, it is easy to infer entropy reliably, and it is well-known that in this case the ML estimator is optimal. However, in high-dimensional problems with n p it becomes extremely challenging to estimate the entropy. Specifically, in the "small n, large p" regime the ML estimator performs very poorly and severely underestimates the true entropy.</p><p>While entropy estimation has a long history tracing back to more than 50 years ago, it is only recently that the specific issues arising in high-dimensional, undersampled data sets have attracted attention. This has lead to two recent innovations, namely the NSB algorithm <ref type="bibr" target="#b26">(Nemenman et al., 2002)</ref> and the Chao-Shen estimator <ref type="bibr" target="#b2">(Chao and Shen, 2003)</ref>, both of which are now widely considered as benchmarks for the small-sample entropy estimation problem <ref type="bibr" target="#b43">(Vu et al., 2007)</ref>.</p><p>Here, we introduce a novel and highly efficient small-sample entropy estimator based on James-Stein shrinkage <ref type="bibr" target="#b12">(Gruber, 1998)</ref>. Our method is fully analytic and hence computationally inexpensive. Moreover, our procedure simultaneously provides estimates of the entropy and of the cell frequencies suitable for plugging into the Shannon entropy formula (Eq. 1). Thus, in comparison the estimator we propose is simpler, very efficient, and at the same time more versatile than currently available entropy estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Conventional Methods for Estimating Entropy</head><p>Entropy estimators can be divided into two groups: i) methods, that rely on estimates of cell frequencies, and ii) estimators, that directly infer entropy without estimating a compatible set of θ k . Most methods discussed below fall into the first group, except for the Miller-Madow and NSB approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Maximum Likelihood Estimate</head><p>The connection between observed counts y k and frequencies θ k is given by the multinomial distribution Prob(y 1 , . . . , y p ; θ 1 , . . . ,</p><formula xml:id="formula_3">θ p ) = n! ∏ p k=1 y k ! p ∏ k=1 θ y k k .</formula><p>(3)</p><p>Note that θ k &gt; 0 because otherwise the distribution is singular. In contrast, there may be (and often are) zero counts y k . The ML estimator of θ k maximizes the right hand side of Eq. 3 for fixed y k , leading to the observed frequencies θML</p><formula xml:id="formula_4">k = y k n with variances Var( θML k ) = 1 n θ k (1 -θ k ) and Bias( θML k ) = 0 as E( θML k ) = θ k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Miller-Madow Estimator</head><p>While θML k is unbiased, the corresponding plugin entropy estimator ĤML is not. First order bias correction leads to</p><formula xml:id="formula_5">ĤMM = ĤML + m &gt;0 -1 2n ,</formula><p>where m &gt;0 is the number of cells with y k &gt; 0. This is known as the Miller-Madow estimator <ref type="bibr" target="#b25">(Miller, 1955)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bayesian Estimators</head><p>Bayesian regularization of cell counts may lead to vast improvements over the ML estimator <ref type="bibr" target="#b0">(Agresti and Hitchcock, 2005)</ref>. Using the Dirichlet distribution with parameters a 1 , a 2 , . . . , a p as prior, the resulting posterior distribution is also Dirichlet with mean</p><formula xml:id="formula_6">θBayes k = y k + a k n + A ,</formula><p>where A = ∑ p k=1 a k . The flattening constants a k play the role of pseudo-counts (compare with Eq. 2), so that A may be interpreted as the a priori sample size. Some common choices for a k are listed in Tab. 1, along with references to the corresponding plugin entropy estimators,</p><formula xml:id="formula_7">ĤBayes = - p ∑ k=1 θBayes k log( θBayes k ).</formula><p>While the multinomial model with Dirichlet prior is standard Bayesian folklore <ref type="bibr" target="#b9">(Gelman et al., 2004)</ref>, there is no general agreement regarding which assignment of a k is best as noninformative prior-see for instance the discussion in <ref type="bibr" target="#b42">Tuyl et al. (2008)</ref> and <ref type="bibr" target="#b8">Geisser (1984)</ref>. But, as shown later in this article, choosing inappropriate a k can easily cause the resulting estimator to perform worse than the ML estimator, thereby defeating the originally intended purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">NSB Estimator</head><p>The NSB approach <ref type="bibr" target="#b26">(Nemenman et al., 2002)</ref> avoids overrelying on a particular choice of a k in the Bayes estimator by using a more refined prior. Specifically, a Dirichlet mixture prior with infinite number of components is employed, constructed such that the resulting prior over the entropy is uniform. While the NSB estimator is one of the best entropy estimators available at present in terms of statistical properties, using the Dirichlet mixture prior is computationally expensive and somewhat slow for practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Chao-Shen Estimator</head><p>Another recently proposed estimator is due to <ref type="bibr" target="#b2">Chao and Shen (2003)</ref>. This approach applies the Horvitz-Thompson estimator <ref type="bibr" target="#b14">(Horvitz and Thompson, 1952)</ref> in combination with the Good-Turing correction <ref type="bibr" target="#b10">(Good, 1953;</ref><ref type="bibr" target="#b29">Orlitsky et al., 2003)</ref> of the empirical cell probabilities to the problem of entropy estimation. The Good-Turing-corrected frequency estimates are θGT k = (1 -m 1 n ) θML k , a k Cell frequency prior Entropy estimator 0 no prior maximum likelihood 1/2 Jeffreys prior (Jeffreys, 1946) Krichevsky and Trofimov (1981) 1 Bayes-Laplace uniform prior Holste et al. (1998) 1/p Perks prior (Perks, 1947) Schürmann and Grassberger (1996) √</p><p>n/p minimax prior <ref type="bibr" target="#b41">(Trybula, 1958)</ref> Table <ref type="table">1</ref>: Common choices for the parameters of the Dirichlet prior in the Bayesian estimators of cell frequencies, and corresponding entropy estimators.</p><p>where m 1 is the number of singletons, that is, cells with y k = 1. Used jointly with the Horvitz-Thompson estimator this results in</p><formula xml:id="formula_8">ĤCS = - p ∑ k=1 θGT k log θGT k (1 -(1 -θGT k ) n )</formula><p>, an estimator with remarkably good statistical properties <ref type="bibr" target="#b43">(Vu et al., 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A James-Stein Shrinkage Estimator</head><p>The contribution of this paper is to introduce an entropy estimator that employs James-Stein-type shrinkage at the level of cell frequencies. As we will show below, this leads to an entropy estimator that is highly effective, both in terms of statistical accuracy and computational complexity. James-Stein-type shrinkage is a simple analytic device to perform regularized highdimensional inference. It is ideally suited for small-sample settings -the original estimator <ref type="bibr" target="#b15">(James and Stein, 1961)</ref> considered sample size n = 1. A general recipe for constructing shrinkage estimators is given in Appendix A. In this section, we describe how this approach can be applied to the specific problem of estimating cell frequencies.</p><p>James-Stein shrinkage is based on averaging two very different models: a highdimensional model with low bias and high variance, and a lower dimensional model with larger bias but smaller variance. The intensity of the regularization is determined by the relative weighting of the two models. Here we consider the convex combination</p><formula xml:id="formula_9">θShrink k = λt k + (1 -λ) θML k ,<label>(4)</label></formula><p>where λ ∈ [0, 1] is the shrinkage intensity that takes on a value between 0 (no shrinkage) and 1 (full shrinkage), and t k is the shrinkage target. A convenient choice of t k is the uniform distribution t k = 1 p . This is also the maximum entropy target. Considering that Bias( θML k ) = 0 and using the unbiased estimator Var( θML</p><formula xml:id="formula_10">k ) = θML k (1-θML k ) n-1 we obtain (cf. Appendix A) for the shrinkage intensity λ = ∑ p k=1 Var( θML k ) ∑ p k=1 (t k -θML k ) 2 = 1 -∑ p k=1 ( θML k ) 2 (n -1) ∑ p k=1 (t k -θML k ) 2 .</formula><p>(5)</p><p>Note that this also assumes a non-stochastic target t k . The resulting plugin shrinkage entropy estimate is</p><formula xml:id="formula_11">ĤShrink = - p ∑ k=1 θShrink k log( θShrink k ).<label>(6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>)</head><p>Remark 1:</p><p>There is a one to one correspondence between the shrinkage and the Bayes estimator.</p><p>If we write t k = a k A and λ = A n+A , then θShrink k = θBayes k . This implies that the shrinkage estimator is an empirical Bayes estimator with a data-driven choice of the flattening constants-see also <ref type="bibr" target="#b4">Efron and Morris (1973)</ref>. For every choice of A there exists an equivalent shrinkage intensity λ. Conversely, for every λ there exist an equivalent</p><formula xml:id="formula_12">A = n λ 1-λ .</formula><p>Remark 2:</p><formula xml:id="formula_13">Developing A = n λ 1-λ = n(λ + λ 2 + . . .)</formula><p>we obtain the approximate estimate Â = n λ, which in turn recovers the "pseudo-Bayes" estimator described in <ref type="bibr" target="#b5">Fienberg and Holland (1973)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3:</head><p>The shrinkage estimator assumes a fixed and known p. In many practical applications this will indeed be the case, for example, if the observed counts are due to discretization (see also the data example). In addition, the shrinkage estimator appears to be robust against assuming a larger p than necessary (see scenario 3 in the simulations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4:</head><p>The shrinkage approach can easily be modified to allow multiple targets with different shrinkage intensities. For instance, using the Good-Turing estimator <ref type="bibr" target="#b10">(Good, 1953;</ref><ref type="bibr" target="#b29">Orlitsky et al., 2003)</ref>, one could setup a different uniform target for the non-zero and the zero counts, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Comparative Evaluation of Statistical Properties</head><p>In order to elucidate the relative strengths and weaknesses of the entropy estimators reviewed in the previous section, we set to benchmark them in a simulation study covering different data generation processes and sampling regimes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Simulation Setup</head><p>We compared the statistical performance of all nine described estimators (maximum likelihood, Miller-Madow, four Bayesian estimators, the proposed shrinkage estimator (Eqs. 4-6), NSB und Chao-Shen) under various sampling and data generating scenarios:</p><p>• The dimension was fixed at p = 1000.</p><p>• Samples size n varied from 10, 30, 100, 300, 1000, 3000, to 10000. That is, we investigate cases of dramatic undersampling ("small n, large p") as well as situations with a larger number of observed counts.</p><p>The true cell probabilities θ 1 , . . . , θ 1000 were assigned in four different fashions, corresponding to rows 1-4 in Fig. <ref type="figure" target="#fig_0">1</ref>:</p><p>1. Sparse and heterogeneous, following a Dirichlet distribution with parameter a = 0.0007, 2. Random and homogeneous, following a Dirichlet distribution with parameter a = 1, 3. As in scenario 2, but with half of the cells containing structural zeros, and 4. Following a Zipf-type power law.</p><p>For each sampling scenario and sample size, we conducted 1000 simulation runs. In each run, we generated a new set of true cell frequencies and subsequently sampled observed counts y k from the corresponding multinomial distribution. The resulting counts y k were then supplied to the various entropy and cell frequencies estimators and the squared error ∑ 1000 i=k (θ k -θk ) 2 was computed. From the 1000 repetitions we estimated the mean squared error (MSE) of the cell frequencies by averaging over the individual squared errors (except for the NSB, Miller-Madow, and Chao-Shen estimators). Similarly, we computed estimates of MSE and bias of the inferred entropies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Summary of Results from Simulations</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> displays the results of the simulation study, which can be summarized as follows:</p><p>• Unsurprisingly, all estimators perform well when the sample size is large.</p><p>• The maximum likelihood and Miller-Madow estimators perform worst, except for scenario 1. Note that these estimators are inappropriate even for moderately large sample sizes. Furthermore, the bias correction of the Miller-Madow estimator is not particularly effective.</p><p>• The minimax and 1/p Bayesian estimators tend to perform slightly better than maximum likelihood, but not by much.</p><p>• The Bayesian estimators with pseudocounts 1/2 and 1 perform very well even for small sample sizes in the scenarios 2 and 3. However, they are less efficient in scenario 4, and completely fail in scenario 1.</p><p>• Hence, the Bayesian estimators can perform better or worse than the ML estimator, depending on the choice of the prior and on the sampling scenario.</p><p>• The NSB, the Chao-Shen and the shrinkage estimator all are statistically very efficient with small MSEs in all four scenarios, regardless of sample size.</p><p>• The NSB and Chao-Shen estimators are nearly unbiased in scenario 3.</p><p>The three top-performing estimators are the NSB, the Chao-Shen and the prosed shrinkage estimator. When it comes to estimating the entropy, these estimators can be considered identical for practical purposes. However, the shrinkage estimator is the only one that simultaneously estimates cell frequencies suitable for use with the Shannon entropy formula (Eq. 1), and it does so with high accuracy even for small samples. In comparison, the NSB estimator is by far the slowest method: in our simulations, the shrinkage estimator was faster by a factor of 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application to Statistical Learning of Nonlinear Gene Association Networks</head><p>In this section we illustrate how the shrinkage entropy estimator can be applied to the problem of inferring regulatory interactions between genes through estimating the nonlinear association network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">From Linear to Nonlinear Gene Association Networks</head><p>One of the aims of systems biology is to understand the interactions among genes and their products underlying the molecular mechanisms of cellular function as well as how disrupting these interactions may lead to different pathologies. To this end, an extensive literature on the problem of gene regulatory network "reverse engineering" has developed in the past decade <ref type="bibr" target="#b7">(Friedman, 2004)</ref>. Starting from gene expression or proteomics data, different statistical learning procedures have been proposed to infer associations and dependencies among genes. Among many others, methods have been proposed to enable the inference of large-scale correlation networks <ref type="bibr" target="#b1">(Butte et al., 2000)</ref> and of high-dimensional partial correlation graphs <ref type="bibr" target="#b3">(Dobra et al., 2004;</ref><ref type="bibr" target="#b33">Schäfer and Strimmer, 2005a;</ref><ref type="bibr" target="#b23">Meinshausen and Bühlmann, 2006)</ref>, for learning vectorautoregressive (Opgen-Rhein and Strimmer, 2007a) and state space models <ref type="bibr" target="#b32">(Rangel et al., 2004;</ref><ref type="bibr" target="#b19">Lähdesmäki and Shmulevich, 2008)</ref>, and to reconstruct directed "causal" interaction graphs <ref type="bibr" target="#b17">(Kalisch and Bühlmann, 2007;</ref><ref type="bibr" target="#b28">Opgen-Rhein and Strimmer, 2007b)</ref>.</p><p>The restriction to linear models in most of the literature is owed at least in part to the already substantial challenges involved in estimating linear high-dimensional dependency structures. However, cell biology offers numerous examples of threshold and saturation effects, suggesting that linear models may not be sufficient to model gene regulation and gene-gene interactions. In order to relax the linearity assumption and to capture nonlinear associations among genes, entropy-based network modeling was recently proposed in the form of the ARACNE <ref type="bibr" target="#b22">(Margolin et al., 2006)</ref> and MRNET <ref type="bibr" target="#b24">(Meyer et al., 2007)</ref> algorithms.</p><p>The starting point of these two methods is to compute the mutual information MI(X, Y) for all pairs of genes X and Y, where X and Y represent the expression levels of the two genes for instance. The mutual information is the Kullback-Leibler distance from the joint probability density to the product of the marginal probability densities:</p><formula xml:id="formula_14">MI(X, Y) = E f (x,y) log f (x, y) f (x) f (y) . (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>The mutual information (MI) is always non-negative, symmetric, and equals zero only if X and Y are independent. For normally distributed variables the mutual information is closely related to the usual Pearson correlation,</p><formula xml:id="formula_16">MI(X, Y) = - 1 2 log(1 -ρ 2 ).</formula><p>Therefore, mutual information is a natural measure of the association between genes, regardless whether linear or nonlinear in nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Estimation of Mutual Information</head><p>To construct an entropy network, we first need to estimate mutual information for all pairs of genes. The entropy representation</p><formula xml:id="formula_17">MI(X, Y) = H(X) + H(Y) -H(X, Y),<label>(8)</label></formula><p>shows that MI can be computed from the joint and marginal entropies of the two genes X and Y. Note that this definition is equivalent to the one given in Eq. 7 which is based on the Kullback-Leibler divergence. From Eq. 8 it is also evident that MI(X, Y) is the information shared between the two variables.</p><p>For gene expression data the estimation of MI and the underlying entropies is challenging due to the small sample size, which requires the use of a regularized entropy estimator such as the shrinkage approach we propose here. Specifically, we proceed as follows:</p><p>• As a prerequisite the data must be discrete, with each measurement assuming one of K levels. If the data are not already discretized, we propose employing the simple algorithm of <ref type="bibr" target="#b6">Freedman and Diaconis (1981)</ref>, considering the measurements of all genes simultaneously.</p><p>• Next, we estimate the p = K 2 cell frequencies of the K × K contingency table for each pair X and Y using the shrinkage approach (Eqs. 4 and 5). Note that typically the sample size n is much smaller than K 2 , thus simple approaches such as ML are not valid.</p><p>• Finally, from the estimated cell frequencies we calculate H(X), H(Y), H(X, Y) and the desired MI(X, Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mutual Information Network for E. Coli Stress Response Data</head><p>For illustration, we now analyze data from Schmidt-Heck et al. ( <ref type="formula">2004</ref>) who conducted an experiment to observe the stress response in E. Coli during expression of a recombinant protein. This data set was also used in previous linear network analyzes, for example, in <ref type="bibr" target="#b34">Schäfer and Strimmer (2005b)</ref>. The raw data consist of 4289 protein coding genes, on which measurements were taken at <ref type="bibr">0, 8, 15, 22, 45, 68, 90, 150, and 180</ref> minutes. We focus on a subset of G = 102 differentially expressed genes as given in <ref type="bibr" target="#b35">Schmidt-Heck et al. (2004)</ref>. Discretization of the data according to <ref type="bibr" target="#b6">Freedman and Diaconis (1981)</ref> yielded K = 16 distinct gene expression levels. From the G = 102 genes, we estimated MIs for 5151 pairs of genes. For each pair, the mutual information was based on an estimated 16 × 16 contingency table, hence p = 256. As the number of time points is n = 9, this is a strongly undersampled situation which requires the use of a regularized estimate of entropy and mutual information.</p><p>The distribution of the shrinkage estimates of mutual information for all 5151 gene pairs is shown in the left side of Fig. <ref type="figure" target="#fig_1">2</ref>. The right hand side depicts the distribution of mutual information values after applying the ARACNE procedure, which yields 112 gene pairs with nonzero MIs.</p><p>The model selection provided by ARACNE is based on applying the information processing inequality to all gene triplets. For each triplet, the gene pair corresponding to the smallest MI is discarded, which has the effect to remove gene-gene links that correspond to indirect rather than direct interactions. This is similar to a procedure used in graphical Gaussian models where correlations are transformed into partial correlations. Thus, both the ARACNE and the MRNET algorithms can be considered as devices to approximate the conditional mutual information <ref type="bibr" target="#b24">(Meyer et al., 2007)</ref>. As a result, the 112 nonzero MIs recovered by the ARACNE algorithm correspond to statistically detectable direct associations.</p><p>The corresponding gene association network is depicted in Fig. <ref type="figure" target="#fig_2">3</ref>. The most striking feature of the graph are the "hubs" belonging to genes hupB, sucA and nuoL. hupB is a well known DNA-binding transcriptional regulator, whereas both nuoL and sucA are key components of the E. coli metabolism. Note that a Lasso-type procedure (that implicitly limits the number of edges that can connect to each node) such as that of <ref type="bibr" target="#b23">Meinshausen and Bühlmann (2006)</ref> cannot recover these hubs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We proposed a James-Stein-type shrinkage estimator for inferring entropy and mutual information from small samples. While this is a challenging problem, we showed that our approach is highly efficient both statistically and computationally despite its simplicity.</p><p>In terms of versatility, our estimator has two distinct advantages over the NSB and Chao-Shen estimators. First, in addition to estimating the entropy, it also provides the underlying multinomial frequencies for use with the Shannon formula (Eq. 1). This is useful in the context of using mutual information to quantify non-linear pairwise dependencies for instance. Second, unlike NSB, it is a fully analytic estimator.</p><p>Hence, our estimator suggests itself for applications in large scale estimation problems. To demonstrate its application in the context of genomics and systems biology, we have estimated an entropy-based gene dependency network from expression data in E. coli. This type of approach may prove helpful to overcome the limitations of linear models currently used in network analysis.</p><p>In short, we believe the proposed small-sample entropy estimator will be a valuable contribution to the growing toolbox of machine learning and statistics procedures for high-dimensional data analysis.</p><p>The above estimator can be slightly generalized by shrinking towards the component average x = ∑ p k=1 x k rather than to zero, resulting in</p><formula xml:id="formula_18">μShrink k = λ x + (1 -λ )x k with estimated shrinkage intensity λ = p -3 ∑ p k=1 (x k -x) 2 .</formula><p>The James-Stein shrinkage principle is very general and can be put to to use in many other high-dimensional settings. In the following we summarize a simple recipe for constructing James-Stein-type shrinkage estimators along the lines of <ref type="bibr" target="#b34">Schäfer and Strimmer (2005b)</ref> and <ref type="bibr" target="#b27">Opgen-Rhein and Strimmer (2007a)</ref>.</p><p>In short, there are two key ideas at work in James-Stein shrinkage:</p><p>i) regularization of a high-dimensional estimator θ by linear combination with a lower-dimensional target estimate θTarget , and ii) adaptive estimation of the shrinkage parameter λ from the data by quadratic risk minimization.</p><p>A general form of a James-Stein-type shrinkage estimator is given by</p><formula xml:id="formula_19">θShrink = λ θTarget + (1 -λ) θ.<label>(9)</label></formula><p>Note that θ and θTarget are two very different estimators (for the same underlying model!). θ as a high-dimensional estimate with many independent components has low bias but for small samples a potentially large variance. In contrast, the target estimate θTarget is low-dimensional and therefore is generally less variable than θ but at the same time is also more biased. The James-Stein estimate is a weighted average of these two estimators, where the weight is chosen in a data-driven fashion such that θShrink is improved in terms of mean squared error relative to both θ and θTarget . A key advantage of James-Stein-type shrinkage is that the optimal shrinkage intensity λ can be calculated analytically and without knowing the true value θ, via </p><p>A simple estimate of λ is obtained by replacing all variances and covariances in Eq. 10 with their empirical counterparts, followed by truncation of λ at 1 (so that λ ≤ 1 always holds). Eq. 10 is discussed in detail in <ref type="bibr" target="#b34">Schäfer and Strimmer (2005b)</ref> and Opgen-Rhein and Strimmer (2007a). More specialized versions of it are treated, for example, in <ref type="bibr" target="#b20">Ledoit and Wolf (2003)</ref> for unbiased θ and in <ref type="bibr" target="#b40">Thompson (1968)</ref> (unbiased, univariate case with deterministic target). A very early version (univariate with zero target) even predates the estimator of James and Stein, see <ref type="bibr" target="#b11">Goodman (1953)</ref>. For the multinormal setting of <ref type="bibr" target="#b15">James and Stein (1961)</ref>, Eq. 9 and Eq. 10 reduce to the shrinkage estimator described in <ref type="bibr" target="#b37">Stigler (1990)</ref>.</p><p>James-Stein shrinkage has an empirical Bayes interpretation <ref type="bibr" target="#b4">(Efron and Morris, 1973)</ref>. Note, however, that only the first two moments of the distributions of θTarget and θ need to be specified in Eq. 10. Hence, James-Stein estimation may be viewed as a quasiempirical Bayes approach (in the same sense as in quasi-likelihood, which also requires only the first two moments).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Comparing the performance of nine different entropy estimators (maximum likelihood, Miller-Madow, four Bayesian estimators, the proposed shrinkage estimator, NSB und Chao-Shen) in four different sampling scenarios (rows 1 to 4). The estimators are compared in terms of MSE of the underlying cell frequencies (except for Miller-Madow, NSB, Chao-Shen) and according to MSE and Bias of the estimated entropies. The dimension is fixed at p = 1000 while the sample size n varies from 10 to 10000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Distribution of estimated mutual information values for all 5151 gene pairs of the E. coli data set. Right: Mutual information values after applying the ARACNE gene pair selection procedure. Note that the most MIs have been set to zero by the ARACNE algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mutual information network for the E. coli data inferred by the ARACNE algorithm based on shrinkage estimates of entropy and mutual information.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this paper we use the following conventions: log denotes the natural logarithm (not base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>or base 10), and we define 0 log 0 = 0.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially supported by an <rs type="funder">Emmy Noether grant of the Deutsche Forschungsgemeinschaft</rs> (to K.S.). We thank the anonymous referees and the editor for very helpful comments.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Recipe For Constructing James-Stein-type Shrinkage Estimators</head><p>The original James-Stein estimator <ref type="bibr" target="#b15">(James and Stein, 1961)</ref> was proposed to estimate the mean of a multivariate normal distribution from a single (n = 1!) vector observation. Specifically, if x is a sample from N p (µ, I) then James-Stein estimator is given by</p><p>Intriguingly, this estimator outperforms the maximum likelihood estimator μML k = x k in terms of mean squared error if the dimension is p ≥ 3. Hence, the James-Stein estimator dominates the maximum likelihood estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Computer Implementation</head><p>The proposed shrinkage estimators of entropy and mutual information, as well as all other investigated entropy estimators, have been implemented in R (R Development Core Team, 2008). A corresponding R package "entropy" was deposited in the R archive CRAN and is accessible at the URL <ref type="url" target="http://cran.r-project.org/web/packages/entropy/">http://cran.r-project.org/web/ packages/entropy/</ref> under the GNU General Public License.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference for categorical data analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agresti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Hitchcock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Meth. Appl</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="297" to="330" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discovering functional relationships between RNA expression and chemotherapeutic susceptibility using relevance networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Butte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="12182" to="12186" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of Shannon&apos;s index of diversity when there are unseen species</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environ. Ecol. Stat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse graphical models for exploring gene expression data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dobra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multiv. Anal</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="196" to="212" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stein&apos;s estimation rule and its competitors-an empirical Bayes approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="117" to="130" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simultaneous estimation of multinomial cell probabilities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="683" to="691" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the histogram as a density estimator: L2 theory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z. Wahrscheinlichkeitstheorie verw. Gebiete</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="453" to="476" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inferring cellular networks using probabilistic graphical models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page" from="799" to="805" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On prior distributions for binary trials</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geisser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="244" to="251" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<meeting><address><addrLine>Boca Raton</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The population frequencies of species and the estimation of population parameters</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="237" to="264" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple method for improving some estimators</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="114" to="117" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving Efficiency By Shrinkage</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H J</forename><surname>Gruber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Marcel Dekker, Inc</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayes&apos; estimators of generalized entropies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Große</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Herzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A: Math. Gen</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2551" to="2566" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation with quadratic loss</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Berkeley Symp</title>
		<meeting>Fourth Berkeley Symp<address><addrLine>Berkeley</addrLine></address></meeting>
		<imprint>
			<publisher>Univ. California Press</publisher>
			<date type="published" when="1961">1961</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="361" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An invariant form for the prior probability in estimation problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jeffreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Roc. Soc. (Lond.) A</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="453" to="461" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the PC-algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The performance of universal encoding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Krichevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Trofimov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="199" to="207" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning the structure of dynamic Bayesian networks from time series and steady state measurements</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lähdesmäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shmulevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="185" to="217" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved estimation of the covariance matrix of stock returns with an application to portfolio selection</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ledoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Empir. Finance</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="603" to="621" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Information Theory, Inference, and Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stolovitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dalla Favera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Califano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Suppl. 1):S7</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-dimensional graphs and variable selection with the Lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information-theoretic inference of large transcriptional regulatory networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kontos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lafitte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bontempi</surname></persName>
		</author>
		<idno type="DOI">10.1155/2007/79879</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Bioinf. Sys. Biol</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Note on the bias of information estimates</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory in Psychology II-B</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Quastler</surname></persName>
		</editor>
		<meeting><address><addrLine>Glencoe, IL</addrLine></address></meeting>
		<imprint>
			<publisher>Free Press</publisher>
			<date type="published" when="1955">1955</date>
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entropy and inference, revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate ranking of differentially expressed genes by a distribution-free shrinkage approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Opgen-Rhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2007">2007a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Opgen-Rhein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Systems Biology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2007">2007b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Always Good Turing: asymptotically optimal probability estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">302</biblScope>
			<biblScope unit="page" from="427" to="431" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Some observations on inverse probability including a new indifference rule</title>
		<author>
			<persName><forename type="first">W</forename><surname>Perks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inst. Actuaries</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="285" to="334" />
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">R: A language and environment for statistical computing</title>
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>R Development Core Team</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling T-cell activation using gene expression profiling and state space modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lioumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sotheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Falciani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1361" to="1372" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical Bayes approach to inferring large-scale gene association networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="754" to="764" />
			<date type="published" when="2005">2005a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Appl. Genet. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2005">2005b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reverse engineering of the stress response during expression of a recombinant protein</title>
		<author>
			<persName><forename type="first">W</forename><surname>Schmidt-Heck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guthke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toepfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Reischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duerrschmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EUNITE symposium</title>
		<meeting>the EUNITE symposium<address><addrLine>Aachen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Verlag Mainz</publisher>
			<date type="published" when="2004-06">2004. 10-12 June 2004</date>
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Entropy estimation of symbol sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schürmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="414" to="427" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Galtonian perspective on shrinkage estimators</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Stigler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="147" to="155" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Cryptography: Theory and Practice</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Entropy and information in neural spike trains</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>De Ruyter Van Steveninck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Letters</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="197" to="200" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Some shrinkage techniques for estimating the mean</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="113" to="122" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Some problems of simultaneous minimax estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Trybula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A comparison of Bayes-Laplace, Jeffreys, and other priors: the case of zero events</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tuyl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mengersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="40" to="44" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coverage-adjusted entropy estimation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Q</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4039" to="4060" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Maximum entropy modeling of short sequence motifs with applications to RNA splicing signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Burge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Biol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="377" to="394" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
