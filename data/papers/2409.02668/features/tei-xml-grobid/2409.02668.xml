<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C34D02CDD696DE68299D3BC973B6C370</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2-SNAPSHOT" ident="GROBID" when="2025-02-21T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTENTS</head><p>understanding of the algorithms made available to her in multiple machine learning packages and software, and that she will be able to assess their prerequisites and limitations, and to extend them and develop new algorithms. Note that, while adopting a presentation with a strong mathematical flavor, we will still make explicit the details of many important machine learning algorithms.</p><p>Unsurprisingly, the book will be more accessible to a reader with some background in mathematics and statistics. It assumes familiarity with basic concepts in linear algebra and matrix analysis, in multivariate calculus and in probability and statistics. We tried to place a limit at the use of measure theoretic tools, that are avoided up to a few exceptions, which are be localized and be accompanied with alternative interpretations allowing for a reading at a more elementary level.</p><p>The book starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability. It also introduces some measure theoretic terminology, and can be used as a reading guide for the sections that use these tools. This chapter is followed by two chapters offering background material on matrix analysis and optimization. The latter chapter, which is relatively long, provides necessary references to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc.</p><p>Chapter 4, which is also introductory, illustrates the bias-variance dilemma in machine learning through the angle of density estimation and motivates chapter 5 in which basic concepts for statistical prediction are provided. Chapter 6 provides an introduction to reproducing kernel theory and Hilbert space techniques that are used in many places, before tackling, with chapters 7 to 11, the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks.</p><p>Chapter 12, which presents sampling methods and an introduction to the theory of Markov chains, starts a series of chapters on generative models, and associated learning algorithms. Graphical models and described in chapters 13 to 15. Chapter 16 introduces variational methods for models with latent variables, with applications to graphical models in chapter 17. Generative techniques using deep learning are presented in chapter 18.</p><p>Chapters 19 to 21 focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds.</p><p>Subtracting F(x) to both sides (which is allowed since F(x) &lt; ∞) and dividing by µ yields f (µ) ≤ f (λ) .</p><p>If F is strictly convex, then, either</p><p>as soon as 0 &lt; µ &lt; λ, yielding f (µ) &lt; f (λ) .</p><p>Now consider the converse statement. By comparing the expression in (3.3) to that obtained with λ = 1, we find, for all x, y ∈ Ω</p><p>which is (3.2). Since F satisfies (3.2) in its domain, it is convex. If the function in (3.3) is increasing, then the inequality is strict for 0 &lt; λ &lt; 1 as soon as the lower bound is finite, and F is strictly convex.</p><p>Proof If x is a local minimum of F, then, obviously, x ∈ dom(F), and for any y ∈ R d and small enough µ &gt; 0, F(x) ≤ F((1µ)x + µy). Using the function in (3.3) for λ = µ and for λ = 1, we get</p><p>so that x is a global minimum.</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relative interior</head><p>If Ω is convex, then Ω and Ω (its topological interior and closure) are convex too (the easy proof is left to the reader). However, topological interiors of interesting convex sets are often empty, and a more adapted notion of relative interior is preferable.</p><p>Define the affine hull of a set Ω, denoted aff(Ω), as the smallest affine subset of R d that contains Ω. The vector space parallel to aff(Ω) (generated by all differences xy, x, y ∈ Ω) will be denoted --→ aff (Ω). Their dimension k, is the largest integer such that there exist x 0 , x 1 , . . . , x k ∈ Ω such that x 1x 0 , . . . , x kx 0 are linearly independent. Moreover, given these points, elements of the affine hull are defined through barycentric coordinates, yielding aff(Ω) = {x = λ (0) x 0 + • • • + λ (k) x k :, λ (0) + • • • + λ (k) = 1} .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contents</head><p>Preface 1 General Notation and Background Material 1.1 Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 Probability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 A Few Results in Matrix Analysis 2.1 Notation and basic facts . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 The trace inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Some matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Introduction to Optimization 3.1 Basic Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Unconstrained Optimization Problems . . . . . . . . . . . . . . . . . . 3.2.1 Conditions for optimality (general case) . . . . . . . . . . . . . 3.2.2 Convex sets and functions . . . . . . . . . . . . . . . . . . . . . 3.2.3 Relative interior . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 Derivatives of convex functions and optimality conditions . . . 3.2.5 Direction of descent and steepest descent . . . . . . . . . . . . 3.2.6 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.7 Line search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Stochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Stochastic approximation methods . . . . . . . . . . . . . . . . 3.3.2 Deterministic approximation and convergence study . . . . . . 3.3.3 The ADAM algorithm . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Constrained optimization problems . . . . . . . . . . . . . . . . . . . . 3.4.1 Lagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Convex constraints . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.4 Projected gradient descent . . . . . . . . . . . . . . . . . . . . . 3.5 General convex problems . . . . . . . . . . . . . . . . . . . . . . . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preface</head><p>Machine learning addresses the issue of analyzing, reproducing and predicting various mechanisms and processes observable through experiments and data acquisition. With the impetus of large technological companies in need of leveraging information included in the gigantic datasets that they produced or obtained through user data, with the development of new data acquisition techniques in biology, physics or astronomy, with the improvement of storage capacity and high-performance computing, this field has experienced an explosive growth over the past decades, in terms of scientific production and technological impact.</p><p>While it is being recognized in some places as a scientific discipline in itself, machine learning (which has received a few almost synonymic denominations across time, including artificial intelligence, machine intelligence or statistical learning), can also be seen as an interdisciplinary field interfacing techniques from traditional domains such as computer science, applied mathematics, and statistics. From statistics, and more specially nonparametric statistics, it borrows its main formalism, asymptotic results and generalization bounds. It also builds on many classical methods that have been developed for estimation and prediction. From computer science, it involves the construction and implementation of efficient algorithms, programming design and architecture. Finally, machine learning leverages classical methods from linear algebra and functional analysis, as well as from convex and nonlinear optimization, fields within which it had also provided new problems and discoveries. It forms a significant part of the larger field commonly called "data science," which includes methods for storing, sharing and managing data, the development powerful computer architectures for increasingly demanding algorithms, and, importantly, the definition of ethical limits and processes through which data should be used in the modern world. This book, which originates from lecture notes of a series of graduate course taught in the Department of Applied Mathematics and Statistics at Johns Hopkins University, adopts a viewpoint (or bias) mainly focused on the mathematical and statistical aspects of the subject. Its goal is to introduce the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used today. It is written with the hope to provide the reader with a deeper <ref type="bibr">Chapter 1</ref> General Notation and Background Material 1.1 Linear algebra 1. The set of all subsets of a given set A is denoted P (A). If A and B are two sets, the notation B A refers to the set of all functions f : A → B. In particular, R A is the space of real-valued functions, and forms a vector space. When A is finite, this space is finite dimensional and can be identified with R |A| , where |A| denotes the cardinality (number of elements) of A.</p><p>The indicator function of a subset C of A will be denoted 1 C : A → {0, 1}, with 1 C (x) = 1 if x ∈ C and 0 otherwise. We will sometimes write 1 x∈C for 1 C (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Elements of the d-dimensional</head><p>Euclidean space R d will be denoted with letters such as x, y, z, and their coordinates will be indexed as parenthesized exponents, so that</p><formula xml:id="formula_0">x =          </formula><p>x (1)  . . .</p><formula xml:id="formula_1">x (d)          </formula><p>(we will always identify element of R d with column vectors). We will not distinguish in the notation between "points" in R d , seen as an affine space, and "vectors" in R d , seen as a vector space. The vectors 0 d and 1 d will denote the d-dimensional vectors with all coordinates equal to 0 and 1, respectively. The identity matrix in R d will be denoted Id R d . The canonical basis of R d , provided by the columns of Id R d will be denoted e 1 , . . . , e d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>The Euclidean norm of a vector x ∈ R d is denoted |x| with |x| = (x (1) ) 2 + • • • + (x (d) ) 2 1/2 .</p><p>It will sometimes be denoted |x| 2 , identifying it as a member of the family of ℓ p norms |x| p = (x (1) ) p + • • • + (x (d) ) p 1/p (1.1) CHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL for p ≥ 1. One can also define |x| p for 0 &lt; p &lt; 1, using (1.1), but in this case one does not get a norm because the triangle inequality |x + y| p ≤ |x| p + |y| p is not true in general. The family is interesting, however, because it approximates, in the limit p → 0, the number of non-zero components of x, denoted |x| 0 , which is a measure of sparsity. Note that we also use the notation |A| to denote the cardinality (number of elements) of a set A, hopefully without risk of confusion.</p><p>While we use single bars (|x|) to represent norms of finite-dimensional vectors, we will use double bars (∥h∥) for infinite-dimensional objects. Entry (i, j) in a matrix A ∈ M m,d (R) will either be denoted A(i, j) or A (i) j . The rows of A will be denoted A (1) , . . . , A (m) and the columns A 1 , . . . , A m .</p><p>The operator norm of a matrix A ∈ M m,d is defined by </p><formula xml:id="formula_2">|A| op = max{|Ax| : x ∈ R d , |x| = 1}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>A k-multilinear mapping is a function a : (x 1 , . . . , x k ) → a(x 1 , . . . , x k ) defined on (R d ) k with values in R q which is linear in each of its variables. The mapping is symmetric if its value is unchanged after any permutation of the variables. If k = 2 and q = 1, one also says that a is a bilinear form. The norm of a k-multilinear mapping is defined as A symmetric bilinear form a is called positive semidefinite if a(x, x) ≥ 0 for all x ∈ R d , and positive definite if it is positive semi-definite and a(x, x) = 0 if and only if x = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">TOPOLOGY</head><p>Symmetric bilinear forms can always be expressed in the form a(x, y) = x T Ay for some symmetric matrix A, and a is positive (semi-)definite if and only A is also. Analogous statements hold for negative (semi-)definite forms and matrices. We will use the notation A ≻ 0 (resp. ⪰ 0) to indicate that A is positive definite (resp. positive semidefinite). Note that, if a(x, y) = x T Ay for A ∈ S d , then |a| = |A| op .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Topology</head><p>1. The open balls in R d will be denoted B(x, r) = {y ∈ R d : |y -x| &lt; r}, with x ∈ R d and r &gt; 0. The closed balls are denoted B(x, r) and contain all y's such that |y -x| ≤ r. A set U ⊂ R d is open if and only if for any x ∈ U , there exists r &gt; 0 such that B(x, r) ⊂ U . A set Γ ⊂ R d is closed if its complement, denoted</p><formula xml:id="formula_3">Γ c = {x ∈ R d : x Γ }</formula><p>is open. The topological interior of a set A ⊂ R d is the largest open set included in A. It will be denoted either by Å or int(A). A point x belongs to Å if and only if B(x, r) ⊂ A for some r &gt; 0.</p><p>2. The closure of A is the smallest closed set that contains A and will be denoted either Ā or cl(A). A point x belongs to Ā if and only if B(x, r) ∩ A ∅ for all r &gt; 0. Alternatively, x belongs to Ā if and only if there exists a sequence (x k ) that converges to x with x k ∈ A for all k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A compact set in R d is a set Γ such that any sequence of points in Γ contains a subsequence that converges to some point in Γ . An alternate definition is that, whenever Γ is covered by a collection of open sets, there exists a finite subcollection that still covers Γ .</p><p>One can show that compact subsets of R d are exactly its bounded and closed subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A metric space is a space B equipped with a distance, i.e., a function ρ : B × B → [0, +∞) that satisfies the following three properties. subsets in metric spaces is the same as above, with ρ(x, y) replacing |x -y|, and one says that (x n ) converges to x if and only if ρ(x n , x) → 0.</p><p>Compact subsets are also defined in the same way, but are not necessarily characterized as bounded and closed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Calculus</head><p>1. If x, y ∈ R d , we will denote by [x, y] the closed segment delimited by x and y, i.e., the set of all points (1t)x + ty for 0 ≤ t ≤ 1. One denotes by [x, y), (x, y] and (x, y)</p><p>the semi-open or open segments, with appropriate strict inequality for t. (Similarly to the notation for open intervals, whether (x, y) denotes an open segment or a pair of points will always be clear from the context.)</p><p>2. The derivative of a differentiable function f : t → f (t) from an interval I ⊂ R to R will be denoted by ∂f , or ∂ t f if the variable t is well identified. Its value at t 0 ∈ I is denoted either as ∂f (t 0 ) or ∂f | t=t 0 . Higher derivatives are denoted as ∂ k f , k ≥ 0, with the usual convention ∂ 0 f = f . Note that notation such as f ′ , f ′′ , f (3) will never refer to derivatives.</p><p>In the following, U is an open subset of R d . If f is a function from U to R m , we let f (i) denote the i th component of f , so that</p><formula xml:id="formula_4">f (x) =           f (1) (x) . . . f (m) (x)           for x ∈ U . If d = 1</formula><p>, and f is differentiable, the derivative of f at x is the column vector of the derivatives of its components,</p><formula xml:id="formula_5">∂f (x) =          </formula><p>∂f (1) (x) . . . ∂f (m) (x)</p><formula xml:id="formula_6">         </formula><p>For d ≥ 1 and j ∈ {1, . . . , d}, the j th partial derivative of f at x is</p><formula xml:id="formula_7">∂ j f (x) = ∂(t → f (x + te j ))| t=0 ∈ R m ,</formula><p>where e 1 , . . . , e d form the canonical basis of R d . If the notation for the variables on which f depends is well understood from the context, we will alternatively use ∂ x j f . (For example, if f : (α, β) → f (α, β), we will prefer ∂ α f to ∂ 1 f .) The differential of f at x is the linear mapping from R d to R m represented by the matrix</p><formula xml:id="formula_8">df (x) = [∂ 1 f (x), . . . , ∂ d f (x)].</formula><p>It is defined so that, for all</p><formula xml:id="formula_9">h ∈ R d df (x)h = ∂(t → f (x + th))| t=0</formula><p>where the right-hand side is the directional derivative of f at x in the direction h. Note that, if f : R d → R (i.e., m = 1), df (x) is a row vector. If f is differentiable on U and df (x) is continuous as a function of x, one says that f is continuously differentiable, or C 1 .</p><p>Differentials obey the product rule and the chain rule. If f , g : U → R, then d(f g)(x) = f (x)dg(x) + g(x)df (x).</p><formula xml:id="formula_10">If f : U → R m , g : Ũ ⊂ R k → U , then d(f • g)(x) = df (g(x))dg(x).</formula><p>If d = m (so that df (x) is a square matrix), we let ∇•f (x) = trace(df (x)), the divergence of f .</p><p>The Euclidean gradient of a differentiable function f :</p><formula xml:id="formula_11">U → R is ∇f (x) = df (x) T .</formula><p>More generally, one defines the gradient of f with respect to a tensor field x → A(x) taking values in S ++ d , as the vector ∇ A f (x) that satisfies the relation</p><formula xml:id="formula_12">df (x)h = ∇ A f (x) T A(x)h</formula><p>for all h ∈ R d , so that</p><formula xml:id="formula_13">∇ A f (x) = A(x) -1 df (x) T . (<label>1.3)</label></formula><p>In particular, the Euclidean gradient is associated with A(x) = Id R d for all x. With some abuse of notation, we will denote ∇ A f = A -1 ∇f when A is a fixed matrix, therefore identified with the constant tensor field x → A.</p><p>3. We here compute, as an illustration and because they will be useful later, the differential of the determinant and the inversion in matrix spaces.</p><p>Recall that, if A = [a 1 , . . . , a d ] ∈ M d is a d by d matrix,, with a 1 , . . . , a d ∈ R d , det(A) is a d-linear form δ(a 1 , . . . , a d ) which vanishes when two columns coincide and such that δ(e 1 , . . . , e d ) = 1. In particular δ changes signs when two of its columns are inverted. It follows from this that ∂ a ij det(A) = δ(a 1 , . . . , a i-1 , e j , a j+1 , . . . , a d ) = (-1) i-1 δ(e j , a 1 , . . . , a i-1 , . . . , a d ) = (-1) i+j det A (ij) ,</p><p>where A (ij) is the matrix A with row i and column j removed. We therefore find that the differential of A → det(A) is the mapping</p><formula xml:id="formula_14">H → trace(cof(A) T H) (1.4)</formula><p>where cof(A) is the matrix composed of co-factors (-1) i+j det A (ij) . As a consequence, if A is invertible, then the differential of log | det(A)| is the mapping </p><formula xml:id="formula_15">H → trace(det(A) -1 cof(A) T H) = trace(A -1 H) (1.</formula><formula xml:id="formula_16">∂ i k • • • ∂ i 1 f (x) = ∂ i k (∂ i k-1 • • • ∂ i 1 f )(x)</formula><p>If all order k partial derivatives of f exist and are continuous, one says that f is k-times continuously differentiable, or C k and, when true, the order in which the derivatives are taken does not matter. In this case, one typically groups derivatives with the same order using a power notation, writing, for example</p><formula xml:id="formula_17">∂ 1 ∂ 2 ∂ 1 f = ∂ 2 1 ∂ 2 f</formula><p>for a C 3 function.</p><p>If f is C k , its k th differential at x is a symmetric k-multilinear map that can also be iteratively defined by (for h 1 , . . . , h k ∈ R d )</p><formula xml:id="formula_18">d k f (x)(h 1 , . . . , h k ) = d(d k-1 f (x)(h 1 , . . . , h k-1 ))h k ∈ R m .</formula><p>It is related to partial derivatives through the relation:</p><formula xml:id="formula_19">d k f (x)(h 1 , . . . , h k ) = d i 1 ,...,i k =1 h (i 1 ) 1 • • • h (i k ) k ∂ i k • • • ∂ i 1 f (x).</formula><p>When m = 1 and k = 2, one denotes by ∇ 2 f (x) = (∂ i ∂ j f (x), i, j = 1, . . . , n) the symmetric matrix formed by partial derivatives of order 2 of f at x. It is called the Hessian of f at x and satisfies</p><formula xml:id="formula_20">h T 1 ∇ 2 f (x)h 2 = d 2 f (x)(h 1 , h 2 ).</formula><p>The Laplacian of f is the trace of ∇ 2 f and denoted ∆f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Taylor's theorem, in its integral form, generalizes the fundamental theorem of calculus to higher derivatives. It expresses the fact that, if f is C k on U and x, y ∈ U are such that the closed segment [x, y] is included in U , then, letting h = yx:</p><formula xml:id="formula_21">f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 (k -1)! d k-1 f (x)(h, . . . , h) + 1 (k -1)! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt (1.7)</formula><p>The last term (remainder) can also be written as</p><formula xml:id="formula_22">1 k! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt 1 0 (1 -t) k-1 dt .</formula><p>If f takes scalar values, then d k f (x + th)(h, . . . , h) is real and the intermediate value theorem implies that there exists some z in [x, y] such that</p><formula xml:id="formula_23">f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 (k -1)! d k-1 f (x)(h, . . . , h) + 1 k! d k f (z)(h, . . . , h). (1.8)</formula><p>This is not true if f takes vector values. However, for any M such that</p><formula xml:id="formula_24">|d k f (z)| ≤ M for z ∈ [x, y] (such M's always exist because f is C k ), one has 1 (k -1)! 1 0 (1 -t) k-1 d k f (x + th)(h, . . . , h) dt ≤ M k! |h| k .</formula><p>Equation (1.7) can be written as</p><formula xml:id="formula_25">f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + 1 (k -1)! 1 0 (1 -t) k-1 (d k f (x + th)(h, . . . , h) -d k f (x)(h, • • • , h)) dt . (1.9) Let ϵ x (r) = max |d k f (x + h) -d k f (x)| : |h| ≤ r .</formula><p>Since d k f is continuous, ϵ x (r) tends to 0 when r → 0 and we have</p><formula xml:id="formula_26">1 0 (1 -t) k-1 |d k f (x + th)(h, . . . , h) -d k f (x)(h, • • • , h)| dt ≤ |h| k k ϵ x (|h|).</formula><p>This shows that (1.7) implies that</p><formula xml:id="formula_27">f (x + h) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + |h| k k! ϵ x (|h|) (1.10) = f (x) + df (x)h + 1 2 d 2 f (x)(h, h) + • • • + 1 k! d k f (x)(h, . . . , h) + o(|h| k ) (1.11)</formula><p>1.4 Probability theory 1. When discussing probabilistic concepts, we will make the convenient assumption that all random variables are defined on a fixed probability space (Ω, P). This means that Ω is large enough to include enough randomness to generate all required variables (and implicitly enlarged when needed).</p><p>We assume that the reader is familiar with concepts related to discrete random variables or continuous variables (with values in R d for some d) and their probability density functions, or p.d.f.'s. In particular, X : Ω → R d is a random variable with p.d.f. f if and only if the expectation of ϕ(X) is given by</p><formula xml:id="formula_28">E(ϕ(X)) = R d ϕ(x)f (x)dx</formula><p>for all bounded and continuous functions ϕ : R d → [0, +∞).</p><p>2. With a few exceptions, we will use capital letters for random variables and small letters for scalars and vectors that represent realizations of these variables. One of these exceptions will be our notation for training data, defined as an independent and identically distributed (i.i.d.) sample of a given random variable. A realization of such a sample will always be denoted T = (x 1 , . . . , x N ), which is therefore a series of observations. We will use the notation T = (X 1 , . . . , X N ) for the collection of i.i.d. random variables that generate the training set, so that T = (X 1 (ω), . . . , X N (ω)) = T (ω)</p><p>for some ω ∈ Ω. Another exception will apply to variables denoted using Greek letters, for which we will use boldface fonts (such as α, β, . . .).</p><p>For a random variable X, the notation [X = x], or [X ∈ A] refers to subsets of Ω, for example, [X = x] = {ω ∈ Ω : X(ω) = x} .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>As much as possible-but not always-we will avoid making explicit reference to measure theory, leaving to readers familiar with this theory the task to complete the notation and sometimes assumption gaps in order to make some of our statements fully rigorous.</p><p>However, there will be situations in which the flexibility of the measure-theoretic formalism is needed for the exposition. The following notions may help the reader navigate through these situations (basic references in measure theory are Rudin <ref type="bibr" target="#b189">[171]</ref>, Dudley <ref type="bibr" target="#b84">[66]</ref>, Billingsley <ref type="bibr" target="#b50">[32]</ref>).</p><p>A measurable space is a pair (S, S ) where S is a set and S ⊂ P (S) contains S, is stable by complementation (if A ∈ S , then A c = S \ A ∈ S ), by countable unions and intersections. Such an S is called a σ -algebra and elements of S form the measurable subsets of S (relative to the σ -algebra).</p><p>A (positive) measure µ on (S, S ) in a mapping from S → [0, +∞) that associates to A ∈ S its measure µ(A), such that the measure of a countable union of disjoint sets is the countable sum of their measures. A function f : Ω → R d is called measurable if the inverse images by f of open subsets of R d are mesurable. A measurable set A (or event) is negligible (for P) if P(A) = 0 and events are said to happen almost surely if their complements are negligible, i.e., P(A c ) = 0. 4. The integral of a function f : Ω → R d with respect to a measure (such as P) is denoted S f (x)µ(dx). This integral is defined, using a limit argument, as a function which is linear in f and such that</p><formula xml:id="formula_29">A µ(dx) = S 1 A (x)µ(dx) = µ(A).</formula><p>The Lebesgue measure, L d , on R d provides an important example. For this measure S is the σ -algebra generated by open subsets, R d f (x)L d (dx) extends the Riemann integral and is denoted R d f (x)dx. Another important example, when S is finite or countable, is the counting measure, denoted card, that return the number of elements of a set, so that card(A) = |A|. In this case, S = P (S) and the integral is simply the sum:</p><formula xml:id="formula_30">S f (x)card(dx) = x∈F f (x).</formula><p>5. If µ and ν are measures on (S, S ), one says that ν is absolutely continuous with respect to µ and write ν ≪ µ if, ∀A ∈ S : µ(A) = 0 ⇒ ν(A) = 0.</p><p>(1.12)</p><p>The Radon-Nikodym theorem states that ν ≪ µ if and only if ν has a density with respect to µ, i.e., there exists a measurable function ϕ : S → [0, +∞) such that S f (x)ν(dx) = S f (x)ϕ(x)µ(dx) for all measurable f : S → [0, +∞).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>If µ 1 is a measure on (S 1 , S 1 ) and µ 2 a measure on (S 2 , S 2 ), their tensor product is denoted µ 1 ⊗ µ 2 . It is a measure on S 1 × S 2 defined by</p><formula xml:id="formula_31">µ 1 ⊗ µ 2 (A 1 × A 2 ) = µ 1 (A 1 )µ 2 (A 2 )</formula><p>for A 1 ∈ S 1 and A 2 ∈ S 2 (the σ -algebra on S 1 × S 2 is the smallest one that contains all sets</p><formula xml:id="formula_32">A 1 × A 2 , A 1 ∈ S 1 , A 2 ∈ S 2 ).</formula><p>The integral, with respect to the product measure, of a function f :</p><formula xml:id="formula_33">S 1 × S 2 → R d is denoted S 1 ×S 2 f (x 1 , x 2 )µ 1 (dx 1 )µ 2 (dx 2 ) = S 1 ×S 2 f (x 1 , x 2 )µ 1 ⊗ µ 2 (dx 1 , dx 2 ).</formula><p>The tensor product between more that two measures is defined similarly, with notation</p><formula xml:id="formula_34">µ 1 ⊗ • • • ⊗ µ n = n k=1 µ k .</formula><p>7. When using measure-theoretic probability, we will therefore assume that the pair (Ω, P) is completed to a triple (Ω, A, P) where A is a σ -algebra and P a probability measure, that is a positive measure on (Ω, A) such that P(Ω) = 1. This triple is called a probability space.</p><p>A random variable X must then also take values in a measurable space, say (S, S ), and must be such that, for all C ∈ S , the set [X ∈ C] belongs to A. This justify the computation of P(X ∈ C), which will also be denoted P X (C).</p><p>A random variable X taking values in R d has a p.d.f. if and only if P X ≪ L d and the p.d.f. is the density provided by the Radon-Nikodym theorem. For a discrete random variable (i.e., taking values in a finite or countable set), the p.m.f. of X is also the density of P X with respect to the counting measure card.</p><p>If X is a random variable with values in R d , the integral of X with respect to P is the expectation of X, denoted E(X). More generally, if (S, S , P ) is a probability space, we will use the notation E P (f ) = S f (x)P (dx).</p><p>If P = P X for some random variable X : Ω → S, we will use E X rather than E P X .</p><p>8. One more technical consideration. Whenever we will consider measurable spaces, and sometimes without additional mention, we will assume that these spaces are complete metric spaces that have a dense countable subset (i.e., that are separable).</p><p>If not specified otherwise, their σ -algebras are given by the smallest ones containing all open sets (the Borel σ -algebra).</p><p>Chapter 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Few Results in Matrix Analysis</head><p>This chapter collects a few results in linear algebra that will be useful in the rest of this book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and basic facts</head><p>We denote by M n,d (R) the space of all n × d matrices with real coefficients <ref type="foot" target="#foot_0">1</ref> . For a matrix A ∈ M n,d (R) and integer k ≤ n and l ≤ d, we let A ⌈kl⌉ ∈ M k,l (R) denote the matrix A restricted to its first k rows and first l columns. The i, j entry of A will be denoted A(i, j) or A (ij) .</p><p>We assume that the reader is familiar with elementary matrix analysis, including, in particular the fact that symmetric matrices are diagonalizable in an orthonormal basis, i.e., if A ∈ M d,d (R) is a symmetric matrix (whose space is denoted S d ), there exists an orthogonal matrix U ∈ O d (i.e., satisfying U T U = U U T = Id R d ) and a diagonal matrix D ∈ M d,d (R) such that</p><formula xml:id="formula_35">A = U DU T .</formula><p>The identity AU = U D then implies that the columns of U form an orthonormal basis of eigenvectors of A.</p><formula xml:id="formula_36">If A ∈ S +</formula><p>d is positive semi-definite (i.e., u T Au ≥ 0 for all u ∈ R d ), the entries of D in the decomposition A = U DU T are non-negative, and one can define the matrix square root of A as S = U D ⊙1/2 U T where D ⊙1/2 is the diagonal matrix formed taking the square roots of all coefficients of D. We will use the notation S = A 1/2 . Note that D 1/2 = D ⊙1/2 if D is diagonal and positive semi-definite.</p><p>If A ∈ S ++ d is positive definite (i.e., A is positive semi-definite and u T Au = 0 implies u = 0) and B is positive semi-definite, both being d ×d matrices, the generalized CHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS eigenvalue problem associated with A and B consists in finding a diagonal matrix D and a matrix U such that BU = AU D and U T AU = Id R d . Letting Ũ = A 1/2 U , the problem is equivalent to solving A -1/2 BA -1/2 Ũ = Ũ D with Ũ T Ũ = Id R d , i.e., finding the eigenvalue decomposition of the symmetric positive-definite matrix A -1/2 BA -1/2 .</p><p>If A ∈ M n,d (R), it can be decomposed as T   where U ∈ O n (R) and V ∈ O d (R)) are orthogonal matrices and D ∈ M n,d (R) is diagonal (i.e., such that D(i, j) = 0 whenever i j) with non-negative diagonal coefficients. These coefficients are called the singular values of A, and the procedure is called a singular-value decomposition (SVD) of A. An equivalent formulation is that there exist orthonormal bases u 1 , . . . , u n of R n and v 1 , . . . , v d of R d (forming the columns of U and V ) such that Av i = λ i u i for i ≤ min(n, d), where λ 1 , . . . , λ min(n,d) are the singular values. Of course, if A is square and symmetric positive semi-definite, an eigenvalue decomposition of A is also a singular value decomposition (and the singular values coincide with the eigenvalues). More generally, if A = U DV T , then AA T = U DD T U T and A T A = V D T DV T are eigenvalue decompositions of AA T and A T A. Singular values are uniquely defined, up to reordering. However, the matrices U and V are not unique up to column reordering in general.</p><formula xml:id="formula_37">A = U DV</formula><p>If m = min(n, d), then, forming the matrices Ũ = U ⌈n,m⌉ (resp. Ṽ = V ⌈d,m⌉ ) by removing from U (resp. V ) its last nm (resp. dm) columns , and D = D ⌈m,m⌉ by removing from D its nm rows and dm columns, one has A = Ũ D Ṽ T with Ũ , D and Ṽ having respectively size n×m, m×m and m×d, Ũ T Ũ = Ṽ T Ṽ = Id R m and D diagonal with non-negative coefficients. This representation provides a reduced SVD of A and one can create a full SVD from a reduced one by completing the missing rows of Ũ and Ṽ to form orthogonal matrices, and by adding the required number of zeros to D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The trace inequality</head><p>We now descibe Von Neumann's trace theorem. Its justification follows the proof given in Mirsky <ref type="bibr" target="#b155">[137]</ref>. (2.1)</p><p>Moreover, if trace(A T B) = m i=1 λ i µ i , then there exist n × n and d × d orthogonal matrices U and V such that U T AV and U T BV are both diagonal, i.e., one can find SVDs of A and B in the same bases of R n and R d .</p><p>Proof We can assume without loss of generality that d ≤ n because, if the result holds for A and B, it also holds for A T and B T . Let A = U 1 ΛV T 1 and B = U 2 MV T 2 be the singular values decompositions of A and B (both Λ and M are n × d matrices). Then trace(</p><formula xml:id="formula_38">A T B) = trace(V 1 Λ T U T 1 U 2 MV 2 ) = trace(Λ T U MV T ) with U = U T 1 U 2 and V = V T 1 V 2 .</formula><p>Let u(i, j), 1 ≤ i, j ≤ n and v(i, j), 1 ≤ i, j ≤ d be the coefficients of the orthogonal matrices U and V . Then</p><formula xml:id="formula_39">trace(Λ T U MV T ) = d i,j=1 u(i, j)v(i, j)λ i µ j ≤ 1 2 d i,j=1 λ i µ j u(i, j) 2 + 1 2 d i,j=1</formula><p>λ i µ j v(i, j) 2  λ i µ j u(i, j</p><formula xml:id="formula_40">) 2 = d i,j=1 d i ′ =i ξ i ′ d j ′ =j η j ′ u(i, j) 2 = d i ′ ,j ′ =1 ξ i ′ η j ′ i ′ i=1 j ′ j=1 u(i, j) 2 ≤ d i ′ ,j ′ =1 ξ i ′ η j ′ min(i ′ , j ′ ) (2.3)</formula><p>where we used the fact that U is orthogonal, which implies that j ′ j=1 u(i, j) 2 and i ′ i=1 u(i, j) 2 are both less than 1. Notice also that, when u(i, j) = δ ij (i.e., u(i, j) = 1 if i = j and zero otherwise), then i ′ i=1 j ′ j=1 u(i, j) 2 = min(i ′ , j ′ ), so that the last inequality is an identity, and the chain of equalities leading to <ref type="bibr">(2.3)</ref> </p><formula xml:id="formula_41">implies d i ′ ,j ′ =1 ξ i ′ η j ′ min(i ′ , j ′ ) = d i=1 λ i µ j .</formula><p>We therefore obtain (for any U ), the fact that</p><formula xml:id="formula_42">d i,j=1 λ i µ j u(i, j) 2 ≤ d i=1 λ i µ j .</formula><p>The same identity obviously holds with v in place of u, and combining the two yields (2.1).</p><p>We now consider conditions for equality. Clearly, if one can find SVD decompositions of A and B with U 1 = U 2 and V 1 = V 2 , then U = Id R n , V = Id R d and (2.1) is an identity. We want to prove the converse statement.</p><p>For (2.1) to be an equality, we first need (2.2) to be an identity, which requires that u(i, j) = v(i, j) as soon as λ i µ j &gt; 0. We also need an equality in (2.3), which requires i ′ i=1 j ′ j=1 u(i, j) 2 = min(i ′ , j ′ ) as soon as λ i ′ &gt; λ i ′ +1 and µ j ′ &gt; µ j ′ +1 . The same identity must be true with v(i, j) replacing u(i, j)</p><p>In view of this, denote by i 1 &lt; • • • &lt; i p (resp. j 1 &lt; • • • &lt; j q ) the indexes at which the singular values of A (resp. B) differ form their successors, with the convention λ d+1 = µ d+1 = 0. Let, for k = 1, . . . , p and l = 1, . . . , q</p><formula xml:id="formula_43">C(k, l) = i k i=1 j l j=1 u(i, j) 2 .</formula><p>Then, we must have C(k, l) = min(i k , j l ) for all k, l and u(i, j) = v(i, j) for i = 1, . . . , i p and j = 1, . . . , j q .</p><p>If, for all i, j ≤ d, we let U ⌈ij⌉ be the matrix formed by the first i rows and j columns of U , the condition C kl = min(i k , j l ) requires that U ⌈i k j l ⌉ U T ⌈i k j l ⌉ = Id R i k if i k ≤ j l and U T ⌈i k j l ⌉ U ⌈i k j l ⌉ = Id R j l if j l ≤ i k . This shows that, if i k ≤ j l , the rows of U ⌈i k j l ⌉ form an orthonormal family, and necessarily, all elements u(i, j) for i ≤ i k and j &gt; j l vanish. The symmetric situation holds if j l ≤ i k .</p><p>Let r k = i ki k-1 and s l = j lj l-1 (with i 0 = j 0 = 0). We now consider possible changes in the SVDs of A and B. With our notation, the matrix Λ takes the form</p><formula xml:id="formula_44">Λ =                              λ i 1 Id R r 1 0 0 • • • 0 0 . . . 0 0 λ i 2 Id R r 2 0 • • • 0 0 . .</formula><p>. 0 . . . . . . . . . . . . . . . 0 0 . . . λ i p Id R r p 0 0 . . . 0 0 . . . 0 0 . . . 0 . . . . . . . . . . . . 0 . . . 0 0 . . . 0</p><formula xml:id="formula_45">                            </formula><p>Let W , W be n × n and d × d orthogonal matrices taking the form</p><formula xml:id="formula_46">W =                    W 1 0 0 • • • 0 0 W 2 0 • • • 0 . . . . . . . . . 0 0 . . . W p 0 0 . . . W p+1                    , W =                    W 1 0 0 • • • 0 0 W 2 0 • • • 0 . . . . . . . . . 0 0 . . . W p 0 0 . . . Wp+1                   </formula><p>where W 1 , . . . , W p are orthogonal with respective sizes r 1 , . . . , r p , W p+1 is orthogonal with size ni p and Wp+1 is orthogonal with size di p . Then we have</p><formula xml:id="formula_47">W D W = D</formula><p>proving that U 1 can be replaced by U 1 W provided that V 1 is replaced by V 1 W . Similar transformations can be made on U 1 and V 2 , with U 2 replaced by U 2 Z and V 2 by</p><formula xml:id="formula_48">V 2 Z with Z =                    Z 1 0 0 • • • 0 0 Z 2 0 • • • 0 . . . . . . . . . 0 0 . . . Z q 0 0 . . . Z q+1                    , Z =                    Z 1 0 0 • • • 0 0 Z 2 0 • • • 0 . . . . . . . . . 0 0 . . . Z q 0 0 . . . Zq+1                   </formula><p>with a structure similar to W and W , replacing r 1 , . . . , r p by s 1 , . . . , s q . As a consequence, U = U T 1 U 2 can be replaced by W T U Z and V by W T V Z. To complete the proof, we need to show that, when (2.1) is an equality, these matrices can be chosen so that W T U Z = Id R n and W T V Z = Id R d .</p><p>Let us consider a first step in this direction, assuming that i 1 ≤ j 1 so that</p><formula xml:id="formula_49">U [i 1 j 1 ] U T ⌈i 1 j 1 ⌉ = Id R i 1 .</formula><p>Complete U T ⌈i 1 j 1 ⌉ into a orthogonal matrix Z 1 = [U T ⌈i 1 j 1 ⌉ , Ũ ]. Build a matrix Z as above by taking Z 2 , . . . , Z q+1 equal to the identity. Then U Z has a first i 1 × i 1 block equal to Id R i 1 , which implies that all coefficients on the right and below this block are zeros. If j 1 ≤ i 1 , a similar construction can be made on the other side, letting W 1 = [U ⌈i 1 j 1 ⌉ Ũ ] with the first j 1 × j 1 block of the new matrix U equal to the identity. Note that, since V ⌈i p j q ⌉ = U ⌈i p j q ⌉ , the same result is obtained on V at the same time.</p><p>Pursuing this way (and skipping the formal induction argument, which is a bit tedious), we can progressively introduce identity blocks into U and V and transform them into new matrices (that we still denote by U and V ) taking the form (letting k = min(i p , j q ))</p><formula xml:id="formula_50">U = Id R k 0 0 Ū and V = Id R k 0 0 V</formula><p>If k = i p (resp. k = j q ), the final reduction can be obtained by choosing W p+1 = Ū and Wp+1 = V (resp. Z p+1 = Ū T and Zp+1 = V T ), leading to SVDs for A and B with identical matrices</p><formula xml:id="formula_51">U 1 = U 2 and V 1 = V 2 .</formula><p>■ Remark 2.2 Note that, since the singular values of -A and of A coincide, theorem 2.1 implies trace(A T B) ≤ m i=1 λ i µ i . <ref type="bibr">(2.4)</ref> for all matrices A and B, with equality if either A and B or -A and B have an SVD using the same bases. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Applications</head><p>Let p and d be integers with p ≤ d. Let A ∈ S d (R), B ∈ S p (R) be symmetric matrices. We consider the following optimization problem: maximize, over matrices U ∈ M d,p (R) such that U T U = Id R p , the function</p><formula xml:id="formula_52">F(U ) = trace(U T AU B) = trace(AU BU T ) .</formula><p>We first note that the singular values of U BU T , which is d × d, are the same as the eigenvalues of B completed with zeros. Letting λ 1 ≥ • • • ≥ λ d be the eigenvalues of A and µ 1 ≥ • • • ≥ µ p those of B, we therefore have, from theorem 2.1,</p><formula xml:id="formula_53">F(U ) ≤ p i=1</formula><p>λ i µ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">APPLICATIONS</head><p>Introduce the eigenvalue decompositions of A and B in the form A = V ΛV T and B = W MW T . For F(U ) to be equal to its upper-bound, we know that we must arrange U BU T to take the form</p><formula xml:id="formula_54">U BU T = V M 0 0 0 V T .</formula><p>Use, as before, the notation V ⌈dp⌉ to denote the matrix formed with the p first columns of V . Take U = V ⌈dp⌉ W T , which satisfies U T U = Id R p . We then have</p><formula xml:id="formula_55">V ⌈dp⌉ W T BW V T ⌈dp⌉ = V ⌈dp⌉ MV T ⌈dp⌉ = V M 0 0 0. V T ,</formula><p>which shows that U is optimal. We summarize this discussion in the next theorem.</p><p>Theorem 2.3 Let A ∈ S d (R) and B ∈ S p (R) be symmetric matrices, with p ≤ d. Let eigenvalue decompositions of A and B be given by A = V ΛV T and B = W MW T , where the diagonal elements of Λ (resp. M) are</p><formula xml:id="formula_56">λ 1 ≥ • • • ≥ λ d (resp. µ 1 ≥ • • • ≥ µ p ).</formula><p>Define F(U ) = trace(AU BU T ), for U ∈ M d,p (R). Then, max F(U ) :</p><formula xml:id="formula_57">U T U = Id R p = p i=1 λ i µ i .</formula><p>This maximum is attained at U = V ⌈d,p⌉ W T .</p><p>The following corollary applies theorem 2.3 with B = diag(µ 1 , . . . , µ p ). µ i e T i Ae i .</p><p>Then, the maximum of F over all orthonormal families e 1 , . . . , e p in R d is p i=1 λ i µ i and is attained when e 1 , . . . , e p are eigenvectors of A with eigenvalues λ 1 , . . . , λ p .</p><p>The minimum of F over all orthonormal families e 1 , . . . , e p in R d is p i=1 λ d-i+1 µ i and is attained when e 1 , . . . , e p are eigenvectors of A with eigenvalues λ d , . . . , λ d-p+1 .</p><p>Proof The statement about the maximum is just a special case of theorem 2.3, with B = diag(µ 1 , . . . , µ p ), noting that the ith diagonal element of U T AU is e T i Ae i where e i is the ith column of U .</p><p>The statement about the minimum is deduced by replacing A by -A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Applying this corollary with p = 1, we retrieve the elementary result that λ 1 = max{u T Au : |u| = 1} and λ d = min{u T Au : |u| = 1}.</p><p>To complete this chapter, we quickly state and prove Rayleigh's theorem. </p><formula xml:id="formula_58">λ k = max V :dim(V )=k min{u T Au, u ∈ V , |u| = 1} = min V :dim(V )=d-k+1 max{u T Au, u ∈ V , |u| = 1}</formula><p>where the min and max are taken over linear subspaces of R d .</p><p>Proof Let e 1 , . . . , e d be an orthonormal basis of eigenvectors of A associated with λ 1 , . . . , λ d . Let, for k ≤ l, W k,l = span(e k , . . . , e l ). Let V be a subspace of dimension k.</p><p>Then V ∩ W k,d ∅ (because the sum of the dimensions of these two spaces is d + 1). Taking u 0 with norm 1 in this intersection, we have</p><formula xml:id="formula_59">min{u T Au, u ∈ V , |u| = 1} ≤ u T 0 Au 0 ≤ max{u T Au, u ∈ W k,d , |u| = 1} = λ k ,</formula><p>where the last identity follows by considering the eigenvalues of A restricted to W k,d . So, the maximum of the right-hand side is indeed less than λ k , and it is attained for V = W 1,k . This proves the first identity, and the second one can be obtained by applying the first one to -A.</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Some matrix norms</head><p>The operator norm of a matrix A ∈ M n,d (R), is defined as</p><formula xml:id="formula_60">|A| op = max{|Ax| : x ∈ R d , |x| = 1}.</formula><p>It is equal to the square root of the largest eigenvalue of A T A, i.e., to the largest singular value of A.</p><p>The Frobenius norm of A is</p><formula xml:id="formula_61">|A| F = trace(A T A) = d i,j=1</formula><p>A(i, j) 2 , so that</p><formula xml:id="formula_62">|A| F =        m k=1 σ 2 k        1/2</formula><p>where σ 1 , . . . , σ m are the singular values of A (and m = min(n, d)).</p><p>The nuclear norm of A is defined by</p><formula xml:id="formula_63">|A| * = d k=1 σ k .</formula><p>One can prove that this is a norm using an equivalent definition, provided by the following proposition. Proof The fact that trace(U AV T ) ≤ |A| * for any U and V is a consequence of the trace inequality applied with B = [Id, 0] or its transpose depending on whether n ≤ d or not. The upper-bound being attained when U and V are the matrices forming the singular value decomposition of A, the proof is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The fact that |A| * is a norm, for which the only non-trivial fact was the triangular inequality, now is an easy consequence of this proposition, because the maximum of the sum of two functions is always less than the sum of their maximums. More precisely, we have |A + B| * = max{trace(U AV T ) + trace(U BV T ) :</p><formula xml:id="formula_64">U T U = Id, V T V = Id} ≤ max{trace(U AV T ) : U T U = Id, V T V = Id} + max{trace(U BV T ) : U T U = Id, V T V = Id} = |A| * + |B| *</formula><p>The nuclear norms is also called the Ky Fan norm of order d. Ky Fan norms of order k (for 1 ≤ k ≤ d) associate to a matrix A the quantity</p><formula xml:id="formula_65">|A| (k) = λ 1 + • • • + λ k ,</formula><p>i.e., the sum of its k largest singular values. One has the following proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2.7 The Ky Fan norms satisfy the triangular inequality.</head><p>Proof We prove this following the argument suggested in Bhatia <ref type="bibr" target="#b46">[28]</ref>. For A ∈ M d,d , and k = 1, . . . , d, let trace (k) (A) be the sum of the k largest diagonal elements of A. Let, for a symmetric matrix A, |A| ′ (k) denote the sum of the k largest eigenvalues of A (it is equal to |A| (k) if A is positive definite, but can also include negative values).</p><p>Then, for any symmetric matrix A ∈ S d ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|A| ′</head><p>(k) = max trace (k) (U AU T ) : U ∈ CO d . <ref type="bibr">(2.5)</ref> To show this, assume that V in O d diagonalizes A, so that D = V AV T is a diagonal matrix. Assume, without loss of generality, that the coefficients λ j = D(j, j) are nonincreasing. Fix U ∈ O d , let B = U AU T and W = V U T so that D = W BW T , or B = W T DW . Then, for any j ≤ d,</p><formula xml:id="formula_66">B(j, j) = d i=1 W (i, j) 2 D(i, i).</formula><p>Then, for any 1</p><formula xml:id="formula_67">≤ j 1 &lt; • • • &lt; j k ≤ d k l=1 B(j l , j l ) = d i=1 D(i, i) k l=1 W (i, j l ) 2 = k i=1 D(i, i) + k i=1 D(i, i) k l=1 W (i, j l ) 2 -1 + d i=k+1 D(i, i) k l=1 W (i, j l ) 2 = k i=1 D(i, i) + k i=1 (D(i, i) -D(k, k)) k l=1 W (i, j l ) 2 -1 + d i=k+1 (D(i, i) -D(k, k)) k l=1 W (i, j l ) 2 + D(k, k)         n i=1 k j=1 W (i, j l ) 2 -k         .</formula><p>Because W is orthogonal, we have k l=1 W (i, j l ) 2 ≤ 1 and</p><formula xml:id="formula_68">n i=1 k j=1 W (i, j l ) 2 = k.</formula><p>This shows that the terms after k i=1 D(i, i) in the upper bound are negative or zero, so that</p><formula xml:id="formula_69">k l=1 B(j l , j l ) ≤ k i=1 D(i, i).</formula><p>The maximum of the left-hand side is trace (k) <ref type="bibr">(B)</ref>. Noting that we get an equality when choosing U = V , the proof of (2.5) is complete.</p><p>Using the same argument as that made above for the nuclear norm, one deduces from this that</p><formula xml:id="formula_70">|A + B| ′ (k) ≤ |A| ′ (k) + |B| ′ (k)</formula><p>for all A, B ∈ S d and all k = 1, . . . , d. Now, let A ∈ M n,d and consider the symmetric matrix</p><formula xml:id="formula_71">Ã = 0 A T A 0 ∈ S n+d .</formula><p>Write a vector u ∈ R n+d as u = u 1 u 2 with u 1 ∈ R d and u 2 ∈ R n . Then u is an eigenvector of Ã for an eigenvalue λ if and only if A T u 2 = λu 1 and Au 1 = λu 2 , which implies that A T Au 1 = λ 2 u 1 and λ 2 is a singular value of A. Conversely, if µ is a nonzero singular value of A, associated with eigenvector u 1 , then</p><p>1/ √ µ and -1/ √ µ are eigenvalues of Ã, associated with eigenvectors u 1 ±Au 1 / √ µ . It follows from this that |A| (k) = | Ã| ′ (k) for k ≤ min(n, d) and therefore satisfies the triangle inequality. ■</p><p>We refer to <ref type="bibr" target="#b46">[28]</ref> for more examples of matrix norms, including, in particular those provided by taking pth powers in Ky Fan's norms, defining</p><formula xml:id="formula_72">|A| (k,p) = (λ p 1 + • • • + λ p k ) 1/p .</formula><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction to Optimization</head><p>This chapter summarizes some fundamental concepts in optimization that will be used later in the book. The reader is referred to textbooks, such as Beck <ref type="bibr">[22]</ref>, Eiselt et al. <ref type="bibr" target="#b86">[68]</ref>, Nocedal and Wright <ref type="bibr" target="#b164">[146]</ref>, Boyd et al. <ref type="bibr" target="#b58">[40]</ref> and many others for proofs and deeper results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Terminology</head><p>1. If I is a subset of R, a lower bound of I is an element u ∈ [-∞, +∞] such that u ≤ x for all x ∈ I. Among these lower bounds, there exists a largest element, denoted inf I ∈ [-∞, +∞], called the infimum of I (by convention, the infimum of an empty set is +∞). Similarly, one defines the supremum of I, denoted sup I, as the smallest upper bound of I (and the supremum of an empty set is -∞). Every set in R has an infimum and a supremum, but these numbers do not necessarily belong to I. When they do, they are respectively called minimal and maximal elements of I, and are denoted min I and max I. So, the statement "u = min I" means u ∈ I and u ≤ v for all v ∈ I.</p><p>2. If F : Ω → R is a real-valued function defined on a subset Ω ⊂ R d , the infimum of F over Ω is defined by inf</p><formula xml:id="formula_73">Ω F = inf{F(x) : x ∈ Ω}</formula><p>and its supremum is sup</p><formula xml:id="formula_74">Ω F = sup{F(x) : x ∈ Ω}.</formula><p>As seen above both numbers are well defined, and can take infinite values. One says that x ∈ Ω is a (global) minimizer (resp. maximizer) of F if F(y) ≥ F(x) (resp. F(y) ≤ F(x)) for all y ∈ Ω. One also says that F reaches its minimum (resp. maximum), or is minimized (resp. maximized) at x. Equivalently, x is a minimizer (resp. maximizer) of F if and only if x ∈ Ω and F(x) = min{F(y) : y ∈ Ω} (resp. max{F(y) : y ∈ Ω}).</p><p>In such cases, one also writes F(x) = min Ω F or F(x) = max Ω F. In particular, the notation u = min Ω F indicates that u = inf Ω F and that there exists an x in Ω such that F(x) = u (i.e., that the infimum of F over Ω is realized at some x ∈ Ω). Note that the infimum of a function always exists, but not necessarily its minimum. Also note that minimizers, when they exist, are not necessarily unique. We will denote by argmin Ω F (resp. argmax Ω F) the (possibly empty) set of minimizers (resp. maximizers) of F 3. One says that x is a local minimizer (resp. maximizer) of F on Ω if there exists an open ball B ⊂ R d such that x ∈ B and F(x) = min Ω∩B F (resp. F(x) = max Ω∩B F).</p><p>4. An optimization problem consists in finding a minimizer or maximizer of an "objective function" F. Focusing from now on on minimization problems (statements for maximization problems are symmetric), we will always implicitly assume that a minimizer exists. The following provides some general assumptions on F and Ω that ensure this fact.</p><p>The sublevel sets of F in Ω are denoted [F ≤ u] Ω (or simply <ref type="bibr">[F ≤ u]</ref> when</p><formula xml:id="formula_75">Ω = R d ) for u ∈ [-∞, +∞] with [F ≤ u] Ω = {x ∈ Ω : F(x) ≤ u} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that argmin</head><formula xml:id="formula_76">Ω F = u&gt;inf F [F ≤ u] Ω .</formula><p>A typical requirement for F is that its sublevel sets are closed in R d , which means that, if a sequence (x n ) in Ω satisfies, for some u ∈ R, F(x n ) ≤ u for all n and converges to a limit x, then x ∈ Ω and F(x) ≤ u. If this is true, one says that F is lower semicontinuous, or l.s.c, on Ω. If, in addition to being closed, the sublevel sets of F are bounded (at least for u small enough-larger than inf F), then argmin Ω F is an intersection of nested compact sets, and is therefore not empty (so that the optimization problem has at least one solution).</p><p>5. Different assumptions on F and Ω lead to different types of minimization problems, with specific underlying theory and algorithms.</p><p>1. If F is C 1 or smoother and Ω = R d , one speaks of an unconstrained smooth optimization problem. 2. For constrained problems, Ω is often specified by a finite number of inequalities, i.e., Ω = {x ∈ R d : γ i (x) ≤ 0, i = 1, . . . , q}.</p><p>If F and all functions γ 1 , . . . , γ q are C 1 one speaks of smooth constrained problems.</p><p>3. If Ω is a convex set (i.e., x, y ∈ Ω ⇒ [x, y] ∈ Ω, where [x, y] is the closed line segment connecting x and y) and F is a convex function (i.e., F((1t)x + ty) ≤ (1t)F(x) + tF(y) for all x, y ∈ Ω), one speaks of a convex optimization problem. <ref type="bibr" target="#b22">4</ref>. Non-smooth problems are often considered in data science, and lead to interesting algorithms and solutions.</p><p>5. When both F and γ 1 , . . . , γ q are affine functions, one speaks of a linear programming problem (or a linear program). (An affine function is a mapping</p><formula xml:id="formula_77">x → b T x + β, b ∈ R d , β ∈ R.)</formula><p>If F is quadratic (F(x) = 1  2 x T Axb T x), and all γ i 's are affine, one speaks of a quadratic programming problem.</p><p>6. Finally, some machine learning problems are specified over discrete or finite sets Ω (for example Z d , or {0, 1} d ), leading to combinatorial optimization problems. (</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unconstrained Optimization Problems</head><p>The following result summarizes (non-identical) necessary and sufficient conditions that are applicable to such a solution.</p><p>Theorem 3.1 Necessary conditions. Assume that F is differentiable over Ω, and that x * is a local minimum of F. Then ∇F(x * ) = 0.</p><p>If F is C 2 , then, in addition, ∇ 2 F(x * ) must be positive semidefinite.</p><p>Sufficient conditions. Assume that F ∈ C 2 (Ω). If x * ∈ Ω is such that ∇F(x * ) = 0 and ∇ 2 F(x * ) is positive definite, then x * is a local minimum of F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof Necessary conditions:</head><p>Since Ω is open, it contains an open ball centered at x * , with radius ϵ 0 and therefore all segments [x * , x * + ϵh] for all ϵ ∈ [0, ϵ 0 ] and all unit norm vectors h. Since x * is a local minimum, we can choose ϵ 0 so that F(x * + ϵh) ≥ F(x * ) for all h with |h| = 1.</p><p>Using Taylor formula, we get (for ϵ ∈ [0, ϵ 0 ], |h| = 1)</p><formula xml:id="formula_79">0 ≤ F(x * + ϵh) -f (x * ) = ϵ 1 0 dF(x * + tϵh)hdt .</formula><p>If dF(x * )h 0 for some h, then, for small enough ϵ, dF(x * + tϵh)h cannot change sign for t ∈ [0, 1] and therefore 1 0 dF(x * + tϵh)hdt has the same sign as dF(x * )(h) which must therefore be positive. But the same argument can be made with h replaced by -h, implying that dF(x * )(-h) = -dF(x * )h is also positive, and this gives a contradiction. We therefore have dF(x * )(h) = 0 for all h, i.e., ∇F(x * ) = 0. Assume that F is C 2 . Then, making a second-order Taylor expansion, one gets</p><formula xml:id="formula_80">0 ≤ F(x * + ϵh) -F(x * ) = ϵ 2 1 0 (1 -t)d 2 F(x * + tϵh)(h, h)dt.</formula><p>The same argument as above shows that, if d 2 F(x * )(h, h) 0, then it must be positive. This shows that d 2 F(x * )(h, h) ≥ 0 for all h and d 2 F(x * ) (or its associated matrix ∇ 2 F(x * )) is positive semidefinite. Now, assume that F is C 2 and ∇ 2 F(x * ) positive definite. One still has</p><formula xml:id="formula_81">F(x * + ϵh) -F(x * ) = ϵ 2 1 0 (1 -t)d 2 F(x * + tϵh)(h, h)dt</formula><p>If ∇ 2 F(x * ) ≻ 0, then ∇ 2 F(x * + tϵh) ≻ 0 for small enough ϵ, showing the the r.h.s. of the identity is positive for h 0, and that F(x * + ϵh) &gt; F(x * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Because maximizing F is the same as minimizing -F, necessary (resp. sufficient) conditions for optimality in maximization problems are immediately deduced from the above: it suffices to replace positive semidefinite (resp. positive definite) by negative semidefinite (resp. negative definite).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Convex sets and functions</head><p>Definition 3.2 One says that a set Ω ⊂ R d is convex if and only if, for all x, y ∈ Ω, the closed segment [x, y] also belongs to Ω.</p><p>A function F : R d → (-∞, +∞] is convex if, for all λ ∈ [0, 1] and all x, y ∈ R d , one has</p><formula xml:id="formula_82">F((1 -λ)x + λy) ≤ (1 -λ)F(x) + λF(y). (3.2)</formula><p>If, whenever the lower bound is not infinite, the inequality above is strict for λ ∈ (0, 1), one says that F is strictly convex.</p><p>Note that, with our definition, convex functions can take the value +∞ but not -∞. In order for the upper-bound to make sense when F takes infinite values, one makes the following convention: a + (+∞) = +∞ for any a ∈ (-∞, +∞]; λ • (+∞) = +∞ for any λ &gt; 0; 0 • (+∞) is not defined but 0 • (+∞) + (+∞) = +∞. Definition 3.3 The domain of F, denoted dom(F) is the set of x ∈ R d such that F(x) &lt; ∞.</p><p>One says that F is proper if dom(F) ∅.</p><p>We will only consider proper convex functions in our discussions, which will simply be referred to as convex functions for brevity. <ref type="bibr">Proposition 3.4</ref> If F is a convex function, then dom(F) is a convex subset of R d . Conversely, if Ω is a convex set and F satisfies (3.2) for all x, y ∈ Ω (i.e., F is convex on Ω), then the extension F defined by F(x) = F(x) if x ∈ Ω and F(x) = +∞ is a convex function defined on R d (such that dom( F) = Ω).</p><p>Proof The first statement is a direct consequence of (3.2), which implies that F is finite on [x, y] as soon as it is finite at x and at y. For the second statement, (3.2) for F is true for x, y ∈ Ω, since it is true for F, and the uper-bound is +∞ otherwise. ■ This proposition shows that there was no real loss of generality in requiring convex functions to be defined on the full space R d . Note also that the upper bound in <ref type="bibr">(3.2)</ref> is infinite unless both x and y belong to dom(F), so that the inequality only needs to be checked in that case.</p><p>One says that a function F is concave if and only if -F is convex. All definitions and properties made for convex functions then easily transcribe into similar statements for concave functions. We say that a function f : I → (-∞, +∞] (where I is an interval) is non-decreasing if, for all x, y ∈ I, x &lt; y implies f (x) ≤ f (y). We say that f is increasing if if, for all x, y ∈ I, x &lt; y implies f (x) &lt; f (y) if f (x) &lt; ∞ and f (y) = ∞ otherwise.</p><p>Inequality <ref type="bibr">(3.</ref>2) has important consequences on minimization problems. For example, it implies the following proposition. Proposition 3.5 Let F be a convex (resp. strictly convex) function on R d . If x ∈ dom(F) and y ∈ R d , the function</p><formula xml:id="formula_83">λ ∈ (0, 1] → 1 λ (F((1 -λ)x + λy) -F(x)) (3.3)</formula><p>is non-decreasing (resp. increasing).</p><p>Conversely, let Ω ⊂ R d be a convex set and F : Ω → (-∞, +∞) be a function such that the expression in <ref type="bibr">(3.3</ref>) is non-decreasing (resp. increasing) for all x ∈ dom(F) and y ∈ R d .</p><p>Then, the extension F of F defined in proposition 3. <ref type="bibr" target="#b22">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>is convex (resp. strictly convex).</head><p>Proof Let f (λ) = (F((1λ)x + λy) -F(x))/λ. Let µ ≤ λ denote z λ = (1λ)x + λy, z µ = (1µ)x + µy. One has z µ = (1ν)x + νz λ , with ν = µ/λ, so that F(z µ ) ≤ (1µ/λ)F(x) + (µ/λ)F(z λ ) .</p><p>The coordinates (λ (0) , . . . , λ (k) ) are uniquely associated to x ∈ aff(Ω) and depend continuously on x. They are indeed obtained by solving the linear system xx 0 = λ (1) </p><formula xml:id="formula_84">(x 1 -x 0 ) + • • • + λ (k) (x k -x 0 )</formula><p>which has a unique solution for x ∈ aff(Ω) by linear independence. To see continuity, one can introduce the k × k matrix G with entries G (ij) given by the inner products (x i -x 0 ) T (x j -x 0 ) and the vector h(x) ∈ R k with entries h (j) (x) = (xx 0 ) T (x jx 0 ). Continuity is then clear since λ = G -1 h(x). <ref type="bibr">Definition 3.7</ref> If Ω is a convex set, then its relative interior, denoted relint(Ω), is the set of all x ∈ Ω such that there exists ϵ &gt; 0 such that aff(Ω) ∩ B(x, ϵ) ⊂ Ω.</p><p>We have the following important property. <ref type="bibr">Proposition 3.8</ref> Let Ω be a nonempty convex set. If x ∈ relint(Ω) and y ∈ Ω, then x λ = (1λ)x + λy ∈ relint(Ω) for all λ ∈ [0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Moreover relint(Ω) is a nonempty convex set.</head><p>Proof Take ϵ such that B(x, ϵ) ∩ aff(Ω) ⊂ Ω. Take any z ∈ B(x λ , (1λ)ϵ) ∩ aff(Ω). Define z such that z = (1λ)z + λy, i.e. z = zλy 1λ .</p><p>Then z ∈ aff(Ω) and</p><formula xml:id="formula_85">|z -x| = |z -x λ | 1 -λ &lt; ϵ</formula><p>so that z, and therefore z belongs to Ω. This proves that B(x λ , (1λ)ϵ) ∩ aff(Ω) ⊂ Ω so that x λ ∈ relint(Ω).</p><p>If both x and y belong to relint(Ω), then x λ ∈ relint(Ω) for λ ∈ [0, 1], showing that this set is convex.</p><p>We now show that relint(Ω) ∅. Let k be the dimension of aff(Ω), so that there exist x 0 , x 1 , . . . , x k ∈ Ω such that x 1x 0 , . . . , x kx 0 are linearly independent. Consider the "simplex" S = {λ (0) x 0 + • • • + λ (k) x k :, λ (0) + • • • + λ (k) = 1, λ (j) ≥ 0, j = 0, . . . , k}, which is included in Ω. Then the average x = (x 0 + • • • + x k )/(k + 1) is such that B(x, ϵ) ∩ aff(Ω) ⊂ S for small enough ϵ. Otherwise, there would exist a sequence (k) (n) = 1 and at least one λ (j) (n) &lt; 0 that converges to x. Let y j be the set of elements in this sequence such that λ (j) (n) &lt; 0. This set is infinite for at least one j and provides a subsequence of y that also converges to x. But this would imply that the j th barycentric coordinate, which depends continuously on x, is non-positive, which is a contradiction.</p><formula xml:id="formula_86">y(n) = λ (0) (n)x 0 + • • • + λ (k) (n)x k such that λ (0) (n) + • • • + λ</formula><p>We therefore have x ∈ relint(Ω), which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The following proposition provides an equivalent definition of the relative interior.</p><p>Proposition 3.9 If Ω is a convex set, then relint(Ω) = {x ∈ Ω : ∀y ∈ Ω, ∃ϵ &gt; 0 such that xϵ(yx) ∈ Ω} .</p><p>(3.4) So x belongs in the relative interior of Ω if, for all y ∈ Ω, the segment [x, y] can be extended on the x side and still remain included in Ω.</p><p>Proof Let A be the set in the r.h.s. of <ref type="bibr">(3.4)</ref>. The proof that relint(Ω) ⊂ A is straightforward and left to the reader. We consider the reverse inclusion.</p><p>Let x ∈ A, and let y ∈ relint(Ω), which is not empty. Then, for some ϵ &gt; 0, we have z = xϵ(yx) ∈ Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since</head><p>x = 1 1 + ϵ (ϵy + z), proposition 3.8 implies that x ∈ relint(Ω).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Convex functions have important regularity properties in the relative interior of their domain, that we will denote ridom(F). Importantly: ridom(F) = relint(dom(F)) int(dom(F)).</p><p>A first such property is provided by the next proposition. Proposition 3.10 Let F be a convex function. Then F is locally Lipschitz continuous on ridom(F), i.e., for every compact subset C ⊂ ridom(F), there exists a constant L &gt; 0 such that |F(x) -F(y)| ≤ L|x -y| for all x, y ∈ C. This implies, in particular, that F is continuous on ridom(F).</p><p>Proof Take x ∈ ridom(F). Let K = h ∈ --→ aff (dom(F)), |h| = 1 . Then, the segment [xah, x + ah] is included in ridom(F) for small enough a and all h ∈ K. Since F is convex, we have, for t ≤ a, F(x + th) -F(x) ≤ t a (F(x + ah) -F(x))</p><p>Writing x = λ(xah) + (1λ)(x + th) with λ = t/(t + a), we also have</p><formula xml:id="formula_87">F(x) ≤ t t + a (F(x -ah) + a t + a F(x + th))</formula><p>which can be rewritten as</p><formula xml:id="formula_88">F(x) -F(x + th) ≤ t a (F(x -ah) -F(x)).</formula><p>These two inequalities show that F is continuous at x along any direction in --→ aff (dom(F)), which implies that F is continuous at x. Given this, the differences F(x+ah)-F(x) are bounded over the compact set C, by some constant M and, the previous inequalities show that</p><formula xml:id="formula_89">|F(y) -F(x)| ≤ M a |x -y| if y ∈ ridom(F), |y -x| ≤ a. ■</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Derivatives of convex functions and optimality conditions</head><p>The following theorem provides a stronger version of optimality conditions for the minimization of differentiable convex functions. Note that we have only defined differentiability of functions defined over open sets.</p><p>Theorem 3.11 Let F be a convex function, with int(dom(F)) ∅. Assume that x ∈ int(dom(F)) and that F is differentiable at x. Then, for all y ∈ R d : ∇F(x) T (yx) ≤ F(y) -F(x) .</p><p>(3.5)</p><p>If F is strictly convex, the inequality is strict for y x. In particular, ∇F(x) = 0 implies that x is a global minimizer of F. It is the unique minimizer if F is strictly convex.</p><p>Conversely, if F is C 1 on an open convex set Ω and satisfies (3.5) for all x, y ∈ Ω, then F is convex.</p><formula xml:id="formula_90">Proof Equation (3.3) implies 1 λ (F((1 -λ)x + λy) -F(x)) ≤ F(y) -F(x), 0 &lt; λ ≤ 1.</formula><p>Taking the limit of the lower bound for λ → 0, λ &gt; 0 yields <ref type="bibr">(3.5)</ref>. If F is strictly convex, the inequality is strict for λ &lt; 1 and, since the l.h.s. is increasing in λ, it remains strict when λ ↓ 0.</p><p>Conversely, assuming (3.5) for all x, y ∈ Ω, the derivative of λ → 1 λ (F((1λ)x + λy) -F(x)) is 1 λ 2 (λ∇F(x + λh) T h -F(x + λh) + F(x)) with h = yx, which is non-negative by <ref type="bibr">(3.5)</ref>. This proves that F is convex. If <ref type="bibr">(3.5)</ref> holds with a strict inequality, then the derivative is positive and 1 λ (F((1λ)x + λy) -F(x)) is increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The next proposition describes C 2 convex functions in terms of their second derivatives. Proposition 3.12 Let F be convex and twice differentiable at x ∈ int(dom(F)). Then ∇ 2 F(x) is positive semi-definite.</p><p>Conversely, assume that Ω = dom(F) is an open set and that F is C 2 on Ω with a positive semi-definite second derivative. Then F (or, rather, its extension F) is convex. If the second derivative is everywhere positive definite, then F is strictly convex.</p><p>Proof Using Taylor formula (1.10) at order 2, we get, for any h ∈ R d with |h| = 1,</p><formula xml:id="formula_91">1 2 d 2 F(x)(h, h) = 1 2t 2 d 2 F(x)(th, th) = 1 t 2 (F(x + th) -F(x) -t∇F(x) T h) + ϵ(t) ≥ ϵ(t)</formula><p>with ϵ(t) → 0 when t → 0, the last inequality deriving from <ref type="bibr">(3.5)</ref>. This shows that d 2 F(x)(h, h) ≥ 0.</p><p>To prove the second statement, assume that F is C 2 and ∇ 2 F is positive semidefinite everywhere. Then <ref type="bibr">(1.8)</ref> implies</p><formula xml:id="formula_92">F(y) -F(x) -∇F(x) T (y -x) = 1 2 (y -x) T ∇ 2 F(z)(y -x)</formula><p>for some z ∈ [x, y]. Since the r.h.s. is non-negative, (3.5) holds. If ∇ 2 F is positive definite everywhere, then the r.h.s. is positive if y x and (3.5) holds with a strict inequality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>If F is C 2 and ∇ 2 F is positive definite and strictly convex, then <ref type="bibr">(1.8)</ref> implies that, for some z ∈ [x, y],</p><formula xml:id="formula_93">F(y) -F(x) -∇F(x) T (y -x) = 1 2 (y -x) T ∇ 2 F(z)(y -x) ≥ ρ min (∇ 2 F(z)) 2 |y -x| 2</formula><p>where ρ min (A) denotes the smallest eigenvalue of A. If this smallest eigenvalue is bounded from below away from zero, there exists a constant m &gt; 0 such that</p><formula xml:id="formula_94">F(y) -F(x) -∇F(x) T (y -x) - m 2 |y -x| 2 ≥ 0. (3.6)</formula><p>This property is captured by the following definition, which does not require F to be C 2 .</p><formula xml:id="formula_95">Definition 3.13 A C 1 function F is strongly convex if 1. int(dom(F)) ∅ 2.</formula><p>There exists m &gt; 0 such that <ref type="bibr">(3.6</ref>) holds for all x ∈ int(dom(F)) and y ∈ R d .</p><p>We have the following proposition.</p><p>Proposition 3.14 If F is strongly convex, then it is strictly convex, so that, in particular argmin F has at most one element.</p><p>If dom(F) = R d , then argmin F is not empty.</p><p>Proof The first part is a direct consequence of (3.6) and theorem 3.11.</p><p>For the second part, (3.6) implies that F.</p><formula xml:id="formula_96">F(x) -F(0) ≥ ∇F(0) T x + m 2 |x| 2 ≥ |x| m 2</formula><p>The set in the r.h.s. involves the minimization of a continuous function on a compact set, and is therefore not empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We will use the following definition. If F is L-C k , then Taylor formula ((1.9)) implies</p><formula xml:id="formula_97">f (x + h) -f (x) -df (x)h - 1 2 d 2 f (x)(h, h) -• • • - 1 k! d k f (x)(h, . . . , h) ≤ L|h| k+1 (k + 1)! (3.7)</formula><p>for which we used the fact that</p><formula xml:id="formula_98">1 0 t(1 -t) k-1 dt = 1 0 (1 -t) k-1 dt - 1 0 (1 -t) k dt = 1 k - 1 k + 1 = 1 k(k + 1)</formula><p>.</p><p>If F is strongly convex and is, in addition, L-C 1 for some L, then using (3.7), one gets the double inequality, for all x, y ∈ int(dom(F)):</p><formula xml:id="formula_99">m 2 |y -x| 2 ≤ F(y) -F(x) -∇F(x) T (y -x) ≤ L 2 |y -x| 2 . (3.8)</formula><p>The following proposition will be used later.</p><p>Proposition 3.16 Assume that F is strongly convex, satisfying <ref type="bibr">(3.6)</ref>, and that argmin F = {x * } with x * ∈ int(dom(F)). Then, for all x ∈ int(dom(F)):</p><formula xml:id="formula_100">m 2 |x -x * | 2 ≤ F(x) -F(x * ) ≤ 1 2m |∇F(x)| 2 (3.9)</formula><p>Proof Since ∇F(x * ) = 0, the first inequality is a consequence of (3.6) applied to x = x * . Switching the role of x and x * , we have</p><formula xml:id="formula_101">F(x * ) -F(x) -∇F(x) T (x * -x) ≥ m 2 |x -x * | 2 so that 0 ≤ F(x) -F(x * ) ≤ -∇F(x) T (x * -x) - m 2 |x -x * | 2 ≤ |∇F(x)| |x -x * | - m 2 |x -x * | 2 (3.10)</formula><p>The maximum of the r.h.s. with respect to |xx * | is attained at |∇F(x)|/m, showing that</p><formula xml:id="formula_102">F(x) -F(x * ) ≤ 1 2m |∇F(x)| 2 ,</formula><p>which is the second inequality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Direction of descent and steepest descent</head><p>Gradient-based algorithms for optimization iteratively update the variable x, creating a sequence governed by an equation taking the form x t+1 = x t + α t h t with α t &gt; 0 and h t ∈ R d . To ensure that the objective function F decreases at each step, h t is chosen to be a direction of descent for F at x t , a notion which, as seen below, is closely connected with the direction of ∇F(x t ).</p><p>Definition 3. <ref type="bibr" target="#b35">17</ref> Let Ω be open in R d and F : Ω → R be a C 1 function. A direction of descent for F at x ∈ Ω is a vector h 0 ∈ R d such that there exists ϵ 0 &gt; 0 such that F(x + ϵh) &lt; F(x) for all ϵ ∈ (0, ϵ 0 ].</p><p>Proposition 3.18 Assume that F : Ω → R is C 1 and take x ∈ Ω. Then any direction h such that h T ∇F(x) &lt; 0 is a direction of descent for F at x. Conversely, if h is a direction of descent, then h T ∇F(x) ≤ 0.</p><p>Proof We have the first-order expansion F(x+ϵh)-F(x) = ϵh T ∇F(x)+o(ϵ). If h T ∇F(x) &lt; 0, the r.h.s. is negative for small enough ϵ and h is a direction of descent. Similarly, if h T ∇F(x) &gt; 0, the r.h.s. is positive for small enough ϵ and h cannot be a direction of descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>In particular, h = -∇F(x) is always a direction of descent. It is called the steepest descent direction because it minimizes h → ∂ α F(x + αh)| α=0 over all h such that |h| 2 = 1. However, this designation has a character of optimality that may be misleading, because using the Euclidean norm for the condition |h| 2 = 1 is not necessarily adapted to the optimization problem at hand. In the absence of additional information on the problem, it does have a canonical nature, as it is (up to rescaling) the only norm invariant to rotations (including permutations) of the coordinates. Such invariance is not necessarily desirable when the variable x has a known structure (e.g., it is organized on a graph) which would be broken by permutation. Also, steepest refers to a local "greedy" evaluation, but may not be optimal from a global perspective. A simple example to illustrate this is the case of a quadratic function</p><formula xml:id="formula_103">F(x) = 1 2 x T Ax -b T x</formula><p>where A ∈ S ++ n is a positive definite symmetric matrix. Then ∇F(x) = Axb, but one may argue that ∇ A F(x) = A -1 ∇F(x) (defined in <ref type="bibr">(1.3)</ref>) is a better choice, because it allows the algorithm to reach the minimizer of F in one step, since x -∇ A F(x) = A -1 b (this statement disregards the cost associated in solving the system Ax = b, which can be an important factor in large dimension). Importantly, if F is any C 1 function, and A ∈ S ++ n , the minimizer of h → ∂ α F(x + αh)| α=0 over all h such that h T Ah = 1 is given by -∇ A F(x), i.e., -∇ A F(x) is the steepest descent for the norm associated with A. This yields a general version of steepest descent methods, iterating</p><formula xml:id="formula_104">x t+1 = x t -α t ∇ A t F(x t )</formula><p>with α t &gt; 0 and A t ∈ S ++ n .</p><p>One can also notice that ∇ A F(x) is also a minimizer of</p><formula xml:id="formula_105">F(x) + ∇F(x) T h + 1 2 h T Ah.</formula><p>When ∇ 2 F(x) is positive definite, it is then natural to choose it as the matrix A, therefore taking h = -∇ 2 F(x) -1 ∇F(x). This provides Newton's method for optimization. However, Newton method requires computing second derivatives of F, which can be computationally costly. It is, moreover, not a gradient-based method, which is the focus of this discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Convergence</head><p>We now consider a descent algorithm</p><p>x t+1 = x t + α t h t <ref type="bibr">(3.11)</ref> where h t is a direction of descent at x t for the objective function F. To ensure convergence, suitable choices for the direction of descent and the step must be made at each iteration, and some assumptions on the objective function are needed.</p><p>Regarding the direction of descent, which must satisfy h T k ∇F(x k ) ≤ 0, we will assume a uniform control away from orthogonality to the gradient, with the condition</p><formula xml:id="formula_106">-h T t ∇F(x t ) ≥ ϵ|h t | |∇F(x t )| (3.12a)</formula><p>for some fixed ϵ &gt; 0. Without loss of generality (given that a multiplicative step α t must also be chosen), we assume that h t is commensurable to the gradient, namely, that</p><formula xml:id="formula_107">γ 1 |∇F(x t )| ≤ |h t | ≤ γ 2 |∇F(x t )| (3.12b)</formula><p>for fixed 0 &lt; γ 1 ≤ γ 2 . If h t = ∇ A t F, these assumptions are satisfied as soon as the smallest and largest eigenvalues of A t are controlled along the trajectory.</p><p>We have the following proposition.</p><p>Proposition 3.19 Assume that F is L-C 1 . Assume that x t satisfies <ref type="bibr">(3.11)</ref> and that (3.12a) and (3.12b) hold. Then, there exist constants ᾱ &gt; 0 and C &gt; 0 that depends on γ 1 , γ 2 and ϵ, such that, for α t ≤ ᾱ, one has</p><formula xml:id="formula_108">F(x t+1 ) -F(x t ) ≤ -Cα t |∇F(x t )| 2 . (<label>3.13)</label></formula><p>Proof Applying (3.7) to x t and x t+1 , we get</p><formula xml:id="formula_109">F(x t+1 ) -F(x t ) -α t ∇F(x t ) T h t ≤ L 2 α 2 t |h t | 2</formula><p>Using (3.11) and (3.12a), this gives 2 . It suffices to take ᾱ = ϵγ 1 /Lγ 2  2 and C = ϵγ 1 /2 to obtain (3.13).</p><formula xml:id="formula_110">F(x t+1 ) -F(x t ) + α t ϵγ 1 |∇F(x t )| 2 ≤ L 2 α 2 t γ 2 2 |∇F(x t )| 2 so that F(x t+1 ) -F(x t ) ≤ -α t ϵγ 1 -α t γ 2 2 L/2 |∇F(x t )|</formula><p>■ Iterating (3.13) for t = 1 to t = T -1 yields</p><formula xml:id="formula_111">T t=1 α t |∇F(x t )| 2 ≤ 1 C (F(x 1 ) -F(x T )).</formula><p>If F is bounded from below, and one takes α t = ᾱ for all t, one deduces that min |∇F(x t )| 2 : t = 1, . . . , T ≤ F(x 1 )inf F CT ᾱ .</p><p>We can deduce from this, for example, that there exists a sequence</p><formula xml:id="formula_112">t 1 &lt; • • • &lt; t n &lt; • • • such that ∇F(x t k ) → 0 when k → ∞.</formula><p>In particular, if one runs <ref type="bibr">(3.11)</ref> until |∇F(x t )| is smaller than a given tolerance level (which is standard), the procedure is guaranteed to terminate in a finite number of steps.</p><p>Stronger results may be obtained under stronger assumptions on F and on the algorithm. The first assumption is an inequality similar to <ref type="bibr">(3.13)</ref> and requires that, for some constant C &gt; 0,</p><formula xml:id="formula_113">F(x t+1 ) -F(x t ) ≤ -C|∇F(x t )| 2 .</formula><p>(3.14)</p><p>Such an inequality can be deduced from <ref type="bibr">(3.13)</ref> under the additional assumption that α t is bounded from below and we will discuss later line search strategies that ensure its validity. The second assumption is that F is convex.</p><p>Theorem 3.20 Assume that F is convex and finite and that its sub-level set [F ≤ F(x 0 )] is bounded. Assume that argmin F is not empty and let x * be a minimizer of F. If <ref type="bibr">(3.14)</ref> is true, then</p><formula xml:id="formula_114">F(x t ) -F(x * ) ≤ R 2 C(t + 1) with R = max{|x -x * | : F(x) ≤ F(x 0 )}.</formula><p>Proof Note that the algorithm never leaves [F ≤ F(x 0 )]. We have</p><formula xml:id="formula_115">F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) -C|∇F(x t )| 2 .</formula><p>Moreover, by convexity, F(x * ) -F(x t ) ≥ ∇F(x t ) T (x *x t ), so that F(x t ) -F(x * ) ≤ ∇F(x t ) T (x tx * ) ≤ |∇F(x t )|R.</p><p>Combining these two inequalities, we get</p><formula xml:id="formula_116">F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) - C R 2 (F(x t ) -F(x * )) 2 .</formula><p>Introducing δ t = (C/R 2 )(F(x t ) -F(x * )), this inequality implies</p><formula xml:id="formula_117">δ t+1 ≤ δ t (1 -δ t ) .</formula><p>Taking inverses, we get 1 δ t+1</p><formula xml:id="formula_118">≥ 1 δ t + 1 1 -δ t ≥ 1 δ t + 1</formula><p>which implies 1 δ t ≥ t + 1 or δ t ≤ 1/(t + 1), which in turn implies the statement of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>A faster convergence rate can be obtained if F is assumed to be strongly convex. Indeed, if <ref type="bibr">(3.6</ref>) and <ref type="bibr">(3.14)</ref> are satisfied, then (using proposition 3.16), F(x t+1 ) -F(x * ) ≤ F(x t ) -F(x * ) -C|∇F(x t )| 2  ≤ F(x t ) -F(x * ) -2Cm(F(x t ) -F(x * )) = (1 -2Cm)(F(x t ) -F(x * )) .</p><p>We therefore get the proposition:</p><p>Proposition 3.21 If F is finite and satisfies <ref type="bibr">(3.6)</ref>, and if the descent algorithm satisfies <ref type="bibr">(3.14)</ref>, then F(x t ) -F(x * ) ≤ (1 -2Cm) t (F(x 0 ) -F(x * )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.7">Line search</head><p>Proposition 3.19 states that, to ensure that <ref type="bibr">(3.14)</ref> holds, it suffices to take a small enough step parameter α. However, the values of α that are acceptable depend on properties of the objective function that are rarely known in practice. Moreover, even if a valid choice is determined (this can sometimes be done in practice by trial and error), setting a fixed value of α for the whole algorithm is often too conservative, as the best α when starting the algorithm may be different from the best one close to convergence.</p><p>For this reason, most gradient descent procedures select a parameter α t at each step using a line search. Given a current position and direction of descent h, a line search explores the values of F(x + αh), α ∈ (0, α max ] in order to discover some α * that satisfies some desirable properties. We will assume in the following that x and h satisfy (3.12a) and (3.12b) for fixed ϵ, γ 1 , γ 2 .</p><p>One possible strategy is to define α * as a minimizer of the scalar function f h (α) = F(x + αh) over (0, α max ] for a given upper-bound ϵ max . This can be implemented using, e.g., binary or ternary search algorithms, but such algorithms would typically require a large number of number of evaluations of the function F, and would be too costly to be run at each iteration of a gradient descent procedure.</p><p>Based on the previous convergence study, we should be happy with a line search procedure that ensures that <ref type="bibr">(3.14)</ref> is satisfied for some fixed value of the constant C. One such condition is the so-called Armijo rule that requires (with a fixed, typically small, value of c 1 &gt; 0): f h (α) ≤ f h (0) + c 1 αh T ∇f (x) . <ref type="bibr">(3.15)</ref> We know that, under the assumptions of proposition 3.19, this condition can always be satisfied with a small enough value of α. Such a value can be determined using a "backtracking procedure," which, given α max and ρ ∈ (0, 1), takes α = ρ k α max where k is the smallest integer such that <ref type="bibr">(3.15)</ref> is satisfied. This value of k is then determined iteratively, trying α max , ρα max , ρ 2 α max , . . . until <ref type="bibr">(3.15)</ref> is true (this provides the "backtracking method").</p><p>A stronger requirement in the line search is to ensure that ∂f h (α) is not "too negative" since one would otherwise be able to further reduce f h by taking a larger value of α. This leads to the weak Wolfe conditions, which combine the Armijo's rule in <ref type="bibr">(3.15)</ref> and showing that α ≥ (1c 2 )ϵ/(Lγ 2 2 ). Moreover</p><formula xml:id="formula_119">∂f h (α) = h T ∇F(x + αh) ≥ c 2 h T ∇F(x)<label>(3</label></formula><formula xml:id="formula_120">F(x + αh) ≤ F(x) + c 1 αh T ∇f (x) ≤ F(x) -c 1 αϵ|∇F(x)| 2 so that F(x + αh) ≤ F(x) - c 1 (1 -c 2 )ϵ 2 Lγ 2 2 |∇F(x)| 2 .</formula><p>We have just proved the following proposition.</p><p>Proposition 3.22 Assume that F is L-C 1 and that (3.12a), (3.12b), <ref type="bibr">(3.15</ref>) and (3.16a) are satisfied. Then there exists C &gt; 0, depending only of L, ϵ, γ 2 , c 1 and c 2 such that</p><formula xml:id="formula_121">F(x + αh) ≤ F(x) -C|∇F(x)| 2 .</formula><p>The Wolfe conditions can always be satisfied by some α as soon as F is C 1 and bounded from below, and h T ∇F(x) &lt; 0. The next proposition shows this result for the weak condition, while providing an algorithm finding an α that satisfies it in a finite number of steps. Proposition 3.23 Let f : α → f (α) be a C 1 function defined on [0, +∞) such that f is bounded from below and ∂ α f (0) &lt; 0. Let 0 &lt; c 1 &lt; c 2 &lt; 1.</p><p>Let α 0,0 = α 0,1 = 0 and α 0 &gt; 0. Define recursively sequences α n,0 , α n,1 and α n as follows.</p><p>(i) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and ∂f (α n ) ≥ c 2 ∂ α f (0) stop the construction.</p><p>(ii) If f (α n ) &gt; f (0) + c 1 α n ∂ α f (0) let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 .</p><formula xml:id="formula_122">(iii) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and ∂f (α n ) &lt; c 2 ∂ α f (0):</formula><p>(a) If α n,1 = 0, let α n+1 = 2α n , α n+1,0 = α n and α n+1,1 = α n,1 . (b) If α n,1 &gt; 0, let α n+1 = (α n + α n,1 )/2, α n+1,0 = α n and α n+1,1 = α n,1 .</p><p>Then the sequences are always finite, i.e., the algorithm terminates in a finite number of steps.</p><p>Proof Assume, to get a contradiction, that the algorithm runs indefinitely, so that case (i) never occurs. If case (ii) never occurs, then one runs step (iii-a) indefinitely, so that α n → ∞ with f (α n ) ≤ f (0) + c 1 α n ∂ α f (0), and f cannot be bounded from below, yielding a contradiction. As soon as case (ii) occurs, we have, at every step, α n,0 ≥ α n-1,0 , α n,1 ≤ α n-1,1 , α n ∈ [α n,0 , α n,1 ], f (α n,1 ) &gt; f (0) + c 1 α n,1 ∂ α f (0), f (α n,0 ) ≤ f (0) + c 1 α n,0 ∂ α f (0) and ∂f (α n,0 ) &lt; c 2 ∂ α f (0). This implies that f (α n,1 )f (α n,0 ) &gt; c 1 (α n,1α n,0 )∂ α f (0). Moreover, the updates imply that (α n+1,1α n+1,0 ) = (α n,1α n,0 )/2. This requires that the three sequences α n , α n,0 and α n,1 converge to the same limit, α. We have</p><formula xml:id="formula_123">∂ α f (α) = lim n→∞ f (α n,1 ) -f (α n,0 ) α n,1 -α n,0 ≥ c 1 ∂ α f (0) and ∂ α f (α) = lim n→∞ ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) yielding c 1 ∂ α f (0) ≤ c 2 ∂ α f (0) which is impossible since c 2 &gt; c 1 and ∂ α f (0) &lt; 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The existence of α satisfying the strong Wolfe condition is a consequence of the following proposition, which also provides an algorithm. Proposition 3.24 Let f : α → f (α) be a C 1 function defined on [0, +∞) such that f is bounded from below and ∂ α f (0) &lt; 0. Let 0 &lt; c 1 &lt; c 2 &lt; 1.</p><p>Let α 0,0 = α 0,1 = 0 and α 0 &gt; 0. Define recursively sequences α n,0 , α n,1 and α n as follows.</p><formula xml:id="formula_124">(i) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and |∂ α f (α n )| ≤ c 2 |∂ α f (0)| stop the construction. (ii) If f (α n ) &gt; f (0) + c 1 α n ∂ α f (0) let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 . (iii) If f (α n ) ≤ f (0) + c 1 α n ∂ α f (0) and |∂ α f (α n )| &gt; c 2 |∂ α f (0)|:</formula><p>(a) If α n,1 = 0 and ∂ α f (α n ) &gt; -c 2 ∂ α f (0), let α n+1 = 2α n , α n+1,0 = α n,0 and α n+1,1 = α n,1 .</p><p>(b) If α n,1 = 0 and ∂ α f (α n ) &lt; c 2 ∂ α f (0), let α n+1 = 2α n , α n+1,0 = α n and α n+1,1 = α n,1 .</p><p>(c) If α n,1 &gt; 0 and ∂ α f (α n ) &gt; -c 2 ∂ α f (0), let α n+1 = (α n + α n,0 )/2, α n+1,1 = α n and α n+1,0 = α n,0 .</p><p>(d) If α n,1 &gt; 0 and ∂ α f (α n ) &lt; c 2 ∂ α f (0), let α n+1 = (α n + α n,1 )/2, α n+1,0 = α n and α n+1,1 = α n,1 .</p><p>Then the sequences are always finite, i.e., the algorithm terminates in a finite number of steps.</p><p>Proof Assume that the algorithm runs indefinitely in order to get a contradiction. If the algorithm never enters case (ii), then α n,1 = 0 for all n, α n tends to infinity and f (α n ) ≤ f (0) + c 1 α n ∂ α f (0), which contradicts the fact that f is bounded from below.</p><p>As soon as the algorithm enter (ii), we have, for all subsequent iterations: α n,0 ≤ α n ≤ α n,1 , α n+1,0 ≥ α n,0 , α n+1,1 ≤ α n,1 and α n+1,1α n+1,0 = (α n,1α n,0 )/2. This implies that both α n,0 and α n,1 converge to the same limit α.</p><p>Moreover, we have, at each step:</p><formula xml:id="formula_125">f (α n,1 ) &gt; f (0) + c 1 α n,1 ∂ α f (0) or ∂ α f (α n,1 ) &gt; -c 2 ∂ α f (0) and f (α n,0 ) ≤ f (0) + c 1 α n,0 ∂ α f (0) and ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) .</formula><p>This implies that, at each step:</p><formula xml:id="formula_126">f (α n,1 ) -f (α n,0 ) α n,1 -α n,0 &gt; c 1 ∂ α f (0) or ∂ α f (α n,1 ) &gt; -c 2 ∂ α f (0) and ∂ α f (α n,0 ) ≤ c 2 ∂ α f (0) .</formula><p>There inequalities remain satisfied at the limit, and we must have</p><formula xml:id="formula_127">∂ α f (α) &gt; c 1 ∂ α f (0) or ∂ α f (α) &gt; -c 2 ∂ α f (0) and ∂ α f (α) ≤ c 2 ∂ α f (0) ,</formula><p>which is a contradiction since c 2 &gt; c 1 and ∂ α f (0) &lt; 0. In some situations, the computation of ∇F can be too costly, if not intractable, to run gradient descent updates while a low-cost stochastic approximation is available. For example, if F is an average of a sum of many terms, the approximation may simply be based on averaging over a randomly selected subset of the terms. This leads to a stochastic approximation algorithm <ref type="bibr" target="#b181">[163,</ref><ref type="bibr" target="#b131">113,</ref><ref type="bibr" target="#b43">25,</ref><ref type="bibr" target="#b85">67]</ref> called stochastic gradient descent (SGD).</p><p>A general stochastic approximation algorithm of the Robbins-Monro type updates a parameter, denoted x ∈ R d , using stochastic rules. One associates to each x a probability distribution (π x ) on some set S, and, for some function H : R d × S → R d , considers the sequence of random iterations:</p><formula xml:id="formula_128">ξ t+1 ∼ π X t X t+1 = X t + α t+1 H(X t , ξ t+1 ) (3.17)</formula><p>where ξ t+1 is a random variable and the notation ξ t+1 ∼ π X t should be interpreted as the more precise statement that the conditional distribution of ξ t+1 given all past random variables U t = (ξ 1 , X 1 , . . . , ξ t , X t ) only depends on X t and is given by π X t .</p><p>It is sometimes assumed in the literature that π x does not depend on x. This is no real loss of generality because under mild assumptions, a random variable ξ following π x can be generated as function U (x, ξ) where ξ follows a fixed distribution (such as that of a family of independent uniformly distributed variables) and one can replace H(x, ξ) by H(x, U (x, ξ)). On the other hand, allowing π to depend on x brings little additional complication in the notation, and corresponds to the natural form of many applications.</p><p>More complex situations can also be considered, in which ξ t+1 is not conditionally independent of the past variables given X t . For example, the conditional distribution of ξ t+1 given the past may also depends on ξ t , which allows for the combination of stochastic gradient methods with Markov chain Monte-Carlo methods. This situation is studied, for example, in Métivier and Priouret <ref type="bibr" target="#b156">[138]</ref>, Benveniste et al. <ref type="bibr" target="#b43">[25]</ref>, and we will discuss an example in section 17.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Deterministic approximation and convergence study</head><p>Introduce the function H(x) = E π x (H(x, •))</p><p>and write X t+1 = X t + α t+1 H(X t ) + α t+1 η t+1</p><p>with η t+1 = H(X t , ξ t+1 ) -H(X t ) in order to represent the evolution of X t in (3.17) as a perturbation of the deterministic algorithm xt+1 = xt + α n+1 H( xt ) <ref type="bibr">(3.18)</ref> by the "noise term" α t+1 η t+1 . In many cases, the deterministic algorithm provides the limit behavior of the stochastic sequence, and one should ensure that this limit is as desired. By definition, the conditional expectation of η t+1 given U t (the past) is zero and one says that α t+1 η t+1 is a "martingale increment." Then,</p><formula xml:id="formula_129">M T = T t=0 α t+1 η t+1 (3.19)</formula><p>is called a "martingale." The theory of martingales offers numerous tools for controlling the size of M T and is often a key element in proving the convergence of the method.</p><p>Many convergence results have been provided in the literature and can be found in textbooks or lecture notes such as Benaïm <ref type="bibr" target="#b41">[23]</ref>, Kushner and Yin <ref type="bibr" target="#b131">[113]</ref>, Benveniste et al. <ref type="bibr" target="#b43">[25]</ref>. These results rely on some smoothness and growth assumptions made on the function H, and on the dynamics of the deterministic equation <ref type="bibr">(3.18)</ref>. Depending on these assumptions, proofs may become quite technical. We will here restrict to a reasonably simple context and assume that (H1) There exists a constant C such that, for all x ∈ R d ,</p><formula xml:id="formula_130">E π x (|H(x, •)| 2 ) ≤ C(1 + |x| 2 ).</formula><p>(H2) There exists x * ∈ R d and µ &gt; 0 such that, for all x ∈ R d</p><formula xml:id="formula_131">(x -x * ) T H(x) ≤ -µ|x -x * | 2 .</formula><p>Assuming this, let A t = |X tx * | 2 and a t = E(A t ). Then, using <ref type="bibr">(3.17)</ref>,</p><formula xml:id="formula_132">A t+1 = A t + 2α t+1 (X t -x * ) T H(X t , ξ t+1 ) + α 2 t+1 |H(X t , ξ t+1 )| 2 .</formula><p>Taking the conditional expectation given past variables yields</p><formula xml:id="formula_133">E(A t+1 | U t ) = A t + 2α t+1 (X t -x * ) T H(X t ) + α 2 t+1 E π x t (|H(X t , •)| 2 ) ≤ A t -2α t+1 µA t + α 2 t+1 C(1 + |X t | 2 ) ≤ (1 -2α t+1 µ + Cα 2 t+1 )A t +</formula><p>α 2 t+1 C with C = 1 + |x * | 2 . Taking expectations on both sides yields a t+1 ≤ (1 -2α t+1 µ + Cα 2 t+1 )a t + α 2 t+1 C. (3.20) We state the next step in the computation as a lemma. Lemma 3.25 Assume that the sequence a t satisfies the recursive inequality a t+1 ≤ (1δ t )a t + ϵ t (3.21) with 0 ≤ δ t ≤ 1. Let v k,t = t j=k+1 (1δ j ). Then a t ≤ a 0 v 0,t + t k=1 ϵ k v k,t . (3.22) Proof Letting b t = a t /v 0,t , we get b t+1 ≤ b t + ϵ t+1 v 0,t+1 so that b t ≤ b 0 + t k=1 ϵ k v 0,k , and a t ≤ a 0 v 0,t + t k=1 ϵ k v k,t . ■ Using (3.20), we can apply this lemma with ϵ t = Cα 2 t and δ t = 2α t µ-Cα 2 t , making the additional assumption that, for all t, α t &lt; min( 1 2µ , 2µ C</p><p>), which ensures that 0 &lt; δ t &lt; 1.</p><p>Starting with a simple case, assume that the steps γ t are constant, equal to some value γ (yielding also constant δ and ϵ). Then, <ref type="bibr">(3.22)</ref> gives</p><formula xml:id="formula_134">a t ≤ a 0 (1 -δ) t + ϵ t k=1 (1 -δ) t-k-1 ≤ a 0 (1 -δ) t + ϵ δ . (3.23)</formula><p>Returning to the expression of δ and ϵ as functions of α, this gives</p><formula xml:id="formula_135">a t ≤ a 0 (1 -2αµ + α 2 C) t + α C 2µ -αC .</formula><p>This shows that limsup a t = O(α).</p><p>Return to the general case in which the steps depend on t, we will use the following simple result, that we state as a lemma for future reference. <ref type="bibr">Lemma 3.26</ref> Assume that the double indexed sequence w st , s ≤ t of non-negative numbers is bounded and such that, for all s, lim t→∞ w st = 0. Let β 1 , β 2 , . . . be such that</p><p>∞ t=1 |β t | &lt; ∞. Then lim t→∞ t s=1 β s w st = 0. Proof For any t 0 , we have t s=1 β s w st ≤ max s |β s | t 0 s=1 w st + max s,t |w st | s=t 0 +1 |β s | so that lim sup t→∞ t s=1 β s w st ≤ max s,t |w st | s=t 0 +1 |β s | and since this upper bound can be made arbitrarily small, the result follows. ■ Lemma 3.25 implies that a t ≤ a 0 v 0,t + C t s=1 α 2 s+1 v s,t .</p><p>Assume that</p><formula xml:id="formula_136">(H3) ∞ k=1 α k = ∞ and ∞ k=1 α 2 k &lt; ∞,</formula><p>Then lim t→∞ v st = 0 for all s and lemma 3.26 implies that a t tends to zero. So, we have just proved that, if (H1), (H2) and (H3) are true, the sequence X t converges in the L 2 sense to x * . Actually, under these conditions, one can show that X t converge to x * almost surely, and we refer to Benveniste et al. <ref type="bibr" target="#b43">[25]</ref>, Chapter 5, for a proof (the argument above for an L 2 convergence follows the one given in Nemirovski et al. <ref type="bibr" target="#b163">[145]</ref>).</p><p>Under (H3), one can say much more on the asymptotic behavior of the algorithm by comparing it with an ordinary differential equation. The "ODE method," introduced in Ljung <ref type="bibr" target="#b138">[120]</ref>, is indeed a fundamental tool for the analysis of stochastic approximation algorithms. The correspondence between discrete and continuous times is provided by the sequence α t . More precisely, let τ 0 = 0 and τ t = τ t-1 + α t , t ≥ 1. From (H3), τ t → ∞ when t → ∞. Define the piecewise linear interpolation x ℓ (ρ) of the sequence x t by X ℓ (ρ) = X t + ρτ t α t+1 (X t+1 -X t ), ρ ∈ [τ t , τ t+1 ).</p><p>Switching to continuous time allows us to interpret the average iteration xt+1 = xt + α t+1 H( xt ) as an Euler discretization scheme for the ordinary differential equation (ODE) ∂ ρ x = H( x). <ref type="bibr">(3.24)</ref> Most of the insight on long-term behavior of stochastic approximations results from the fact that the random process x behaves asymptotically like solutions of this ODE. One has, for example, the following result, for which we introduce some additional notation.</p><p>Assume that <ref type="bibr">(3.24)</ref> has unique solutions for given initial conditions on any finite interval, and denote by ϕ(ρ, ω) its solution at time ρ initialized with x(0) = ω. Let α c (ρ) and η c (ρ) be piecewise constant interpolations of (α t ) and (η t ) defined by α c (ρ) = α t+1 and η c (ρ) = η t+1 on the interval [τ t , τ t+1 ). The following proposition (see <ref type="bibr" target="#b41">[23]</ref>) compares the tails of the process x ℓ (i.e., the functions x ℓ (ρ + s), s ≥ 0) with the solutions of the ODE over finite intervals. Proposition 3.27 (Benaim) Assume that H is Lipschitz and bounded. Then, for some constant C(T ) that only depends on T and H, one has, for all ρ ≥ 0 sup h∈[0,T ]</p><formula xml:id="formula_137">|X ℓ (ρ + h) -ϕ(h, X ℓ (ρ))| ≤ C(T ) ∆(ρ -1, T + 1) + max s∈[ρ,ρ+T ] α c (s) . (<label>3.25)</label></formula><p>Recall that H being Lipschitz means that there exists a constant C such that</p><formula xml:id="formula_138">| H(w) -H(w ′ )| ≤ C|w -w ′ |</formula><p>for all w, w ′ ∈ R p .</p><p>In the upper-bound in <ref type="bibr">(3.25)</ref>, the term ∆(ρ -1, T + 1) is a random variable. It can be related to the variations</p><formula xml:id="formula_139">∆ ′ (t, N ) = max k=0,...,N |M t+k -M t |,</formula><p>where M is defined in <ref type="bibr">(3.19)</ref>, because, if m(ρ) is the largest integer t such that τ t ≤ ρ, then ∆ ′ (m(ρ) + 1, m(ρ + T )m(ρ)) ≤ ∆(ρ, T ) ≤ ∆ ′ (m(ρ), m(ρ + T )m(t) + 1).</p><p>In the case we are considering, one can use martingale inequalities (called Doob's inequalities) to control ∆ ′ . One has, for example,</p><formula xml:id="formula_140">P max 0≤k≤N |M t+k -M t | &gt; λ ≤ E(|M t+N -M t | 2 ) λ 2 . (3.26)</formula><p>Furthermore, using the fact that E(η k+1 η l+1 ) = 0 if k l, one has</p><formula xml:id="formula_141">E(|M t+N -M t | 2 ) = t+N k=t α 2 k+1 E(|η t+1 | 2 ).</formula><p>If we assume (to simplify) that H is bounded and ∞ k=1 α 2 k &lt; ∞ then, for some constant C, we have</p><formula xml:id="formula_142">E(|M t+N -M t | 2 ) ≤ C ∞ k=t α 2</formula><p>k+1 → 0 and inequality <ref type="bibr">(3.26)</ref> can then be used in <ref type="bibr">(3.25)</ref> to control the probability of deviation of the stochastic approximation from the solution of the ODE over finite intervals (a little more work is required under weaker assumptions on H, such as (H1)).</p><p>Proposition 3.27 cannot be used with T = ∞ because the constant C(T ) typically grows exponentially with T . In order to draw conclusions on the limit of the process W , one needs additional assumptions on the stability of the ODE. We refer to <ref type="bibr" target="#b41">[23]</ref> for a collection of results on the relationship between invariant sets and attractors of the ODE and limit trajectories of the stochastic approximation. We here quote one of these results which is especially relevant for SGD. Proposition 3. <ref type="bibr" target="#b46">28</ref> Assume that H = -∇E is the gradient of a function E and that ∇E only vanishes at a finite number of points. Assume also that X t is bounded. Then X t converges to a point x * such that ∇E(x * ) = 0. Some additional conditions on H can ensure that stochastic approximation trajectories remain bounded. The simplest one assumes the existence of a "Lyapunov function" that controls the ODE at infinity. The following result is a simplified version of Theorem 17 in Benveniste et al. <ref type="bibr" target="#b43">[25]</ref>. <ref type="bibr">Theorem 3.29</ref> In addition to the hypotheses previously made, assume that there exists a C 2 function U with bounded second derivatives and K 0 &gt; 0 such that, for allx such that |x| ≥ K 0 , ∇U (x) T H(x) ≤ 0, U (x) ≥ γ|x| 2 , γ &gt; 0.</p><p>Then, the trajectories X ℓ (ρ) are almost surely bounded.</p><p>Note that hypothesis (H2) above implies the theorem's assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">The ADAM algorithm</head><p>ADAM (for adaptive moment estimation <ref type="bibr" target="#b120">[102]</ref>) is a popular variant of stochastic gradient descent. When dealing with high-dimensional vectors W , using a single "gain" parameter (γ n+1 in (11.2)) is a limiting assumption since all parameters do not need to scale the same way. This can sometimes be handled by reweighting the components of H, i.e., using iterations</p><formula xml:id="formula_143">X t+1 = X t + α t D t H(X t , ξ t+1 )</formula><p>where D t is a (typically diagonal) matrix. The previous theory can be applied to situations in which D may be random, provided it converges almost surely to a fixed matrix.</p><p>The ADAM algorithm provides such a construction (without the theoretical guarantees) in which D t is computed using past iterations of the algorithm. It requires several parameters, namely: α: the algorithm gain, taken as constant (e.g., α = 0.001); Two parameters β 1 and β 2 for moment estimates (e.g. β 1 = 0.9 and β 2 = 0.999); A small number ϵ (e.g., ϵ = 10 -8 ) to avoid divisions by 0. In addition, ADAM defines two vectors: a mean m and a second moment v, respectively initialized at 0 and 1. The ADAM iterations are given below, in which g ⊗2 denotes the vector obtained by squaring each coefficient of a vector g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3.1 (ADAM)</head><p>1. Let X t be the current state, m t and v t the current mean and variance.</p><p>2. Generate ξ t+1 and let g t+1 = H(X t , ξ t+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Update</head><formula xml:id="formula_144">m t+1 = β 1 m t + (1 -β 1 )g t+1 . 4. Update v t+1 = β 2 v t + (1 -β 2 )g ⊙2 t+1 . 5. Let mt+1 = m t+1 /(1 -β t+1 1 ) and vt+1 = v t+1 /(1 -β t+1 2 ) 6. Set X t+1 = X t -α mt+1 √ vt+1 + ϵ</formula><p>Note that the iteration on m t and v t correspond to defining</p><formula xml:id="formula_145">mt = β 1 1 -β t 1 t k=0 (1 -β 1 ) t-k g k and vt = β 2 1 -β t 2 t k=0 (1 -β 2 ) t-k g ⊙2 k .</formula><p>3.4 Constrained optimization problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Lagrange multipliers</head><p>A constrained optimization problem minimizes a function F over a closed subset Ω of R d , with Ω R d . This restriction invalidates, in a large part, the optimality conditions discussed in section 3.2. These conditions indeed apply to minimizers belonging to the interior of Ω, and therefore do not hold when they lie at its boundary, which is a very common situation in practice (Ω often has an empty interior).</p><p>In this section, which follows the discussion given in Wright and Recht <ref type="bibr" target="#b222">[204]</ref>, we review conditions for optimality for constrained minimization of smooth functions, in two cases. The first one, discussed in this section, is when Ω is defined by a finite number of smooth constraints, leading, under some assumptions, to the Karush-Kuhn-Tucker (or KKT) conditions. The second one, in the next section, specializes to closed convex Ω.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KKT conditions</head><p>We introduce some notation. Let γ i , for i ∈ C, be C 1 functions γ i : R d → R, where C is a finite set of indices. We assume that C is divided into two non-intersecting parts, C = E ∪ I and consider minimization problems searching for</p><formula xml:id="formula_146">x * ∈ argmin Ω F (3.27)</formula><p>where</p><formula xml:id="formula_147">Ω = {x ∈ R d : γ i (x) = 0, i ∈ E and γ i (x) ≤ 0, i ∈ I }.</formula><p>(3.28)</p><p>The set Ω of all x that satisfy the constraints is called the feasible set for the considered problem. We will always assume that it is non-empty. If x ∈ Ω, one defines the set A(x) of active constraints at x to be</p><formula xml:id="formula_148">A(x) = {i ∈ C : γ i (x) = 0} .</formula><p>One obviously has E ⊂ A(x) for x ∈ Ω.</p><p>To be valid, the KKT conditions require some additional assumptions on potential minimizers, called "constraint qualifications." An instance of such assumptions is provided by the next definition. (MF2) There exists a vector h ∈ R d such that h T ∇γ i (x) = 0 for all i ∈ E and h T ∇γ i (x) &lt; 0 for all i ∈ A(x) ∩ I .</p><p>A sufficient (and easier to check) condition for x to satisfy these constraints is when the vectors (∇γ i (x), i ∈ A(x)) are linearly independent <ref type="bibr" target="#b55">[37]</ref>. Indeed, if the latter "LI-CQ" condition is true, then any set of values can be assigned to h T ∇γ i (x) with the existence of a vector h that achieves them.</p><p>We introduce the Lagrangian</p><formula xml:id="formula_149">L(x, λ) = F(x) + i∈C λ i γ i (x) (3.29)</formula><p>where the real numbers λ i , i ∈ C are called Lagrange multipliers. The following theorem (stated without proof, see, e.g., <ref type="bibr" target="#b164">[146,</ref><ref type="bibr" target="#b53">35]</ref>) provides necessary conditions satisfied by solutions of the constrained minimization problem that satisfy the constraint qualifications.</p><p>Theorem 3.31 Assume x * ∈ Ω is a solution of (3.27), and that x * satisfies the MF-CQ conditions. Then there exist Lagrange multipliers λ i , i ∈ C, such that</p><formula xml:id="formula_150">∂ x L(x * , λ) = 0 λ i ≥ 0 if i ∈ I , with λ i = 0 when i A(x * ) (3.30)</formula><p>Conditions <ref type="bibr">(3.30)</ref> are the KKT conditions for the constrained optimization problem.</p><p>The second set of conditions is often called the complementary slackness conditions and state that λ i = 0 for an inequality constraint unless this constraint is satisfied with an equality. The next section provides examples in which the MF-CQ conditions are not satisfied and Theorem 3.31 does not hold. However, these conditions are not needed in the special case when the constraints are affine.</p><p>Theorem 3.32 Assume that for all i ∈ A(x * ), the functions γ i are affine, i.e., γ i (x) = b T i x + β i for some b ∈ R d and β ∈ R. Then (3.30) holds at any solution of (3.27).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3.33</head><p>We have taken the convention to express the inequality constraints as γ i (x) ≤ 0, i ∈ I . With the reverse convention, i.e., γ i (x) ≥ 0, i ∈ I , one generally defines the Lagrangian as</p><formula xml:id="formula_151">L(x, λ) = F(x) - i∈C λ i γ i (x)</formula><p>and the KKT conditions remain unchanged. ♦</p><p>Examples. Constraint qualifications are important to ensure the validity of the theorem. Consider a problem with equality constraints only, and replace it by</p><formula xml:id="formula_152">x * ∈ argmin Ω F subject to γi (x) = 0, i ∈ E, with γi = γ 2 i .</formula><p>We clearly did not change the problem. However, the previous theorem applied to the Lagrangian</p><formula xml:id="formula_153">L(x, λ) = F(x) + i∈C λ i γi (x)</formula><p>would require an optimal solution to satisfy ∇F(x) = 0, because ∇ γi (x) = 2γ i (x)∇γ i (x) = 0 for any feasible solution. Minimizers of constrained problems do not necessarily satisfy ∇F(x) = 0, however. This is no contradiction with the theorem since ∇ γi (x) = 0 for all i shows that no feasible point satisfies the MF-CQ.</p><p>To take a more specific example, still with equality constraints, let d = 3, C = {1, 2} with F(x, y, z) = x/2+y and γ 1 (x, y, z) = x 2 -y 2 , γ 2 (x, y, z) = y -z 2 . Note that γ 1 = γ 2 = 0 implies that y = |x|, so that, for a feasible point, F(x, y, z) = |x| + x/2 ≥ 0 and vanishes only when x = y = 0, in which case z = 0 also. So (0, 0, 0) is a global minimizer. We have dF(0) = (1/2, 1, 0), dγ 1 (0) = (0, 0, 0) and dγ 2 (0) = (0, 1, 0) so that 0 does not satisfy the MF-CQ. The equation</p><formula xml:id="formula_154">dF(0) + λ 1 dγ 1 (0) + λ 2 dγ 2 (0) = 0</formula><p>has no solution (λ 1 , λ 2 ), so that the conclusion of the theorem does not hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Convex constraints</head><p>We now consider the case in which Ω is a closed convex set. To specify the optimality conditions in this case, we need the following definition.</p><formula xml:id="formula_155">Definition 3.34 Let Ω ⊂ R d be convex and let x ∈ Ω. The normal cone to Ω at x is the set N Ω (x) = {h ∈ R d : h T (y -x) ≤ 0 for all y ∈ Ω} (3.31)</formula><p>The normal cone is an example of convex cone. (A convex subset Γ of R d is called a convex cone, if it is such that λx ∈ Γ for all x ∈ Γ and λ ≥ 0, a property obviously satisfied by N Ω (x).) It should also be clear from the definition that non-zero vectors in N Ω (x) always point outside Ω, i.e., x + h Ω if h ∈ N Ω (x), h 0. Here are some examples.</p><p>• If x is in the interior of Ω, then N Ω (x) = {0}.</p><p>• Assume that Ω is a half space, i.e., Ω = {x : b T x + β ≤ 0} with |b| = 1, and take x ∈ ∂Ω, i.e., b T x + β = 0. Then</p><formula xml:id="formula_156">N Ω (x) = {h = µb : µ ≥ 0} .</formula><p>Indeed, any element of R d can be written as y = x +λb +q with q T b = 0, and y ∈ Ω if and only if λ ≤ 0. Fix such a y and take h ∈ R d , decomposed as h = µb + r, with r T b = 0. We have h T (yx) = λµ + r T q. Clearly, if µ &lt; 0, or if r 0, one can find λ ≤ 0 and q ⊥ b such that h T (yx) &gt; 0. One the other hand, if µ ≤ 0 and r = 0, we have h T (yx) ≤ 0 for all y ∈ Ω, which proves the above statement.</p><p>• With a similar argument, if Ω = {x : b T x + β = 0} is a hyperplane, one finds that</p><formula xml:id="formula_157">N Ω (x) = {h = λb : λ ∈ R} .</formula><p>One can build normal cones to domains associated with multiple inequalities or equalities based on the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3.35</head><p>Let Ω 1 and Ω 2 be two convex sets with relint(Ω 1 )∩relint(Ω 2 ) ∅.</p><formula xml:id="formula_158">Then, if x ∈ Ω 1 ∩ Ω 2 N Ω 1 ∩Ω 2 (x) = N Ω 1 (x) + N Ω 2 (x)</formula><p>Here, the addition is the standard sum between sets in a vector space:</p><formula xml:id="formula_159">A + B = {x + y : x ∈ A, y ∈ B}.</formula><p>Finally, we note that, if x ∈ relint(Ω), then</p><formula xml:id="formula_160">N Ω (x) = {h ∈ R d : h T (y -x) = 0, y ∈ Ω}. (3.32)</formula><p>Indeed, if y ∈ Ω, then x + ϵ(yx) ∈ Ω for small enough ϵ (positive or negative). For h ∈ N Ω (x), the condition ϵh T (yx) ≤ 0 for small enough ϵ requires that h T (yx) = 0.</p><p>With this definition in hand, we have the following theorem.</p><p>Theorem 3.36 Let F be a C 1 function and Ω a closed convex set. If x * ∈ argmin Ω F, then -∇F(x * ) ∈ N Ω (x * ). (3.33) If F is convex and (3.33) holds, then x * ∈ argmin Ω F.</p><formula xml:id="formula_161">Proof Assume that x * ∈ argmin Ω F. If y ∈ Ω, then x * + t(y -x * ) ∈ Ω for all t ∈ [0, 1] and the function f (t) = F(x + t(y -x * )) is C 1 on [0, 1],</formula><p>with a minimum at t = 0. This requires that</p><formula xml:id="formula_162">∂ t f (0) = ∇F(x * ) T (y -x * ) ≥ 0, because, if ∂ t f (0) &lt; 0, a Taylor expansion would show that f (t) &lt; f (0) for small enough t &gt; 0.</formula><p>If F is convex and (3.33) holds, we have F(y) ≥ F(x * )+∇F(x * ) T (y -x * ) by convexity, so that</p><formula xml:id="formula_163">F(x * ) ≤ F(y) + (-∇F(x * )) T (y -x * ) ≤ F(y). ■</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Applications</head><p>Lagrange multipliers revisited. Consider Ω defined by (3.28), with the additional assumptions that</p><formula xml:id="formula_164">γ i (x) = b T i x + β i for i ∈ E and γ i is convex for i ∈ I , which ensure that Ω is convex. Define N ′ γ (x) =          g = i∈A(x) λ i ∇γ i (x) : λ i ≥ 0, i ∈ A(x) ∩ I          .</formula><p>Then, the KKT conditions in (3.30) can be rewritten as</p><formula xml:id="formula_165">-∇F(x * ) ∈ N ′ γ (x * ).</formula><p>Note that one always have</p><formula xml:id="formula_166">N ′ γ (x) ⊂ N Ω (x) since, for g = i∈A(x) λ i ∇γ i (x) ∈ N ′ γ (x), one has, for y ∈ Ω, g T (y -x) = i∈A(x) λ i ∇γ i (x) T (y -x) = i∈E λ i (a T i y -a T i x) + i∈A(x)∩I λ i (γ i (x) + ∇γ i (x) T (y -x)) = i∈A(x)∩I λ i (γ i (x) + ∇γ i (x) T (y -x)) ≤ λ i γ i (y) ≤ 0,</formula><p>in which the have used the facts that a T i x = a T i y = -β i for x, y ∈ Ω, i ∈ E, γ i (x) = 0 for i ∈ A(x) and the convexity of γ i . Constraint qualifications such as those considered above are sufficient conditions that ensure the identity between the two sets.</p><p>Consider now the situation of theorem 3.32, and assume that all constraints are affine inequalities, γ i (x) = b T i x + β ≤ 0, i ∈ I . Then, the statement N Ω (x) ⊂ N ′ γ (x) can be reexpressed as follows. All h ∈ R d such that h T (yx) ≤ 0 as soon as b T i (yx) ≤ 0 for all i ∈ A(x) must take the form</p><formula xml:id="formula_167">h = i∈A(x) λ i b i</formula><p>with λ (i) ≥ 0. This property is called Farkas's lemma (see, e.g. <ref type="bibr" target="#b185">[167]</ref>). Note that affine equalities b T i x + β = 0 can be included as two inequalities b T i x + β ≤ 0, -b T i xβ ≤ 0, which removes the sign constraint on the corresponding λ (i) and therefore yields theorem 3.32.</p><p>Positive semi-definite matrices. We now take an example in which theorem 3.32 does not apply directly. Let Ω = S + n be the space of positive semidefinite n × n matrices, considered as a subset of the space M n of n × n matrices, itself identified with R n 2 . With this identification, the Euclidean inner product between two matrices can be expressed as (A, B) → trace(A T B).</p><p>We have A ∈ S + n if and only if, for all u ∈ R d , u T Au ≥ 0, which provides an infinity of linear inequality constraints on A. Elements of N S + (A) are matrices H ∈ M n such that trace(H T (B -A)) ≤ 0 for all B ∈ S + n , and we want to make this normal cone explicit. We first note that, every square matrix H can be decomposed as the sum of a symmetric matrix, H s and of a skew symmetric one, H a (namely, H s = (H + H T )/2 and H a = (H -H T )/2). We have moreover trace(H T a (B -A)) = 0, so the condition is only on the symmetric part of H.</p><p>For any u ∈ R d , one can take B = A+uu T , which belongs to S + n , with trace(H T s (B-A)) = u T H s u. This shows that, for H to belong to N S + n (A), one needs H s ⪯ 0. Now, take an eigenvector u of A with eigenvalue ρ &gt; 0. Then B = Aαuu T is also in S + n as soon as 0 ≤ α ≤ ρ, and trace(H T s (B -A)) = -αu T H s u. So, if H ∈ N S + n (A), we have u T H s u ≥ 0, and since H s ⪯ 0, this gives u T H s u = 0. Still because H s is negative semi-definite, this implies H s u = 0. (This can be shown, for example, using Schwarz's inequality which says that (u</p><formula xml:id="formula_168">T H s v) 2 ≤ (u T H s u)(v T H s v) for all v ∈ R d .)</formula><p>Decomposing A with respect to its non-zero eigenvectors, i.e., writing</p><formula xml:id="formula_169">A = p k=1 ρ k u k u T k</formula><p>where p = rank(A), we get AH s = H s A = 0. We therefore obtained the proposition</p><p>Proposition 3.37 Let A ∈ S + n . Then H ∈ M n belongs to N S + n (A) if and only if -H s ∈ S + n and H s A = 0, where H s</p><formula xml:id="formula_170">= (H + H T )/2.</formula><p>Now, if one wants to minimize a function F over positive semidefinite matrices, and A * is a minimizer, we get the necessary condition that A * (∇F(A * )) s = 0 with (∇F(A * )) s positive semidefinite. These conditions are sufficient if F is convex.</p><p>For example, take</p><formula xml:id="formula_171">F(A) = 1 2 trace(A 2 ) -trace(BA) (3.34) with B ∈ S n . Then (∇F(A)) s = A -B and the condition is A(A -B) = 0 with A ⪰ B. If B is diagonalized in the form B = U T DU</formula><p>, with U orthogonal and D diagonal, then the solution is A * = U T D + U where D + is deduced from U by replacing non-negative entries by zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection.</head><p>Let Ω be closed convex,</p><formula xml:id="formula_172">x 0 ∈ R d and F(x) = 1 2 |x -x 0 | 2 . We have min Ω F = min Ω∩ B(0,R)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>for large enough R (e.g., larger than F(x) for any fixed point in Ω), and since the latter minimization is over a compact set, argmin Ω F is not empty. The function F being strongly convex, its minimizer over Ω is unique and called the projection of x 0 on Ω, denoted proj Ω (x 0 ).</p><p>Since ∇F(x) = xx 0 , theorem 3.36 implies that proj Ω (x 0 ) is characterized by proj Ω (x 0 ) ∈ Ω and x 0proj Ω (x 0 ) ∈ N Ω (proj Ω (x 0 )) <ref type="bibr">(3.35)</ref> or (x 0proj Ω (x 0 )) T (yproj Ω (x 0 )) ≤ 0 for all y ∈ Ω.</p><p>(3.36)</p><formula xml:id="formula_173">If x 0 Ω, then proj Ω (x 0 ) ∈ ∂Ω, since otherwise we would have N Ω (proj Ω (x 0 )) = {0} and x 0 = proj Ω (x 0 ), a contradiction. Of course, if x 0 ∈ Ω, then proj Ω (x 0 ) = x 0 .</formula><p>Here are some important examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>Let Ω = z 0 + V , where z 0 ∈ R d and V is a linear space (i.e., Ω is an affine subset of R d ). Then N Ω (x) = z 0 + V ⊥ = x + V ⊥ for all x ∈ Ω, where V ⊥ is the vector space of vectors orthogonal to V , and proj Ω (x 0 ) is characterized by proj Ω (x 0 ) ∈ Ω and</p><formula xml:id="formula_174">(x 0 -proj Ω (x 0 )) ∈ V ⊥</formula><p>which is the usual characterization of the orthogonal projection on an affine space (compare to section 6.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>If Ω = B(0, 1), the closed unit sphere, then N Ω (x) = R + x for x ∈ ∂Ω (i.e., |x| = 1).</p><p>One can indeed note that, if h 0 in normal to Ω at x, then h/|h| ∈ Ω so that</p><formula xml:id="formula_175">h T h |h| -x ≤ 0 which yields |h| ≤ h T x.</formula><p>The Cauchy-Schwartz inequality implying that h T x ≤ |h| |x| = |h|, we must have equality, h T x = |h| |x|, which is only possible when x and h are collinear.</p><p>Given x 0 ∈ R d with x 0 ≥ 1, we see that proj Ω (x 0 ) must satisfy the conditions |proj Ω (x 0 )| = 1 (to be in ∂Ω) and x 0proj Ω (x 0 ) = λx 0 for some λ ≥ 0, which gives proj Ω (x 0 ) = x 0 /|x 0 |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>If Ω = S + n and B (taking the role of x 0 ) is a symmetric matrix, then proj Ω (B) was found in the previous section, and is given by A = U T D + U where U T DU provides a diagonalization of B.</p><p>The projection has the important property of being 1-Lipschitz.</p><formula xml:id="formula_176">Proposition 3.38 Let Ω be a closed convex subset of R d . Then, for all x, y ∈ R d |proj Ω (x) -proj Ω (y)| ≤ |x -y|. (3.37)</formula><p>Proof This proposition is a special case of proposition 3.55 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Projected gradient descent</head><p>The projected gradient descent algorithm minimizes F over Ω by iterating</p><formula xml:id="formula_177">x t+1 = proj Ω (x t -α t ∇F(x t )),<label>(3.38)</label></formula><p>which provides a feasible method when proj Ω is easy to compute. An equivalent formulation is</p><formula xml:id="formula_178">x t+1 = argmin Ω F(x t ) + ∇F(x t ) T (x -x t ) + 1 2α t |x -x t | 2 . (3.39)</formula><p>To justify this last statement it suffices to notice that the function in the r.h.s. can be written as</p><formula xml:id="formula_179">1 2α t |x -x t + α t ∇F(x t )| 2 - α t 2 |∇F(x t )| 2 + F(x t )</formula><p>and apply the definition of the projection.</p><p>The convergence properties of this algorithm will be discussed in section 3.5.5, in a more general context. </p><formula xml:id="formula_180">epi(F) = (x, a) ∈ R d × R : F(x) ≤ a . (3.40) One says that F is closed if epi(F) is a closed subset of R d × R, that is: if x = lim n x n and a = lim n a n with F(x n ) ≤ a n , then F(x) ≤ a. Clearly, if (x, a) ∈ epi(F), then x ∈ dom(F). It should also be clear that epi(F) is always convex when F is convex: If (x, a), (y, b) ∈ epi(F), then F((1 -t)x + ty) ≤ (1 -t)F(x) + tF(y) ≤ (1 -t)a + tb so that (1 -t)(x, a) + t(y, b) ∈ epi(F).</formula><p>To illustrate the definition, consider a simple example. Let F be the function defined on R by F(x) = |x| if |x| &lt; 1 and F(x) = +∞ otherwise. It is convex, but not closed, as can be seen by taking the sequence (1 -1/n, 1) ∈ epi(F), with, at the limit, F(1) = +∞ &gt; 1. In contrast, the function defined by F(x) = |x| if |x| ≤ 1 and F(x) = +∞ otherwise is convex and closed.</p><p>We have the following proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 3.40 A convex function F is closed if and only if all its sub-level sets</head><formula xml:id="formula_181">Λ a (F) = x ∈ R d : F(x) ≤ a are closed subsets of R d . Proof If F is closed, then Λ a (F) is the intersection of the set {(x, a) : x ∈ R d }, which is</formula><p>obviously closed, and of epi(F). It is therefore a closed set.</p><p>Conversely, assume that all Λ a (F) are closed and take a sequence (x n , a n ) in epi(F) that converges to (x, a). Then, fixing ϵ &gt; 0, x n ∈ Λ a+ϵ for large enough n, and since this set is closed, F(x) ≤ a + ϵ. Since this is true for all ϵ &gt; 0, we have F(x) ≤ a and (x, a) ∈ epi(F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Note that, if F is continuous, then it is closed, so that closedness generalizes continuity for convex functions, but it also applies to the non-smooth case.</p><p>If Ω is a convex subset of R d , its indicator function σ Ω (such that σ Ω (x) = 0 for x ∈ Ω and σ Ω (x) = +∞ otherwise) is closed if and only if Ω is a closed subset of R d . This is obvious since Λ a (σ Ω ) = Ω if a ≥ 0 and ∅ otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Subgradients</head><p>Several machine learning problems involve convex functions that are not C 1 , requiring a generalization of the notion of derivative provided by the following definition. Definition 3.41 If F is a convex function and x ∈ dom(F), a vector g ∈ R d such that</p><formula xml:id="formula_182">F(x) + g T (y -x) ≤ F(y) (3.41) for all y ∈ R d is called a subgradient of F at x.</formula><p>The set of subgradients of F at x is denoted ∂F(x) and called the subdifferential of F at x.</p><formula xml:id="formula_183">If x ∈ int(dom(F)) and F is differentiable at x, (3.5) implies that ∇F(x) ∈ ∂F(x).</formula><p>This is in this case the only element of ∂F(x).</p><formula xml:id="formula_184">Proposition 3.42 If F is differentiable at x ∈ int(dom(F)), then ∂F(x) = {∇F(x)}.</formula><p>Proof We need to prove that there is no other subgradient. Assume that ∇F(x) exists and take</p><formula xml:id="formula_185">y = x + ϵu in (3.41) (u ∈ R d ). One gets, for g ∈ ∂F(x), ϵg T u ≤ F(x + ϵu) -F(x) = ϵ∇F(x) T u + o(ϵ)</formula><p>Dividing by |ϵ| and letting ϵ → 0 gives (depending on the sign of ϵ) g T u ≤ ∇F(x) T u andg T u ≤ -∇F(x) T u This is only possible if g T u = ∇F(x) T u for all u ∈ R d , which itself implies g = ∇F. ■ The next theorem, which is an obvious consequence of definition 3.41, characterizes minimizers of convex functions in the general case.</p><formula xml:id="formula_186">Theorem 3.43 Let F : R d → R be convex. Then x is a (global) minimizer of F if and only if 0 ∈ ∂F(x).</formula><p>The following result shows that subgradients exist under generic conditions. We note that g ∈ ∂F(x) if and only if proj--→ aff (dom(F)) (g) ∈ ∂F, because (3.41) is trivial if F(y) = +∞. So ∂F cannot be bounded unless aff(dom(D)) = R d . However, it is the part of this set that is included in the</p><formula xml:id="formula_187">--→ aff (dom(F)) that is of interest. Proposition 3.44 For all x ∈ R d , ∂F(x) is a closed convex set (possibly empty, in par- ticular for x dom(F)). If x ∈ ridom(F), then ∂F(x) ∅ and ∂F(x) ∩ --→ aff (dom(F)) is compact.</formula><p>Proof The convexity and closedness of ∂F(x) is clear from the definition. If x ∈ ridom(F), there exists ϵ &gt; 0 such that x + ϵh ∈ ridom(F) for all h ∈ --→ aff (dom(F)) with</p><formula xml:id="formula_188">|h| = 1. For all g ∈ ∂F(x) ∩ --→ aff (dom(F)), one has |g| = max{g T h : h ∈ --→ aff (dom(F)), |h| = 1} ≤ max((F(x + ϵh) -F(x))/ϵ : h ∈ --→ aff (dom(F)), |h| = 1)</formula><p>and the upper bound is finite because it is the maximum of a continuous function over a bounded set. This shows that ∂F(x) is bounded. We defer the proof that ∂F(x) ∅ to section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Subdifferentials are, under mild conditions, additive. More precisely, we have the following proposition. Theorem 3.45 Let F 1 and F 2 be convex functions such that</p><formula xml:id="formula_189">ridom(F 1 ) ∩ ridom(F 2 ) ∅ . Then, for all x ∈ R d , ∂(F 1 + F 2 )(x) = ∂F 1 (x) + ∂F 2 (x).</formula><p>Note that the inclusion</p><formula xml:id="formula_190">∂F 1 (x) + ∂F 2 (x) ⊂ ∂(F 1 + F 2 )(x)</formula><p>as can be immediately checked by summing the inequalities satisfied by subgradients. The reverse inclusion requires the use of separation theorems for convex sets (see section 3.7).</p><p>Another important point is how the chain rule works with compositions with affine functions.</p><formula xml:id="formula_191">Theorem 3.46 Let F be a convex function on R d , A a d × m matrix and b ∈ R d . Let G(x) = F(Ax + b), x ∈ R m . Assume that there exists x 0 ∈ R m such that Ax 0 ∈ ridom(F). Then, for all x ∈ R m , ∂G(x) = A T ∂F(Ax + b).</formula><p>One direction is straightforward and does not require the condition on ridom(F). If</p><formula xml:id="formula_192">g ∈ ∂F(Ax + b), then F(z) -F(Ax + b) ≥ g T (z -Ax -b), z ∈ R d and applying this inequality to z = Ay + b for y ∈ R m yields G(y) -G(x) ≥ g T A(y -x)</formula><p>so that A T g ∈ ∂G and A T ∂F ⊂ ∂G. The reverse inclusion is proved in section 3.7.</p><p>Subdifferentials can be seen as generalizations of normal cones. Proposition 3.47 Assume that Ω is a closed convex subset of R d . Then σ Ω (the indicator function of Ω) has a subdifferential everywhere on Ω with</p><formula xml:id="formula_193">∂σ Ω (x) = N Ω (x), x ∈ Ω Proof For x ∈ Ω, (3.41) is g T (y -x) ≤ σ Ω (y) for y ∈ R d , but since σ Ω (y) = +∞ outside of Ω, g ∈ ∂σ Ω (x) is equivalent to g T (y -x) ≤ 0</formula><p>for y ∈ Ω, which is exactly the definition of the normal cone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Given this proposition, it is also clear (after noting that σ</p><formula xml:id="formula_194">Ω 1 + σ Ω 2 = σ Ω 1 ∩Ω 2 ) that theorem 3.</formula><p>45 is a generalization of theorem 3.35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Directional derivatives</head><p>From proposition 3.5, applied with y = x + h, we see that</p><formula xml:id="formula_195">t → 1 t (F(x + th) -F(x))</formula><p>is increasing as a function of t. This property allows us to define directional derivatives of F at x. Definition 3.48 Let F be convex and x ∈ dom(F). The directional derivative of F at x in the direction h ∈ R d is defined by</p><formula xml:id="formula_196">dF(x, h) = lim t↓0 1 t (F(x + th) -F(x)),<label>(3.42</label></formula><p>)</p><formula xml:id="formula_197">and belong to [-∞, +∞].</formula><p>Note that, still from proposition 3.5, one has, for all x ∈ dom(F) and y ∈ R d :</p><formula xml:id="formula_198">F(y) ≥ F(x) + dF(x, y -x) (3.43)</formula><p>We have the proposition:</p><formula xml:id="formula_199">Proposition 3.49 If F is convex, then x * ∈ argmin(F) if and only if dF(x * , h) ≥ 0 for all h ∈ R d .</formula><p>Proof If dF(x * , h) ≥ 0, then F(x * +th)-F(x * ) ≥ 0 for all t &gt; 0 and this being true for all h implies that x * is a minimizer. Conversely, if x * is a minimizer, dF(x * , h) is a limit of non-negative numbers and is therefore non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Proposition 3.50 If F is convex and x ∈ dom(F), then dF(x, h) is positively homogeneous and subadditive (hence convex) as a function of h, namely</p><formula xml:id="formula_200">dF(x, λh) = λdF(x, h), λ &gt; 0 and dF(x, h 1 + h 2 ) ≤ dF(x, h 1 ) + dF(x, h 2 ).</formula><p>Proof Positive homogeneity is straightforward and left to the reader. For the second one, we can write</p><formula xml:id="formula_201">F(x + th 1 + th 2 ) ≤ 1 2 (F(x + th 1 /2) + F(x + th 2 /2))</formula><p>by convexity so that</p><formula xml:id="formula_202">1 t (F(x + th 1 + th 2 ) -F(x)) ≤ 1 2 1 t (F(x + th 1 /2) -F(x)) + 1 t (F(x + th 2 /2) -F(x)) .</formula><p>Taking t ↓ 0,</p><formula xml:id="formula_203">dF(x; h 1 + h 2 ) ≤ 1 2 (dF(x; h 1 /2) + dF(x, h 2 /2)) = dF(x, h 1 ) + dF(x, h 2 ). ■ Proposition 3.51 If F is convex and x ∈ dom(F), then dF(x, h) ≥ sup{g T h, g ∈ ∂F(x)}. If x ∈ ridom(F), then dF(x, h) = max{g T h, g ∈ ∂F(x)}. Proof If g ∈ ∂F(x), then for all t &gt; 0 F(x + th) -F(x) ≥ tg T h.</formula><p>Dividing by t and passing to the limit yields dF(x, h) ≥ g T h.</p><p>We prove that the maximum is attained at some g ∈ ∂F(x) when x ∈ ridom(F). In this case, the domain of the convex function G : h → dF(x, h) is the vector space parallel to aff(dom(F)), namely</p><formula xml:id="formula_204">dom(G) = {h : x + h ∈ aff(dom(F))}.</formula><p>Indeed, for any h in this set, there exists ϵ &gt; 0 such that x + th ∈ dom(F) for 0 &lt; t &lt; ϵ and dF(x, h) ≤ (F(x + th) -F(x))/t &lt; ∞. Conversely, if h ∈ dom(G), then F(x + th) -F(x) must be finite for small enough t, so that x + th ∈ dom(F) and x + h ∈ aff(dom(F)).</p><p>As a consequence, for any h ∈ aff(dom(F)), there exists ĝ ∈ ∂G(h), which therefore satisfies dF(x, h) ≥ dF(x, h) + ĝT ( hh)</p><formula xml:id="formula_205">for any h ∈ R d (the upper bound is infinite if h dom(G)). Letting h → 0, we get dF(x, h) ≤ ĝT h.</formula><p>Also, by positive homogeneity, we have</p><formula xml:id="formula_206">tdF(x, h) ≥ dF(x, h) + ĝT (t h -h)</formula><p>for all t &gt; 0, which requires dF(x, h) ≥ ĝT h for all h, and in particular dF(x, h) = ĝT h.</p><formula xml:id="formula_207">Since F(x + h) -F(x) ≥ dF(x, h) ≥ ĝT h</formula><p>we see that ĝ ∈ ∂F(x), with ĝT h = dF(x, h), which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The next proposition gives a criterion for a vector g to belong to ∂F(x) based on directional derivatives.</p><formula xml:id="formula_208">Proposition 3.52 Assume that x ∈ dom(F) where F is convex. If g ∈ R d is such that dF(x, h) ≥ g T h for all h ∈ R d , then g ∈ ∂F(x).</formula><p>Proof Just use the fact that dF(x, h) ≤ F(x + h) -F(x). ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Subgradient descent</head><p>When F is a non-differentiable a convex function, directions g such that -g ∈ ∂F(x) do not always provide directions of descent. Indeed, g ∈ ∂F(x) implies</p><formula xml:id="formula_209">F(x -αg) ≥ F(x) -α|g| 2</formula><p>but the inequality goes in the "wrong direction." However, we know that, for any h ∈ R d , there exists g h ∈ ∂F(x) such that</p><formula xml:id="formula_210">dF(x, -h) = -g T h h ≥ -g T h</formula><p>for all g ∈ ∂F(x). As a consequence, any non-vanishing solution of the equation h = g h will provide a direction of descent. This suggests looking for h ∈ ∂F(x) such that h 0 and |h| 2 ≤ g T h for all g ∈ ∂F(x).</p><p>Since g T h ≤ |g| |h|, this requires that |h| ≤ |g| for all g ∈ ∂F(x), i.e., h = argmin ∂F(x) (g → |g|). (3.44) Conversely, if h is the minimal-norm element of ∂F(x) (which is necessarily unique since the norm is strictly convex and ∂F(x) is convex and compact), then |h| 2 ≤ |h + t(gh)| 2 for all g ∈ ∂F(x) and t ∈ [0, 1], and taking the difference yields 2th</p><p>T</p><formula xml:id="formula_211">(g -h) + t 2 |g -h| 2 ≥ 0.</formula><p>The fact that this holds for all t ≥ 0 requires that h T (gh) ≥ 0 as required. We have therefore proved that h defined by (3.44) is a descent direction for F at x (it is actually the steepest descent direction: see <ref type="bibr" target="#b222">[204]</ref> for a proof), justifying the algorithm</p><formula xml:id="formula_212">x t+1 = x t -α t argmin ∂F(x) (g → |g|)</formula><p>as subgradient descent iterations.</p><p>Example. Consider the minimization of</p><formula xml:id="formula_213">F(x) = ψ(x) + λ n i=1 |x (i) |</formula><p>where</p><formula xml:id="formula_214">ψ is a C 1 convex function on R d . Let A(x) = {i : x (i) = 0}. Then ∂F(x) =          ∇ψ(x) + λ i A(x) sign(x (i) ) + λ i∈A(x) ρ i e i : |ρ i | ≤ 1, i ∈ A(x)         </formula><p>where e i is the ith vector of the canonical basis of R d .</p><p>For g = ∇ψ(x) + λ i A(x) sign(x (i) ) + λ i∈A(x) ρ i e i , we have</p><formula xml:id="formula_215">|g| 2 = i A(x) (∂ i F(x) + λ sign(x (i) )) 2 + i∈A(x) (∂ i ψ(x) -λρ i ) 2 .</formula><p>Define s(t) = sign(t) min(|t|, 1).</p><p>Then h satisfying (3.44) is given by</p><formula xml:id="formula_216">h (i) = ∂ i ψ(x) -λ sign(x (i) ) if i A(x) λ s(∂ i ψ(x)/λ) if i ∈ A(x).</formula><p>In more complex situations, the extra minimization step at each iteration of the algorithm can be challenging computationally. The following subgradient method uses an averaging approach to minimize F without requiring finding subgradients with minimal norms. It simply defines</p><formula xml:id="formula_217">x t+1 = x t -α t g t , g t ∈ ∂F(x t ) and computes xt = t j=1 α j x j t j=1 α j .</formula><p>We refer to <ref type="bibr" target="#b222">[204]</ref> for a proof of convergence of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">Proximal Methods</head><p>Proximal operator. We start with a few simple facts. Let F be a closed convex function and ψ be convex and differentiable, with dom</p><formula xml:id="formula_218">(ψ) = R d . Let G = F + ψ.</formula><p>Then G is a closed convex function. Indeed, consider the sub-level set Λ a (G) = {x : G(x) ≤ a} and assume that x n → x with x n ∈ Λ a (g). Then ψ(x n ) → ψ(x) by continuity, and for all ϵ &gt; 0, we have, for large enough n, F(x n ) ≤ aψ(x) + ϵ. This inequality remains true at the limit because F is closed, yielding G(x) ≤ a + ϵ for all ϵ &gt; 0, so that x ∈ Λ a (G).</p><p>We have ridom(F) ∩ ridom(ψ) ∅ so that (by theorem 3.45 and proposition 3.42) ∂G(x) = ∇ψ(x) + ∂F(x). In particular, x * is a minimizer of G if and only if -∇ψ(x * ) ∈ ∂F(x * ).</p><p>It one assumes that ψ is strongly convex, so that there exists m and L such that</p><formula xml:id="formula_219">m 2 |y -x| 2 ≤ ψ(y) -ψ(x) -∇ψ(x) T (y -x) ≤ L 2 |y -x| 2</formula><p>for all x, y ∈ R d , then a minimizer of G exists and is unique. To see this, fix x 0 ∈ ridom(F) and consider the closed convex set</p><formula xml:id="formula_220">Ω 0 = Λ G(x 0 ) (G) = {x : G(x) ≤ G(x 0 )}.</formula><p>Any minimizer of G must clearly belong to Ω 0 . If x ∈ Ω 0 , we have</p><formula xml:id="formula_221">F(x) + ψ(x 0 ) + ∇ψ(x 0 ) T (x -x 0 ) + m 2 |x -x 0 | 2 ≤ G(x) ≤ G(x 0 ) .</formula><p>Moreover, there exists (from proposition 3.44) an element g ∈ ∂F(x 0 ) so that F(x) ≥ F(x 0 ) + g T (x -x 0 ) for all x ∈ R d . We therefore get</p><formula xml:id="formula_222">F(x 0 ) + ψ(x 0 ) + (g + ∇ψ(x 0 )) T (x -x 0 ) + m 2 |x -x 0 | 2 ≤ G(x 0 ) .</formula><p>for all x ∈ Ω 0 , which shows that Ω 0 must be bounded and therefore compact. There exists a minimizer x * of G on Ω 0 , and therefore on all R d . This minimizer is unique, since the sum of a convex function and a strictly convex function is strictly convex.</p><p>In particular, for any closed convex F, we can apply the previous remarks to</p><formula xml:id="formula_223">G : v → F(v) + 1 2 |x -v| 2 where x ∈ R d is fixed. The function ψ : v → |v -x| 2 /2 is strongly convex (with L = m = 1)</formula><p>and G therefore has a unique minimizer v * . This is summarized in the following definition.</p><p>Definition 3.53 Let F be a closed convex function. The proximal operator associated to F is the mapping prox F : R d → dom(F) defined by</p><formula xml:id="formula_224">prox F (x) = argmin R d (v → F(v) + 1 2 |x -v| 2 ). (3.45)</formula><p>From the previous discussion, we also deduce Proposition 3.54 Let F be a closed convex function and α &gt; 0. We have</p><formula xml:id="formula_225">x ′ = prox αF (x) if and only if x ∈ x ′ + α∂F(x ′ ). In particular, x * is a minimizer of F if and only if x * = prox αF (x * )</formula><p>Let us take a few examples.</p><p>• Let F(x) = λ|x|, x ∈ R d , for some λ &gt; 0. Then F is differentiable everywhere except at x = 0 and dom(F) = R d . We have ∂F(x) = λx/|x| for x 0. A vector g belongs to ∂F(0) if and only if g T x ≤ λ|x| for all x ∈ R d , which is equivalent to |g| ≤ λ so that ∂F(0) = B(0, λ).</p><p>We have</p><formula xml:id="formula_226">x ′ = prox F (x) if and only if x ′ 0 and x = x ′ + λx ′ /|x ′ | or x ′ = 0 and |x| ≤ λ. For |x| &gt; λ, the equation x = x ′ + λx ′ /|x ′ | is solved by x ′ = |x| -λ |x| x yielding prox F (x) =          |x| -λ |x| x if |x| ≥ λ 0 otherwise (3.46)</formula><p>• Let Ω be a closed convex set. Then prox σ Ω = proj Ω , the projection operator on Ω, as directly deduced from the definition.</p><p>The following proposition can then be compared to proposition 3.38.</p><formula xml:id="formula_227">Proposition 3.55 Let F be a closed convex function. Then prox F is 1-Lipschitz: for all x, y ∈ R d , | prox F (x) -prox F (y)| ≤ |x -y|. (3.47)</formula><p>Proof Let x ′ = prox F (x) and y ′ = prox F (y). Then, there exists g ∈ ∂F(x ′ ) and h ∈ ∂ F (y ′ ) such that x = x ′ + g and y = y ′ + h. Moreover, we have</p><formula xml:id="formula_228">F(y ′ ) -F(x ′ ) ≥ g T (y ′ -x ′ ) F(x ′ ) -F(y ′ ) ≥ h T (x ′ -y ′ ) from which we deduce g T (y ′ -x ′ ) ≤ h T (y ′ -x ′ ) or (h -g) T (x ′ -y ′ ) ≥ 0. Expressing g, h in terms or x, x ′ , y, y ′ , we get (y -x -y ′ + x ′ ) T (y ′ -x ′ ) ≥ 0 or |y ′ -x ′ | 2 ≤ (y -x) T (y ′ -x ′ ) ≤ |y -x| |y ′ -x ′ | which is only possible if |y ′ -x ′ | ≤ [y -x|. ■ If F is differentiable, then x ′ = prox αF (x) satisfies x ′ = x -α∇F(x ′ )</formula><p>so that x → prox αF (x) can be interpreted as an implicit version of the standard gradient step x → x-α∇F(x). The iterations x(t+1) = prox α t F (x(t)) provide an algorithm that converges to a minimizer of F (this will be justified below). This algorithm is rarely practical, however, since the minimization required at each step is not necessarily much easier to perform than minimizing F itself. The proximal operator, however, is especially useful when combined with splitting methods.</p><p>Proximal gradient descent. Assume that the objective function F takes the form</p><formula xml:id="formula_229">F(x) = G(x) + H(x) (3.48)</formula><p>where G is C 1 on R d and H is a closed convex function. We first note that</p><formula xml:id="formula_230">dF(x, h) = lim t↓0 F(x + th) -F(x) t is well defined (even if G is not convex, because it is smooth), with dF(x, h) = ∇G(x) T h + dH(x, h)</formula><p>In particular, if x * be a minimizer of F, then dF(x, h) ≥ 0 for all h so that dH(x, h) ≥ -∇G(x) T h for all h. Using proposition 3.52, this shows that -∇G(x) ∈ ∂H(x), which is a necessary condition for optimality for F (which is sufficient if G is convex).</p><p>Proximal gradient descent implements the algorithm</p><formula xml:id="formula_231">x t+1 = prox α t H (x t -α t ∇G(x t )). (3.49)</formula><p>We note that a stationary point of this algorithm, i.e. a point x such that x = prox α t H (xα t ∇G(x)) must be such that xα t ∇G(x) ∈ x + α t ∂H(x), so that -∇G(x) ∈ ∂H(x). This shows that the property of being stationary does not depend on α t &gt; 0, and is equivalent to the necessary optimality condition that was just discussed.</p><p>We first study this algorithm under the assumption that G is L-C 1 , which implies that, for all x, y ∈ R d .</p><formula xml:id="formula_232">G(y) ≤ G(x) + ∇G(x) T (y -x) + L 2 |x -y| 2 .</formula><p>At iteration t, we have</p><formula xml:id="formula_233">x t -α t ∇G(x t ) ∈ x t+1 + α t ∂H(x t+1 )</formula><p>which implies, in particular</p><formula xml:id="formula_234">α t H(x t )-α t H(x t+1 ) ≥ (x t -x t+1 ) T (x t -x t+1 -α t ∇G(x t )) = |x t -x t+1 | 2 +α t ∇G(x t ) T (x t+1 -x t )</formula><p>Dividing by α t and adding G(x t ) -G(x t+1 ), we get</p><formula xml:id="formula_235">F(x t ) -F(x t+1 ) ≥ 1 α t |x t -x t+1 | 2 + G(x t ) + ∇G(x t ) T (x t+1 -x t ) -G(x t+1 ) ≥ 1 α t - L 2 |x t -x t+1 | 2 (3.50)</formula><p>so that proximal gradient descent iterations reduce the objective function as soon as</p><formula xml:id="formula_236">α t ≤ 2/L.</formula><p>Assuming that α t &lt; 2/L, (3.50) can be rewritten as</p><formula xml:id="formula_237">x t+1 -x t α t 2 ≤ 2 α t (2 -α t L) (F(x t ) -F(x t+1 ).</formula><p>This inequality should be compared to <ref type="bibr">(3.11)</ref> in the unconstrained case. It yields, in particular, the inequality</p><formula xml:id="formula_238">min x t+1 -x t α t : t ≤ T ≤ F(x 0 ) -min F 2T min{α t (2 -α t L), t ≤ T } . (3.51)</formula><p>As a consequence, if one runs proximal gradient descent until |x t+1x t |/α t is small enough, the algorithm will terminate in finite time as soon as α t is bounded from below (and, in particular, if α t is constant).</p><p>If we assume that G is convex, in addition to being L-C 1 , then we have a stronger result. Let x * be a minimizer of F. Then, using again x t -α t ∇G(x t ) ∈ x t+1 +α t ∂H(x t+1 ), we have</p><formula xml:id="formula_239">α t H(x * ) -α t H(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 -α t ∇G(x t ))</formula><p>and</p><formula xml:id="formula_240">α t F(x * ) -α t F(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) -α t (x * -x t+1 ) T ∇G(x t )) + α t G(x * ) -α t G(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) -α t (x * -x t ) T ∇G(x t )) + α t G(x * ) + α t (x t+1 -x t ) T ∇G(x t ) -α t G(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) - α t L 2 |x t -x t+1 | 2</formula><p>Assuming that α t L ≤ 1, then</p><formula xml:id="formula_241">α t F(x * ) -α t F(x t+1 ) ≥ (x * -x t+1 ) T (x t -x t+1 ) - 1 2 |x t -x t+1 | 2 = 1 2 (|x t+1 -x * | 2 -|x t -x * | 2 ),</formula><p>which we rewrite as</p><formula xml:id="formula_242">α t (F(x t+1 ) -F(x * )) ≤ 1 2 (|x t -x * | 2 -|x t+1 -x * | 2 )</formula><p>Note that, from (3.50), we also have</p><formula xml:id="formula_243">F(x t+1 ) ≤ F(x t ) - 1 2α t |x t -x t+1 | 2</formula><p>when α t L ≤ 1, which shows, in particular that F(x t ) is decreasing. Fixing a time T , we have, from these two observations</p><formula xml:id="formula_244">α t (F(x T ) -F(x * )) ≤ 1 2 (|x t -x * | 2 -|x t+1 -x * | 2 )</formula><p>for all t ≤ T -1, and summing over T ,</p><formula xml:id="formula_245">(F(x T ) -F(x * )) T -1 t=0 α t ≤ 1 2 (|x 0 -x * | 2 -|x T -x * | 2 ) yielding F(x T ) -F(x * ) ≤ |x 0 -x * | 2 2 T -1 t=0 α t . (3.52)</formula><p>We summarize this in the following theorem, specializing to the case of constant step α t .</p><p>Theorem 3.56 Let G be am L-C 1 function defined on R d and H be closed convex. Assume that F = G+H has a minimizer x * . Then the algorithm (3.49) run with α t = α ≤ 1/L for all t is such that, for all T &gt; 0,</p><formula xml:id="formula_246">F(x T ) -F(x * ) ≤ |x 0 -x * | 2 2αT . (3.53)</formula><p>Also, when G = 0, F = H, we retrieve the proximal iterations algorithm</p><formula xml:id="formula_247">x t+1 = prox αF (x t ),<label>(3.54)</label></formula><p>and we have just proved that it converges for any α &gt; 0 as soon as F is a closed convex function.</p><p>One gets a stronger result under the assumption that G is C 2 , and is such that the eigenvalues of</p><formula xml:id="formula_248">∇ 2 G(x) are included in a fixed interval [m, L] for all x ∈ R d with m &gt; 0.</formula><p>Such a G is strongly convex, which implies that F has a unique minimizer. We have</p><formula xml:id="formula_249">|x t+1 -x * | = prox α t H (x t -α t ∇G(x t )) -prox α t H (x * -α t ∇G(x * ) ≤ |x t -x * -α t (∇G(x t )) -∇G(x * ))| . Write |x t -x * -α t (∇G(x t )) -∇G(x * )| = 1 0 (Id R n -α t ∇ 2 G(x * + t(x t -x * )))(x t -x * )dt ≤ 1 0 (Id R n -α t ∇ 2 G(x * + t(x t -x * )))(x t -x * ) dt ≤ max(|1 -α t m|, |1 -α t L|)|x t -x * |</formula><p>where we have use the fact that the eigenvalues of Id R nα t ∇ 2 G(x) are included in [1-α t L, 1-α t m] for all x ∈ R d . If one assumes that α t ≤ 1/L, so that max(|1-α t m|, |1-</p><formula xml:id="formula_250">α t L|) ≤ 1 -α t m, one gets |x t+1 -x * | ≤ (1 -α t m)|x t -x * | .</formula><p>Iterating this inequality, we get the theorem that we state for constant α t .</p><formula xml:id="formula_251">Theorem 3.57 Let F = G + H where G is a C 2 convex function and H is a closed convex function. Assume that the eigenvalues of ∇ 2 G are uniformly included in [m, L] with m &gt; 0. Let x * argmin F. Let (x t ) satisfy (3.49) with constant α t = α &lt; 1/L. Then |x t -x * | ≤ (1 -αm) t |x 0 -x * |.</formula><p>Note that these results also apply to projected gradient descent (section 3.4.4), which is a special case (taking G = σ Ω ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Duality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Generalized KKT conditions</head><p>A constrained convex minimization problem consists in the minimization of a closed convex function F over a closed convex set Ω ⊂ ridom(F). We have seen in theorem 3.36 that, for smooth F, any solution x * of this problem had to satisfy -∇F(x * ) ∈ N Ω (x) where</p><p>N Ω (x) = {h : h T (yx) ≤ 0 for all y ∈ Ω} .</p><p>The next theorem generalizes this property to the non-smooth convex case, for which the necessary optimality condition is also sufficient.</p><formula xml:id="formula_252">Theorem 3.58 Let F be a closed convex function, Ω ⊂ ridom(F) a nonempty closed con- vex set. Then x * ∈ argmin Ω F if and only if 0 ∈ ∂F(x * ) + N Ω (x * ) Proof Introduce the indicator function σ Ω . Then minimizing F over Ω is the same as minimizing G = F+σ Ω over R d . The assumptions imply that ridom(σ Ω ) = relint(Ω) ⊂ ridom(F) and therefore ∂G(x) = ∂F(x) + ∂σ Ω (x) for all x ∈ Ω. Since ∂σ Ω (x) = N Ω (x)</formula><p>the result follows for the characterization of minimum of convex functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>In the following, we will restrict to the situation in which F is finite (i.e., dom(F) = R d ) and Ω is defined through a finite number of equalities and inequalities, taking the form</p><formula xml:id="formula_253">Ω = x ∈ R d : γ i (x) = 0, i ∈ E and γ i (x) ≤ 0, i ∈ I for functions (γ i , i ∈ C = E ∪I ) such that γ i : x → b T i x+β t is</formula><p>affine for all i ∈ E and γ i is closed convex for all i ∈ I . This is similar to the situation considered in section 3.4.1, with additional convexity assumptions, but without assuming smoothness. We recall the definition of active constraints from section 3.4.1, namely, for x ∈ Ω,</p><formula xml:id="formula_254">A(x) = {i ∈ C : γ i (x) = 0}.</formula><p>Following the discussion in the smooth case, define the set</p><formula xml:id="formula_255">N ′ γ (x) ⊂ R d by N ′ γ (x) =          i∈A(x) λ i s i : s i ∈ ∂γ i (x), i ∈ A(x), λ i ≥ 0, i ∈ A(x) ∩ I          .</formula><p>The property 0 ∈ ∂F(x * ) + N γ (x * ) is the expression of the KKT conditions in the nonsmooth case. It holds for x * ∈ argmin Ω F as soon as N Ω (x * ) = N ′ γ (x * ), which is true under appropriate constraint qualifications. We here replace the MF-CQ in definition 3.30 by the following conditions that do not involve gradients. Definition 3.59 Let (γ i , i ∈ C = E ∪ I ) be a set of equality and inequality constraints, with γ i : x → b T i x + β i , i, ∈ E and γ i closed convex, i ∈ I . One says that these constraints satisfy the Slater constraint qualifications (Sl-CQ) if and only if:</p><formula xml:id="formula_256">(Sl 1) The vectors (b i , i ∈ E) are linearly independent. (Sl 2) There exists x ∈ R d such that γ i (x) = 0 for i ∈ E and γ i (x) &lt; 0 for i ∈ I .</formula><p>The first constraint is a very mild condition. When it is not satisfied, this means that some b i 's are linear combinations of others, and equality constraints for the latter implies equality constraints for the former. These redundancies can therefore be removed without changing the problem.</p><p>Note that (Sl2) can be replaced by the apparently weaker condition that, for all i ∈ I , there exists x i ∈ R d satisfying all the constraints and γ i (x i ) &lt; 0. Indeed, if this is true, then the average, x, of (x i , i ∈ I ) also satisfies the equality constraints by linearity, and if i ∈ I ,</p><formula xml:id="formula_257">γ i ( x) ≤ 1 |I | j∈I γ i (x (j) ) ≤ 1 |I | γ i (x (i) ) &lt; 0.</formula><p>The following proposition makes a connection between the Slater conditions and the MF-CQ in definition 3.30. Proposition 3.60 Assume that γ i , i ∈ I are convex C 1 functions. Then, if there exists a feasible point x * that satisfies the MF-CQ, there exists another point x satisfying the Sl-CQ. Conversely, if there exists x satisfying the Sl-CQ, then every feasible point x * satisfies the MF-CQ.</p><p>Proof The linear independence conditions on equality constraints are the same in MF-CQ and Sl-CQ, so that we only need to consider inequality constraints.</p><p>Let x * satisfy MF-CQ, and take h 0 such that b T i h = 0 for all i ∈ E, and ∇γ i (x * ) T h &lt; 0, i ∈ A(x) ∩ I . Then x * + th satisfies the equality constraints for all t ∈ R. If i ∈ I is not active, then γ i (x * ) &lt; 0 and this will remain true at x * + th for small t by continuity. If i ∈ A(x) ∩ I , then a first order expansion gives γ i (x * + th) = t∇γ i (x * ) T h + o(|h|), which is also negative for small enough t &gt; 0. So, x * + th satisfies the Sl-CQ for small enough t &gt; 0.</p><p>Conversely, let x satisfy the Sl-CQ. Take a feasible point x * . If x * = x, then there is no active inequality constraint and x * satisfies MF-CQ. Assume x * x and let h = xx * . Then b T i h = 0 for all i ∈ E, and if i ∈ I ∩ A(x * ),</p><formula xml:id="formula_258">0 &gt; γ i (x) = γ i (x * + h) ≥ γ i (x * ) + ∇γ i (x * ) T h = ∇γ i (x * ) T h</formula><p>so that x * satisfies MF-CQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The following theorem, that we give without proof, states that the Slater conditions implies that the KKT conditions are satisfied for a minimizer. Theorem 3.61 Assume that all the constraints are affine, or that they satisfy the Sl-CQ in definition 3. <ref type="bibr" target="#b77">59</ref></p><formula xml:id="formula_259">. Let x * ∈ argmin Ω F. Then N Ω (x * ) = N ′ γ (x * ), so that there exist s 0 ∈ ∂F(x * ), s i ∈ ∂γ i (x * ), i ∈ A(x * ), (λ i , i ∈ A(x * )) with λ i ≥ 0 if i ∈ I ∩ A(x * ), such that s 0 + i∈A(x) λ i s i = 0 (3.55)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Dual problem</head><p>Consider the Lagrangian</p><formula xml:id="formula_260">L(x, λ) = F(x) + i∈C λ i γ i (x)</formula><p>defined in <ref type="bibr">(3.29)</ref> and let D = {λ : λ i ≥ 0, i ∈ I }. Because the functions γ i are nonpositive on Ω, we have</p><formula xml:id="formula_261">L(x, λ) ≤ F(x)</formula><p>for all x ∈ Ω and λ ∈ D, which implies that We did not need much of our assumptions (not even F to be convex) to reach this conclusion. When the converse inequality is true (so that the duality gap pd vanishes), the dual problem provides important insights on the primal problem, as well as alternative ways to solve it. This is true under the Slater conditions. Theorem 3.62 The duality gap vanishes when the constraints are all affine, or when they satisfy the Sl-CQ in definition 3.59. In this case, any solution λ * of the dual problem provides Lagrange multipliers in theorem 3.61 and conversely.</p><formula xml:id="formula_262">L * (λ) = inf{L(x, λ) : x ∈ R d } is such that L * (λ) ≤ F(x)</formula><p>We justify this statement, as a consequence of theorem 3.61 and the following analysis. The Lagrangian L(x, λ) is linear in λ, and when λ ∈ D, is a convex function of x. Moreover, one can use subdifferential calculus (theorem 3.45) to conclude that, for any λ ∈ D, <ref type="bibr">(3.55)</ref> expresses the fact that 0 ∈ ∂ x L(x * , λ), i.e., that</p><formula xml:id="formula_263">x * ∈ argmin R d L(•, λ). Fixing x ∈ R d , one can also consider the maximization of L in λ ∈ D. Clearly, if</formula><p>x Ω, so that γ i (x) 0 for some i ∈ E or γ i (x) &gt; 0 for some i ∈ I , then max D L(x, λ) = +∞. If x ∈ Ω, then the slackness conditions, which require λ (i) </p><formula xml:id="formula_264">γ i (x) = 0, i ∈ I , ensure that λ ∈ argmax D L(x, •).</formula><p>As a consequence, any pair We therefore obtain the equivalence of the two properties, for (x * , λ * ) ∈ R d × D:</p><formula xml:id="formula_265">x * ∈ Ω, λ * ∈ D satisfying the KKT conditions is such that L(x * , λ) ≤ L(x * , λ * ) ≤ L(x, λ * ) (3.</formula><p>(i) x * ∈ Ω and (x * , λ * ) satisfies the KKT conditions.</p><p>(ii) Equation (3.56) holds for all (x, λ) ∈ R d × D.</p><p>Consider now the additional condition that</p><formula xml:id="formula_266">(iii) x * ∈ argmin Ω F and λ * ∈ argmax D L * .</formula><p>We already know that, if (x * , λ * ) satisfy the KKT conditions, then</p><formula xml:id="formula_267">x * ∈ argmin Ω F (because N ′ γ (x * ) ⊂ N Ω (x * )). Moreover, if (3.56) holds, then the inequality L(x * , λ) ≤ L(x * , λ * ) implies that L * (λ) ≤ L(x * , λ * ) for all λ ∈ D. The inequality L(x * , λ * ) ≤ L(x, λ * ) for all x implies that L(x * , λ * ) ≤ L * (λ * ).</formula><p>We therefore obtain the fact that λ * ∈ argmax L * (λ).</p><p>To summarize, we have (i) ⇔ (ii) ⇒ (iii).</p><p>To obtain the final equivalence, we need to assume constraints qualifications, such as Slater's conditions, to ensure that N ′ γ (x * ) = N Ω (x * ). If this holds, then (iii) implies (via theorem 3.61) that there exists λ such that (i) and (ii) are satisfied for (x * , λ), with L(x * , λ) = L * ( λ) and λ ∈ argmin D L * . This shows that L * ( λ) = L * (λ * ). Moreover, from <ref type="bibr">(3.56)</ref>, we have</p><formula xml:id="formula_268">L(x * , λ * ) ≤ L(x * , λ) = L * ( λ),</formula><p>and, by definition of L * , L(x * , λ * ) ≥ L * (λ * ). This shows that L(x * , λ * ) = L(x * , λ). As a consequence, for all (x, λ) ∈ R d × D:</p><formula xml:id="formula_269">L(x * , λ) ≤ L(x * , λ) = L(x * , λ * ) = L * (λ * ) = inf R d L(•, λ * ) ≤ L(x, λ * )</formula><p>so that (x * , λ * ) satisfies (ii).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Example: Quadratic programming</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quadratic programming problems minimize</head><formula xml:id="formula_270">F(x) = 1 2 x T Ax -b T x, where A is a pos- itive semidefinite matrix and b ∈ R d , subject to affine constraints c T i x -d i = 0, i ∈ E and c T i x -d i ≤ 0, i ∈ I .</formula><p>We here consider the following objective function. Introduce variables x ∈ R d , x 0 ∈ R and ξ ∈ R N and minimize, for a fixed parameter γ,</p><formula xml:id="formula_271">F(x, x 0 , ξ) = 1 2 |x| 2 + γ N k=1</formula><p>ξ (k)   subject to constraints, for k = 1, . . . , N ξ (k) ≥ 0 and</p><formula xml:id="formula_272">b k (x 0 + x T a k ) + ξ (k) ≥ 1</formula><p>where b k ∈ {-1, 1} and a k ∈ R n respectively denote the kth output and input training sample. This algorithm minimizes a quadratic function of the input variables (x, x 0 , ξ) subject to linear constraints, and is an instance of a quadratic programming problem (this is actually the support vector machine problem for classification, which will be described in section 8.4.1).</p><p>Introduce Lagrange multipliers η k for the constraint ξ (k) ≥ 0 and α k for b k (x 0 + x T a k ) + ξ (k) ≥ 1. The Lagrangian then takes the form</p><formula xml:id="formula_273">L(x, x 0 , ξ, α, η) = 1 2 |x| 2 + γ N k=1 ξ (k) - N i=1 η k ξ (k) - N k=1 α k (b k (x 0 + x T a k ) + ξ (k) -1) = 1 2 |x| 2 + N k=1 (γ -η k -α k )ξ (k) -x 0 N k=1 α k b k -x T N k=1 α k b k a k + N k=1 α k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">DUALITY</head><p>We compute the dual Lagrangian L * by minimizing with respect to the primal variables. We note that L * (α, η) = -∞ when N k=1 alpha k b k 0, so that N k=1 α k b k = 0 is a constraint for the dual problem. The minimization in ξ (k) also gives -∞ unless γη kα k = 0, which is therefore another constraint. Finally, the optimal values of x is</p><formula xml:id="formula_274">x = N k=1 α k b k a k</formula><p>and we obtain the expression of the dual problem, which maximizes</p><formula xml:id="formula_275">- 1 2 N k,l=1 α k α l b k b l a T k a l + N k=1 α k subject to η k , α k ≥ 0, γ -η k -α k = 0 and N k=1 α k b k = 0.</formula><p>The conditions on η k and α k can be rewritten as 0 ≤ α k ≤ γ, η k = γα k , and since the rest of the problem does not depends on η, the dual problem can be reduced to maximizing</p><formula xml:id="formula_276">L * (α) = - 1 2 N k,l=1 α k α l a T k a l + N k=1 α k subject to 0 ≤ α k ≤ γ and N k=1 α k b k = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">Proximal iterations and augmented Lagrangian</head><p>The concave function L * can be maximized by minimizing -L * using proximal iterations ((3.54)):</p><formula xml:id="formula_277">λ(t + 1) = prox -α t L * (λ(t)) = argmax D (λ → L * (λ) - 1 2α t |λ -λ(t)| 2 ).</formula><p>Introduce the function</p><formula xml:id="formula_278">ϕ(x, λ) = F(x) + i∈C λ (i) γ i (x) - 1 2α t |λ -λ(t)| 2 so that λ(t + 1) = argmax µ∈D inf x∈R n ϕ(x, µ).</formula><p>The function ϕ is convex in x and strongly concave in µ. Results in "minimax theory" <ref type="bibr" target="#b45">[27]</ref> implies that one has the equality</p><formula xml:id="formula_279">max µ∈D inf x∈R n ϕ(x, µ) = inf x∈R d sup µ∈D ϕ(x, µ) (3.57)</formula><p>(Note that the left-hand side of this equation is never larger than the right-hand side, but their equality requires additional hypotheses-which are satisfied in our context-in order to hold.)</p><p>Importantly, the maximization in µ in the right-hand side has a closed form solution. It requires to maximize</p><formula xml:id="formula_280">i∈C µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2</formula><p>subject to µ i ≥ 0 for i ∈ I , and each µ i can be computed separately. For i ∈ E, there is no constraint on µ i , and one finds</p><formula xml:id="formula_281">µ i = λ i (t) + α t γ i (x),</formula><p>and</p><formula xml:id="formula_282">µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2 = λ i (t)γ i + α t 2 γ i (x) 2 = 1 2α t (λ i (t) + α t γ i (x)) 2 - λ i (t) 2 2α t .</formula><p>For i ∈ I , the solution is</p><formula xml:id="formula_283">µ i = max(0, λ i (t) + α t γ i (x))</formula><p>and one can check that, in this case:</p><formula xml:id="formula_284">µ i γ i (x) - 1 2α t (µ i -λ i (t)) 2 = 1 2α t max(0, λ i (t) + α t γ i (x)) 2 - λ i (t) 2 2α t</formula><p>As a consequence, the right-hand side of (3.57) requires to minimize</p><formula xml:id="formula_285">G(x) = F(x) + 1 2α t i∈E (λ i (t) + α t γ i (x)) 2 + 1 2α t i∈I max(0, λ i (t) + α t γ i (x))) 2 - 1 2α t i∈C λ i (t) 2 .</formula><p>If we assume that the sub-level sets {x ∈ Ω : F(x) ≤ ρ} are bounded (or empty) for any ρ ∈ R, then so are the sets {x ∈ R n : G(x) ≤ ρ}, and this is a sufficient condition for the existence of a saddle point for ϕ, which is a pair (x * , λ * ) such that, for all (x, λ) ∈ R n ×D,</p><formula xml:id="formula_286">ϕ(x * , λ) ≤ ϕ(x * , λ * ) ≤ ϕ(x, λ * ).</formula><p>One can then check that this implies that x * ∈ argmin R n G while λ * = λ(t + 1), so that the latter can be computed as follows:</p><formula xml:id="formula_287">                             x(t) = argmin x∈R n F(x) + 1 2α t i∈E (λ i (t) + α t γ i (x)) 2 + 1 2α t i∈I max(0, λ i (t) + α t γ i (x))) 2 λ i (t + 1) = λ i (t) + α t γ i (x(t)), i ∈ E λ i (t + 1) = max(0, λ i (t) + α t γ i (x(t))), i ∈ I (3.58)</formula><p>These iterations define the augmented Lagrangian algorithm. Starting this algorithm with some λ(0) ∈ R |C| , and constant α, λ(t) will converge to a solution λ of the dual problem. The last two iterations stabilizing imply that γ i (x(t)) converges to 0 for i ∈ E, and also for i ∈ I such that λi &gt; 0, and that lim sup γ i (x(t)) = 0 otherwise. This shows that, if x(t) converges to a limit x, then G( x) = F( x). However, for any x ∈ Ω, we have</p><formula xml:id="formula_288">G(x(t)) ≤ G(x) ≤ F(x)</formula><p>(the proof being left to the reader), showing that x ∈ argmin Ω F.</p><p>Note that the augmented Lagrangian method can also be used in non-convex optimization problems <ref type="bibr" target="#b164">[146]</ref>, requiring in that case that α is small enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.5">Alternative direction method of multipliers</head><p>We return to a situation considered in section 3.5.5 where the function to minimize takes the form F(x) = G(x) + H(x). Here, we do not assume that G or H is smooth, but we will need their respective proximal operators to be easy to compute.</p><p>The problem can be reformulated as a minimization with equality constraints, namely that of minimizing F(x, z) = G(x) + F(z) subject to x = z. We will actually consider a more general situation, namely the problem minimizing a function F(x, z) subject to constraints Ax + Bz = c where A and B are respectively</p><formula xml:id="formula_289">d × n and d × m matrices, x ∈ R n , z ∈ mR m , c ∈ R d .</formula><p>The augmented Lagrangian algorithm applied to this problem leads to iterate (with only equality constraints)</p><formula xml:id="formula_290">         x t , z t = argmin{G(x) + F(z) + 1 2α t |λ t + α t (Ax + Bz -c)| 2 } λ t+1 = λ t + α t (Ax t + Bz t -c) with λ t ∈ R d .</formula><p>One can now consider splitting the first step in two and iterate:</p><formula xml:id="formula_291">                   x t = argmin{G(x) + F(z t-1 ) + 1 2α t |λ t + α t (Ax + Bz t-1 -c)| 2 } z t = argmin{G(x t ) + F(z) + 1 2α t |λ t + α t (Ax t + Bz -c)| 2 } λ t+1 = λ t + α t (Ax t + Bz t -c) (3.59)</formula><p>These iterations constitute the "alternative direction method of multipliers," or ADMM (it is also sometimes called Douglas-Rachford splitting). It is not equivalent to the augmented Lagrangian algorithm (one would need to iterate a large number of times over the first two steps before applying the third one for this), but still satisfies good convergence properties. The reader can refer to Boyd et al. <ref type="bibr" target="#b58">[40]</ref> for a relatively elementary proof that shows that this algorithm converges as soon as, in addition to the hypotheses that were already made, the Lagrangian</p><formula xml:id="formula_292">L(x, z, λ) = G(x) + H(z) + λ T (Ax + Bz -c)</formula><p>has a saddle point: there exists x * , z * , y * such that</p><formula xml:id="formula_293">max y L(x * , z * , λ) = L(x * , z * , λ * ) = min x,z L(x, z, λ * ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Convex separation theorems and additional proofs</head><p>We conclude this chapter by completing some of the proofs left aside when discussion convex functions. These proofs use convex separation theorems, stated below (without proof).</p><p>Theorem 3.63 (c.f., Rockafellar <ref type="bibr" target="#b185">[167]</ref>) Let Ω 1 and Ω 2 be two nonempty convex sets with relint(Ω 1 ) ∩ relint(Ω 2 ) = ∅. Then there exists b ∈ R d and β ∈ R such that b 0, b T x ≤ β for all x ∈ Ω 1 and b T x ≥ β for all x ∈ Ω 2 , with a strict inequality for at least one</p><formula xml:id="formula_294">x ∈ Ω 1 ∪ Ω 2 .</formula><p>Theorem 3.64 Let Ω 1 and Ω 2 be two nonempty convex sets with for all y ∈ Ω with a strict inequality for some y ∈ Ω. One says that b and β provide a supporting hyperplane for Ω at x.</p><formula xml:id="formula_295">Ω 1 ∩ Ω 2 = ∅ and Ω 1 compact. Then there exists b ∈ R n , β ∈ R and ϵ &lt; 0 such that b T x ≤ β -ϵ for all x ∈ Ω 1 and b T x ≥ β + ϵ for all x ∈ Ω 2 .</formula><formula xml:id="formula_296">Now, if F is a convex function, with epi(F) = {(y, a) ∈ R d × R : F(y) ≤ a} then relint(epi(F)) = {(y, a) ∈ ridom(F) × R : F(y) &lt; a}</formula><p>(this simple fact is proved in lemma 3.65 below). In particular, if x ∈ dom(F), then (x, F(x)) must be in the relative boundary of epi(F). This implies that there exists (b, b 0 ) (0, 0) ∈ R d × R such that, for all (y, a) ∈ epi(F):</p><formula xml:id="formula_297">b T y + b 0 a ≥ b T x + b 0 F(x) .</formula><p>If one assumes that x ∈ ridom(F), then, necessarily, b 0 0. To show this, assume otherwise, so that b T y ≥ b T x for all y ∈ dom(F), with b 0. We get a contradiction using the fact that, for some ϵ &gt; 0, [y, x-ϵ(y -x)] belongs to dom(Ω), because b T (y -x) cannot have a constant sign on this segment. So b 0 0 and necessarily b 0 &gt; 0 to ensure that b T y + b 0 a is bounded from below for all a ≥ F(y). Without loss of generality, we can assume b 0 = 1 and we get, for all y ∈ dom(F)</p><formula xml:id="formula_298">F(y) + b T y ≥ F(x) + b T x</formula><p>which shows that -b ∈ ∂F(x), justifying the fact that ∂F(x) ∅ for x ∈ ridom(F).</p><p>We now state and prove the result announced above on the relative interior of the epigraph of a convex function. Then, (x, F(x)) ∈ epi(dom(F)) and (y, a)ϵ((x, F(x)) -(y, a)) ∈ epi(F) for small enough ϵ, showing that F(yϵ(xy)) ≤ (1 + ϵ)a -ϵF(x) and yϵ(xy) ∈ dom(F). This proves that y ∈ ridom(F) and the fact that relint(epi(F)) ⊂ Γ .</p><p>Take (y, a) ∈ Γ , and (x, b) ∈ epi(F). We need to show that (yϵ(xy), aϵ(ba)) ∈ epi(F) for small enough ϵ, i.e., that</p><formula xml:id="formula_299">F(y -ϵ(x -y)) ≤ a -ϵ(b -a)</formula><p>for small enough ϵ. But this is an immediate consequence of the facts that F is continuous at y ∈ ridom(G) and F(y) &lt; a. Assume that there exists x ∈ ridom(F 1 ) ∩ ridom(F 2 ). Take x ∈ dom(F 1 ) ∩ dom(F 2 ) and g ∈ ∂(F 1 + F 2 )(x). We want to show that g = g 1 + g 2 with g 1 ∈ ∂F 1 (x) and g 2 ∈ ∂F 2 (x).</p><p>By definition, we have</p><formula xml:id="formula_300">F 1 (y) + F 2 (y) ≥ F 1 (x) + F 2 (x) + g T (y -x)</formula><p>for all y. We want to decompose g as g = g 1 + g 2 with g 1 ∈ ∂F 1 (x) and g 2 ∈ ∂F 2 (x). Equivalently, we want to find g 2 ∈ R d such that, for all y ∈ R d ,</p><formula xml:id="formula_301">F 1 (y) ≥ F 1 (x) + (g -g 2 ) T (y -x) F 2 (y) ≥ F 2 (x) + g T 2 (y -x)</formula><p>First note that we can replace F 1 by y → F 1 (y) -F 1 (x)g T (yx) and F 2 by y → F 2 (y) -F 2 (x) and assume with loss of generality that F 1 (x) = F 2 (x) = 0 and g = 0.</p><p>Making this assumption, we need to find g 2 such that</p><formula xml:id="formula_302">F 1 (y) ≥ -g T 2 (y -x) F 2 (y) ≥ g T 2 (y -x)</formula><p>for all y ∈ R d and some g 2 ∈ R d , under the assumption that F 1 (y) + F 2 (y) ≥ 0 for all y. Introduce the two convex sets in R d × R</p><formula xml:id="formula_303">Ω 1 = epi(F 1 ) = {(y, a) ∈ R d × R : F 1 (y) ≤ a} Ω 2 = {(y, a) ∈ R d × R : F 2 (y) ≤ -a} .</formula><p>The set Ω 2 is the image of epi(F 2 ) by the transformation (y, a) → (y, -a). We have</p><formula xml:id="formula_304">relint(Ω 1 ) = epi(F 1 ) = {(y, a) ∈ ridom(F 1 ) × R : F 1 (y) &lt; a} relint(Ω 2 ) = {(y, a) ∈ ridom(F 2 ) × R : F 2 (y) &lt; -a} .</formula><p>Since F 1 + F 2 ≥ 0, Ω 1 and Ω 2 have non-intersecting relative interiors. We can apply the first separation theorem, providing b</p><formula xml:id="formula_305">= (b, b 0 ) ∈ R d × R and β ∈ R such that b (0, 0), b T y + b 0 a -β ≤ 0 for (y, a) ∈ Ω 1 and b T y + b 0 a -β ≥ 0 for (y, b) ∈ Ω 2 ,</formula><p>with a strict inequality for at least one point in Ω 1 ∪ Ω 2 . We therefore obtain the fact that, for all y and a,</p><formula xml:id="formula_306">F 1 (y) ≤ a ⇒ b T y + b 0 a -β ≤ 0 F 2 (y) ≤ -a ⇒ b T y + b 0 a -β ≥ 0.</formula><p>We claim that b 0 0. Indeed, if b 0 = 0, the statement for F 1 would imply that b T yβ ≤ 0 for all y ∈ dom(F 1 ) and the one on F 2 that b T yβ ≥ 0 for y ∈ dom(F 2 ). The point x ∈ relint(Ω 1 ) ∩ relint(Ω 2 ) should then satisfy b T xβ = 0. We know that there exists a point y ∈ Ω 1 ∪ Ω 2 such that b T y β. Assume that y ∈ Ω 1 , so that b T yβ &lt; 0 and take ϵ &gt;</p><formula xml:id="formula_307">0 such that ỹ = x -ϵ(y -x) ∈ Ω 1 . Then b T ỹ -β = -ϵ(b T y -β) &lt; 0,</formula><p>which is a contradiction. A similar contradiction is obtained when y belongs to Ω 2 , yielding the fact that b 0 cannot vanish.</p><p>Moreover, we clearly need b 0 &lt; 0 to ensure that b T y + b 0 aβ ≤ 0 for all large enough a if y ∈ dom(Ω 1 ). There is then no loss of generality in assuming b 0 = -1 and we get</p><formula xml:id="formula_308">F 1 (y) ≤ a ⇒ b T y -β ≤ a F 2 (y) ≤ -a ⇒ b T y -β ≥ a, which is equivalent to -F 2 (y) ≤ b T y -β ≤ F 1 (y)</formula><p>Taking y = x gives β = b T x and we get the desired inequality with g 2 = -b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.3">Proof of theorem 3.46</head><p>Let x ∈ R m such that A x ∈ ridom(F). We need to prove that ∂G(x)</p><formula xml:id="formula_309">⊂ A T ∂F(Ax + b) when G(x) = F(Ax + b).</formula><p>We assume in the following that b = 0, since the theorem with</p><formula xml:id="formula_310">G(x) = F(x + b) is obvious. If g ∈ ∂G(x), we have F(Ay) ≥ F(Ax) + g T (y -x)</formula><p>for all y ∈ R m . We want to show that there exists h ∈ R d such that g = A T h and, for all z ∈ R d ,</p><formula xml:id="formula_311">F(z) ≥ F(Ax) + h T (z -Ax) = F(Ax) + h T z -g T x.</formula><p>Let</p><formula xml:id="formula_312">Ω 1 = epi(F) = {(z, a) :, z ∈ R d , F(z) ≤ a} and Ω 2 = {(Ay, a) : y ∈ R m , a = g T (y -x) + G(x)} ⊂ R d × R.</formula><p>Note that Ω 2 is an affine space with relint(Ω 2 ) = Ω 2 . If (z, a) ∈ relint(Ω 1 ) ∩ Ω 2 , then z = Ay for some y ∈ R m and g T (yx) + G(x) &gt; F(z) = G(y). This contradicts the fact that g ∈ ∂G(x) and shows that relint(Ω 1 ) ∩ Ω 2 = ∅. As a consequence, there exist (b, b 0 ) (0, 0) and β such that</p><formula xml:id="formula_313">F(z) ≤ a ⇒ b T z + b 0 a ≤ β z = Ay, a = g T (y -x) + G(x) ⇒ b T z + b 0 a ≥ β</formula><p>Assume, to get a contradiction, that b 0 = 0 (so that b 0). Then b T Ay ≥ β for all y, which is only possible if b is perpendicular to the range of A and β ≤ 0. On the other hand,</p><formula xml:id="formula_314">F(A x) &lt; ∞ implies that 0 = b T A x + b 0 F(A x) ≤ β, so that β = 0. Furthermore,</formula><p>we know that one of the inequalities above has to be strict for at least one element of Ω 1 ∪ Ω 2 , but this cannot be true on Ω 2 , so there exists</p><formula xml:id="formula_315">z ∈ dom(F) such that b T z &lt; 0. Since b T A x = 0 and A x ∈ ridom(F), we have A x -ϵ(z -A x) ∈ dom(F), so that b T (-ϵz) ≤ 0, yielding a contradiction.</formula><p>So, we need b 0 0, and the first pair of inequalities clearly requires b 0 &lt; 0, so that we can take b 0 = -1. This shows that</p><formula xml:id="formula_316">b T z -β ≤ F(z)</formula><p>for all z and b T Ayβ ≥ g T (yx) + F(Ax) for all y. Taking y = x, z = Ax, we find that β = b T Ax -F(Ax) yielding</p><formula xml:id="formula_317">F(z) -F(Ax) ≥ b T (z -x)</formula><p>for all z and b T A(yx) ≥ g T (yx) for all y. This last inequality implies that g = A T b and the first one that b ∈ ∂F(Ax), therefore concluding the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction: Bias, Variance and Density Estimation</head><p>In this chapter, we illustrate the bias variance dilemma in the context of density estimation, in which problems are similar to those encountered in classical parametric or non-parametric statistics <ref type="bibr" target="#b177">[159,</ref><ref type="bibr" target="#b78">60,</ref><ref type="bibr" target="#b172">154]</ref>.</p><p>For density estimation, one assumes that a random variable X is given with unknown p.d.f. f and we want to build an estimator, i.e., a mapping (x, T ) → f (x; T ) that provides an estimation of f (x) based on a training set T = (x 1 , . . . , x N ) containing N i.i.d. realizations of X (i.e., T is a realization of T = (X 1 , . . . , X N ), N independent copies of X). Alternatively, we will say that the mapping T → f ( • ; T ) is an estimator of the full density f . Note that, to further illustrate our notation, f (x; T ) is a number while f (x; T ) is a random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parameter estimation and sieves</head><p>Parameter estimation is the most common density estimation method, in which one restrict f to belong to a finite-dimensional parametric class, denoted (f θ , θ ∈ Θ), with Θ ⊂ R p . For example, f θ can be a family of Gaussian distributions on R d . With our notation, a parametric model provides estimators taking the form</p><formula xml:id="formula_318">f (x; T ) = f θ(T ) (x)</formula><p>and the problem becomes the estimation of the parameter θ.</p><p>There are several, well-known methods for parameter estimation, and, since this is not the focus of the book, we only consider the most common one, maximum likelihood, which consists in computing θ that maximizes the log-likelihood</p><formula xml:id="formula_319">C(θ) = 1 N N k=1 log f θ (x k ) . (4.1)</formula><p>The resulting θ (when it exists) is called the maximum likelihood estimator of θ, or m.l.e.</p><p>If the true f belongs to the parametric class, so that f = f θ * for some θ * ∈ Θ, standard results in mathematical statistics <ref type="bibr" target="#b47">[29,</ref><ref type="bibr" target="#b136">118]</ref> provide sufficient conditions for θ to converge to θ * when N tends to infinity. However, the fact that the true p.d.f. belongs to the finite dimensional class (f θ ) is an optimistic assumption that is generally false. In this regard, the standard theorems in parametric statistics may be regarded as analyzing a "best case scenario," or as performing a "sanity check," in which one asks whether, in the ideal situation in which f actually belongs to the parametric class, the designed estimator has a proper behavior. In non-parametric statistics, a parametric model can still be a plausible approach in order to approximate the true f , but the relevant question should then be whether f provides (asymptotically), the best approximation to f among all f θ , θ ∈ Θ. The maximum likelihood estimator can be analyzed from this viewpoint, if one measures the difference between two density functions by the Kullback-Liebler divergence (also called differential entropy):</p><formula xml:id="formula_320">KL(f ∥f θ ) = R d log f (x) f θ (x) f (x)dx (4.2)</formula><p>which is positive unless f = f θ (and may be equal to +∞).</p><p>This expression of the divergence is a simplification of its general measure-theoretic definition, that we now provide for completeness-and future use. Let µ and ν be two probability measures on a set Ω. One says that µ is absolutely continuous with respect to ν, with notation µ ≪ ν, if, for every (measurable) subset A ⊂ Ω, ν(A) = 0 implies µ(A) = 0. The Radon-Nikodym theorem <ref type="bibr" target="#b49">[31]</ref> states that µ ≪ ν is and only if there exists a non-negative function g defined on Ω such that</p><formula xml:id="formula_321">µ(A) = A g(x)dν(x).</formula><p>In terms of random variables, this says that, if X : Ω → Ω and Y : Ω → Ω are two random variables with respective distributions µ and ν, and ϕ :</p><formula xml:id="formula_322">Ω → R is measurable, then E(ϕ(X)) = E(g(Y )ϕ(Y ))</formula><p>. The function g is called the Radon-Nikodym derivative of µ with respect to ν and is denoted dµ/dν (it is defined up to a modification on a set of ν-probability zero). The general definition of the Kullback-Liebler divergence between µ and ν is then:</p><formula xml:id="formula_323">KL(µ∥ν) =            Ω log dµ dν dµ dν dν if µ ≪ ν + ∞ otherwise (4.3)</formula><p>In the case when µ = f dx and ν = f dx are both probability measures on R d with respective p.d.f.'s f and f , µ ≪ ν means that f / f is well defined everywhere except on a set of ν-probability zero. It is then equal to dµ/dν. If µ ≪ ν, we can therefore write</p><formula xml:id="formula_324">KL(µ∥ν) = R d f (x) f (x) log f (x) f (x) f (x)dx = R d log f (x) f (x) f (x)dx</formula><p>and we will make the abuse of notation of writing KL(f ∥ f ) for KL(f dx∥ f dx), which gives the expression provided in (4.2).</p><p>The general definition also gives a simple expression when Ω is a finite set, with</p><formula xml:id="formula_325">KL(µ∥ν) = x∈ Ω log µ(x) ν(x) µ(x),</formula><p>that we will use later in these notes (if there exists x such that µ(x) &gt; 0 and ν(x) = 0, then KL(µ∥ν) = ∞). The most important property for us is that the Kullback-Liebler divergence can be used as a measure of discrepancy between two probability distribution, based on the following proposition. Proof Assume that µ ≪ ν since the statement is obvious otherwise and let g = dµ/dν. We have Ω gdν = 1 (since, by definition, it is equal to µ( Ω)) so that</p><formula xml:id="formula_326">KL(µ∥ν) = Ω (g log g + 1 -g)dν.</formula><p>We have t log t + 1t ≥ 0 with equality if and only t = 1 (the proof being left to the reader) so that KL(µ∥ν) = 0 if and only if g = 1 with ν-probability one, i.e., if and only if µ = ν.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Minimizing KL(f ∥f θ ) with respect to θ is equivalent to maximizing</p><formula xml:id="formula_327">E f (log f θ ) = R d log f θ (x)f (x)dx ,</formula><p>and an empirical evaluation of this expectation is<ref type="foot" target="#foot_1">foot_1</ref> </p><formula xml:id="formula_328">N N k=1 log f θ (x k )</formula><p>, which provides the maximum likelihood method. Seen in this context, consistency of the maximum likelihood estimator states that this estimator almost surely converges to a best approximator of the true f in the class (f θ , θ ∈ Θ). More precisely, if one assumes that the function θ → log f θ (x) is continuous 1 in θ for almost all x and that, for all θ ∈ Θ, there exists a small enough δ &gt; 0 such that</p><formula xml:id="formula_329">R d sup |θ ′ -θ|&lt;δ log f θ ′ (x) f (x) dx &lt; ∞</formula><p>then, letting Θ * denote the set of maximizers of E f (log f θ ), and assuming that it is not empty, the maximum likelihood estimator θN is such that, for all ϵ &gt; 0 and all compact subsets K ⊂ Θ, lim</p><formula xml:id="formula_330">N →∞ P d( θN , Θ * ) &gt; ϵ and θN ∈ K → 0</formula><p>where d( θN , Θ * ) is the Euclidean distance between θN and the set Θ * . The interested reader can refer to Van der Vaart <ref type="bibr" target="#b212">[194]</ref>, Theorem 5.14, for a proof of this statement. Note that this assertion does not exclude the situation in which θN goes to infinity (i.e., steps out of ever compact subset K in Θ), and the boundedness of the m.l.e. is either asserted from additional properties of the likelihood, or by simply restricting Θ to be a compact set.</p><p>If Θ * = {θ * } and the m.l.e. almost surely converges to θ * , the speed of convergence can also be quantified by a central limit theorem (see Van der Vaart <ref type="bibr" target="#b212">[194]</ref>, Theorem 5.23) ensuring that, in standard cases √ N ( θNθ * ) converges to a normal distribution.</p><p>Even though these results relate our present subject to classical parametric statistics, they are not sufficient for our purpose, because, when f f θ * , the convergence of the m.l.e. to the best approximator in Θ still leaves a gap in the estimation of f . This gap is often called the bias of the class (f θ , θ ∈ Θ). One can reduce it by considering larger classes (e.g., with more dimensions), but the larger the class, the less accurate the estimation of the best approximator becomes for a fixed sample size (the estimator has a larger variance). This issue is known as the "bias vs. variance dilemma," and to address it, it is necessary to adjust the class Θ to the sample size in order to optimally balance the two types of error (and all non-parametric estimation methods have at least one mechanism that allows for this). When the "tuning parameter" is the dimension of Θ, the overall approach is often referred to as the method of sieves <ref type="bibr" target="#b101">[83,</ref><ref type="bibr" target="#b98">80]</ref>, in which the dimension of Θ is increased as a function of N in a suitable way.</p><p>Gaussian mixture models provide one of the most popular choices with the method of sieves. Modeling in this setting typically follows some variation of the following construction. Fix a sequence (m N , N ≥ 1) and let</p><formula xml:id="formula_331">Θ N = f : f (x) = m N j=1 α j e -|x-µ j | 2 /2σ 2 (2πσ 2 ) d/2 , µ 1 , . . . , µ m N ∈ R d , α 1 + • • • + α m N = 1, α 1 , . . . , α m N ∈ [0, +∞), σ &gt; 0 . (4.4)</formula><p>There are therefore (d + 1)m N free parameters in Θ N . The integer m N allows one to adjust the dimension of Θ N and therefore controls the bias-variance trade-off. If m N tends to infinity "slowly enough," the m.l.e. will converges (almost surely) to the true p.d.f. f <ref type="bibr" target="#b98">[80]</ref>. However, determining optimal sequences N → m N remains a challenging and largely unsolved problem.</p><p>In practice the computation of the m.l.e. in this context uses an algorithm called EM, for expectation-maximization. This algorithm will be described later in chapter 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Kernel density estimation</head><p>Kernel density estimators <ref type="bibr" target="#b168">[150,</ref><ref type="bibr" target="#b195">177,</ref><ref type="bibr" target="#b196">178]</ref> provide alternatives to the method of sieves. They also lend themselves to some analytical developments that provide elementary illustrations of the bias-variance dilemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define a kernel function as a function</head><formula xml:id="formula_332">K : R d → [0, +∞) such that R d K(x)dx = 1, R d |x|K(x) dx &lt; ∞, R d xK(x) dx = 0. (4.5)</formula><p>Note that the third equation is satisfied, in particular, when K is an even function, i.e., K(-x) = K(x).</p><p>Given K and a scalar σ &gt; 0, the rescaled kernel is defined by</p><formula xml:id="formula_333">K σ (x) = 1 σ d K x σ .</formula><p>Using the change of variable y = x/σ (so that dy = dx/σ d ) one sees that K σ satisfies (4.5) as soon as K does.</p><p>Based on a training set T = (x 1 , . . . , x N ), the kernel density estimator defines the family of densities</p><formula xml:id="formula_334">fσ (x; T ) = 1 N N k=1 K σ (x -x k ) One has R d K σ (x -x k ) dx = 1</formula><p>so that it is clear that fσ is a p.d.f. In addition,</p><formula xml:id="formula_335">R d xK σ (x -x k ) dx = R d (y + x k )K σ (y) dy = x k so that R d x fσ (x; T ) dx = x where x = (x 1 + • • • + x N )/N . A typical choice for K is a Gaussian kernel, K(y) = e -|y| 2 /2 /(2π) d/2</formula><p>. In this case, the estimated density is a sum of bumps centered at the data points x 1 , . . . , x N . The width of the bumps is controlled by the parameter σ . A small σ implies less rigidity in the model, which will therefore be more affected by changes in the data: the estimated density will have a larger variance. The converse is true for large σ , at the cost of being less able to adapt to variations in the true density: the model has a larger bias (see fig. As we now show, in order to get a consistent estimator, one needs to let σ = σ N depend on the size of the training set. We have, taking expectations with respect to training data,</p><formula xml:id="formula_336">E( fσ (x; T )) = 1 N σ d N k=1 E K((x -X k )/σ ) = 1 σ d R d K((x -y)/σ )f (y)dy = R d K(z)f (x -σ z)dz</formula><p>The bias of the estimator, i.e., the average difference between fσ (x; T ) and f (x) is therefore given by</p><formula xml:id="formula_337">E( fσ (x; T )) -f (x) = R d K(z)(f (x -σ z) -f (x))dz.</formula><p>Interestingly, this bias does not depend on N , but only on σ , and it is clear that, under mild continuity assumptions on f , it will go to zero with σ .</p><p>The variance of fσ (x; T ) is given by var( fσ (x; T )) = 1 N σ 2d var(K((x -X)/σ )) with</p><formula xml:id="formula_338">1 N σ 2d var(K((x -X)/σ )) = 1 N σ 2d d R K((x -y)/σ ) 2 f (y)dy - 1 N σ 2d R d K((x -y)/σ )f (y)dy 2 = 1 N σ d R d K(z) 2 f (x -σ z)dz - 1 N R d K(z)f (x -σ z)dz 2</formula><p>The total mean-square error of the estimator is</p><formula xml:id="formula_339">E(( fσ (x) -f (x)) 2 ) = var( fσ (x)) + (E( fσ (x)) -f (x)) 2 .</formula><p>Clearly, this error cannot go to zero unless we allow σ = σ N to depend on N . For the bias term to go to zero, we know that we need σ N → 0, in which case we can expect the second term in the variance to decrease like 1/N , while, for the first term to go to zero, we need N σ d N to go to infinity. This illustrates the bias-variance dilemma: σ N must go to zero in order to cancel the bias, but not too fast in order to also cancel the variance. There is, for each N , an optimal value of σ that minimizes the error, and we now proceed to a more detailed analysis and make this statement a little more precise.</p><p>Let us make a Taylor expansion of both bias and variance, assuming that f has at least three bounded derivatives and that R d |x| 3 K(x) dx &lt; ∞. We can write</p><formula xml:id="formula_340">f (x -σ z) = f (x) -σ z T ∇f (x) + σ 2 2 z T ∇ 2 f (x)z + O(σ 3 |z| 3 ),</formula><p>where ∇ 2 f (x) denotes the matrix of second derivatives of f at x. Since zK(z)dz = 0, this gives</p><formula xml:id="formula_341">E( fσ (x; T )) -f (x) = σ 2 2 M f (x) + o(σ 2 ) with M f = K(z) z T ∇ 2 f (x)z dz. Similarly, letting S = K 2 (z) dz, var( fσ (x)) = 1 N σ d Sf (x) + o(σ d + σ 2 ) .</formula><p>Assuming that f (x) &gt; 0, we can obtain an asymptotically optimal value for σ by minimizing the leading terms of the mean square error, namely</p><formula xml:id="formula_342">σ 4 4 M 2 f + S N σ d f (x) which yields σ N = O(N -1/(d+4)</formula><p>) and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E((</head><formula xml:id="formula_343">fσ N (x; T ) -f (x)) 2 ) = O(N -4/(d+4) ).</formula><p>If f has r + 1 derivatives, and K has r -1 vanishing moments (this excludes the Gaussian kernel) one can reduce this error to N -2r 2r+d . These rates can be shown to be "optimal," in the "min-max" sense, which roughly expresses the fact that, for any other estimator, there exists a function f for which the convergence speed is at least as "bad" as the one obtained for kernel density estimation. This result says that, in order to obtain a given accuracy ϵ in the worst case scenario, N should be chosen of order (1/ϵ) 1+(d/2r) which grows exponentially fast with the dimension. This is the curse of dimensionality which essentially states that the issue of density estimation may be intractable in large dimensions. The same statement is true also for most other types of machine learning problems. Since machine learning essentially deals with high-dimensional data, this issue can be problematic.</p><p>Obviously, because the min-max theory is a worst-case analysis, not all situations will be intractable for a given estimator, and some cases that are challenging for one of them may be quite simple for others: even though all estimators are "cursed," the way each of them is cursed differs. Moreover, while many estimators are optimal in the min-max sense, this theory does not give any information on "how often" an estimator performs better than its worst case, or how it will perform on a given class of problems. (For kernel density estimation, however, what we found was almost universal with respect to the unknown density f , which indicates that this estimator is not a good choice in large dimensions.)</p><p>Another important point with this curse of dimensionality is that data may very often appear to be high dimensional while it has a simple, low-dimensional structure, maybe because many dimensions are irrelevant to the problem (they contain, for example, just random noise), or because the data is supported by a non-linear low-dimensional space, such as a curve or a surface. This information is, of course, not available to the analysis, but can sometimes be inferred using some of the dimension reduction methods that will be discussed later in chapter 20. Sometimes, and this is also important, information on the data structure can be provided by domain knowledge, that is, by elements, provided by experts, that specify how the data has been generated (such as underlying equations) and reasonable hypotheses that are made in the field. This source of information should never be ignored in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 5</head><p>Prediction: Basic Concepts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General Setting</head><p>The goal of prediction is to learn, based on training data, an input-output relationship between two random variables X and Y , in the sense of finding, for a specified criterion, the best function of the input X that predicts the output Y . (In statistics, Y is often called the dependent variable, and X the independent variable.) We will, as always, assume that all the variables mentioned in this chapter are defined on a fixed probability space (Ω, P). We assume that X : Ω → R X , where R X is the input space, and Y : Ω → R Y , where R Y is the output space. The input-output relationship is therefore captured by an unknown function f : R X → R Y , the predictor.</p><p>The following two subclasses of prediction problems are important enough to have learned their own names and specific literature.</p><p>• Quantitative output: R Y = R q (often with q = 1). One then speaks of a regression problem.</p><p>• Categorical output: R Y = {g 1 , . . . , g q } is a finite set. One then speaks of a classification problem.</p><p>In most cases, the input space is Euclidean, i.e., R X = R d . Note also that, in classification, instead of a function f : R → R Y , one sometimes estimates a function f : R X → Π(R Y ), where Π(R Y ) is the space of probability distributions on R Y . We will return to this in remark 5.4.</p><p>The quality of a prediction is assessed through the definition of a risk function. Such a function, denoted r, is defined on R Y ×R Y , takes values in [0, +∞) and should be understood as r(True output, Predicted output), <ref type="bibr">(5.1)</ref> so that r(y, y ′ ) assigns a cost to the situation in which a true y is predicted by y ′ . Note that this definition is asymmetric, and there is no requirement that r(y, y ′ ) = r(y ′ , y). It is important to remember our convention that the first variable is the true observation and the second one is a place-holder for a prediction. Risk functions are also called loss functions, or simply cost functions and we will use these terms as synonyms.</p><p>The goal in prediction is to minimize the expected risk, also called the generalization error:</p><formula xml:id="formula_344">R(f ) = E(r(Y , f (X))).</formula><p>We will prove that an optimal f can be easily described based on the joint distribution of X and Y (which is, unfortunately, never available). We will need for this to use conditional expectations and conditional probabilities and proceed first to a reminder of their definitions and properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Conditional expectation</head><formula xml:id="formula_345">If ξ : Ω → R ξ and η : Ω → R η ⊂ R d are discrete random variables, then P(η = η | ξ = ξ) = P(η = η, ξ = ξ)/P(ξ = ξ)</formula><p>if P(ξ = ξ) &gt; 0 and is undefined otherwise. Then, if η is real-valued and discrete, one defines the conditional expectation of η given ξ, denoted E(η | ξ), by</p><formula xml:id="formula_346">E(η | ξ)(ω) = η∈R η ηP(η = η | ξ = ξ(ω))</formula><p>for all ω such that P(ξ = ξ(ω)) &gt; 0. Note that E(η | ξ) is a random variable, defined over Ω. It however only depends on the values of ξ, in the sense that</p><formula xml:id="formula_347">E(η | ξ)(ω) = E(η | ξ)(ω ′ ) if ξ(ω) = ξ(ω ′ ). We will use the notation E(η | ξ = ξ) = η∈R η ηP(η = η | ξ = ξ), which is now a function defined on R ξ . One has E(η | ξ)(ω) = E(η | ξ = ξ(ω)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One can characterize E(η | ξ) by the properties</head><formula xml:id="formula_348">       E(η | ξ) is a function of ξ ∀f : R η → R, E(E(η | ξ)f (ξ)) = E(ηf (ξ)). (5.2)</formula><p>The proof that our definition of E(η | ξ) for discrete random variables is the only one satisfying these properties is left to the reader. The interest of reformulating the definition of the conditional expectation via <ref type="bibr">(5.2)</ref> is that this provides a definition that works for general random variables (with the additional assumption that f is measurable), not only for discrete ones. We assume below that (R ξ , S ξ ) and (R η , S η ) are measurable spaces.</p><formula xml:id="formula_349">Definition 5.1 Assume that R η = R d . Let ξ : Ω → R ξ and η : Ω → R η be two random variables with E(|η|) &lt; ∞. The conditional expectation of η given ξ is a random variable ζ : Ω → R η such that (i) There exists a function h : R ξ → R such that ζ = h • ξ almost surely.</formula><p>(ii) For any measurable function g : R ξ → [0, +∞), one has</p><formula xml:id="formula_350">E(ηg • ξ) = E(ζg • ξ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The variable ζ is then denoted E(η|ξ) and the function h in</head><formula xml:id="formula_351">(i) is denoted E(η|ξ = •).</formula><p>Importantly, functions ζ satisfying conditions (i) and (ii) always exists and are almost surely unique, in the sense that if another function ζ ′ satisfies these conditions, then ζ = ζ ′ with probability one. One obtains an equivalent definition if one restricts functions g in (ii) to indicators of measurable sets, yielding the condition that, if A ⊂ R ξ is measurable, E(η1 ξ∈B ) = E(ζ1 ξ∈B ).</p><p>Taking g(ξ) = 1 for all ξ ∈ R ξ in condition (ii), one gets the well-known identity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(E(η|ξ)) = E(η).</head><p>Moreover, for any function g defined on R ξ we have E(ηg</p><formula xml:id="formula_352">• ξ|ξ) = (g • ξ)E(η|ξ),</formula><p>which can be checked by proving that the right-hand side satisfies the conditions (i) and (ii).</p><p>Conditional expectations share many of the properties of simple expectations. For example, if η ≤ η ′ , both taking scalar values, then</p><formula xml:id="formula_353">E(η | ξ) ≤ E(η ′ | ξ) almost surely. Jensen's inequality also holds: if γ : R d → R is convex and γ • η is integrable, then γ • E(η | ξ) ≤ E(γ • η | ξ).</formula><p>We will discuss convex functions in chapter</p><p>3, but two important examples for this section are γ(η) = |η| and γ(η) = |η| 2 . The first one implies that |E(η | ξ)| ≤ E(|η| | bf xi) and, taking expectations on both sides: E(|E(η | ξ)|) ≤ E(|η|), the upper bound being finite by assumption. For the square norm, we find that, if η is square integrable, then so is E(η | ξ) and</p><formula xml:id="formula_354">E(|E(η | ξ)| 2 ) ≤ E(|η| 2 ).</formula><p>If η is square integrable, then this inequality shows that E(η | ξ) minimizes E[|η -ζ| 2 ] among all square integrable functions ζ : Ω → R η that satisfy (i). In other terms, the conditional expectation is the optimal least-square approximation of η by a function of ξ. To see this, just write</p><formula xml:id="formula_355">E[|η -ζ| 2 | ξ] = E[|η| 2 | ξ] -2E[η T ζ | ξ] + |ζ| 2 = E[|η| 2 | ξ] -2E(η | ξ) T ζ + |ζ| 2 = E[|η| 2 | ξ] -|E(η | ξ)| 2 + |E(η | ξ) -ζ| 2 = E[|η -E(η | ξ)| 2 | ξ] + |E(η | ξ) -ζ| 2 ≥ E[|η -E(η | ξ)| 2 | ξ]</formula><p>and taking expectations on both sides yields the desired result.</p><formula xml:id="formula_356">If A is a measurable subset of R η , the conditional expectation E(1 A | ξ) (resp. E(1 A | ξ = ξ)) is denoted P(η ∈ A | ξ) (resp. P(η ∈ A | ξ = ξ)), or P η (A | ξ) (resp. P η (A | ξ = ξ))</formula><p>. While these functions are defined separately up to modifications on sets of probability zero. Under general assumptions on the set R η and its σ -algebra (always satisfied in our discussions), these conditional probabilities can be defined together so that, for all ω</p><formula xml:id="formula_357">∈ Ω A → P η (A | ξ)(ω) is a probability distribution on R η such that E(η | ξ) = R η ηP(dη | ξ).</formula><p>Assume that the the sets R ξ and R η are equipped with measures, say µ ξ and µ η such that the joint distribution of (ξ, η) is absolutely continuous with respect to µξ ⊗ µ η , so that there exists a function ϕ : R ξ × R η → R (the p.d.f. of (ξ, η) with respect to µξ ⊗ µ η ) such that</p><formula xml:id="formula_358">P(ξ ∈ A, η ∈ B) = A×B ϕ(ξ, η)µ ξ ⊗ µ η (dx, dη).</formula><p>Then P η (• | ξ) is absolutely continuous with respect to µ η , with density given by the conditional p.d.f. of η given ξ, namely,</p><formula xml:id="formula_359">ϕ(• | ξ) : (η, ω) → ϕ(η, ξ(ω)) R η ϕ(η ′ , ξ(ω)) µ η (dη ′ ) = ϕ(η | ξ = ξ(ω)). (5.3) Note that P        ω : R η ϕ(η ′ , ξ(ω)) µ η (dη ′ ) = 0        = 0</formula><p>so that the conditional density can be defined arbitrarily when the numerator vanishes 1 .</p><p>The most common example is when R ξ and R η are Euclidean spaces and µ ξ , µ η are Lebesgue's measures, in which case <ref type="bibr">(5.3)</ref> is the usual definition of conditional p.d.f.'s. Note also that, for discrete random variables, (5.3) coincides with the definition of conditional probabilities P(η = η | ξ = ξ(ω)) when µ x and µ η are counting measures. As a last example, if</p><formula xml:id="formula_360">R ξ = R d , µ ξ is Lebesgue's measure and η is discrete, then ϕ(η | ξ = ξ(ω)) = ϕ(η, ξ(ω)) η ′ ∈R η ϕ(η ′ , ξ(ω))</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Bayes predictor</head><p>Recall that r : (y, y ′ ) → r(y, y ′ ) denotes the risk function and that we want to minimize</p><formula xml:id="formula_361">R(f ) = E(r(Y , f (X)) over all possible predictors f . Definition 5.2 A Bayes predictor is a measurable function f : R X → R Y such that, for all x ∈ R X , E r(Y , f (x)) | X = x = min E r(Y , y ′ ) | X = x : y ′ ∈ R Y</formula><p>There can be multiple Bayes predictors if the minimum in the proposition is not uniquely attained. Note that, if f * is a Bayes predictor and f any other predictor, we have, by definition</p><formula xml:id="formula_362">E r(Y , f * (X)) | X ≤ E r(Y , f (X)) | X .</formula><p>Passing to expectations, this implies R(f * ) ≤ R( f ). We therefore have the following result:</p><p>Theorem 5.3 Any Bayes predictor f * is optimal, in the sense that it minimizes the generalization error R.</p><p>Example 1. Regression with mean-square error. When R X = R d and R Y = R q , the most common risk function is the squared norm r(y, y ′ ) = |yy ′ | 2 . The resulting generalization error is called the MSE (mean square error) and given by R(f</p><formula xml:id="formula_363">) = E(|Y - f (X)| 2 ). The Bayes predictor is such that f * (x) minimizes t → E(|Y -t| 2 | X = x). 1 Letting ϕ ξ (ξ) = R η ϕ(η ′ , ξ) µ η (dη ′ )</formula><p>, which is the marginal p.d.f. of ξ with respect to µ ξ , we have</p><formula xml:id="formula_364">P(ϕ ξ (ξ) = 0) = R ξ 1 ϕ ξ (ξ)=0 ϕ ξ (ξ)µ ξ (dξ) = 0. Let f * (x) = E(Y | X = x) and write E(|Y -t| 2 | X = x) =E(|Y -f * (x)| 2 | X = x) + 2E((Y -f * (x)) T (f * (x) -t) | X = x) + |f * (x) -t| 2 =E(|Y -f * (x)| 2 | X = x) + 2E((Y -f * (x)) T | X = x)(f * (x) -t) + |f * (x) -t| 2 =E(|Y -f * (x)| 2 | X = x) + |f * (x) -t| 2 .</formula><p>This proves that E(Y | X = x) is the unique Bayes classifier (up to a modification on a set of probability 0).</p><p>Example 2. Classification with zero-one loss. Let R X = R d and R Y be a finite set. The zero-one loss function is defined by r(y, y ′ ) = 1 if y y ′ and 0 otherwise. From this, it results that the generalization error is the probability of misclassification R(f ) = P (Y f (X)) (also called the misclassification error).</p><p>The Bayes predictor is such that f * (x) minimizes</p><formula xml:id="formula_365">g → P(Y g | X = x) = 1 -P(Y = g | X = x).</formula><p>It is therefore given by the so-called posterior mode:</p><formula xml:id="formula_366">f * (x) = argmax g P(Y = g | X = x).</formula><p>Remark 5.4 As mentioned at the beginning of the chapter, one sometimes replaces a pointwise prediction of the output by a probabilistic one, so that f (x) is a probability distribution on R Y . If A is a (measurable) subset of R Y , we will write f (x, A) rather than f (x)(A).</p><p>In such a case, the loss function, r, is defined on R Y × Π(R Y ), and the expected risk is still defined by E(r(Y , f (X))).</p><p>It is quite natural to require that π → r(y, π) is minimized. For classification problems, where R Y is finite, one can choose</p><formula xml:id="formula_367">r(y, π) = -log π(y) (5.4)</formula><p>The Bayes estimator is then a minimizer of π</p><formula xml:id="formula_368">→ -E(log π(Y ) | X = x). The solution is (unsurprisingly) f (x, y) = P(Y = y | X = x) since we always have -E(log π(Y ) | X = x) = - y∈R Y log π(y)f (x, y) ≥ - y∈R Y log f (x, y)f (x, y).</formula><p>The difference between these terms is indeed</p><formula xml:id="formula_369">y∈R Y log f (x, y) π(y) f (x, y) = KL(f (x, •), π) ≥ 0.</formula><p>For regression problems, with R Y = R q , one can choose</p><formula xml:id="formula_370">r(y, π) = R q |z -y| 2 π(dz)</formula><p>which is indeed minimum when π is concentrated on y. Here, the Bayes estimator minimizes (with respect to π)</p><formula xml:id="formula_371">R q R q |z -y| 2 π(dz)P Y (x, dy) = R q R q |z -y| 2 P Y (x, dy) π(dz)</formula><p>where</p><formula xml:id="formula_372">P Y (x, •) is the conditional distribution of Y given X = x.</formula><p>For any z, one has</p><formula xml:id="formula_373">R q |y -z| 2 f (x, dy) ≥ R q |y -E(Y | X = x)| 2 f (x, dy)</formula><p>♦ which shows that the Bayes estimator is, in this case, the Dirac measure concentrated at E(Y | X = x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Examples: model-based approach</head><p>Bayes predictors are never available in practice, because the true distribution of (X, Y ), or that of Y given X, are unknown. These distributions can only be inferred from observations, i.e., from a training set: T = (x 1 , y 1 , . . . , x N , y N ). This is the approach followed by model-based, or generative methods, namely using training data to approximate the joint distribution of X and Y with a statistical model estimated from data before using the Bayes estimator derived from this model for prediction. We now illustrate this approach with a few examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Gaussian models and naive Bayes</head><p>Consider a regression problem with R Y = R, and model the joint distribution of (X, Y ) as a (d + 1)-dimensional Gaussian distribution with mean µ and covariance matrix Σ, which must be estimated from data. Write µ = m µ 0 , with</p><formula xml:id="formula_374">µ 0 ∈ R, m ∈ R d</formula><p>and Σ in the form, for some symmetric matrix S and d-dimensional vector u</p><formula xml:id="formula_375">Σ = S u u T σ 2 00 .</formula><p>Then, letting ∆ = σ 2 00u T S -1 u,</p><formula xml:id="formula_376">Σ -1 = 1 ∆ ∆S -1 + S -1 uu T S -1 -S -1 u -u T S -1 1 .</formula><p>This shows that the joint p.d.f. of (X, Y ) is proportional to</p><formula xml:id="formula_377">exp - 1 2∆ (y -µ 0 ) 2 -2u T S -1 (x -m)(y -µ 0 ) + (terms not depending on y) .</formula><p>In particular</p><formula xml:id="formula_378">E(Y |X) = µ 0 + u T S -1 (x -m),</formula><p>which provides the least-square linear regression predictor. (In this expression, u is the covariance between X and Y and S is the covariance matrix of X.)</p><p>If one restricts the model to having a diagonal covariance matrix S, then</p><formula xml:id="formula_379">E(Y |X) = µ 0 + d j=1 u (j) s jj (x (j) -m (j) ).</formula><p>This predictor is often called the naive Bayes predictor for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Kernel regression</head><p>Let R X = R d and R Y = R. Let K 1 : R d → R and K 2 : R → R be two kernels, therefore satisfying</p><formula xml:id="formula_380">R d K 1 (x)dx = R K 2 (x)dx = 1; R d xK 1 (x)dx = R yK 2 (y)dy = 0. Let K(x, y) = K 1 (x)K 2 (y) so that R d+1 K(x, y)dydx = 1 R d+1 yK(y, x)dydx = 0 R d+1 xK(y, x)dydx = 0.</formula><p>The kernel estimator of the joint p.d.f., ϕ, of (X, Y ) at scale σ is, in this case:</p><formula xml:id="formula_381">φ(x, y) = 1 N N k=1 1 σ d+1 K 1 x -x k σ K 2 y -y k σ .</formula><p>Based on φ, the conditional expectation of Y given X is</p><formula xml:id="formula_382">f (x) = 1 N N k=1 1 σ d+1 R yK 1 x-x k σ K 2 y-y k σ dy 1 N N k=1 1 σ d+1 R K 1 x-x k σ K 2 y-y k σ dy .</formula><p>Using the fact that σ -1 R yK 2 y-y k σ dy = y k , we can simplify this expression to obtain</p><formula xml:id="formula_383">f (x) = N k=1 y k K 1 x-x k σ N k=1 K 1 x-x k σ .</formula><p>This the kernel-density regression estimator <ref type="bibr" target="#b157">[139,</ref><ref type="bibr" target="#b220">202]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">A classification example</head><p>Let R Y = {0, 1} and assume R X = N = {0, 1, 2, . . .}. Let p = P(Y = 1) and assume that conditionally to Y = g, X follows a Poisson distribution with mean µ g . Assume that</p><formula xml:id="formula_384">µ 0 &lt; µ 1 .</formula><p>The posterior distribution of Y given X = x is<ref type="foot" target="#foot_2">foot_2</ref> </p><formula xml:id="formula_385">P(Y = g | X = x) ∝ (1 -p)µ x 0 e -µ 0 if g = 0 pµ x 1 e -µ 1 if g = 1 A Bayes classifier is then provided by taking f (x) = 1 if log p + x log µ 1 -µ 1 ≥ log(1 -p) + x log µ 0 -µ 0 that is: x log µ 1 µ 0 ≥ log 1 -p p + µ 1 -µ 0</formula><p>Since we are assuming that µ 1 &gt; µ 0 , we find that f (x) = 1 if<ref type="foot" target="#foot_3">foot_3</ref> </p><p>x ≥ log((1p)/p) + µ 1µ 0 log(µ 1 /µ 0 ) and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Empirical risk minimization 5.5.1 General principles</head><p>Model-based approaches for prediction are based on the estimation of the joint distribution of the input and output variables, which is arguably a harder problem than prediction <ref type="bibr" target="#b214">[196]</ref>. Since the goal is to find f minimizing the expected risk</p><formula xml:id="formula_386">R(f ) = E(r(Y , f (X))</formula><p>, one may prefer a direct approach and consider the minimization of an empirical estimate of this risk, based on training data T = (x 1 , y 1 , . . . , x N , y N ), namely</p><formula xml:id="formula_387">R(f ) = 1 N N k=1 r(y k , f (x k )).</formula><p>This strategy is called empirical risk minimization.</p><p>Importantly, R must be minimized over a restricted class, F , of predictors to avoid overfitting. For example, with R Y = R and R = R d , one can take</p><formula xml:id="formula_388">F =        f : f (x) = β 0 + d i=1 b (i) x (i) : β 0 , b (1) . . . , b (d) ∈ R        . Minimizing the empirical mean-square error R(f ) = 1 N N k=1 (y k -f (x k )) 2</formula><p>over f ∈ F leads to the standard least-square regression estimator.</p><p>As another example, consider</p><formula xml:id="formula_389">F =          f : f (x) = p j=1 w j ψ        β j0 + d i=1 β ji x (i)        , w j , β ji ∈ R          .</formula><p>with a fixed function ψ. This corresponds to a two-layer perceptron model.</p><p>As a last example for now (we will see many others in the rest of this book), taking d = 1, the set</p><formula xml:id="formula_390">F = f : R f ′′ (x) 2 dx &lt; µ</formula><p>(with µ &gt; 0) provides an infinite dimensional space of predictors, which leads to spline regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Bias and variance</head><p>We give a further illustration of the bias-variance dilemma in the regression case, using the mean-square error and taking q = 1 to simplify. Denote the Bayes predictor by</p><formula xml:id="formula_391">f * (x) = E(Y | X = x).</formula><p>Fix a function space F , and let f * be the optimal predictor in F , in the sense that it minimizes E(|Yf (X)| 2 ) over f ∈ F . Then, letting fN ∈ F denote an estimated predictor,</p><formula xml:id="formula_392">R( fN ) = E(|Y -fN (X)| 2 ) = E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ) +2E((Y -f * (X))( f * (X) -fN (X))</formula><p>Let us make the assumption that there exists ϵ &gt; 0 such that</p><formula xml:id="formula_393">f λ = f * + λ( fN -f * ) belongs to F for λ ∈ [-ϵ, ϵ].</formula><p>This happens when F is a linear space, or more generally when F is convex and f * is in its relative interior (see chapter 3). Let ψ : λ → E(|Yf λ (X)| 2 ), which is minimal at λ = 0. We have</p><formula xml:id="formula_394">ψ(λ) =E(|Y -f * (X) -λ( fN (X) -f * (X))| 2 ) =E(|Y -f * (X)| 2 ) -2λE((Y -f * (X))( fN (X) -f * (X))) + λ 2 E(| fN (X) -f * (X))| 2 ) and 0 = ψ ′ (0) = 2E((Y -f * (X))( f * (X) -fN (X)))</formula><p>We therefore get the identity</p><formula xml:id="formula_395">R( fN ) = E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ) = "Bias" + "Variance".</formula><p>The bias can be further decomposed as</p><formula xml:id="formula_396">E(|Y -f * (X)| 2 ) = E(|Y -f * (X)| 2 ) + E(|f * (X) -f * (X)| 2 )</formula><p>because f * is the conditional expectation. As a result, we obtain an expression the generalization error with three contributions, namely,</p><formula xml:id="formula_397">R( fN ) ≤ E(|Y -f * (X)| 2 ) + E(|f * -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 ).</formula><p>The first term is the Bayes error. It is fixed by the joint distribution of X and Y and measures how well Y can be approximated by a function of X. The second term compares f * to its best approximation in F , and is therefore reduced by taking larger model spaces. The last term is the error caused by using the data to estimate f * . It increases with the size of F . This is illustrated in Figure <ref type="figure" target="#fig_1">5</ref>.1.</p><p>Remark 5.5 If the assumption made on f * is not valid, one can write</p><formula xml:id="formula_398">R( fN ) = E(|Y -fN (X)| 2 ) ≤ 2 E(|Y -f * (X)| 2 ) + E(| fN (X) -f * (X)| 2 )</formula><p>and still obtain a control (as an inequality) of the generalization error by a bias-plusvariance sum.</p><formula xml:id="formula_399">♦ P P * F f f * f *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability space</head><p>Predictor space Figure <ref type="figure" target="#fig_1">5</ref>.1: Sources of errors in statistical Learning: When P * is the distribution of the data, the optimal predictor f * minimizes the expected loss function. Based on data Z 1 , . . . , Z N , the sample-based distribution is P = (δ</p><formula xml:id="formula_400">Z 1 + • • • + δ Z N )/N</formula><p>and the empirical loss is minimized over a subset S of the space of all possible estimators. The expected discrepancy between the resulting estimator and the one minimizing the true expected loss on the subspace is the "variance" of the method, and the expected discrepancy between this subspace-constrained estimator and and the optimal one is the "bias."</p><p>5.6 Evaluating the error 5.6.1 Generalization error Given input and output variables X : Ω → R X and Y : Ω → R Y and a risk function r : R Y × R Y → [0. + ∞), we have defined the generalization (or prediction) error as R(f ) = E(r(Y , f (X))) . Recall that a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) is a realization T = T (ω) of the random variable T = ((X 1 , Y 1 ), . . . , (X N , Y N )), an i.i.d. sample of the joint distribution of (X, Y ). A learning algorithm is a function T → fT defined on the set of training sets, namely, ∞ N =1 (R × R Y ) N and taking values in F .</p><p>For a given T and a specific algorithm, one is primarily interested in evaluating R( fT ), the generalization error of the predictor estimated from observed data. To emphasize the fact that the training set is fixed in this expression, one often writes:</p><formula xml:id="formula_401">R( fT ) = E(r(Y , fT (X))|T = T )</formula><p>If we also take the expectation with respect to T (for fixed N ), we obtain the averaged generalization risk as</p><formula xml:id="formula_402">E(R( fT )) = E(r(Y , fT (X))),</formula><p>which provides an evaluation of the average quality of the algorithm when evaluated on random training sets of size N . If A : T → fT denotes the learning algorithm, we will denote R N (A) = E(R( fT )).</p><p>Since their computation requires the knowledge of the joint distribution of X and Y , these errors are not available in practice. Given a training set T and a predictor f , one can compute the empirical error</p><formula xml:id="formula_403">RT (f ) = 1 N N k=1 r(y k , f (x k )) .</formula><p>Under the usual moment conditions, the law of large numbers implies that RT (f ) → R(f ) with probability one for any given predictor f . However, the law of large numbers cannot be applied to assess whether the in-sample error,</p><formula xml:id="formula_404">E T ∆ = RT ( fT ) = 1 N N k=1 r(y k , fT (x k )),</formula><p>is a good approximation of the generalization error R( fT ). This is because each term in the sum depends on the full data set, so that E T is not a sum of independent terms. The in-sample error typically under-estimates the generalization error, sometimes with a large discrepancy.</p><p>When one has enough data, however, it is possible to set some of it aside to form a test set. Formally, a test set is a collection</p><formula xml:id="formula_405">T ′ = (x ′ 1 , y ′ 1 , . . . , x ′ N ′ , y ′ N ′ ) considered as a realization of an i.i.d. sample of (X, Y ), T ′ = (X ′ 1 , Y ′ 1 , . . . , X ′ N ′ , Y ′ N ′ ), independent of T .</formula><p>The test set error is then given by</p><formula xml:id="formula_406">E T ,T ′ = RT ′ ( fT ) = 1 N ′ N ′ k=1 r(y ′ k , fT (x ′ k )).</formula><p>The law of large numbers (applied conditionally to T = T ) implies that E T ,T ′ converges to R( fT ) with probability one when N ′ → ∞.</p><p>However, in many applications, data acquisition is difficult or expensive (e.g., in the medical field) and sparing a part of it in order to form a test set is not a reasonable option. In such situations, cross-validation is generally a preferred alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">Cross validation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-validation error</head><p>The n-fold cross-validation method (see, e.g., Stone <ref type="bibr" target="#b202">[184]</ref>) separates the training set into n non-overlapping sets of equal sizes, and estimates n predictors by leaving out one of these subsets as a temporary test set. A generalization error is estimated from each test set and averaged over the n results.</p><p>Let us formalize this computation after introducing some notation. We represent training data in the form T = (z 1 , . . . , z N ), a sample of a random variable Z. With this notation, we can include supervised problems, such as prediction (taking Z = (X, Y )) and unsupervised ones such as density estimation (taking Z = X). One tries to estimate a function f within a given class (e.g., a predictor, or a density) and one has a measure of "loss", denoted ℓ(f , z) ≥ 0 measuring how badly f performs on the data z. For prediction, one takes ℓ(f , z) = r(y, f (x)) with z = (x, y) and for density estimation, e.g., ℓ(f , z) =log f (z), the negative log-likelihood. One then lets R(f ) = E(ℓ(f , Z)). For an algorithm A : T → fT , the loss R(A) is the quantity of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given another set T</head><formula xml:id="formula_407">′ = (z ′ 1 , . . . , z ′ N ′ ), the empirical loss is RT ′ (f ) = 1 N ′ N ′ k=1 ℓ(f , z ′ k )</formula><p>and, using T as a training set and T ′ as a test set, we let</p><formula xml:id="formula_408">E T ,T ′ = RT ′ ( fT ).</formula><p>To define an n-fold cross-validation estimator of the error, one assumes that the training set T is partitioned into n subsets of equal sizes (up to one element if N is not a multiple of n), T 1 , . . . , T n , so that T i and T j are non-intersecting if i j, and T = n i=1 T i . For each i, let T (i) = T \ T i , which provides the training data with the elements of T i removed. Then, the n-fold cross-validation error is defined by</p><formula xml:id="formula_409">E CV (T ) = 1 n n i=1 E T (i) ,T i .</formula><p>Assuming, to simplify, that N is a multiple of n, the expectation of the crossvalidation error is E(R( fT N ′ )), where the average is made over training sets T N ′ of size N ′ = N -N /n. Note that the cross-validation error is an estimate of the average error of the algorithm over random training sets, not necessarily that of the current estimator fT . It returns an evaluation of the algorithm A : T → fT . When needed, one can emphasize this and write RCV,T (A).</p><p>The limit case when n = N is called leave-one-out (LOO) cross validation. In this case E CV is an almost unbiased estimator of E(R( fT )), but, because it is an average of functions of the training set that are quite similar (and that will therefore be positively correlated), its variance (as a function of T ) may be quite large. Conversely, smaller values of n will have smaller variances, but larger biases. In practice, it is difficult to assess which choice of n is optimal, although 5-or 10-fold cross-validation is quite popular. LOO cross-validation is also often used, especially when N is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model selection using cross validation</head><p>Because it evaluates the quality of an algorithm, cross-validation is often used to perform model selection. Indeed, many learning algorithms depends on a parameter, that we will denote λ. In kernel density estimation, for example, λ = σ is the kernel width. For mixtures of Gaussian, λ = m is the number of Gaussian terms in the mixtures. Formally, this means that one has, for every λ, an algorithm A λ : T → fT ,λ . Once this λ * is obtained, the final estimator is fT ,λ * (T ) , obtained by rerunning the algorithm one more time on the full training set. This defines a new training algorithm, A * : T → fT ,λ * (T ) . It is a common mistake to consider that the cross-validation error associated to this algorithm is still given by e(λ * (T )). This is false, because the computation of λ * uses the full training set. To compute the cross-validation error of A * , one needs to encapsulate this model selection procedure in an other cross-validation loop. So, one needs to compute, using the previous notation,</p><formula xml:id="formula_410">E * CV (T ) = 1 n n i=1 RT i ( fT (i) ,λ * (T (i) ) )</formula><p>where each fT (i) ,λ * (T (i) ) is computed by running a cross-validated model selection procedure restricted to T (i) . This is often called a double-loop cross-validation procedure (the number of folds in the inner and outer loops do not have to coincide). Note that each λ * (T (i) ) that does not necessarily coincide with the optimal λ * (T ) obtained with the full training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 6</head><p>Inner Products and Reproducing Kernels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Introduction</head><p>We will discuss later in this book various methods that specify the prediction is as a linear function of the input. These methods are often applied after taking transformations of the original variables, in the form x → h(x) (i.e., the prediction algorithm is applied to h(x) instead of x). We will refer to h as a "feature function," which typically maps the initial data x ∈ R to a vector space, sometimes of infinite dimensions, that we will denote H (the "feature space").</p><p>The present chapter provides a formal description of this framework, focusing, in particular, on situations in which H has an inner product, as this inner product is often instrumental in the design of linear methods on H. Many machine learning methods can indeed be expressed either as functions of the coordinates of the input data in some space, or as functions of the inner products between the input samples. Such methods can bypass the difficulty of using high-dimensional features with the help of the theory of "reproducing kernels," <ref type="bibr" target="#b30">[12,</ref><ref type="bibr" target="#b219">201]</ref> which ensures that the inner product between special classes of feature functions h(x) and h(x ′ ) can be explicitly computed as a function of x and x ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Basic Definitions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Inner-product spaces</head><p>We recall that a real vector space<ref type="foot" target="#foot_4">foot_4</ref> is a set, H, on which an addition and a scalar product are defined, namely (h,</p><formula xml:id="formula_411">h ′ ) ∈ H × H → h + h ′ ∈ H and (λ, h) ∈ R × H → λh ∈</formula><p>H, and we assume that the reader is familiar with the theory of finite-dimensional CHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS spaces.</p><p>An inner product on a vector space H is a bilinear function, typically denoted (ξ, η) → ⟨ξ , η⟩ such that ⟨ξ , ξ⟩ ≥ 0 with ⟨ξ , ξ⟩ = 0 if and only if ξ = 0. A vector space equipped with an inner product is called an inner-product space. We will often denote the inner product with a subscript referring to the space (e.g., ⟨• , •⟩ H ). Given such a product, the function ξ → ∥ξ∥ H = ⟨ξ , ξ⟩ H is a norm, so that H is also a normed space (but not all normed spaces are innerproduct spaces) <ref type="foot" target="#foot_5">2</ref> .</p><p>When a normed space is complete with respect to the topology induced by its norm, it is called a Banach space, or a Hilbert space when the norm is associated with an inner product. Completeness means that Cauchy sequences in this space always have a limit, i.e., if the sequence (ξ n ) is such that, for any ϵ &gt; 0, there exists n 0 &gt; 0 such that ∥ξ nξ m ∥ H &lt; ϵ for all n, m ≥ n 0 , then there exists ξ such that ∥ξ n -ξ∥ H → 0. Completeness is a very natural property. It allows, for example, for the definition of integrals such as h(t)dt as limits of Riemann sums for suitable functions h : R → H, leading (with more general notions of integrals) to proper definitions of expectations of H-valued random variables. Using a standard (abstract) construction, one can prove that any normed space (resp. inner-product) can be extended to a Banach (resp. Hilbert) space within which it is dense.</p><p>Note that finite-dimensional normed spaces are always complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Feature spaces and kernels</head><p>Now, consider an input set, say R, and a mapping h from R to H, where H is an inner product space. For us, R is the set over which the original input data is observed, typically R d , and H is the feature space. One can define the function</p><formula xml:id="formula_412">K h : R × R → R by K h (x, y) = ⟨h(x) , h(y)⟩ H .</formula><p>The function K h satisfies the following two properties.</p><p>[K1] K h is symmetric, namely K h (x, y) = K h (y, x) for all x and y in R.</p><p>[K2] For any n &gt; 0, for any choice of scalars λ 1 , . . . , λ n ∈ R and any x 1 , . . . , x n ∈ R, one</p><formula xml:id="formula_413">has n i,j=1 λ i λ j K h (x i , x j ) ≥ 0. (6.1)</formula><p>The first property is obvious, and the second one results from the fact that one can write</p><formula xml:id="formula_414">n i,j=1 λ i λ j K h (x i , x j ) = n i,j=1 λ i λ j ⟨h(x i ) , h(x j )⟩ H = n i=1 λ i h(x i ) 2 H ≥ 0. (6.2)</formula><p>This leads us to the following definition. One says that the kernel is positive definite if the sum in (6.1) cannot vanish unless (i)</p><formula xml:id="formula_415">λ 1 = • • • = λ n = 0 or (ii) x i = x j for some i j.</formula><p>An equivalent definition of positive kernels can be given using kernel matrices, for which we introduce a notation. Definition 6.2 If K : R × R → R is given, we define, for every x 1 , . . . , x n ∈ R, the kernel matrix K K (x 1 , . . . , x n ) with entries K(x i , x j ), for i, j = 1, . . . , n. (If K is understood from the context, we will simply write K(x 1 , . . . , x n ) instead of K K (x 1 , . . . , x n ).)</p><p>Given this notation, it is clear that K is a positive kernel if and only if for all x 1 , . . . , x n ∈ R, the matrix K K (x 1 , . . . , x n ) is symmetric, positive semidefinite. It is a positive definite kernel if K K (x 1 , . . . , x n ) is positive definite as soon as all x j 's are distinct. This latter condition is obviously needed since, if x i = x j , the ith and jth columns of K coincide and this matrix cannot be full-rank. Remark 6.3 It is important to point out that K being a positive kernel does not require that K(x, y) ≥ 0 for all x, y ∈ R (see examples in the next section). However, it does imply that K(x, x) ≥ 0 for all x ∈ R, since diagonal elements of positive semi-definite matrices are non-negative. ♦</p><p>The function K h defined above is therefore always a positive kernel, but not always positive definite, as seen below. We will also see later that the converse statement is true: any positive kernel K : R × R → R can be expressed as K h for some feature function h between R and some feature space H.</p><p>Given a feature function h : R → H, we will denote by V h = span(h(x), x ∈ R) the vector space generated by the features, which, by definition, is the space of all linear combinations</p><formula xml:id="formula_416">ξ = n i=1 λ i h(x i )</formula><p>with λ 1 , . . . , λ m ∈ R, x 1 , . . . , x n ∈ R and n ≥ 0 (by convention, ξ = 0 if n = 0). Then K h is positive definite if and only if any family (h(x 1 ), . . . , h(x n )) with distinct x i 's is linearly independent. This is a direct consequence of (6.2).</p><formula xml:id="formula_417">n i,j=1 λ i λ j K h (x i , x j ) = n i=1 λ i h(x i ) 2 H .</formula><p>This implies in particular that positive-definite kernels over infinite input spaces R can only be associated to infinite-dimensional spaces H, since V h ⊂ H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">First examples 6.3.1 Inner product</head><p>Clearly, if R is an inner product space, it has an associated reproducing kernel, defined by</p><formula xml:id="formula_418">K(x, y) = ⟨x , y⟩ R .</formula><p>This kernel is equal to K h with H = R and h = id (the identity mapping). In particular K(x, y) = x T y is a positive kernel if R = R d . This kernel can obviously take positive and negative values.</p><p>Notice that this kernel is not positive definite, because the rank of K(x 1 , . . . , x n ) is equal to the dimension of span(x 1 , . . . , x n ), which can be less than n even when the x i 's are distinct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Polynomial Kernels</head><p>Consider R = R d and define</p><formula xml:id="formula_419">h(x) = (x (i 1 ) . . . x (i k ) , 1 ≤ i 1 , . . . , i k ≤ d),</formula><p>which contains all products of degree k formed from variables x (1) , . . . , x (d) , i.e., all monomials of degree k in x. This function takes its values in the space H = R N k , where N k = d k . Using, in H, the inner product ⟨ξ , η⟩ H = ξ T η, we have</p><formula xml:id="formula_420">K h (x, y) = 1≤i 1 ,...,i k ≤d (x (i 1 ) y (i 1 ) ) • • • (x (i k ) y (i k ) ) = (x T y) k .</formula><p>This provides the homogeneous polynomial kernel of order k.</p><p>If one now takes all monomials of order less than or equal to k, i.e., h(x) = (x (i 1 ) . . .</p><formula xml:id="formula_421">x (i l ) , 1 ≤ i 1 , . . . , i l ≤ d, 0 ≤ l ≤ k), which now takes values in a space of dimension 1 + d + • • • + d k , the corresponding kernel is K h (x, y) = 1 + (x T y) + • • • + (x T y) k = (x T y) k+1 -1 x T y -1 .</formula><p>This provides a polynomial kernel of order k. It is important to notice here that, even though the dimension of the feature space increases exponentially in k, so that the computation of the feature function rapidly becomes intractable, the computation of the kernel itself remains a relatively mild operation.</p><p>One can make variations on this construction. For example, choosing any family c 0 , c 1 , . . . , c k of positive numbers, one can take</p><formula xml:id="formula_422">h(x) = (c l x (i 1 ) . . . x (i l ) , 1 ≤ i 1 , . . . , i l ≤ d, 0 ≤ l ≤ k) yielding K h (x, y) = c 2 0 + c 2 1 (x T y) + • • • + c 2 k (x T y) k . Taking c l = k l 1/2</formula><p>α l for some α &gt; 0, we get another form of polynomial kernel, namely, K h (x, y) = (1 + α 2 x T y) k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Functional Features</head><p>We now consider an example in which H is infinite dimensional. Let R = R d . We assume that a function s : R d → R is chosen, such that s is both (absolutely) integrable and square integrable. We also fix a scaling parameter ρ &gt; 0. Associate to x ∈ R d the function</p><formula xml:id="formula_423">ξ x : y → s((y -x)/ρ),</formula><p>which is also square integrable (as a function of y). We define the feature function</p><formula xml:id="formula_424">h : x → ξ x from R d to H = L 2 (R d</formula><p>), the space of square integrable functions on R d with inner product</p><formula xml:id="formula_425">⟨ξ , η⟩ H = R d ξ(z)η(z)dz.</formula><p>The resulting kernel is</p><formula xml:id="formula_426">K h (x, y) = R d s(z/ρ -x)s(z/ρ -y) dz = ρ d R d s(z)s(z -(y -x)/ρ) dz.</formula><p>Note that K h (x, y) is "translation-invariant," which means that it only depends on xy. It takes the form K h (x, y) = ρ d Γ ((yx)/ρ) where</p><formula xml:id="formula_427">Γ (u) = R d s(z)s(z -u) dz.</formula><p>is the convolution<ref type="foot" target="#foot_6">foot_6</ref> of s with s : z → s(-z).</p><p>Let σ be the Fourier transform of s, i.e.,</p><formula xml:id="formula_428">σ (ω) = R d e -2iπω T u s(u)du.</formula><p>Because s is real-valued, we have σ (-ω) = σ (ω), the complex conjugate of σ . Moreover, σ is also the Fourier transform of s. Using the fact that the Fourier transform of the convolution of two functions is the product of their Fourier transforms, we see that the Fourier transform of Γ = s * s is equal to |σ | 2 . Applying the inverse transform, we find</p><formula xml:id="formula_429">Γ (u) = R d e 2iπω T u |σ (ω)| 2 dω = R d e -2iπω T u |σ (-ω)| 2 dω .</formula><p>This form is (almost) characteristic of translation-invariant kernels.</p><p>Let us consider a few examples of kernels that can be obtained in this way.</p><p>(1) Take d = 1 and let s be the indicator function of the interval [-1 2 , 1  2 ]. Then, one finds Γ (t) = max(1 -|t|, 0) .</p><p>In this case, the space V h is the space of all functions expressed as finite sums</p><formula xml:id="formula_430">z → n j=1 λ j 1 [x j -ρ/2,x j +ρ/2] (z) ,</formula><p>and therefore is a space of compactly-supported piecewise constant functions. Such a function computed with distinct x j 's cannot vanish everywhere unless all λ j 's vanish, so that K h is positive definite. Indeed, let</p><formula xml:id="formula_431">f (z) = n j=1 λ j 1 [x j -ρ/2,x j +ρ/2] (z)</formula><p>and assume without loss of generality that x 1 &lt; x 2 &lt; • • • &lt; x n and let x n+1 = ∞. Let i 0 be the smallest index j such that λ j 0, assuming that such an index exists. Then</p><formula xml:id="formula_432">f (z) = λ i 0 &gt; 0 for all z ∈ [x i 0 -ρ/2, x i 0 +1 -ρ/2</formula><p>) which is a non-empty interval. So, if f vanishes almost everywhere, we must have λ j = 0 for all j = 1, . . . , n.</p><p>(2) Still with d = 1, let s(z) = e -|z| . Then, for t &gt; 0,</p><formula xml:id="formula_433">Γ (t) = ∞ -∞ e -|z| e -|z-t| dz = 0 -∞</formula><p>e z e z-t dz + t 0 e -z e z-t dz</p><formula xml:id="formula_434">+ ∞ t e -z e -z+t dz = e -t 2 + te -t + e -t 2 = (1 + t)e -t</formula><p>Using the fact that Γ (-t) = Γ (t) (make the change of variable z → -z in the integral), we get</p><formula xml:id="formula_435">Γ (t) = (1 + |t|)e -|t| .</formula><p>for all t. This shows that</p><formula xml:id="formula_436">K(x, y) = (1 + |x -y|)e -|x-y|</formula><p>is a positive kernel on R d .</p><p>(</p><formula xml:id="formula_437">) Take s(z) = e -|z| 2 /2 , z ∈ R d . Then Γ (u) = R d e -|z| 2 +|u-z| 2 2 dz = e -|u| 2 4 R d e -|z-u/2| 2 dz = (4π) d/2 e -|u| 2 4 .<label>3</label></formula><p>This provides a special case of Gaussian kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">General construction theorems Translation invariance</head><p>As introduced above, a kernel K is translation invariant if it takes the form K(x, y) = Γ (x -y) for some continuous function Γ defined on R d . Bochner's theorem <ref type="bibr" target="#b51">[33]</ref> states that such a K is a positive kernel if and only if Γ is the Fourier transform of a positive measure, namely,</p><formula xml:id="formula_438">Γ (x) = R d e -2iπ⟨x,ω⟩ dµ(ω)</formula><p>where µ is a positive and symmetric (invariant by sign change) measure on R d . For example one can take dµ(ω) = ν(ω)dω, where ν is a integrable, positive and even function.</p><p>This theorem provides an at least numerical, and sometimes analytical, method for constructing kernels. The previous section exhibited a special case of translationinvariant kernel for which ν = |σ | 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Radial kernels</head><p>A radial kernel takes the form K(x, y) = γ(|x -y| 2 ), for some continuous function γ defined on [0, +∞). Shoenberg's theorem <ref type="bibr" target="#b191">[173]</ref> states that, if this function γ is universally valid, i.e., K is a kernel for all dimensions d, then, it must take the form</p><formula xml:id="formula_439">γ(t) = ∞ 0 e -λt dµ(λ)</formula><p>for some positive finite measure µ on [0, +∞).</p><p>For example, when µ is a Dirac measure, i.e., µ = δ (2a) -1 for some a &gt; 0, then K(x, y) = exp(-|x -y| 2 /2a), which is the Gaussian kernel. Taking dµ = e -aλ dλ yields γ(t) = 1/(t + a), and dµ = λe -aλ dλ yields γ(t) = 1/(a + t) 2 .</p><p>There is also, in Schoenberg <ref type="bibr" target="#b191">[173]</ref>, a characterization of radial kernels for a fixed dimension d. Such kernels must take the form</p><formula xml:id="formula_440">γ(t) = +∞ 0 Ω d (tλ)dµ(λ) with Ω d (t) = Γ (d/2)(2/t) (d-2)/2 J (d-2)/2 (t)</formula><p>where J (d-2)/2 is Bessel's function of the first kind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Operations on kernels</head><p>Kernels can be combined in several ways as described in the next proposition. Proposition 6.4 Let K 1 : R × R → R and K 2 : R × R → R be positive kernels. Then the following assertions hold.</p><formula xml:id="formula_441">(i) If λ 1 , λ 2 &gt; 0, λ 1 K 1 + λ 2 K 2 is a positive kernel. It is positive definite as soon as either K 1 or K 2 is positive definite. (ii) For any function f : R ′ → R, K ′ 1 (x ′ , y ′ ) ∆ = K 1 (f (x ′ ), f (y ′ )</formula><p>) is a positive kernel. It is positive definite as soon as K 1 is positive definite and f is one-to-one.</p><p>(iii) K(x, y) = K 1 (x, y)K 2 (x, y) is a positive kernel. It is positive definite as soon as K 1 and K 2 are positive definite.</p><p>(iv) Let K 1 and K 2 be translation-invariant with R = R d , taking the form</p><formula xml:id="formula_442">K i (x, y) = Γ i (x -y), where Γ i is continuous ( i = 1, 2). Assume that one of the two functions Γ 1 , Γ 2 is integrable on R d . Then K(x, y) = R d K 1 (x, z)K 2 (z,</formula><p>y)dz is also a positive kernel. Proof Point (i) is obvious. Point (ii) is almost as simple, because, for any λ 1 , . . . , λ n ∈ R and x ′ 1 , . . . , x ′ n ∈ R ′ , n i,j=1</p><formula xml:id="formula_443">λ i λ j K ′ 1 (x ′ i , x ′ j ) = n i,j=1 λ i λ j K 1 (f (x ′ i ), f (x ′ j )) ≥ 0.</formula><p>If K 1 is positive definite, then the latter sum can only vanish if all λ i are zero, or some of the points in (f (x ′ 1 ), . . . , f (x ′ n )) coincide. If, in addition, f is one-to-one, then this is equivalent to all λ i are zero, or some of the points in (x ′ 1 , . . . , x ′ n ) coincide, so that K ′ 1 is positive definite.</p><p>To prove point (iii), take x 1 , . . . , x N ∈ R d and form the matrices</p><formula xml:id="formula_444">K i = K i (x 1 , . . . , x N ), i = 1, 2</formula><p>, which are, by assumption positive semi-definite. The matrix K = K(x 1 , . . . , x N ) is the element-wise (or Hadamard) product of K 1 and K 2 , and the conclusion follows from the linear algebra result stating that the Hadamard product of two positive semi-definite (resp. positive definite) matrices A = (a(i, j), 1 ≤ i, j ≤ N ) and B = (b(i, j), 1 ≤ i, j ≤ N ) is positive semi-definite (resp. positive definite). This is proved by diagonalizing, say, A in an orthonormal basis u 1 , . . . , u N , with eigenvalues λ 1 , . . . , λ N and writing</p><formula xml:id="formula_445">N i,j=1 α (i) a(i, j)b(i, j)α (j) = N i,j,k=1 α (i) u (k) i u (k) j λ k b(i, j)α (j) = N k=1 λ k N i,j=1 (α (i) u (k) i )(α (j) u (k) j ))b(i, j) ≥ 0</formula><p>If B is positive definite, then the sum above can be zero only if, for each k, either λ k = 0 or α (i) u (k) i = 0 for all i. If A is also positive definite, then the only possibility is α (i) u (k) i = 0 for all i and k, which implies α (i) = 0 for all i since u i 0.</p><p>To prove point (iv) <ref type="foot" target="#foot_7">4</ref> , we first note that a translation invariant kernel K ′ (x, y) = Γ ′ (xy) is always bounded. Indeed, the matrix</p><formula xml:id="formula_446">K ′ (x, 0) is positive semi-definite, with determinant Γ ′ (0) 2 -Γ ′ (x) 2 &gt; 0, showing that |Γ ′ (x)| &lt; Γ ′ (0)</formula><p>. This shows that the integral defining K(x, y) converges as soon as one of the two functions Γ 1 or Γ 2 is integrable. Moreover, we have K(x, y) = Γ (xy) with</p><formula xml:id="formula_447">Γ (x) = R d Γ 1 (x -z)Γ 2 (z) dz = R d Γ 1 (x -u)Γ 2 (u -y) du</formula><p>Using the fact that both Γ 1 and Γ 2 are even, and making the change of variable z → -z, one easily shows that Γ (x) = Γ (-x), which implies that K is symmetric.</p><p>We proceed with the assumption that Γ 2 is integrable and use Bochner's theorem to write</p><formula xml:id="formula_448">Γ 1 (y) = R d e -iξ T y dµ 1 (ξ)</formula><p>for some positive finite measure µ 1 . Then</p><formula xml:id="formula_449">Γ (x) = R d R d e -2iπξ T (x-z) dµ 1 (ξ) Γ 2 (z) dz = R d e -2iπξ T x R d e 2iπξ T z Γ 2 (z) dz dµ 1 (ξ)</formula><p>The shift in the order of the variables ξ and z uses Fubini's theorem. The function</p><formula xml:id="formula_450">ψ(ξ) = R d e 2iπξ T z Γ 2 (z) dz</formula><p>is the inverse Fourier transform of Γ 2 . Because Γ 2 is bounded and integrable, it is also square integrable, which implies that its inverse Fourier transform is also a square integrable function. Since Bochner's theorem implies that Γ 2 is the Fourier transform of a positive measure µ 2 , we find, using the injectivity of the Fourier transform, that ψ is non-negative. So Γ is the Fourier transform of the finite positive measure ψdµ 1 , which implies that K is a positive kernel.</p><p>■ Point (iv) can be related to the following discrete statement on symmetric matrices: assume that A and B are positive semi-definite and that they commute, so that AB = BA: then AB is positive semi-definite (see ??). In the case of kernels, one may consider the symmetric linear operators K i : f → R d K i (•, y)f (y)dy which maps the space of square integrable functions into itself. Then K 1 and K 2 commute and K = K 1 K 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.6">Canonical Feature Spaces</head><p>Let K be a positive kernel on a set R. The following construction, which is fundamental, shows that K can always be associated with a feature function h taking values in a suitably chosen inner-product space H.</p><p>Associate to each x ∈ R the function ξ x : y → K(y, x) (we will also write ξ x = K(•, x)), and let H K = span(ξ x , x ∈ R), a subspace of the vector space of all functions from R to R. Define the feature function h : x → ξ x from R to H K . There is a unique inner product on H K such that K = K h . Indeed, by definition, this requires</p><formula xml:id="formula_451">⟨K(•, x) , K(•, y)⟩ H K = K(x,</formula><p>y) . (6.3) Moreover, by linearity, for any ξ = n i=1 λ i K(•, x i ) and η = m i=1 µ i K(•, y i ), one needs ⟨ξ , η⟩ H K = n i=1 m j=1</p><p>λ i µ j K(x i , y j ) , so that the inner product is uniquely specified on H K . To make sure that this innerproduct is well defined, we must check that there is no ambiguity, in the sense that, if ξ has an alternative decomposition ξ = n ′ i=1 λ ′ i K(•, x ′ i ), then, the value of ⟨ξ , η⟩ H K remains unchanged. But this is clear, because one can also write ⟨ξ , η⟩ H K = m j=1 µ j ξ(y j ) , which only depends on ξ and not on its decomposition. The linearity of the product with respect to ξ is also clear from this expression, and the bilinearity by symmetry.</p><p>The Schwartz inequality implies that</p><formula xml:id="formula_452">|⟨ξ , η⟩ H K | ≤ ∥ξ∥ H K ∥η∥ H K</formula><p>From which we deduce that ∥ξ∥ H K = 0 implies that ⟨ξ , η⟩ H K = 0 for η ∈ H K . Since ⟨ξ , K(•, y)⟩ H K = ξ(y) for all y, this also implies that ξ = 0, completing the proof that H K is an inner-product space. Equation (6.3) is the "reproducing property" of the kernel for the inner-product on H K . In functional analysis, the completion, ĤK , of H K for the topology associated to its norm is then a Hilbert space, and is referred to as a "reproducing kernel Hilbert space," or RKHS.</p><p>More generally, an inner-product space H of functions h : R → R is a reproducing kernel Hilbert space if H is a complete space (which makes it Hilbert) and there exists a positive kernel K such that,</p><formula xml:id="formula_453">[RKHS1] For all x ∈ R, K(•, x) belongs to H, [RKHS2] For all h ∈ H and x ∈ R, ⟨h , K(•, x)⟩ H = h(x) .</formula><p>Returning to the example of functional features in section 6.3.3, we have two different representations of the kernel in feature space, namely in H = L 2 (R d ), or in H K , with a different inner product. There is not a contradiction, and simply shows that the representation of a positive kernel in terms of a feature function is not unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Projection on a finite-dimensional subspace</head><p>If H is an inner-product space and V is a subspace of H, one defines the orthogonal projection of an element ξ ∈ H on V as its closest point in V , that is, the element η * of V minimizing the function F : η → ∥η -ξ∥ 2  H over all η ∈ V . This closest point does not always exist, but it does in the special case in which V is finite dimensional (or, more generally, when V is a closed subspace of H; see Yosida <ref type="bibr" target="#b223">[205]</ref>). We state, without proof, some of the properties of this operation.</p><p>Assuming that V is closed, this minimizer is unique and will be denoted η * = π V (ξ). Moreover, π V is a linear transformation from H to V , and η * is characterized by the properties</p><formula xml:id="formula_454">η * ∈ V ξ -η * ⊥ V ,</formula><p>the last condition meaning that ⟨ξη * , η⟩ H = 0 for all η ∈ V .</p><p>Because</p><formula xml:id="formula_455">∥ξ∥ 2 H = ∥π V (ξ)∥ 2 H + ∥ξ -π V (ξ)∥ 2 H , one always has ∥π V (ξ)∥ H ≤ ∥ξ∥ H , with inequality if and only if π V (ξ) = ξ, i.e., if and only if ξ ∈ V . If V is finite-dimensional and η 1 , . . . , η n is a basis of V , then π V (ξ) is given by π V (ξ) = n i=1 α (i) η i</formula><p>with α (considered as a column vector in R n ) given by α = Gram(η 1 , . . . , η n ) -1 λ , where λ ∈ R n is the vector with coordinates λ (i) = ⟨ξ , η i ⟩ H , i = 1, . . . , n. The Gram matrix of η 1 , . . . , η n , denoted Gram(η 1 , . . . , η n ), is the n by n matrix with entries ⟨η i , η j ⟩ H for i, j = 1, . . . , n.</p><p>If A is a subset of H, the set A ⊥ consists of all vectors perpendicular to A, namely</p><formula xml:id="formula_456">A ⊥ = h ∈ H : ⟨h , h⟩ H = 0 for all h ∈ A .</formula><p>If V is a finite-dimensional (or, more generally, closed) subspace of H, then any point in h is decomposed as</p><formula xml:id="formula_457">h = π V (h) + h -π V (h) with h -π V (h) ∈ V ⊥ . This shows that π V ⊥ is well defined and equal to id H -π V .</formula><p>Orthogonal projections can be applied to function interpolation in an RKHS. Indeed, assuming that H is an RKHS, as described at the end of the previous section, with a positive-definite kernel. Given distinct points x 1 , . . . , x N ∈ R and values α 1 , . . . , α N ∈ R, the interpolation problem consists in finding h ∈ H with minimal norm satisfying h(x k ) = α k , k = 1, . . . , N . Consider the finite dimensional space</p><formula xml:id="formula_458">V = span {K(•, x k ), k = 1, . . . N } .</formula><p>Then there exists an element h 0 ∈ V that satisfies the constraints. Indeed, looking for h 0 in the form</p><formula xml:id="formula_459">h 0 (x) = N l=1 K(x, x l )λ l one has h 0 (x k ) = N l=1 K(x k , x l )λ l so that           λ 1 . . . λ N           = K(x 1 , . . . , x N ) -1           α 1 . . . α N          </formula><p>Any other function h satisfying the constraints satisfies h(x k )h 0 (x k ) = 0, which, using RKHS2, is equivalent to ⟨hh 0 , K(•, x k )⟩ H = 0, i.e., to hh 0 ∈ V ⊥ . This shows that h 0 = π V (h), so that ∥h∥ H ≥ ∥h 0 ∥ H and h 0 provides the optimal interpolation. We summarize this in the proposition: Proposition 6.5 Let H is an RKHS with a positive-definite kernel. Let x 1 , . . . , x N ∈ R be distinct points and α 1 , . . . , α N ∈ R. Then the function h ∈ H with minimal norm satisfying</p><formula xml:id="formula_460">h(x k ) = α k , k = 1, . . . , N takes the form h(x k ) = N l=1 K(x k , x l )λ l (6.4a) with           λ 1 . . . λ N           = K(x 1 , . . . , x N ) -1           α 1 . . . α N           . (<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.4b)</head><p>A variation of this problem replaces the constraint by a penalty that complete the minimization associated with the orthogonal projection, namely, minimizing (in h ∈ H)</p><formula xml:id="formula_461">∥h∥ 2 H + σ 2 N k=1 |h(x k ) -α k | 2 .</formula><p>Letting h 0 = π V (h), so that h 0 (x k ) = h(x k ) for all k, this expression can be rewritten as</p><formula xml:id="formula_462">∥h 0 ∥ 2 H + ∥h -h 0 ∥ 2 H + σ 2 N k=1 |h 0 (x k ) -α k | 2 .</formula><p>This shows that the optimal h must coincide with its projection on V , and therefore belong to that subspace. Looking for h in the form</p><formula xml:id="formula_463">h(•) = N l=1 K(•, x l )λ l ,</formula><p>the objective function is rewritten as</p><formula xml:id="formula_464">N k,l=1 K(x k , x l )λ k λ l + σ 2 N k=1 N l=1 K(x k , x l )λ l -α k 2 ,</formula><p>which, in vector notation gives, writing λ =</p><formula xml:id="formula_465">          λ 1 . . . λ N           and α =           α 1 . . . α N           , λ T K(x 1 , . . . , x N )λ + σ 2 (K(x 1 , . . . , x N )λ -α) T (K(x 1 , . . . , x N )λ -α) .</formula><p>The differential of this expression in λ is K(x 1 , . . . , x N )λ + 2σ 2 K(x 1 , . . . , x N )(K(x 1 , . . . , x N )λα).</p><p>Assuming that x 1 , . . . , x N are distinct, this vanishes if and only if</p><formula xml:id="formula_466">λ = (K(x 1 , . . . , x N ) + (1/σ 2 )Id R N ) -1 α.</formula><p>We have just proved the proposition: Proposition 6.6 Let H is an RKHS with a positive-definite kernel. Let x 1 , . . . , x N ∈ R be distinct points and α 1 , . . . , α N ∈ R. Then the unique minimizer of</p><formula xml:id="formula_467">h → ∥h∥ 2 H + σ 2 N k=1 |h(x k ) -α k | 2</formula><p>on H is given by</p><formula xml:id="formula_468">h(x k ) = N l=1 K(x k , x l )λ l (6.5a) with           λ 1 . . . λ N           = (K(x 1 , . . . , x N ) + (1/σ 2 )Id R N ) -1           α 1 . . . α N           . (6.5b)</formula><p>Chapter 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Models for Regression</head><p>In regression, linear models refer to situations in which one tries to predict the de-</p><formula xml:id="formula_469">pendent variable Y ∈ R Y = R q by a function f (X) of the dependent variable X ∈ R X ,</formula><p>where f is optimized over a linear space F . The most common situation is the "standard linear model," for which R X = R d and</p><formula xml:id="formula_470">F = {f (x) = a 0 + b T x : a 0 ∈ R q , b ∈ M d,q (R)}.<label>(7.1)</label></formula><p>More generally, with q = 1, given a mapping h : R → H, where H is an innerproduct space, one can take:</p><formula xml:id="formula_471">F = {f (x) = a 0 + ⟨b , h(x)⟩ H : a 0 ∈ R, b ∈ H}. (7.2)</formula><p>Note that h can be nonlinear, and F can be infinite dimensional. Such sets corresponds to linear models using feature functions, and will be addressed using kernel methods in this chapter.</p><p>Note also that, even if the model is linear, the associated training algorithms can be nonlinear, and we will review in fact several situations in which solving the estimation problem requires nonlinear optimization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Least-Square Regression</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Notation and Basic Estimator</head><p>We denote by Y and X the dependent and independent variables of the regression problem. We will assume that Y takes values in R q and that X takes values in a set R X , which will, by default, be equal to R d , except when discussing kernel methods, for which this set can be arbitrary (provided that there is a mapping h from R X to an inner product space H with an easily computable kernel).</p><p>Least-square regression uses the risk function r(y, y ′ ) = |yy ′ | 2 . The prediction error is then R(f ) = E(|Yf (X)| 2 ) for any predictor f and the Bayes predictor is the conditional expectation x → E(Y | X = x) (see item Example 1. in section 5.3). We also start with the standard setting where R X = R d and F given by (7.1).</p><p>We will use the following notation, which sometimes simplifies the computation.</p><formula xml:id="formula_472">If x ∈ R d , we let x = 1 x</formula><p>, which belongs to R d+1 . The linear predictor f (x) = a 0 + b T x with a 0 ∈ R q , b ∈ M d,q (R) can then be written as</p><formula xml:id="formula_473">f (x) = β T x with β = a T 0 b ∈ M d+1,q (R).</formula><p>In a model-based approach, the linear model is a Bayes predictor under the generative assumption that Y = a 0 + b T X + ϵ where ϵ is a residual noise satisfying E(ϵ | X) = 0, which is true, for example, when ϵ is centered and independent of X. If one further specifies the model so that ϵ is Gaussian, centered and independent of X, and one assumes that the distribution of X does not depend on a 0 and b, then the maximum likelihood estimator of these parameters based on a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) must minimize the "residual sum of squares:"</p><formula xml:id="formula_474">RSS(β) ∆ = N R(f ) ∆ = N k=1 |y k -f (x k )| 2 = N k=1 |y k -β T xk | 2 .</formula><p>In other terms, the model-based approach is identical, under these (standard) assumptions, to empirical risk minimization (section 5.5), on which we now focus. (Recall that, even when using a model-based approach, one does not make assumptions on the true distribution of X and Y ; one rather treats the model as an approximation of these distributions, estimated by maximum likelihood, and uses the Bayes predictor for the estimated model.)</p><p>The computation of the optimal regression parameters is made easier by the introduction of the following matrices. Introduce the N × (d + 1) matrix X with rows xT 1 , . . . , xT N and the N × q matrix Y with rows y T 1 , . . . , y T N , that is:</p><formula xml:id="formula_475">X =             1 x (1) 1 • • • x (d) 1 . . . . . . . . . 1 x (1) N • • • x (d) N             , Y =             y (1) 1 • • • y (q) 1 . . . . . . y (1) N • • • y (q) N             .</formula><p>With this notation, we have RSS(β) = |Y -X β| 2 2 . with |A| 2 2 = trace(A T A) for a rectangular matrix A. The solution of the problem is then provided by the following theorem. Theorem 7.1 Assume that the matrix X has rank d + 1. Then the RSS is minimized for β = (X T X ) -1 X T Y Proof We provide two possible proofs of this elementary problem. The first one is an optimization argument noting that F(β) ∆ = RSS(β) is a convex function defined on M d+1,q (R) and with values in R. Since F is quadratic, we have, for any matrix</p><formula xml:id="formula_476">h ∈ M d+1,q (R), dF(β)h = ∂ ϵ F(β + ϵh)| ϵ=0 = -2trace(h T X T (Y -X β)) and dF(β) = 0 ⇔ X T (Y -X β) = 0 ⇔ β = β.</formula><p>One can alternatively proceed with a direct computation. We have</p><formula xml:id="formula_477">RSS(β) = |Y | 2 2 -2trace(β T X T Y ) + trace(β T X T X β) = |Y | 2 2 -2trace(β T X T X β) + trace(β T X T X β).</formula><p>Replacing β by β and simplifying yields</p><formula xml:id="formula_478">RSS( β) = |Y | 2 2 -trace( βT X T X β) It follows that RSS(β) =RSS( β) + trace( βT X T X β) -2trace(β T X T X β) + trace(β T X T X β) =RSS( β) + |X ( β -β)| 2 2</formula><p>so that the left-hand side is minimized at β = β. ■ Remark 7.2 If X does not have rank d + 1, then optimal solutions exist, but they are not unique. By convexity, the solutions are exactly the vectors β at which the gradient vanishes, i.e., those that satisfy X T X β = X T Y . The set of solutions can be obtained by introducing the SVD of X in the form X = U DV T and letting γ = V T β and</p><formula xml:id="formula_479">Z = U T Y . Then X T X β = X T Y ⇔ D T Dγ = D T Z.</formula><p>Letting d (1) , . . . , d (m) denote the nonzero diagonal entries of D (so that m ≤ d + 1), we find γ (i) = z (i) /d (i) for i ≤ m (the other equalities being 0 = 0). So, the d + 1m last entries of γ can be chosen arbitrarily (and β = V γ). ♦</p><p>An alternate representation of the solution use a two-step computation that estimates b first, then a 0 . Indeed, for fixed b, the minimum of x k .</p><formula xml:id="formula_480">N k=1 |y k -a 0 -x T k b| 2</formula><p>This shows that b itself must be a minimizer of</p><formula xml:id="formula_481">N k=1 |y k -ȳ -(x k -x) T b| 2 .</formula><p>Denote by Y c and X c the matrices</p><formula xml:id="formula_482">X c =             x (1) 1 -x (1) • • • x (d) 1 -x (d) . . . . . . x (1) N -x (1) • • • x (d) N -x (d)             , Y c =             y (1) 1 -y (1) • • • y (q) 1 -y (q) . . . . . . y (1) N -y (1) • • • y (q) N -y (q)             . Then b must minimize |Y c -X c b| 2 , yielding b = (X T c X c ) -1 X T c Y c , â0 = ȳ -xT b.</formula><p>The reader may want to double-check that this solution coincides with the one provided in theorem 7.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Limit behavior</head><p>The matrix</p><formula xml:id="formula_483">ΣXX = 1 N X T c X c = 1 N N k=1 (x k -x)(x k -x) T</formula><p>is a sample estimate of the covariance matrix of X, that we will denote Σ XX . Similarly, ΣXY = X T c Y c /N is a sample estimate of Σ XY , the covariance between X and Y . With this notation, we have b = Σ-1 XX ΣXY , which, by the law of large numbers, converges to b</p><formula xml:id="formula_484">* = Σ -1 XX Σ XY . Let a * 0 = m Y -m T X b * . Then f * (x) = a * 0 +(b * ) T</formula><p>x is the least-square optimal approximation of Y by a linear function of X, and the linear predictor f (x) = â0 + bT x converges a.s. to f * (x). Of course, f * generally differs from f : x → E(Y | X = x), which is the least-square optimal approximation of Y by any (square-integrable) function of X, so that the linear estimator will have a residual bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Gauss-Markov theorem</head><p>If one makes the (unlikely) assumption that the linear model is exact, i.e., f (x) = f * (x), one has:</p><formula xml:id="formula_485">E( β) = E(E( β | X )) = E((X T X ) -1 X T E(Y | X )) = E((X T X ) -1 X T X β) = β</formula><p>and the estimator is "unbiased." Under this parametric assumption, many other properties of linear estimators can be proved, among which the well-known Gauss-Markov theorem on the optimality of least-square estimation that we now state and prove. For this theorem, for which we take (for simplicity) q = 1, we also assume that var(Y | X = x), the variance of Y for its conditional distribution given X does not depend on x, and denote it by σ 2 . This typically correspond to the standard regression model in which one assumes that Y = f (X) + ϵ where ϵ is independent of X with variance σ 2 .</p><p>Recall that a symmetric matrix A is said to be larger than or equal to another symmetric matrix, B, writing A ⪰ B, if and only if A -B is positive semi-definite.</p><p>Theorem 7.3 (Gauss-Markov) Assume that an estimator β takes the form β = A(X )Y (it is linear) and is unbiased conditionally to X : E β ( β | X ) = β (for all β). Then (under the assumptions above) the covariance matrix of β cannot be smaller than that of the least square estimate, β.</p><p>Proof We write A = A(X ) for short. The condition that E(AY | X ) = β for all β yields AX β = β for all β, or AX = Id R d+1 (A is a (d + 1) × N matrix). Since β is unbiased, its covariance matrix is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(AY</head><formula xml:id="formula_486">Y T A T ) -ββ T and E(AY Y T A T ) = E(E(AY Y T A T | X )) = σ 2 E(AA T ).</formula><p>For β = β, for which A = (X T X ) -1 X T , we get E(AY Y T A T ) = σ 2 E((X T X ) -1 ). We therefore need to show that E(AA T ) ⪰ E(X T X ), i.e., that for any u ∈ R d+1 ,</p><formula xml:id="formula_487">u T E(AA T )u ≥ u T E((X T X ) -1 )u</formula><p>as soon as AX = Id R d+1 . We in fact have the stronger result (without expectations):</p><formula xml:id="formula_488">AX = Id R d+1 ⇒ AA T ⪰ (X T X ) -1 .</formula><p>To see this, fix u and consider the problem of minimizing F u (A) = A → u T AA T u subject to the linear constraint AX = Id R d+1 . The Lagrange multipliers for this affine constraint can be organized in a matrix C and the Lagrangian is</p><formula xml:id="formula_489">u T AA T u + trace(C T (AX -Id R d+1 )).</formula><p>Taking the derivative in A, we find that optimal solutions must satisfy 2u T AH T u + trace(C T HX ) = 0 for all H, which yields trace(H T (2uu T A + CX T )) = 0 for all H. This is only possible when 2uu T A + CX T = 0, which in turn implies that 2uu T AX = -CX T X . Using the constraint, we get</p><formula xml:id="formula_490">C = -2uu T (X T X ) -1</formula><p>so that uu T A = uu T (X T X ) -1 X T . This implies that A = (X T X ) -1 X T (the least-square estimator) is a minimizer of F u (A) for all u.</p><p>Any other solution that satisfies uu T A = uu T (X T X ) -1 X T for all u. Taking u = e i and summing over i (with d+1 i=1 e i e T i = Id R d+1 ) yields A = (X T X ) -1 X T . ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Kernel Version</head><p>We now assume that X takes its values in an arbitrary set R X , with a representation h : R X → H into an inner-product space. This representation does not need to be explicit or computable, but the associated kernel K(x, y) = ⟨h(x) , h(y)⟩ H is assumed to be known and easy to compute. (Recall that, from chapter 6, a positive kernel is always associated with an inner-product space.) In particular, any algorithm in this context should only rely on the kernel, and the function h only has a conceptual role.</p><p>Assume that q = 1 to lighten the notation, so that the dependent variable is scalarvalued. We here let the space of predictors be</p><formula xml:id="formula_491">F = {f (x) = a 0 + ⟨b , h(x)⟩ H : a 0 ∈ R, b ∈ H}.</formula><p>The residual sum of squares associated with this function space is</p><formula xml:id="formula_492">RSS(a 0 , b) = N k=1 (y k -a 0 -⟨b , h(x k )⟩) 2 .</formula><p>The following result (or results similar to it) is a key step in almost all kernel methods in machine learning. Proposition 7.4 Let V = span(h(x 1 ), . . . , h(x N )) be the finite-dimensional subspace of H generated by the feature functions evaluated on training input data. Then</p><formula xml:id="formula_493">RSS(a 0 , b) = RSS(a 0 , π V (b)).</formula><p>where π V is the orthogonal projection on V .</p><p>Proof The justification is immediate: since h(x k ) ∈ V , we have </p><formula xml:id="formula_494">⟨b , h(x k )⟩ H = ⟨π V (b) , h(x k )⟩ H for all b ∈ H.</formula><formula xml:id="formula_495">= N k=1 α k h(x k ) (7.3)</formula><p>and the regression problem can be reformulated as a function of the coefficients α 1 , . . . , α N ∈ R, with</p><formula xml:id="formula_496">f (x) = a 0 + N k=1 α k ⟨h(x) , h(x k )⟩ H = a 0 + N k=1 α k K(x, x k ),</formula><p>which only depends on the kernel. (This reduction is often referred to as the "kernel trick.") However, the solution of the problem is, in this context, not very interesting. Indeed, assume that K is positive definite and that all observations in the training set are distinct. Then the matrix K(x 1 , . . . , x N ) formed by the kernel evaluations K(x i , x j ) is invertible, and one can solve exactly the equations</p><formula xml:id="formula_497">y k = N j=1 α j K(x k , x j ), k = 1, . . . , N</formula><p>to get a zero RSS with a 0 = 0. Unless there is no noise, such a solution will certainly overfit the data. If K is not positive definite, and the dimension of V is less than N (since this would place us in the previous situation otherwise), then it is more efficient to work directly in a basis of V rather than using the over-parametrized kernel representation. We will see however, starting with the next section, that kernel methods become highly relevant as soon as the regression is estimated with some control on the size of the regression coefficients, b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Ridge regression and Lasso</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Ridge Regression</head><p>Method. When the set F of possible predictors is too large, some additional complexity control is needed to reduce the estimation variance. One simple approach is to limit the number of parameters to be estimated, which, for regression, corresponds to limiting the number of possible predictors. This is related to the methods of Sieves mentioned in section 4.1. In contrast, ridge regression and lasso control the size of the parameters, as captured by their norm.</p><p>In both cases, one assigns a measure of complexity, denoted f → γ(f ) ≥ 0, to each element f ∈ F . Given γ, one can either optimize this predictor (using, for example, the RSS) with the constraint that γ(f ) ≤ C for some constant C, or add a penalty λγ(f ) to the objective function for some λ &gt; 0. In general, the two approaches (constraint or penalty) are equivalent.</p><p>In linear spaces, complexity measures are often associated with a norm, and ridge regression uses the sum of squares of coefficients of the prediction matrix b, minimizing</p><formula xml:id="formula_498">N k=1 |y k -a 0 -b T x k | 2 + λ trace(b T b) , (<label>7.4)</label></formula><p>which can be written in vector form as</p><formula xml:id="formula_499">|Y -X β| 2 2 + λ trace(β T ∆β),</formula><p>where ∆ = diag(0, 1, . . . , 1). In the following, we will work with an unspecified (d + 1) × (d + 1) symmetric positive semi-definite matrix ∆. Various choices are indeed possible, for example, ∆ = diag(0, σ 2 (1), . . . , σ 2 (d)), where σ 2 (i) is the empirical variance of the ith coordinate of X in the training set. This last choice is quite natural, because it ensures that, whenever one of the variable X (i) is rescaled by a factor c, the corresponding optimal i th row of b T is rescaled by 1/c, leaving the predictor unchanged.</p><p>Under this assumption, the optimal parameter is βλ = (X T X + λ∆) -1 X T Y , with a proof similar to that made for least-square regression. We obviously retrieve the original formula for regression when λ = 0.</p><p>Alternatively, assuming that ∆ = 0 0 0 ∆ ′ , so that no penalty is imposed on the intercept, we have bλ</p><formula xml:id="formula_500">= (X T c X c + λ∆ ′ ) -1 X T c Y c (7.5)</formula><p>and â0 λ = ȳ -( bλ ) T x. The proof of these statements is left to the reader.</p><p>Analysis in a special case To illustrate the impact of the penalty term on balancing bias and variance, we now make a computation in the special case when Y = Xβ + ϵ, where var(ϵ) = σ 2 and ϵ is independent of X. In the following computation, we assume that the training set is fixed (or rather, compute probabilities and expectations conditionally to it). Also, to simplify notation, we denote</p><formula xml:id="formula_501">S λ = X T X + λ∆ = N k=1 xT k xk + λ∆</formula><p>and Σ = E( XT X) for a single realization of X. Finally, we assume that q = 1, also to simplify the discussion.</p><p>The mean-square prediction error is</p><formula xml:id="formula_502">R(λ) = E((Y -XT βλ ) 2 ) = E(( XT (β -βλ ) + ϵ) 2 ) = ( βλ -β) T Σ( βλ -β) + σ 2 .</formula><p>Denote by ϵ k the (true) residual ϵ k = y k -xT k β on training data and by ϵ the vector stacking these residuals. We have, writing S 0 = S λ -λ∆,</p><formula xml:id="formula_503">βλ = S -1 λ X T Y = S -1 λ S 0 β + S -1 λ X T ϵ = β -λS -1 λ ∆β + S -1 λ X T ϵ</formula><p>So we can rewrite</p><formula xml:id="formula_504">R(λ) = λ 2 β T ∆S -1 λ ΣS -1 λ ∆β -2λϵ T X S -1 λ ΣS -1 λ ∆β + ϵ T X S -1 λ ΣS -1 λ X T ϵ + σ 2 .</formula><p>Let us analyze the quantities that depend on the training set in this expression. The first one is S λ = S 0 + λ∆. From the law of large numbers, S 0 /N → Σ when N tends to infinity, so that, assuming in addition that λ = λ N = O(N ), we have</p><formula xml:id="formula_505">S -1 λ = O(1/N ). The second one is ϵ T X = N k=1 ϵ k xk</formula><p>which, according to the central limit theorem, is such that</p><formula xml:id="formula_506">N -1/2 ϵ T X ∼ N (0, σ 2 Var( X))</formula><p>when N → ∞. So, we can expect the coefficient of λ 2 in R(λ) to have order N -2 , the coefficient of λ to have order N -3/2 and the constant coefficient of have order N -1 . This suggests taking λ = µ √ N so that all coefficients have roughly the same order when expanding in powers of µ. This gives S λ = N (S 0 /N + µ∆/ √ N ) ≃ N Σ and we make the approximation, letting ξ = N -1/2 σ -1/2 ϵX T and γ = Σ -1/2 ∆β, that</p><formula xml:id="formula_507">N (R(λ) -σ 2 ) ≃ µ 2 |γ| 2 -2µξ T γ + ξ T ξ.</formula><p>With this approximation, the optimal µ should be</p><formula xml:id="formula_508">µ = ξ T γ |γ| 2 .</formula><p>Of course, this µ cannot be computed from data, but we can see that, since ξ converges to a centered Gaussian random variable, its value cannot be too large. It is therefore natural to choose µ to be constant and use ridge regression in the form</p><formula xml:id="formula_509">N k=1 (y k -xT k β) 2 + √ N µβ T ∆β.</formula><p>In all cases, the mere fact that we find that the optimal µ is not 0 shows that, under the simplifying (and optimistic) assumptions that we made for this computation, allowing for a penalty term always reduces the prediction error. In other terms, introducing some estimation bias in order to reduce the variance is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel Ridge Regression</head><p>We now return to the feature-space situation and take h : R X → H with associated kernel K. We still take q = 1 for simplicity. One formulates the ridge regression problem in this context as the minimization of</p><formula xml:id="formula_510">N k=1 (y l -a 0 -⟨b , h(x l )⟩ H ) 2 + λ∥b∥ 2 H</formula><p>with respect to β = (a 0 , b). Introducing the space V generated by the feature function evaluated on the training set, we know from proposition 7.4 that replacing b by π V (b) leaves the residual sum of squares invariant. Moreover, one has ∥π</p><formula xml:id="formula_511">V (b)∥ 2 H ≤ ∥b∥ 2</formula><p>H with equality if and only if b ∈ V . This shows that the solution b must belong to V and therefore take the form <ref type="bibr">(7.3)</ref>.</p><p>Using this expression, one finds that the problem is reduced to finding the minimum of</p><formula xml:id="formula_512">N k=1        y k -a 0 - N l=1 K(x l , x k )α l        2 + λ N k,l=1 α k α l K(x k , x l )</formula><p>with respect to a 0 , α 1 , . . . , α N . Recall that we have denoted by K = K(x 1 , . . . , x N ) the kernel matrix with entries K(x i , x j ), i, j = 1, . . . , N . We will assume in the following that K is invertible.</p><p>Introduce the vector 1 N ∈ R N with all coordinates equal to one. Let K = 1 N K and K ′ = 0 0 0 K .</p><p>Let α ∈ R N be the vector with coefficients α 1 , . . . , α N and α = a 0 α . With this notation, the function to minimize is</p><formula xml:id="formula_513">F(α) = |Y -K α| 2 + λ αT K ′ α.</formula><p>This takes the same form as standard ridge regression, replacing β by α, X by K and ∆ by K ′ . The solution therefore is</p><formula xml:id="formula_514">αλ = ( KT K + λK ′ ) -1 KT Y .</formula><p>Note that K being invertible implies that KT K + λK ′ is invertible. 1   To write the equivalent of (7.5), we need to use the equivalent of the matrix X c , that is, the matrix K with the average of the jth column subtracted to each (i, j) entry, given by:</p><formula xml:id="formula_515">K c = K - 1 N 1 N 1 T N K.</formula><p>Introduce the matrix P = Id -1 N 1 T N /N . It is easily checked that P 2 = P (P is a projection matrix). Since K c = P K, we have K T c K c = KP K. One deduces from this the expression of the optimal vector α λ , namely,</p><formula xml:id="formula_516">α λ = (KP K + λK) -1 KP Y c = (P K + λId R N ) -1 Y c</formula><p>where we have, in addition, used the fact that P Y c = Y c . Finally, the intercept is given by</p><formula xml:id="formula_517">a 0 = y - 1 N (α λ ) T K1 N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Equivalence of constrained and penalized formulations</head><p>Case of ridge regression. Returning to the basic case (without feature space), we now introduce an alternate formulation of ridge regression. Let ridge(λ) denote the ridge regression problem that we have considered so far, for some parameter 1 Indeed, let u = w 0 w with w 0 ∈ R and w ∈ R N be such that u T ( KT K + λK ′ )u = 0. This requires Ku = 0 and u T K ′ u = 0. The latter quantity is w T Kw, which shows that w = 0 since K has rank N . Then K = 1 N w 0 so that w 0 = 0 also.</p><p>λ. Consider now the following problem, which will be called ridge ′ (C): minimize N k=1 |y k -xT k β| 2 subject to the constraint β T ∆β ≤ C. We claim that this problem is equivalent to the ridge regression problem, in the following sense: for any C, there exists a λ such that the solution of ridge ′ (C) coincides with the solution of ridge(λ) and vice-versa. Indeed, fix a C &gt; 0. Consider an optimal β for ridge ′ (C). Assuming as above that ∆ is symmetric positive semi-definite, we let V be its null space and P V the orthogonal projection on V . Write β = β 1 + β 2 with β 1 = P V β. Let d 1 and d 2 be the respective dimensions of V and V ⊥ so that d 1 + d 2 = d. Identifying R d with the product space V × V ⊥ (i.e., making a linear change of coordinates), the problem can be rewritten as the minimization of</p><formula xml:id="formula_518">|Y -X 1 β 1 -X 2 β 2 | 2 subject to β T 2 ∆β 2 ≤ C, where X 1 (resp. X 2 ) is N × d 1 (resp. N × d 2 ).</formula><p>The gradient of the constraint γ(</p><formula xml:id="formula_519">β 2 ) = β T 2 ∆β 2 -C is ∇γ(β 2 ) = 2∆β 2 .</formula><p>Assume first that ∆β 2 0. Then the solution must satisfy the KKT conditions, which require that there exists µ ≥ 0 such that β is a stationary point of the Lagrangian</p><formula xml:id="formula_520">|Y -X 1 β 1 -X 2 β 2 | 2 + µβ T 2 ∆β 2 ,</formula><p>with µ &gt; 0 only possible if β T ∆β = C. This requires that</p><formula xml:id="formula_521">X T 1 X 1 β 1 + X T 1 X 2 β 2 = X T Y , X T 2 X 1 β 1 + X T 2 X 2 β 2 + µ∆β 2 = X T Y .</formula><p>Since ∆β 1 = 0, and using X = (X 1 , X 2 ), we have</p><formula xml:id="formula_522">β = (X T X + µ∆) -1 X T Y ,</formula><p>which is the only solution of ridge(µ).</p><p>If ∆β 2 = 0, then, necessarily, β 2 = 0. Since C &gt; 0, β must then be the solution of the unconstrained problem, which is ridge(0). Conversely, any solution β of ridge(λ) satisfies the first-order optimality conditions for ridge ′ (C) for C = β T ∆β (or any C ≥ β T ∆β if λ = 0). This shows the equivalence of the two problems.</p><p>General case. We now consider this equivalence in a more general setting. Consider a penalized optimization problem, denoted var(λ) which consists in minimizing in β some objective function of the form U (β) + λϕ(β), λ ≥ 0. Consider also the family of problems var ′ (C), with C &gt; inf(ϕ), which minimize U (β) subject to ϕ(β) ≤ C.</p><p>We make the following assumptions.</p><p>(i) U and ϕ are continuous functions from R n to R.</p><formula xml:id="formula_523">(ii) ϕ(β) → ∞ when β → ∞.</formula><p>(iii) For any λ ≥ 0, there is a unique solution of var(λ), denoted β λ .</p><p>(iv) For any C, there is a unique solution of var ′ (C). denoted β ′ C .</p><p>Assumptions (ii) and (iv) are true, in particular, when U is strictly convex, ϕ is convex and U has compact level sets. We show that, with these assumptions, the two families of problems are equivalent.</p><p>We first discuss the penalized problems and prove the following proposition, which has its own interest. Moreover, β λ varies continuously as a function of λ.</p><p>Proof Consider two parameters λ and λ ′ . We have</p><formula xml:id="formula_524">U (β λ ) + λϕ(β λ ) ≤ U (β λ ′ ) + λϕ(β λ ′ ) and U (β λ ′ ) + λ ′ ϕ(β λ ′ ) ≤ U (β λ ) + λ ′ ϕ(β λ )</formula><p>since both left-hand sides are minimizers. This implies</p><formula xml:id="formula_525">λ(ϕ(β λ ) -ϕ(β λ ′ )) ≤ U (β λ ′ ) -U (β λ ) ≤ λ ′ (ϕ(β λ ) -ϕ(β λ ′ )). (<label>7.6)</label></formula><p>In particular: (λ ′λ)(ϕ(β λ )ϕ(β λ ′ )) ≥ 0. Assume that λ &lt; λ ′ . Then this last inequality implies ϕ(β λ ) ≥ ϕ(β λ ′ ) and (7.6) then implies that U (β λ ) ≤ U (β λ ′ ), which proves the first part of the proposition. Now assume that there exists ϵ &gt; 0 such that ϕ(β λ ) &gt; inf ϕ + ϵ for all λ ≥ 0. Take β such that ϕ( β) ≤ inf ϕ + ϵ/2. For any λ &gt; 0, we have</p><formula xml:id="formula_526">U (β λ ) + λϕ(β λ ) ≤ U ( β) + λϕ( β) so that U (β λ ) &lt; U ( β) -λϵ/2. Since U (β λ ) ≥ U (a 0 ), we get U (a 0 ) = -∞,</formula><p>which is a contradiction. This shows that ϕ(β λ ) tends to inf(ϕ) when λ tends to infinity.</p><p>We now prove that λ → β λ is continuous. Define G(β, λ) = U (β)+λϕ(β). Since we assume that ϕ(β) → ∞ when β → ∞, and we have just proved that ϕ(β λ ) ≤ ϕ(a 0 ) for any λ, we obtain the fact that the set (|β λ |, λ ≥ 0) is bounded, say by a constant B ≥ 0.</p><p>Consider a sequence λ n that converges to λ. We want to prove that β λ n → β λ , for which (because β λ is bounded) it suffices to show that if any subsequence of (β λ n ) converges to some β, then β = β λ .</p><p>So, consider such a converging subsequence, that we will still denote by β λ n for convenience. Since G is continuous, one has G(β λ n , λ n ) → G( β, λ) when n tends to infinity. Let us prove that G(β λ , λ) is continuous in λ. For any pair λ, λ ′ and any β, we have</p><formula xml:id="formula_527">G(β λ ′ , λ ′ ) ≤ G(β λ , λ ′ ) = G(β λ , λ) + (λ ′ -λ)ϕ(β λ ) ≤ G(β λ , λ) + |λ ′ -λ|ϕ(a 0 ) .</formula><p>This yields, by symmetry,</p><formula xml:id="formula_528">|G(β λ ′ , λ ′ ) -G(β λ , λ)| ≤ ϕ(a 0 )|λ -λ ′ |, proving the continuity in λ.</formula><p>So we must have G( β, λ) = G(β λ , λ). This implies that both β and β λ are solutions of var(λ), so that β λ = β because we assume that the solution is unique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We now prove that the classes of problems var(λ) and var</p><formula xml:id="formula_529">′ (C) are equivalent. First, β λ is a minimizer of U (β) subject to the constraint ϕ(β) ≤ C, with C = ϕ(β λ ). Indeed, if U (β) &lt; U (β λ ) for some β with ϕ(β) ≤ ϕ(β λ ), then U (β) + λϕ(β) &lt; U (β λ ) + λϕ(β λ ) which is a contradiction. So β λ = β ′ ϕ(β λ )</formula><p>. Using the continuity of β λ and ϕ, this proves the equivalence of the problems when C is in the interval (a, ϕ(a 0 )) where a = lim λ→∞ ϕ(β λ ) = inf(ϕ). So, it remains to consider the case C &gt; ϕ(a 0 ). For such a C, the solution of var ′ (C) must be a 0 since it is a solution of the unconstrained problem, and satisfies the constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.3">Lasso regression</head><p>Problem statement Assume that the output variable is scalar, i.e., q = 1. Let σ 2 (i) be the empirical variance of the ith variable X (i) . Then, the lasso estimator is defined as a minimizer of</p><formula xml:id="formula_530">N k=1 (y k -xT k β) 2 subject to the constraint d i=1 σ (i)|β (i) | ≤ C.</formula><p>Compared to ridge regression, the sum of squares for β is simply replaced by a weighted sum of absolute values, but we will see that this change may significantly affect the nature of the solutions.</p><p>As we have just seen, the penalized formulation, minimizing</p><formula xml:id="formula_531">N k=1 (y k -xT k β) 2 + λ d i=1 σ (i)|β (i) |</formula><p>provides an equivalent family of problems, on which we will focus (because it is easier to analyze). Since one uses a non-Euclidean norm in the penalty, there is no kernel version of the lasso and we only discuss the method in the original input space R = R d .</p><p>For a vector a ∈ R k , we let |a| 1 = |a (1) </p><formula xml:id="formula_532">| + • • • + |a (k) |,</formula><p>the ℓ 1 norm of a. Using the previous notation for Y and X , the quantity to minimize can be rewritten as |Y -X β| 2 + λ|Dβ| 1 where D is the d × (d + 1) matrix with d(i, i + 1) = σ (i) for i = 1, . . . , d and all other coefficients equal to 0. This is a convex optimization problem which, unlike ridge regression, does not have a closed form solution. ADMM. The alternating direction method of multipliers (ADMM) that was described in section 3.6, (3.59) is one of the state-of-the-art algorithm to solve the lasso problem, especially in large dimensions. Other iterative methods include subgradient descent (see the example in section 3.5.4) and proximal gradient descent. Since x has a different meaning here, we change the notation in (3.59) by replacing x, z, u by β, γ, τ, and rewrite the lasso problem as the minimization of |Y -X β| 2 + λ|γ| 1 subject to Dβγ = 0. Applying (3.59) with A = D, B = -Id and c = 0, the ADMM iterations are</p><formula xml:id="formula_533">                     β(n + 1) = argmin β |Y -X β| 2 + 1 2ρ |Dβ -γ(n) + τ(n)| 2 γ (i) (n + 1) = argmin t λ|t| + 1 2ρ (t -Dβ (i) (n + 1) -τ (i) (n)) 2 , i = 1, . . . , d τ(n + 1) = τ(n) + Dβ(n + 1) -γ(n + 1)</formula><p>The solutions of both minimization problems are explicit, yielding the following algorithm, which converges to a solution if ρ is small enough.</p><p>Algorithm 7.1 (ADMM for lasso) Let ρ &gt; 0 be chosen. Starting with initial values β (0) , γ (0) and τ (0) , the ADMM algorithm for lasso iterates:</p><formula xml:id="formula_534">                 β(n + 1) = X T X + D T D 2ρ -1 X T Y + D T 2ρ (γ(n) -τ(n)) γ (i) (n + 1) = S λρ Dβ (i) (n + 1) + τ (i) (n) , i = 1, . . . , d τ(n + 1) = τ(n) + Dβ(n + 1) -γ(n + 1)</formula><p>until the difference between the variables at steps n and n + 1 is below a small tolerance level. Here, S λρ is the so-called shrinkage operator</p><formula xml:id="formula_535">S λρ (v) =            v -λρ if v ≥ λρ 0 if |v| ≤ λρ v + λρ if v ≤ -λρ</formula><p>Note that the ADMM algorithm makes an iterative approximation of the constraints, so that they are only satisfied at some precision level when the algorithm is stopped.</p><p>Exact computation. We now provide a more detailed characterization of the solution of the lasso problem and analyze, in particular, how this solution changes when λ (or C) varies. To simplify the exposition, and without loss of generality, we will assume that the variables have been normalized so that σ (i) = 1 and the penalty simply is the sum of absolute values. Let</p><formula xml:id="formula_536">G λ (β) = N k=1 (y k -a 0 -x T k b) 2 + λ d i=1 |b(i)|.</formula><p>The following proposition, in which we let</p><formula xml:id="formula_537">r b = 1 N N k=1 (y k -a 0 -x T k b)x k ,</formula><p>characterizes the solution of the lasso.</p><p>Proposition 7. <ref type="bibr" target="#b24">6</ref> The pair (a 0 , b) is the optimal solution of the lasso problem with parameter λ if and only if a 0 = ȳ -xT b and, for all i = 1, . . . , d,</p><formula xml:id="formula_538">|r (i) b | ≤ λ 2N (7.7) with r (i) b = sign(b (i) ) λ 2N if b (i) 0. (7.8)</formula><p>In particular |r</p><formula xml:id="formula_539">(i) b | &lt; λ/(2N ) implies b (i) = 0.</formula><p>Proof Using the subdifferential calculus in theorem 3.45, one can compute the subgradients of G by adding the subdifferentials of the terms that compose it. All these terms are differentiable except |b (i) | when b (i) = 0, and the subdifferential of t → |t| at t = 0 is the interval <ref type="bibr">[-1, 1]</ref>.</p><p>This shows that g ∈ ∂G λ (β) if and only if g = -2N r b + λz with z (i) = sign(b (i) ) if b (i) 0 and |z (i) | ≤ 1 otherwise. Proposition 7.6 immediately follows by taking g = 0.</p><p>■ Let ζ = sign(b), the vector formed by the signs of the coordinates of b, with sign(0) = 0. Then proposition 7.6 uniquely specifies a 0 and b once λ and ζ are known. Indeed, let J = J ζ denote the ordered subset of indices j ∈ {1, . . . , d} such that ζ (j) 0, and let b(J), x k (J), ζ(J), etc., denote the restrictions of vectors to these indices. Equation (7.8) can be rewritten as (after replacing a 0 by its optimal value)</p><formula xml:id="formula_540">X c (J) T X c (J)b(J) = X c (J) T Y c - λ 2 ζ(J)</formula><p>where</p><formula xml:id="formula_541">X c (J) =           (x 1 (J) -x(J)) T . . . (x N (J) -x(J)) T           . This yields b(J) = (X c (J) T X c (J)) -1 X c (J) T Y c - λ 2 ζ(J) ,<label>(7.9)</label></formula><p>which fully determine b since b (j) = 0 if j J, by definition.</p><p>For given λ, only one sign configuration ζ will provide the correct solution, with correct signs for nonzero values of b above, and correct inequalities on r b . Calling this configuration ζ λ , one can note that if ζ λ is known for a given value of λ, it remains valid if we increase or decrease λ until one of the optimality conditions changes, i.e., either one of the coordinates b (i) , i ∈ J ζ λ , vanishes, or one of the inequalities for i J ζ λ becomes an equality. Moreover, proposition 7.6 shows that between these events both b and therefore r b depend linearly on λ, which makes easy the task of determining maximal intervals around a given λ over which ζ remains unchanged.</p><p>Note that solutions are known for λ = 0 (standard least squares) and for λ large enough (for which b = 0). Indeed, for b = 0 to be a solution, it suffices that</p><formula xml:id="formula_542">λ &gt; λ 0 ∆ = 2 max i N k=1 (y k -y)(x (i) k -x (i) ) .</formula><p>These remarks set the stage for an algorithm computing the optimal solution of the lasso problem for all values of λ, starting either from λ = 0 or λ &gt; λ 0 . We will describe this algorithm starting for λ &gt; λ 0 , which has the merit to avoid complications due to underconstrained least squares when d is large. For this purpose, we need a little more notation. For a given ζ, let</p><formula xml:id="formula_543">b ζ = (X c (J ζ ) T X c (J ζ )) -1 X c (J ζ ) T Y c and u ζ = 1 2 (X c (J ζ ) T X c (J ζ )) -1 ζ(J ζ ), so that b(J ζ ) = b ζ -λu ζ .</formula><p>The residuals then take the form</p><formula xml:id="formula_544">r (i) b = 1 N N k=1 (y k -a 0 -b T ζ x k )x (i) k + λ N N k=1 (x T k u ζ )(x (i) k -x (i) ) = ρ (i) ζ + λd (i) ζ ,</formula><p>where the last equation defines ρ ζ and d ζ .</p><p>Assume that one wants to minimize G λ * for some λ * &gt; 0. We need to describe the sequence of changes to the minimizers of G λ when λ decreases from some value larger than λ 0 to the value λ * .</p><p>If λ * ≥ λ 0 , then the optimal solution is b = 0, so we can assume that λ * &lt; λ 0 . When λ is slightly smaller than λ 0 , one needs to introduce some non-zero values in ζ. Those values are at the indexes i such that</p><formula xml:id="formula_545">λ 0 = 2 N k=1 (y k -y)(x (i) k -x (i) )</formula><p>The sign of ζ (i) is also determined since sign(b (i) ) = sign(r</p><formula xml:id="formula_546">(i) b ) when b (i) 0.</formula><p>The algorithm will then continue by progressively adding non-zero entries to ζ when the covariance between some unused variables and the residual becomes too large, or by removing non-zero values when the optimal b crosses a zero. We now describe it in detail. Algorithm 7.2 (Exact minimization for lasso)</p><p>1. Initialization: let λ(0) = 1 + λ 0 , σ (0) = 0 and the corresponding values a 0 (0) = y and b(0) = 0.</p><p>2. Assume that the algorithm has reached step n with current variables λ(n), σ (n), a 0 (n) and b(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Determine the first λ</head><formula xml:id="formula_547">′ &lt; λ(n) for which either (i) For some i, ζ (i) (n) 0 and b (i) ζ(n) -λ ′ u (i) ζ(n) = 0. (ii) For some i, ζ (i) (n) = 0 and (1 -2N d (i) ζ(n) )λ ′ -2N ρ ζ(n) = 0. (iii) For some i, ζ (i) (n) = 0 and (1 + 2N d (i) ζ(n) )λ ′ + 2N ρ ζ(n) = 0.</formula><p>4. Then, there are two cases: </p><formula xml:id="formula_548">(a) If λ ′ ≥ λ * , set λ(n + 1) = λ ′ . Let ζ (i) (n + 1) = ζ (i) (n) if i does not satisfy (i), (ii) or (iii). If i is in case (i), set ζ (i) (n+1) = 0. For i in case (ii) (resp. (iii)), set ζ (i) (n+1) = 1 (resp. -1). (b) If λ ′ &lt; λ * ,</formula><formula xml:id="formula_549">(i) = b (i) ζ(n) -λ * u (i) ζ(n) , ζ (i) (n) 0</formula><p>and a 0 = ȳb T x to obtain the final solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Other Sparsity Estimators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">LARS estimator</head><p>Algorithm. The LARS algorithm can be seen as a simplification of the previous lasso algorithm in which one always adds active variables at each step. We assume as above that input variables are normalized such that σ (i) = 1.</p><p>Given a current set J of selected variables, the algorithm will decide either to stop or to add a new variable to J according to a criterion that depends on a parameter λ &gt; 0. Let b (J) ∈ R |J| be the least-square estimator based on variables in J b</p><formula xml:id="formula_550">(J) = (X c (J) T X c (J)) -1 X c (J) T Y c . Let b J ∈ R d such that b (i) J = b (i)</formula><p>(J) for i ∈ J and 0 otherwise. The covariances between the remaining variables and the residuals are given by</p><formula xml:id="formula_551">r (i) J = 1 N N k=1 (y k -y -(x k -x) T b J )(x (i) k -x (i) ), i J.</formula><p>If, for all i ∈ J, |r</p><formula xml:id="formula_552">(i) J | ≤ √ λ/N ,</formula><p>the procedure is stopped. Otherwise, one adds to J the variable i such that |r (i) J | is largest and continues. Justification. Recall the notation |b| 0 for the number of non-zero entries of b. Consider the objective function</p><formula xml:id="formula_553">L(b) = |Y c -X c b| 2 + λ|b| 0 .</formula><p>Let J be the set currently selected by the algorithm, and b J defined as above. We consider the problem of adding one non-zero entry to b. Fix i J, and let b ∈ R d have all coordinates equalt to those of b J for all except the ith one, which is therefore allowed to be non-zero. Then</p><formula xml:id="formula_554">L( b) = N k=1 y k -y - j∈J (x (j) k -x)b (j) -(x (i) k -x) b(i) 2 + λ|J| + λ,</formula><p>so that (using σ (i) = 1)</p><formula xml:id="formula_555">L( b) = L(b J ) -2N r (i) J b(i) + N ( b(i) ) 2 + λ</formula><p>Now, L( b) is an upper-bound for L(b J∪{i} ), and so is its minimum with respect to b(i) . This yields:</p><formula xml:id="formula_556">L(b J∪{i} ) ≤ L(b J ) -N (r<label>(i)</label></formula><p>J ) 2 + λ The LARS algorithm therefore finds the value of i that minimizes this upper-bound, provided that the resulting minimum is less that L(b J ).</p><p>Variant. The same argument can be made with |b| 0 replaced by |b| 1 and one gets</p><formula xml:id="formula_557">L( b) = L(b J ) -2N r (i) J b(i) + N ( b(i) ) 2 + λ| b(i) |</formula><p>Minimizing this expression with respect to b(i) yields the upper bound:</p><formula xml:id="formula_558">L(b J∪{i} ) ≤            L(b J ) -N |r (i) J | - λ 2N 2 if |r (i) J | ≥ λ 2N L(b J ) if |r (i) J | ≤ λ 2N</formula><p>This leads to the following alternate form of LARS. Given a current set J of selected variables, compute</p><formula xml:id="formula_559">r (i) J = 1 N N k=1 (y k -y -(x k -x) T b J )(x (i) k -x (i) ), i J .</formula><p>If, for all i J, |r (i) J | ≤ λ/2N , stop the procedure. Otherwise, add to J the variable i such that |r (i) J | is largest and continue. This form tends to add more variables since the stopping criterion decreases in 1/N instead of 1/ √ N .</p><p>Why "least angle"? Let µ J,k = y k -y -(x k -x) T b J denote the residual after regression. The empirical correlation between µ and x (i) is equal to the cosine of the angle, say θ (i) J between µ J ∈ R N and x (i) -x both considered as vectors in R N . This cosine is also equal to cos θ</p><formula xml:id="formula_560">(i) J = µ T J (x (i) -x (i) ) |x (i) -x (i) | |µ J | = √ N r (i) J |µ J |</formula><p>where we have used the fact that |x (i) -</p><formula xml:id="formula_561">x (i) |/ √ N = σ (i) = 1.</formula><p>Since |µ J | does not depend on i, looking for the largest value of |r (i) J | is equivalent to looking for the smallest value of |θ (i)</p><p>J |, so that we are looking for the unselected variable for which the angle with the current residual is minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">The Dantzig selector</head><p>Noise-free case. Assume that one wants to solve the equation X β = Y when the dimension, N , of Y is small compared to number of columns, d, in X . Since the system is under-determined, one needs additional constraints on β and a natural one is to look for sparse solutions, i.e., find solutions with a maximum number of zero coefficients. However, this is numerically challenging, and it is easier to minimize the ℓ 1 norm of β instead (as seen when discussing the lasso, using this norm often provides sparse solutions). In the following, we assume that the empirical variance of each variable is normalized, so that, denoting X (i) the ith column of X , we have</p><formula xml:id="formula_562">|X (i)| = 1.</formula><p>The Dantzig selector <ref type="bibr" target="#b65">[47]</ref> </p><formula xml:id="formula_563">minimizes d i=1 |β (i) |</formula><p>subject to the constraint X β = Y . This results in a linear program (therefore easy to implement). More precisely, introducing slack variables, it is indeed equivalent to minimize</p><formula xml:id="formula_564">d i=1 ξ(i) + d i=1 ξ * (i) subject to constraints ξ(i) ≥ β (i) , ξ(i) * ≥ -β (i) , ξ(i) ≥ 0, ξ * (i) ≥ 0 and X β = Y .</formula><p>Sparsity recovery Under some assumptions, this method does recover sparse solutions when they exist. More precisely, let β be the solution of the linear programming problem above. Assume that there is a set J * ⊂ {1, . . . , d} such that X β = Y for some β ∈ R d with β (i) = 0 if i J * . Conditions under which β is equal to β are provided in Candes and Tao <ref type="bibr" target="#b65">[47]</ref> and involve the correlations between pairs of columns of X , and the size of J.</p><p>That the size of J * must be a factor is clear, since, for the statement to make sense, there cannot exist two β's satisfying X β = Y and β (i) = 0 for i J * . Uniqueness is obviously not true if |J| &gt; N , because, even if one knew J, the condition would be under-constrained for β. Since the set J * is not known, and we also want to avoid any other solution associated to a set of same size. So, there cannot exist β and β respectively vanishing outside of J * and J * , where J * and J * have same cardinality, such that X β = Y = X β. The equation X (β -β) = 0 would be under-constrained as soon as the number of non-zero coefficients of β -β is larger than N , and since this number can be as large as</p><formula xml:id="formula_565">|J * | + | J * | = 2|J * |, we see that one should impose at least |J * | ≤ N /2.</formula><p>Given this restriction, another obvious remark is that, if the set J on which β does not vanish is known, with |J| small enough, then X β = Y is over-constrained and any solution is (typically) unique. So the issue really is whether the set J β listing the non-zero indexes of a solution β is equal to y J * .</p><p>As often, precious insight on the solution of this minimization problem is obtained by considering the dual problem. Introducing Lagrange multipliers λ(i) ≥ 0, i = 1, . . . , d for the constraints ξ(i)β (i) ≥ 0, λ * (i) ≥ 0, i = 1, . . . , d for ξ * (i) + β (i) ≥ 0, γ(i), γ * (i) ≥ 0 for ξ(i) ≥ 0 and ξ * (i) ≥ 0, and α ∈ R N for X β = Y , the Lagrangian is</p><formula xml:id="formula_566">L(β, ξ, λ, λ * , α) = (1 d -λ -γ) T ξ + (1 d -λ * -γ * ) T ξ * + (λ -λ * + X T α) T β -α T Y . The KKT conditions require γ = 1 d -λ, γ * = 1 d -λ * , X α = λ * -λ and the comple- mentary slackness conditions give (1 -λ(i))ξ(i) = (1 -λ * i )ξ * (i) = 0, λ(i)(β (i) -ξ(i)) = λ * (i)(β (i) + ξ * (i)) = 0.</formula><p>The dual problem requires to minimize α T Y subject to the constraints X T α = λ *λ and 0 ≤ λ(i), λ * (i) ≤ 1. Assume that (α, λ, λ * ) is a solution of this dual problem. One has the following cases.</p><p>(1) If λ(i) ∈ (0, 1), then ξ(i) = β (i)ξ(i) = 0, which implies ξ(i) = β (i) = 0, and, as a consequence (1λ * (i))ξ * (i) = λ * (i)ξ * (i) = 0, so that also ξ * (i) = 0 .</p><p>(2) Similarly, λ * (i) ∈ (0, 1) implies</p><formula xml:id="formula_567">ξ(i) = ξ * (i) = β (i) = 0. (3) If λ(i) = λ * (i) = 1, then β (i) -ξ(i) = β (i) + ξ(i) = 0 with ξ(i), ξ * (i) ≥ 0, so that also ξ(i) = ξ * (i) = β (i) = 0. (4) If λ(i) = λ * (i) = 0, then ξ(i) = ξ * (i) = 0 and since β (i) ≤ ξ(i) and β (i) ≤ -ξ * (i), we get β (i) = 0.</formula><p>(5) The only remaining situation, in which β (i) can be non-zero, is when λ(i) = 1λ * (i) ∈ {0, 1}, or, equivalently, when |λ(i)λ * (i)| = 1. This discussion allows one to reconstruct the set J β associated with the primal problem given the solution of the dual problem. Note that |λ(i)λ * (i)| = |α T X (i)|, so that the set of indexes with |λ(i)λ * (i)| = 1 is also</p><formula xml:id="formula_568">I α ∆ = i : |α T X (i)| = 1 .</formula><p>One has</p><formula xml:id="formula_569">α T Y = α T X β = d i=1 β (i) α T X (i) ≤ i∈J β |β (i) | |α T X (i)| ≤ i∈J β |β (i) |.</formula><p>The upper-bound is achieved when α T X (i) = sign(β (i) ) for i ∈ J β . So, if a vector α can be found such that</p><formula xml:id="formula_570">(i) α T X (i) = sign(β (i) ) for i ∈ J * , (ii) |α T X (j)| &lt; 1 for j J * ,</formula><p>then it is a solution of the dual problem with J α = J * .</p><p>Let s J = (s (j) , j ∈ J) be defined by s (j) = sign(β (j) ). One can always decompose α ∈ R N in the form α = X J * ρ + w where ρ ∈ R |J * | and w ∈ R N is perpendicular to the columns of X J * . From X T J * α = s J , we get ρ = (X T J * X J * ) -1 s J * . Letting α J * be the solution with w = 0, the question is therefore whether one can find w such that w T X (j) = 0,</p><formula xml:id="formula_571">j ∈ J * |α T J X (k) + w T X (k)| &lt; 1, k J *</formula><p>Denote for short Σ JJ ′ = X T J X J ′ . One can show that such a solution exists when the matrices Σ JJ are close to the identity as soon as |J| is small enough <ref type="bibr" target="#b65">[47]</ref>. More precisely, denote, for q ≤ d δ(q) = max</p><formula xml:id="formula_572">|J|≤q max(∥Σ JJ ∥, ∥Σ -1 JJ ∥ -1 ) -1,</formula><p>in which one uses the operator norm on matrices, and</p><formula xml:id="formula_573">θ(q, q ′ ) = max z T Σ T T ′ z ′ : |J|, |J ′ | ≤ q.J ∩ J ′ = ∅, |z| = |z ′ | = 1 .</formula><p>Then, the following proposition is true.</p><p>Proposition 7.7 (Candes-Tao) Let q = |J * | and s = (s (j) , j ∈ J * ) ∈ R q . Assume that δ(2q) + θ(q, 2q) &lt; 1. Then there exists α ∈ R N such that α T X (j) = s (j) for j ∈ J * and</p><formula xml:id="formula_574">|α T X (j)| ≤ θ(q, q) 1 -δ(2q) -θ(q, 2q) if j J * .</formula><p>So α has the desired property as soon as δ(2q) + θ(q, q) + θ(q, 2q) ≤ 1. to control subsets of variables of size less than 3q to obtain the conclusion, which is important, of course, when q is small compared to d.</p><p>Noisy case Consider now the noisy case. We here again introduce quantities that were pivotal for the lasso and LARS estimators, namely, the covariances between the variables and the residual error. So, we define, for a given β r (i)</p><formula xml:id="formula_575">β = X (i) T (Y -X β)</formula><p>which depends linearly on β. Then, the Dantzig selector is defined by the linear program: Minimize:</p><formula xml:id="formula_576">d j=1 |β (j) | subject to the constraint: max j=1,...,d |r (j) β | ≤ C.</formula><p>The explicit expression of this problem as a linear program is obtained as before by introducing slack variables ξ(j), ξ * (j), j = 1, . . . , d and minimizing</p><formula xml:id="formula_577">d j=1 ξ(j) + d j=1 ξ * (j) with constraints ξ(j), ξ * (j) ≥ 0, ξ ≥ β, ξ * ≥ -β, max j=1,...,d |r (j) β | ≤ C.</formula><p>Similar to the noise-free case, the Dantzig selector can identify sparse solutions (up to a small error) if the columns of X are nearly orthogonal, with the same type of conditions <ref type="bibr" target="#b66">[48]</ref>. Interestingly enough, the accuracy of this algorithm can be proved to be comparable to that of the lasso in the presence of a sparse solution <ref type="bibr" target="#b48">[30]</ref>.</p><p>7.4 Support Vector Machines for regression 7.4.1 Linear SVM Problem formulation We start by discussing support vector machines (SVM) <ref type="bibr" target="#b214">[196,</ref><ref type="bibr" target="#b215">197]</ref> with R X = R d equipped with the standard inner product (generally referred to as linear SVM) and will extend the theory to kernel methods in the next section. SVMs solve a linear regression problem, but replace the least-squares loss function by (y, y ′ ) → V (yy ′ ) with A plot of the function V is provided in fig. <ref type="figure">7</ref>.1. This function is an example of what is often called a robust loss function. The quadratic error used in linear regression had the advantage of providing closed form expressions for the solution, but is quite sensitive to outliers. For robustness, it is preferable to use loss functions that, like V , increase at most linearly at infinity. One sometimes choose them as smooth convex functions, for example V (t) = (1cos γt)/(1cos γϵ) for |t| &lt; ϵ and f (t) = |t| for t ≥ ϵ, where γ is chosen so that γϵ sin γϵ/(1cos γϵ) = 1. In such a case, minimizing</p><formula xml:id="formula_578">V (t) = 0 if |t| &lt; ϵ |t| -ϵ if |t| ≥ ϵ (7.10)</formula><formula xml:id="formula_579">F(β) = N k=1 V (y k -a 0 -x T k b)</formula><p>can be done using gradient descent methods. Using V in (7.10) will require a little more work, as we see now.</p><p>The SVM regression problem is generally formulated as the minimization of</p><formula xml:id="formula_580">N k=1 V (y k -a 0 -x T k b) + λ|b| 2 ,</formula><p>and we will study a slightly more general problem, minimizing</p><formula xml:id="formula_581">F(a 0 , b) = N k=1 V (y k -a 0 -x T k b) + λb T ∆b ,</formula><p>where ∆ is a symmetric positive-definite matrix. This objective function exhibits the following features:</p><p>• A penalty on the coefficients of b, similar to ridge regression.</p><p>• A linear penalty (instead of quadratic) for large errors in the prediction.</p><p>• An ϵ-tolerance for small errors, often referred to as the margin of the regression SVM.</p><p>We now describe the various steps in the analysis and reduction of the problem. They will lead to simple minimization algorithms, and possible extensions to nonlinear problems.</p><p>Reduction to a quadratic programming problem Introduce slack variables ξ k , ξ * k , k = 1, . . . , N . The original problem is equivalent to the minimization, with respect to (a 0 , b, ξ, ξ * ), of</p><formula xml:id="formula_582">N k=1 (ξ k + ξ * k ) + λb T ∆b under the constraints:            ξ k , ξ * k ≥ 0 ξ k -y k + a 0 + x T k b + ϵ ≥ 0 ξ * k + y k -a 0 -x T k b + ϵ ≥ 0 (7.11)</formula><p>The simple proof of this equivalence, which results in a quadratic programming problem, is left to the reader. As often, one gains additional insight by studying the dual problem.</p><p>Dual problem Introduce 4N non-negative Lagrange multipliers for the 4N constraints in the problem, namely, η k , η * k ≥ 0 for the positivity constraints, and α k , α * k ≥ 0 for the last two in <ref type="bibr">(7.11)</ref>. The resulting Lagrangian is</p><formula xml:id="formula_583">L(a 0 , b, ξ, ξ * , α, α * , η, η * ) = N k=1 (ξ k + ξ * k ) + λb T ∆b - N k=1 (η k ξ k + η * k ξ * k ) - N k=1 α k (ξ k -y k + a 0 + x T k b + ϵ) - N k=1 α * k (ξ * k + y k -a 0 -x T k b + ϵ).</formula><p>In this formulation, (a 0 , b, ξ, ξ * ) are the primal variables, and α, α * , η, η * the dual variables.</p><p>The KKT conditions are provided by the system:</p><formula xml:id="formula_584">                                               N k=1 (α k -α * k ) = 0 2λ∆b - N k=1 (α k -α * k )x k = 0 1 -η k -α k = 0 1 -η * k -α * k = 0 α k (ϵ + ξ k -y k + a 0 + x T k b) = 0 α * k (ϵ + ξ * k + y k -a 0 -x T k b) = 0 η k ξ k = η * k ξ * k = 0 (7.12)</formula><p>The first four equations are the derivatives of the Lagrangian with respect to a 0 , b, ξ k , ξ * k in this order and the last three are the complementary slackness conditions.</p><p>The dual problem maximizes the function</p><formula xml:id="formula_585">L * (α, α * , η, η * ) = inf β,ξ,ξ * L.</formula><p>under the previous positivity constraints. Since the Lagrangian is linear in a 0 , ξ k and ξ * k , its minimum is -∞ unless the coefficients vanish. The linear terms must therefore vanish for L * to be finite. With these conditions plus the fact that ∂ b L = 0, we retrieve the first four equations of system (7.12). Using</p><formula xml:id="formula_586">η k = 1 -α k , η * k = 1 -α * k and b = 1 2λ N k=1 (α k -α * k )∆ -1 x k (7.13)</formula><p>one can express L * uniquely as a function of α, α * , yielding</p><formula xml:id="formula_587">L * (α, α * ) = - 1 4λ N k,l=1 (α k -α * k )(α l -α * l )x T k ∆ -1 x l -ϵ N k=1 (α k + α * k ) + N k=1 (α k -α * k )y k .</formula><p>This quantity must be maximized subject to the constraints 0 ≤ α k , α * k ≤ 1 and</p><formula xml:id="formula_588">N k=1 (α k -α * k ) = 0.</formula><p>This still is a quadratic programming problem, but it now has nice additional features and interpretations.</p><p>Step 3: Analysis of the dual problem The dual problem only depends on the x k 's through the matrix with coefficients x T k ∆ -1 x l , which is the Gram matrix of x 1 , . . . , x N for the inner product associated with ∆ -1 . This property will lead to the the kernel version of SVMs discussed in the next section. The obtained predictor can also be expressed as a function of these products, since</p><formula xml:id="formula_589">y = a 0 + x T b = a 0 + 1 2λ N k=1 (α k -α * k )(x T k ∆ -1 x) .</formula><p>Moreover, the dimension of the dual problem is 2N , which allows the method to be used in large (possibly infinite) dimensions with a bounded cost.</p><p>We now analyze the solutions α, α * of the dual problem. The complementary slackness conditions reduce to:</p><formula xml:id="formula_590">           α k (ϵ + ξ k -y k + a 0 + x T k b) = 0 α * k (ϵ + ξ * k + y k -a 0 -x T k b) = 0 (1 -α k )ξ k = (1 -α * k )ξ * k = 0 (7.14)</formula><p>These conditions have the following consequences, based on the prediction error made for each training sample.</p><p>(i) First consider indexes k such that the error is strictly within the tolerance margin ϵ: |y ka 0x T k b| &lt; ϵ. Then the terms between parentheses in first two equations of (7.14) are strictly positive, which implies that α k = α * k = 0. The last two equations in <ref type="bibr">(7.14)</ref> then imply ξ k = ξ * k = 0. (ii) Consider now the case when the prediction is strictly less accurate than the tolerance margin. Assume that y ka 0x T k b &gt; ϵ. The second and third equations in <ref type="bibr">(7.14)</ref> </p><formula xml:id="formula_591">imply that α * k = ξ * k = 0.</formula><p>The assumption also implies that</p><formula xml:id="formula_592">ξ k = y k -a 0 -x T k b -ϵ &gt; 0 and α k = 1. The case y k -a 0 -x T k b &lt; -ϵ is symmetric and provides α k = ξ k = 0, ξ * k &gt; 0 and α * k = 1.</formula><p>(iii) Finally, consider samples for which the prediction error is exactly at the tolerance margin. If</p><formula xml:id="formula_593">y k -a 0 -x T k b = ϵ, we have α * k = ξ k = ξ * k = 0. The fact that α * k = ξ * k = 0 is clear.</formula><p>To prove that ξ k = 0, we note that would have otherwise ξ k -y k +a 0 +x T k b+ϵ &gt; 0, which would imply that α k = 0 and we reach a contradiction with <ref type="bibr">(</ref></p><formula xml:id="formula_594">1-α k )ξ k = 0. Sim- ilarly, y k -a 0 -x T k b = -ϵ implies that α k = ξ k = ξ * k = 0. The points for which |y k -a 0 -x T k b| = ϵ are called support vectors.</formula><p>One important information deriving from this discussion is that the variables (α k , α * k ) have prescribed values as long as the error y ka 0x T k b is not exactly ϵ in absolute value: (1, 0) if the error is larger than ϵ, (0, 0) if it is strictly between -ϵ and ϵ and (0, 1) if it is less than -ϵ. Also in all cases, at least one of α k and α * k must vanish. Only in the case of support vectors does the previous discussion fail to provide a value for one of these variables. Now, we want to reverse the discussion and assume that the dual problem is solved to see how the variables a 0 and b of the primal problem can be retrieved. For b, this is easy, thanks to (7.13). For a 0 a direct computation can be made if a support vector is identified, either because 0</p><formula xml:id="formula_595">&lt; α k &lt; 1, which implies that a 0 = y k -x T k b -ϵ, or because 0 &lt; α * k &lt; 1, which yields a 0 = y k -x T k b + ϵ.</formula><p>If no support vector can be identified, a 0 is not uniquely determined (note that the objective function is not strictly convex in a 0 ). However, the coefficients α k , α * k provide some information on this intercept, in the form of inequalities. More precisely, let</p><formula xml:id="formula_596">J + = {k : α k = 1}, J -= {k : α * k = 1} and J 0 = {k : α k = α * k = 0}. Then k ∈ J + implies that y k -a 0 -b T x k ≥ ϵ, so that a 0 ≤ y k -b T x k -ϵ. Similarly, k ∈ J -implies that a 0 ≥ y k -b T x k + ϵ. Finally, k ∈ J 0 implies that a 0 ≥ y k -b T x k -ϵ and a 0 ≤ y k -b T x k + ϵ.</formula><p>As a consequence, one can take a 0 to be any point in the interval [a 0 -, a 0 + ], where</p><formula xml:id="formula_597">a 0 -= max max k∈J -(y k -x T k b + ϵ), max k∈J 0 (y k -x T k b -ϵ) a 0 + = min min k∈J + (y k -x T k b -ϵ), min k∈J 0 (y k -x T k b + ϵ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">The kernel trick and SVMs</head><p>Returning to our feature space notation, let X take values in R X and h : R X → H be a feature function with values in an inner-product space H with associated kernel K. SVMs in feature space must minimize, with</p><formula xml:id="formula_598">a 0 ∈ R and b ∈ H F(a 0 , b) = N k=1 V y k -a 0 -⟨h(x k ) , b⟩ H + λ∥b∥ 2 H .</formula><p>Letting as before V = span(h(x 1 ), . . . , h(x N )), the same argument as that made for ridge regression works, namely that the first term in F is unchanged if b is replaced by π V (b) and the second one is strictly reduced unless b ∈ V , leading to a finitedimensional formulation in which</p><formula xml:id="formula_599">b = N k=1 c k h(x k )</formula><p>and one minimizes</p><formula xml:id="formula_600">F(a 0 , c) = N k=1 V y k -a 0 - N l=1 K(x k , x l )c l + λ N k,l=1 K(x k , x l )c k c l .</formula><p>This function has the same form as the one studied in the linear case with b replaced by c ∈ R N , x k replaced by the vector with coefficients K(x k , x l ), l = 1, . . . , N , that we will denote K (k) and</p><formula xml:id="formula_601">∆ = K = K(x 1 , . . . , x N ). Note that K (k) is the kth column of K, so that K (k) T K -1 K (l) = K(x k , x l ).</formula><p>Using this, we find that the dual problem requires to maximize</p><formula xml:id="formula_602">L * (α, α * ) = - 1 4λ N k,l=1 (α k -α * k )(α l -α * l )K(x k , x l ) -ϵ N k=1 (α k + α * k ) + N k=1 (α k -α * k )y k . with                  0 ≤ α k ≤ 1 0 ≤ α * k ≤ 1 N k=1 (α k -α * k ) = 0</formula><p>The associated vector c satisfies</p><formula xml:id="formula_603">2λc = N k=1 (α k -α * k )K -1 K (k) = α -α * .</formula><p>and the regression function is</p><formula xml:id="formula_604">f (x) = a 0 + ⟨b , h(x)⟩ H = a 0 + 1 2λ N k=1 (α k -α * k )K(x, x k ) .</formula><p>Finally, the discussions on the values of α, α * and on the computation of a 0 remain unchanged.</p><p>Chapter 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models for linear classification</head><p>In this chapter, Y is categorical and takes values in the finite set R Y = g 1 , . . . , g q . The goal is to predict this class variable from the input X, taking values in a set R X .</p><p>Using the same progression as in the regression case, we will first discuss basic linear methods, for which R X = R d before extending them, whenever possible, to kernel methods, for which R X can be arbitrary as soon as a feature space representation is available.</p><p>Classifiers will be based on a training set T = ((x 1 , y 1 ), . . . , (x N , y N )) with x k ∈ R X and y k ∈ R Y for k = 1, . . . , N . For g ∈ R Y , we will also let N g denote the number of samples in the training set such that y k = g, i.e.,</p><formula xml:id="formula_605">N g = {k : y k = g} = N k=1 1 y k =g .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Logistic regression 8.1.1 General Framework</head><p>Logistic regression uses the fact that, in order to apply Bayes's rule, only the conditional distribution of the class variables Y given X is needed, and trains a parametric model of this distribution. More precisely, if one denotes by p(g|x) the probability that Y = g conditional to X = x, logistic regression assumes that, for some parameters (a 0 (g), b(g), g ∈ R Y ) with a 0 (g) ∈ R and b(g</p><formula xml:id="formula_606">) ∈ R d , one has p = p a 0 ,b with log p a 0 ,b (g | x) = a 0 (g) + x T b(g) -log(C(a 0 , b, x)), where C(a 0 , b, x) = g∈R Y exp(a 0 (g) + x T b(g)).</formula><p>Introduce the functions, defined over mappings µ : R Y → R (which can be identified with vectors in R q )</p><formula xml:id="formula_607">F g (µ) = µ(g) -log g ′ ∈R Y e µ(g ′ ) . (8.1)</formula><p>With this notation, letting</p><formula xml:id="formula_608">β(g) = a 0 (g) b(g) and x = 1 x , one has log p β (g|x) = F g (β T x),</formula><p>where</p><formula xml:id="formula_609">β T x is the function (g ′ → β(g ′ ) T x).</formula><p>For any constant function (g → µ 0 ∈ R) one has</p><formula xml:id="formula_610">F g (µ + µ 0 ) = µ(g) + µ 0 -log g ′ ∈R Y e µ(g ′ )+µ 0 = µ(g) + µ 0 -µ 0 -log g ′ ∈R Y e µ(g ′ ) = F g (µ).</formula><p>As a consequence, if one replaces, for all g, β(g) by β(g</p><formula xml:id="formula_611">) = β(g) + γ, with γ ∈ R d+1 , then βT x = β T x + γ T x and log p β (g | x) = log p β (g | x).</formula><p>This shows that the model is over-parametrized. One therefore needs a (d + 1)dimensional constraint to ensure uniqueness, and we will enforce a linear constraint in the form</p><formula xml:id="formula_612">g∈R Y ρ g β(g) = c</formula><p>with g ρ g 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Conditional log-likelihood</head><p>The conditional log-likelihood computed from the training set is:</p><formula xml:id="formula_613">ℓ(β) = N k=1 log p β (y k | x k ) .</formula><p>Logistic regression computes a maximizer β of this log-likelihood. The classification rule given a new input x then chooses the class g for which p β (g | x) is largest, or, equivalently, the class g that maximizes xT β(g).</p><formula xml:id="formula_614">Proposition 8.1 Let mg = k:y g =k x k /N g . The conditional log-likelihood ℓ is concave with first derivative ∂ β(g) ℓ = N g mT g - N k=1 xT k p β (g|x k ) (8.2)</formula><p>and negative semi-definite second derivative</p><formula xml:id="formula_615">∂ β(g) ∂ β(g ′ ) ℓ = -1 [g=g ′ ] N k=1 xk xT k p β (g|x k ) + N k=1 xk xT k p β (g|x k )p β (g ′ |x k ) . (8.3) Remark 8.2</formula><p>In this discussion, we consider ℓ as a function defined over collections (β(g), g ∈ R Y ), or, if one prefers, on the q(d + 1)-dimensional linear space, F , of functions β : R Y → R q+1 . With this in mind, the differential dℓ(β) is a linear form from F to R, therefore associating to any family u = (u(g), g ∈ R Y ), the expression</p><formula xml:id="formula_616">dℓ(β) u = g∈R Y ∂ β(g) ℓ u(g).</formula><p>Similarly, the second derivative is the bilinear form</p><formula xml:id="formula_617">d 2 ℓ(β)(u, u ′ ) = g,g ′ ∈R Y u(g) T ∂ β(g) ∂ β(g ′ ) ℓ u(g ′ ).</formula><p>The last statement in the proposition expresses the fact that</p><formula xml:id="formula_618">d 2 ℓ(β)(u, u) ≤ 0 for all u ∈ F . ♦ Proof First consider the function F g in (8.1), so that ℓ(β) = N k=1 F y k (β T xk ).</formula><p>We have, for ζ : R Y → R,</p><formula xml:id="formula_619">dF g (µ)ζ = ζ(g) - g ′ ∈R Y e µ(g ′ ) ζ(g ′ ) g ′ ∈R Y e µ(g ′ )</formula><p>as can be easily computed by evaluating the derivative of F(µ + ϵu) at ϵ = 0. Introducing the notation</p><formula xml:id="formula_620">q µ (g) = e µ(g) g∈R Y e µ(g) and ⟨ζ⟩ µ = g∈R Y ζ(g)q µ (g), we have dF g (µ)ζ = ζ(g) -⟨ζ⟩ µ .</formula><p>Evaluating the derivative of dF g (µ + ϵu ′ )(ζ) at ϵ = 0, one gets (the computation being left to the reader)</p><formula xml:id="formula_621">d 2 F g (µ)(ζ, ζ ′ ) = -⟨ζζ ′ ⟩ µ + ⟨ζ⟩ µ ⟨ζ ′ ⟩ µ . (8.4) Note that -d 2 F g (µ)(ζ, ζ)</formula><p>is the variance of ζ for the probability mass function q µ and is therefore non-negative (so that F µ is concave). This immediately shows that ℓ is concave as a sum of concave functions.</p><p>Using the chain rule, we have, for u :</p><formula xml:id="formula_622">R Y → R q , dℓ(β)u = N k=1 dF y k (β T xk ) xT k u(•) = N k=1 xT k u(y k ) - N k=1 ⟨ xT k u(•)⟩ β T xk .</formula><p>Reordering the first sum in the right-hand side according to the values of y k gives</p><formula xml:id="formula_623">N k=1 u(y k ) T xk = g∈R Y N g u(g) T mg .</formula><p>Noting that q β T x = p β (•|x), we find</p><formula xml:id="formula_624">dℓ(β)(u) = g∈R Y N g mT g u(g) - g∈R Y xT k u(g)p β (g|x k ), yielding<label>(8.2)</label></formula><p>. Applying the chain rule again, we have</p><formula xml:id="formula_625">d 2 ℓ(β)(u, u ′ ) = N k=1 d 2 F y k (β T xk )( xT k u(•), xT k u ′ (•))<label>(8.5)</label></formula><p>with</p><formula xml:id="formula_626">d 2 F y k (β T xk )( xT k u(•), xT k u ′ (•)) = -⟨u(•) T xk xT k u ′ (•)⟩ β T xk + ⟨ xT k u(•)⟩ β T xk ⟨ xT k u ′ (•)⟩ β T xk = - g∈R Y u(g) T xk xT k u ′ (g)p β (g|x k ) + g,g ′ ∈R Y u(g) T xk xT k u ′ (g ′ )p β (g|x k )p β (g ′ |x k ) from which (8.3) follows. ■ Remark 8.3 From F g (µ + µ 0 ) = F g (µ) when µ 0 is constant on R Y , one deduces (taking the derivative at µ 0 = 0) that dF g (µ)1 = 0 for all µ, where 1 denotes the constant function equal to 1 on R Y . For h ∈ R d+1 , let c h denote the constant function c h (g) = h, g ∈ R Y . We have dℓ(β) c h = N k=1 dF y k (β T xk ) xT k c h = N k=1 dF y k (β T xk )( xT k h1) = N k=1 ( xT k h)dF y k (β T xk )1 = 0.</formula><p>Taking one extra derivative we see that</p><formula xml:id="formula_627">dℓ(β)(c h , u) = 0</formula><p>for all functions u : R Y → R q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>♦</head><p>We now discuss whether there are other elements in the null space of the second derivative of ℓ. We will use notation introduced in the proof of proposition 8.1. From (8.4), we have d 2 F g (µ)(ζ, ζ) = 0 if and only if the variance of ζ for q µ vanishes, which, since q µ &gt; 0, is equivalent to ζ being constant. So, the null space of d 2 F g (µ) is one-dimensional, and composed of scalar multiples of 1. Using (8.5), we see that</p><formula xml:id="formula_628">d 2 ℓ(u, u) = 0 if and only if , for all k = 1, . . . , N , (g → xT k u(g)) is a constant function.</formula><p>Assume that this is true. Then, letting ū =<ref type="foot" target="#foot_8">foot_8</ref> q g∈R Y u(g), one has, for all g ∈ R Y and k = 1, . . . , N , xT k u(g) = xT k ū so that u(g) -ū is in the null space of the matrix X . This leads to the following proposition.</p><p>Proposition 8.4 Assume that X has rank d + 1. Then the null space of d 2 ℓ(β) is the set of all vectors u = c h for h ∈ R d+1 . In particular, for any c ∈ R d+1 , the function ℓ restricted to the space</p><formula xml:id="formula_629">M =          β : g∈R Y ρ g β(g) = c          is strictly concave as soon as the scalar coefficients (ρ g , g ∈ R Y ) are such that g∈R Y ρ g 0.</formula><p>Proof From the discussion before the proposition, u ∈ Null(d 2 ℓ) implies that X (u(g)ū) = 0 for all g, and since we assume that X has rand d + 1, this requires that u(g) = ū for all g, i.e., u = c ū . This proves the first point.</p><p>If one restricts ℓ to M, then we must restrict d 2 ℓ(β) to those u's such that g∈R Y ρ g u(g) = 0. But if d 2 ℓ(β)(u, u) = 0 for such an u, then u = c ū and</p><formula xml:id="formula_630">g∈R Y ρ g u(g) = g∈R Y ρ g ū.</formula><p>Since we assume that g∈R Y ρ g 0, this requires ū = 0, and therefore u = 0. This shows that the second derivative of the restriction of ℓ to M is negative definite, so this restriction is strictly concave. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Training algorithm</head><p>Given that we have expressed the first and second derivatives of ℓ in closed form 1 , we can use Newton-Raphson gradient ascent to maximize ℓ over the affine space:</p><formula xml:id="formula_631">M =          β : g∈R Y ρ g β(g) = c         </formula><p>with g∈R Y ρ g 0. We assume in the following that the matrix X has rand d + 1 so that proposition 8.4 applies. Since the constraint is affine, it is easy to express one of the parameters β(g) as a function of the others and solve the strictly concave problem as a function of the remaining variables. It is not much harder, and arguably more elegant to solve the problem without breaking its symmetry with respect to the class indexes, as described below.</p><p>Let</p><formula xml:id="formula_632">M 0 =          β : g∈R Y ρ g β(g) = 0          .</formula><p>We still have the second order expansion</p><formula xml:id="formula_633">ℓ(β + u) = ℓ(β) + dℓ(β)u + 1 2 d 2 ℓ(β)(u, u) + o(|u| 2 )</formula><p>and we consider the maximization of the first three terms, simply restricting to vectors u ∈ M 0 . To allow for matrix computation, we use our ordering R Y = (g 1 , . . . , g q ) and identify a with the column vector</p><formula xml:id="formula_634">          u(g 1 ) . . . u(g q )           ∈ R q(d+1)</formula><p>Similarly, we let</p><formula xml:id="formula_635">∇ℓ(β) =           ∂ β(g 1 ) ℓ . . . ∂ β(g q ) ℓ          </formula><p>and let ∇ 2 (ℓ)(β) be the block matrix with i, j block given by ∂ β(g i ) ∂ β(g j ) ℓ(β). We let ρ be the (d + 1) × q(d + 1) row block matrix</p><formula xml:id="formula_636">ρ(g 1 )Id R d+1 • • • ρ(g q )Id R d+1 so that u ∈ M 0 is just ρu = 0 in vector notation. Given this we have ℓ(β + u) = ℓ(β) + ∇ℓ(β) T u + 1 2 u T ∇ 2 (ℓ)(β)u + o(|u| 2 ). The maximum of ℓ(β) + u T ∇ℓ(β) + 1 2 u T ∇ 2 (ℓ)(β)u subject to ρu = 0 is a stationary point of the Lagrangian L = ℓ(β) + u T ∇ℓ(β) + 1 2 u T ∇ 2 (ℓ)(β)u + λ T ρu</formula><p>for some λ ∈ R d+1 and is characterized by</p><formula xml:id="formula_637">∇ 2 (ℓ)(β)u + ∇ℓ(β) + ρT λ = 0 ρu = 0</formula><p>This shows that the Newton-Raphson iterations can be implemented as</p><formula xml:id="formula_638">β n+1 = β n -ϵ n+1 u n+1 (8.6) with u n+1 λ = ∇ 2 (ℓ)(β n ) ρT ρ 0 -1 ∇ℓ(β n ) 0 . (8.7)</formula><p>We summarize this discussion in the following algorithm.</p><p>Algorithm 8.1 (Logistic regression with Newton's gradient ascent)</p><formula xml:id="formula_639">(1) Input: (i) training data (x 1 , y 1 , . . . , x N , y N ) with x i ∈ R d and y i ∈ R Y ; (ii) coefficients ρ g , g ∈ R Y</formula><p>with non-zero sum and target value c ∈ R; (iii) algorithm step ϵ small enough.</p><p>(2) Initialize the algorithm with β 0 such that g ρ g β 0 (g) = c.</p><p>(3) At iteration n, compute ∇ℓ(β n ) and ∇ 2 (ℓ)(β n ) as provided by proposition 8.1.</p><p>(4) Update β n using (8.6) and (8.7), with ϵ n+1 = ϵ. Alternatively, optimize ϵ n+1 using a line search.</p><p>(5) Stop the procedure if the change in the parameter is below a small tolerance level. Otherwise, return to step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.4">Penalized Logistic Regression</head><p>Logistic regression can be combined with a penalty term, e.g., maximizing</p><formula xml:id="formula_640">ℓ 2 (β) = ℓ(β) -λ d i=1 |b (i) | 2 (8.8) or ℓ 1 (β) = ℓ(β) -λ d i=1 |b (i) | (8.9)</formula><p>where b (i) is the q-dimensional vector formed with the ith coefficients of b(g) for g ∈ R Y . Similarly to penalized regression, one generally normalizes the x variables to have unit standard deviation before applying the method.</p><p>Maximization with the ℓ 2 norm The problem in <ref type="bibr">(8.8)</ref> relates to ridge regression and can be solved using a Newton-Raphson method (Algorithm 8.1) with minor changes. More precisely, letting</p><formula xml:id="formula_641">∆ = 0 0 0 Id R d we have, considering β as a d + 1 by q matrix, ℓ 2 (β) = ℓ(β) -λtrace(β T ∆β) and dℓ 2 (β)u = dℓ(β)u -2λtrace(β T ∆u), d 2 ℓ 2 (β)(u, u ′ ) = dℓ(β)(u, u ′ ) -2λtrace(u T ∆u ′ ).</formula><p>In addition, when λ &gt; 0, the problem is over-parametrized only up to the addition of a constant to (g → a 0 (g)), so that one only needs a single constraint g ρ g a 0 (g) = 0 and the Lagrange coefficient in (8.7) is one dimensional.</p><p>Maximization with the ℓ 1 norm The maximization in (8.9) can be run using proximal gradient ascent (section 3.5.5), writing the objective function in the form</p><formula xml:id="formula_642">ℓ 1 (a 0 , b) = ℓ(a 0 , b) -λγ(a 0 , b) with γ(a 0 , b) = d i=1 g∈R Y b (i) (g) 2 .</formula><p>Here, ℓ is concave and γ is convex and the proximal gradient iterations are</p><formula xml:id="formula_643">β n+1 = prox ϵλγ (β n + ϵ∇ℓ(β n )) (8.10)</formula><p>where ∇ℓ is the gradient of ℓ projected on the set of functions u :</p><formula xml:id="formula_644">R Y → R d+1 satis- fying g∈R Y ρ g u (0) (g) = 0</formula><p>where u (0) (g) is the first coordinate of u(g). This projection can be computed by subtracting</p><formula xml:id="formula_645">g ′ ∈R Y ρ(g ′ )∂ a 0 (g ′ ) ℓ g ′ ∈R Y ρ(g ′ ) 2 ρ(g)</formula><p>to ∂ a 0 (g) ℓ. This algorithm will converge for small enough ϵ.</p><p>We already know the gradient of ℓ, so it only remains to determine the proximal operator of γ to make (8.10) explicit. Let us denote the coordinates of a function u : R Y → R d+1 as u (i) (g) for i = 0, . . . , d and g ∈ R Y .</p><formula xml:id="formula_646">prox ϵλγ (u) = argmin ũ       γ( ũ) + 1 2λϵ d i=0 g∈R Y (u (i) (g) -ũ(i) (g)) 2       .</formula><p>Since γ does not depend on ũ(0) (•), the optimal ũ(0</p><formula xml:id="formula_647">) ( ˙) is ũ(0) (•) = u (0) (•). One can optimize separately each group ( ũ(i) (g), g ∈ R Y ), which must minimize g∈R Y ũ(i) (g) 2 + 1 2λϵ g∈R Y (u (i) (g) -ũ(i) (g)) 2 .</formula><p>The function t → √ t being differentiable everywhere except at 0, we first search for a solution for which at least one ũ(i) (g) does not vanish. If such a solution exists, it must satisfy, for all</p><formula xml:id="formula_648">g ∈ R Y ũ(i) (g) g ′ ∈R Y ũ(i) (g ′ ) 2 + 1 λϵ ( ũ(i) (g) -u (i) (g)) = 0 Letting | ũ(i) (•)| = g∈R Y ũ(i) (g) 2 we get ũ(i) (•)(| ũ(i) (•)| + λϵ) = u (i) (•)| ũ(i) (•)|</formula><p>Taking the norm on both sides and dividing by | ũ(i, •)| (which is assumed not to vanish) yields</p><formula xml:id="formula_649">| ũ(i) (•)| + λϵ = |u (i) (•)|,</formula><p>which has a positive solution only if |u (i) (•)| &gt; λϵ, and gives in that case</p><formula xml:id="formula_650">ũ(i) (•) = |u (i) (•)| -ϵλ |u (i) (•)| u (i) (•) If |u (i) (•)| ≤ λϵ, then we must take ũ(i) (•) = 0.</formula><p>We have therefore obtained:</p><formula xml:id="formula_651">prox ϵλg (a) = ũ with ũ(0) (•) = u (0) (•) and ũ(i) (•) = max |u (i) (•)| -ϵλ |u (i) (•)| , 0 u (i) (•)</formula><p>for i ≥ 1. We summarize this discussion in the next algorithm. (2) Initialize the algorithm with β 0 = (a 00 , b 0 ) such that g ρ g a 00 (g) = c.</p><p>(</p><formula xml:id="formula_652">) At iteration n, compute u = β n + ϵ∇ℓ(β n ), with β n = (a 0,n , b n ). (4) Let a n+1,0 (•) = u (0) (•) and for i ≥ 1, b (i) n+1 (•) = max |u (i) (•)| -ϵλ |u (i) (•)| , 0 u (i) (•)<label>3</label></formula><p>(5) Stop the procedure if the change in the parameter is below a small tolerance level. Otherwise, return to step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.5">Kernel logistic regression</head><p>Let h : R X → H be a feature function with values in a Hilbert space</p><formula xml:id="formula_653">H with K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H .</formula><p>The kernel version of logistic regression uses the model:</p><formula xml:id="formula_654">log p a 0 ,b (g | x) = a 0 (g) + ⟨h(x) , b(g)⟩ H -log g∈R Y exp(a 0 ( g) + ⟨h(x) , b( g)⟩ H ) with b(g) ∈ H for g ∈ R Y .</formula><p>Using the usual kernel argument, one sees that, when maximizing the log-likelihood, there is no loss of generality is assuming that each b(g) belongs to V = span(h(x 1 ), . . . , h(x N )). Taking</p><formula xml:id="formula_655">b(g) = N k=1 α k (g)h(x k ), we have log p α (g | x) = a 0 (g) + N k=1 α k (g)K(x, x k ) -log         g∈R Y exp(a 0 ( g) + N k=1 α k ( g)K(x, x k ))         .</formula><p>To avoid overfitting, one must include a penalty term in the likelihood, and (in order to take advantage of the kernel), one can take this term proportional to g ∥b(g)∥ 2</p><p>H . The complete learning procedure then requires to maximize the concave penalized likelihood</p><formula xml:id="formula_656">ℓ(α) = N k=1 log p α (y k | x k ) -λ g∈R Y N k,l=1 α k (g)α l (g)K(x k , x l ).</formula><p>The computation of the first and second derivatives of this function is similar to that for the original version, and we skip the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Linear Discriminant analysis 8.2.1 Generative model in classification and LDA</head><p>Generative model In classification, the class variable Y generally has a causal role upon which the variable X is produced. Prediction can therefore be seen as an inverse problem where the cause is deduced from the result. In terms of generative modeling, one should therefore model the distribution of Y , followed by the the conditional distribution of X given Y .</p><p>Taking R X = R d , denote by f g the conditional p.d.f. of X given Y = g and let π g = P (Y = g). The Bayes estimator for the 0-1 loss maximizes the posterior probability</p><formula xml:id="formula_657">P(Y = g | X = x) = π g f g (x) g ′ ∈R Y π g ′ f g ′ (x)</formula><p>. Since the denominator does not depend on g the Bayes estimator equivalently maximizes (taking logarithms) log f g (x) + log π g .</p><p>One generally speaks of a linear classification method when the prediction is based on the maximization in g of a function U (g, x) where U is affine in x. In this sense, logistic regression is linear, and kernel logistic regression is linear in feature space. For the generative approach, this occurs when one uses the following model, which provides the generative form of linear discriminant analysis (LDA). Assume that the distributions f g are all Gaussian with mean m g and common variance S, so that <ref type="bibr">.11)</ref> In this case, the optimal predictor must maximize (in g)</p><formula xml:id="formula_658">f g (x) = 1 (2π) d det Σ e -1 2 (x-m g ) T S -1 (x-m g ) . (<label>8</label></formula><formula xml:id="formula_659">- 1 2 (x -m g ) T S -1 (x -m g ) + log π g . Introduce m = E(X) = g∈R Y π g m g .</formula><p>Then the optimal classifier must maximize</p><formula xml:id="formula_660">- 1 2 (x -m) T S -1 (x -m) + (x -m) T S -1 (m g -m) - 1 2 (m g -m) T S -1 (m g -m) + log π g .</formula><p>Since the first term does not depend on g, it is equivalent to maximize</p><formula xml:id="formula_661">(x -m) T S -1 (m g -m) - 1 2 (m g -m) T S -1 (m g -m) + log π g (8.12)</formula><p>with respect to the class g, which provides an affine function of x.</p><p>Training Training for LDA simply consists in estimating the class means and common variance in <ref type="bibr">(8.11)</ref> from data. We introduce some notation for this purpose (this notation will be reused through the rest of this chapter).</p><p>Recall that N g , g ∈ R Y denotes the number of samples with class g in the training set T = (x 1 , y 1 , . . . , x N , y N ). We let c g = N g /N and C be the diagonal matrix with diagonal coefficients c g 1 , . . . , c g q . We also let ζ ∈ R q denote the vector with the same coordinates. For g ∈ R Y , µ g denotes the class average</p><formula xml:id="formula_662">µ g = 1 N g n k=1</formula><p>x k 1 y k =g and µ the global average</p><formula xml:id="formula_663">µ = 1 N N k=1 x k = g∈R Y c g µ g .</formula><p>Let Σ g denote the sample covariance matrix in class g, defined by</p><formula xml:id="formula_664">Σ g = 1 N g N k=1 (x k -µ g )(x k -µ g ) T 1 y k =g ,</formula><p>and Σ w the pooled class covariance (also called within-class covariance) defined by</p><formula xml:id="formula_665">Σ w = 1 N N k=1 (x k -µ y k )(x k -µ y k ) T = g∈R Y c g Σ g .</formula><p>Let, in addition, Σ b denotes the "between-class" covariance matrix, given by</p><formula xml:id="formula_666">Σ b = g∈R Y c g (µ g -µ)(µ g -µ) T</formula><p>The global covariance matrix, given by,</p><formula xml:id="formula_667">Σ XX = 1 N N k=1 (x k -µ)(x k -µ) T satisfies Σ XX = Σ w + Σ b . This identity is proved by noting that, for any g ∈ R Y , 1 N g N k=1 (x k -µ)(x k -µ) T 1 y k =g = Σ g + (µ g -µ)(µ g -µ) T .</formula><p>We will finally denote by M the matrix</p><formula xml:id="formula_668">M =            (µ g 1 -µ) T . . . (µ g q -µ) T            . Note that Σ b = M T CM.</formula><p>Given this notation, one can in particular take mg = µ g , m = µ and Ŝ = Σ w in (8.12). The class probabilities, π g , can be deduced from the normalized frequencies of y 1 , . . . , y N . However, in many applications, one prefers to simply fix π g = 1/q, in order to balance the importance of each class. Remark 8.5 If one relaxes the assumption of common class variances, one needs to use Σ g in place of Σ w for class g. The decision boundaries are not linear in this case, but provided by quadratic equations (and the resulting method if often called quadratic discriminant analysis, or QDA). QDA requires the estimation of qd(d + 3)/2 coefficients, which may be overly ambitious when the sample size is not large compared to the dimension, in which case QDA is prone to overfitting. (Even LDA, which involves qd +d(d +1)/2 parameters, may be unrealistic in some cases.) We also note a variant of QDA that uses class covariance matrices given by Σg = αΣ w + (1α)Σ g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Dimension reduction</head><p>One of the interests of LDA is that it can be combined with a rank reduction procedure. LDA with q classes can always be seen as a (q -1)-dimensional problem after suitable projection on a data-dependent affine space. Recall that the classification rule after training requires to maximize w.r.t. g ∈ R Y the function</p><formula xml:id="formula_669">(x -µ) T Σ -1 w (µ g -µ) - 1 2 (µ g -µ) T Σ -1 w (µ g -µ) + log π g . Define the "spherized" data 2 by xk = Σ -1/2 w (x k -µ), where Σ 1/2 w is the positive sym- metric square root of Σ w . Also let μg = Σ -1/2 w (µ g -µ).</formula><p>With this notation, the predictor chooses the class g that maximizes</p><formula xml:id="formula_670">xT μg - 1 2 | μg | 2 + log π g with x = Σ -1/2 w (x -μ). Now, let V = span{ μg , g ∈ R Y }.</formula><p>Since g c g μg = 0, this space is at most (q -1)dimensional. Let P V denote the orthogonal projection on V . We have xT z = (P V x) T z for any z ∈ V and x ∈ R d .</p><p>The classification rule can then be replaced by maximizing</p><formula xml:id="formula_671">(P V x) T μg - 1 2 | μg | 2 + log π g with x = Σ -1/2 w (x -μ). Recall that M =            (µ g 1 -µ) T . . . (µ g q -µ) T            and let M =            μT g 1 . . . μT g q           </formula><p>. The dimension, denoted r, of V is equal to the rank of M. Let ( ẽ1 , . . . , ẽr ) be an orthonormal basis of V . One has</p><formula xml:id="formula_672">P V x = r j=1 ( xT ẽj ) ẽj .</formula><p>Given an input x, one must therefore compute the "scores" γ j (x) = xT ẽj and maximize</p><formula xml:id="formula_673">r j=1 γ j (x)γ j (µ g ) - 1 2 r j=1 γ j (µ g ) 2 + log π g .</formula><p>The following proposition is key to the practical implementation of LDA with dimension reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 8.6 An orthonormal basis of</head><formula xml:id="formula_674">V = span( μg , g ∈ CG) is provided by the the first r eigenvectors of M T C M associated with eigenvalues λ 1 ≥ • • • ≥ λ r &gt; 0 (all other eigenvalues being zero). Proof Indeed, if x is perpendicular to V , we have MT C M x = g∈R Y c g ( μT g x) μg = 0 so that V ⊥ ⊂ Null( MT C M)</formula><p>, and both spaces coincide because they have the same dimension (dr). This shows that</p><formula xml:id="formula_675">V = Null( M T C M) ⊥ = Range( MT C M). Since M T C M is symmetric, Null( M T C M)</formula><p>⊥ is generated by eigenvectors with non-zero eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Returning to the original variables, we have M = MΣ -1/2 w and M T CM = Σ b , the between class covariance matrix. This implies that</p><formula xml:id="formula_676">M T C M = Σ -1/2 w Σ b Σ -1/2</formula><p>w and each eigenvector ẽj therefore satisfies</p><formula xml:id="formula_677">Σ b Σ -1/2 w ẽj = λ j Σ 1/2 w ẽj = λ j Σ w (Σ -1/2 w ẽj ).</formula><p>Therefore, letting e j = Σ -1/2 w ẽj , (e 1 , . . . , e r ) are the solutions of the generalized eigenvalue problem Σ b e = λΣ w e that are associated with non-zero eigenvalues (they are however normalized so that e T j Σ w e j = 1). Moreover, the scores are given by</p><formula xml:id="formula_678">γ j (x) = xT ẽj = (x -µ) T Σ -1/2 w ẽj = (x -µ) T e j</formula><p>and can therefore be computed directly from the original data and the vectors e 1 , . . . , e r . An example of training data and its representation in the LDA space (associated with the scores) in provided in fig. <ref type="figure" target="#fig_135">8</ref>.1.</p><p>We can now describe the LDA learning algorithm with dimension reduction.</p><formula xml:id="formula_679">Algorithm 8.3 (LDA with dimension reduction) 1. Compute µ g , g ∈ R Y , Σ w and Σ b from training data. 2. Estimate (if needed) π g , g ∈ R Y</formula><p>3. Solve the generalized eigenvalue problem Σ b e = λΣ w e. Let e 1 , . . . , e r be the eigenvectors associated with non-zero eigenvalues, normalized so that e T j Σ w e j = 1. 4. Choose a reduced dimension r 0 ≤ r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Precompute mean scores</head><formula xml:id="formula_680">γ j (µ g ) = (µ g -µ) T e j , g ∈ R Y , j = 1, . . . , r 0 .</formula><p>6. To classify a new example x, compute γ j (x) = (xµ) T e j and choose the class that maximizes</p><formula xml:id="formula_681">r 0 j=1 γ j (x)γ j (µ g ) - 1 2 r 0 j=1 γ j (µ g ) 2 + log π g .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.3">Fisher's LDA</head><p>This characterization leads to the discriminative interpretation of LDA, also called Fisher's LDA. Indeed, the generalized eigenvalue problem Σ b e = λΣ w e is directly related to the maximization of the ratio e T Σ b e subject to e T Σ w e = 1, which provides directions that have a large between-class variance for within class variance equal to 1. More precisely, e 1 is the direction that achieves the maximum; e 2 is the second best direction, constrained to being perpendicular to e 1 , and so on until e r which is the optimal constrained to be perpendicular to (e 1 , . . . , e r-1 ). We are therefore looking for directions that have the largest ratio of between-class variance to within-class variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.4">Kernel LDA</head><p>Mean and covariance in feature space We assume the usual construction where h :</p><formula xml:id="formula_682">R X → H is a feature function, H a Hilbert space with kernel K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H . (</formula><p>The assumption that H is a complete space is here required for the expectations below to be meaningful.)</p><p>We now discuss the kernel version of LDA by plugging the feature space representation directly in the classification rule. So, consider h : R → H. Let X : Ω → R be a random variable such that E(∥h(X)∥ 2 H ) &lt; ∞. Then, its mean feature m = E(h(X)) is well defined as an element of H , and so are the class averages,</p><formula xml:id="formula_683">m g = E(h(X) | Y = g).</formula><p>In this possibly infinite-dimensional setting, the covariance "matrix" is defined as a linear operator S : H → H such that, for all ξ, η ∈ H: <ref type="bibr">(8.13)</ref> which is equivalent to defining</p><formula xml:id="formula_684">⟨ξ , Sη⟩ H = E ⟨h(X) -m , ξ⟩ H ⟨h(X) -m , η⟩ H ,</formula><formula xml:id="formula_685">Sη = E(⟨h(X) -m , η⟩ H (h(X) -m))</formula><p>for η ∈ H. This definition generalizes the identity for a random variable U : Ω → R d :</p><formula xml:id="formula_686">S U w = E((U -E(U ))(U -E(U )) T )w = E(((U -E(U )) T w) (U -E(U )))</formula><p>One can similarly define the covariance matrix in class g, S g , by conditioning the right-hand side in <ref type="bibr">(8.13</ref>) by Y = g and replacing m by m g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDA in feature space</head><p>Following the LDA model, we assume that the operators S g are all equal to a fixed operator, the within-class covariance operator denoted S.</p><p>Assuming that S is invertible, one can generalize the LDA classification rule to data represented in feature space by classifying a new input x in class g when</p><formula xml:id="formula_687">⟨h(x) -m , S -1 (m g -m)⟩ H - 1 2 ⟨m g -m , S -1 (m g -m)⟩ H + log π g (8.14)</formula><p>is maximal over all classes. Notice that this is a transcription of the finite-dimensional Bayes rule, but cannot be derived from a generative model, because the assumption that h(X) is Gaussian is not valid in general. (It would require that h takes values in a d-dimensional linear space, which would eliminate all interesting kernel representations.)</p><p>Let, as before, T = (x 1 , y 1 , . . . , x N , y N ) be the training set, N g denote the number of examples in class g and c g = N g /N . When h is known (which, we recall, is not a practical assumption, but we will fix this later), one can estimate the class averages from training data by</p><formula xml:id="formula_688">µ g = 1 N g N k=1 h(x k )1 y k =g</formula><p>and the within-class covariance operator by</p><formula xml:id="formula_689">⟨ξ , Σ w η⟩ H = 1 N N k=1 ⟨h(x k ) -µ y k , ξ⟩ H ⟨h(x k ) -µ y k , η⟩ H .</formula><p>Unfortunately, the resulting variance estimator cannot be directly used in <ref type="bibr">(8.14)</ref>, because it is not invertible if dim(H) &gt; N . Indeed, one has Σ w η = 0 as soon as η is perpendicular to V ∆ = span(h(x 1 ), . . . , h(x N )).</p><p>One way to address the degeneracy of the estimated covariance operator is to add to Σ w a small multiple of the identity, say ρId H , <ref type="foot" target="#foot_10">3</ref> and let the classification rule maximize in g:</p><formula xml:id="formula_690">⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H - 1 2 ⟨µ g -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H + log π g . (8.15)</formula><p>where µ is the average of h(x 1 ), . . . , h(x N ). Taking this option, we still need to make this expression computable and remove the dependency in the feature function h.</p><p>Reduction We have µ g ∈ V for all g ∈ R Y and, since</p><formula xml:id="formula_691">Σ w η = 1 N N k=1 ⟨h(x k ) -µ y k , η⟩ H (h(x k ) -µ y k ),</formula><p>this operator maps H to V , which implies that Σ w + ρId H maps V into itself. Moreover, this mapping is onto: If v ∈ V and u = (Σ w + ρId H ) -1 v, then, u ∈ V . Indeed, for any z ⊥ V , we have ⟨z , Σ w u + ρu⟩ H = ⟨z , v⟩ H . We have ⟨z , Σ w u⟩ H = 0 (because Σ w maps H to V ) and ⟨z , v⟩ H = 0 (because v ∈ V ), so that we can conclude that ⟨z , u⟩ H = 0. Since this is true for all z ⊥ V , this requires that u ∈ V . 4   We now express the classification rule in (8.15) as a function of the kernel associated with the feature-space representation. Denote, for any vector u ∈ R N ,</p><formula xml:id="formula_692">ξ(u) = N k=1 u (k) h(x k ), therefore defining a mapping ξ from R N onto V . Letting as usual K = K(x 1 , . . . , x N )</formula><p>be the matrix formed by pairwise evaluations of K on training inputs, we have the identity</p><formula xml:id="formula_693">⟨ξ(u) , ξ(u ′ )⟩ H = u T Ku ′ .</formula><p>for all u, u ′ ∈ R N . For simplicity, we will assume in the rest of the discussion that K is invertible.</p><p>We have µ g = ξ(1 g /N g ), where 1 g ∈ R N is the vector with kth coordinate equal to 1 if y k = g and 0 otherwise. Also µ = ξ(1/N ) (recall that 1 is the vector with all coordinates equal to 1).</p><p>For u ∈ R N , we want to characterize v ∈ R N such that Σ w ξ(u) = ξ(v). Let δ k denote the vector with 1 at the kth entry and 0 otherwise. We have</p><formula xml:id="formula_694">Σ w ξ(u) = 1 N N k=1 ⟨ξ(u) , h(x k ) -µ y k ⟩ H (h(x k ) -µ y k ) = 1 N N k=1 ⟨ξ(u) , ξ(δ k -1 y k /N y k )⟩ H ξ(δ k -1 y k /N y k ) = 1 N N k=1 ((δ k -1 y k /N y k ) T Ku) ξ(δ k -1 y k /N y k ) = ξ        1 N N k=1 ((δ k -1 y k /N y k ) T Ku) (δ k -1 y k /N y k )        so that Σ w ξ(u) = ξ(P Ku) with P = 1 N N k=1 (δ k -1 y k /N y k )(δ k -1 y k /N y k ) T 4 One has (V ⊥ ) ⊥ = V for finite-dimensional-or more generally closed-subspaces of H</formula><p>Note that one has</p><formula xml:id="formula_695">• N k=1 δ k δ T k = Id R N , • N k=1 1 y k N y k δ T k = g∈R Y 1 g N g k:y k =g δ T k = g∈R Y 1 g 1 T g N g = N k=1 δ k 1 y k N y k T , • N k=1 1 y k N y k 1 y k N y k T = g∈R Y 1 g 1 T g N g .</formula><p>This shows that P can be expressed as</p><formula xml:id="formula_696">P = 1 N         Id R N - g∈R Y 1 g 1 T g /N g         .</formula><p>We have therefore proved that</p><formula xml:id="formula_697">• (Σ w + ρId H )ξ(u) = ξ ((P K + ρId R N )u) • (Σ w + ρId H ) -1 ξ( ũ) = ξ (P K + ρId R N ) -1 ũ .</formula><p>Recall that the feature-space LDA classification rule maximizes</p><formula xml:id="formula_698">⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H - 1 2 ⟨µ g -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H + log π g .</formula><p>All terms belong to V , except h(x), but this term can be replaced by its orthogonal projection on V without changing the result. This projection can be made explicit in terms of the representation ξ as follows. For x ∈ R, let ξ(ψ(x)) denote the orthogonal projection of h(x) on V (this defines the function ψ). If v(x) denotes the vector with coordinates K(x, x k ), k = 1, . . . , N , then ψ(x) = K -1 v(x), as can be obtained by identifying the inner products ⟨h(x) , h(x k )⟩ H and ⟨ξ(ψ(x)) , h(x k )⟩ H .</p><p>We are now ready to rewrite the kernel LDA classification rule in terms of quantities that only involve K. We have</p><formula xml:id="formula_699">⟨h(x) -µ , (Σ w + ρId H ) -1 (µ g -µ)⟩ H = ⟨ξ(ψ(x) -1/N ) , ξ((P K + ρId R N ) -1 (1 g /N g -1/N ))⟩ H = (ψ(x) -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) Given this, the classification rule must maximize (ψ(x) -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) - 1 2 (1 g /N g -1/N ) T K(P K + ρId R N ) -1 (1 g /N g -1/N ) + log π g . (8.16) Dimension reduction Note that K(P K + ρId R N ) -1 = K(KP K + ρK) -1 K is a symmet- ric matrix.</formula><p>So, the expression in <ref type="bibr">(8.16</ref>) can be written as</p><formula xml:id="formula_700">(v(x) -ν) T R -1 (ν g -ν) - 1 2 (ν g -ν) T R -1 (ν g -ν) + log π g .</formula><p>with R = KP K + ρK, ν g = K1 g /N g and ν = K1/N . Clearly, if v 1 , . . . , v N are the column vectors K, we have</p><formula xml:id="formula_701">ν g = 1 N g N k=1 v k 1 y k =g , ν = 1 N N k=1 v k .</formula><p>We therefore retrieve an expression similar to finite-dimensional LDA, provided that one replaces x by v(x), x k by v k and Σ w by R. Letting</p><formula xml:id="formula_702">Q = 1 N g∈R Y N g (ν g -ν)(ν g -ν) T</formula><p>be the between-class covariance matrix, the discriminant directions are therefore solutions of the generalized eigenvalue problem</p><formula xml:id="formula_703">Qf j = λ j Rf j with f T j Rf j = 1 with R = (KP K + ρK). Note that KP K = 1 N N k=1 (v k -νy k )(v k -νy k ) T</formula><p>is the within-class covariance matrix for the training data (v 1 , y 1 , . . . , v N , y N ).</p><p>The following summarizes the kernel LDA classification algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 8.4 (Kernel LDA)</head><p>(1) Select a positive kernel K and a coefficient ρ &gt; 0.</p><p>(2) Given T = (x 1 , y 1 , . . . , x N , y N ) and a kernel K, compute the kernel matrix K = K(x 1 , . . . , x N ) and the matrix R = KP K + ρK. Let v 1 , . . . , v N be the column vectors of K.</p><p>(3) Compute, for g ∈ R Y ,</p><formula xml:id="formula_704">ν g = 1 N g N k=1 v k 1 y k =g , ν = 1 N N k=1 v k and let Q = 1 N g∈R Y N g (ν g -ν)(ν g -ν) T .</formula><p>(4) Fix r 0 ≤ q-1 and compute the eigenvectors f 1 , . . . , f r 0 associated with the r 0 largest eigenvalues for the generalized eigenvalue problem Qf = λRf , normalized such that f T j Rf j = 1.</p><p>(5) Compute the scores γ jg = (ν g -ν) T f j .</p><p>(6) Given a new observation x, let v(x) be the vector with coordinates K(x, x k ), k = 1, . . . , N . Compute the scores γ j (x) = (v(x) -ν) T f j , j = 1, . . . , r 0 . Classify x in the class g maximizing</p><formula xml:id="formula_705">r i=1 γ i (x)γ ig - 1 2 r i=1 γ 2 ig + log π g . (<label>8</label></formula><p>.17)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Optimal Scoring</head><p>It is possible to apply linear regression (chapter 7) to solve a classification problem by mapping the set R Y to a collection of r-dimensional row vectors, or "scores."</p><p>These scores (which have a different meaning from the LDA scores) will be represented by a function θ : R Y → R r . As an example, one can take r = q and θ(g</p><formula xml:id="formula_706">1 ) =                    1 0 0 . . . 0                    , θ(g 2 ) =                    0 1 0 . . . 0                    , . . . , θ(g q ) =                    0 0 . . . 0 1                    .</formula><p>Given a training set T = (x 1 , y 1 , . . . , x N , y N ) and a score function θ, a linear model can then be estimated from data by minimizing</p><formula xml:id="formula_707">N k=1 |θ y k -a 0 -b T x k | 2</formula><p>where b is a d ×q matrix and a 0 ∈ R q . Letting as before β be the matrix with a T 0 added as first row to b and X the matrix with first row containing only ones and subsequent rows given by x T 1 , . . . , x T N , one gets the least square estimator β = (X T X ) -1 X T Y , where Y is the N × q matrix of stacked θ T y k row vectors.</p><p>Given an input vector x, the row vector xT β will generally not coincide with one of the score vectors. Assignment to a class can then be made by minimizing</p><formula xml:id="formula_708">|a 0 + b T x -θ g | over all g in R Y .</formula><p>Since the scores θ are free to choose, one may also try to optimize them, resulting in the optimal scoring algorithm. To describe it, we will need the notation already introduced for LDA, plus the following. We will write, for short, θ j = θ(g j ) and</p><formula xml:id="formula_709">introduce the q × r matrix Θ =           θ T 1 . . . θ T q          </formula><p>. We also denote by ρ 1 , . . . , ρ r the column vectors of Θ, so that Θ = [ρ 1 , . . . , ρ r ]. Let u g i , for i = 1, . . . , q, denote the q-dimensional vector with ith coordinate equal to 1 and all others equal to 0. As before, N g denote the class sizes, c g = N g /N , C is the diagonal matrix with coefficients c g 1 , . . . , c g q and ζ =</p><formula xml:id="formula_710">          c g 1 . . . c g q          </formula><p>.</p><p>The goal of optimal scoring is to minimize, now with respect to θ, a 0 and b, the function</p><formula xml:id="formula_711">F(θ, a 0 , b) = N k=1 |θ(y k ) -a 0 -b T x k | 2 .</formula><p>Some normalizing conditions are clearly needed, because this problem is underconstrained. (In the form above, the optimal choice is to take all free parameters equal to 0.) We now discuss the various indeterminacies and redundancies in the model, (a) If R is an r × r orthogonal matrix, then F(Rθ, Ra 0 , bR T ) = F(θ, a 0 , b), yielding an infinity of possible equivalent solutions (that all lead to the same classification rule). This implies that there is no loss of generality in assuming that Θ T CΘ is diagonal (introducing C here will turn out to be convenient). Indeed, given any (θ, a 0 , b), one can just take R such that RΘ T CΘR T is diagonal and replace Θ by RΘ, a 0 by Ra 0 and b by bR T to get an equivalent solution satisfying the constraint.</p><p>(b) Let D be an r by r diagonal matrix with positive entries. Replace θ, a 0 and b respectively by Dθ, Da 0 and bD. The resulting objective function is</p><formula xml:id="formula_712">F(Dθ, Da 0 , bD T ) = N k=1 |Dθ(y k ) -Da 0 -Db T x k | 2 = r j=1 N k=1 d 2 jj θ(y k , j) -a 0 (j) - d i=1 b(i, j)x k (i) 2</formula><p>If the coefficient d jj is free to chose, then the objective function can always be reduced by letting d jj → 0, which removes one of the dimensions in θ. In order to avoid this, one needs to fix the diagonal values of Θ T CΘ, and, by symmetry, it is natural to require</p><formula xml:id="formula_713">Θ T CΘ = Id R r . (c) Given any δ ∈ R r , one has F(θ, a 0 , b) = F(θ -δ, a 0 + δ, b</formula><p>), with identical classification rule. One can therefore without loss of generality introduce r linear constraints, and a convenient choice is</p><formula xml:id="formula_714">Θ T ζ = g∈R Y c g θ g = 0.</formula><p>Given this reduction, we can now describe the optimal scoring problem as the minimization of</p><formula xml:id="formula_715">N k=1 |θ y k -a 0 -b T x k | 2 subject to Θ T CΘ = Id R r and Θ T ζ = 0.</formula><p>The optimal a 0 is given by â0</p><formula xml:id="formula_716">= 1 N N k=1 θ y k -b T µ = -b T µ,</formula><p>so that the problem is reduced to minimizing</p><formula xml:id="formula_717">N k=1 |θ y k -b T (x k -µ)| 2</formula><p>subject to the same constraints. Using the facts that</p><formula xml:id="formula_718">θ y k = Θ T u y k , that N k=1 u y k u T y k = g∈R Y N g u g u T g = N C</formula><p>and that</p><formula xml:id="formula_719">N k=1 u y k (x k -µ) T = N g∈R Y u g k:y k =g (x k -µ) T = N g∈R Y u g N g (µ g -µ) T = N CM, one can write N k=1 |θ y k -b T (x k -µ)| 2 = N k=1 |Θ T u y k -b T (x k -µ)| 2 = N k=1 u T y k ΘΘ T u y k -2 N k=1 (x k -µ) T bΘ T u y k + N k=1 (x k -µ) T bb T (x k -µ) = N k=1 trace(Θ T u y k u T y k Θ) -2 N k=1 trace(Θ T u y k (x k -µ) T b) + N k=1 trace(b T (x k -µ)(x k -µ) T b) = N trace(Θ T CΘ) -2N trace(Θ T CMb) + N trace(b T Σ XX b) .</formula><p>Note that, since Θ T CΘ = Id R r , then trace(Θ T CΘ) = r. We therefore obtain a concise form of the optimal scoring problem: minimize</p><formula xml:id="formula_720">-2trace(Θ T CMb) + trace(b T Σ XX b). subject to Θ T CΘ = Id R r and Θ T ζ = 0.</formula><p>Given Θ, the optimal b is Σ -1 XX M T CΘ, and replacing it in the objective function, one finds that Θ must minimize</p><formula xml:id="formula_721">-2trace(Θ T CMΣ -1 XX M T CΘ) + trace(Θ T CMΣ -1 XX M T CΘ)</formula><p>i.e., maximize trace(Θ T CMΣ -1 XX M T CΘ) subject to Θ T CΘ = Id R r and Θ T ζ = 0. We now recall the following linear algebra result (see chapter 2). Proposition 8.7 Let A and B be respectively positive definite and non-negative semidefinite symmetric q by q matrices. Then, the maximum, over all q by r matrices S such that trace(S T AS) = Id R r , of trace(S T BS) is attained at S = [σ 1 , . . . , σ r ], where the columns vectors σ 1 , . . . , σ r are the solutions of the generalized eigenvalue problem Bσ = λAσ associated with the largest eigenvalues, normalized so that σ T i Aσ i = 1 for i = 1, . . . , r..</p><p>Given this proposition, let ρ 1 , . . . , ρ r be the r first eigenvectors for the problem</p><formula xml:id="formula_722">CMΣ -1 XX M T Cρ = λCρ. (8.18)</formula><p>Assume that r is small enough so that the associated eigenvalues are not zero. Let Θ = [ρ 1 , . . . , ρ r ]. We now prove that Θ is indeed a solution of the optimal scoring problem, and the only point to show to complete the statement is that this Θ satisfies the constraints Θ T ζ = 0. But we have</p><formula xml:id="formula_723">M T C1 q = g c g (µ g -μ) = 0,</formula><p>which implies that 1 q is a solution of the generalized eigenvalue problem associated with λ = 0. This in turn implies that 1</p><formula xml:id="formula_724">T q Cρ i = ζ T ρ i = 0, which is exactly Θ T ζ = 0.</formula><p>To summarize, we have found that the solution θ, b minimizing</p><formula xml:id="formula_725">-2trace(Θ T CMb) + trace(b T Σ XX b) subject to Θ T CΘ = Id R r and Θ T ζ = 0 is given by (i) Θ = [ρ 1 , . . . , ρ r ]</formula><p>where ρ 1 , . . . , ρ r are the eigenvectors for the problem</p><formula xml:id="formula_726">CMΣ -1 XX M T Cρ = λCρ</formula><p>associated with the r largest eigenvalues, normalized so that ρ T Cρ = 1.</p><formula xml:id="formula_727">(ii) b = Σ -1 XX M T CΘ.</formula><p>The computation can, however, be further simplified. Let λ 1 , . . . , λ r be the eigenvalues associated with ρ 1 , . . . , ρ r . Letting D be the associated diagonal matrix, one can write</p><formula xml:id="formula_728">CMΣ -1 XX M T CΘ = CΘD. This yields Θ = MΣ -1 XX M T CΘD -1 = MbD -1 , from which we deduce that θ g = Θ T u g = D -1 b T (µ g -μ)</formula><p>. So, given a new input vector x, the decision rule is to assign it to the class g for which</p><formula xml:id="formula_729">|θ g -b T (x -μ)| 2 = |Θ T u g -b T (x -μ)| 2 = |D -1 b T (µ g -μ) -b T (x -μ)| 2 is minimal. Letting b 1 , . . . , b r denote the r columns of b, this is equivalent to mini- mizing, in g r j=1 (b T j (µ g -μ)) 2 /λ 2 j -2 r j=1 (b T j (x -μ))(b T j (µ g -μ))/λ j . (8.19) From b = Σ -1 XX M T CΘ and Θ = MbD -1 we see that bD = Σ -1 XX M T CMb, so that Σ b b = Σ XX bD. This shows that the columns of b are solution of the eigenvalue problem Σ b u = λΣ XX u. Moreover, from Θ T CΘ = Id R r , we get b T Σ b b = D 2 . Since b T Σ b b = b T Σ XX bD, we get that b must be normalized to that b T Σ XX b = D.</formula><p>This shows that the solution of the optimal scoring problem can be reformulated uniquely in terms of b: if b 1 , . . . , b r are the r principal solutions of the eigenvalue problem</p><formula xml:id="formula_730">Σ b u = λΣ XX u, normalized so that u T Σ XX u = λ, a new input is classified into the class g minimizing r j=1 γ j (µ g ) 2 /λ 2 j -2 r j=1 γ j (x)γ j (µ g )/λ j . with γ j (x) = b T j (x -μ).</formula><p>Remark 8.8 The following computation shows that optimal scoring is closely related to LDA. Recall the identity</p><formula xml:id="formula_731">Σ XX = Σ w + Σ b . It implies that a solution of Σ b u = λΣ XX u is also a solution of Σ b u = λΣ w u with λ = λ/(1 -λ). If u T Σ XX u = λ, then u T Σ w u = λ -u T Σ b u = λ -λ 2 = λ (1 + λ) 2 , which shows that ũ = 1 + λ √ λ u satisfies ũT Σ w ũ = 1.</formula><p>So,</p><formula xml:id="formula_732">e j = 1 + λj λj b j</formula><p>coincide with the LDA directions. We have, letting γj (x) = e T j (x -μ) = λj γ j (x)/(1 + λj ):</p><formula xml:id="formula_733">r j=1 γ j (µ g ) 2 /λ 2 j -2 r j=1 γ j (x)γ j (µ g )/λ j = r j=1 γj (µ g ) 2 / λj -2 r j=1 γ j (x)γ j (µ g )/(1 + λj )</formula><p>which relates the classification rules for the two methods. ♦ Remark 8.9 Optimal scoring can be modified by adding a penalty in the form</p><formula xml:id="formula_734">γ r i=1 b T i Ωb i = γtrace(b T Ωb) (8.20)</formula><p>where Ω is a weight matrix. This only modifies the previous discussion by adding γΩ/N to both Σ XX and Σ w . ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Kernel optimal scoring</head><p>Let h : R X → H be the feature function and K the associated kernel, as usual. Optimal scoring in feature space requires to minimize</p><formula xml:id="formula_735">N k=1 |θ y k -a 0 -b(h(x k ))| 2 + γ∥b∥ 2 H ,</formula><p>where we have introduced a penalty on b. Here, b is a linear operator from H to R r , therefore taking the form</p><formula xml:id="formula_736">b(h) =           ⟨b 1 , h⟩ H . . . ⟨b r , h⟩ H          </formula><p>with b 1 , . . . , b r ∈ H, and we take</p><formula xml:id="formula_737">∥b∥ 2 H = r i=1 ∥b i ∥ 2 H .</formula><p>It is once again clear (and the argument is left to the reader) that the problem can be reduced to the finite dimensional space V = span(h(x 1 ), . . . , h(x N )), and that the optimal b 1 , . . . , b r must take the form</p><formula xml:id="formula_738">b j = N l=1 α li h(x l ) .</formula><p>Introduce the kernel matrix K = K(x 1 , . . . , x N ) with kth column denoted K (k) . Let α be the N by r matrix with entries α kj , k = 1, . . . , N , j = 1, . . . , r. Then b(h(x k )), which is the vector with coordinates</p><formula xml:id="formula_739">⟨b j , h(x k )⟩ = N l=1 α li K(x k , x l ), is equal to α T K (k) . Moreover ∥b∥ 2 H = r j=1 N k,l=1 α kj K(x k , x l )α lj = trace(α T Kα).</formula><p>We therefore need to minimize</p><formula xml:id="formula_740">N k=1 |θ y k -a 0 -α T K (k) | 2 + γtrace(α T Kα),</formula><p>so that the problem is reduced to penalized optimal scoring, with x k replaced by K (k) , b replaced by α and the matrix Ω in (8.20) replaced by K. Introducing the matrix P = Id R N -11 T /N and K c = P K, the covariance matrix</p><formula xml:id="formula_741">Σ XX becomes K T c K c /N = KP K/N .</formula><p>The class averages µ g are equal to K1(g)/N g while µ = K1/N , so that the matrix</p><formula xml:id="formula_742">M is equal to            1(g 1 ) T /N g 1 -1 T /N . . . 1(g q ) T /N g q -1 T /N            K which gives Σ b = M T CM = KQK, where Q is Q = P CP = g∈R Y N g N 1(g) N g - 1 N 1(g) N g - 1 N T</formula><p>So, the columns of α are the r principal eigenvectors ρ 1 , . . . , ρ r of the problem</p><formula xml:id="formula_743">KQKρ = 1 N (KP K + γK)ρ.</formula><p>Given α, one then has, for any x ∈ R d ,</p><formula xml:id="formula_744">⟨b i , h(x)⟩ H = N k=1 α ki K(x, x k ) and a 0 (i) = 1 N N k,l=1 α ki K(x l , x k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Separating hyperplanes and SVMs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.1">One-layer perceptron and margin</head><p>In this whole section, we restrict to two-class problems, and let</p><formula xml:id="formula_745">R Y = {-1, 1}. Given a 0 ∈ R and b 0 ∈ R d , the equation a 0 + b T x = 0 defines a hyperplane in R d . The function f (x) = sign(a 0 + x T b</formula><p>) defines a classifier that attributes a class ±1 to x according to which side of the hyperplane it belongs (we ignore the ambiguity when x is on the hyperplane). With this notation, a pair (x, y), where y is the true class, is correctly classified if and only if y(a 0 + x T b) &gt; 0.</p><p>Let T = (x 1 , y 1 ), . . . , (x N , y N ) denote, as usual, the training data. A hyperplane, represented by the parameters (a 0 , b) is separating for T if it correctly classifies all its samples, i.e., if y k (a 0 + x T k b) &gt; 0 for k = 1, . . . , N . If such a hyperplane exists, one says that T is linearly separable.</p><p>The perceptron algorithm computes a 0 and b by minimizing</p><formula xml:id="formula_746">L(β) = N k=1 [y k (a 0 + x T k b)] -</formula><p>with u -= max(0, -u), , or more precisely, fixing a small positive number δ:</p><formula xml:id="formula_747">L(β) = N k=1 [δ -y k (a 0 + x T k b)] + .</formula><p>The problem can be recast as a linear program, i.e., minimize</p><formula xml:id="formula_748">N k=1 ξ k subject to ξ k ≥ 0, ξ k + y k (a 0 + x T k b) -δ ≥ 0 for k = 1, . . . , N .</formula><p>However, when T is linearly separable, separating hyperplanes are not uniquely defined, and there is in general (depending on the choice made for δ) an infinity of solutions to the perceptron problem. Intuitively, one should prefers a solution that classifies the training data with some large margin, rather than one for which training points may be very close to the separating boundary (see fig.  This leads to the maximum margin separating hyperplane classifier, also called linear SVM, introduced by Vapnik and Chervonenkis <ref type="bibr" target="#b214">[196,</ref><ref type="bibr" target="#b215">197]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.2">Maximizing the margin</head><p>We will use the following result.</p><formula xml:id="formula_749">Proposition 8.10 The distance of a point x ∈ R d to the hyperplane M : a 0 + b T x = 0 is given by |a 0 + x T b|/|b|. Proof By definition, distance(x, M) = |x -π M (x)| where π M is the orthogonal projec- tion on M. Since b is normal to M and letting h = π M (x), we have x = λb + h so that |λb| = distance(x, M). Writing a 0 + b T h = 0 in this equation implies a 0 + b T x = λ|b| 2 so that |λ| |b| = |a 0 + x T b|/|b|.</formula><p>■ Assume that T is linearly separable and let M : a 0 + b T x = 0 be a separating hyperplane. The classification margin is defined as the minimal distance of the input vectors x 1 , . . . , x N to this hyperplane, i.e.,</p><formula xml:id="formula_750">m(a 0 , b) = min{|a 0 + x T k b|/|b| : k = 1, . . . , N }.</formula><p>Because the hyperplane is separating, we have y k (a 0 + x T k b) = |a 0 + x T k b| for all k, so that we also have</p><formula xml:id="formula_751">m(a 0 , b) = min{y k (a 0 + x T k b)/|b| : k = 1, . . . , N }.</formula><p>We want to maximize this margin among all separating hyperplanes. This can be expressed as maximizing, with respect to a 0 , b, the quantity</p><formula xml:id="formula_752">min{y k (a 0 + x T k b)/|b| : k = 1, . . . , N } subject to the constraint that the hyperplane is separating, namely y k (a 0 + x T k b) ≥ 0, k = 1, . . . , N .</formula><p>Introducing a new variable C representing the margin, the previous problem is equivalent to maximizing C subject to</p><formula xml:id="formula_753">y k (a 0 + x T k b) ≥ C|b|, k = 1, . . . , N .</formula><p>The problem is now overparametrized, and there is no loss of generality in enforcing the additional constraint C|b| = 1. Noting that maximizing C is the same as minimizing |b| 2 , we can now reformulate the maximum margin hyperplane problem as minimizing |b| 2 /2 subject to</p><formula xml:id="formula_754">y k (a 0 + x T k b) ≥ 1, k = 1, .</formula><p>. . , N , with C (the margin) given by C = 1/|b|. This results in a quadratic programming problem.</p><p>If the data is not separable, there is no feasible point for this problem. To also account for this situation (which is common), we can replace the constraint by a penalty and minimize, with respect to a 0 and b:</p><formula xml:id="formula_755">|b| 2 2 + γ N k=1 (1 -y k (a 0 + x T k b)) +</formula><p>for some γ &gt; 0. (Recall that x + = max(x, 0).) This is equivalent to minimizing the perceptron objective function, with δ = 1, and with an additional penalty term equal to |b| 2 /(2γ). This minimization problem is equivalent to a quadratic programming problem obtained by introducing slack variables ξ k , k = 1, . . . , N and minimizing</p><formula xml:id="formula_756">1 2 |b| 2 + γ N k=1 ξ k , subject to the constraints ξ k ≥ 0, y k (a 0 + x T k b) + ξ k ≥ 1, for k = 1, . . . , N .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.3">KKT conditions and dual problem</head><p>Introduce Lagrange multipliers η k ≥ 0 for ξ k ≥ 0 and</p><formula xml:id="formula_757">α k ≥ 0 for y k (a 0 + x T k b) + ξ k ≥ 1. The Lagrangian is then given by L = 1 2 |b| 2 + γ N k=1 ξ k - N k=1 η k ξ k - N k=1 α k y k (a 0 + x T k b) + ξ k -1 .</formula><p>The KKT conditions are</p><formula xml:id="formula_758">                                     b - N k=1 α k y k x k = 0 N k=1 α k y k = 0 γ -η k -α k = 0, k = 1, . . . , N ξ k η k = 0, k = 1, . . . , N α k y k (a 0 + x T k b) + ξ k -1 = 0, k = 1, . . . , N<label>(8.21)</label></formula><p>Minimizing L with respect to a 0 , b and ξ 1 , . . . , ξ N and ensuring that the minimum is finite provides the first three KKT conditions. The resulting dual formulation therefore requires to maximize</p><formula xml:id="formula_759">N k=1 α k - 1 2 N k,l=1 α k α l y k y l x T k x l subject to the constraints 0 ≤ α k ≤ γ, N k=1 α k y k = 0.</formula><p>We now discuss the consequences of the complementary slackness conditions based on the position of training sample relative to the separating hyperplane.</p><p>(i) First consider indices k such that (x k , y k ) is correctly classified beyond the margin, i.e., y k (a 0 + x T k b) &gt; 1. The last KKT condition and the constraint ξ k ≥ 0 require α k = 0, and the third one then gives ξ k = 0.</p><p>(ii) For samples that are misclassified or correctly classified below the margin <ref type="foot" target="#foot_11">5</ref> , i,e.,</p><formula xml:id="formula_760">y k (a 0 + x T k b) &lt; 1, the constraint y k (a 0 + x T k b) + ξ k ≥ 1 implies ξ k &gt; 0, so that α k = γ and y k (a 0 + x T k b) + ξ k = 1. (iii) If (x k , y k</formula><p>) is correctly classified exactly at the margin, then ξ k = 0 and there is no constrain on α k beside belonging to [0, γ]. Training samples that lie exactly at the margin are called support vectors.</p><p>Given a solution α 1 , . . . , α N of the dual problem, one immediately recovers b via the first equation in <ref type="bibr">(8.21)</ref>. For a 0 , one must, similarly to the regression case, rely on support vectors, which can be identified when 0 &lt; α k &lt; γ. In this case, one can take</p><formula xml:id="formula_761">a 0 = y k -x T k b.</formula><p>If no support vector is found, then a 0 is not uniquely determined, and can be any value such that</p><formula xml:id="formula_762">y k (a 0 + b T x k ) ≥ 1 if α k = 0 and y k (a 0 + b T x k ) ≤ 1 if α k = γ. This shows that a 0 can be any point in the interval [β - 0 , β + 0 ] with a 0 -= max{y k -x T k b : (y k = 1 and α k = 0) or (y k = -1 and α k = γ)} a 0 + = min{y k -x T k b : (y k = -1 and α k = 0) or (y k = 1 and α k = γ)}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.4">Kernel version</head><p>We make the usual assumptions: h : R X → H is a feature map with values in an inner-product space with K(x, y) = ⟨h(x) , h(y)⟩ H . The predictors take the form f (x) = sign(a 0 + ⟨b , h(x)⟩ H ), a 0 ∈ R and b ∈ H, and the goal is to minimize</p><formula xml:id="formula_763">1 2 ∥b∥ 2 H + γ N k=1 ξ k , subject to ξ k ≥ 0, y k (a 0 + ⟨h(x k ) , b⟩ H ) + ξ k ≥ 1, k = 1, . . . , N . Let V = span(h(x 1 ), . . . , h(x N )).</formula><p>The usual projection argument implies that the optimal b must belong to V and therefore take the form</p><formula xml:id="formula_764">b = N k=1 u k h(x k ).</formula><p>We therefore need to minimize</p><formula xml:id="formula_765">1 2 N k,l=1 u k u l K(x k , x l ) + γ N k=1 ξ k , subject to y k a 0 + N l=1 K(x k , x l )a l + ξ k ≥ 1 for k = 1, . . . , N .</formula><p>Introducing the same Lagrange multipliers as before, the Lagrangian is</p><formula xml:id="formula_766">L = 1 2 N k,l=1 u k u l K(x k , x l ) + γ N k=1 ξ k - N k=1 η k ξ k - N k=1 α k y k a 0 + N l=1 K(x k , x l )u l + ξ k -1 .</formula><p>Using vector notation, we have</p><formula xml:id="formula_767">L = 1 2 u T Ku + ξ T (γ1 -η -α) -a 0 α T y -(α ⊙ y) T Ku + α T 1</formula><p>where y ⊙ α is the vector with coordinates y k α k . The infimum of L is -∞ unless γ1ηα = 0 and α T y = 0. If these identities are true, then the optimal u is u = α ⊙ y and the minimum of L is</p><formula xml:id="formula_768">- 1 2 (α ⊙ y) T K(α ⊙ y) + α T 1</formula><p>The dual problem therefore requires to minimize</p><formula xml:id="formula_769">1 2 (α ⊙ y) T K(α ⊙ y) -α T 1 = α T (K ⊙ yy T )α -α T 1 subject to γ1 -η -α = 0 and α T y = 0.</formula><p>This is exactly the same problem as the one we obtained in the linear case, up to the replacement of the Euclidean inner products x T k x l by the kernel evaluations K(x k , x l ). Given the solution of the dual problem, the optimal b is b</p><formula xml:id="formula_770">= k u k h(x k ) = N k=1 α k y k h(x k ).</formula><p>It is no computable, but the classification rule is explicit and given by</p><formula xml:id="formula_771">f (x) = sign        a 0 + N k=1 α k y k K(x k , x)        .</formula><p>Similarly to the linear case, the coefficient a 0 can be identified using a support vector, or is otherwise not uniquely determined. More precisely, if one of the α k 's is strictly between 0 and γ, then a 0 is given by a 0 = y kl α l y l K(x k , x l ). Otherwise, a 0 is any number between</p><formula xml:id="formula_772">β - 0 = max        y k - l α l y l K(x k , x l ) : (y k = 1 and α k = 0) or (y k = -1 and α k = γ)       </formula><p>and</p><formula xml:id="formula_773">β + 0 = min        y k - l α l y l K(x k , x l ) : (y k = -1 and α k = 0) or (y k = 1 and α k = γ)        . Chapter 9</formula><p>Nearest-Neighbor Methods</p><p>Unlike linear models, nearest-neighbor methods are completely non-parametric and assume no regularity on the decision rule or the regression function. In their simplest version, they require no training and rely on the proximity of a new observation to those that belong to the training set. We will discuss in this chapter how these methods are used for regression and classification, and study some of their theoretical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Nearest neighbors for regression 9.1.1 Consistency</head><p>We let R X denote the input space, and R Y = R q be the output space. We assume that a distance, denoted dist is defined on R X . This means that dist : R X × R X → [0, +∞] (we allow for infinite values) is a symmetric function such that dist(x, x ′ ) = 0 if and only if x = x ′ and, for all</p><formula xml:id="formula_774">x, x ′ , x ′′ ∈ R X dist(x, x ′ ) ≤ dist(x, x ′′ ) + dist(x ′′ , x ′ ),</formula><p>which is the triangle inequality.</p><formula xml:id="formula_775">Let T = (x 1 , y 1 , . . . , x N , y N ) be the training set. For x ∈ R X , let D T (x) = (dist(x, x k ), k = 1, . . . , N )</formula><p>be the collection of all distances between x and the inputs in the training set. We consider regression estimators taking the form</p><formula xml:id="formula_776">f (x) = N k=1 W k (x)y k (9.1)</formula><p>where W 1 (x), . . . , W N (x) is a family of coefficients, or weights, that only depends on D T (x).</p><p>We will, more precisely, use the following construction <ref type="bibr" target="#b201">[183]</ref>. Assume that a family of numbers</p><formula xml:id="formula_777">w 1 ≥ w 2 ≥ • • • ≥ w N ≥ 0 is chosen, with N j=1 w j = 1. Given x ∈ R d and k ∈ {1, . . . , N }, we let r + k (x) denote the number of indexes k ′ such that dist(x, x k ′ ) ≤ dist(x, x k ) and r - k (x) the number of such indexes such that d(x, x k ′ ) &lt; d(x, x k ).</formula><p>The coefficients defining f in (9.1) are then chosen as:</p><formula xml:id="formula_778">W k (x) = r + k (x) k ′ =r - k (x)+1 w k ′ r + k (x) -r - k (x) . (9.2)</formula><p>To emphasize the role of (w 1 , . . . , w N ) is this definition, we will denote the resulting estimation as fw . If there is no tie in the sequence of distances between x and elements of the training set, then r + k (x) = r - k (x) + 1 is the rank of x k when training data is ordered according to their proximity to x, and</p><formula xml:id="formula_779">W k (x) = w r + k (x) . In this case, defining l 1 , . . . , l N such that d(x, x l 1 ) &lt; • • • &lt; d(x, x l N ), we have fw (x) = N j=1 w j y l j .</formula><p>In the general case, the weights w j associated with tied observations are averaged.</p><p>If p is an integer, the p-nearest neighbor (p-NN) estimator (that we will denote fp ) is associated to the weights w j = 1/p for j = 1, . . . , p and 0 otherwise. If there is no tie for the definition of the pth nearest neighbor of x, W k (x) = 1/p if k is among the p nearest-neighbors and W k (x) = 0 otherwise, so that fp is the average of the output values over these p nearest neighbors. If the pth nearest neighbors are tied, their output value is averaged before being used in the sum. For example, assume that N = 5 and p = 2 and let the distances between x and x k for k = 1, . . . , 5 be respectively 9, 3, 2, 4, 6. Then f2 (x) = (y 2 + y 3 )/2. If the distances were 9, 3, 2, 3, 6, then we would have f2</p><formula xml:id="formula_780">(x) = (y 2 + y 4 )/4 + y 3 /2. When R X = R d and d(x, x ′ ) = |x -x ′ |, the following result is true. Theorem 9.1 ([183]) Assume that E(Y 2 ) &lt; ∞. Assume that, for each N , a sequence w (N ) = w (N ) 1 ≥ • • • ≥ w (N ) N ≥ 0 is chosen with N j=1 w (N ) j = 1. Assume, in addition, that (i) lim N →∞ w (N ) 1 = 0 (ii) lim N →∞ j≥αN w (N ) j</formula><p>→ 0, for some α ∈ (0, 1).</p><p>Then the corresponding classifier fw (N ) converges in the L 2 norm to E(Y | X):</p><formula xml:id="formula_781">E | fw (N ) (X) -E(Y | X)| 2 → 0.</formula><p>For nearest-neighbor regression, (i) and (ii) mean that the number of nearest neighbors p N must be chosen such that p N → ∞ and p N /N → 0.</p><p>Proof We give a proof under the assumption that f :</p><formula xml:id="formula_782">x → E(Y | X = x) is uniformly</formula><p>continuous and bounded (one can, in fact, prove that it is always possible to reduce to this case).</p><p>To lighten the notation, we will not make explicit the dependency on N in of quantities such as W or w. One has</p><formula xml:id="formula_783">fw (X) -E(Y | X) = N k=1 W k (X)(f (X k ) -f (X)) + N k=1 W k (X)(Y k -f (X k )) (9.3)</formula><p>and the two sums can be addressed separately.</p><p>We start with the first sum and write, by Schwartz's inequality:</p><formula xml:id="formula_784">       k W k (X)(f (X k ) -f (X))        2 ≤ k W k (X)(f (X k ) -f (X)) 2 .</formula><p>It therefore suffices to study the limit of</p><formula xml:id="formula_785">E( k W k (X)(f (X k ) -f (X)) 2 . Fix ϵ &gt; 0. By assumption, there exists M, a &gt; 0 such that |f (x)| ≤ M for all x and |x -y| ≤ a ⇒ |f (x) -f (y)| 2 ≤ ϵ. Then E        k W k (X)(f (X k ) -f (X)) 2        =E        k W k (X)(f (X k ) -f (X)) 2 1 |X k -X|≤a        + E        k W k (X)(f (X k ) -f (X)) 2 1 |X k -X|&gt;a        ≤ ϵ 2 + 4M 2 E        k W k (X)1 |X k -X|&gt;a        .</formula><p>Since ϵ can be made arbitrarily small, we need to show that, for any positive a, the second term in the upper-bound tends to 0 when N → ∞. We will use the following fact, which requires some minor measure theory argument to prove rigorously.</p><formula xml:id="formula_786">Define S = {x : ∀δ &gt; 0, P(|X -x| &lt; δ) &gt; 0} .</formula><p>This set is called the support of X. Then, one can show that P(X ∈ S) = 1. This means that, if X is independent from X with the same distribution, then, for any δ &gt; 0, P(|X -X| &lt; δ|X) &gt; 0 with probability one. 1   Let</p><formula xml:id="formula_787">N a (x) = | {k : |X k -x| ≤ a} |.</formula><p>We have, for all x ∈ S and a &gt; 0, and using the law of large numbers,</p><formula xml:id="formula_788">N a (x) N = 1 N N k=1 1 |X k -x|≤a → P (|X -x| ≤ a) &gt; 0. If |X -X k | &gt; a, then r - k (X) &gt; N a (x) so that k W k (X)1 |X k -X|&gt;a ≤ j≥N a (X)</formula><p>w j , and we have, taking 0</p><formula xml:id="formula_789">&lt; α &lt; P (|X -x| ≤ a), E         j≥N a (X) w j         ≤ j≥αN w j + P(N a (X) &lt; αN )</formula><p>and both terms in the upper bound converge to 0. This shows that the first sum in (9.3) tends to 0.</p><p>We now consider the second sum in <ref type="bibr">(9.3)</ref>.</p><formula xml:id="formula_790">Let Z k = Y k -E(Y | X k ). We have E(Z k | X k ) = 0 and E(Z 2 k ) &lt; ∞. We can write E         N k=1 W k (X)Z k 2         = E         E         N k=1 W k (X)Z k 2 X, X 1 , . . . , X N                 = E        N k=1 W k (X) 2 E(Z 2 k | X k )        + N k l=1 E(W k (X)W l (X)E(Z k Z l | X i , X j ))</formula><p>1 This statement is proved as follows (with the assumption that X is Borel measurable). Let S c denote the complement of S. Then S c is open. Indeed if x S, there exists δ x &gt; 0 such that, letting B(x, δ x ) denote the open ball with radius δ x , P(X ∈ B(x, δ x )) = 0. Then P(X ∈ B(x ′ , δ x /3)) = 0 as soon as</p><formula xml:id="formula_791">|x -x ′ | &lt; δ x /3, so that B(x, δ x /3) ⊂ S c . If K ⊂ S c is compact, then K ⊂ x∈K B(x, δ x )</formula><p>and one can find a finite subset M ⊂ K such that K ⊂ x∈M B(x, δ x ), which proves that P(X ∈ K) = 0. Since P(X ∈ S c ) = max K P(X ∈ K) where the maximum is over all compact subsets of S c , we find P(X ∈ S c ) = 0 as required.</p><p>The cross products in the last term vanish because E(Z k | X k ) = 0 and the samples are independent. So it only remains to consider</p><formula xml:id="formula_792">E        N k=1 W k (X) 2 E(Z 2 k | X k )        The random variable E(Z k | X k ) = E(Y 2 k | X k ) -E(Y k | X k ) 2 is a fixed non-negative</formula><p>function of X k , that we will denote h(X k ). We have</p><formula xml:id="formula_793">E        N k=1 W k (X) 2 h(X k )        ≤ w 1 E        N k=1 W k (X)h(X i )        with w 1 → 0 and the proof is concluded by showing that E N k=1 W k (X)h(X k ) is bounded.</formula><p>Recall that the weights W k are functions of X and of the whole training set, and we will need to make this dependency explicit and write W i (X, T X ) where T X = (X 1 , . . . , X N ). Similarly, the ranks in (9.2) will be written r + j (X, T X ) and r - j (X, T X ).</p><p>Because X, X 1 , . . . , X N are i.i.d., we can switch the role of X and X k in the kth term of the sum, yielding</p><formula xml:id="formula_794">E        N k=1 W k (X, T X )h(X k )        = E               N i=1 W k (X k , T (k) X )        h(X)        with T (k) X = (X 1 , . . . , X k-1 , X, X k+1 , . . . , X N ). We now show that N k=1 W k (X k , T (k) X ) is bounded independently of X, X 1 , . . . , X N .</formula><p>For this purpose, we group X 1 , . . . , X N according to approximate alignment with X. For u ∈ R d with |u| = 1 and for δ ∈ (0, π/4), denote by Γ (u, δ) the cone formed by all vectors v in R d such that ⟨v , u⟩ &gt; |v| cos δ (i.e., the angle between v and u is less</p><formula xml:id="formula_795">than δ). Notice that if v, v ′ ∈ Γ (u, δ), then ⟨v , v ′ ⟩ ≥ cos(2δ)|v| |v ′ | and if |v ′ | ≤ |v|, then |v| 2 -|v -v ′ | 2 = |v ′ |(2|v| cos(2δ) -|v ′ |) &gt; 0 (9.4) because cos(2δ) &gt; 1/2.</formula><p>Fixing δ, let C d (δ) be the minimal number of such cones needed to cover R d .</p><p>Choosing such a covering Γ (u 1 , δ), . . . , Γ (u M , δ) where M = C d (δ), we define the following subsets of {1, . . . , M}:</p><formula xml:id="formula_796">I 0 = {k : X k = X} I q = k I 0 : X k -X ∈ Γ (u q , δ) , q = 1, . . . , M</formula><p>(these sets may overlap). We have</p><formula xml:id="formula_797">N k=1 W k (X k , T (k) X ) ≤ M q=0 k∈I q W k (X k , T (k) X ) If k ∈ I 0 , then r - k (X k , T (k) X ) = 0 and r + k (X k , T (k) X ) = c with c = |I 0 |. This implies that, for k ∈ I 0 , we have W k (X k , T (k) X ) = c j=1 w j /c and k∈I 0 W k (X k , T (k) X ) = c j=1 w j .</formula><p>We now consider I q with q ≥ 1. Write</p><formula xml:id="formula_798">I q = {i 1 , . . . , i r } ordered so that |X i j -X| is non-decreasing. If j ′ &lt; j, we have (using (9.4)) |X i j -X i j ′ | &lt; |X -X i j |. This implies that r - i j (X i j , T (i j ) X ) ≥ j -1 and r + i j (X i j , T (i j ) X ) -r -i j (X i j , T (i j ) X ) ≥ c + 1. Therefore, W i j (X i j , T (i j ) X ) ≤ 1 c + 1 c+j j ′ =j w j ′ and k∈I q W k (X k , T (k) X ) ≤ 1 c + 1 N j=1 c+j j ′ =j w j ′ = 1 c + 1         c j ′ =1 j ′ w j ′ + (c + 1) N j ′ =c+1 w j ′         . This yields N k=1 W k (X k , T (k) X ) ≤ c j=1 w j + C d (δ)         1 c + 1 c j ′ =1 j ′ w j ′ + N j ′ =c+1 w j ′         ≤ C d (δ) + 1.</formula><p>We therefore have</p><formula xml:id="formula_799">E        N k=1 W k (X) 2 E(Z 2 k | X k )        ≤ w 1 (C d (δ) + 1)E(h(X)) → 0,</formula><p>which concludes the proof. ■ Theorem 9.1 is proved in Stone <ref type="bibr" target="#b201">[183]</ref> with weaker hypotheses allowing for more flexibility in the computation of distances, in which, for example, differences X -X i can be normalized by dividing them by a factor σ i that may depend on the training set. These relaxed assumptions slightly complicate the proof, and we refer the reader to Stone <ref type="bibr" target="#b201">[183]</ref> for a complete exposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">Optimality</head><p>The NN method can be shown to be optimal over some classes of functions. Optimality is in the min-max sense, and works as follows. We assume that the regression function f</p><formula xml:id="formula_800">(x) = E(Y | X = x) belongs to some set F of real-valued functions on R d .</formula><p>Most of the time, the estimation methods must be adapted to a given choice of F , and various choices have arisen in the literature: classes of functions with r bounded derivatives, Sobolev or related spaces, functions whose Fourier transforms has given properties, etc.</p><p>Consider now an estimator of f , denoted fN , based on a training set of size N . We can measure the error by, say:</p><formula xml:id="formula_801">fN -f 2 = R d ( fN (x) -f (x)) 2 dx 1/2</formula><p>Since fN is computed from a random sample, this error is a random variable. One can study, when b N → 0, the probability</p><formula xml:id="formula_802">P f ∥ fN -f ∥ 2 2 ≥ cb N</formula><p>for some constant c and, for example, for the model: Y = f (X) + noise. Here, the notation P f refers to the model assumption indicating the unobserved function f . The min-max method considers the worst case and computes</p><formula xml:id="formula_803">M N (c) = sup f ∈F P f ∥ fN -f ∥ 2 2 ≥ cb N .</formula><p>This quantity now only depends on the estimation algorithm. One defines the notion of "lower convergence rate" as a sequence b N such that, for any choice of the estimation algorithm, M N (c) can be found arbitrarily close to 1 (i.e., ∥ fNf ∥ 2 2 ≥ cb N with arbitrarily high probability for all f ∈ F ), for arbitrarily large N (and for some choice of c). The mathematical statement is</p><formula xml:id="formula_804">∃c &gt; 0 : lim inf N →∞ M N (c) = 1.</formula><p>So, if b N is a lower convergence rate, then, for every estimator, there exists a constant c such that the accuracy cb N cannot be achieved.</p><p>On the other hand, one says that b N is an achievable rate of convergence if there exists an estimator such that, for some c ′ , lim sup</p><formula xml:id="formula_805">N →∞ M N (c ′ ) = 0.</formula><p>This says that for large N , and for some c ′ , the accuracy is higher than c ′ b N for the given estimator. Notice the difference: a lower rate holds for all estimators, and an achievable rate for at least one estimator.</p><p>The final definition of a min-max optimal rate is that it is both a lower rate and an achievable rate (obviously for different constants c and c ′ ). And an estimator is optimal in the min-max sense if it achieves an optimal rate. One can show that the p-NN estimator is optimal (under some assumptions on the ratio p N /N ) when F is the class of Lipschitz functions on R d , i.e., the class of functions such that there exists a constant K with</p><formula xml:id="formula_806">|f (x) -f (y)| ≤ K|x -y|</formula><p>for all x, y ∈ R d . In this case, the optimal rate is b N = N -1/(2+d) (notice again the "curse of dimensionality": to achieve a given accuracy in the worst case, the number of data points must grow exponentially with the dimension).</p><p>If the function class consists of smoother functions (for example, several derivatives), the p-NN method is not optimal. This is because the local averaging method is too crude when one knows already that the function is smooth. But it can be modified (for example by fitting, using least squares, a polynomial of some degree instead of computing an average) in order to obtain an optimal rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">p-NN classification</head><p>Let (x 1 , y 1 , . . . , x N , y N ) be the training set, with x i ∈ R d and y i ∈ R Y where R Y is a finite set of classes. Using the same notation as in the previous section, define</p><formula xml:id="formula_807">π w (y|x) = N k=1 W k (x)1 y k =y .</formula><p>Let the corresponding classifier be fw</p><formula xml:id="formula_808">(x) = argmax y∈R Y π w (y|x). Theorem 9.1 may be applied, for y ∈ R Y , to the function f y (x) = π(y | x) = E(1 Y =y | X = x</formula><p>), which allows one to interpret the estimator π(y | x) as a nearest-neighbor predictor of the random variable 1 Y =y as a function of X. We therefore obtain the consistency of the estimated posteriors when N → ∞ under the same assumption as those of theorem 9.1. This implies that, for large N , the classification will be close to Bayes's rule.</p><p>An asymptotic comparison with Bayes's rule can already be made with p = 1. Let ŷN (x) be the 1-NN estimator of Y given x and a training set of size N , and let ŷ(x) be the Bayes estimator. We can compute the Bayes error by</p><formula xml:id="formula_809">P( ŷ(X) Y ) = 1 -P( ŷ(X) = Y ) = 1 -E(P( ŷ(X) = Y |X)) = 1 -E(max y∈R Y π(y|X))</formula><p>For the 1-NN rule, we have</p><formula xml:id="formula_810">P( ŷN (X) Y ) = 1 -P( ŷN (X) = Y ) = 1 -E(P( ŷN (X) = Y |X))</formula><p>Let us make the assumption that nearest neighbors are not tied (with probability one). Let k * (x, T ) denote the index of the nearest neighbor to x in the training set T . We have</p><formula xml:id="formula_811">P( ŷN (X) = Y | X) = E(P( ŷN (X) = Y | X, T )) = E N k=1 P(Y = Y k | X, T )1 k * (X,T )=k = E N k=1 P(Y = Y k | X, X k )1 k * (X,T )=k = E N k=1 g∈R Y P(Y = g, Y k = g | X, X k )1 k * (X,T )=k = E N k=1 g∈R Y π(g | X)π(g | X k )χ k * (X,T )=k = E g∈R Y π(g | X)π(g | X k * (X,T ) )</formula><p>Now, assume the continuity of x → π(g | x) (although the result can be proved without this simplifying assumption). We know that X k * (X,T ) → X when N → ∞ (see the proof of theorem 9.1), which implies that π(g | X k * (X,T ) ) → π(x | X) and at the limit</p><formula xml:id="formula_812">P( ŷN (X) = Y | X) → g∈R Y π(g | X) 2 .</formula><p>This implies that the asymptotic 1-NN misclassification error is always smaller than 2 times the Bayes error, that is</p><formula xml:id="formula_813">1 -E g∈R Y π(g | X) 2 ≤ 2(1 -E(max g π(g | X)))</formula><p>Indeed, the left-hand term is smaller than 1 -E(max g π(g|x) 2 ) and the result comes from the fact that for any t ∈ R. 1t 2 ≤ 2 -2t.</p><p>Remark 9.2 Nearest neighbor methods may require large computation time, since, for a given x, the number of comparisons which are needed is the size of the training set. However, efficient (tree-based) search algorithms can be used in many cases to reduce it to a logarithm in the size of the database, which is acceptable. A reduction of the size of the training set by clustering also is a possibility for improving the efficiency.</p><p>The computation time is also generally proportional to the dimension d of the input x. When d is large, a reduction of dimension is often a good idea. Principal components (see chapter 20), or LDA directions (see chapter 8) can be used for this purpose. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Designing the distance</head><p>LDA-based distance The most important factor in the design of a NN procedure probably is the choice of the distance, something we have not discussed so far. Intuitively, the distance should increase fast in the directions "perpendicular" to the regions of constancy of the class variables, and slowly (ideally not at all) within these regions. The following construction uses discriminant analysis <ref type="bibr" target="#b105">[87]</ref>.</p><p>For g ∈ R Y , let Σ g be the covariance matrix in class g, and Σ w = g∈R Y π g Σ g be the within-class variance, where π g is the frequency of class g. Let Σ b denote the between-class covariance matrix (see section 8.2).</p><p>For x ∈ R d , define the spherized vector x * = Σ -1/2 w x. The between-class variance computed for spherized data is</p><formula xml:id="formula_814">Σ * b = Σ -1/2 w Σ b Σ -1/2 w . A direction is discriminant if it is close to the principal eigenvectors of Σ * b .</formula><p>This suggests the introduction of the norm</p><formula xml:id="formula_815">|x| 2 * = (x * ) T Σ * b x * = x T Σ -1/2 w (Σ -1/2 w Σ b Σ -1/2 w )Σ -1/2 w x = x T Σ -1 w Σ b Σ -1 w x.</formula><p>This replaces the standard Euclidean norm (the method can be made more robust by adding ϵId R d to Σ * b .)</p><p>Tangent distance Designing the distance, however, can sometimes be based on a priori knowledge on some invariance properties associated with the classes. A successful example comes from character recognition, where it is known that transforming images by slightly rotating, scaling, or translating the character should not change its class. This corresponds to the following general framework.</p><p>For each input x ∈ R d , assume that one can make small transformations without changing the class of x. We model these transformations as parametrized functions</p><formula xml:id="formula_816">x → x θ = ϕ(x, θ) ∈ R d , such that ϕ(x, 0) = x and ϕ is smooth in θ, which is a q- dimensional parameter.</formula><p>The assumption is that ϕ(x, θ) and x should be from the same class, at least for small θ. This will be used to improve on the Euclidean distance on R d .</p><p>Take x, x ′ ∈ R d . Ideally, one would like to use the distance</p><formula xml:id="formula_817">D(x, x ′ ) = inf θ,θ ′ dist(x θ , x θ ′ )</formula><p>where θ and θ ′ are restricted to a small neighborhood of 0. A more tractable expression can be based on first-order approximations</p><formula xml:id="formula_818">x θ ≃ x + ∇ θ ϕ(x, 0)u = x + q i=1 u i ∂ θ i ϕ(x, 0) and x ′ θ ≃ x ′ + ∇ θ ϕ(x ′ , 0)u ′ = x ′ + q i=1 u ′ i ∂ θ i ϕ(x ′ , 0)</formula><p>yielding the approximation (also called the tangent distance)</p><formula xml:id="formula_819">D(x, x ′ ) 2 ≃ inf u,u ′ ∈R q x -x ′ + ∇ θ ϕ(x, 0)u -∇ θ ϕ(x ′ , 0)u ′ 2 .</formula><p>The computation now is a simple least-squares problem, for which the solution is given by the system</p><formula xml:id="formula_820">∇ θ ϕ(x, 0) T ∇ θ ϕ(x, 0) -∇ θ ϕ(x, 0) T ∇ θ ϕ(x ′ , 0) -∇ θ ϕ(x ′ , 0) T ∇ θ ϕ(x, 0) ∇ θ ϕ(x ′ , 0) T ∇ θ ϕ(x ′ , 0) u v = ∇ θ ϕ(x, 0) T (x ′ -x) ∇ θ ϕ(x ′ , 0) T (x -x ′ ) .</formula><p>A slight modification, to ensure that the norms of u and u ′ are not too large, is to add a penalty λ(|u| 2 + |u ′ | 2 ), which results in adding λId R q to the diagonal blocs of the above matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 10</head><p>Tree-based Algorithms, Randomization and Boosting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Recursive Partitioning</head><p>Recursive partitioning methods implement a "divide and conquer" strategy to address the prediction problem. They separate the input space R X into small regions on which prediction is "easy," i.e., such that the observed values of the output variable are (almost) constant for input values in these regions. The regions are estimated by recursive divisions until they become either too small or homogeneous. These divisions are conveniently represented in the form of binary trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.1">Binary prediction trees</head><p>Define a binary node to be a structure ν that contains the following information (note that the definition is recursive):</p><p>• A label L(ν) that uniquely identifies the node.</p><p>• A set of children, C(ν), which is either empty or a pair of nodes (l(ν), r(ν)).</p><p>• A binary feature, i.e., a function γ ν : R X → {0, 1}, which is "None" (i.e., irrelevant) if the node has no children.</p><p>• A predictor, f ν : R X → R Y , which is "None" if the node has children.</p><p>A node without children is called a terminal node, or a leaf.</p><p>A binary prediction tree T is a finite set of nodes, with the following properties:</p><p>(i) Only one node has no parent (the root, denoted ρ or ρ T );</p><p>(ii) Each other node has exactly one parent;</p><p>(iii) No node is a descendent of itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.2">Training algorithm</head><p>Assume that a family Γ of binary features γ : R X → {0, 1} is chosen, together with a family F of predictors f : R X → R Y . Assume also the existence of two "algorithms" as follows:</p><p>• Feature selection: Given the feature set Γ and a training set T , return an optimized binary feature γ T ,Γ ∈ Γ .</p><p>• Predictor optimization: Given the predictor set F and a training set T , return an optimized predictor fT ,F ∈ F .</p><p>Finally, assume that a stopping rule is defined, as a function of training sets σ : T → σ (T ) ∈ {0, 1}, where 0 means "continue", and 1 means "stop".</p><p>Given a training set T 0 , the algorithm builds a binary tree T using a recursive construction. Each node ν ∈ T will be associated to a subset of T 0 , denoted T ν . We define below a recursive operation, denoted Node(T , j) that adds a node ν to a tree T given a subset T of T 0 and a label j. Starting with T = ∅, calling Node(T 0 , 0) will then create the desired tree.</p><p>Algorithm 10.1 (Node insertion: Node(T , j)) (a) Given T and j, let T ν = T and L(ν) = j.</p><formula xml:id="formula_821">(b) If σ (T ) = 1, let C(ν) = ∅, γ ν = "None" and f ν = fT ,F . (c) If σ (T ) = 0, let f ν = "None", γ ν = γ T ,Γ and C(ν) = (l(ν), r(ν)) with l(ν) = Node(T l , 2j + 1), r(ν) = Node(T r , 2j + 2)</formula><p>where</p><formula xml:id="formula_822">T l = {(x, y) ∈ T : γ ν (x) = 0}, T r = {(x, y) ∈ T : γ ν (x) = 1}</formula><p>(d) Add ν to T and return.</p><p>Remark 10.1 Note that, even though the learning algorithm for prediction trees can be very conveniently described in recursive form as above, efficient computer implementations should avoid recursive calls, which may be inefficient and memory demanding. Moreover, for large trees, it is likely that recursive implementations will reach the maximal number of recursive calls imposed by compilers. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.3">Resulting predictor</head><p>Once the tree is built, the predictor x → fT (x) is recursively defined as follows.</p><p>(a) Initialize the computation with ν = ρ.</p><p>(b) At a given step of the algorithm, let ν be the current node.</p><p>• If ν has no children: then let fT (x) = f ν (x).</p><p>• Otherwise: replace ν by l(ν) if γ ν (x) = 0 and by r(ν) if γ ν (x) = 1 and go back to (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.4">Stopping rule</head><p>The function σ , which decides whether a node is terminal or not is generally defined based on very simple rules. Typically, σ (T ) = 1 when one the following conditions is satisfied:</p><p>• The number of training examples in T is small (e.g., less than 5).</p><p>• The values y k in T have a small variance (regression) or are constant (classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.5">Leaf predictors</head><p>When one reaches a terminal node ν (so that σ (T ν ) = 1), a predictor f ν must be determined. This function can be optimized within any set F of predictors, using any learning algorithm, but in practice, one usually makes this fairly simple and defines F to be the family of constant functions taking values in R Y . The function fT ,F is then defined as:</p><p>• the average of the values of y k , for (x k , y k ) ∈ T (regression);</p><p>• the mode of the distribution of y k , for (x k , y k ) ∈ T (classification).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.6">Binary features</head><p>The space Γ of possible binary features must be specified in order to partition nonterminal nodes. A standard choice, used in the CART model <ref type="bibr" target="#b61">[43]</ref> with</p><formula xml:id="formula_823">R X = R d , is Γ = γ(x) = 1 [x (i) ≥θ] , i = 1, . . . , d, θ ∈ R (10.1)</formula><p>where x (i) is the ith coordinate of x. This corresponds to splitting the space using a hyperplane parallel to one of the coordinate axes.</p><p>The binary function γ T ,Γ can be optimized over Γ using a greedy evaluation of the risk, assuming that the prediction is based on the two nodes resulting from the split. For γ ∈ Γ , f 0 , f 1 ∈ F , define</p><formula xml:id="formula_824">F γ,f 0 ,f 1 (x) = f 0 (x) if γ(x) = 0 f 1 (x) if γ(x) = 1</formula><p>Given a risk function r, one then evaluates</p><formula xml:id="formula_825">E T (γ) = min f 0 ,f 1 ∈F (x,y)∈T r(y, F γ,f 0 ,f 1 (x))</formula><p>One then chooses γ T ,Γ = argmin γ∈Γ (E T (Γ )).</p><p>Example 10.2 (Regression) Consider the regression case, taking squared differences as risk and letting F contain only constant functions. Then</p><formula xml:id="formula_826">E T (γ) = min m 0 ,m 1 (x,y)∈T (y -m 0 ) 2 1 γ(x)=0 + (y -m 1 ) 2 1 γ(x)=1 .</formula><p>Obviously, the optimal m 0 and m 1 are the averages of the output values, y, in each of the subdomains defined by γ. For CART (see (10.1)), this cost must be minimized over all choices (i, θ) with i = 1, . . . , d and θ ∈ R where γ i,θ (x) = 1 if x(i) &gt; θ and 0 otherwise. ♦ Example 10.3 (Classification.) For classification, one can apply the same method, with the 0/1 loss, letting</p><formula xml:id="formula_827">E T (γ) = min g 0 ,g 1 (x,y)∈T 1 y g 0 1 γ(x)=0 + 1 y g 1 1 γ(x)=1 .</formula><p>The optimal g 0 and g 1 are the majority classes in T ∩ {γ = 0} and T ∩ {γ = 1}. ♦ Example 10.4 (Entropy selection for classification) For classification trees, other splitting criteria may be used based on the empirical probability p T on the set T , defined as</p><formula xml:id="formula_828">p T (A) = 1 N |{k : (x k , y k ) ∈ A}| for A ⊂ R X × R Y . The previous criterion, E T (γ), is proportional to p T (γ = 0)(1 -max g p T (g | γ = 0)) + p T (γ = 1)(1 -max g p T (g | γ = 1)).</formula><p>One can define alternative objectives in the form</p><formula xml:id="formula_829">p T (γ = 0)H(p T (g | γ = 0)) + p T (γ = 1)H(p T (g | γ = 1))</formula><p>where π → H(π) associates to a probability distribution π a "complexity measure" that is minimal when π is concentrated on a single class (which is the case for π → 1max g π(g)).</p><p>Many such measures exists, and many of them are defined as various forms of entropy designed in information theory. The most celebrated is Shannon's entropy <ref type="bibr" target="#b194">[176]</ref>, defined by</p><formula xml:id="formula_830">H(p) = - g∈R Y p(g) log p(g) .</formula><p>It is always positive, and minimal when the distribution is concentrated on a single class. Other entropy measures include:</p><p>• The Tsallis entropy: H(p) = 1 1-q g∈R Y (p(g) q -1), for q 1. (Tsallis entropy for q = 2 is sometimes called the Gini impurity index.)</p><p>• The Renyi entropy: H(p) = 1  1-q log g∈R Y p(g) q , for q ≥ 0, q 1. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1.7">Pruning</head><p>Growing a decision tree to its maximal depth (given the amount of available data) generally leads to predictors that overfit the data. The training algorithm is usually followed by a pruning step that removes some some nodes based on a complexity penalty.</p><p>Letting τ(T) denote the set of terminal nodes in the tree T and fT the associated predictor, pruning is represented as an optimization problem, where one minimizes, given the training set T ,</p><formula xml:id="formula_831">U λ (T, T ) = RT ( fT ) + λ|τ(T)|</formula><p>where RT is as usual the in-sample error measured on the training set T .</p><p>To prune a tree, one selects one or more internal nodes and remove all their descendants (so that these nodes become terminal). Associate to each node ν in T its local in-sample error E T ν equal to the error made by the optimal classifier estimated from the training data associated with ν. Then,</p><formula xml:id="formula_832">U λ (T, T ) = ν∈τ(T) |T ν | |T | E T ν + λ|τ(T)|</formula><p>If ν is a node in T (internal or terminal), let T ν be the subtree of T containing ν as a root and all its descendants. Let T (ν) be the tree T will all descendants of ν removed (keeping ν). Then</p><formula xml:id="formula_833">U λ (T, T ) = U 0 (T (ν) , T ) - |T ν | |T | (E T ν -U 0 (T ν , T ν )) + λ(|τ(T ν )| -1).</formula><p>Note also that, if ν is internal, and ν ′ , ν ′′ are its children, then</p><formula xml:id="formula_834">U 0 (T ν , T ν ) = |T ν ′ | |T ν | U 0 (T ν ′ , T ν ′ ) + |T ν ′′ | |T ν | U 0 (T ν ′′ , T ν ′′ )</formula><p>This formula can be used to compute U 0 (T ν ) recursively for all nodes, starting with leaves for which</p><formula xml:id="formula_835">U 0 (T ν ) = E(T ν ). (We also have |τ(T ν )| = |τ(T ν ′ )| + |τ(T ν ′′ )|.)</formula><p>The following algorithm converges to a global minimizer of U λ .</p><p>Algorithm 10.2 (Pruning)</p><p>(1) Start with a complete tree T(0) built without penalty.</p><p>(2) Compute, for all nodes U 0 (T ν ) and |τ(T ν )|. Let</p><formula xml:id="formula_836">ψ ν = |T ν | |T | (E T ν -U 0 (T ν )) -λ(|τ(T ν )| -1).</formula><p>(3) Iterate the following steps.</p><p>• If ψ ν &lt; 0 for all internal nodes ν, exit the program and return the current T(n).</p><p>• Otherwise choose an internal node ν such that ψ ν is largest.</p><formula xml:id="formula_837">• Let T(n + 1) = T (ν) (n). Subtract λ(|τ(T ν (n))| -1) to ρ ν ′ for all ν ′ ancestor of ν.</formula><p>10.2 Random Forests</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.1">Bagging</head><p>A random forest <ref type="bibr" target="#b25">[7,</ref><ref type="bibr" target="#b60">42]</ref> is a special case of composite predictors (we will see other examples later in this chapter when describing boosting methods) that train multiple individual predictors under various conditions and combine them, through averaging, or majority voting. With random forests, one generates individual trees by randomizing the parameters of the learning process. One way to achieve this is to randomly sample from the training set before running the training algorithm.</p><p>Letting as before T 0 = (x 1 , y 1 , . . . , x N , y N ) denote the original set, with size N , one can create "new" training data by sampling with replacement from T 0 . More precisely, consider the family of independent random variables ξ = (ξ 1 , . . . , ξ N ), with each ξ j following a uniform distribution over {1, . . . , N }. One can then form the random training set</p><formula xml:id="formula_838">T 0 (ξ) = (x ξ 1 , y ξ 1 , . . . , x ξ N , y ξ N ).</formula><p>Running the training algorithm using T 0 (ξ) then provides a random tree, denoted T(ξ). Now, by sampling K realizations of ξ, say ξ (1) , . . . , ξ (K) , one obtains a collection of K random trees (a random forest) T * = (T 1 , . . . , T K ), with T j = T(ξ (j) ) that can be combined to provide a final predictor. The simplest way to combine them is to average the predictors returned by each tree (assuming, for classification, that this predictor is a probability distribution on classes), so that</p><formula xml:id="formula_839">f T * (x) = 1 K K j=1 f T j (x). (10.2)</formula><p>For classification, one can alternatively let each individual tree "vote" for their most likely class.</p><p>Obviously, randomizing training data and averaging the predictors is a general approach that can be applied to any prediction algorithm, not only to decision trees. In the literature, the approach described above has been called bagging <ref type="bibr" target="#b59">[41]</ref>, which is an acronym for "bootstrap aggregating" (bootstrap itself being a general resampling method in statistics that samples training data with replacement to determine some properties of estimators). Another way to randomize predictors (especially when d, the input dimension is large), is to randomize input data by randomly removing some of the coordinates, leading to a similar construction.</p><p>With decision trees one can in addition randomize the binary features use to split nodes, as described next. While bagging may provide some enhancement to predictors, feature randomization for decision trees often significantly improves the performance, and is the typical randomization method used for random forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.2">Feature randomization</head><p>When one decides to split a node during the construction of a prediction tree, one can optimize the binary feature γ over a random subset of Γ rather than exploring the whole set. For CART, for example, one can select a small number of dimensions i 1 , . . . , i q ∈ {1, . . . , d} with q ≪ d, and optimize γ by thresholding one of the coordinates x (i j ) for j ∈ {1, . . . , q}. This results in a randomized version of the node insertion function. (c) If σ (T ′ ) = 0, sample (e.g., uniformly without replacement) a subset Γ ν of Γ and let</p><formula xml:id="formula_840">f ν = "None", γ ν = γT ,Γ ν and C(ν) = (l(ν), r(ν)) with l(ν) = Node(T l , 2j + 1) r(ν) = Node(T r , 2j + 2)</formula><p>where</p><formula xml:id="formula_841">T l = {(x, y) ∈ T : γ ν (x) = 0} T r = {(x, y) ∈ T : γ ν (x) = 1}</formula><p>(d) Add ν to T and return. Now, each time the function RNode(T 0 , 0) is run, it returns a different, random, tree. If it is called K times, this results in a random forest T * = (T 1 , . . . T K ), with a predictor F T * given by (10.2). Note that trees in random forests are generally not pruned, since this operation has been observed to bring no improvement in the context of randomized tress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Top-Scoring Pairs</head><p>Top-Scoring Pair (TSP) classifiers were introduced in Geman et al. <ref type="bibr" target="#b96">[78]</ref> and can be seen as forests formed with depth-one classification trees in which splitting rules are based on the comparison of pairs of variables. More precisely, define</p><formula xml:id="formula_842">γ ij (x) = 1 x (i) &gt;x (j) .</formula><p>A decision tree based on these rules only relies on the order between the features, and is therefore well adapted to situations in which the observations are subject to increasing transformations, i.e., when the observed variable X is such that X (j) = ϕ(Z (j) ), where ϕ : R → R is random and increasing and Z is a latent (unobserved) variable. Obviously, in such a case, order-based splitting rules do not depend on ϕ. Such an assumption is relevant, for example, when experimental conditions (such as temperature) may affect the actual data collection, without changing their order, which is the case when measuring high-throughput biological data, such as microarrays, for which the approach was introduced.</p><p>Assuming two classes, a depth-one tree in this context is simply the classifier f ij = γ ij . Given a training set, the associated empirical error is</p><formula xml:id="formula_843">E ij = 1 N N k=1 1 γ ij (x k ) y k = 1 N N k=1 |y k -γ ij (x k )| 10.4. ADABOOST</formula><p>and the balanced error (better adapted to situations in which one class is observed more often than the other) is</p><formula xml:id="formula_844">E b ij = N k=1 w k |y k -γ ij (x k )| with w k = 1/(2N y k )</formula><p>, where N 0 , N 1 are the number of observations with y k = 0, y k = 1. Pairs (i, j) with small errors are those for which the order between the features switch with high probability when passing from class 0 to class 1.</p><p>In its simplest form, the TSP classifier defines the set</p><formula xml:id="formula_845">P = argmin ij E b ij</formula><p>of global minimizers of the empirical error (which may just be a singleton) and predicts the class based on a majority vote among the family of predictors (f ij , (i, j) ∈ P ). Equivalently, selected variables maximize the score</p><formula xml:id="formula_846">∆ ij = 1 -E b ij , leading to the method's name.</formula><p>Such classifiers, which are remarkably simple, have been found to be competitive among a wide range of "advanced" classification algorithms for large-dimensional problems in computational biology. The method has been refined in Tan et al. <ref type="bibr" target="#b208">[190]</ref>, leading to the k-TSP classifier, which addresses the following remarks. First, when j, j ′ are highly correlated, and (i, j) is a high-scoring pair, then (i, j ′ ) is likely to be one too, and their associated decision rules will be redundant. Such cases should preferably be pruned from the classification rules, especially if one wants to select a small number of pairs. Second, among pairs of features that switch with the same probability, it is natural to prefer those for which the magnitude of the switch is largest, e.g., when the pair of variables switches from a regime in which one of them is very low and the other very high to the opposite. In Tan et al. <ref type="bibr" target="#b208">[190]</ref>, a rank-based tie-breaker is introduced, defined as</p><formula xml:id="formula_847">ρ ij = N k=1 w k (R k (i) -R k (j))(2y k -1), where R k (i) denotes the rank of x (i) k in x (1) k , . . . , x (d)</formula><p>k . One can now order pairs (i, j) and (i ′ , j ′ ) by stating that the former scores higher if (i)</p><formula xml:id="formula_848">∆ ij &gt; ∆ i ′ j ′ , or (ii) ∆ ij = ∆ i ′ j ′ and ρ ij &gt; ρ i ′ j ′ .</formula><p>The k-TSP classifier is formed by selecting pairs, starting from the highest scoring one, and use as lth pair (for l ≤ k) the highest scoring ones among all those that do not overlap with the previously selected ones. In <ref type="bibr" target="#b208">[190]</ref>, the value of k is optimized using cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Adaboost</head><p>Boosting methods refer to algorithms in which classifiers are enhanced by recursively making them focus on harder data. We first address the issue of classification, and describe one of the earliest algorithms (Adaboost). We will then interpret it as a greedy gradient descent algorithm, as this interpretation will lead to further extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.1">General set-up</head><p>We first consider binary classification problems, with R Y = {-1, 1}. We want to design a function x → F(x) ∈ {-1, 1} on the basis of a training set T = (x 1 , y 1 , . . . , x N , y N ). With the 0-1 loss, minimizing the empirical error is equivalent to maximizing</p><formula xml:id="formula_849">E T (F) = 1 N N k=1 y k F(x k ).</formula><p>Boosting algorithms build the function F as a linear combination of "base classifiers," f 1 , . . . , f M , taking</p><formula xml:id="formula_850">F(x) = sign         M j=1 α j f j (x)         .</formula><p>We assume that each base classifier, f j , takes values in [-1, 1] (the interval).</p><p>The sequence of base classifiers is learned by progressively focusing on the hardest examples. We will therefore assume that the training algorithm for base classifiers takes as input the training set T as well a family of positive weights W = (w 1 , . . . , w N ). More precisely, letting</p><formula xml:id="formula_851">p W (k) = w k N k=1 w k ,</formula><p>the weighted algorithm should implement (explicitly or implicitly) the equivalent of an unweighted algorithm on a simulated training set obtained by sampling with replacement K ≫ N elements of T according to p W (ideally letting K → ∞). Let us take a few examples.</p><p>• Weighted LDA: one can use LDA as described in section 8.2 with</p><formula xml:id="formula_852">c g = k:y k =g p W (k), µ g = 1 c g k:y k =g p W (k)x k , µ = g∈R Y c g µ g 10.4. ADABOOST</formula><p>and the covariance matrices:</p><formula xml:id="formula_853">Σ w = N k=1 p W (k)(x k -µ y k )(x k -µ y k ) T , Σ b = g∈R Y c g (µ g -μ)(µ g -μ) T .</formula><p>• Weighted logistic regression: just maximize</p><formula xml:id="formula_854">N k=1 p W (k) log π θ (y k |x k )</formula><p>where π θ is given by the logistic model.</p><p>• Empirical risk minimization algorithms can be modified in order to minimize</p><formula xml:id="formula_855">RT ,W (f ) = N k=1 w k r(y k , f (x k )).</formula><p>• Of course, any algorithm can be run on a training set resampled using p W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.2">The Adaboost algorithm</head><p>Boosting algorithms keep track of a family of weights and modify it after the jth classifier f j is computed, increasing the importance of misclassified examples, before computing the next classifier. The following algorithm, called Adaboost <ref type="bibr" target="#b190">[172,</ref><ref type="bibr" target="#b91">73]</ref>, describes one such approach.</p><p>Algorithm 10.4 (Adaboost)</p><p>• Start with uniform weights, letting W (1) = (w 1 (1), . . . , w N (1)) with w k (1) = 1/N , k = 1, . . . , N . Fix a number ρ ∈ (0, 1] and an integer M &gt; 0.</p><p>• Iterate, for j = 1, . . . , M:</p><p>(1) Fit a base classifier f j using the weights W (j) = (w 1 (j), . . . , w N (j)). Let</p><formula xml:id="formula_856">S + w (j) = N k=1 w k (j)(2 -|y k -f j (x k )|) (10.3a) S - w (j) = N k=1 w k (j)|y k -f j (x k )| (10.3b)</formula><p>and define α j = ρ log S + w (j)/S - w (j) (2) Update the weights by</p><formula xml:id="formula_857">w k (j + 1) = w k (j) exp α j |y k -f j (x k )|/2 .</formula><p>• Return the classifier:</p><formula xml:id="formula_858">F(x) = sign         M j=1 α j f j (x)         . If f j is binary, i.e., f j (x) ∈ {-1, 1}, then |y k -f j (x k )| = 21 y k f j (x k ) , so that S + W /2</formula><p>is the weighted number of correct classifications and S - W /2 is the weighted number of incorrect ones.</p><p>For α j to be positive, the jth classifier must do better than pure chance on the weighted training set. If not, taking α j ≤ 0 reflects the fact that, in that case, -f j has better performance on training data.</p><p>Algorithms that do slightly better than chance with high probability are called "weak learners" <ref type="bibr" target="#b190">[172]</ref>. The following proposition <ref type="bibr" target="#b91">[73]</ref> shows that, if the base classifiers reliably perform strictly better than chance (by a fixed, but not necessarily large, margin), then the boosting algorithm can make the training-set error arbitrarily close to 0.</p><p>Proposition 10.5 Let E T be the training set error of the estimator F returned by Algorithm 10.4, i.e.,</p><formula xml:id="formula_859">E T = 1 N N k=1 1 y k F(x k ) .</formula><p>Then</p><formula xml:id="formula_860">E T ≤ M j=1 ϵ ρ j (1 -ϵ j ) 1-ρ + ϵ 1-ρ j (1 -ϵ j ) ρ</formula><p>where</p><formula xml:id="formula_861">ϵ j = S - W (j) S + W (j) + S - W (j)</formula><p>. Proof We note that example k is misclassified by the final classifier if and only if</p><formula xml:id="formula_862">M j=1 α j y k f j (x k ) ≤ 0 or M j=1 e -α j y k f j (x k )/2 ≥ 1 10.4. ADABOOST 225 Noting that |y k -f j (x k )| = 1 -y k f j (x k ), we see that example k is misclassified when M j=1 e α j |y k -f j (x k )|/2 ≥ M j=1 e α j /2 .</formula><p>This shows that</p><formula xml:id="formula_863">E T = 1 N N k=1 1 y k F(x k ) = 1 N N k=1 1 M j=1 e α j |y k -f j (x k )|/2 ≥ M j=1 e α j /2 ≤ 1 N N k=1 M j=1 e α j |y k -f j (x k )|/2 M j=1 e -α j /2 .</formula><p>Let, for q ≤ M,</p><formula xml:id="formula_864">U q = 1 N N k=1 q j=1 e α j |y k -f j (x k )|/2 .</formula><p>Since</p><formula xml:id="formula_865">w k (q) = 1 N q-1 j=1 e α j |y k -f j (x k )|/2 ,</formula><p>we also have U q = N k=1 w k (q + 1) = (S + W (q + 1) + S - W (q + 1))/2.</p><p>We will use the inequality 1 1 This inequality is clear for α = 0. Assuming α 0, the difference between the upper and lower bound is</p><formula xml:id="formula_866">e αt ≤ 1 -(1 -e α )t,</formula><formula xml:id="formula_867">q(t) = 1 -e αt -(1 -e α )t.</formula><p>The function q is concave (its second derivative is -α 2 e αt ) with q(0) = q(1) = 0 and therefore nonnegative over [0, 1].</p><p>which is true for all α ∈ R and t ∈ [0, 1], to write</p><formula xml:id="formula_868">U q ≤ 1 N N k=1 q-1 j=1 e α j |y k -f j (x k )|/2 (1 -(1 -e α q )|y k -f q (x k )|/2) = N k=1 w k (q)(1 -(1 -e α q )|y k -f q (x k )|/2) = N k=1 w k (q) -(1 -e α q ) N k=1 w k (q)|y k -f q (x k )|/2 = U q-1 (1 -(1 -e α q )ϵ q )</formula><p>This gives (using U 0 = 1)</p><formula xml:id="formula_869">U M ≤ M j=1 1 -(1 -e α j ϵ j )</formula><p>and</p><formula xml:id="formula_870">E T ≤ M j=1 1 -(1 -e α j )ϵ j e -α j /2 .</formula><p>It now suffices to replace e α j by (1ϵ j ) ρ ϵ -ρ j and note that</p><formula xml:id="formula_871">1 -(1 -(1 -ϵ j ) ρ ϵ -ρ j )ϵ j (1 -ϵ j ) -ρ/2 ϵ ρ/2 j = ϵ ρ j (1 -ϵ j ) 1-ρ + ϵ 1-ρ j (1 -ϵ j ) ρ</formula><p>to conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>For ϵ ∈ [0, 1], one has</p><formula xml:id="formula_872">ϵ ρ (1 -ϵ) 1-ρ + ϵ 1-ρ (1 -ϵ) ρ = 1 -(ϵ ρ -(1 -ϵ) ρ )(ϵ 1-ρ -(1 -ϵ) -1-ρ ) ≤ 1</formula><p>with equality if and only if ϵ = 1/2, so that each term in the upper-bound reduces the error unless the corresponding base classifier does not perform better than pure chance. The parameter ρ determines the level at which one increases the importance of misclassified examples for the next step. Let S+ W (j) and S-W (j) denote the expressions in (10.3a) and (10.3b) with w k (j) replaced by w k (j + 1). Then, in the case when the base classifiers are binary, ensuring that |y kf j (x k )|/2 = 1 y k f j (x k ) , one can easily check that S+ W (j)/ S-W (j) = (S + W (j)/S - W (j)) 1-ρ . So, the ratio is (of course) unchanged if ρ = 0, and pushed to a pure chance level if ρ = 1. We provide below an interpretation of boosting as a greedy optimization procedure that will lead to the value ρ = 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.3">Adaboost and greedy gradient descent</head><p>We here restrict to the case of binary base classifiers and denote their linear combination by</p><formula xml:id="formula_873">h(x) = M j=1 α j f j (x).</formula><p>Whether an observation x is correctly classified in the true class y is associated to the sign of the product yh(x), but the value of this product also has an important interpretation, since, when it is positive, it can be thought of as a margin with which x is correctly classified.</p><p>Assume that the function F is evaluated, not only on the basis of its classification error, but also based on this margin, using a loss function of the kind</p><formula xml:id="formula_874">Ψ (h) = N k=1 ψ(y k h(x k )) (10.4)</formula><p>where ψ is decreasing. The boosting algorithm can then be interpreted as an classifier which incrementally improves this objective function.</p><p>Let, for j &lt; M, h (j) = j q=1 α q f q .</p><p>The next combination h (j+1) is equal to h (j) + α j+1 f j+1 , and we now consider the problem of minimizing, with respect to f j+1 and α j+1 , the function Ψ (h (j+1) ), without modifying the previous classifiers (i.e., performing a greedy optimization). So, we want to minimize, with respect to the base classifier f and to α ≥ 0, the function</p><formula xml:id="formula_875">U (α, f ) = N k=1 ψ y k h (j) (x k ) + αy k f (x k )</formula><p>Using the fact that f is a binary classifier, this can be written</p><formula xml:id="formula_876">U (α, f ) = N k=1 ψ(y k h (j) (x k ) + α)1 y k = f (x k ) + N k=1 ψ(y k h (j) (x k ) -α)1 y k f (x k ) (10.5) = N k=1 (ψ(y k h (j) (x k ) -α) -ψ(y k h (j) (x k ) + α))1 y k f (x k ) + N k=1 ψ(y k h (j) (x k ) + α).</formula><p>This shows that α and f have inter-dependent optimality conditions. For a given α, the best classifier f must minimize a weighted empirical error with non-negative weights (since ψ is decreasing)</p><formula xml:id="formula_877">w k = ψ(y k h (j) (x k ) -α) -ψ(y k h (j) (x k ) + α).</formula><p>Given f , α must minimize the expression in <ref type="bibr">(10.5)</ref>. One can use an alternative minimization procedure to optimize both f (as a weighted basic classifier) and α. However, for the special choice ψ(t) = e -t , this optimization turns out to only require one step.</p><p>In this case, we have</p><formula xml:id="formula_878">U (α, f ) = N k=1 (e α -e -α )e -y k h (j) (x k ) 1 y k f (x k ) + e -α N k=1 e -y k h (j) (x k ) = e -α (j) (e α -e -α ) N k=1 w k (j)1 y k f (x k ) + e -α (j) e -α N k=1 w k (j) with w k (j +1) = e α (j) -y k h (j) (x k ) and α (j) = α 1 +• • •+α j . This shows that f should minimize N k=1 w k (j + 1)1 y k f (x k ) .</formula><p>We note that</p><formula xml:id="formula_879">w k (j + 1) = w k (j)e α j (1-y k f j (x k )) = w k (j)e α j |y k -f k (x k )| ,</formula><p>which is identical to the weight updates in algorithm Algorithm 10.4 (this is the reason why the term α (j) was introduced in the computation). The new value of α must minimize (using the notation of Algorithm 10.4)</p><formula xml:id="formula_880">e -α S + W (j) + e α S - W (j), which yields α = 1 2 log S + W (j)/S - W (j)</formula><p>. This is the value α j+1 in Algorithm 10.4 with ρ = 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">Gradient boosting and regression 10.5.1 Notation</head><p>The boosting idea, and in particular its interpretation as a greedy gradient procedure, can be extended to non-linear regression problems <ref type="bibr" target="#b93">[75]</ref>. Let us denote by F 0 the set of base predictors, therefore functions from R X = R d to R Y = R q , since we are considering regression problems. The final predictor is a linear combination</p><formula xml:id="formula_881">F(x) = M j=1 α j f j (x)</formula><p>with α 1 , . . . , α M ∈ R and f 1 , . . . , f M ∈ F 0 . Note that the the coefficients α j are redundant when the class F 0 is invariant by multiplication by a scalar. Replacing if needed F 0 by {f = αg, α ∈ R, g ∈ F 0 }, we will assume that this property holds and therefore remove the coefficients α j from the problem.</p><p>In accordance with the principle of performing greedy searches, we let</p><formula xml:id="formula_882">F (j) (x) = j q=1 f q (x),</formula><p>and consider the problem of minimizing over f ∈ F 0 ,</p><formula xml:id="formula_883">U (f ) = N k=1 r(y k , F (j) (x k ) + f (x k )),</formula><p>where T = (x 1 , y 1 , . . . , x N , y N ) is the training data and r is the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.2">Translation-invariant loss</head><p>In the case, which is frequent in regression, when r(y, y ′ ) only depends on yy ′ , the problem is equivalent to minimizing</p><formula xml:id="formula_884">U (f ) = N k=1 r(y k -F (j) (x k ), f (x k )),</formula><p>i.e., to let f j+1 be the optimal predictor (in F 0 and for the loss r) of the residuals</p><formula xml:id="formula_885">y (j) k = y k -F (j) (x k ).</formula><p>In this case, this provides a conceptually very simple algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 10.5 (Gradient boosting for regression with translation-invariant loss)</head><p>• Let T = (x 1 , y 1 , . . . , x N , y N ) be a training set and r a loss function such that r(y, y ′ ) only depends on yy ′ .</p><p>• Let F 0 be a function class such that f ∈ F 0 ⇒ αf ∈ F 0 for all α ∈ R.</p><p>• Select an integer M &gt; 0 and let F (0) = 0, y</p><formula xml:id="formula_886">(0) k = y k , k = 1, . . . , N . • For j = 1, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , M:</head><p>(1) Find the optimal predictor f j ∈ F 0 for the training set (x 1 , y (j-1) 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, . . . , x N , y (j-1) N</head><p>).</p><p>(2) Let y</p><formula xml:id="formula_887">(j) k = y (j-1) k -f j (x k ) • Return F = M k=1 f j .</formula><p>Remark 10.6 Obviously, the class F 0 should not be a linear class for the boosting algorithm to have any effect. Indeed, if f , f ′ ∈ F 0 implies f +f ′ ∈ F 0 , no improvement could be made to the predictor after the first step. ♦</p><p>A successful example of this algorithm uses regression trees as base predictors. Recall that the functions output by such trees take the form</p><formula xml:id="formula_888">f (x) = A∈C w A 1 x∈A</formula><p>where C is a finite partition of R d . Each set in the partition is specified by the value taken by a finite number of binary features (denoted by γ in our discussion of prediction trees) and the maximal number of such features is the depth of the tree. We assume that the set Γ of binary features is shared by all regression trees in F 0 , and that the depth of these trees is bounded by a fixed constant. These restrictions prevent F 0 from forming a linear class. <ref type="foot" target="#foot_12">2</ref> Note that the maximal depth of tree learnable from a finite training set is always bounded, since such trees cannot have more nodes than the size of the training set (but one may want to restrict the maximal depth of base predictors to be way less than N ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.3">General loss functions</head><p>We now consider situations in which the loss function is not necessarily a function of the difference between true and predicted output. We are still interested in the problem of minimizing U (f ), but we now approximate this problem using the firstorder expansion</p><formula xml:id="formula_889">U (f ) = N k=1 r(y k , F (j) (x k )) + N k=1 ∂ 2 r(y k , F (j) (x k )) T f (x k ) + o(f ),</formula><p>where ∂ 2 r denotes the derivative of r with respect to its second variable. This suggests (similarly to gradient descent) to choose f such that f</p><formula xml:id="formula_890">(x k ) = -α∂ 2 r(y k , F (j) (x k ))</formula><p>for some α &gt; 0 and all k = 1, . . . , N . However, such an f may not exist in the class F 0 , and the next best choice is to pick f = α f with f minimizing</p><formula xml:id="formula_891">N k=1 | f (x k ) + ∂ 2 r(y k , F (j) (x k ))| 2</formula><p>over all f ∈ F 0 . This is similar to projected gradient descent in optimization, and α such that f = α f should minimize</p><formula xml:id="formula_892">N k=1 r(y k , F (j) (x k ) + α f (x k )).</formula><p>This provides a generic "gradient boosting" algorithm <ref type="bibr" target="#b93">[75]</ref>, summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 10.6 (Gradient boosting)</head><p>• Let T = (x 1 , y 1 , . . . , x N , y N ) be a training set and r a differentiable loss function.</p><p>• Let F 0 be a function class such that f ∈ F 0 ⇒ αf ∈ F 0 for all α ∈ R.</p><p>• Select an integer M &gt; 0 and let F (0) = 0.</p><p>• For j = 1, . . . , M:</p><formula xml:id="formula_893">(1) Find fj ∈ F 0 minimizing N k=1 | f (x k ) + ∂ 2 r(y k , F (j-1) (x k ))| 2 over all f ∈ F 0 .</formula><p>(2) Let f j = α j fj where α j minimizes</p><formula xml:id="formula_894">N k=1 r(y k , F (j-1) (x k ) + α fj (x k )).</formula><p>(3) Let F (j) = F (j-1) + f j .</p><p>• Return F = F (M) .</p><p>Remark 10.7 Importantly, the fact that F 0 is stable by scalar multiplication implies that the function fj satisfies</p><formula xml:id="formula_895">N k=1 f (x k ) T ∂ 2 r(y k , F (j-1) (x k )) ≤ 0,</formula><p>♦ that is, excepted in the unlikely case in which the above sum is zero, it is a direction of descent for the function U (because one could otherwise replace fj by -fj and improve the approximation of the gradient).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.4">Return to classification</head><p>A slight modification of this algorithm may also be applied to classification, provided that the classifier f is obtained by learning the conditional distribution, denoted g → p(g|x), of the output variable (assumed to take values in a finite set R Y ) given the input (assumed to take values in R X = R d ).</p><p>Our goal is to estimate an unknown target conditional distribution, µ, therefore taking the form µ(g|x) for g ∈ R Y and x ∈ R d . We assume that a family µ k , k = 1, . . . , N of distributions on the set R Y is observed, where each µ k is assumed to be an approximation of the unknown µ(•|x k ) (typically, µ k (g) = 1 g=y k , i.e., µ k = δ y k ). The risk function must take the form r(µ, µ ′ ) where µ, µ ′ ∈ S(R Y ), the set of probability distributions on R Y . We will work with</p><formula xml:id="formula_896">r(µ, µ ′ ) = - g∈R Y µ(g) log µ ′ (g). One can note that r(µ, µ ′ ) = KL(µ∥µ ′ ) + r(µ, µ),</formula><p>which is therefore minimal when µ ′ = µ. Moreover, in the special case</p><formula xml:id="formula_897">µ k = δ y k , the empirical risk is R(p) = N k=1 r(µ k , p(•|x k )) = - N k=1 log p(y k |x k ),</formula><p>so that minimizing it is equivalent to maximizing the conditional likelihood that was used for logistic regression.</p><p>Before applying the previous algorithm, one must address the issue that probability distributions do not form a vector space, and cannot be added to form new probability distributions. In Friedman <ref type="bibr" target="#b93">[75]</ref>, Hastie et al. <ref type="bibr" target="#b105">[87]</ref>, it is suggested to use the representation, which can be associated with any function</p><formula xml:id="formula_898">F : (g, x) → F(g|x) ∈ R, p F (g|x) = e F(g|x)</formula><p>h∈R Y e F(h|x) . Because the representation if not unique (p F = p F ′ if F -F ′ only depends on x), we will require in addition that h∈R Y F(h|x) = 0 for all x ∈ R d . The space formed by such functions F is now linear, and we can consider the empirical risk</p><formula xml:id="formula_899">R(F) = - N k=1 g∈R Y µ k (g) log p F (g|x k ) = - N k=1 g∈R Y µ k (g)F(g|x k ) + N k=1 log         g∈R Y e F(g|x k )         .</formula><p>One can evaluate the derivative of this risk with respect to a change on F(g|x k ), and a short computation gives</p><formula xml:id="formula_900">∂R ∂F(g|x k ) = - N k=1 (µ k (g) -p F (g|x k )).</formula><p>Now assume that a basic space F 0 of functions f : (g, x) → f (g|x) is chosen, such that all function in F 0 satisfy g∈R Y f (g|x) = 0 for all x ∈ R d . The gradient boosting algorithm then requires to minimize (in Step (1)):</p><formula xml:id="formula_901">N k=1 g∈R Y (µ k (g) -p F (j-1) (g|x k ) -f (g|x k )) 2</formula><p>with respect to all functions f ∈ F 0 . Given the optimal fj , the next step requires to minimize, with respect to α ∈ R:</p><formula xml:id="formula_902">-α N k=1 g∈R Y µ k (g) fj (g|x k ) + N k=1 log         g∈R Y e F (j-1) (g|x k )+α fj (g|x k )        </formula><p>. This is a scalar convex problem that can be solved, e.g., using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5.5">Gradient tree boosting</head><p>We now specialize to the situation in which the set F 0 contains regression trees. In this situation, the general algorithm can be improved by taking advantage of the fact that the predictors returned by such trees are piecewise constant functions, where the regions of constancy are associated with partitions C of R d defined by the leaves of the trees. In particular, fj (x) in Step (1) takes the form</p><formula xml:id="formula_903">fj (g|x) = J A∈C fj,A (g)1 x∈A .</formula><p>The final f at Step (2) should therefore take the form A∈C α fj,A (g)1 x∈A but not much additional complexity is introduced by freely optimizing the values of f j on A, that is, by looking at f in the form</p><formula xml:id="formula_904">A∈C f j,A (g)1 x∈A</formula><p>where the values f j,A (g) optimize the empirical risk. This risk becomes</p><formula xml:id="formula_905">- N k=1 A∈C g∈R Y µ k (g)f j,A (g)1 x k ∈A + N k=1 A∈C log         g∈R Y e F (j-1) (g|x k )+f j,A (g)         1 x k ∈A .</formula><p>The values f j,A (g), g ∈ R Y can therefore be optimized separately, minimizing</p><formula xml:id="formula_906">- k=1:x k ∈A g∈R Y µ k (g)f j,A (g) + k:x k ∈A log         g∈R Y e F (j-1) (g|x k )+f j,A (g)         1 x k ∈A .</formula><p>This is still a convex program, which has to be run at every leaf of the optimized tree. If computing time is limited (or for large-scale problems), the determination of f j,A (g) may be restricted to one step of gradient descent starting at f j,A = 0. A simple computation indeed shows that the first derivative of the function above with respect to f j,A (g) is</p><formula xml:id="formula_907">a A (g) = - k:x k ∈A (µ k (g) -p F (g|x k )).</formula><p>The derivative of this expression with respect to f j,A (g) (for the same g)</p><formula xml:id="formula_908">is b A (g) = k:x k ∈A p F (g|x k )(1 -p F (g|x k )).</formula><p>The off-diagonal terms in the second derivative are, for g h,</p><formula xml:id="formula_909">- k:x k ∈A p F (g|x k )p F (h|x k ).</formula><p>In Friedman et al. <ref type="bibr" target="#b92">[74]</ref>, it is suggested to use an approximate Newton step, where the off-diagonal terms in the second derivative are neglected. This corresponds to minimizing</p><formula xml:id="formula_910">g∈R Y a A (g)f j,A (g) + 1 2 g∈R Y b A (g)f j,A (g) 2 .</formula><p>The solution is (introducing a Lagrange multiplier for the constraint g f j,A (g) = 0)</p><formula xml:id="formula_911">f j,A (g) = - a A (g) -λ b A (g) with λ = g∈R Y a A (g)/b A (g) g∈R Y 1/b A (g)</formula><p>.</p><p>A small value ϵ can be added to b A to avoid divisions by zero. We refer the reader to Friedman et al. <ref type="bibr" target="#b92">[74]</ref>, Friedman <ref type="bibr" target="#b93">[75]</ref>, Hastie et al. <ref type="bibr" target="#b105">[87]</ref> for several variations on this basic idea. Note that an approximate but highly efficient implementation of boosted trees, called XGBoost, has been developed in Chen and Guestrin <ref type="bibr" target="#b71">[53]</ref>.</p><p>Chapter 11</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterated Compositions of Functions and Neural Nets 11.1 First definitions</head><p>We now discuss a class of methods in which the predictor f is built using iterated compositions, with a main application to neural nets. We will structure these models using directed acyclic graphs (DAG). These graphs are composed with a set of vertexes (or nodes) V = {0, . . . , m + 1} and a collection L of directed edges i → j between some vertexes. If an edge exists between i and j, one says that i is a parent of j and j a child of i and will use the notation pa(i) (resp. ch(i)) denote the set of parents (resp. children) of i. The graphs we consider must satisfy the following conditions:</p><p>(i) No index is a descendant of itself, i.e., that the graph is acyclic.</p><p>(ii) The only index without parent is i = 0 and the only one without children in i = m + 1.</p><p>To each node i in the graph, one associates a dimension d i and a variable z i ∈ R d i .</p><p>The root node variable, z 0 = x, is the input and z m+1 is the output. One also associates to each node i 0 a function ψ i defined on the product space j∈pa(i) R d j and taking values in R d i . The input-output relation is then defined by the family of equations:</p><formula xml:id="formula_912">z i = ψ i (z pa(i) )</formula><p>where z pa(i) = (z j , j ∈ pa(i)). Since there is only one root and one terminal node, these iterations implement a relationship y = z m+1 = f (x), with z 0 = x. We will refer to the z 1 , . . . , z m as the latent variables of the network.</p><p>Each function ψ i is furthermore parametrized by an s i -dimensional vector w i ∈ R s i , so that we will write</p><formula xml:id="formula_913">z i = ψ i (z pa(i) ; w i ).</formula><p>We let W denote the vector containing all parameters w 1 , . . . , w m+1 , which therefore has dimension s = s 1 + • • • + s m+1 . The network function f is then parametrized by W and we will write y = f (x; W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Neural nets 11.2.1 Transitions</head><p>Most neural networks iterate functions taking the form (1 +   j∈pa(i) d j )-dimensional); ρ is defined on and takes values in R, and we make the abuse of notation, for any d and</p><formula xml:id="formula_914">ψ i (z; w) = ρ(bz + β 0 ), z ∈ R d j where b is a d i × ( j∈pa(i) d j ) matrix and β 0 ∈ R d i (so that w = (b, β 0 ) is s i = d i</formula><formula xml:id="formula_915">u ∈ R d ρ(u) =           ρ(u (1) ) . . . ρ(u (d) )           .</formula><p>The most popular choice for ρ is the positive part, or ReLU (for rectified linear unit), given by ρ(t) = max(t, 0). Other common choices are ρ(t) = 1/(1 + e -t ) (sigmoid function), or ρ(t) = tanh(z).</p><p>Residual neural networks (or ResNets <ref type="bibr" target="#b107">[89]</ref>) are discussed in section 11.6. They iterate transitions between inputs and outputs of same dimension, taking</p><formula xml:id="formula_916">z i+1 = z i + ψ(z i ; w). (11.1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.2">Output</head><p>The last node of the graph provides the prediction, y. Its expression depends on the type of predictor that is learned</p><p>• For regression, y can be chosen as an affine function of is its parents: z m+1 = bz pa(m+1) + a 0 .</p><p>• For classification, one can also use a linear model z m+1 = bz pa(m+1) + a 0 where z m+1 is q-dimensional and let the classification be argmax(z (i) m+1 , i = 1, . . . , q). Alternatively, one uses "softmax" transformation, with</p><formula xml:id="formula_917">z (i) m+1 = e ζ (i) m+1 q j=1 e ζ (j) m+1 with ζ m+1 = bz pa(m+1) + a 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3.">GEOMETRY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.3">Image data</head><p>Neural networks have achieved top performance when working with organized structures such as images. A typical problem in this setting is to categorize the content of the image, i.e., return a categorical variable naming its principal element(s). Other applications include facial recognition or identification. In this case, the transition function can take advantage of the 2D structure, with some special terminology.</p><p>Instead of speaking of the total dimension, say, d, of the considered variables, writing z = (z (1) , . . . , z (d) ), images are better represented with three indices z(u, v, λ) where u = 1, . . . , U and U is the width of the image, v = 1, . . . , V and V is the height of the image, λ = 1, . . . , Λ and Λ is the depth of the image. (With this notation d = U V Λ.) Typical images have length and width of one or two hundred pixels, and depth Λ = 3 for the three color channels. This three-dimensional structure is conserved also for latent variables, with different dimensions. Deep neural networks often combine compression in width and height with expansion in depth while transitioning from input to output.</p><p>The linear transformation b mapping one layer with dimensions U i , V i , Λ i to another with dimensions U i+1 , V i+1 , Λ i+1 is then preferably seen as a collection of numbers: b(u ′ , v ′ , λ ′ , u, v, Λ) so that the transition from z i to z i+1 is given by</p><formula xml:id="formula_918">z i+1 (u ′ , v ′ , λ ′ ) = ρ         β 0 (u ′ , v ′ , λ ′ ) + U i u=1 V i v=1 Λ i λ=1 b(u ′ , v ′ , λ ′ , u, v, λ)z i (u, v, λ)         .</formula><p>For images, it is often preferable to use convolutional transitions, providing convolutional neural networks ( <ref type="bibr" target="#b134">[116,</ref><ref type="bibr" target="#b133">115]</ref>, or CNNs. If U i = U i+1 and V i = V i+1 , such a transition requires that b(u ′ , v ′ , λ ′ , u, v, λ) only depends on λ, λ ′ and on the differences u ′ -u and v -v ′ . In general, one also requires that b(u ′ , v ′ , λ ′ , u, v, λ) is non-zero only if |u ′ -u| and |v ′ -v| are both less than a constant, typically a small number. Also, there is generally little computation across depths: each output at depth λ ′ only uses values from a single input depth. These restrictions obviously reduce dramatically the number of free parameters involved in the transition.</p><p>After one or a few convolutions, the dimension is often reduced by a "pooling" operation, dividing the image into small non-overlapping windows and replacing each such window by a single value, either the max (max-pooling) or the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Geometry</head><p>In addition to the transitions between latent variables and resulting changes of dimension, the structure of the DAG defining the network is an important element in the design of a neural net. The simplest choice is a purely linear structure (as shown in Figure <ref type="figure" target="#fig_120">11</ref>.1), as was, for example, used for image categorization in <ref type="bibr" target="#b128">[110]</ref>.</p><p>More complex architectures have been introduced in recent years. Their design is in a large part heuristic and based on an analysis of the kind of computation that should be done in the network to perform a particular task. For example, an architecture used for image segmentation in summarized in fig. <ref type="figure" target="#fig_120">11.</ref>2.</p><p>An important feature of neural nets is their modularity, since "simple" architectures can be combined (e.g., by placing the output of a network as input of another one) and form a more complex network that still follows the basic structure defined above. One example of such a building block is the "attention module," which take as input three vectors Q, K, V (for query, key, and value) and return softmax(QK T )V .</p><p>These modules are fundamental elements of "transformer networks" <ref type="bibr" target="#b216">[198]</ref>, that are used, among other tasks, for automatic translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4">Objective function 11.4.1 Definitions</head><p>We now return to the general form of the problem, with variables z 0 , . . . , z m+1 satisfying</p><formula xml:id="formula_919">z i = ψ i (z pa(i) ; w i ) Let T = (x 1 , y 1 , . . . , x N , y N ) denote the training data.</formula><p>For regression problems, the objective function minimized by the algorithm is typically the empirical risk, the simplest choice being the mean square error, which gives</p><formula xml:id="formula_920">F(W ) = 1 N N k=1 |y k -z k,m+1 (W )| 2 . with z k;m+1 (W ) = f (x k ; W ).</formula><p>For classification, with the dimension of the output variable equal to the number of classes and the decision based on the largest coordinate, one can take (letting z k,m+1 (i; W ) denote the ith coordinate of z k,m+1 (W )):</p><formula xml:id="formula_921">F(W ) = 1 N N k=1        -z k,m+1 (y k ; W ) + log q i=1 exp(z k,m+1 (i; W ))        .</formula><p>This objective function is similar to that minimized in logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4.2">Differential</head><p>General computation. The computation of the differential of F with respect to W may look daunting, but it has actually a simple structure captured by the backpropagation algorithm. Even if programming this algorithm can often be avoided by using an automatic differentiation software, it is important to understand how it works, and why the implementation of gradient-descent algorithms remains feasible.</p><p>Consider the general situation of minimizing a function G(W , z) over W ∈ R s and z ∈ R r , subject to a constraint γ(W , z) = 0 where γ is defined on R s × R r and takes values in R r (here, it is important that the number of constraints is equal to the dimension of z). We will denote below by ∂ W and ∂ z the derivatives of these functions with respect to the multi-dimensional variables W and z. We make the assumptions that ∂ z γ, which is an r × r matrix, is invertible, and that the constraints can be solved to express z as a function of W , that we will denote Z (W ). This allows us to define the function F(W ) = G(W , Z (W )) and we want to compute the gradient of F. (Clearly, the function F in the previous section satisfies these assumptions). Taking h ∈ R s , we have</p><formula xml:id="formula_922">dF(W )h = ∂ W Gh + ∂ z G dZ h.</formula><p>Moreover, since γ(W , Z ) = 0 by definition of Z , we have</p><formula xml:id="formula_923">∂ W γh + ∂ z γ dZ h = 0, so that dF(W )h = ∂ W Gh -∂ z G ∂ z γ -1 ∂ W γh.</formula><p>Let p ∈ R r be the solution of the linear system</p><formula xml:id="formula_924">∂ z γ T p = ∂ z G T . Then, dF(W )h = (∂ W G -p T ∂ W γ)h or ∇F = ∂ W G T -∂ W γ T p.</formula><p>Note that, introducing the "Hamiltonian"</p><formula xml:id="formula_925">H (p, z, W ) = p T γ(W , z) -G(W , z),</formula><p>one can summarize the previous computation with the system</p><formula xml:id="formula_926">           ∂ p H = 0 ∂ z H = 0 ∇F = -∂ W H T .</formula><p>Application: back-propagation. In our case, we are minimizing a function of the form</p><formula xml:id="formula_927">G(W , z 1 , . . . , z N ) = 1 N N k=1 r(y k , z k,m+1 ) subject to constraints z k,i+1 = ψ i (z k,pa(i) ; w i ), i = 0, . . . , m, z k,0 = x k .</formula><p>We focus on one of the terms in the sum, therefore fixing k, that we will temporarily drop from the notation.</p><p>So, we evaluate the gradient of G(W , z) = r(y, z m+1 ) with z i+1 = ψ i (z pa(i) ; w i ), i = 0, . . . , m, z 0 = x. With the notation of the previous paragraph, we take γ = (γ 1 , . . . , γ m+1 ) with</p><formula xml:id="formula_928">γ i (W , z) = ψ i (z pa(i) ; w i ) -z i</formula><p>These constraints uniquely define z as a function of W , which was one of our assumptions. For the derivative, we have, for u = (u 1 , . . . , u m+1 ) ∈ R r (with</p><formula xml:id="formula_929">r = d 1 + • • • + d m+1 , u i ∈ R d i )</formula><p>, and for i = 1, . . . , m + 1</p><formula xml:id="formula_930">∂ z γ i u = j∈pa(i) ∂ z j ψ i (z pa(i) ; w i )u j -u i</formula><p>Taking p = (p 1 , . . . , p m+1 ) ∈ R r , we get</p><formula xml:id="formula_931">p T ∂ z γu = m+1 i=1 j∈pa(i) p T i ∂ z j ψ i (z pa(i) ; w i )u j - m+1 i=1 p T i u i = m+1 j=1 i∈ch(j) p T i ∂ z j ψ i (z pa(i) ; w i )u j - m+1 j=1 p T j u j</formula><p>This allows us to identify ∂ z γ T p as the vector g = (g 1 , . . . , g m+1 ) with</p><formula xml:id="formula_932">g j = i∈ch(j) ∂ z j ψ i (z pa(i) ; w i ) T p i -p j .</formula><p>For j = m + 1 (which has no children), we get g m+1 = -p m+1 , so that the equation ∂ z γ T p = g can be solved recursively by taking p m+1 = -g m+1 and propagating backward, with</p><formula xml:id="formula_933">p j = -g j + i∈ch(j) ∂ z j ψ i (z pa(i) ; w i ) T p i for j = m, . . . , 1.</formula><p>To compute the gradient of G, the propagation has to be applied to g = ∂ z G. Since G only depends on z m+1 , we have g m+1 = ∂ z m+1 r(y, z m+1 ) and g j = 0 for j = 1, . . . , m. Moreover, G does not depend on W , so that ∂ W G = 0. We have</p><formula xml:id="formula_934">∂ W γ i = ∂ w i ψ i (z pa(i) , w i ) yielding ∂ W γ T p = (ζ 1 , . . . , ζ m ) with ζ j = ∂ w j ψ j (z pa(j) , w j ) T p j .</formula><p>We can now formulate an algorithm that computes the gradient of F with respect to W , reintroducing training data indexes in the notation. Algorithm 11.1 (Back-propagation) Let (x 1 , y 1 , . . . , x N , y N ) be the training set and R k (z) = r(y k , z) so that</p><formula xml:id="formula_935">F(W ) = 1 N N k=1 R k (z k,m+1 (W )) with z k,m+1 (W ) = f (x k , W ).</formula><p>Let W be a family of weights. The following steps compute ∇F(W ).</p><p>1. For all k = 1, . . . , N and all i = 1, . . . , m + 1, compute z k,i (W ) (forward computation through the network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Initialize variables</head><formula xml:id="formula_936">p k,m+1 = -∇R k (z k,m+1 (W )), k = 1, . . . , N .</formula><p>3. For all k = 1, . . . , N and all j = 1, . . . , m, compute p k,j using iterations</p><formula xml:id="formula_937">p k,j = i∈ch(j) ∂ z j ψ i (z k,pa(i) , w i ) T p k,i . 4. Let ∇F(W ) = - 1 N N k=1 m+1 i=1 D T i ∂ w i ψ i (z k,pa(i) , w i ) T p k,i ,</formula><p>where D i is the s i × s matrix such that D i h = h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.4.3">Complementary computations</head><p>The back-propagation algorithm requires the computation of the gradient of the costs R k and of the derivatives of the functions ψ i , and this can generally be done in closed form, with relatively simple expressions.</p><formula xml:id="formula_938">• If R k (z) = |y k -z| 2 (which is the typical choice for regression models) then ∇R k (z) = 2(z -y k ). • In classification, with R k (z) = -z(y k ) + log q i=1 exp(z (i) ) , one has ∇R k (z) = -u y k + exp(z) q i=1 exp(z (i) )</formula><p>where u y k ∈ R d is the vector with 1 at position y k and zero elsewhere, and exp(z) is the vector with coordinates exp(z (i) ), i = 1, . . . , d.</p><p>• For dense transition functions in the form ψ(z; w) = ρ(bz</p><formula xml:id="formula_939">+ β 0 ) with w = (β 0 , b), then ∂ z ψ(z, w) = diag(ρ ′ (β 0 + bz))b so that ∂ z ψ(z, w) T p = b T diag(ρ ′ (β 0 + bz))p • Similarly ∂ w ψ(z, w) T p = diag(ρ ′ (β 0 + bz))p, diag(ρ ′ (β 0 + bz))pz T .</formula><p>Note that neural network packages implement these functions (and more) automatically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5">Stochastic Gradient Descent</head><formula xml:id="formula_940">= (ξ 1 , . . . , ξ N ) such that ξ k ∈ {0, 1} and N k=1 ξ k = ℓ. Define H(W , ξ) = ∇ W        1 ℓ N k=1 ξ k r(y k , f (x k , W ))        = 1 ℓ N k=1 ξ k ∇ W r(y k , f (x k , W ))</formula><p>where ξ follows the uniform distribution on B ℓ . Consider the stochastic approximation algorithm:</p><formula xml:id="formula_941">W n+1 = W n -γ n+1 H(W n , ξ n+1 ). (11.2) Because E(ξ k ) = ℓ/N , we have E(H(W , ξ)) = ∇ W E T (f (•, W )) and (11.</formula><p>2) provides a stochastic gradient descent algorithm to which the discussion in section 3.3 applies. Such an approach is often referred to as "mini-batch" selection in the deep-learning literature, since it correspond to sampling ℓ examples from the training set without replacement and only computing the gradient of the empirical loss computed from these examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.5.2">Dropout</head><p>Introduced for deep learning in Srivastava et al. <ref type="bibr" target="#b199">[181]</ref>, "dropout" is a learning paradigm that brings additional robustness (and, maybe, reduces overfitting risks) to massively parametrized predictors.</p><p>Assume that a random perturbation mechanism of the model parameters has been designed. We will represent it using a random variable η (interpreted as noise) and a transformation W ′ = ϕ(W , η) describing how η affects a given weight configuration W to form a perturbed one W ′ . In order to shorten notation, we will write ϕ(W , η) = η • W , borrowing the notation for a group action from group theory. As a typical example, η can be chosen as a vector of Bernoulli random variables (therefore taking values in {0,1}), with same dimension as W and one can simply let η •W = η ⊙W be the pointwise multiplication of the two vectors. This corresponds to replacing some of the parameters by zero ("dropping them out") while keeping the others unchanged. One generally preserves the parameters of the final layer (g m ), so that the corresponding η's are equal to one, and let the other ones be independent, with some probability p of being one, say, p = 1/2.</p><p>Returning to the general case, in which η is simply assumed to be a random variable with known probability distribution, the dropout method replaces the objective function</p><formula xml:id="formula_942">F(W ) = E T (f (•, W )) by its expectation over perturbed predictors G(W ) = E(E T (f (•, η • W )))</formula><p>where the expectation is taken with respect to the random variable η. While this expectation cannot be computed explicitly, its minimization can be performed using stochastic gradient descent, with</p><formula xml:id="formula_943">W n+1 = W n -γ n+1 L(W n , η n+1 ),</formula><p>where η 1 , η 2 , . . . is a sequence of independent realizations of η and</p><formula xml:id="formula_944">L(W , η) = ∇ W (E T (f (•, η • W ))) . Then, averaging in η L(W ) = E(∇ W F(η ⊙ W )) = ∇G(W ).</formula><p>In the special case where η • W is just pointwise multiplication, then</p><formula xml:id="formula_945">L(W , η) = η ⊙ ∇F(η ⊙ W ).</formula><p>So this quantity can be evaluated by using back-propagation to compute ∇F(η • W ) and multiplying the result by η pointwise. Obviously, random weight perturbation can be combined with mini-batch selection in a hybrid stochastic gradient descent algorithm, the specification of which being left to the reader. We also note that stochastic gradient descent in neural networks is often implemented using the ADAM algorithm (section 3.3.3).</p><p>11.6 Continuous time limit and dynamical systems 11.6.1 Neural ODEs Equation (11.1) expresses the difference of the input and output of a neural transition as a non-linear function f (z; w) of the input. This strongly suggests passing to continuous time and replacing the difference by a derivative, i.e., replacing the neural network by a high-dimensional parametrized dynamical system. The continuous model then takes the form [52] ∂ t z(t) = ψ(z(t); w(t)) (11.3)</p><p>where t varies in a a fixed interval, say, [0, T ]. The whole process is parametrized by W = (w(t), t ∈ [0, T ]). We need to assume existence and uniqueness of solutions of (11.3), which usually restricts the domain of admissibility of parameters W .</p><p>Typical neural transition functions are Lipschitz functions whose constant depend on the weight magnitude, i.e., are such that</p><formula xml:id="formula_946">|ψ(z, w) -ψ(z ′ , w)| ≤ C(w)|z -z ′ | (11.4)</formula><p>where C is a continuous function of W . For example, for ψ(z, w) = ρ(bz + β 0 ), w = (b, β 0 ), one can take C(w) = C ρ |b| op . The Caratheodory theorem <ref type="bibr" target="#b35">[17]</ref> implies that solutions are well-defined as soon as</p><formula xml:id="formula_947">T 0 C(w(t))dt &lt; ∞.<label>(11.5)</label></formula><p>This is a relatively mild requirement, on which we will return later. Assuming this, we can consider z(T ) as a function of the initial value, z(0) = x and of the parameters, writing z(T ) = f (x, W ).</p><p>Given a training set, we consider the problem of minimizing</p><formula xml:id="formula_948">F(W ) = 1 N N k=1 r(y k , f (x k , W )). (11.6)</formula><p>The discussion in section section 11.4.2 applies-formally, at least-to this continuous case, and we can consider the equivalent problem of minimizing</p><formula xml:id="formula_949">G(W , z 1 , . . . , z N ) = 1 N N k=1 r(y k , z k (T )) with ∂ t z k (t) = ψ(z k (t); w(t)), z k (0) = x k .</formula><p>Once again, we consider each k separately, which boils down to considering N = 1 and we drop the index k from the notation, letting</p><formula xml:id="formula_950">F(W ) = r(y, f (x, W )) G(W , z) = r(y, z(T )).</formula><p>We define γ(W , z) to return the function</p><formula xml:id="formula_951">t → γ(W , z)(t) = ψ(z(t); w(t)) -∂ t z(t).</formula><p>Let p : [0, T ] → R d . We want to determine the expression of u = ∂ z γ T p, which satisfies</p><formula xml:id="formula_952">T 0 u(t) T δz(t)dt = T 0 p(t) T (∂ z ψ(z(t), w(t))δz(t) -∂ t δz(t))dt</formula><p>After an integration by parts, the r.h.s. becomes</p><formula xml:id="formula_953">-p(T ) T δz(T ) + T 0 ∂ t p(t) T δz(t)dt + T 0 p(t) T ∂ z ψ(z(t), w(t))δz(t))dt which gives u(t) = -p(T )δ T + ∂ t p(t) + ∂ z ψ(z(t), w(t)) T p(t).</formula><p>The equation</p><formula xml:id="formula_954">∂ z γ T p = ∂ z G T therefore gives -p(T )δ T + ∂ t p(t) + ∂ z ψ(z(t), w(t)) T p(t) = ∂ 2 r(y, z(T ))δ T ,</formula><p>so that p satisfies p(T ) = -∂ 2 r(y, z(T )) and</p><formula xml:id="formula_955">∂ t p(t) = -∂ z ψ(z(t), w(t)) T p(t). (<label>11.7)</label></formula><p>We have</p><formula xml:id="formula_956">∂ W G = 0 and v = ∂ W γ T p satisfies T 0 v(t) T δw(t)dt = T 0 p(t) T ∂ w ψ(z(t), w(t))δw(t)dt so that ∇F(W ) = (t → -∂ w ψ(z(t), w(t)) T p(t)).</formula><p>This informal derivation (more work is needed to justify the existence of various differentials in appropriate function spaces) provides the continuous-time version of the back-propagation algorithm, which is also known as the adjoint method in the optimal control literature <ref type="bibr" target="#b109">[91,</ref><ref type="bibr" target="#b141">123]</ref>. In that context, z represents the state of the control system, w is the control and p is called the costate, or covector. We summarize the gradient computation algorithm, reintroducing N training samples. Algorithm 11.2 (Adjoint method for neural ODE) Let (x 1 , y 1 , . . . , x N , y N ) be the training set and R k (z) = r(y k , z) so that</p><formula xml:id="formula_957">F(W ) = 1 N N k=1 R k (z k (T , W )) with ∂ t z k = ψ(z k , W ), z k (0) = x k .</formula><p>Let W be a family of weights. The following steps compute ∇F(W ).</p><p>1. For all k = 1, . . . , N and all t ∈ [0, T ], compute z k (t, W ) (forward computation through the dynamical system).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Initialize variables p</head><formula xml:id="formula_958">k (T ) = -∇R k (z k (T , W ))/N , k = 1, . . . , N .</formula><p>3. For all k = 1, . . . , N and all j = 1, . . . , m, compute p k (t) by solving (backwards in time)</p><formula xml:id="formula_959">∂ t p k (t) = -∂ z ψ(z k (t), w(t)) T p k (t). 4. Let, for t ∈ [0, T ], ∇F(W )(t) = - N k=1 ∂ w ψ(z k (t), w(t)) T p k (t).</formula><p>Of course, in numerical applications, the forward and backward dynamical systems need to be discretized, in time, resulting in a finite number of computation steps. This can be done explicitly (for example using basic Euler schemes), or using ODE solvers <ref type="bibr" target="#b70">[52]</ref> available in every numerical software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.6.2">Adding a running cost</head><p>Optimal control problems are usually formulated with a "running cost" that penalizes the magnitude of the control, which in our case is provided by the function W : t → w(t). Penalties on network weights are rarely imposed with discrete neural networks, but, as discussed above, in the continuous setting, some assumptions on the function W , such as <ref type="bibr">(11.5)</ref>, are needed to ensure that the problem is well defined.</p><p>It is therefore natural to modify the objective function in <ref type="bibr">(11.6</ref>) by adding a penalty term ensuring the finiteness of the integral in (11.5), taking, for example, for some λ &gt; 0,</p><formula xml:id="formula_960">F(W ) = λ T 0 C(w(t)) 2 dt + N k=1 r(y k , f (x k , W )). (11.8)</formula><p>The finiteness of the integral of the squared C(w) 2 implies, by Cauchy-Schwartz, the integrability of C(w) itself, and usually leads to simpler computations.</p><p>If C(w) is known explicitly and is differentiable, the previous discussion and the back-propagation algorithm can be adapted with minor modifications for the minimization of <ref type="bibr">(11.8)</ref>. The only difference appears in Step 4 of Algorithm 11.2, with</p><formula xml:id="formula_961">∇F(W )(t) = 2λ∇C(w(t)) - 1 N N k=1 ∂ w ψ(z k (t), w(t)) T p k (t).</formula><p>Computationally, one should still ensure that C and its gradient are not too costly to compute. If ψ(z, w) = ρ(bz + β 0 ), w = (b, β 0 ), the choice C(w) = C ρ |b| op is valid, but not computationally friendly. The simpler choice C(w) = C ρ |b| 2 is also valid, but cruder as an upper-bound of the Lipschitz constant. It leads however to straightforward computations.</p><p>The addition of a running cost to the objective is important to ensure that any potential solution of the problem leads to a solvable ODE. It does not guarantee that an optimal solution exists, which is a trickier issue in the continuous setting than in the discrete setting. This is an important theoretical issue, since it is needed, for example, to ensure that various numerical discretization schemes lead to consistent approximations of a limit continuous problem. The existence of minimizers is not known in general for ODE networks. It does hold, however, in the following nonparametric (i.e., weight-free) context that we now describe.</p><p>The function ψ in the r.h.s. of (11.3), is, for any fixed w, a function that maps z ∈ R d to a vector ψ(z, w) ∈ R d . Such functions are called vector fields on R d , and the collection ψ(•, w), w ∈ R s is a parametrized family of vector fields.</p><p>The non-parametric approach replaces this family of functions by a general vector field, v so that the time-indexed parametrized family of vector fields (t → ψ(•, w(t))) becomes an unconstrained family (t → f (t, •)). Following the general non-parametric framework in statistics, one needs to define a suitable function space for the vector fields, and use a penalty in the objective function.</p><p>We will assume that, at each time, f (t, •) belongs to a reproducing kernel Hilbert space (RKHS), as introduced in chapter 6. However, because we are considering a space of vector fields rather than scalar-valued functions, we need work with matrixvalued kernels <ref type="bibr" target="#b23">[5]</ref>, for which we give a definition that generalizes definition 6.1 (which corresponds to q = 1 below). T for all x and y in R d .</p><formula xml:id="formula_962">Definition 11.1 A function K : R d × R d → M q (R) satisfying [K1-vec] K is symmetric, namely K(x, y) = K(y, x)</formula><p>[K2-vec] For any n &gt; 0, for any choice of vectors λ 1 , . . . , λ n ∈ R q and any x 1 , . . . ,</p><formula xml:id="formula_963">x n ∈ R d , one has n i,j=1 λ T i K(x i , x j )λ j ≥ 0. (11.9)</formula><p>is called a positive (matrix-valued) kernel.</p><p>One says that the kernel is positive definite if the sum in (6.1) cannot vanish, unless (i)</p><formula xml:id="formula_964">λ 1 = • • • = λ n = 0 or (ii) x i = x j for some i j.</formula><p>If κ is a "scalar kernel" (satisfying definition 6.1), then K(x, y) = κ(x, y)Id R q is a matrix-valued kernel.</p><p>A reproducing kernel Hilbert space of vector-valued functions is a Hilbert space H of functions from R d to R q such that there exists a reproducing kernel K : R d × R d → M q (R) with the following properties</p><formula xml:id="formula_965">[RKHS1] For all x ∈ R d and λ ∈ R q , K(•, x)λ belongs to H, [RKHS2] For all h ∈ H, x ∈ R d and λ ∈ R q , ⟨h , K(•, x)λ⟩ H = λ T h(x) .</formula><p>Proposition 6.5 remains valid in the for vector-valued RKHS, with the following modifications: λ 1 , . . . , λ N and α 1 , . . . , α N are q-dimensional vectors and the matrix K(x 1 , . . . , x N ) is now an N q × N q block matrix, with q × q blocks given by K(x k , x l ), k, l = 1, . . . , N .</p><p>Returning to the specification of the nonparametric control problem, we will assume that a vector-valued RKHS, H, has been chosen, with q = d in definition 11.1. We further assume that elements of H are Lipschitz continuous, with</p><formula xml:id="formula_966">|v(z) -v(z)| ≤ C∥v∥ H |z -z|<label>(11.10)</label></formula><p>for some constant C and all v ∈ H. We note that, for every λ ∈ R d ,</p><formula xml:id="formula_967">|λ T (v(z) -v(z))| 2 = |⟨v , K(•, z)λ -K(•, z)λ⟩ H | 2 ≤ ∥v∥ 2 H ∥K(•, z)λ -K(•, z)λ∥ 2 H = ∥v∥ 2 H (λ T K(z, z)λ -2λ T K(z, z)λ + λ T K(z, z)) ≤ |λ| 2 ∥v∥ 2 H |K(z, z) -2K(z, z) + K(z, z)| .</formula><p>This shows that (11.10) can be derived from regularity properties of the kernel, namely, that</p><formula xml:id="formula_968">|K(z, z) -2K(z, z) + K(z, z)| ≤ C|z -z| 2</formula><p>for some constant C and all z, z ∈ R d . This property is satisfied by most of the kernels that are used in practice.</p><p>Let η : t → η(t) be a function from [0, 1] to H. This means that, for each t, η(t) is a vector field x → η(t)(x) on R d , and we will write indifferently η(t) and η(t, •), with a preference for η(t, x) rather than η(t)(x). We consider the objective function</p><formula xml:id="formula_969">F(f ) = λ T 0 ∥η(t)∥ 2 H dt + 1 N N k=1 r(y k , z k (1)), (11.11) with ∂ t z k (t) = η(t, z k (t)), z k (0) = x k .</formula><p>To compare with (11.8), the finite-dimensional w ∈ R s is now replaced with an infinite-dimensional parameter, η, and the transition ψ(z, w) becomes η(z).</p><p>Using the vector version of proposition 6.5 (or the kernel trick used several times in chapters 7 and 8), one sees that there is no loss of generality in replacing η(t) by its projection onto the vector space</p><formula xml:id="formula_970">V (t) =        N l=1 K(•, z l (t))w l : w 1 , . . . , w N ∈ R d        . Noting that, if η(t) takes the form η(t) = N l=1 K(•, z l (t))w l (t), then ∥η(t)∥ 2 H = N k,l=1 w k (t) T K(z k (t), z l (t))w l (t).</formula><p>This allows us to replace the infinite-dimensional parameter η by a family W = (w(t), t ∈ [0, T ] with w(t) = (w k (t), k = 1, . . . , N ). The minimization of F in <ref type="bibr">(11.11</ref>) can be replaced by that of</p><formula xml:id="formula_971">F(W ) = λ T 0 N k,l=1 w k (t) T K(z k (t), z l (t))w l (t)dt + 1 N N k=1 r(y k , z k (1)),<label>(11.12)</label></formula><p>with</p><formula xml:id="formula_972">∂ t z k (t) = N l=1 K(z k (t), z l (t))w l (t).</formula><p>This optimal control problem has a similar form to that considered in <ref type="bibr">(11.8)</ref>, where the running cost C(w) 2 is replaced by a cost that depends on the control (still denoted w) and the state z. The discussion in section section 11.6.1 can be applied with some modifications. Let K(z) be the dN × dN matrix formed with d × d blocks K(z k (t), z l (t)) and w(t) the dN -dimensional vector formed by stacking w 1 , . . . , w N . Let</p><formula xml:id="formula_973">G(W , z) = λ T 0 w(t) T K(z(t))w(t)dt + 1 N N k=1 r(y k , z k (1)) and γ(W , z)(t) = K(z(t))w(t) -∂ t z(t) .</formula><p>The backward ODE in step 3. of Algorithm 11.2 now becomes</p><formula xml:id="formula_974">∂ t p k (t) = -∂ z k (w(t) T K(z(t))p(t)) + λ∂ z k (w(t) T K(z(t))w(t)) for k = 1, . . . , N . Step 4. becomes (for t ∈ [0, T ]), ∇F(W )(t) = K(z(t))(2λ -p(t)).</formula><p>The resulting algorithm was introduced in <ref type="bibr" target="#b227">[209]</ref>. It has the interesting property (shared with neural ODE models with smooth controlled transitions) to determine an implicit diffeomorphic transformation of the space, i.e., the function x → f (x; W , z) = z(T ) which returns the solution at time T of the ODE</p><formula xml:id="formula_975">∂ t z(t) = N l=1 K(z(t), z l (t))w l (t)</formula><p>(or ∂z(t) = ψ(z(t); w(t)) for neural ODEs) is smooth, invertible, with a smooth inverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 12</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monte-Carlo Sampling</head><p>The goal of this section is to describe how, from a basic random number generator that provides samples from a uniform distribution on [0, 1], one can generate samples that follow, or approximately follow, complex probability distributions on finite or general spaces. This, combined with the law of large numbers, permits to approximate probabilities or expectations by empirical averages over a large collection of generated samples.</p><p>We assume that as many as needed independent samples of the uniform distribution are available, which is only an approximation of the truth. In practice, computer programs are only able to generate pseudo-random numbers, which are highly chaotic recursive sequences, but still deterministic. Also, these numbers are generated as integers, which only provide, after normalization, a distribution on a finite discretization of the unit interval. We will neglect these facts, however, and work as if the output of the function random (or any similar name) in a computer program is a true realization of the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">General sampling procedures</head><p>Real-valued variables. We will use the following notation for the left limit of a function F at a given point z</p><formula xml:id="formula_976">F(z -0) = lim y→z,y&lt;z F(y)</formula><p>assuming, of course that this limit exists (which is always true, for example when F is non-decreasing). Recall that F is left continuous if and only if F = F( • -0). Moreover, it is easy to see that F( • -0) is left-continuous <ref type="foot" target="#foot_13">1</ref> . Note also that, if F is non-decreasing, one always has F(z) ≤ F(y -0) whenever z &lt; y. The following proposition provides a basic mechanism for Monte-Carlo sampling.</p><p>Proposition 12.1 Let Z be a real-valued random variable with c.d.f.</p><formula xml:id="formula_977">F Z . For u ∈ [0, 1], define F - X (u) = max{z : F Z (z -0) ≤ u}. Let U be uniformly distributed over [0, 1]. Then F - Z (U ) has the same distribution as Z. Proof Let A z = {u ∈ [0, 1] : F - Z (u) ≤ z}. Assume first that u &lt; F Z (z). Then F Z (y -0) ≤ u implies that y ≤ z, since y &gt; z would imply that F Z (z) ≤ F Z (y -0). This shows that sup{z ′ : F Z (z ′ -0) ≤ u} ≤ z, i.e., u ∈ A z .</formula><p>Now, take u &gt; F Z (z). Because c.d.f.'s are right continuous, there exists y &gt; z such that u &gt; F Z (y), which implies that F - Z (u) ≥ y and u A z .</p><p>We have therefore shown that [0,</p><formula xml:id="formula_978">F Z (z)) ⊂ A z ⊂ [0, F Z (z)]. If U is uniformly dis- tributed on [0, 1], then P (U &lt; F Z (z)) = P (U ≤ F Z (z)) = F Z (z), showing that P(F - Z (U ) ≤ z) = P(U ∈ A z ) = F Z (z).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This proposition shows that one can generate random samples of a real-valued random variable Z as soon as one can compute F - Z and generate uniformly distributed variables. Note that, if F Z is strictly increasing, then F - Z = F -1 Z , the usual function inverse.</p><p>The proposition also shows how to sample from random variables taking values in finite sets. Indeed, if Z takes values in Ω Z = {z 1 , . . . , z n } with p i = P(Z = z i ), sampling from Z is equivalent to sampling from the integer valued random variable Z with P(</p><formula xml:id="formula_979">Z = i) = p i . For this variable, F - Z (u) is the largest i such that p 1 + • • • + p i-1 ≤ u (this sum being zero if i = 1</formula><p>), which provides the standard sampling scheme for discrete probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.2">Rejection sampling</head><p>While the previous approach can be generalized to multivariate distributions, it quickly becomes unfeasible when the dimension gets large, excepting simple cases in which the variables are independent, or, say, Gaussian. Rejection sampling is a simple algorithm that allows, in some cases, for the generation of samples from a complicated distribution based on repeated sampling of a simpler one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Without loss of generality, we can assume that y</head><formula xml:id="formula_980">′ ≥ z ′ , yielding |F(z -0) -F(y -0)| ≤ 2ϵ, showing the left continuity of F( • -0).</formula><p>Let us assume that we want to sample from a variable Z taking values R Z , and that there exists a measure µ on R Z with respect to which the distribution of Z is absolutely continuous, i.e., so that this distribution has a density f Z with respect to µ. For example, R Z = R d , and f Z is the p.d.f. of Z with respect to Lebesgue's measire. Assume that g is another density functions (with respect to µ) from which it is "easy" to sample. Consider the following algorithm, which includes a function a : z → a(z) ∈ [0, 1] that will be specified later.  The probability of exiting at step 3 is ρ = R d g(z)a(z)µ(dz). So, the algorithm simulates a random variable with p.d.f.</p><formula xml:id="formula_981">f (z) = g(z)a(z)(1 + (1 -ρ) + (1 -ρ) 2 + • • • ) = g(z)a(z) ρ .</formula><p>As a consequence, in order to simulate f Z , one must choose a so that f Z (z) is proportional to g(z)a(z), which, (assuming that g(z) &gt; 0 whenever f Z (z) &gt; 0), requires that a(z) is proportional to f Z (z)/g(z). Since a(z) must take values in [0, 1], but should otherwise be chosen as large as possible to ensure that fewer iterations are needed, one should take</p><formula xml:id="formula_982">a(z) = f Z (z) cg(z)</formula><p>where c = max{f Z (z)/g(z) : z ∈ R d }, which must therefore be finite. This fully specifies a rejection sampling algorithm for f Z . Note that g is free to choose (with the restriction that f Z (z)/g(z) must be bounded), and should be selected so that sampling from it is easy, and the coefficient c above is not too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3">Markov chain sampling</head><p>When dealing with high-dimensional distributions, the constant c in the previous procedure is typically extremely large, and the rejection-sampling algorithm becomes unfeasible, because it keeps rejecting samples for very long times. In such cases, one can use alternative simulation methods that iteratively updates the variable Z by making small changes at each step, resulting in a procedure that asymptotically converges to a sample of the target distribution. Such sampling schemes are usually described as Markov chains, leading to the name Markov-chain Monte Carlo (or MCMC) sampling.</p><p>Assume that we want to sample from a random variable that takes values in some (measurable) set B = R X . <ref type="foot" target="#foot_14">2</ref> A Markov chain is the probabilistic analogous of a recursive sequence X n+1 = Φ(X n ), which is fully defined by the function Φ : B → B and the initial value X 0 ∈ B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.1">Definitions</head><p>For Markov chains, X 0 is a random variable, which therefore does not have a fixed value, but follows a probability distribution that we will generally denote µ 0 : P 0 (x) = P (X 0 = x). The computation of X n+1 given X n is not deterministic either, but given the conditional probabilities</p><formula xml:id="formula_983">P n,n+1 (x, A) = P(X n+1 ∈ A | X n = x).</formula><p>where A ⊂ B is measurable. The left-hand side of this equation, P n,n+1 is called a transition probability, according to the following definition. Definition 12.2 Let F 1 and F 2 be two sets equipped with σ -algebras A 1 and A 2 . A transition probability from</p><formula xml:id="formula_984">F 1 to F 2 is a function p : F 1 × A 2 → [0, 1] such that, for all x ∈ F 1 , the function A → p(x, A) is a probability on F 2 and for all A ∈ A 2 , the function x → p(x, A), x ∈ F 1 , is measurable.</formula><p>When F 2 is discrete, the probabilities are fully specified by their values on singleton sets, and we will write p(x, y) for p(x, {y}).</p><p>When P n,n+1 (x, •) does not depend on n, the Markov chain is said to be homogeneous. To simplify notation, we will restrict to homogeneous chains (and therefore only write P (x, A)), although some of the chains used in MCMC sampling may be inhomogeneous. This is not a very strong loss of generality, however, because inhomogeneous Markov chains can be considered as homogeneous by extending the space Ω on which they are defined to Ω × N, and defining the transition probability</p><formula xml:id="formula_985">p (x, n), A × {r} = 1 r=n+1 p n,n+1 (x, A).</formula><p>An important special case is when B is countable, in which case one only needs to specify transition probabilities for singletons A = {y}, and we will write</p><formula xml:id="formula_986">p(x, y) = P (x, {y}) = P(X n+1 = y | X n = x)</formula><p>for the p.m.f. associated with P (x, •).</p><p>Another simple situation is when B = R d and each P (x, •) has a p.d.f. that we will also denote as p(x, •). In this latter case, assuming that P 0 also have a p.d.f. that we will denote by µ 0 , the joint p.d.f. of (X 0 , . . . , X n ) on (R d ) n+1 is given by</p><formula xml:id="formula_987">f (x 0 , x 1 , . . . , x n ) = µ 0 (x 0 )p(x 0 , x 1 ) • • • p(x n-1 , x n ).</formula><p>(12.1)</p><p>The same expression holds for the joint p.m.f. in the discrete case.</p><p>In the general case (invoking measure theory), the joint distribution is also determined by the transition probabilities, and we leave the derivation of the expression to the reader. An important point is that, in both special cases considered above, and under some very mild assumptions in the general case , these transition probabilities also uniquely define the joint distribution of the infinite process (X 0 , X 1 , . . .) on B ∞ , which gives theoretical support to the consideration of asymptotic properties of Markov chains. In this discussion, we are interested in conditions ensuring that the chain asymptotically samples from a target probability distribution Q, i.e., whether P(X n ∈ A) converges to Q(A) (one says that X n converges in distribution to Q). In practice, Q is given or modeled, and the goal is to determine the transition probabilities. Note that the marginal distribution of X n is computed by integrating (or summing) (12.1) with respect to x 0 , . . . , x n-1 , which is generally computationally challenging.</p><p>Given a transition probability P on B, we will use the notation, for a function f : B → R:</p><formula xml:id="formula_988">P f (x) = B f (y)P (x, dy).</formula><p>If Q is a probability distribution on B, it will also be convenient to write</p><formula xml:id="formula_989">Qf (x) = B f (y)Q(dy).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.2">Convergence</head><p>We will denote P x (•) the conditional distribution P(• | X 0 = x) and P n (x, A) = P x (X n ∈ A), which is a probability distribution on B. The goal of Markov Chain Monte Carlo sampling is to design the transition probabilities such that P n x (A) converges to Q(A) when n tends to infinity. One furthermore wants to complete this convergence with a law of large numbers, ensuring that</p><formula xml:id="formula_990">1 n n k=1 f (X k ) → B f (x)Q(dx)</formula><p>when n → ∞, where X n is the generated Markov chain and f is Q-integrable.</p><p>Introduce the total variation distance between two probability measures on a given probability space,</p><formula xml:id="formula_991">D var (µ 1 , µ 2 ) = sup A (µ 1 (A) -µ 2 (A)). (12.2)</formula><p>where the supremum is taken over all measurable sets A. We will say that the Markov chain with transition P asymptotically samples from</p><formula xml:id="formula_992">Q if lim n→∞ D var (P n (x, •), Q) = 0 (12.3) for Q-almost all x ∈ B.</formula><p>The chain must satisfy specific conditions for this to be guaranteed.</p><p>We now discuss some properties of the total variation distance that will be useful later. First, we note that the supremum in the r.h.s. of (12.2) is achieved. Indeed, there exists a set A 0 such that, for all measurable sets A, µ</p><formula xml:id="formula_993">1 (A ∩ A 0 ) ≥ µ 2 (A ∩ A 0 ) and µ 1 (A∩A c 0 ) ≤ µ 2 (A∩A c 0 ). If B is a finite set, it suffices to let A 0 = {x ∈ B : µ 1 (x) ≥ µ 2 (x)}; if both µ 1</formula><p>and µ 2 have p.d.f.'s ϕ 1 and ϕ 2 with respect to Lebesgue's measure (with B = R d ), then one can take A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}. In the general case, one can take µ = µ 1 + µ 2 so that µ 1 , µ 2 ≪ µ, and letting ϕ i = dµ i /dµ, also take</p><formula xml:id="formula_994">A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}.</formula><p>(This is also a special case of the Hahn-Jordan decomposition of signed measures <ref type="bibr" target="#b84">[66]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, it is clear that, for any</head><formula xml:id="formula_995">A µ 1 (A) -µ 2 (A) = µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) + µ 1 (A ∩ A c 0 ) -µ 2 (A ∩ A c 0 ) ≤ µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) ≤ µ 1 (A ∩ A 0 ) -µ 2 (A ∩ A 0 ) + µ 1 (A c ∩ A 0 ) -µ 2 (A c ∩ A 0 ) = µ 1 (A 0 ) -µ 2 (A 0 ) showing that D var (µ 1 , µ 2 ) = µ 1 (A 0 ) -µ 2 (A 0 ).</formula><p>The following proposition lists additional properties.</p><formula xml:id="formula_996">Proposition 12.3 (i) If µ 1 , µ 2 have a densities ϕ 1 , ϕ 2</formula><p>with respect to some positive measure µ (such as µ 1 + µ 2 ), then</p><formula xml:id="formula_997">D var (µ 1 , µ 2 ) = 1 2 B |ϕ 1 (x) -ϕ 2 (x)|µ(dx).</formula><p>In particular, if B is finite</p><formula xml:id="formula_998">D var (µ 1 , µ 2 ) = 1 2 x∈B |µ 1 (x) -µ 2 (x)|.</formula><p>(ii) For general B,</p><formula xml:id="formula_999">D var (µ 1 , µ 2 ) = sup f B f (x)µ 1 (dx) - B f (x)µ 2 (dx) . (12.4)</formula><p>where the supremum is taken over all measurable functions f taking values in [0, 1].</p><formula xml:id="formula_1000">(iii) If f : B → R is bounded, define the maximal oscillation of f by osc(f ) = sup{f (x) -f (y) : x, y ∈ B}.</formula><p>Then</p><formula xml:id="formula_1001">D var (µ 1 , µ 2 ) = sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : osc(f ) ≤ 1 (iv) Conversely, for any bounded measurable f : B → R, osc(f ) = sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1 Proof If one takes A 0 = {x ∈ B : ϕ 1 (x) ≥ ϕ 2 (x)}, then D var (µ 1 , µ 2 ) = A 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx) = A 0 |ϕ 1 (x) -ϕ 2 (x)|µ(dx).</formula><p>But, because both µ 1 and µ 2 are probability measures</p><formula xml:id="formula_1002">B (ϕ 1 (x) -ϕ 2 (x))µ(dx) = 0 so that A c 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx) = - A 0 (ϕ 1 (x) -ϕ 2 (x))µ(dx).</formula><p>However, the l.h.s. is also equal to</p><formula xml:id="formula_1003">- A c 0 |ϕ 1 (x) -ϕ 2 (x)|µ(dx) so that B |ϕ 1 (x) -ϕ 2 (x)|µ(dx) = 2 A 0 (ϕ 1 (x) -ϕ 2 (x)) = 2D var (µ 1 , µ 2 ),</formula><p>which proves (i).</p><p>To prove (ii), first notice that, for all A,</p><formula xml:id="formula_1004">µ 1 (A) -µ 2 (A) = B f (x)µ 1 (dx) - B f (x)µ 2 (dx) for f = 1 A , so that D var (µ 1 , µ 2 ) ≤ sup f B f (x)µ 1 (dx) - B f (x)µ 2 (dx) .</formula><p>Conversely, using A 0 as above, and taking f with values in [0, 1]</p><formula xml:id="formula_1005">B f (x)µ 1 (dx) - B f (x)µ 2 (dx) = A 0 f (x)(µ 1 -µ 2 )(dx) + A c 0 f (x)(µ 1 -µ 2 )(dx) ≤ A 0 f (x)(µ 1 -µ 2 )(dx) ≤ A 0 (µ 1 -µ 2 )(dx) = D var (µ 1 , µ 2 )</formula><p>This shows (ii). For (iii), one can note that, if f takes values in [0, 1], then osc(f ) ≤ 1 so that</p><formula xml:id="formula_1006">D var (µ 1 , µ 2 ) ≤ sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : osc(f ) ≤ 1 Conversely, take f such that osc(f ) ≤ 1, ϵ &gt; 0 and y such that f (y) ≥ inf x f (x) + ϵ. Let f ϵ (x) = (f (x) -f (y) + ϵ)/(1 + ϵ), which takes values in [0, 1]. Then D var (µ 1 , µ 2 ) ≥ B f ϵ (x)µ 1 (dx) - B f ϵ (x)µ 2 (dx) = 1 1 + ϵ B f (x)µ 1 (dx) - B f (x)µ 2 (dx)</formula><p>and since this is true for all ϵ &gt; 0, we get</p><formula xml:id="formula_1007">B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ D var (µ 1 , µ 2 )</formula><p>which completes the proof of (iii).</p><p>Using (iii), we find, for any µ 1 , µ 2 and any bounded f</p><formula xml:id="formula_1008">B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ D var (µ 1 , µ 2 )osc(f ) which shows that sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1 ≤ osc(f ).</formula><p>However, taking µ 1 = δ x and µ 2 = δ y , so that D var (µ 1 , µ 2 ) = 0 is x = y and 1 otherwise, we get</p><formula xml:id="formula_1009">f (x) -f (y) = B f (x)µ 1 (dx) - B f (x)µ 2 (dx) ≤ sup B f (x)µ 1 (dx) - B f (x)µ 2 (dx) : D var (µ 1 , µ 2 ) ≤ 1</formula><p>which yields (iv) after taking the supremum with respect to x and y.</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.3">Invariance and reversibility</head><p>If a Markov chain converges to Q, then Q must be an "invariant distribution," in the sense that, if X n ∼ Q for some n, then so does X n+1 and as a consequences all X m for n ≥ m. This can be seen by writing</p><formula xml:id="formula_1010">P n+1 (x, A) = P x (X n+1 ∈ A) = E x (P (X n , A)) = E P n (x,•) (P (•, A)) If P n (x,</formula><p>•) (and therefore also P n+1 (x, •)) converges to Q, then passing to the limit above yields</p><formula xml:id="formula_1011">Q(A) = E Q (P (•, A))</formula><p>and this states that, if X n ∼ Q, then so does X n+1 . If Q has a p.d.f. (resp. p.m.f.), say, q, this gives</p><formula xml:id="formula_1012">q(y) = R d p(x, y)q(x)dx, (resp. q(y) = x∈B p(x, y)q(x)).</formula><p>So, if one designs a Markov chain with a target asymptotic distribution Q, the first thing to ensure is that Q is invariant.</p><p>While invariance leads to an integral equation for q, a stronger condition, called reversibility is easier to assess.</p><p>Assume that Q is invariant by P . Make the assumption that P (x, •) has a density p * with respect to Q (this is, essentially, no loss of generality, see argument below), so that</p><formula xml:id="formula_1013">P (x, A) = A p * (x, y)Q(dy).</formula><p>Taking A = B above, we have</p><formula xml:id="formula_1014">B p * (x, y)Q(dy) = P (x, B) = 1 but we also have, because Q is invariant, that B p * (x, y)Q(dx) = Q(B) = 1.</formula><p>One says that the density is "doubly stochastic" with respect to Q.</p><p>Conversely, if a transition probability P has a doubly stochastic density p * with respect to some probability Q on B, then Q is invariant by P , since</p><formula xml:id="formula_1015">B P (x, A)Q(dx) = B A p * (x, y)Q(dy)Q(dx) = A B p * (x, y)Q(dx)Q(dy) = A Q(dy) = Q(A).</formula><p>The property of being doubly stochastic can be reinterpreted in terms of time reversal for Markov chains. Let Q 0 be an initial distribution for a Markov chain with transition P (not necessarily invariant) so that, for any n ≥ 0, the distribution of X n is Q n = Q 0 P n . Fixing any m &gt; 0, we are interested in the reversed process Xk = X m-k . We first notice that the conditional distribution of X n given its future X n+1 , . . . , X m (with n &lt; m) only depends on X n+1 , so that the reversed process is also Markov. Indeed, for any positive function f : B → R, g : B m-n → R, one has, using the fundamental properties of conditional expectations and the fact that</p><formula xml:id="formula_1016">(X n ) is a Markov chain, E(f (X n )g(X n+1 , . . . , X m )) = E (E(f (X n )g(X n+1 , . . . , X m ) | X n , X n+1 )) = E (f (X n )E(g(X n+1 , . . . , X m ) | X n , X n+1 )) = E (f (X n )E(g(X n+1 , . . . , X m ) | X n+1 )) = E (E(f (X n ) | X n+1 )E(g(X n+1 , . . . , X m ) | X n+1 )) = E (E(f (X n ) | X n+1 )g(X n+1 , . . . , X m )) . This shows that E(f (X n ) | X n+1 , . . . , X m ) = E(f (X n ) | X n+1 ),</formula><p>which is what we wanted. To identify the conditional distribution of X n given X n+1 , we note that for any x ∈ B, the transition probability P (x, •) is absolutely continuous with respect to Q n+1 since</p><formula xml:id="formula_1017">Q n+1 (A) = B P (x, A)Q n (dx)</formula><p>and the r.h.s. is zero only if P (x, A) = 0 Q n -almost everywhere <ref type="foot" target="#foot_15">3</ref> . This shows that there exists a function r n+1 : B × B → R such that, for all</p><formula xml:id="formula_1018">A P (x, A) = A r n+1 (x, y)Q n+1 (dy).</formula><p>Given this point, one can write</p><formula xml:id="formula_1019">E(f (X n )g(X n+1 ) = B 2 f (x n )g(x n+1 )P (x n , dx n+1 )Q n (dx n ) = B 2 f (x n )g(x n+1 )r n+1 (x n , x n+1 )Q n+1 (dx n+1 )Q n (dx n ) = B B f (x n )r n+1 (x n , x n+1 )Q n (dx n ) g(x n+1 )Q n+1 (dx n+1 )</formula><p>which shows that the conditional distribution of X n given X n+1 = x n+1 has density x n → r n+1 (x n , x n+1 ) relatively to Q n . Note that, for discrete probabilities, one has</p><formula xml:id="formula_1020">r n+1 (x, y) = P (x, y) Q n+1 (y)</formula><p>and</p><formula xml:id="formula_1021">P(X n = x | X n+1 = y) = Q n (x)P (x, y) Q n+1 (y) . (12.5)</formula><p>The formula is identical if both Q 0 and P (x, •) have p.d.f.'s with respect to a fixed reference measure µ on B (for example, Lebesgue's measure when B = R d ), denoting these p.d.f's by q 0 and p(x, •). Then, the p.d.f. of the distribution of X n given X n+1 = y is pn (y, x) = q n (x)p(x, y) q n+1 (y) <ref type="bibr">(12.6)</ref> where q n is the p.d.f. of Q n . Note that the transition probabilities of the reversed Markov chain depend on n, i.e., the reversed chain is non-homogeneous in general.</p><p>However, if one assumes that Q 0 = Q is invariant by P , then Q n = Q for all n and therefore r n (x, y) = p * (x, y), using the previous notation. In this case, the reversed chain has transitions independent of n and its transition probability has density p * (x, y) = p * (y, x) with respect to Q. In the discrete case, letting p(x, y</p><formula xml:id="formula_1022">) = P (X n+1 = y | X n = x), we have p * (x, y) = p(x, y)/Q(y), so that the reversed transition (call it p) is such that p(x, y) Q(y) = p(y, x) Q(x) , i.e., Q(y)p(y, x) = Q(x) p(x, y). (12.7)</formula><p>One retrieves easily the fact that if p is such that there exists Q and p such that (12.7) is satisfied, then (summing the equation over y) Q is an invariant probability for p.</p><p>Let Q be a probability on B. One says that the Markov chain (or the transition probability p) is Q-reversible if and only if p(x, •) has a density p * (x, •) with respect to Q such that p * (x, y) = p * (y, x) for all x, y ∈ B. Since such a density is necessarily doubly stochastic, Q is then invariant by p. Reversibility is equivalent to the property that, whenever X n ∼ Q, the joint distribution of (X n , X n+1 ) coincides with that of (X n+1 , X n ). Alternatively, Q-reversibility requires that for all A, B ⊂ B,</p><formula xml:id="formula_1023">A P (z, B)dQ(z) = B P (z, A)dQ(z).</formula><p>(12.8)</p><p>In the discrete case, (12.8) is equivalent to the "detailed balance" condition:</p><formula xml:id="formula_1024">Q(y)p(y, x) = Q(x)p(x, y). (12.9)</formula><p>While Q can be an invariant distribution for a Markov chain without that chain being Q-reversible, the latter property is easier to ensure when designing transition probabilities, and most sampling algorithms are indeed reversible with respect to their target distribution.</p><p>Remark 12.4 A simple example of non-reversible Markov chain with invariant probability Q is often obtained in practice by alternating two or more Q-reversible transition probabilities. Assume, to simplify, that B is discrete and that p 1 and p 2 are transition probabilities that satisfy (12.9). Consider a composite Markov chain for which the transition from X n to X n+1 consists in generating first Y n according to p 1 (X n , •) and then X n+1 according to p 2 (Y n , •). The resulting composite transition probability is p(x, y)</p><formula xml:id="formula_1025">= z∈B p 1 (x, z)p 2 (z, y).</formula><p>Trivially, Q is invariant by p, since it is invariant by p 1 and p 2 , but p is not Qreversible. Indeed, p satisfies (12.7) with p(x, y) = z∈B p 2 (x, z)p 1 (z, y). ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.4">Irreducibility and recurrence</head><p>While necessary, invariance is not sufficient for a Markov chain to converge to Q in distribution. However, it simplifies the general ergodicity conditions compared to the general theory of Markov chains <ref type="bibr" target="#b165">[147,</ref><ref type="bibr" target="#b178">160]</ref>, as summarized below, following <ref type="bibr" target="#b210">[192]</ref> (see also <ref type="bibr" target="#b31">[13]</ref>). We therefore assume that the transition probability P is such that Q is P -invariant.</p><p>One says that the Markov chain is Q-irreducible (or, simply, irreducible in what follows) if and only if, for all z ∈ B and all (measurable) B ⊂ B such that Q(B) &gt; 0, there exists n &gt; 0 with P z (X n ∈ B) &gt; 0. (Irreducibility implies that Q is the only invariant probability of the Markov chain.) A Markov chain is called periodic if there exists m &gt; 1 such that B can be covered by disjoint subsets B 0 , . . . , B m-1 that satisfy P (x, B j ) = 1 for all x ∈ B j-1 if j ≥ 1 and all x ∈ B m-1 if j = 0. In other terms, the chain loops between the sets B 0 , . . . , B m-1 . If such a decomposition does not exists, the chain is called aperiodic.</p><p>A periodic chain cannot satisfy <ref type="bibr">(12.3)</ref>. Indeed, periodicity implies that P x (X n ∈ B i ) = 0 for all x ∈ B i unless i = 0 (mod d). Since the sets B i cover B, (12.3) is only possible with Q = 0. Irreducibility and aperiodicity are therefore necessary conditions for ergodicity. Combined with the fact that Q is an invariant probability distribution, these conditions are also sufficient, in the sense that (12.3) is true for Q-almost all x. (See <ref type="bibr" target="#b210">[192]</ref> for a proof.) Without the knowledge that the chain has an invariant probability, showing convergence usually requires showing that the chain is recurrent, which means that, for any set B such that Q(B) &gt; 0, the probability that, starting from x, X n ∈ B for an infinite number of n, written as P x (X n ∈ B i.o.) (for infinitely often) is positive for all</p><p>x ∈ E and equal to 1 Q-almost surely. The fact that irreducibility and aperiodicity combined with Q-invariance imply recurrence (or, more precisely, Q-positive recurrent <ref type="bibr" target="#b165">[147]</ref>) is an important remark that significantly simplifies the theory for MCMC simulation. Note that, by restricting B to a suitable set of Q-probability 1, one can assume that P x (X n ∈ B i.o.) = 1 for all x ∈ B, which is called Harris recurrence. It the chain is Harris recurrent, then (12.3) holds with µ 0 = δ x for all x ∈ B. 4   One says that C ⊂ B is a "small" set if Q(C) &gt; 0 and there exists a triple (m 0 , ϵ, ν), with ϵ &gt; 0 and ν a probability distribution on B, such that</p><formula xml:id="formula_1026">P m 0 (x, •) ≥ ϵν(•)</formula><p>for all x ∈ C. A slightly different result, proved in <ref type="bibr" target="#b31">[13]</ref>, replaces irreducibility by the property that there exists a small set C ⊂ B such that P x (∃n : X n ∈ C) &gt; 0 4 Harris recurrence is also associated with the uniqueness of right eigenvectors of P , that is functions h : B → R such that</p><formula xml:id="formula_1027">P h(x) ∆ = B P (x, dy)h(y) = h(x).</formula><p>Such functions are also called harmonic for P . Because P is a transition probability, constant functions are always harmonic. Harris recurrence, in the current context, is equivalent to the fact that every bounded harmonic function is constant.</p><p>for Q-almost all x ∈ B. One then replaces aperiodicity by the similar condition that the greatest common divisor of the set of integers m such that there exists ϵ m with P m (x, •) ≥ ϵ m ν(•) for all x ∈ C is equal to 1. These two conditions combined with Q-invariance also imply that (12.3) holds for Q-almost all x ∈ B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.5">Speed of convergence</head><p>It is also important to quantify the speed of convergence in <ref type="bibr">(12.3)</ref>. Efficient algorithms typically have a geometric convergence speed, namely</p><formula xml:id="formula_1028">D var (P n x , Q) ≤ M(x)r n (12.10)</formula><p>for some 0 ≤ r &lt; 1 and some function M(x), or uniformly geometric convergence speed, for which the function M is bounded (or, equivalently, constant).</p><p>A sufficient condition for geometric ergodicity is provided in Nummelin <ref type="bibr" target="#b165">[147,</ref><ref type="bibr">Proposition 5.21]</ref>. Assume that the chain is Harris recurrent and that there exist r &gt; 1, a small set C and a "drift function" h with</p><formula xml:id="formula_1029">sup x C (rE(h(X n+1 ) | X n = x) -h(x)) &lt; 0 (12.11a) and sup x∈C E(h(X n+1 )1 X n+1 C | X n = x) &lt; ∞. (12.11b)</formula><p>Then the Markov chain is geometrically ergodic. Note that E(h(X n+1 ) | X n = x) = P h(x). Equations (12.11a) and (12.11b) can be summarized in a single equation <ref type="bibr" target="#b154">[136]</ref>, namely P h(x) ≤ βh(x) + M1 C (x) (12.12)</p><p>with β = 1/r &lt; 1 and M ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.6">Models on finite state spaces</head><p>Uniform geometric ergodicity is implied by the simple condition that the whole set B is small, requiring in a uniform lower bound, for some probability distribution ν,</p><formula xml:id="formula_1030">P m 0 (x, •) ≥ ϵν(•) (12.13) for all x ∈ B.</formula><p>Such uniform conditions usually require strong restrictions on the space B, such as compactness or finiteness. To illustrate this consider the case in which the set B is finite. Assume, to simplify, that Q(x) &gt; 0 for all x ∈ B (one can restrict the Markov chain to such x's otherwise). Arbitrarily labeling elements of B as B = {x 1 , . . . , x N }, we can consider p(x, y) as the coefficients of a matrix P = (p(x k , x l ), k, l = 1, . . . , N ). Such a matrix, which has non-negative entries and row sums equal to 1, is called a stochastic matrix.</p><p>We will denote the nth power of P as P n = (p (n) (x k , x l ), k, l = 1, . . . , N ). One immediately sees that irreducibility is equivalent to the fact that, for all x, y ∈ B, there exists m (that may depend of x and y) such that p (m) (x, y) &gt; 0. One can furthermore show that the chain is irreducible and aperiodic if one can choose m independent of x and y above, that is, if there exists m such that P m has positive coefficients. This condition clearly implies uniformly geometric ergodicity, which is therefore valid for all irreducible and aperiodic Markov chains on finite sets.</p><p>This result can also be deduced from properties of matrices with non-negative or positive coefficients. The Perron-Frobenius theorem <ref type="bibr" target="#b111">[93]</ref> states that the eigenvalue 1 (associated with the eigenvector 1) is the largest, in modulus, eigenvalue of a stochastic matrix P with positive entries, that it has multiplicity one and that all other eigenvalues have a modulus strictly smaller that one. If P m has positive entries, this implies that all the eigenvalues of (P m -1Q) (where Q is considered as a row vector) have modulus strictly less than one. This fact can then be used to prove uniformly geometric ergodicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3.7">Examples on R d</head><p>To take a geometrically ergodic example that is not uniform, consider the simple random walk provided by the iterations</p><formula xml:id="formula_1031">X n+1 = ρX n + τ 2 ϵ n</formula><p>where ϵ n ∼ N (0, Id R d ), τ 2 &gt; 0 and 0 &lt; ρ &lt; 1. One shows easily by induction that the conditional distribution of X n given X 0 = x is Gaussian with mean m n = ρ n x and covariance matrix σ 2 n Id R d with</p><formula xml:id="formula_1032">σ 2 n = 1 -ρ 2n 1 -ρ 2 τ 2 .</formula><p>In particular, the distribution</p><formula xml:id="formula_1033">Q = N (0, σ 2 ∞ Id R d ), with σ 2 ∞ = τ 2 /(1 -ρ 2 )</formula><p>, is invariant. Estimates on the variational distances between Gaussian distributions, such as those provided in Devroye et al. <ref type="bibr" target="#b79">[61]</ref>, can then be used to show that</p><formula xml:id="formula_1034">D var (P n x , Q) ≤ M(x)ρ n</formula><p>where M grows linearly in x but is not bounded.</p><p>Situations in which on can, as above, compute the probability distributions of X n are rare, however, and proving geometric convergence is significantly more difficult than for finite-state chains. For chains on R d (or, more generally, locally compact metric spaces), the drift function criterion (12.12) can be used. Assume that P h(•), given by</p><formula xml:id="formula_1035">P h(x) = E(h(X n+1 ) | X n = x) = R d h(y)P (x, dy)</formula><p>is continuous as soon as the function h : R d → R is continuous (one says that the chain is weak Feller). This true, for example, if P (x, •) has a p.d.f. with respect to Lebesgue's measure which is continuous in x. In such a situation, one can see that compact sets are small sets, and (12.12) can be restated as the existence if a positive function h with compact sub-level sets and such that h(x) ≥ 1, of a compact set C ⊂ R d and of positive constants β &lt; 1 and b such that, for all x ∈ R d ,</p><formula xml:id="formula_1036">P h(x) ≤ βh(x) + b1 C (x).</formula><p>(12.14)</p><p>As an example, consider the Markov chain defined by</p><formula xml:id="formula_1037">X n+1 = X n -δ∇H(X n ) + τϵ n+1</formula><p>where ϵ 2 , ϵ 2 , . . . are i.i.d. standard d-dimensional Gaussian variables and</p><formula xml:id="formula_1038">H : R d → R is C 2 .</formula><p>This chain is clearly irreducible (with respect to Lebesgue's measure). One has</p><formula xml:id="formula_1039">P h(x) = 1 (2πτ 2 ) d/2 R d h(y)e -1 2τ 2 |y-x+δ∇H(x)| 2 dy = 1 (2π) d/2 R d h(x-δ∇H(x)+τu)e -|u| 2 2 dy.</formula><p>Let us make the assumption that H is L-C 1 for some L &gt; 0 (c.f. definition 3.15) and furthermore assume that |∇H(x)| tends to infinity when x tends to infinity, ensuring the fact that the sets {x : |∇H(x)| ≤ c} are compact for c &gt; 0. We want to show that, if δ is small enough, (12.14) holds for h(x) = exp(mH(x)) and m small enough.</p><p>We first compute an upper bound of</p><formula xml:id="formula_1040">g(x, u) = mH(x -δ∇H(x) + τu) - |u| 2 2 .</formula><p>Using the L-C 1 property, we have</p><formula xml:id="formula_1041">g(x, u) ≤ mH(x) + m(-δ∇H(x) + τu) T ∇H(x) + mL 2 |δ∇H(x) -τu| 2 - |u| 2 2 = mH(x) -mδ(1 -δL/2)|∇H(x)| 2 + mτ(1 -τL)∇H(x) T u - 1 -mLτ 2 2 |u| 2 = mH(x) - 1 -mLτ 2 2 u - mτ(1 -τL) 1 -mLτ 2 ∇H(x) 2 -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) |∇H(x)| 2</formula><p>Assume that mLτ 2 ≤ 1. It follows that</p><formula xml:id="formula_1042">P h(x) = 1 (2π) d/2 R d e g(x,u) du ≤ h(x) (1 -mLτ 2 ) d/2 exp -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) |∇H(x)| 2</formula><p>Using this upper bound, we see that <ref type="bibr">(12.14)</ref> will hold if one first chooses δ such that δL &lt; 2, then m such that mLτ 2 &lt; 1 and</p><formula xml:id="formula_1043">mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) &lt; δ(1 -δL/2)</formula><p>and finally choose C = {x : |∇H(x)| ≤ c} where c is large enough so that</p><formula xml:id="formula_1044">1 (1 -mLτ 2 ) d/2 exp -m δ(1 -δL/2) - mτ 2 (1 -τL) 2 2(1 -mLτ 2 ) c 2 &lt; 1.</formula><p>Note that this Markov chain is not in detailed balance. Since P (x, •) has a p.d.f., being in detailed balance requires the ratio p(x, y)/p(y, x) to simplify as a ratio q(y)/q(x) for some function q, which does not hold. However, we can identify the invariant distribution approximately with small δ and τ, that we will assume to satisfy τ = a √ δ for a fixed a &gt; 0, with δ a small number.</p><p>We can write</p><formula xml:id="formula_1045">p(x, y) = 1 (2πτ 2 ) d/2 exp - 1 2τ 2 |y -x + δ∇H(x)| 2 = 1 (2πτ 2 ) d/2 exp - 1 2τ 2 |y -x| 2 - δ τ 2 (y -x) T ∇H(x) - δ 2 2τ 2 |∇H(x)| 2 .</formula><p>If q is a density, we have</p><formula xml:id="formula_1046">qP (y) = R d q(x)p(x, y)dx = 1 (2π) d/2 R d q(y + a √ δu) exp - 1 2 |u| 2 + √ δ a u T ∇H(y + a √ δu) - δ 2a 2 |∇H(y + a √ δu)| 2 du</formula><p>Make the expansions:</p><formula xml:id="formula_1047">q(y + a √ δu) = q(y) + a √ δ∇q(y) T u + a 2 δ 2 u T ∇ 2 q(y)u + o(δ|u| 2 ) and exp √ δ a u T ∇H(y + a √ δu) - δ 2a 2 |∇H(y + a √ δu)| 2 = 1 + √ δ a u T ∇H(y) - δ 2a 2 |∇H(y)| 2 + δu T ∇ 2 H(u)u + δ 2a 2 (u T ∇H(y)) 2 + o(δ|u| 2 ).</formula><p>Taking the product and using the fact that (2π</p><formula xml:id="formula_1048">) -d/2 R d u exp(-|u| 2 /2)du = 0 and that (2π) -d/2</formula><p>R d u T Au exp(-|u| 2 /2)du = trace(A) for any symmetric matrix A, we can write, taking the product:</p><formula xml:id="formula_1049">qP (y) = q(y) + δ a 2 2 ∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) + o(δ)</formula><p>This indicates that, if q is invariant by P , it should satisfy</p><formula xml:id="formula_1050">a 2 2 ∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) = o(1).</formula><p>The partial differential equation </p><formula xml:id="formula_1051">a 2 .</formula><p>Assuming that this function is integrable, this computation suggests that, for small δ, the Markov chain approximately samples from the probability distribution</p><formula xml:id="formula_1052">q 0 = 1 Z e - 2H(x) a 2 .</formula><p>This is further discussed in the next remark that involves stochastic differential equations. We will also present a correction of this Markov chain that samples from q 0 for all δ in section 12.5.2.</p><p>Remark 12.5 (Langevin equation) This chain is indeed the Euler discretization <ref type="bibr" target="#b124">[106]</ref> of the stochastic differential equation,</p><formula xml:id="formula_1053">dx t = -∇H(x t )dt + adw t (12.16)</formula><p>where w t is a Brownian motion. Under general hypotheses, this stochastic diffusion equation, called a Langevin equation, indeed converges in distribution to q 0 (x). 5   5 Providing a rigorous account of the theory of stochastic differential equations is beyond our scope, and we refer the reader to the many textbooks on the subject, such as McKean <ref type="bibr" target="#b149">[131]</ref>, Ikeda and Watanabe <ref type="bibr" target="#b114">[96]</ref>, Ethier and Kurtz <ref type="bibr" target="#b87">[69]</ref> (see also <ref type="bibr">Berglund [26]</ref> for a short introduction). Such diffusions are continuous-time Markov processes (X t , t ≥ 0), which means that the probability distribution of X t+s given all events before and including time s only depends on X s and is provided by a transition probability P t , with</p><formula xml:id="formula_1054">P(X t+1 ∈ A | X s = x) = P t (x, A).</formula><p>Similarly to deterministic ordinary differential equations, one shows that under sufficient regularity conditions (e.g., ∇H is C 1 ), equations such as <ref type="bibr">(12.16</ref>) have solutions up to some positive (random) explosion time, and that this explosion time is finite under additional conditions that ensure that |∇H(x)| does not grow too fast when x tends to infinity.</p><p>If ϕ is a smooth enough function (say, C 2 , with compact support), the function (t, x) → P t ϕ(x) satisfies the partial differential equation, called Kolmogorov's backward equation,</p><formula xml:id="formula_1055">∂ t P t ϕ(x) = -∇H(x) T ∇P t ϕ(x) + a 2 2 ∆P t ϕ(x)</formula><p>with initial condition P 0 ϕ(x) = ϕ(x). If P t (x, •) has at all times t a p.d.f. p t (x, •), then this p.d.f. must satisfy the forward Kolmogorov equation:</p><formula xml:id="formula_1056">∂ t p t (x, y) = ∇ 2 • (∇H(y)p t (x, y)) + a 2 2 ∆ 2 p t (x, y)</formula><p>where ∇ 2 and ∆ 2 indicate differentiation with respect to the second variable (y). (Recall that δf denotes the Laplacian of f .) Moreover, if Q is an invariant distribution with p.d.f. q, it satisfies the equation</p><formula xml:id="formula_1057">∇ • (q∇H) + a 2 2 ∆q(y) = 0.</formula><p>Noting that ∇ • (q∇H) = ∇q T ∇H + q∆H, we retrieve <ref type="bibr">(12.15)</ref>. Convergence properties (and, in particular, geometric convergence) of the Langevin equation to its limit distribution are studied in Roberts and Tweedie <ref type="bibr" target="#b184">[166]</ref>, using methods introduced in Meyn and Tweedie <ref type="bibr" target="#b152">[134,</ref><ref type="bibr" target="#b153">135,</ref><ref type="bibr" target="#b154">136</ref>] ♦ 12.4 Gibbs sampling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.4.1">Definition</head><p>The Gibbs sampling algorithm <ref type="bibr" target="#b97">[79]</ref> was introduced to sample from a distribution on large sets for which direct sampling is intractable and rejection samping is inefficient. It generates a Markov chain that converges (under some hypotheses) in distribution to this target probability. A general version of this algorithm is described below.</p><p>Let Q be a probability distribution on B. Consider a finite family U 1 , . . . , U K of random variables defined on B with values in measurable spaces B ′ 1 , . . . ,</p><formula xml:id="formula_1058">B ′ K . Let Q i = Q U i denote the image of Q by U i , defined by Q i (B i ) = Q(U i ∈ B i ) for B i ⊂ B i .</formula><p>Also, assume that there exists, for all i, a regular family of conditional probabilities for Q given U i , defined as a collection of transition probabilities (u i , A)</p><formula xml:id="formula_1059">→ Q i (u i , A) for u i ∈ B i and A ⊂ B, that satisfy A g(U i (x))Q(dx) = B i Q i (u i , A)g(u i )Q i (du i )</formula><p>for all nonnegative measurable functions g : B i → R. In simpler terms, Q i (u i , A) determine a consistent set of conditional probabilities for Q(A | U i = u i ). For discrete random variables (resp. variables with p.d.f.'s on R d ), they are just elementary conditional probabilities.</p><p>We then consider the following algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 12.2 (Gibbs sampling)</head><p>Initialize the algorithm with some z(0) = z 0 ∈ B and iterate the following two update steps given a current z(n) ∈ B:</p><p>(1) Select j ∈ {1, . . . , K} according to some pre-defined scheme, i.e., at random according to a probability distribution π (n) on the set {1, . . . , K}.</p><p>(2) Sample a new value z(n+1) according to the probability distribution Q j (U j (z(n)), •).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One typically chooses the probability distribution in</head><p>Step 1 equal to the uniform distribution on {1, . . . , K} (in which case it is independent on n), or to π (n) = δ j n where j n = 1+(n (mod) K) (periodic scheme). Strictly speaking, Gibbs sampling is a Markov chain if π (n) does not depend on n, and we will make this simplifying assumption in the rest of our discussion (therefore replacing π (n) by π). One obvious requirement for the feasibility of the method is that step (2) can be performed efficiently since it must be repeated a very large number of times.</p><p>One can see that the Markov chain generated by this algorithm is Q-reversible. Indeed, assume that X n ∼ Q. For any (measurable) subsets A and B in B, one has, using the definition of conditional expectations,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><formula xml:id="formula_1060">n ∈ A, X n+1 ∈ B) = K i=1 E 1 Z∈A Q i (U i (Z), B) π(i).</formula><p>(12.17) Now, for any i</p><formula xml:id="formula_1061">E 1 Z∈A Q i (U i (Z), B) = A Q i (U i (z), B)Q(dz) = B i Q i (u i , A)Q i (u i , B)Q i (du i ) which is symmetric in A and B.</formula><p>Note that, in the discrete case</p><formula xml:id="formula_1062">P (z, z) = n i=1 π(i) Q(z)1 U i (z)=U i (z) z ′ :U i (z ′ )=U i (z) Q(z ′ ) (12.18)</formula><p>and the relation</p><formula xml:id="formula_1063">Q(z)P (z, z) = Q(z)P (z, z) is obvious.</formula><p>The conditioning variables U 1 , . . . , U K should ensure, at least, that the associated Markov chain is irreducible and aperiodic. For irreducibility, this requires that Z can visits Q-almost all elements of B by a sequence of steps that lead one of the U i 's invariant.</p><p>Remark 12.6 In the standard version of Gibbs sampling, B is a product space B 1 × • • • × B K , and</p><formula xml:id="formula_1064">B ′ j = B 1 × • • • × B j-1 × B j+1 × • • • × B K .</formula><p>One then takes U j (z (1) , . . . , z (K) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (K) ). In other terms, step 2 in the algorithm replaces the current value of z (j) (n) by a new one sampled from the conditional distribution of Z (j) given the current values of z (i) (n), i j. ♦ Remark 12.7 We have considered a fixed number of conditioning variables, U 1 , . . . , U K , for simplicity, but the same analysis can be carried on if one replaces U j by a function U : (x, θ) → U θ (x) defined on a product space B × Θ, taking values in some space B, where Θ is a probability space equipped with a probability distribution π and B is measurable. The previous discussion corresponds to Θ = {1, . . . , K} and</p><formula xml:id="formula_1065">B = K i=1 {i} × B i (so that U i (x) is replaced by (i, U i (x))).</formula><p>One may then define Q θ as the image of Q by U θ and let Q θ (u, A) provide a version of Q(A | U θ = u). The only change in the previous discussion (besides using θ in index) is that (12.17) becomes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><formula xml:id="formula_1066">n ∈ A, X n+1 ∈ B) = Θ E 1 Z∈A Q θ (U θ (Z), B) π(dθ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>♦</head><p>Remark 12.8 Using notation from the previous remark, and allowing π = π (n) to depend on n, it is possible to allow π (n) to depend on the current state z(n) using the following construction.</p><p>For every step n, assume that there exists a subset Θ n of Θ such that π (n) (z, Θ n ) = 1 and that, for all θ ∈ Θ n , π (n) can be expressed in the form</p><formula xml:id="formula_1067">π (n) (z, •) = ψ (n) θ (U θ (z), •)</formula><p>for some transition probability ψ </p><formula xml:id="formula_1068">(n) θ from B θ to Θ n . The resulting chain remains Q-reversible, since P(X n ∈ A, X n+1 ∈ B) = B Θ n 1 z∈A Q θ (U θ (z), B)π (n) (z, dθ)Q(dz) = Θ n B 1 z∈A Q θ (U θ (z), B)ψ (n) θ (U θ (z), dθ)Q(dz) = Θ n B Q θ (u, A)Q θ (u, B)ψ (n) θ (u, dθ)Q θ (du). ♦<label>12</label></formula><formula xml:id="formula_1069">q(z) = 1 C exp         L j=1 αz (j) + L i,j=1,i&lt;j β ij z (i) z (j)         .</formula><p>Note that, although B is a finite set, its cardinality, 2 L , is too large for the enumerative procedure described in section 12.1 to be applicable as soon as L is, say, larger than 30. In practical applications of this model, L is orders of magnitude larger, typically in the thousands or tens of thousands.</p><p>We here apply standard Gibbs sampling, as described in remark 12.6, defining B j = {0, 1} and U i (z (1) , . . . , z (L) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (L) ).</p><p>The conditional distribution of Z (j) given U j (z) is a Bernoulli distribution with parameter</p><formula xml:id="formula_1070">q Z (j) (1 | U j (z)) = exp(α + L j ′ =1,j ′ j β jj ′ z (j ′ ) ) 1 + exp(α + L j ′ =1,j ′ j β ij z (j) )</formula><p>(taking β jj ′ = β j ′ j for j &gt; j ′ ). Gibbs sampling for this model will generate a sequence of variables Z(0), Z(1), . . . by fixing Z(0) arbitrarily and, given Z(n) = z, applying the two steps:</p><p>1. Select j ∈ {1, . . . , L} at random according to a probability distribution π (n) on the set {1, . . . , L}.</p><p>2. Sample a new value ζ ∈ {0, 1} according to the Bernoulli distribution with parameter q Z (j) n (1 | U j (z)), and set Z (j) (n + 1) = ζ and Z (j ′ ) (n + 1) = Z (j ′ ) (n) for j ′ j.</p><p>Let us now consider the Ising model with fixed total activation, namely the previous distribution conditional to S(z)</p><formula xml:id="formula_1071">∆ = z (1) + • • • + z (L) = h where 0 &lt; h &lt; L.</formula><p>The distribution one wants to sample from now is</p><formula xml:id="formula_1072">q h (z) = 1 C h exp         L j=1 αz (j) + L i,j=1,i&lt;j β ij z (i) z (j)         1 S(z)=h .</formula><p>In that case, the previous choice for the one-step transitions does not work, because fixing all but one coordinate of z also fixes the last one (so that the chain would not move from its initial value and would certainly not be irreducible). One can however fix all but two coordinates, therefore defining U ij (z (1) , . . . , z (L) ) = (z (1) , . . . , z (i-1) , z (i+1) , . . . , z (j-1) , z (j+1) , . . . , z (L) )</p><p>and</p><formula xml:id="formula_1073">B ij = {0, 1} 2 . If U ij (z)</formula><p>is fixed, the only acceptable configurations are z and the configuration z ′ deduced from z by switching the value of z (i) and z (j) . Thus, there is no possible change is z (i) = z (j) . If z (i) z (j) , then the probability of flipping the values of z (i) and z (j) is q h (z ′ )/(q h (z) + q h (z ′ )).</p><p>12.5 Metropolis-Hastings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.5.1">Definition</head><p>Gibbs sampling is a special case of a generic MCMC algorithm called Metropolis-Hastings that is defined as follows <ref type="bibr" target="#b151">[133,</ref><ref type="bibr" target="#b106">88]</ref>. Assume that the distribution Q has a density q with respect to a measure µ on B. Specify a transition probability on B, represented by a family of density functions with respect to µ, (g(z, •), z ∈ B), and a family of acceptance functions (z, z ′ ) → a(z, z ′ ) ∈ [0, 1]. Two basic examples are when B is finite, µ is the counting measure, and q and g are probability mass functions, and when B = R d , µ is Lebesgue's measure and q and g are probability density functions.</p><p>The sampling algorithm is then defined as follows. It invokes a function a that will be specified below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 12.3 (Metropolis-Hastings)</head><p>Initialize the algorithm with Z(0) = z(0) ∈ B. At step n, the current value Z(n) = z is then updated as follows.</p><p>• "Propose" a new configuration z ′ drawn according to g(z, •).</p><p>• "Accept" z ′ (i.e., set Z(n + 1) = z ′ ) with probability a(z, z ′ ). If the new value is rejected, keep the current one, i.e., let Z(n + 1) = z.</p><p>The transition probabilities for this process are p(x, y) = g(x, y)a(x, y) if x y and p(x, x) = 1y x p(x, y). The chain is Q-reversible is the detailed balance equation</p><formula xml:id="formula_1074">q(z)g(z, z ′ )a(z, z ′ ) = q(z ′ )g(z ′ , z)a(z ′ , z) (12.19)</formula><p>is satisfied. The functions g and a are part of the design of the algorithm, but <ref type="bibr">(12.19)</ref> suggest that g should satisfy the "weak symmetry" condition:</p><formula xml:id="formula_1075">∀x, y ∈ Ω : g(x, y) = 0 ⇔ g(y, x) = 0. (<label>12.20)</label></formula><p>Note that this condition is necessary to ensure (12.19) if q(z) &gt; 0 for all z. If q(z) &gt; 0, the fact that acceptance probabilities are less than 1 requires that a(z, z ′ ) ≤ min 1, q(z ′ )g(z ′ , z) q(z)g(z, z ′ ) .</p><p>If one takes a(z, z ′ ) equal to the r.h.s., so that</p><formula xml:id="formula_1076">a(z, z ′ ) = min 1, q(z ′ )g(z ′ , z) q(z)g(z, z ′ ) , (<label>12.21)</label></formula><p>then <ref type="bibr">(12.19</ref>) is satisfied as soon as q(z) &gt; 0. If q(z) = 0, then this definition ensures that a(z ′ , z) = 0 and (12.19) is also satisfied. Note also that the case g(z, z ′ ) = 0 is not relevant, since z ′ is not attainable from z in one step in this case. This shows that (12.21) provides a Q-reversible chain. Obviously, if g already satisfies q(z)g(z, z ′ ) = q(z ′ )g(z ′ , z), which is the case for Gibbs sampling, then one should take a(z, z ′ ) = 1 for all z and z ′ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.5.2">Sampling methods for continuous variables</head><p>While the Gibbs sampling and Metropolis-Hastings methods can be (and were) formulated for general variables and probability distributions, proving that the related chains are ergodic, and checking conditions for geometric convergence speed is much harder when dealing with general state spaces than with finite or compact spaces (see, e.g., <ref type="bibr" target="#b182">[164,</ref><ref type="bibr" target="#b150">132,</ref><ref type="bibr" target="#b24">6,</ref><ref type="bibr" target="#b183">165]</ref>). On the other hand, interesting choices of proposal transitions for Metropolis-Hastings are available when B = R d and µ is Lebesgue's measure, taking advantage, in particular, of differential calculus. More precisely, assume that q takes the form</p><formula xml:id="formula_1077">q(z) = 1 C exp(-H(z))</formula><p>for some smooth function H (at least C 1 ), such that exp(-H) is integrable. We saw in section 12.3.7 that, under suitable assumptions, the Markov chain</p><formula xml:id="formula_1078">X n+1 = X n - δ 2 ∇H(X n ) + √ δϵ n+1 (12.22)</formula><p>with ϵ n+1 ∼ N (0, Id R d ) has q as invariant distribution in the limit δ → 0. Its transition probability, such that g(z, •) is the p.d.f. of N (z-δ 2 ∇H(z), δId R d ), is therefore a natural choice for a proposal distribution in the Metropolis-Hastings algorithm. In addition to converging from the exact target distribution, this "Metropolis Adjusted Langevin Algorithm" (or MALA) can also be proved to satisfy geometric convergence under less restrictive hypotheses than <ref type="bibr">(12.22)</ref>  <ref type="bibr" target="#b184">[166]</ref>.</p><p>Another approach, similar to MALA is the Hamiltonian Monte-Carlo methods (or hybrid Monte-Carlo) <ref type="bibr" target="#b83">[65,</ref><ref type="bibr" target="#b160">142]</ref>. Inspired by physics, the method introduces a new variable, p ∈ R d , called "momentum," and defines the "Hamiltonian:"</p><formula xml:id="formula_1079">H(z, m) = H(z) + 1 2 |m| 2 .</formula><p>Fix a time θ &gt; 0. The proposal transition g(z, •) is then defined as the value ζ(θ) that is obtained by solving the Hamiltonian dynamical system</p><formula xml:id="formula_1080">∂ t ζ(t) = ∂ p H(ζ(t), µ(t)) = µ(t) ∂ t µ(t) = -∂ z H(ζ(t), µ(t)) = -∇H(ζ(t)) (12.23) with ζ(0) = z and µ(0) ∼ N (0, Id R d ). One can easily see that ∂ t H(ζ(t), µ(t)) = 0, which implies that H(ζ(t)) + 1 2 |µ(t)| 2 = H(z) + 1 2 |µ(0)| 2</formula><p>at all times t, or, denoting by ϕ N the p.d.f. of the d-dimensional standard Gaussian,</p><formula xml:id="formula_1081">q(ζ(t))ϕ N (µ(t)) = q(ζ(0))ϕ N (µ(0)).</formula><p>Moreover, if one denotes by Φ t (z, m) = (z t (z, m), m t (z, m)) the solution (ζ(t), µ(t)) of the system started with ζ(0) = z and µ(0) = m, one can also see that det(dΦ t (z, m)) = 1 at all times. Indeed, applying (1.5) and the chain rule, we have</p><formula xml:id="formula_1082">∂ t log det(dΦ t (z, m)) = trace(dΦ t (z, m) -1 ∂ t dΦ t (t, m)). From ∂ t z t (z, m) = m t (z, p) ∂ t p t (z, m) = -∇H(z t (z, m)) we get ∂ t dΦ t (z, m) = ∂ z m t (z, m) ∂ m m t (z, m) -∇ 2 H(z t (z, m))∂ z z t (z, m) -∇ 2 H(z t (z, m))∂ m z t (z, m) = 0 Id R d -∇ 2 H(z t (z, m)) 0 dΦ t (z, m).</formula><p>We therefore get</p><formula xml:id="formula_1083">∂ t log det(dΦ t (z, m)) = trace 0 Id R d -∇ 2 H(z t (z, m)) 0 = 0</formula><p>showing that the determinant is constant. Since Φ 0 (z, m) = (z, m) by definition, we get det(dΦ t (z, m)) = 1 at all times.</p><p>Let qt denote the p.d.f. of Φ t (z, m) and assume that q0 (z, m) = q(z)ϕ N (m). We have, using the change of variable formula</p><formula xml:id="formula_1084">qt (Φ t (z, m))| det dΦ t (z, m)| = q(z)ϕ N (m)</formula><p>but the r.h.s. is, from the remarks above also equal to</p><formula xml:id="formula_1085">q(z t (z, m))ϕ N (m t (z, m))| det dΦ t (z, m)| yielding the identification qt (z ′ , m ′ ) = q(z ′ )ϕ N (m ′ )</formula><p>This shows that Q (with p.d.f. q) is left invariant by this Markov chain. One can actually show that chain is in detailed balance for the joint density q(z, m) = q(z)ϕ N (m). This is due to the fact that the system (12.23) is reversible, in the sense that</p><formula xml:id="formula_1086">Φ t (z t (z, m), -m t (z, m)) = (z, -m),</formula><p>i.e., the system solved from its end point after changing the sign of the momentum returns to its initial state after changing the sign of the momentum a second time.</p><p>In other terms, letting J(z, m) = (z, -m), we have We have</p><formula xml:id="formula_1087">Φ -1 t = JΦ t •J. So, consider a function f : (R d × R d ) 2 → R.</formula><formula xml:id="formula_1088">E(f (Z n , M n , Z n+1 , M n+1 )) = f (z, m, z(z, m), m)ϕ N (m)ϕ N ( m)ϕ N ( m)q(z)dmd md mdz.</formula><p>Make the change of variables z ′ = z(z, m), m ′ = m(z, m), which has Jacobian determinant 1, and is such that z = z(z ′ , -m ′ ), m = -m(z ′ , -m ′ ). We get</p><formula xml:id="formula_1089">E(f (Z n , M n , Z n+1 , M n+1 )) = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (-m(z ′ , -m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , -m ′ ))dm ′ d md mdz ′ = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (m(z ′ , -m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , -m ′ ))dm ′ d md mdz ′ = f (z(z ′ , -m ′ ), m, z ′ , m)ϕ N (-m ′ )ϕ N ( m)ϕ N ( m)q(z ′ )dm ′ d md mdz ′ ,</formula><p>using the conservation of H. Making the change of variables m ′ → -m ′ , we get</p><formula xml:id="formula_1090">E(f (Z n , M n , Z n+1 , M n+1 ) = f (z(z ′ , m ′ ), m, z ′ , m)ϕ N (m ′ )ϕ N ( m)ϕ N ( m)q(z ′ )dm ′ d md mdz ′ which is equal to E(f (Z n+1 , M n+1 , Z n , M n ))</formula><p>showing the reversibility of the chain. This simulation scheme can potentially make large moves in the current configuration z while maintaining detailed balance (therefore not requiring an accept/reject step). However, practical implementations require discretizing (12.23), which breaks the conservation properties that were used in the argument above, therefore requiring a Metropolis-Hastings correction. For example, a second-order Runge Kutta (RK2) scheme with time step α gives</p><formula xml:id="formula_1091">           Z n+1 = Z n + αM n - α 2 2 ∇H(Z n ) M n+1 = M n - α 2 (∇H(Z n ) + ∇H(Z n + hM n ))</formula><p>Only the update for Z n matters, however, since M n+1 is discarded and resampled at each step. Importantly, if we let δ = √ α the first equation in the system becomes</p><formula xml:id="formula_1092">Z n+1 = Z n - δ 2 ∇H(Z n ) + δM n</formula><p>with M n ∼ N (0, 1), which is exactly <ref type="bibr">(12.22)</ref>. Note that one can, in principle, solve <ref type="bibr">(12.23</ref>) for more that one discretization step (the continuous equation can be solved for an arbitrary time), but one must then face the challenge of computing the Metropolis correction since the Hamiltonian is not conserved at each step.</p><p>One can however use schemes that are more adapted to solving Hamiltonian systems <ref type="bibr" target="#b137">[119]</ref>, such as the Störmer-Verlet scheme, which is</p><formula xml:id="formula_1093">               M n+1/2 = M n - α 2 ∇H(Z n ) Z n+1 = Z n + αM n+1/2 M n+1 = M n+1/2 - α 2 ∇H(Z n+1 ) This scheme computes ψ 1 • ψ 2 • ψ 1 (z, m) with ψ 1 (z, m) = (z, m -(α/2)∇H(z)</formula><p>) and ψ 2 (z, m) = (z + αm, m). Because both ψ 1 and ψ 2 have a Jacobian determinant equal to 1, so does their composition. This scheme is also reversible, since we have</p><formula xml:id="formula_1094">               -M n+1/2 = -M n+1 - α 2 ∇H(Z n+1 ) Z n = Z n+1 -αM n+1/2 -M n = -M n+1/2 - α 2 ∇H(Z n )</formula><p>These properties are conserved if one applies the Störmer-Verlet scheme more than once at each iteration, that is, fixing some N &gt; 0 and letting</p><formula xml:id="formula_1095">Φ(z, m) = (ψ 1 •ψ 2 •ψ 1 ) •N , then Φ -1 = JΦ • J, with J(z, m) = (z, -m) with det dΦ = 1. Considering again the aug- mented chain which, starting from (Z n , M n ), samples M ∼ N (0, Id R d ), then computes (Z ′ , M′ ) = Φ(Z n , M)</formula><p>and finally samples M ′ ∼ N (0, Id R d ) as a Metropolis-Hastings proposal to sample from (z, m) → q(z)ϕ N (m), then, assuming that (Z, M) follows this target distribution and letting (Z ′ , M ′ ) be the result of the proposal distribution, we have, as computed above</p><formula xml:id="formula_1096">E(f (Z, M, Z ′ , M ′ )) = f (z, m, z(z, m), m)ϕ N (m)ϕ N ( m)ϕ N ( m)q(z)dmd md mdz = f (z(z ′ , m ′ ), m, z ′ , m)ϕ N (m(z ′ , m ′ ))ϕ N ( m)ϕ N ( m)q(z(z ′ , m ′ ))dm ′ d md mdz ′</formula><p>This shows that the acceptance probability in the Metropolis step is</p><formula xml:id="formula_1097">a(z, m, z ′ , m ′ ) = min 1, ϕ N (m(z ′ , m ′ ))q(z(z ′ , m ′ )) ϕ N (m)q(z) = exp (-max (H(z(z ′ , m ′ ), m(z ′ , m ′ )) -H(z, m)) , 0)</formula><p>While the Hamiltonian is not kept invariant by the Störmer-Verlet scheme, so that an accept-reject step is needed, it is usually quite stable over extended periods of time so that the acceptance probability is generally close to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.6">Perfect sampling methods</head><p>The Markov chain simulation methods, provided in the previous sections do not provide exact samples from the distribution q, but only increasingly accurate ap-proximations. Perfect sampling algorithms <ref type="bibr" target="#b174">[156,</ref><ref type="bibr" target="#b175">157,</ref><ref type="bibr" target="#b89">71]</ref> use Markov chains "backwards" to generate exact samples. To describe them, it is easier to describe a Markov chain as a stochastic recursive equation of the form</p><formula xml:id="formula_1098">X n+1 = f (X n , U n+1 ) (12.24)</formula><p>where U n+1 is independent of X n , X n-1 , . . ., and the U k 's are identically distributed. In the discrete case (assumed in this section), and given a stochastic matrix P , one can take U n to be the uniformly distributed variable used to sample from (p(X n , x), x ∈ B).</p><p>Conversely, the transition probability associated to (12.24) is p(x, y) = P (f (x, U ) = y).</p><p>It will be convenient to consider negative times also. For n &gt; 0, recursively define F -n (x, u -n+1 , . . . , u 0 ) by</p><formula xml:id="formula_1099">F -n-1 (x, u -n , . . . , u 0 ) = F -n (f (x, u -n ), u -n+1 , . . . , u 0 ) and F -1 (x, u 0 ) = f (x, u 0 ). Denote, for short, U 0 -n = (U -n , . . . , U 0 ). The function F -n (x, u 0 -n+1 ) provides the value of X 0 when X -n = x and U 0 -n+1 = u 0 -n+1 .</formula><p>For an infinite past sequence, u 0 -∞ , let ν(u 0 -∞ ) denote the first integer n such that F -n (x, u 0 -n+1 ) does not depend on x (the function "coalesces"). Then, the following theorem is true: Theorem 12.9 Assume that the chain defined by (12.24) is ergodic, with invariant distribution Q. Then ν = ν(U 0 -∞ ) is finite with probability 1, and</p><formula xml:id="formula_1100">X * := F -ν (x, U 0 -ν+1 ) (12.<label>25</label></formula><formula xml:id="formula_1101">)</formula><p>(which is independent of x) has distribution Q.</p><p>Proof Because the chain is ergodic, we know that there exists an integer N such that one can pass from any state to any other with positive probability. So the chain can, starting from anywhere, coalesce with positive probability in N steps; ν being infinite would imply that this event never occurs in an infinite number of trials, and this has probability 0.</p><p>For any k &gt; 0 and any x ∈ B, we have</p><formula xml:id="formula_1102">X * = F -ν (f -k (x, U -ν -ν-k+1 ), U 0 -ν+1 ) = F -ν-k (x, U 0 -ν-k+1 ). (12.26)</formula><p>But, because the chain is ergodic, we have, for any x ∈ B</p><formula xml:id="formula_1103">lim k→∞ P(F -k (x, U 0 -k+1 ) = y) = Q(y).</formula><p>We can write</p><formula xml:id="formula_1104">P(F -k (x, U 0 -k+1 ) = y) = P(F -k (x, U 0 -k+1 ) = y, ν ≤ k) + P(F -k (x, U 0 -k+1 ) = y, ν &gt; k) = P(X * = y, ν ≤ k) + P(F -k (x, U 0 -k+1 ) = y, ν &gt; k)</formula><p>The right-hand side tends to P(X * = y) when k tends to infinity (because P(ν &gt; k) tends to 0), and the left-hand side tends to Q(y), which gives the second part of the theorem.</p><p>■ From (12.26), which is the key step in proving that X * follows the invariant distribution, one can see why it is important to consider sampling that expands backward in time rather than forward. More specifically, consider the coalescence time for the forward chain, letting ν(u ∞ 0 ) be the first index for which X * := F ν (x, u ν 0 ) is independent from the starting point, x. For any k ≥ 0, one still has the fact that F ν+k (x, u ν+k 0 ) does not depend on x, but its value depends on k and will not be equal to X * anymore, which prevents the rest of the proof of theorem 12.9 to carry on. An equivalent algorithm is described in the next proposition (the proof is easy and left to the reader).</p><p>Proposition 12.10 Using the same notation as above, the following algorithm generates a perfect sample, ξ * , of the invariant distribution of an ergodic Markov chain.</p><p>Assume that an infinite sample u 0 -∞ of U is available. Given this sequence, the algorithm, starting with t 0 = 2, is:</p><p>1. For all x ∈ B, define ξ x -t , t = -t 0 , . . . , 0 by ξ x -t 0 = x and ξ x -t+1 = f (ξ x -t , u -t+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If ξ x</head><p>0 is constant (independent of x), let ξ * be equal to this constant value and stop. Otherwise, return to step 1 replacing t 0 with 2t 0 .</p><p>In practice, the u -k 's are only generated when they are needed. But it is important to consider the sequence as fixed: once u -k is generated, it must be stored (or identically regenerated, using the same seed) for further use. It is important to strengthen the fact that this algorithm works backward in time, in the sense that the first states of the sequence are not identical at each iteration, because they are generated using random numbers with indexes further in the past. Such an algorithm is not feasible when |B| is too large, since one would have to consider an intractable number of Markov chains (one for each x ∈ B). However there are cases in which the constancy of ξ x 0 over all B can be decided from its constancy over a small subset of B.</p><p>One situation in which this is true is when the Markov chain is monotone, according to the following definition. Assume that B can be partially ordered, and that f in (12.24) is increasing in x, i.e.,</p><formula xml:id="formula_1105">x ≤ x ′ ⇒ ∀u, f (x, u) ≤ f (x ′ , u).</formula><p>(12.27)</p><p>Let B min and B max be the set of minimal and maximal elements in B. Then the sequence coalesces for the algorithm above if and only if it coalesces over B min ∪ B max . Indeed, any x ∈ B is smaller than some maximal element, and larger than some minimal element in B. By (12.27), these inequalities remain true at each step of the sampling process, which implies that when chains initialized with extremal elements coalesce, so do the other ones. Therefore, it suffices to run the algorithm with extremal configurations only.</p><p>One can rewrite (12.27) in terms of transition probabilities p(x, y), assuming that U follows a uniform distribution on [0, 1] and, for all x ∈ B, there exists a partition (I xy , y ∈ B) of B, such that f (x, u) = y ⇔ u ∈ I x,y and I xy is an interval with length p xy . Condition (12.27) is then equivalent to</p><formula xml:id="formula_1106">x ≤ x ′ ⇒ ∀y ∈ B, I xy ⊂ y ′ ≥y I x ′ y ′ .</formula><p>This requires in particular that y≥y 0 p(x, y) ≤ y≥y 0 p(x ′ , y) whenever x ≤ x ′ (one says that p(x, •) is stochastically smaller than p(x ′ , •)).</p><p>One example in which this reduction works is with the ferromagnetic Ising model, for which B = {-1, 1} L and</p><formula xml:id="formula_1107">q(x) = 1 C exp L s,t=1,s&lt;t β st x (s) x (t)</formula><p>with β st ≥ 0 for all {s, t}. Then, the Gibbs sampling algorithm iterates the following steps: take a random s ∈ {1, . . . , L} and update x (s) according to the conditional distribution g s (y (s) | x (s c ) ) = e y (s) v s (x) e -v s (x) + e v s (x) with v s (x) = t s β st x (t) . Order B so that x ≤ x if and only if x (s) ≤ x(s) for all s = 1, . . . , L. The minimal and maximal elements are unique in this case, with x (s) min ≡ -1 and x (s) max ≡ 1. Moreover, because all β st are non-negative, v s is an increasing function of x so that, if x ≤ x, then g s <ref type="bibr">(</ref></p><formula xml:id="formula_1108">1 | x (s) ) ≤ g s (1 | x(s) ).</formula><p>To define the stochastic iterations, first introduce</p><formula xml:id="formula_1109">f s (x, u) =        1 (s) ∧ x (s c ) if u ≤ q s (1 | x (s) ) (-1) (s) ∧ x (s c ) if u &gt; q s (1 | x (s) ),</formula><p>which satisfies <ref type="bibr">(12.27)</ref>. The whole updating scheme can then be implemented with the function</p><formula xml:id="formula_1110">f (x, (u, ũ)) = L s=1 δ I s ( ũ)f s (x, u)</formula><p>where (I s , s ∈ V ) is any partition of [0, 1] in intervals of length 1/L. This is still monotonic. The algorithm described in proposition 12.10 can therefore be applied to sample exactly, in finite time, from the ferromagnetic Ising model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.7">Application: Stochastic approximation with Markovian transitions</head><p>Using the material developed in this chapter, we now discuss the convergence of stochastic approximation methods (such as stochastic gradient descent) when the random random variable in the update term follows Markovian transitions. In section 3.3, we considered algorithms in the form</p><formula xml:id="formula_1111">ξ t+1 ∼ π X t X t+1 = X t + α t+1 H(X t , ξ t+1 )</formula><p>where ξ t : Ω → R ξ is a random variable. We now want to addres situations in which the random variable ξ t+1 is obtained through a transition probability, therefore considering the algorithm ξ t+1 ∼ P X t (ξ t , •)</p><formula xml:id="formula_1112">X t+1 = X t + α t+1 H(X t , ξ t+1 )<label>(12.28)</label></formula><p>Here P x is, for all x, a transition probability from R ξ to R ξ . We will assume that, for all x ∈ R d , the Markov chain with transition P x is geometrically ergodic, and we denote by π x its invariant distribution. We let, as in section 3.3, H(x) = E π x (H(x, •)).</p><p>We will use the notation for a function f : R d × R ξ → R</p><formula xml:id="formula_1113">P x f : (x ′ , ξ) ∈ R d × R ξ → P x f (x ′ , ξ) = R ξ f (x ′ , ξ ′ )P x (ξ, dξ ′ ) and π x f : x ′ ∈ R d → π x f (x ′ ) = R ξ f (x ′ , ξ)π x (dξ).</formula><p>In particular, H(x) = π x H(x). We also define h(x, ξ) = H(x, ξ) -H(x) and h(x, ξ) = P x h(x, ξ). We make the following assumptions.</p><p>(H1) There exists constants C 0 , C 1 , c 2 such that, for all x, y ∈ R d , sup</p><formula xml:id="formula_1114">ξ∈R ξ |H(x, ξ)| ≤ C 0 , (12.29a) sup ξ∈R ξ | h(x, ξ)| ≤ C 1 , (12.29b) sup ξ∈R ξ | h(x, ξ) -h(y, ξ)| ≤ C 1 |x -y|, (12.29c) D var (π x , π y ) ≤ C 2 |x -y| (12.29d) (H2) There exists x * ∈ R d and µ &gt; 0 such that, for all x ∈ R d (x -x * ) T H(x) ≤ -µ|x -x * | 2 .</formula><p>(12.30) (H3) We assume that there exists a constant M and a non-decreasing function ρ : [0, +∞) → [0, 1) such that, for all probability distributions Q and</p><formula xml:id="formula_1115">Q ′ on R ξ , D var (QP n x , Q ′ P n x ) ≤ Mρ(|x|) n D var (Q, Q ′ ). (<label>12</label></formula><p>.31) (H4) The sequence α 1 , α 2 , . . . is non-increasing, with ∞ t=1 α t = +∞ and ∞ t=1 α 2 t &lt; +∞. (12.32a) Let σ t = t s=1 α s . If C 1 &gt; 0, we also require that lim t→∞ α t σ t (1ρ(σ t )) -1 = 0 (12.32b) and t s=2 α 2 s</p><formula xml:id="formula_1116">σ s (1 -ρ(σ s )) -2 &lt; ∞.<label>(12.32c)</label></formula><p>Given this, the following theorem holds.</p><p>Theorem 12.11 Assuming (H1) to (H4), the sequence defined by (12.28) is such that</p><formula xml:id="formula_1117">lim t→∞ E(|X t -x * | 2 ) = 0</formula><p>Remark 12.12 Condition (H1) assumes that H is bounded and uniformly Lipschitz in x, which is more restrictive than what was assumed in section 3.3.2, but applies, for example, to situations considered in Younes <ref type="bibr" target="#b224">[206]</ref> and later in this book in section 17.2.2.</p><p>Condition (H3) implies that the Markov chain with transition P x is uniformly geometrically ergodic, but the ergodicity rate may depend on x and in particular converge to 1 when x tends to ∞, which is the situation targeted in this theorem.</p><p>The reader may refer to <ref type="bibr" target="#b226">[208]</ref> for a general discussion of this problem with relaxed hypotheses and almost sure convergence, at the expense of significantly longer proofs.</p><p>♦</p><p>Proof We note that, from (12.29a), one has</p><formula xml:id="formula_1118">|X t -x * | ≤ C 0 σ t |X 0 -x * |.<label>(12.33)</label></formula><p>Similarly to section 3.3.2, we let A t = |X tx * | 2 and a t = E(A t ). One can then write</p><formula xml:id="formula_1119">A t+1 = A t +2α t+1 (X t -x * ) T H(X t )+2α t+1 (X t -x * ) T (H(X t , ξ t+1 )-H(X t ))+α 2 t+1 |H(X t , ξ t+1 )| 2 but we do not have E((X t -x * ) T (H(X t , ξ t+1 ) -H(X t )) | U t ) = 0</formula><p>anymore, where U t is the σ -algebra of all past events up to time t (all events depending of X s , ξ s , s ≤ t). Indeed the Markovian assumption implies that</p><formula xml:id="formula_1120">E((X t -x * ) T (H(X t , ξ t+1 ) -H(X t )) | U t ) = (X t -x * ) T       R ξ H(X t , ξ)P X t (ξ t , dξ) -H(X t )       = (X t -x * ) T ((P X t H(X t , •))(ξ t ) -H(X t )),</formula><p>which does not vanish in general. Following Benveniste et al. <ref type="bibr" target="#b43">[25]</ref>, this can be addressed by introducing the solution g(x, •) of the "Poisson equation"</p><formula xml:id="formula_1121">g(x, •) -P x g(x, •) = h(x, •).<label>(12.34)</label></formula><p>(Recall that h(x, ξ) = H(x, ξ) -H(x).) One can then write</p><formula xml:id="formula_1122">(X t -x * ) T h(X t , ξ t+1 ) = (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t+1 )</formula><p>and</p><formula xml:id="formula_1123">A t+1 ≤ (1 -2α t+1 µ)A t + 2α t+1 (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t ))) + 2α t+1 (X t -x * ) T P X t g(X t , ξ t ) -2α t+1 (X t -x * ) T P X t g(X t , ξ t+1 ) + α 2 t+1 |H(X t , ξ t+1 )| 2</formula><p>Introducing the notation</p><formula xml:id="formula_1124">η st = E((X s -x * ) T P X s g(X s , ξ t )).</formula><p>Using the fact that</p><formula xml:id="formula_1125">E (X t -x * ) T (g(X t , ξ t+1 ) -P X t g(X t , ξ t ))) | U t = 0</formula><p>and and noting that |H(X t , ξ t+1 )| 2 ≤ C 2 0 , this gives, after taking expectations,</p><formula xml:id="formula_1126">a t+1 ≤ (1 -2α t+1 µ)a t + 2α t+1 η tt -2α t+1 η t,t+1 + α 2 t+1 C 2 0 .</formula><p>Applying lemma 3.25, and letting v s,t = t j=s+1 (1 -2α j+1 µ), we get</p><formula xml:id="formula_1127">a t ≤ a 0 v 0,t + 2 t s=1 v s,t α s+1 (η ss -η s,s+1 ) + C 2 0 t s=1 v s,t α 2 s+1 .</formula><p>We now want to ensure that each term in the upper bound converges to 0. Similarly to section 3.3.2, (12.32a) implies that this holds the first and last terms and we therefore focus on the middle one, writing</p><formula xml:id="formula_1128">t s=1 v s,t α s+1 (η ss -η s,s+1 ) = v 1,t α 2 η 11 -α t+1 η t,t+1 + t s=2 (v s,t α s+1 -v s-1,t α s )η ss (12.35) + t s=2 v s-1,t α s (η ss -η s-1,s )</formula><p>We will need the following estimates on the function g in (12.34), which is defined by</p><formula xml:id="formula_1129">g(x, ξ) = ∞ n=0 P n x h(x, ξ) = h(x, ξ) + ∞ n=0 P n x h(x, ξ).</formula><p>Lemma 12. <ref type="bibr" target="#b31">13</ref> We have</p><formula xml:id="formula_1130">|g(x, •)| ≤ C 0 + 2C 1 M(1 -ρ(x)) -1 , (<label>12.36a</label></formula><p>)</p><formula xml:id="formula_1131">|P x g(x, •)| ≤ 2C 1 M(1 -ρ(x)) -1 . (12.36b)</formula><p>and, for all x, y ∈ R d and ξ ∈ R ξ Using lemma lemma 12.13 (which is proved at the end of the section), we can control the terms intervening in <ref type="bibr">(12.35)</ref>. Note that the first term, v 1t α 2 η 11 , converges to 0 since (12.32a) implies that v 1t converges to 0.</p><formula xml:id="formula_1132">|P x g(x, ξ) -P y (g(y, ξ)| = M 2 C 1 C 2 (1 -ρ) -2 + MC 1 (1 + C 2 )(1 -ρ) -1 . (<label>12</label></formula><p>We have,</p><formula xml:id="formula_1133">α t+1 |E((X t -x * ) T P X t g(X t , ξ t+1 ))| ≤ 2MC 1 α t+1 σ t (1 -ρ(σ t )) -1 ,</formula><p>so that (12.32b) implies that α t+1 η t,t+1 → 0. and since α s+1 ≤ α s , we have</p><formula xml:id="formula_1134">t s=2 (v s,t α s+1 -v s-1,t α s )η ss ≤ t s=2 |v st α s -v s,t α s+1 | |η ss | ≤ MC 1 t s=2 |v s-1,t α s -v s,t α s+1 | α s+1 σ s (1 -ρ(σ s )) -1 ≤ C t s=2 |v s-1,t α s -v s,t α s+1 | for some constant C, since α s+1 σ s (1 -ρ(σ s )) -1 is bounded. Writing v s,t α s+1 -v s-1,t α s = v st (α s+1 -α s + 2µα 2 s ), we get (using α s+1 ≤ α s ) t s=2 |v s-1,t α s -v s,t α s+1 | ≤ t s=2 v st (α s -α s+1 ) + t s=2 v st 2µα 2 s .</formula><p>Since both s (α sα s+1 ) and t s=2 α 2 s converge (the former is just α 1 ), lemma 3.26 implies that t s=2 (v s,t α s+1v s-1,t α s )η ss tends to zero. The last term to consider is</p><formula xml:id="formula_1135">t s=2 v s-1,t α s (η ss -η s-1,s ) = t s=2 v s-1,t α s E((X s -X s-1 ) T P X s g(X s , ξ s )) + t s=2 v s-1,t α s E((X s-1 -x * ) T (P X s g(X s , ξ s )) -P X s-1 g(X s-1 , ξ s ))).</formula><p>We have</p><formula xml:id="formula_1136">t s=2 v s-1,t α s E((X s -X s-1 ) T P X s g(X s , ξ s )) ≤ 2C 0 C 1 M t s=2 v s-1,t α 2 s (1 -ρ(σ s )) -1</formula><p>and</p><formula xml:id="formula_1137">t s=2 v s-1,t α s E((X s-1 -x * ) T (P X s g(X s , ξ s )) -P X s-1 g(X s-1 , ξ s ))) ≤ 2M 2 C 0 C 1 (1 + C 2 )|X 0 -x * | t s=2 v s-1,t α 2 s σ s (1 -ρ(σ s )) -2</formula><p>and lemma 3.26 implies that both terms vanish at infinity. This concludes the proof of theorem 12.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Proof (Proof of lemma 12.13) Condition (H3) and proposition 12.3 and imply that (since π x h = 0)</p><formula xml:id="formula_1138">|P n x h(x, ξ)| ≤ D var (P n x (ξ, •), π x )osc( h(x, •)) ≤ 2C 1 Mρ(x) n so that g is well defined with |g(x, •)| ≤ C 0 + 2C 1 M(1 -ρ(x)) -1 , |P x g(x, •)| ≤ 2C 1 M(1 -ρ(x)) -1 .</formula><p>We will also need to control differences of the kind P x g(x, ξ) -P y g(y, ξ).</p><p>We consider the nth term in the series, writing</p><formula xml:id="formula_1139">P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 (P n-k x P k y h(y, ξ) -(P n-k-1</formula><p>x P k+1 y h(y, ξ))</p><formula xml:id="formula_1140">+ P n x h(x, ξ) -P n x h(y, ξ).</formula><p>This gives</p><formula xml:id="formula_1141">P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y) + π x P k+1 y h(y)) + n-1 k=0 (π x P k y h(y) -π x P k+1 y h(y)) + P n x h(x, ξ) -P n x h(y, ξ) = n-1 k=0 P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y) + π x P k+1 y h(y))</formula><p>+ π x h(y)π x P n y h(y) + P n x h(x, ξ) -P n x h(y, ξ)</p><formula xml:id="formula_1142">Finally P n x h(x, ξ) -P n y h(y, ξ) = n-1 k=0 P n-k-1</formula><p>x (P x P k y h(y, ξ) -P k+1 y h(y, ξ)π x P k y h(y) + π x P k+1 y h(y))</p><formula xml:id="formula_1143">+ P n x ( h(x, ξ) -h(y, ξ) + π x h(y)) -(π x -π y )P n y h(y)</formula><p>Using proposition 12.3, we can write, letting ρ = max(ρ(|x|), ρ(|y|)),</p><formula xml:id="formula_1144">|P n-k-1 x (P x P k y h(y, ξ) -P k+1 y h(y, ξ) -π x P k y h(y, ξ) + π x P k+1 y h(y, ξ))| ≤ M ρn-k-1 osc(P x P k y h(y, ξ) -P k+1 y h(y, ξ)) ≤ C 2 M ρn-k-1 |x -y|osc(P k y h(y, ξ)) ≤ C 2 C 1 M 2 ρn-1 |x -y| We also have |P n x ( h(x, ξ) -h(y, ξ) + π x h(y, ξ))| ≤ MC 1 ρn |x -y| and |(π x -π y )P n y h(y, ξ)| ≤ MC 2 C 1 ρn |x -y| so that |P n x h(x, ξ) -P n y h(y, ξ)| ≤ MC 1 ρn-1 (nMC 2 + (1 + C 2 ) ρ)|x -y|</formula><p>From this, it follows that</p><formula xml:id="formula_1145">|P x g(x, ξ) -P y (g(y, ξ)| ≤ MC 1 ∞ n=1 ρn-1 (nMC 2 + (1 + C 2 )|x -y| = M 2 C 1 C 2 (1 -ρ) -2 + MC 1 (1 + C 2 )(1 -ρ) -1 . ■ Chapter 13</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markov Random Fields</head><p>With this chapter, we start a discussion of large-scale statistical models in data science, starting with graphical models (Markov random fields and Bayesian networks) before discussing more recent approaches using, notably, deep learning. Important textbook references for the present chapter include Pearl <ref type="bibr" target="#b169">[151]</ref>, Ancona et al. <ref type="bibr" target="#b26">[8]</ref>, Winkler <ref type="bibr" target="#b221">[203]</ref>, Lauritzen <ref type="bibr" target="#b132">[114]</ref>, Cowell et al. <ref type="bibr" target="#b74">[56]</ref>, Koller and Friedman <ref type="bibr" target="#b126">[108]</ref>.</p><p>13.1 Independence and conditional independence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1.1">Definitions</head><p>We consider random variables X, Y , Z . . ., and denote by R X , R Y , R Z . . . the sets in which they take their values. We discuss in this section concepts of independence and conditional independence between random variables. To simplify the exposition, we will work (unless mentioned otherwise) with discrete random variables (X is discrete if R X is finite or countable) <ref type="foot" target="#foot_16">1</ref> . We start with a basic definition.</p><formula xml:id="formula_1146">Definition 13.1 Two discrete random variables X : Ω → R X and Y : Ω → R Y are inde- pendent if and only if ∀x ∈ R X , ∀y ∈ R Y : P(X = x, Y = y) = P(X = x)P(Y = y).</formula><p>The general definition for arbitrary r.v.'s is that</p><formula xml:id="formula_1147">E(f (X)g(Y )) = E(f (X)) E(g(Y ))</formula><p>for any pair of (measurable) non-negative functions f : R X → [0, +∞) and g : R Y → [0, +∞).</p><p>One can easily check that X and Y are independent if and only if, for any nonnegative function g : R Y → R, one has</p><formula xml:id="formula_1148">E(g(Y ) | X) = E(g(Y )).</formula><p>Notation 13.2 Independence is a property that involves two variables X and Y and an underlying probability distribution P. Independence of X and Y relative to P will be denoted (X Y ) P . However we will only write X Y when there is no ambiguity on P. ♦ More than independence, the concept of conditional independence will be fundamental in this chapter. It requires three variables, say X, Y , Z. Returning to the discrete case, one says that X and Y are conditionally independent given Z is, for any</p><formula xml:id="formula_1149">x ∈ R X , y ∈ R Y and z ∈ R Z such that P(Z = z) &gt; 0, P(X = x, Y = y | Z = z) = P(X = x | Z = z) P(Y = y | Z = z). (13.1)</formula><p>An equivalent statement is that, for any z such that P(Z = z) 0, X and Y are independent when P is replaced by the conditional distribution P(• | Z = z).</p><p>In the general case conditional independence means that, for any pair of nonnegative measurable functions f and g,</p><formula xml:id="formula_1150">E(f (X)g(Y ) | Z) = E(f (X) | Z) E(g(Y ) | Z). (13.2)</formula><p>From now, we restrict our discussion to discrete random variables.</p><p>Multiplying both terms in (13.1) by P(Z = z) 2 , we get the equivalent statement:</p><p>X and Y are conditionally independent given Z if and only if, ∀x, y, z : P(X = x, Y = y, Z = z)P(Z = z) = P(X = x, Z = z) P(Y = y, Z = z). <ref type="bibr">(13.3)</ref> Note that the identity is meaningful, and always true, for P(Z = z) = 0, so that this case does not need to be excluded anymore.</p><p>Conditional independence can be interpreted by the statement that X brings no more information on Y than what is already provided by Z: one has</p><formula xml:id="formula_1151">P(Y = y | X = x, Z = z) = P(Y = y, X = x, Z = z) P(X = x, Z = z) = P(Y = y, Z = z) P(Z = z)</formula><p>as directly deduced from <ref type="bibr">(13.3)</ref>. (This computation being valid as soon as P(X =</p><p>x, Z = z) &gt; 0.)</p><formula xml:id="formula_1152">Notation 13.3</formula><p>To indicate that X and Y are conditionally independent given Z for the distribution P, we will write (X Y | Z) P or simply (X Y | Z).</p><formula xml:id="formula_1153">♦ (CI2) Decomposition: (X (Y , W ) | Z) ⇒ (X Y | Z). (CI3) Weak union: (X (Y , W ) | Z) ⇒ (X Y | (Z, W )). (CI4) Contraction: (X Y | Z) and (X W | (Z, Y )) ⇒ (X (Y , W ) | Z).</formula><p>(CI5) Intersection: assume that the joint distribution of W , Y and Z is positive. Then</p><formula xml:id="formula_1154">(X W | (Z, Y )) and (X Y | (Z, W )) ⇒ (X (Y , W ) | Z).</formula><p>Proof Properties (CI1) and (CI2) are easily deduced from (13.3) and left to the reader. To prove the last three, we will use the notation P (x), P (x, y) etc. instead of P(X = x), P(X = x, Y = y), etc. to save space. Identities are assumed to hold for all</p><p>x, y, z, w unless stated otherwise.</p><p>For (CI3), we must prove, according to (13.3), that P (x, y, z, w)P (z, w) = P (x, z, w)P (y, z, w) (13.4) whenever P (x, y, z, w)P (z) = P (x, z)P (y, z, w). Summing this last equation over y (or applying (CI2)) yields P (x, z, w)P (z) = P (x, z)P (z, w). We can note that all terms in (13.4) vanish when P (z) = 0, so that the identity is true in this case. When P (z) 0, the right-hand side of (13.4) becomes (P (x, z)P (z, w)/P (z))P (y, z, w) = (P (x, z)P (y, z, w)/P (z))P (z, w) = P (x, y, z, w)P (z, w), using once again the hypothesis. This proves (CI3).</p><p>For (CI4), the hypotheses are</p><formula xml:id="formula_1155">      </formula><p>P (x, y, z)P (z) = P (x, z)P (y, z) P (x, y, z, w)P (y, z) = P (x, y, z)P (y, z, w)</p><p>and the conclusion must be P (x, y, z, w)P (z) = P (x, z)P (y, z, w). (13.5) Since (13.5) is true when P (y, z) = 0, we assume that this probability does not vanish and write P (x, y, z, w)P (z) = P (x, y, z)P (z)P (y, z, w)/P (y, z) = P (x, z)P (y, z)P (y, z, w)/P (y, z) = P (x, z)P (y, z, w) yielding (13.5).</p><p>For (CI5), assuming</p><formula xml:id="formula_1156">      </formula><p>P (x, y, z, w)P (y, z) = P (x, y, z)P (y, z, w) P (x, y, z, w)P (z, w) = P (x, z, w)P (y, z, w), <ref type="bibr">(13.6)</ref> we want to show that P (x, y, z, w)P (z) = P (x, z)P (y, z, w).</p><p>Since this identity is true when any of the events W = w, Y = y or Z = z has zero probability, we can assume that their probabilities are positive, which, by assumption, also implies that all joint probabilities are positive. From the two identities, we get P (x, y, z, w)/P (y, z, w) = P (x, y, z)/P (y, z) = P (x, z, w)/P (z, w)</p><p>This implies P (x, y, z) = P (y, z)P (x, z, w)/P (z, w)</p><p>that we can sum over y to obtain P (x, z) = P (z)P (x, z, w)/P (z, w)</p><p>We therefore get P (x, y, z, w)/P (y, z, w) = P (x, z, w)/P (z, w) = P (x, z)/P (z),</p><p>which is what we wanted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>A counter-example of (CI5) when the positivity assumption is not satisfied can be built as follows: let X be a Bernoulli random variable, and let Y = W = X. Let Z be any Bernoulli random variable, independent from X. Given Z and W , X and Y are constant and therefore independent. Similarly, given Z and Y , X and W are constant and therefore independent. However, given Z, X and (Y , W ) are not independent (they are equal and non constant).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1.3">Mutual independence</head><p>Another concept of interest is the mutual (conditional) independence of more than two random variables. The random variables (X 1 , . . . , X n ) are mutually conditionally independent given Z if and only if</p><formula xml:id="formula_1157">E(f 1 (X 1 ) • • • f n (X n ) | Z) = E(f 1 (X 1 ) | Z) • • • E(f n (X n ) | Z)</formula><p>for any non-negative measurable functions f 1 , . . . , f n . In terms of discrete probabilities, this can be written as</p><formula xml:id="formula_1158">P (X 1 = x 1 , . . . , X n = x n , Z = z)P (Z = z) n-1 = P (X 1 = x 1 , Z = z) • • • P (X n = x n , Z = z).</formula><p>This will be summarized with the notation</p><formula xml:id="formula_1159">(X 1 • • • X n | Z).</formula><p>We have the proposition Proposition 13.6 For variables X 1 , . . . , X n and Z, the following properties are equivalent.</p><formula xml:id="formula_1160">(i) (X 1 • • • X n | Z);</formula><p>(ii) For all S, T ⊂ {1, . . . , n} with S ∩ T = ∅, we have:</p><formula xml:id="formula_1161">((X i , i ∈ S) (X j , j ∈ T ) | Z);</formula><p>(iii) For all s ∈ {1, . . . , n}, we have: (X s (X t , t s) | Z);</p><p>(iv) For all s ∈ {2, . . . , n}, we have:</p><formula xml:id="formula_1162">(X s (X 1 , . . . , X s-1 ) | Z).</formula><p>Proof It is clear that (i) ⇒ • • • ⇒ (iv) so it suffices to prove that (iv) ⇒ (i). For this, simply write (applying (iv) repeatedly to</p><formula xml:id="formula_1163">s = n -1, n -2, . . .) E(f 1 (X 1 ) • • • f n (X n ) | Z) = E(f 1 (X 1 ) • • • f n-1 (X n-1 ) | Z) E(f n (X n ) | Z) = E(f 1 (X 1 ) • • • f n-2 (X n-2 ) | Z) E(f n-1 (X n-1 ) | Z) E(f n (X n ) | Z) . . . = E(f 1 (X 1 ) | Z) • • • E(f n (X n ) | Z).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1.4">Relation with Information Theory</head><p>Several concepts in information theory are directly related to independence between random variables. Recall that the (Shannon) entropy of a discrete probability distribution over a finite set R is defined by</p><formula xml:id="formula_1164">H(P ) = - ω∈R log P (ω)P (ω). (13.7)</formula><p>Similarly, the entropy of a random variable X : Ω → R X is defined by</p><formula xml:id="formula_1165">H(X) ∆ = H(P X ) = - x∈R X log P (X = x)P (X = x). (13.8)</formula><p>The entropy is always non-negative, and provides a measure of the uncertainty associated to P . For a given finite set R, it is maximal when P is uniform over R, and minimal (and vanishes) when P is supported by a single ω ∈ R (i.e. P (ω) = 1).</p><p>One defines the entropy of two or more random variables as the entropy of their joint distribution, so that, for example,</p><formula xml:id="formula_1166">H(X, Y ) = - (x,y)∈R X ×R Y log P(X = x, Y = y)P(X = x, Y = y).</formula><p>We have the proposition: Proposition 13.7 For random variables X 1 , . . . , X n , one has</p><formula xml:id="formula_1167">H(X 1 , . . . , X n ) ≤ H(X 1 ) + • • • + H(X n )</formula><p>with equality if and only if (X 1 , . . . , X n ) are mutually independent.</p><p>Proof The proof of this proposition uses properties of the Kullback-Leibler divergence (c.f. (4.3)), given by, for two probability distributions π and π ′ on a finite set B,</p><formula xml:id="formula_1168">KL(π∥π ′ ) = ω∈B π(ω) log π(ω) π ′ (ω) .</formula><p>with the convention π log(π/π ′ ) = 0 if π = 0 and = ∞ if π &gt; 0 and π ′ = 0. Returning to proposition 13.7, a straightforward computation (which is left to the reader) shows that</p><formula xml:id="formula_1169">H(X 1 ) + • • • + H(X n ) -H(X 1 , . . . , X n ) = KL(π∥π ′ ) with π(x 1 , . . . , x n ) = P(X 1 = x 1 , . . . , X n = x n ) and π ′ (x 1 , . . . , x n ) = n k=1 P(X k = x k</formula><p>). This makes proposition 13.7 a direct consequence of proposition 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The mutual information between two random variables X and Y is defined by</p><formula xml:id="formula_1170">I (X, Y ) = H(X) + H(Y ) -H(X, Y ). (<label>13</label></formula><p>.9)</p><p>From proposition 13.7, I (X, Y ) is nonnegative and vanishes if and only if X and Y are independent. Also from the proof of proposition 13.7, I (X, Y ) is equal to KL(P (X,Y ) ∥P X ⊗ P Y ) where the first probability is the joint distribution of X and Y and the second one the product of the marginals of X and Y , which coincides with P X,Y if and only if X and Y are independent.</p><p>If X and Y are two random variables, and y ∈ R Y with P(Y = y) &gt; 0, the entropy of the conditional probability</p><formula xml:id="formula_1171">x → P(X = x | Y = y) is denoted H(X | Y = y), and is a function of y. The conditional entropy of X given Y , denoted H(X | Y ) is the expectation of H(X | Y = y) for the distribution of Y , i.e., H(X | Y ) = y∈R Y H(X | Y = y)P(Y = y) = - x∈R X y∈R Y log P(X = x | Y = y)P(X = x, Y = y).</formula><p>So, we have (with a straightforward proof) Proposition 13.8 Given two random variables X and Y , we have</p><formula xml:id="formula_1172">H(X | Y ) = -E X,Y (log P(X = • | Y = •)) (13.10) = H(X, Y ) -H(Y )</formula><p>This proposition also immediately yields:</p><formula xml:id="formula_1173">I (X, Y ) = H(X) -H(X | Y ) = H(Y ) -H(Y | X).</formula><p>(13.11)</p><p>The identity H(X, Y ) = H(X | Y ) + H(Y ) that is deduced from proposition 13.8 can be generalized to more than two random variables (the proof being left to the reader), yielding, if X 1 , . . . , X n are random variables:</p><formula xml:id="formula_1174">H(X 1 , . . . , X n ) = n k=1 H(X k | X 1 , . . . , X k-1 ). (13.12)</formula><p>If Z is an additional random variable, the following identity is obtained by applying the previous one to conditional distributions given Z = z and taking averages over z:</p><formula xml:id="formula_1175">H(X 1 , . . . , X n | Z) = n k=1 H(X k | X 1 , . . . , X k-1 , Z). (<label>13.13)</label></formula><p>The following proposition characterizes conditional independence in terms of entropy.</p><p>Proposition 13.9 Let X, Y and Z be three random variables. The following statements are equivalent.</p><p>(i) X and Y are conditionally independent given Z.</p><formula xml:id="formula_1176">(ii) H(X, Y | Z) = H(X | Z) + H(Y | Z) (iii) H(X | Y , Z) = H(X | Y )</formula><p>Moreover, when (i) to (iii) are satisfied, we have:</p><formula xml:id="formula_1177">(iv) I (X, Y ) ≤ min(I (X, Z), I (Y , Z)).</formula><p>Proof From proposition 13.7, we have, for any three random variables X, Y , Z, and any z such that P (Z = z) &gt; 0,</p><formula xml:id="formula_1178">H(X, Y | Z = z) ≤ H(X | Z = z) + H(Y | Z = z).</formula><p>Taking expectations on both sides implies the important inequality</p><formula xml:id="formula_1179">H(X, Y | Z) ≤ H(X | Z) + H(Y | Z) (13.14)</formula><p>and equality occurs if and only if P(</p><formula xml:id="formula_1180">X = x, Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z) whenever P(Z = z) &gt; 0,</formula><p>that is, if and only if X and Y are conditionally independent given Z. This proves that (i) and (ii) are equivalent. The fact that (ii) and (iii) are equivalent comes from (13.13), which gives, for any three random variables</p><formula xml:id="formula_1181">H(X, Y | Z) = H(X | Y , Z) + H(Y | Z). (13.15)</formula><p>To prove that (i)-(iii) implies (iv), we note that (13.14) and (13.15) imply that, for any three random variables:</p><formula xml:id="formula_1182">H(X | Y , Z) ≤ H(X | Y ).</formula><p>If X and Y are conditionally independent given Z, then the right-hand side is equal to H(X | Z) and this yields</p><formula xml:id="formula_1183">I (X, Y ) = H(X) -H(X | Y ) ≤ H(X) -H(X | Z) = I (X, Z).</formula><p>By symmetry, we must also have I (X, Y ) ≤ I (Y , Z) so that (iv) is true. ■ Statement (iv) is often called the data-processing inequality, and has been used to infer conditional independence within gene networks <ref type="bibr" target="#b143">[125]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2">Models on undirected graphs 13.2.1 Graphical representation of conditional independence</head><p>An undirected graph is a collection of vertexes and edges, in which edges link pairs of vertexes without order. Edges can therefore be identified to subsets of cardinality two of the set of vertexes, V . This yields the definition: Definition 13.10 An undirected graph G is a pair G = (V , E) where V is a finite set of vertexes and elements e ∈ E are subsets e = {s, t} ⊂ V .</p><p>Note that edges in undirected graphs are defined as sets, i.e., unordered pairs, which are delimited with braces in these notes. Later on, we will use parentheses to represent ordered pairs, (s, t) (t, s). We will write s ∼ G t, or simply s ∼ t to indicate that s and t are connected by an edge in G (we also say that s and t are neighbors in G). Definition 13.11 A path in an undirected graph G = (V , E) is a finite sequence (s 0 , . . . , s N ) of vertexes such that s k-1 ∼ s k ∈ E. (A sequence, (s 0 ), of length 1 is also a path by extension.) We say that s and t are connected by a path if either s = t or there exists a path (s 0 , . . . , s N ) such that s 0 = s and s N = t.</p><p>A subset S ⊂ G is connected if any pair of elements in S can be connected by a path.</p><p>A subset T ⊂ G separates two other subsets S and S ′ if all paths between S and S ′ must pass in T . We will write (S S ′ | T ) in such a case.</p><p>One of the goals of this chapter is to relate the notion of conditional independence within a set of variables to separation in a suitably chosen undirected graph with vertexes in one-to-one correspondence with the variables. This will also justifies the similarity of notation used for separation and conditional independence.</p><p>We have the following simple fact: Lemma 13.12 Let G = (V , E) be an undirected graph, and S, S ′ , T ⊂ V . Then</p><formula xml:id="formula_1184">(S S ′ | T ) ⇒ S ∩ S ′ ⊂ T .</formula><p>Indeed, if (S S ′ | T ) and s 0 ∈ S ∩ S ′ , the path (s 0 ) links S and S ′ and therefore must pass in T . Proposition 13.5 translates into similar properties for separation: Proposition 13.13 Let (V , E) be an undirected graph and S, T , U , R be subsets of V . The following properties hold</p><formula xml:id="formula_1185">(i) (S T |U ) ⇔ (T S|U ). (ii) (S T ∪ R|U ) ⇒ (S T |U ). (iii) (S T ∪ R|U ) ⇒ (S T |U ∪ R). (iv) (S T | U ) and (S R | U ∪ T ) ⇔ (S T ∪ R | U ). (v) U ∩ R = ∅, (S R | U ∪ T ) and (S U | T ∪ R) ⇒ (S U ∪ R | T ).</formula><p>Proof (i) is obvious, and for (ii) (and (iii)), if any path between S and T ∪ R must pass by U , the same is obviously true for a path between S and T .</p><p>For the ⇒ part of (iv), if a path links S and T ∪ R, then it either links S and T and must pass through U by the first assumption, or link S and R and therefore pass through U or T by the second assumption. But if the path passes through T , it must also pass through U before by the first assumption. In all cases, the path passes through U . The ⇐ part of (iv) is obvious.</p><p>Finally, consider (v) and take a path between two distinct elements in S and U ∪R. Consider the first time the path hits U or R, and assumes that it hits U (the other case being treated similarly by symmetry). Notice that the path cannot hit both U and R at the same point since U ∩ R = ∅. From the assumptions, the path must hit T ∪ R before passing by U , and the intersection cannot be in R, so it is in T , which is the conclusion we wanted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>To make a connection between separation in graphs and conditional independence between random variables, we consider a graph G = (V , E) and a family of random variables (X (s) , s ∈ V ) indexed by V . Each variable is assumed to take values in a set F s = R X (s) . The collection of values taken by the random variables will be called configurations, and the sets F s , s ∈ V are called the state spaces.</p><p>Letting F denote the collection (F s , s ∈ V ), we will denote the set of such configurations as F (V , F ). Then F is clear from the context, we will just write F (V ). If S ⊂ V and x ∈ F (V , F ), the restriction of x to S is denoted x (S) = (x (s) , s ∈ S). The set formed by those restrictions will be denoted F (S, F ) (or just F (S)).</p><p>Remark 13.14 Some care needs to be given to the definition of the space of configurations, to avoid ambiguities when two sets F s coincide. The configuration x = (x (s) , s ∈ V ) should be understood, in an emphatic way, as the collection x = ((s, x (s) ), s ∈ V ), which makes explicit the fact that x (s) is the value observed at vertex s. Similarly the emphatic notation for x (S) ∈ F (V , F ) is x(S) = ((s, x (s) ), s ∈ S).</p><p>In the following, we will not use the emphatic notation to avoid overly heavy expressions, but its relevance should be clear with the following simple example. Take V = {1, 2, 3} and F 1 = F 2 = F 3 = {0, 1}. Let x (1) = 0, x (2) = 0 and x (3) = 1. Then the sub-configurations x ({1,3}) and x ({2,3}) both corresponds to values (0, 1), but we consider them as distinct. In the same spirit, x (1) = x (2) , but</p><formula xml:id="formula_1186">x ({1}) x ({2}) ♦ If S, T ⊂ V with S ∩ T = ∅, x (S) ∈ F (S, F ), y (T ) ∈ F (T , F</formula><p>), we will denote their concatenation by x (S) ∧ y (T ) , which is the configuration z</p><formula xml:id="formula_1187">= (z s , s ∈ S ∪ T ) ∈ F (T ∪ S, F ) such that z (s) = x (s) if s ∈ S and z (s) = y (s) if s ∈ T .</formula><p>We define a random field over V as a random configuration X : Ω → F (V , F ), that we will denote for short X = (X (s) , s ∈ V ). If S ⊂ V , the restriction X (S) will also be denoted (X (s) , s ∈ S).</p><p>We can now write the definition: Definition 13.15 Let G = (V , E) be an undirected graph and X = (X (s) , s ∈ V ) a random field over V . We say that X is Markov (or has the Markov property) relative to G (or is G-Markov, or is a Markov random field on G) if and only if, for all S, T , U ⊂ V :</p><formula xml:id="formula_1188">(S T | U ) ⇒ (X (S) X (T ) | X (U ) ).</formula><p>(13.16)</p><p>Letting the observation over an empty set S be empty, i.e., X ∅ = ∅, this definition includes the statement that, if S and T are disconnected (i.e., there is no path between them: they are separated by the empty set), then (X (S) X (T ) | ∅): X (S) and X (T ) are independent.</p><p>We will say that a probability distribution π on</p><formula xml:id="formula_1189">F (V ) is G-Markov if its associated canonical random field X = (X (s) , s ∈ V ) defined on Ω = F (V ) by X (s) (x) = x (s) is G- Markov.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.2">Reduction of the Markov property</head><p>We now proceed, in a series of steps, to a simplification of definition 13.15 in order to obtain a minimal number of conditional independence statements. Note that, in its current form, definition 13.15 requires to check (13.16) for any three subsets of V , which provides a huge number of conditions. Fortunately, as we will see, these conditions are not independent, and checking a much smaller number of them will ensure that all of them are true.</p><p>The first step for our reduction is provided by the following lemma.</p><p>Lemma 13.16 Let G = (V , E) be an undirected graph and X = (X s , s ∈ V ) a set of random variables indexed by V . Then X is G-Markov if and only if, for S, T , U ⊂ V ,</p><formula xml:id="formula_1190">S ∩ U = T ∩ U = ∅ and (S T | U ) ⇒ (X (S) X (T ) | X (U ) ).</formula><p>(13.17)</p><p>Proof Assume that (13.17) is true, and take any S, T , U with</p><formula xml:id="formula_1191">(S T | U ). Let A = S ∩ U , B = T ∩ U and C = A ∪ B. Partition S in S = S 1 ∪ A, T in T 1 ∪ B and U in U 1 ∪ C. From (S T | U ), we get (S 1 T 1 | U ). Since S 1 ∩ U = T 1 ∩ U = ∅, this implies (X (S 1 ) X (T 1 ) | X (U )</formula><p>). But this implies ((X (S 1 ) , X (A) ) (X (T 1 ) , X (B) ) | X (U ) ). Indeed, this property requires</p><formula xml:id="formula_1192">P X (x (S 1 ) ∧ x (A) ∧ x (T 1 ) ∧ x (B) ∧ x (U 1 ) ∧ y (C) )P X (x (U 1 ) ∧ y (C) ) = P X (x (S 1 ) ∧ x (A) ∧ x (U 1 ) ∧ y (C) )P X (x (T 1 ) ∧ x (B) ∧ x (U 1 ) ∧ y (C) )</formula><p>If the configurations x (A) , x (B) , y (C) are not consistent (i.e., x (t) y (t) for some t ∈ C), then both sides vanish. So we can assume x (C) = y (C) and remove x (A) and x (B) from the expression, since they are redundant. The resulting identity is true since it exactly states that (X (S 1 ) X (T 1 ) | X (U ) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Define the set of neighbors of s ∈ V (relative to the graph G) as the set of t s such that {s, t} ∈ E and denote this set by V s . For S ⊂ V define also</p><formula xml:id="formula_1193">V S = S c ∩ s∈S V s</formula><p>which is the set of neighbors of all vertexes in S that do not belong to S. (Here S c denotes the complementary set of S, S c = V \ S.) Finally, let W S denote the vertexes that are "remote" from S, W S = (S ∪ V S ) c .</p><p>We have the following important reduction of the condition in definition 13.15.</p><p>Proposition 13.17 X is Markov relative to G if and only if, for any S ⊂ V , (X (S) X (W S ) | X (V S ) ). <ref type="bibr">(13.18)</ref> This says that P(</p><formula xml:id="formula_1194">X (S) = x (S) | X (S c ) = x (S c ) )</formula><p>only depends (when defined) on variables x (t) for t ∈ S ∪ V S .</p><p>Proof First note that (S W S | V S ) is always true, since any path reaching S from S c must pass through V S . This immediately proves the "only if" part of the proposition.</p><p>Consider now the "if" part. Take S, T , U such that (S T | U ). We want to prove that (X S X T | X U ). According to lemma 13.16, we can assume, without loss of generality, that S ∩ U = T ∩ U = ∅.</p><p>Define R as the set of vertexes v in V such that there exists a path between S and v that does not pass in U . Then:</p><p>1. S ⊂ R: the path (s) for s ∈ S does not pass in U since S ∩ U = ∅.</p><p>2. U ∩ R = ∅ by definition.</p><p>3. V R ⊂ U : assume that there exists a point r in V R which is not in U . Then r has a neighbor, say r ′ in R. By definition of R, there exists a path from S to r ′ that does not hit U , and this path can obviously be extended by adding r at the end to obtain a path that still does not hit U . But this implies that r ∈ R, which contradicts the fact that</p><formula xml:id="formula_1195">V R ∩ R = ∅. 4. T ∩ (R ∪ V R ) = ∅: if t ∈ T , then t R from (S T | U ) and t V R from T ∩ U = ∅.</formula><p>We can then write (each decomposition being a partition, implicitly defining the sets A, B and C, see Fig. <ref type="figure" target="#fig_160">13</ref>.</p><formula xml:id="formula_1196">1) R = S ∪ A, U = V R ∪ C, (R ∪ V R ) c = T ∪ C ∪ B, and from (X (R) X (W R ) | X (V R ) ), we get ((X (S) , X (A) ) (X (T ) , X (C) , X (B) ) | X (V R ) )</formula><p>Figure <ref type="figure" target="#fig_160">13</ref>.1: See proof of proposition 13.17 for details which implies ((X (S) , X (A) ) (X (T ) , X (B) ) | X (U ) ) by (CI3), which finally implies (X (S) X (T ) | X (U ) ) by (CI2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>For positive probabilities, it suffices to consider singletons in proposition 13.17.</p><p>Proposition 13.18 If the joint distribution of (X (s) , s ∈ V ) is positive and, for any s ∈ V , (X (s) X (W s ) | X (V s ) ), <ref type="bibr">(13.19)</ref> then X is Markov relative to G. The converse statement is true without the positivity assumption.</p><p>Proof It suffices to prove that, if <ref type="bibr">(13.18</ref>) is true for S and T ⊂ V , with T ∩ S = ∅, it is also true for S ∪ T . The result will then follow by induction.</p><formula xml:id="formula_1197">So, let U = V S∪T and R = W S∪T = V \ (S ∪ T ∪ U ). Then, we have (X (S) X (W S ) | X (V S ) ) ⇒ (X (S) X (R) | (X (U ) , X (T ) ))</formula><p>because R ⊂ W S (if s ∈ V S , then it is either in U or in T and therefore cannot be in R). Similarly, (X (T ) X (R) | (X (U ) , X (S) )), and (CI5) (for which we need P positive) now implies ((X (T ) , X (S) ) X (R) | X (U ) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>To see that the positivity assumption is needed, consider the following example with six variables X (1) , . . . , X (6) , and a graph linking consecutive integers and closing with an edge between 1 and 6. Assume that X (1) = X (2) = X (4) = X (5) , and that X (1) , X (3)  and X (6) are independent. Then <ref type="bibr">(13.19</ref>) is true, since, for k = 1, 2, 4, 5, X (k) is constant given its neighbors, and X (3) (resp. X (6) ) is independent of the rest of the variables. But (X (1) , X (2) ) is not independent of (X (4) , X (5) ) given the neighbors X (3) , X (6) .</p><p>Finally, another statement equivalent to proposition 13.18 is the following:</p><p>Proposition 13. <ref type="bibr" target="#b37">19</ref> If the joint distribution of (X (s) , s ∈ V ) is positive and, for any s, t ∈ V ,</p><formula xml:id="formula_1198">s ≁ G t ⇒ (X (s) X (t) | X (V \{s,t}) ),</formula><p>then X is Markov relative to G. The converse statement is true without the positivity assumption.</p><p>Proof Fix s ∈ V and assume that (X (s) X (R) | X (V \R) ) for any R ⊂ W s with cardinality at most k (the statement is true for k = 1 by assumption). Consider a set R ⊂ W s of cardinality k + 1, that we decompose into R ∪ {t} for some t ∈ R. We have (X (s) X (t) | X (V \ R) , X R ) from the initial hypothesis and (X (s) X (R) | X (V \ R) , X t ) from the induction hypothesis. Using property (CI5), this yields (X (s) X ( R) | X (V \ R) ). This proves the proposition by induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Remark 13.20 It is obvious from the definition of a G-Markov process that, if X is Markov for a graph G = (V , E), it is automatically Markov for any richer graph, i.e., any graph G = (V , Ẽ) with E ⊂ Ẽ. This is because separation in G implies separation in G. Moreover, any X is G-Markov for the complete graph on V , for which s ∼ t for all s t ∈ V . This is because no pair of sets can be separated in a complete graph.</p><p>Any graph with respect to which X is Markov must be richer than the graph G X = (V , E X ) defined by s ≁ G X t if and only (X (s) X (t) | X ({s,t} c ) ). This is true because, for any graph G for which X is Markov, we have</p><formula xml:id="formula_1199">s ≁ G t ⇒ (X (s) X (t) | X ({s,t} c ) ) ⇒ s ≁ G X t.</formula><p>Interestingly, proposition 13.19 states that X is G X -Markov as soon as its joint distribution is positive. This implies that G X is the minimal graph over which X is Markov in this case. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.3">Restricted graph and partial evidence</head><p>Assume that some variables X (T ) = (X (t) , t ∈ T ) (with T ⊂ V ) have been observed, with observed values x (T ) = (x (t) , t ∈ T ). One would like to use this partial evidence to get additional information on the remaining variables, X (S) where S = V \T . From the probabilistic point of view, this means computing the conditional distribution of X (S) given X (T ) = x (T ) .</p><p>One important property of G-Markov models is that the Markov property is essentially conserved when passing to conditional distributions. We introduce for this the following definitions.</p><formula xml:id="formula_1200">Definition 13.21 If G = (V , E) is an undirected graph, a subgraph of G is a graph G ′ = (V ′ , E ′ ) with V ′ ⊂ V and E ′ ⊂ E.</formula><p>If S ⊂ V , the restricted graph, G S , of G to S is defined by G S = (S, E S ) with E S = {e = {s, t} : s, t ∈ S and e ∈ E} .</p><p>(13.20)</p><p>We have the following proposition.</p><p>Proposition 13.22 Let G = (V , E) be an undirected graph and X be G-Markov. Let S ⊂ V and T = S c . Given a partial evidence x (T ) such that P (X (T ) = x (T ) ) &gt; 0, X (S) , conditionally to</p><formula xml:id="formula_1201">X (T ) = x (T ) , is G S -Markov.</formula><p>Proof The proof is straightforward once it is noticed that</p><formula xml:id="formula_1202">(A B | C) G S ⇒ (A B | C ∪ T ) G ■ so that (A B | C) G S ⇒ (X (A) X (B) | X (C) , X (T ) ) P ⇒ (X (A) X (B) | X (C) ) P (•|X (T ) =x (T ) )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.2.4">Marginal distributions</head><p>The effect of taking marginal distributions for a G-Markov model is, unfortunately, not as much a mild operation as computing conditional distributions, in the sense that the conditional independence structure of the marginal distribution may be much more complex than the original one.</p><p>Let G = (V , E) be an undirected graph, and let S be a subset of V . Define the graph G S = (S, E S ) by {s, t} ∈ E S if and only if {s, t} ∈ E or there exist u, u ′ ∈ S c such that {s, u} ∈ E, {t, u ′ } ∈ E and u and u ′ are connected by a path in S c . In other terms E S links all s, t ∈ S that can be connected by a path, all but the extremities of which are included in S c . With this notation, the following proposition holds.</p><p>Proposition 13.23 Let G = (V , E) be an undirected graph, and</p><formula xml:id="formula_1203">S ⊂ V . Assume that X = (X (s) , s ∈ V ) is a family of random variables which is G-Markov. Then X (S) = (x (s) , s ∈ S) is G S -Markov.</formula><p>Proof It suffices to prove that, for A, B, C ⊂ S,</p><formula xml:id="formula_1204">(A B | C) G S ⇒ (A B | C) G .</formula><p>So, assume that A and B are separated by C in G S . If a path connects A and B in G, we can, by definition of E S , remove from this path any portion that passes in S c and obtain a valid path in G S . By assumption, this path must pass in C, and therefore so does the original path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The graph G S can be much more complex than the restricted graph G S introduced in the previous section (note that, by definition, G S is richer than G S ). Take, for example, the graph that corresponds to "hidden Markov models," for which (cf. fig. <ref type="figure" target="#fig_160">13.</ref>2)</p><formula xml:id="formula_1205">V = {1, . . . , N } × {0, 1}</formula><p>and edges {s, t} ∈ E have either s = (k, 0) and t = (l, 0) with |k -l| = 1, or s = (k, 0) and t = (k, 1). Let S = {1, . . . , N } × {1}. Then, G S is totally disconnected (E S = ∅), since no edge in G links two elements of S. In contrast, any pair of elements in S is connected by a path in S c , so that G S is a complete graph.</p><p>Figure <ref type="figure" target="#fig_160">13</ref>.2: In this graph, variables in the lower row are conditionally independent given the first row, while their marginal distribution requires a completely connected graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3">The Hammersley-Clifford theorem</head><p>The Hammersley-Clifford theorem, which will be proved in this section, gives a complete description of positive Markov processes relative to a given graph, G. It states that positive G-Markov models are associated to families of positive local interactions indexed by cliques in the graph. We now introduce each of these concepts.</p><p>13.3.1 Families of local interactions Definition 13.24 Let V be a set of vertexes and (F s , s ∈ V ) a collection of state spaces. A family of local interactions is a collection of non-negative functions Φ = (ϕ C , C ∈ C) indexed over some subset C of P (V ), such that each ϕ C only depends on configurations restricted to C (i.e., it is defined on F (C)), with values in [0, +∞). (Recall that P (V ) is the set of all subsets of V .) Such a family has order p if no C ∈ C has cardinality larger than p. A family of local interactions of order 2 is also called a family of pair interactions.</p><p>Such a family is said to be consistent, if there exists an x ∈ F (V ) such that C∈C ϕ C (x (C) ) 0.</p><p>To a consistent family of local interactions, one associates the probability distribution π Φ on F (V ) defined by</p><formula xml:id="formula_1206">π Φ (x) = 1 Z Φ C∈C ϕ C (x (C) ) (13.21)</formula><p>for all x ∈ F (V ), where Z Φ is a normalizing constant.</p><p>Given C ⊂ P (V ), define the graph G C = (V , E C ) by letting {s, t} ∈ E C if and only if there exists C ∈ C such that {s, t} ∈ C. We then have the following proposition.</p><formula xml:id="formula_1207">Proposition 13.25 Let Φ = (ϕ C , C ⊂ C) be a consistent family of local interactions, asso- ciated to some C ⊂ P (V ). Then the associated distribution π Φ is G C -Markov.</formula><p>Proof Let X be a random field associated with π = π Φ . According to proposition 13.17, we must show that, for any S ⊂ V , one has</p><formula xml:id="formula_1208">(X (S) X (W S ) | X (V S ) )</formula><p>where V S is the set of neighbors of S in G C and</p><formula xml:id="formula_1209">W S = V \ (V S ∪ S). Define the set U S by U S = C∈C,S∩C ∅ C so that V S = U S \ S and W S = V \ U S .</formula><p>To prove conditional independence, we need to prove that, for any x ∈ F:</p><formula xml:id="formula_1210">π(x)π V S (x (V S ) ) = π U S (x (U S ) )π V \S (x (V \S) )<label>(13.22)</label></formula><p>(where we denote π A the marginal distribution of π on F (A).)</p><p>From the definition of π, we have</p><formula xml:id="formula_1211">π(x) = 1 Z C∈C ϕ C (x (C) ) = 1 Z C:C∩S ∅ ϕ C (x (C) ) C:C∩S=∅ ϕ C (x (C) ).</formula><p>The first term in the last product only depends on x (U S ) , and the second one only on</p><formula xml:id="formula_1212">x (V \S) . Introduce the notation                  µ 1 (x (V S ) ) = y (U S ) :y (V S ) =x (V S ) C:C∩S ∅ ϕ C (x (C) ) µ 2 (x (V S ) ) = y (V \S) :y (V S ) =x (V S ) C:C∩S=∅ ϕ C (x (C) ).</formula><p>With this notation, we have:</p><formula xml:id="formula_1213">                     π U S (x (U S ) ) = (µ 2 (x (V S ) )/Z) C:C∩S ∅ ϕ C (x (C) ) π V \S (x (V \S) ) = (µ 1 (x (V S ) )/Z) C:C∩S=∅ ϕ C (x (C) ) π V S (x (V S ) ) = µ 1 (x (V S ) )µ 2 (x (V S ) )/Z</formula><p>from which <ref type="bibr">(13.22</ref>) can be easily obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We now discuss conditional distributions and marginals for processes associated with local interactions. If T ⊂ V , we let π T = π Φ T denote the marginal distribution of π on T .</p><p>We start with a discussion of conditionals. Let π be associated with Φ, and let S ⊂ V and T = V \S. Assume that a configuration y (T ) is given, such that π T (y (T ) ) &gt; 0, and consider the conditional distribution π S|T (x (S) | y (T ) ) = π(x (S) ∧ y (T ) )/π T (y (T ) ).</p><p>(13.23)</p><p>We have the following proposition. </p><formula xml:id="formula_1214">C S = C : C ⊂ S, ∃C ∈ C : C = C ∩ S and ϕ C|y (T ) (x ( C) ) = C∈C:C∩S= C ϕ C (x ( C) ∧ y (C∩T ) ).</formula><p>Proof From (13.23) and the definition of π, it is easy to sees that</p><formula xml:id="formula_1215">π S|T (x (S) | y (T ) ) = 1 Z(y (T ) ) C:C∩S ∅ ϕ C (x (C∩S) ∧ y (C∩T ) ),</formula><p>where Z(y (T ) ) is a constant that only depends on y (T ) . The fact that π S|T (• | y (T ) ) is associated to Φ |y (T ) is then obtained by reorganizing the product over distinct S ∩ C's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This result, combined with proposition 13.25, is consistent with proposition 13.22, in the sense that the restriction to G C to S coincides with the graph G C S . The easy proof is left to the reader.</p><p>We now consider marginals, and more specifically marginals when only one node is removed, which provides the basis for "node elimination." Proposition 13.27 Let π be associated to Φ = (ϕ C , C ∈ C) as above. Let t ∈ V and S = V \ {t}. Define C t ∈ P (V ) as the set</p><formula xml:id="formula_1216">C t = {C ∈ C : t C} ∪ { Ct } with Ct = C∈C:t∈C C \ {t}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define a family of local interactions</head><formula xml:id="formula_1217">Φ t = ( φ C , C ∈ C t ) by φ C = ϕ C if C Ct and: • If Ct C: φ Ct (x ( Ct ) ) = y (t) ∈F t C∈C,t∈C ϕ C (x ( Ct ) ∧ y (t) ). • If Ct ∈ C: φ Ct (x ( Ct ) ) = ϕ C t (x ( Ct ) ) y (t) ∈F t C∈C,t∈C ϕ C (x (C t ) ∧ y (t) )</formula><p>Then the marginal, π S , of π over S is the distribution associated to Φ t .</p><p>The proof is almost straightforward by summing over possible values of y t in the expression of π and left to the reader.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.3.2">Characterization of positive G-Markov processes</head><p>Using families of local interactions is a typical way to build graphical models in applications. The previous section describes a graph with respect to which the obtained process is Markov. Conversely, given a graph G, the Hammersley-Clifford theorems states that families of local interactions over the cliques of G are the only ways to build positive graphical models, which reinforces the importance of this construction. We now pass to the statement and proof of this theorem, starting with the following definition.</p><p>Definition 13.28 Let G = (V , E) be an undirected graph. A clique in G is a nonempty subset C ⊂ V such that s ∼ G t whenever s, t ∈ C, s t. (In particular, subsets of cardinality one are always cliques.) Cliques therefore form complete subgraphs of G.</p><p>The set of cliques of a graph G will be denoted C G .</p><p>A clique that cannot be strictly included in any other clique is called a maximal clique, and their set denoted C * G .</p><p>(Note that some authors call cliques what we refer to as maximal cliques.)</p><p>Given G = (V , E), consider a random field X = (X (s) , s ∈ V ). We assume that X (s) takes values in a finite set F s with P(X (s) = a) &gt; 0 for any a ∈ F s (this is no loss of generality since one can always restrict F s to such a's). If S ⊂ V , we denote as before F (S) the set of restrictions of configurations to S. With this notation, X is positive, according to definition 13.4, if and only if P (X = x) &gt; 0 for all x ∈ F (V ). We will let π = P X be the probability distribution of X, so that π(x) = P(X = x) and use as above</p><formula xml:id="formula_1218">the notation: for S, T ⊂ V        π S (x (S) ) = P(X (S) = x (S) ) π S|T (x (S) | x (T ) ) = P(X (S) = x (s) | X (T ) = x (T ) ).</formula><p>(13.24) (For the first notation, we will simply write π if S = V .)</p><p>We will also need to fix a reference, or "zero," configuration in F (V ) that we will denote 0 = (0 (s) , s ∈ V ), with 0 (s) ∈ F s for all s. We can choose it arbitrarily. Given this, we have the theorem: Theorem 13.29 (Hammersley-Clifford) With the previous notation, X is a positive G-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Markov process if and only if its distribution, π, is associated to a family of local interactions</head><formula xml:id="formula_1219">Φ = (ϕ C , C ⊂ C G ) such that ϕ C (x (C) ) &gt; 0 for all x (C) ∈ F (C).</formula><p>Moreover, Φ is uniquely characterized by the additional constraint: ϕ C (x (C) ) = 1 as soon as there exists s ∈ C such that x (s) = 0 (s) .</p><p>Letting λ C =log ϕ C , we get an equivalent formulation of the theorem in terms of potentials, where a potential is defined as a family of functions</p><formula xml:id="formula_1220">Λ = (λ C , C ∈ C)</formula><p>indexed by a subset C of P (V ), such that λ C only depends on x (C) . The distribution associated to Λ is</p><formula xml:id="formula_1221">π(x) = 1 Z Λ exp - C∈C λ C (x (C) ) . (<label>13.25)</label></formula><p>With this terminology, we trivially have an equivalent formulation:</p><formula xml:id="formula_1222">Theorem 13.30 X is a positive G-Markov process if and only if its distribution, π, is associated to a potential Λ = (λ C , C ⊂ C G ).</formula><p>Moreover, Λ is uniquely characterized by the additional constraint: λ C (x (C) ) = 0 as soon as there exists s ∈ C such that x (s) = 0 (s) .</p><p>We now prove this theorem.</p><p>Proof Let us start with the "if" part. If π is associated to a potential over C G , we have already proved that π is G C G -Markov, so that it suffices to prove that</p><formula xml:id="formula_1223">G C G = G, which is almost obvious: If s ∼ G t, then {s, t} ∈ C G and s ∼ G C G t by definition of G C G . Conversely, if s ∼ G C G t, there exists C ∈ C G such that {s, t} ⊂ C, which implies that s ∼ G t, by definition of a clique.</formula><p>We now prove the "only if" part, which relies on a combinatorial lemma, which is one of Möbius's inversion formulas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 13.31</head><p>Let A be a finite set and f :</p><formula xml:id="formula_1224">P (A) → R, B → f B . Then, there is a unique function λ : P (A) → R such that ∀B ⊂ A, f B = C⊂B λ C , (<label>13.26)</label></formula><p>and λ is given by</p><formula xml:id="formula_1225">λ C = B⊂C (-1) |C|-|B| f B . (13.27)</formula><p>To prove the lemma, first notice that the space F of functions f : P (A) → R is a vector space of dimension 2 |A| and that the transformation ϕ : λ → f with f B = C⊂B λ C is linear. It therefore suffices to prove that, given any f , the function λ given in (13.27) satisfies ϕ(λ) = f , since this proves that ϕ is onto from F to F and therefore necessarily one to one.</p><p>So consider f and λ given by (13.27). Then</p><formula xml:id="formula_1226">ϕ(λ)(B) = C⊂B λ C = C⊂B B⊂C (-1) |C|-| B| f B = B⊂B         C⊃ B,C⊂B (-1) |C|-| B|         f B = f B</formula><p>The last identity comes from the fact that, for any finite set B ⊂ B, B B, we have So the lemma is proved. We now proceed to proving the existence and uniqueness statements in theorem 13.30. Assume that X is G-Markov and positive. Fix x ∈ F (V ) and consider the function, defined on P (V ) by</p><formula xml:id="formula_1227">f B (x (B) ) = -log π(x (B) ∧ 0 (B c ) ) π(0) . Then, letting λ C (x (C) ) = B⊂C (-1) |C|-|B| f B (x (B) ), we have f B (x (B) ) = C⊂B λ C (x (C) ).</formula><p>In particular, for B = V , this gives</p><formula xml:id="formula_1228">π(x) = 1 Z exp - C⊂V λ C (x (C) )</formula><p>with Z = P (0). We now prove that λ C (x (C) ) = 0 if x (s) = 0 (s) for some s ∈ V or if C C G .</p><p>This will prove (13.25) and the existence statement in theorem 13.30.</p><p>So, assume x (s) = 0 (s) . Then, for any B such that s B, we have f B (x (B) ) = f {s}∪B (x ({s}∪B) ).</p><p>Now take C with s ∈ C. We have</p><formula xml:id="formula_1229">λ C (x (C) ) = B⊂C,s∈B (-1) |C|-|B| f B (x (B) ) + B⊂C,s B (-1) |C|-|B| f B (x (B) ) = B⊂C,s B (-1) |C|-|B∪{s}| f B∪{s} (x (B∪{s}) ) + B⊂C,s B (-1) |C|-|B| f B (x (B) ) = B⊂C,s B ((-1) |C|-|B∪{s}| + (-1) |C|-|B| )f B (x (B) ) = 0.</formula><p>Now assume that C is not a clique, and let s t ∈ C such that s ≁ t. We can write, using decompositions similar to the above,</p><formula xml:id="formula_1230">λ C (x (C) ) = B⊂C\{s,t} (-1) |C|-|B| f B∪{s,t} (x (B∪{s,t}) ) -f B∪{s} (x (B∪{s}) ) -f B∪{t} (x (B∪{t}) ) + f B (x (B) ) . But, for B ⊂ C \ {s, t}, we have f B∪{s,t} (x (B∪{s,t}) ) -f B∪{s} (x (B∪{s}) ) = -log π(x (B∪{s,t}) ∧ 0 (B c \{s,t}) ) π(x (B∪{s}) ∧ 0 (B c \{s}) ) = log π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) ) π t (0 (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) )</formula><p>and</p><formula xml:id="formula_1231">f B∪{t} (x (B∪{t}) ) -f B (x (B) ) = -log π(x (B∪{t}) ∧ 0 (B c \{t}) ) π(x (B) ∧ 0 (B c ) ) = log π t (x (t) | x (B) ∧ 0 (B c \{t}) ) π t (0 (t) | x (B) ∧ 0 (B c \{t}) )</formula><p>.</p><p>So, we can write</p><formula xml:id="formula_1232">λ C (x (C) ) = B⊂C\{s,t} (-1) |C|-|B| log π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) )π t (0 (t) | x (B) ∧ 0 (B c \{t}) ) π t (0 (t) | x B∪{s} ∧ 0 (B c \{s,t}) )π t (x (t) | x (B) ∧ 0 (B c \{t}) )</formula><p>which vanishes, because</p><formula xml:id="formula_1233">π t (x (t) | x (B∪{s}) ∧ 0 (B c \{s,t}) ) = π t (x (t) | x (B) ∧ 0 (B c \{t}) )</formula><p>when s ≁ t.</p><p>To prove uniqueness, note that, for any zero-normalized Λ satisfying (13.25), we must have π(0) = 1/Z and therefore, for any x,</p><formula xml:id="formula_1234">-log π(x (B) ∧ 0 (B c ) ) π(0) = C⊂B λ C (x (C) )</formula><p>(extending Λ so that λ C = 0 for C C G ). But, from lemma 13.31, this uniquely defines Λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The exponential form of the distribution in the Hammersley-Clifford theorem is related to what is called a Gibbs distribution in statistical mechanics. More precisely: Definition 13.32 Let F be a finite set and W : F → R be a scalar function. The Gibbs distribution with energy W at temperature T &gt; 0 is defined by</p><formula xml:id="formula_1235">π(x) = 1 Z T e -W (x) T , x ∈ F</formula><p>The normalizing constant Z T = y∈F exp(-W (y)/T ) is called the partition function.</p><formula xml:id="formula_1236">If Λ = (λ C , C ⊂ V</formula><p>) is a potential then its associated energy is</p><formula xml:id="formula_1237">W (x) = C⊂V λ C (x (C) ).</formula><p>So the Hammersley-Clifford theorem implies that any positive G-Markov model is associated to a unique zero-normalized potential defined over the cliques of G. This representation can also be used to provide an alternate proof of proposition 13.19, which is left to the reader. Finally, one can restate proposition 13.26 in terms of potentials, yielding:</p><p>Proposition 13.33 Let P be a Gibbs distribution associated with a zero-normalized potential λ = (λ C , C ⊂ V ). Let S ⊂ V and T = S c . Then the conditional distribution of X (S)  given X (T ) = x (T ) is the Gibbs distribution associated with the zero-normalized potential λ = ( λC , C ⊂ S) where</p><formula xml:id="formula_1238">λC (y (C) ) = C ′ ⊂V ,C ′ ∩S=C λ C ′ (y (C) ∧ x (T ∩C ′ ) ).</formula><p>13.4 Models on acyclic graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.1">Finite Markov chains</head><p>We now review a few important examples of Markov processes X associated to specific graphs G = (V , E). We will always denote by F s the space in which X (s) takes his values, for s ∈ V .</p><p>The simplest example of G-Markov process (for any graph G) is the case when X = (X (s) , s ∈ V ) is a collection of independent random variables. In this case, we can take G X = (V , ∅), the totally disconnected graph on V . Another simple fact is that, as already remarked, any X is Markov for the complete graph (V , P 2 (V )) where P 2 (V ) contains all subsets of V with cardinality 2.</p><p>Beyond these trivial (but nonetheless important) cases, the simplest graph-Markov processes are those associated with linear graphs, providing finite Markov chains. For this, we let V be a finite ordered set, say,</p><formula xml:id="formula_1239">V = {0, . . . , N } .</formula><p>We say that X is a finite Markov chain if, for any k = 1, . . . , N (X (k) (X (0) , . . . , X (k-2) ) | X (k-1) ).</p><p>So we have the identity</p><formula xml:id="formula_1240">P(X (0) = x (0) , . . . , X (k) = x (k) )P (X (k-1) = x (k-1) ) = P(X (0) = x (0) , . . . , X (k-1) = x (k-1) )P(X (k-1) = x (k-1) , X (k) = x (k) ).</formula><p>The distribution of a Markov chain is therefore fully specified by P(X (0) = x (0) ), x 0 ∈ F 0 (the initial distribution) and the conditional probabilities</p><formula xml:id="formula_1241">p k (x (k-1) , x (k) ) = P(X (k) = x (k) | X (k-1) = x (k-1) ) (13.28)</formula><p>(with an arbitrary choice when P(X (k-1) = x (k-1) ) = 0). Indeed, assume that P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) is known (for all x (0) , . . . , x (k-1) ). Then, either: (i) P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) = 0, in which case P(X (0) = x (0) , . . . , X (k) = x (k) ) = 0 for any x (k) , or:</p><p>(ii) P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ) &gt; 0, in which case, necessarily, P(X (k-1) = x (k-1) ) &gt; 0, and</p><formula xml:id="formula_1242">P(X (0) = x (0) , . . . , X (k) = x (k) ) = p k (x (k-1) , x (k) )P(X (0) = x (0) , . . . , X (k-1) = x (k-1) ).</formula><p>Note that p k in (13.28) is a transition probability (according to definition 12.2) between F k-1 and F k .</p><p>We have the following identification of a finite Markov chain with a graph-Markov process:</p><formula xml:id="formula_1243">Proposition 13.34 Let X = (X (0) , . . . , X (N ) ) be a finite Markov chain, such that X is posi- tive. Then X is G-Markov for the linear graph G = (V , E) with V = {1, . . . , N } E = {{1, 2}, . . . , {N -1, N }} .</formula><p>The converse is true without the positivity assumption: a G-Markov process for the graph above is always a finite Markov chain.</p><p>Proof We prove the direct statement (the converse one being obvious). Let s and t be nonconsecutive distinct integers, with, say, s &lt; t. From the Markov chain assumption, we have (X (t) (X (s) , X ({1,t-2}\{s,}) ) | X (t-1) ), which, using (CI3), yields (X (t) X (s) | X ({1,...,t-1}\{s}) ). Define Y (u) = X ({1,...,u}\{s,t}) : what we have proved is (X (t) X (s) | Y (t) ).</p><p>We now proceed by induction and assume that (X (t) X (s) | Y (u) ) for some u ≥ t. Then, we have (X (u+1) (X (s) , X (t) , Y (u-1) ) | X (u) ), which implies (from (CI3)) (X (u+1) X (t) | X (s) , Y (u) ). Applying (CI4) to (X (t) X (s) | Y (u) ) and (X (t) X (u+1) | X (s) , Y (u) ), we obtain (X (t) (X (s) , X (u+1) ) | Y (u) ) and finally, (X (t) X (s) | Y (u+1) ). By induction, this gives (X (t) X (s) | Y (N ) ) and therefore proposition 13.19 now implies that X is G-Markov.</p><p>(The proposition can also be proved as a consequence of the decomposition P(X (0) = x (0) , . . . , X (N ) = x (N ) ) = P(X (0) = x (0) )p 1 (x (0) , x (1) ) . . . p N (x (N -1) , x (N ) ).) ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.4.2">Undirected acyclic graph models and trees</head><p>The situation with acyclic graphs is only slightly more complex than with linear graphs, but will require a few new definitions, including those of directed graphs and trees.</p><p>The difference between directed and undirected graphs is that the edges of the former are ordered pairs, namely:</p><formula xml:id="formula_1244">Definition 13.35 A (finite) directed graph G is a pair G = (V , E) where V is a finite set of vertexes and E is a subset of V × V \ {(s, s), s ∈ V } , which satisfies, in addition, (s, t) ∈ E ⇒ (t, s) E.</formula><p>So, for directed graphs, edges (s, t) and (t, s) have different meanings, and we allow at most one of them in E. We say that the edge e = (s, t) stems from s and points to t.</p><p>The parents of a vertex s are the vertexes t such that (t, s) ∈ E, and its children are the vertexes t such that (s, t) ∈ E. We will also use the notation s → G t to indicate that (s, t) ∈ E (compare to s ∼ G t for undirected graphs).</p><p>Definition 13.36 A path in a directed graph G = (V , E) is a sequence (s 0 , . . . , s N ) such that, for all k = 1, . . . , N , s k → G s k+1 (this includes the "trivial", one-vertex, paths (s 0 )).</p><p>(The definition was the same for undirected graph, replacing s k → G s k+1 by s k ∼ G s k+1 .) For both directed and undirected cases, one says that a path is closed if s 0 = s N .</p><p>In an undirected graph, a path is folded if it can be written as (s 0 , . . . , s N -1 , s N , s N -1 , . . . , s 0 ).</p><formula xml:id="formula_1245">If G = (V , E</formula><p>) is directed, one says that t ∈ V is a descendant of s ∈ V (or that s is an ancestor of t) if there exists a path starting at s and ending at t. In particular, every vertex is both a descendant and an ancestor of itself.</p><p>We finally define acyclic graphs. Definition 13.37 A loop in a directed (resp. an undirected) graph G is a path (s 0 , s 1 , . . . , s N ), with N ≥ 3, such that s N = s 0 , which passes only once through s 0 , . . . , s N -1 (no selfintersection except at the end).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A (directed or undirected) graph G is acyclic if it contains no loop.</head><p>The following property will be useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 13.38 In a directed graph, any non-trivial closed path contains a loop (i.e., one can delete vertexes from it to finally obtain a loop.)</head><p>In an undirected graph, any non-trivial closed path which is not a union of folded paths contains a loop.</p><p>Proof Take γ = (s 0 , s 1 , . . . , s N ) with s N = s 0 . The path being non-trivial means N &gt; 1.</p><p>First take the case of a directed graph. Clearly, N ≥ 3 since a two-vertex path cannot be closed in an directed graph. Consider the first occurrence of a repetition, i.e., the first index for which s j ∈ {s 0 , . . . , s j-1 }.</p><p>Then there is a unique j ′ ∈ {0, . . . , j -1} such that s j ′ = s j , and the path (s j ′ , . . . , s j-1 ) must be a loop (any repetition in the sequence would contradict the fact that j was the first occurrence. This proves the result in the directed case.</p><p>Consider now an undirected graph. We can recursively remove all folded subpaths, by keeping everything but their initial point, since each such operation still provide a path at the end. Assume that this is done, still denoting the remaining path (s 0 , s 1 , . . . , s N ), which therefore has no folded subpath. We must have N ≥ 3 since N = 1 implies that the original path was a union of folded paths, and N = 2 provides a folded path. Let, 0 ≤ j ′ &lt; j be as in the directed case. Note that one must have j ′ &lt; j -2, since j ′ = j -1 would imply an edge between j and itself and j ′ = j -2 induces a folded subpath. But this implies that (s j ′ , . . . , s j-1 ) is a loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Directed acyclic graphs (DAG) will be important for us, because they are associated with Bayesian networks that we will discuss later. For now, we are interested with undirected acyclic graphs and their relation to trees, which form a subclass of directed acyclic graphs, defined as follows.</p><p>Definition 13.39 A forest is a directed acyclic graph with the additional requirement that each of its vertexes has at most one parent.</p><p>A root in a forest is a vertex that has no parent. A forest with a single root is called a tree.</p><p>It is clear that a forest has at least one root, since one could otherwise describe a nontrivial loop by starting from a any vertex and passing to its parent until the sequence self-intersects (which must happen since V is finite). We will use the following definition.</p><formula xml:id="formula_1246">Definition 13.40 If G = (V , E) is a directed graph, its flattened graph, denoted G ♭ = (V , E ♭</formula><p>) is the undirected graph obtained by forgetting the edge ordering, namely</p><formula xml:id="formula_1247">{s, t} ∈ E ♭ ⇔ (s, t) ∈ E or (t, s) ∈ E.</formula><p>The following proposition relates forests and undirected acyclic graphs.</p><formula xml:id="formula_1248">Proposition 13.41 If G is a forest, then G ♭ is an undirected acyclic graph.</formula><p>Conversely, if G is an undirected acyclic graph, there exists a forest G such that G♭ = G.</p><p>Proof Let G = (V , E) be a forest and, in order to reach a contradiction, assume that G ♭ has a loop, s 0 , . . . , s N -1 , s N = s 0 . Assume that (s 0 , s 1 ) ∈ E; then, also (s 1 , s 2 ) ∈ E (otherwise s 1 would have two parents), and this propagates to all (s k , s k+1 ) for k = 0, . . . , N -1. But, since s N = s 0 , this provides a loop in G which is not possible. This proves thet G ♭ has no loop since the case (s 1 , s 0 ) ∈ E is treated similarly. Now, let G be an undirected acyclic graph. Fix a vertex s ∈ V and consider the following procedure, in which we recursively define sets S k of processed vertexes, and Ẽk of oriented edges, k ≥ 0, initialized with S 0 = {s} and Ẽ0 = ∅.</p><p>-At step k of the procedure, assume that vertexes in S k have been processed and edges in Ẽk have been oriented so that (S k , Ẽk ) is a forest, and that Ẽ♭ k is the set of edges {s, t} ∈ E such that s, t ∈ S k (so, oriented edges at step k can only involve processed vertexes).</p><p>-If S k = V : stop, the proposition is proved.</p><p>-Otherwise, apply the following construction. Let F k be the set of edges in E that contain exactly one element of S k .</p><p>(1) If F k = ∅, take any s ∈ V \ S k as a new root and let S k+1 = S k ∪ {s}, Ẽk+1 = Ẽk .</p><p>(2) Otherwise, add to Ẽk the oriented edges (s, t) such that s ∈ S k and {s, t} ∈ F k , yielding Ẽk+1 , and add to S k the corresponding children (t's) yielding S k+1 .</p><p>We need to justify the fact that Gk+1 = (S k+1 , Ẽk+1 ) above is still a forest. This is obvious after Case (1), so consider Case (2). First Gk+1 is acyclic, since any oriented loop is a fortiori an unoriented loop and G is acyclic. So we need to prove that no vertex in S k+1 has two parents. Since we did not add any parent to the vertexes in S k and, by assumption, (S k , Ẽk ) is a forest, the only possibility for a vertex to have two parents in S k+1 is the existence of t such that there exists s, s ′ ∈ S k with {s, t} and {s ′ , t} in E. But, since s and s ′ have unaccounted edges containing them, they cannot have been introduced in S k before the previously introduced root has been added, so they are both connected to this root: but the two connections to t would create a loop in G which is impossible.</p><p>So the procedure carries on, and must end with S k = V at some point since we keep adding points to S k at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Note that the previous proof shows there is more than one possible orientation of a connected undirected tree into a tree is not unique, although uniquely specified once a root is chosen. The proof is constructive, and provides an algorithm building a forest from an undirected acyclic graph.</p><p>We now define graphical models supported by trees, which constitute our first Markov models associated with directed graphs. Define the depth of a vertex in a tree G = (V , E) to be the number of edges in the unique path that links it to the root. We will denote by G d the set of vertexes in G that are at depth d, so that G 0 contains only the root, G 1 the children of the root and so on. Using this, we have the definition:</p><formula xml:id="formula_1249">Definition 13.42 Let G = (V , E) be a tree. A process X = (X (s) , s ∈ V ) is G-Markov if</formula><p>and only, for each d ≥ 1, and for each s ∈ G d , we have</p><formula xml:id="formula_1250">(X (s) (X (G d \{s}) , X (G q \{pa(s)}) , q &lt; d) | X (pa(s)) ) (13.29)</formula><p>where pa(s) is the parent of s.</p><p>So, conditional to its parent, X (s) is independent from all other variables at depth smaller or equal to the depth of s.</p><p>Note that, from (CI3), definition 13.42 implies that, for all</p><formula xml:id="formula_1251">s ∈ G d , (X (s) X (G d \{s}) | X (G q ) , q &lt; d),</formula><p>which, using proposition 13.6, implies that the variables (X (s) , s ∈ G d ) are mutually independent given X (G q ) , q &lt; d. This implies that, for d = 1 (letting s 0 denote the root in G):</p><p>P(X (G 1 ) = x (G 1 ) , X (s 0 ) = x (s 0 ) ) = P(X (s 0 ) = x (s 0 ) )</p><formula xml:id="formula_1252">s∈G 1 P(X (s) = x (s) | X (s 0 ) = x (s 0 ) ).</formula><p>(If P(X (s 0 ) = x (s 0 ) ) = 0, the choice for the conditional probabilities can be made arbitrarily without changing the left-hand side which vanishes.) More generally, we have, letting</p><formula xml:id="formula_1253">G &lt;d = G 0 ∪ • • • ∪ G d-1 , P(X (G ≤d ) = x (G ≤d ) ) = s∈G d P(X (s) = x (s) | X (pa(s)) = x (pa(s)) )P(X (G &lt;d ) = x (G &lt;d ) )</formula><p>(with again an arbitrary choice for conditional probabilities that are not defined) so that, we obtain, by induction, for x ∈ F (V ) P(X = x) = P(X (s 0 ) = x (s 0 ) ) s s 0 p s (x (pa(s)) , x (s) ) <ref type="bibr">(13.30)</ref> where p s (x (pa(s)) , x (s) )</p><formula xml:id="formula_1254">∆ = P(X (s) = x (s) | X (pa(s)) = x (pa(s))</formula><p>) are the tree transition probabilities between a parent and a child. So we have the following proposition.</p><p>Proposition 13.43 A process X is Markov relative to a tree G = (V , E) if and only if there exists a probability distribution p 0 on F s 0 and a family (p st , (s, t) ∈ E) such that p st is a transition probability from F s to F t and</p><formula xml:id="formula_1255">P X (x) = p 0 (x s 0 ) (s,t)∈E p st (x (s) , x (t) ), x ∈ F (V ). (<label>13.31)</label></formula><p>We only have proved the "only if" part, but the "if" part is obvious from <ref type="bibr">(13.31)</ref>.</p><p>Another property that becomes obvious with this expression is the first part of the following proposition.</p><formula xml:id="formula_1256">Proposition 13.44 If a process X is Markov relative to a tree G = (V , E) then it is G ♭ Markov. Conversely, if G = (V , E) is an undirected acyclic graph and X is G-Markov, then X is Markov relative to any tree G such that G♭ = G.</formula><p>Proof To prove the converse part, assume that G = (V , E) is undirected acyclic and that X is G-Markov. Take G such that G♭ = G. For s ∈ V and its parent pa(s) in G, the sets {s} and G≤d \ {s, pa(s)} are separated by pa(s) in G. To see this, assume that there exists a t ∈ G≤d \ {s, pa(s)} with a path from t to s that does not pass through pa(s).</p><p>Then we can complete this path with the path from t to the first common ancestor (in G) of t and s and back to s to create a path from s to s that passes only once through {pa(s), s} and therefore contains a loop by proposition 13.38.</p><p>The G-Markov property now implies (X (s) (X ( Gd \{s}) , X ( Gq \{pa(s)}) , q &lt; d) | X (pa(s)) )</p><p>which proves that X is G-Markov.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Remark 13. <ref type="bibr" target="#b63">45</ref> We see that there is no real gain in generality with passing from undirected to directed graphs when working with trees. This is an important remark, because directionality in graphs is often interpreted as causality. For example, there is a natural causal order in the statements (it rains) → (car windshields get wet) → (car wipers are on) in the sense that each event can be seen as a logical precursor to the next one. However, because one can pass from this directed chain to an equivalent undirected chain and then back to a equivalent directed tree by choosing any of the three variables as roots, there is no way to infer, from the observation of the joint distribution of the three events (it rains, car windshields get wet, wipers are on), any causal relationship between them: the joint distribution cannot resolve whether wipers are on because it rains, or whether turning wipers on automatically wets windshields which in turn triggers a shower ! To infer causal relationships, one needs a different kind of observation, that would modify the distribution of the system. Such an operation (called an intervention), can be done, for example, by preventing the windshields from being wet (doing, for example, the observation in a parking garage), or forcing them to be wet (using a hose). Then, one can compare observations made with these new conditions, and those made with the original system, and check, for example, whether they modified the probability that rain occurs outside. The answer (likely to be negative !) would refute any causal relationship from "windshields are wet" to "it rains." On the other hand, the intervention might modify how wipers are used, which would indicate a possible causal relationship from "windshields are wet" to "wipers are on." ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5">Examples of general "loopy" Markov random fields</head><p>We will see that acyclic models have very nice computational properties that make them attractive in designing distributions. However, the absence of loops is a very restrictive constraint, which is not realistic in many practical situations. Feedback effects are often needed, for example. Most models in statistical physics are supported by a lattice, in which natural translation/rotation invariance relations forbid using any non-trivial acyclic model. As an example, we now consider the 2D Ising model on a finite grid, which is a model for (anti)-ferromagnetic interaction in a spin system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let G = (V , E). A (positive) G-Markov model is said to have only pair interactions if and only if can be written in the form</head><formula xml:id="formula_1257">π(x) = 1 Z exp - s∈G h s (x (s) ) - {s,t}∈E</formula><p>h {s,t} (x (s,t) ) .</p><p>Relating to theorem 13.30, this says that π is associated to a potential involving cliques of order 2 at most (note that this does not mean that the cliques of the associated graph have order 2 at most; there can be higher-order cliques, which would then have a zero potential). The functions in the potential are indexed by sets, as they should be from the general definition. However, models with pair interactions are often written in the form</p><formula xml:id="formula_1258">π(x) = 1 Z exp - s∈G h s (x (s) ) - {s,t}∈E hst (x (s) , x (t) )</formula><p>with hst (λ, µ) = hts (µ, λ) (which is equivalent, taking h = h/2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.5.">EXAMPLES OF GENERAL "LOOPY" MARKOV RANDOM FIELDS</head><p>The Ising model is a special case of models with pair interactions, for which the state space, F s , is equal to {-1, 1} for all s and h s (x (s) ) = α s x (s) , h {s,t} (x (s) , x (t) ) = β st x (s) x (t) .</p><p>In fact, for binary variables, this is the most general pair interaction model. The Ising model is moreover usually defined on a regular lattice, which, in two dimensions, implies that V is a finite rectangle in Z 2 , for example</p><formula xml:id="formula_1259">V = {-N , . . . , N } 2 .</formula><p>The simplest choice of a translation-and 90-degree rotation-invariant graph is the nearest-neighbor graph for which {(i, j), <ref type="figure" target="#fig_160">13.3</ref>). With this graph, one can furthermore simplify the model to obtain the isotropic Ising model given by</p><formula xml:id="formula_1260">(i ′ , j ′ )} ∈ E if and only if |i -i ′ | + |j -j ′ | = 1 (see fig.</formula><formula xml:id="formula_1261">π(x) = 1 Z exp -α s∈V x (s) -β s∼t x (s) x (t) .</formula><p>When β &lt; 0, the model is ferromagnetic: each pair of neighbors with identical signs brings a negative contribution to the energy, making the configuration more likely (since lower energy implies higher probability).</p><p>The Potts model generalizes the Ising model to finite, but non-necessarily binary, state spaces, say, F s = F = {1, . . . , n}. Define the function δ(λ, µ) = 1 if λ = µ and (-1) otherwise. Then the Potts model is given by</p><formula xml:id="formula_1262">π(x) = 1 Z exp -α s∈V h(x (s) ) -β s∼t δ(x (s) , x (t) ) (13.32)</formula><p>for some function h defined on F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.6">General state spaces</head><p>Our discussion of Markov random fields on graphs was done under the assumption of finite state spaces, which notably simplifies many of the arguments and avoids relying too much on measure theory. While this situation does cover a large range of application, there are cases in which one wants to consider variables taking values in continuous spaces, or in countable (infinite) spaces.</p><p>The results obtained for discrete variables can most of the time be extended to variables whose distribution has a p.d.f. with respect to a product of measures on the sets in which they take their values. For example, let X, Y , Z takes values in R X , R Y , R Z , equipped with σ -algebras S X , S Y , S Z and measures µ X , µ Y , µ Z . Assume that P X,Y ,Z is absolutely continuous with respect to µ X ⊗ µ Y ⊗ µ Z , with density ϕ XY Z . In such a situation, (13.3) remains valid, in that X is conditionally independent of Y given Z if and only if ϕ XY Z (x, y, z)ϕ Z (z) = ϕ XZ (x, z)ϕ Y Z (y, z) <ref type="bibr">(13.33)</ref> almost everywhere (relative to µ X ⊗ µ Y ⊗ µ Z ). Here, ϕ XZ , ϕ Y Z , ϕ Z are marginal densities of the indexed random variables. The only difficulty in the argument, provided below for the interested reader, is dealing properly with sets of measure zero.</p><p>Proof (Proof of (13.33)) Introduce the conditional densities</p><formula xml:id="formula_1263">ϕ XY |Z (x, y | z) = ϕ XY Z (x, y, z) ϕ Z (z)</formula><p>and similarly ϕ X|Z and ϕ Y |Z , which are defined when z M Z = {z ∈ R Z : ϕ Z (z) = 0}. By definition of conditional independence, we have, for all</p><formula xml:id="formula_1264">A ∈ S X , B ∈ S X A×B ϕ XY |Z (x, y | z)µ X (dx)µ Y (dy) = A×B ϕ X|Z (x | z)ϕ Y |Z (y | z)µ X (dx)µ Y (dy)</formula><p>for all z M Z , which implies that, for all z M Z , there exists a set</p><formula xml:id="formula_1265">N z ⊂ R X × R Y such that µ X × µ Y (N z ) = 0 and ϕ XY |Z (x, y | z) = ϕ X|Z (x | z)ϕ Y |Z (y | z)</formula><p>for all z M Z and (x, y) N z . This immediately implies (13.33) for those (x, y, z).</p><formula xml:id="formula_1266">If z ∈ M Z , then 0 = ϕ Z (z) = R X ϕ XZ (x, z)µ X (dx) = R Y ϕ Y Z (x, z)µ Y (dy)</formula><p>implying that ϕ XZ (x, z) = ϕ Y Z (y, z) = 0 excepted on some set N z such that µ X ⊗ µ Y (N z ) = 0, and (13.33) is therefore also true outside of this set. Now, letting N = {(x, y, z) : (x, y) ∈ N z }, we find that (13.33) is true for all (x, y, z) N and</p><formula xml:id="formula_1267">µ X ⊗µ Y ⊗µ Z (N ) = R X ×R Y ×R Z 1 (x,y)∈N z µ X (dx)µ Y (dy)µ Z (dz) = R Z µ X ⊗µ Y (N z )µ Z (dz) = 0.</formula><p>(This argument involves Fubini's theorem <ref type="bibr" target="#b189">[171]</ref>.)</p><p>■ With this definition, the proof of proposition 13.5 can be caried on without change, with the positivity condition expressing the fact that there exists RX ⊂ R X , RY ⊂ R Y and RZ ⊂ R X such that ϕ XY Z (x, y, z) &gt; 0 for all x, y, z ∈ RX × RY × RZ . (This proposition is actually valid in full generality, with a proper definition of positivity.)</p><p>When considering random fields with general state spaces, we will restrict to the similar situation in which each state space F s is equipped with a σ -algebra S s and a measure µ s , and the joint distribution, P X of the random field X = (X s , s ∈ V ) is absolutely continuous with respect to µ ∆ = s∈V µ s , denoting by π the corresponding p.d.f. We will says that π is positive if there exists F = ( Fs , s ∈ V ) with measurable Fs ⊂ F s such that π(x) &gt; 0 for all x ∈ F (V , F). Without loss of generality unless one considers multiple random fields with different supports, we will assume that Fs = F s for all s.</p><p>The definition of consistent families of local interactions (definition 13.24) must be modified by adding the condition that</p><formula xml:id="formula_1268">F (V ) C∈C ϕ C (x (C) )µ(dx) &lt; ∞. (13.34)</formula><p>This requirement is obviously needed to ensure that the normalizing constant in (13.21) is finite. Proposition 13.25 is then true (with sums replaced by integrals in the proof) and so are propositions 13.26 and 13.27. Finally, the Hammersley-Clifford theorem (theorem 13.29) extends to this context.</p><p>Even though it is a natural requirement, condition (13.34) may be hard to assess with general families of local interactions. In the case of Gaussian distributions, however, one can provide relatively simple conditions. Assume that F s = R for all s ∈ V , and condider a potential Λ = (λ C , C ∈ C) with only univariate and bivariate interactions, such that, for some vector a ∈ R d (with d = |V |) and symmetric matrix</p><formula xml:id="formula_1269">b ∈ S d ,          λ {s} (x ({s}) ) = -a (s) x (s) + 1 2 b ss (x (s) ) 2 λ {s,t} (x ({s,t}) ) = b st x (s) x (t)</formula><p>Then, considering x ∈ F (V ) as a d-dimensional vector, we have</p><formula xml:id="formula_1270">π(x) = 1 Z exp a T x - 1 2 x T bx ,</formula><p>with the integrability requirement that b ≻ 0 (positive definite). The random field then follows a Gaussian distribution with mean m = b -1 a and covariance matrix Σ = b -1 . The normalizing constant, Z, is given by</p><formula xml:id="formula_1271">Z = e -1 2 a T ba (2π) d/2 √ det b .</formula><p>This Markov random field parametrization of Gaussian distributions emphasizes the conditional structure of the variables rather than their covariances. It is useful when the associated graph, represented by the matrix b is sparse. In particular, the conditional distribution of X (s) given the other variables is Gaussian, with mean (a (s)t s b st x (t) )/b ss and variance 1/b ss .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 14</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probabilistic Inference for Random Fields</head><p>Once the joint distribution of a family of variables has been modeled as a random field, this model can be used to estimate the probabilities of specific events, or the expectations of random variables of interest. For example, if the modeled variables relate to a medical condition, in which variables such as diagnosis, age, gender, clinical evidence can interact, one may want to compute, say, the probability of someone having a disease given other observable factors. Note that, being able to compute expectations of the modeled variables for G-Markov processes also ensures that one can compute conditional expectations of some modeled variables given others, since, by proposition 13.22, conditional G-Markov distributions are Markov over restricted graphs.</p><p>We assume that X is G-Markov for a graph G = (V , E) and restrict (unless specified otherwise) to finite state spaces. We condider the basic problem to compute P(X (S) = x (S) ) when S ⊂ V , starting with one-vertex marginals, P(X (s) = x (s) ).</p><p>The Hammersley-Clifford theorem provides a generic form for general positive G-Markov processes, in the form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><formula xml:id="formula_1272">= x) = π(x) = 1 Z exp - C∈C G h C (x (C) ) . (14.1)</formula><p>So, formally, marginal distributions are given by the ratio</p><formula xml:id="formula_1273">P(X (S) = x (S) ) = y∈F (V ),y (S) =x (S) exp -C∈C G h C (y (C) ) y∈F (V ) exp -C∈C G h C (y (C) )</formula><p>.</p><p>The problem is that the sums involved in this ratio involve a number of terms that grows exponentially with the size of V . Unless V is very small, a direct computation of these sums is intractable. An exception to this is the case of acyclic graphs, as we will see in section 14.2. But for general, loopy, graphs, the sums can only be approximated, using, for example, Monte-Carlo sampling, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.1">Monte Carlo sampling</head><p>Markov chain Monte Carlo methods are well adapted to sampling from Markov random fields, because conditional distributions used in Gibbs sampling, or, more generally, ratios of probabilities used in the Metropolis-Hastings algorithm do not in require the computation of the normalizing constant Z in (14.1). The simplest use of Gibbs sampling generalizes the Ising model example of section 12.4.2. Using the notation of Algorithm 12.2, one lets B ′ s = F (s c ) (with the notation</p><formula xml:id="formula_1274">s c = V \ {s}) and U s (x) = x (s c ) . The conditional distribution given U s is Q s (U s (x), y) = P(X (s) = y (s) | X (s c ) = x (s c ) )1 y (s c ) =x (s c ) .</formula><p>The conditional probability in the r.h.s. of this equation takes the form</p><formula xml:id="formula_1275">π s (y (s) | x (s c ) ) ∆ = P(X (s) = y (s) | X (s c ) = x (s c ) ) = 1 Z s (x (s c ) ) exp         - C∈C,s∈∈C h C (y (s) ∧ x (C∩s c ) )         with Z s (x (s c ) ) = z (s) ∈F s exp         - C∈C,s∈∈C h C (z (s) ∧ x (C∩s c ) )         .</formula><p>The Gibbs sampling algorithm samples from Q s by visiting all s ∈ V infinitely often, as described in Algorithm 12.2. Metropolis-Hastings schemes are implemented similarly, the most common choice using a local update scheme in Algorithm 12.3 such that g(x, •) only changes one coordinate, chosen at random, so that</p><formula xml:id="formula_1276">g(x, y) = 1 |V | s∈V 1 y (s c ) =x (s c ) g s (y (s) )</formula><p>where g s is some probability distribution on F s . The acceptance probability a(x, y) is equal to 1 when y = x. If y x and g(x, y) &gt; 0, there is a unique s for which y (s c ) = x (s c ) and a(x, y) = min 1, π(y)g(y, x) π(x)g(x, y) with π(y)g(y, x) π(x)g(x, y) = π s (y (s) | x (s c ) )g s (x (s) ) π s (x (s) | x (s c ) )g s (y (s) ) .</p><p>Note that the latter equation avoids the computation of the local normalizing constant Z s (x (s c ) ), which simplifies in the ratio.</p><p>Both algorithms have a transition probability P that satisfies P m (x, y) &gt; 0 for all x, y ∈ F (V ), with m = |V | (for Metropolis-Hastings, one must assume that g s (y (s) ) &gt; 0 for all y (s) ∈ F s . This ensures that the chain is uniformly geometrically ergodic, i.e., (12.10) is satisfied with a constant M and some ρ &lt; 1. However, in many practical cases (especially for strongly structured distributions and large sets V ), the convergence rate, ρ can be very close to 1, resulting in a slow convergence.</p><p>Acceleration strategies have been designed to address this issue, which is often due to the existence of multiple configurations that are local modes of the probability π. Such configurations are isolated from other high-probability configurations because local updating schemes need to make multiple low-probability changes to access them from the local mode. The following two approaches provide examples designed to address this issue.</p><p>a. Cluster sampling. To facilitate escaping from such local modes, it is sometimes possible to augment the state space by introducing a new configuration space, with variable denoted ξ, and designing a joint distributions π(ξ, x) such that the marginal distribution on F (V ) (summing over ξ) is the targeted π. The additional variable can create high-probability bridges between local modes for π, and accelerate convergence.</p><p>To take an example, assume that all sets F s are identical (letting F = F s , s ∈ V ) and that the auxiliary variable ξ takes values in the set of functions from E to {0, 1}, that we will denote B(E), i.e., that it takes the form (ξ (st) , {s, t} ∈ E), with ξ (st) ∈ {0, 1}. For x ∈ F (V ), introduce the set B x containing all ξ ∈ B(E), such that for all {s, t} ∈ E,</p><formula xml:id="formula_1277">x (s) x (t) ⇒ ξ (st) = 1.</formula><p>Assume that the conditional distribution of ξ given x is supported by B x , such that, for ξ ∈ B x</p><formula xml:id="formula_1278">P(ξ = ξ | X = x) = π(ξ | x) = 1 ζ(x) exp         - {s,t}∈E µ st ξ (st)         .</formula><p>The coefficients µ st are free to choose (and one possible choice is to take µ st = 0 for all {s, t} ∈ E). For this distribution, all ξ (st) are independent conditionally to X = x, with ξ (st) = 1 with probability 1 if x (s) x (t) , and</p><formula xml:id="formula_1279">P (ξ (st) = 1 | X = x) = e -µ st 1 + e -µ st (14.2) if x (s) = x (t)</formula><p>. This conditional distribution is, as a consequence, very easy to sample from. Moreover, the normalizing constant ζ(x) has closed form and is given by</p><formula xml:id="formula_1280">ζ(x) = {s,t}∈E (1 x (s) =x (t) + e -µ st ) = exp         {s,t}∈E log (1 + e -µ st ) + {s,t}∈E log(1 + e µ st )1 x (s) x (t)        </formula><p>. Now consider the conditional probability that X = x given ξ = ξ. For this distribution, one has, with probability 1, X (s) = X (t) when ξ (st) = 0. This implies that X is constant on the connected components of the subgraph (V , E ξ ) of (V , E), where {s, t} ∈ E ξ if and only if ξ (st) = 0. Let V 1 , . . . , V m denote these connected components (these components and their number depend on ξ). The conditional distribution of X given ξ is therefore supported by the configurations such that there exists c 1 , . . . , c m ∈ F such that x (s) = c j if and only if s ∈ V j , that we will denote, with some abuse of notation:</p><formula xml:id="formula_1281">c (V 1 ) 1 ∧ • • • ∧ c (V m )</formula><p>m . Given this remark, the conditional distribution of X given ξ = ξ is equivalent to a distribution on F m , which may be feasible to sample from directly if |F| and m are not too large. To sample from π, one now needs to alternate between sampling ξ given X and the converse, yielding the following first version of cluster-based sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 14.1 (Cluster-based sampling: Version 1)</head><p>This algorithm samples from (14.1).</p><p>(1) Initialize the algorithm some configuration x ∈ F (V ).</p><p>(2) Loop over the following steps: a. Generate a configuration ξ ∈ B x such that ξ (st) = 1 with probability given by (14.2) when x (s) = x (t) . b. Determine the connected components, V 1 , . . . , V m , of the graph G ξ = (V , E ξ ) with edges given by pairs {s, t} such that ξ (st) = 1. c. Sample values c 1 , . . . , c m ∈ F according to the distribution</p><formula xml:id="formula_1282">q(c 1 , . . . , c m ) ∝ π(c (V 1 ) 1 ∧ • • • ∧ c (V m ) m ) ζ(c (V 1 ) 1 ∧ • • • ∧ c (V m ) m ) . d. Set x = c (V 1 ) 1 ∧ • • • ∧ c (V m ) m .</formula><p>Step (2.c) takes a simple form in the special case when π is a non-homogeneous Potts model ((13.32)) with positive interactions, that we will write as</p><formula xml:id="formula_1283">π(x) = exp         - s∈V α s x (s) - {s,t}∈E β st 1 x (s) x (t)         with β st ≥ 0. Then π(x) ζ(x) ∝ exp         - s∈V α s x (s) - {s,t}∈E (β st -β ′ st )1 x s s t        </formula><p>with β ′ st = log(1 + e µ st ). If one chooses µ st such that β ′ st = β st (which is possible since β st ≥ 0), then the interaction term disappears and the probability q in (2.c) is proportional to</p><formula xml:id="formula_1284">m j=1 exp          - s∈V j α s         </formula><p>so that c 1 , . . . , c m can be generated independently. The resulting algorithm is the Swendsen-Wang sampling algorithm for the Potts model <ref type="bibr" target="#b204">[186]</ref>. The presentation given here adapts the one introduced in Barbu and Zhu <ref type="bibr" target="#b34">[16]</ref>.</p><p>For more general models, step (2.c) can be computationally costly, especially if the number of connected components is large. In this case, this step can be replaced by a Gibbs sampling step for one of the c ′ j s conditional to the others (and ξ) that we summarize in the following variation of Algorithm 14.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 14.2 (Cluster-based sampling: Version 2)</head><p>This algorithm samples from (14.1).</p><p>(1) Initialize the algorithm some configuration x ∈ F (V ).</p><p>(2) Loop over the following steps: a. Generate a configuration ξ ∈ B x such that ξ (st) = 1 with probability given by (14.2) when x (s) = x (t) . b. Determine the connected components, V 1 , . . . , V m , of the graph G ξ = (V , E ξ ) with edges given by pairs {s, t} such that ξ (st) = 1. Note that x is constant on each of these connected components, i.e., there exists c 1 , . . . , c m ∈ F such that x = c</p><formula xml:id="formula_1285">(V 1 ) 1 ∧ • • • ∧ c (V m )</formula><p>m . c. Select at random one of the components, say, j 0 ∈ {1, . . . , m}. d. Sample the value cj 0 ∈ F according to the distribution</p><formula xml:id="formula_1286">q( cj 0 ) ∝ π( c(V 1 ) 1 ∧ • • • ∧ c(V m ) m ) ζ( c(V 1 ) 1 ∧ • • • ∧ c(V m ) m )</formula><p>.</p><p>with cj = c j if j j 0 . e. Set x (s) = cj 0 for s ∈ V j 0 .</p><p>Unlike single-variable updating schemes, these algorithms can update large chunks of the configurations at each step, and may result in significantly faster convergence of the sampling procedure. Note that step (2.d) in Algorithm 14.2 can be replaced by a Metropolis-Hastings update with a proper choice of proposal probability <ref type="bibr" target="#b34">[16]</ref>.</p><p>b. Parallel tempering. We now consider a different kind of extension in which we allow π depends continuously on a parameter β &gt; 0, writing π β and, the goal is to sample from π 1 . For example, one can extend (14.1) by the family of probability distributions</p><formula xml:id="formula_1287">π β (x) = 1 Z β exp         -β C∈C G h C (x (C) )        </formula><p>for β ≥ 0. For small β, π β gets close to the uniform distribution on F (V ) (achieved for β = 0), so that it becomes easier to move from local mode to local mode. This implies that sampling with small β is more efficient and the associated Markov chain moves more rapidly in the configuration space.</p><p>Assume given, for all β, two ergodic transition probabilities on F (V ), q β and qβ such that (12.7) is satisfied with π β as invariant probability, namely π β (y)q β (y, x) = π β (x) qβ (x, y) (</p><p>for all x, y ∈ F (V ) (as seen in (12.7), qβ is the transition probability for the reversed chain). The basic idea is that q β provides a Markov chain that converges rapidly for small β and slowly when β is closer to 1. Parallel tempering (this algorithm was introduced in Neal <ref type="bibr" target="#b158">[140]</ref> based on ideas developed in Marinari and Parisi <ref type="bibr" target="#b144">[126]</ref>) leverages this fact (and the continuity of π β in β) to accelerate the simulation of π 1 by introducing intermediate steps sampling at low β values.</p><p>The algorithm specifies a sequence of parameters 0 ≤ β 1 ≤ • • • ≤ β m = 1. One simulation steps goes down, then up this scale, as described in the following algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 14.3 (Parallel Tempering)</head><p>Start with an initial configuration x 0 ∈ F (V ). This configuration is then updated at each step, using the following sequence of operations.</p><p>(1) For j = 1, . . . , m, generate a configuration x j according to qβ j (x j-1 , •).</p><p>(2) Generate a configuration z m-1 according to q β m (x m , •).</p><p>(3) For j = m -1, . . . , 1, generate a configuration z j-1 according to q β j (z j , •).</p><p>(4) Set x 0 = z 0 with probability min</p><formula xml:id="formula_1289">        1, π β 0 (z 0 ) π β 0 (x 0 )         m-1 j=1 π β j (x j-1 ) π β j (x j )         π β m (x m-1 ) π β m (z m-1 )         m-1 j=1 π β j (z j ) π β j (z j-1 )                 . (Otherwise, keep x 0 unchanged).</formula><p>Importantly, the acceptance probability at step (4) only involves ratios of π ′ β s and therefore no normalizing constant. We now show that this algorithm is π β 0 -reversible. Let p(•, •) denote the transition probability of the chain. If z 0 x 0 , p(x 0 , z 0 ) corresponds to steps (1) to <ref type="bibr" target="#b21">(3)</ref>, with acceptance at step(4), and is therefore given by the sum, over all x 1 , . . . , x m and z 1 , . . . , z m , of products</p><formula xml:id="formula_1290">qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ) min         1, π β 0 (z 0 ) π β 0 (x 0 )         m-1 j=1 π β j (x j-1 ) π β j (x j )         π β m (x m-1 ) π β m (z m-1 )         m-1 j=1 π β j (z j ) π β j (z j-1 )                 Applying (14.3), this is equal to min qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ), π β 0 (z 0 ) π β 0 (x 0 ) q β 1 (x 0 , x 1 ) • • • q β m (x m-1 , x m ) qβ m (x m , z n-1 ) • • • qβ 1 (z 1 , z 0 ) So, π β 0 (x 0 )p(x 0 , z 0 ) = min π β 0 (x 0 ) qβ 1 (x 0 , x 1 ) • • • qβ m (x m-1 , x m )q β m (x m , z m-1 ) • • • q β 1 (z 1 , z 0 ), π β 0 (z 0 )q β 1 (x 1 , x 0 ) • • • q β m (x m , x m-1 ) qβ m (z m-1 , x m ) • • • qβ 1 (z 0 , z 1 )</formula><p>where the sum is over all x 1 , . . . , x m , z 1 , . . . , z m-1 ∈ F (V ). The sum is, of course, unchanged if one renames x 1 , . . . , x m , z 1 , . . . , z m-1 to z 1 , . . . , z m , x 1 , . . . , x m-1 , but doing so provides the expression of π β 0 (z 0 )p(z 0 , x 0 ), proving the reversibility of the chain with respect to π β 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.2">Inference with acyclic graphs</head><p>We now switch to deterministic methods to compute, or approximate, marginal probabilities of Markov random fields. In this section, we consider a directed acyclic graph G = (V , E). As we have seen, Markov processes for acyclic graphs are also Markov for any tree structure associated with the graph. Introducing such a tree, G = (V , Ẽ) with G♭ = G, we know that a Markov process on G can be written in the form (letting s 0 denote the root in G):</p><formula xml:id="formula_1291">π(x) = p s 0 (x (s 0 ) ) (s,t)∈ Ẽ p st (x (s) , x (t) ) (14.4)</formula><p>where p s 0 is a probability and p st a transition probability.</p><p>We now show how to compute marginal probabilities of configurations x (S) , denoted π S (x (S) ), for a set S ⊂ V , starting with singletons S = {s}. The computation can be done by propagating down the tree as follows. For s = s 0 , the probability is known, with π s 0 = p s 0 . Now take an arbitrary s s 0 and let pa(s) be its parent. Then π s (x (s) ) = P(X (s) = x (s) ) = y (pa(s)) ∈F pa(s) P (X (s) = x (s) | X (pa(s)) = y (pa(s)) )P (x (pa(s)) = y (pa(s)) ) = y (pa(s)) ∈F pa(s) π pa(s) (y (pa(s)) )p pa(s) (y pa(s) , x (s) ) so that the marginal probability at any s s 0 can be computed given the marginal probability of its parent. We can propagate the computation down the tree, with a total cost for computing π s proportional to n k=1 |F t k-1 | |F t k | where t 0 = s 0 , t 1 , . . . , t n = s is the unique path between s 0 and s. This is linear in the depth of the tree, and quadratic (not exponential) in the sizes of the state spaces. The computation of all singleton marginals requires an order of (s,t)∈E |F s | |F t | operations. Now, assume that probabilities of singletons have been computed and consider an arbitrary set S ⊂ V . Let s ∈ V be an ancestor of every vertex in S, maximal in the sense that none of its children also satisfy this property. Consider the subtrees of G starting from each of the children of s, denoted G1 , . . . , Gn with Gk = (V k , Ẽk ). Let S k = S ∩ V k . From the conditional independence,</p><formula xml:id="formula_1292">π S (x (S) ) = y (s) ∈F s P (X (S\{s}) = x (S\ { s}) | X (s) = y (s) )π s (y (s) ) = y (s) ∈F s n k=1,S k ∅ P (X (S k ) = x (S k ) | X (s) = y (s) )π s (y s ) Now, for all k = 1, . . . , n, we have |S k | &lt; |S|: this is obvious if S is not completely included in one of the V k 's. But if S ⊂ V k then the root, s k , of V k</formula><p>is an ancestor of all the elements in S and is a child of s, which contradicts the assumption that s is maximal. So we have reduced the computation of π S (x S ) to the computations of n probabilities of smaller sets, namely P (X (S k ) = x (S k ) | X (s) = y (s) ) for S k ∅. Because the distribution of X (V k ) conditioned at s is a Gk -Markov model, we can reiterate the procedure until only sets of cardinality one remain, for which we know how to explicitly compute probabilities. This provides a feasible algorithm to compute marginal probabilities with trees, at least when its distribution is given in tree-form, like in <ref type="bibr">(14.4)</ref>. We now address the situation in which one starts with a probability distribution associated with pair interactions (cf. definition 13.24) over the acyclic graph G π</p><formula xml:id="formula_1293">(x) = 1 Z s∈V ϕ s (x (s) )</formula><p>{s,t}∈E ϕ st (x (s) , x (t) ). (14.5)</p><p>We assume these local interactions to be consistent, still allowing for some vanishing ϕ st (x (s) , x (t) ).</p><p>Putting π in the form (14.4) is equivalent to computing all joint probability distributions π st (x (s) , x (t) ) for {s, t} ∈ E, and we now describe this computation. Denote</p><formula xml:id="formula_1294">U (x) = s∈V ϕ s (x (s) )</formula><p>{s,t}∈E ϕ st (x (s) , x (t) ) so that Z = y∈F (V ) U (y). For the tree G = (V , Ẽ), and t ∈ V , we let Gt = (V t , Ẽt ) be the subtree of G rooted at t (containing t and all its descendants). For S ⊂ V , define</p><formula xml:id="formula_1295">U S (x (S) ) = s∈S ϕ s (x (s) ) {s,s ′ }∈E,s,s ′ ∈D ϕ ss ′ (x (s) , x (s ′ ) ) and Z t (x (t) ) = y (V * t ) ∈F (V * t ) U V t (x (t) ∧ y (V * t ) ).</formula><p>with V * t = V t \ {t}.</p><p>Lemma 14.1 Let G = (V , E) be a directed acyclic graph and π = P X be the G-Markov distribution given by <ref type="bibr">(14.5)</ref>. With the notation above, we have π s 0 (x (s 0 ) ) = Z s 0 (x (s 0 ) )</p><formula xml:id="formula_1296">y (s 0 ) ∈F s 0 Z s 0 (y (s 0 ) ) (14.6)</formula><p>and, for (s, t) ∈ Ẽ,</p><formula xml:id="formula_1297">p st (x (s) , x (t) ) = P (X (t) = x (t) | X (s) = x (s) ) = ϕ st (x (s) , x (t) )Z t (x (t) ) y (t) ∈F t ϕ st (x (s) , y (t) )Z t (y (t) ) (14.7)</formula><p>Proof Let W t = V \V t . Clearly, Z = x (0) ∈F s 0 Z s 0 (x (0) ) and π s 0 (x (0) ) = Z s 0 (x (0) )/Z which gives <ref type="bibr">(14.6)</ref>. Moreover, if s ∈ V , we have</p><formula xml:id="formula_1298">P(X (V * s ) = x (V * s ) | X (s) = x (s) ) = y (W s ) U (x (V s ) ∧ y (W s ) ) y (V * s ) ,y (W s ) U (x (s) ∧ y (V * s ) ∧ y (W s )</formula><p>) .</p><p>We can write</p><formula xml:id="formula_1299">U (x (s) ∧ y (V * s ) ∧ y (W s ) ) = U V s (x (s) ∧ y (V * s ) )U {s}∪W s (x (s) ∧ y (W s ) )ϕ s (x (s) ) -1</formula><p>yielding the simplified expression</p><formula xml:id="formula_1300">P(X (V * s ) = x (V * s ) | X (s) = x (s) ) = U V s (x (V s ) )ϕ s (x (s) ) -1 y W s U {s}∪W s (x (s) ∧ y (W s ) ) ϕ s (x (s) ) -1 y (V * s ) U V s (x (s) ∧ y (V * s ) ) y (W s ) U {s}∪W s (x (s) ∧ y (W s ) ) = U V s (x (V s ) ) Z s (x (s) )</formula><p>Now, if t 1 , . . . , t n are the children of s, we have</p><formula xml:id="formula_1301">U V s (x (V s ) ) = ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 U V t k (x (V t k ) ), so that P(X (t k ) = x (t k ) , k = 1, . . . , n | X (s) = x (s) ) = 1 Z s (x (s) ) y (V * t k ) ,k=1,...,n ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 U V t k (x (t k ) ∧ y (V * t k ) ) = ϕ s (x (s) ) n k=1 ϕ st k (x (s) , x (t k ) ) n k=1 Z t k (x (t k ) ) Z s (x (s) )</formula><p>This implies that the transition probability needed for the tree model, p st 1 (x (s) , x (t 1 ) ), must be proportional to ϕ st 1 (x (s) , x (t 1 ) )Z t 1 (x (t 1 ) ) which proves the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This lemma reduces the computation of the transition probabilities to the computation of Z s (x (s) ), for s ∈ V . This can be done efficiently, going upward in the tree (from terminal vertexes to the root). Indeed, if s is terminal, then V s = {s} and Z s (x (s) ) = ϕ s (x (s) ). Now, if s is non-terminal and t 1 , . . . , t n are its children, then, it is easy to see that Z s (x (s) ) = ϕ s (x (s) )</p><formula xml:id="formula_1302">x (t 1 ) ∈F t 1 ,...,x (t n ) ∈F t n n k=1 ϕ st k (x (s) , x (t k ) )Z t k (x (t k ) ) = ϕ s (x (s) ) n k=1 x (t k ) ∈F t k ϕ st k (x (s) , x (t k ) )Z t k (x (t k ) ) (14.8)</formula><p>So, Z s (x (s) ) can be easily computed once the Z t (x (t) )'s are known for the children of s.</p><p>Equations (14.6) to (14.8) therefore provide the necessary relations in order to compute the singleton and edge marginal probabilities on the tree. It is important to note that these relations are valid for any tree structure consistent with the acyclic graph we started with. We now rephrase them with notation that only depend on this graph and not on the selected orientation.</p><p>Let {s, t} be an edge in E. Then s separates the graph G \ {s} into two components. Let V st be the component that contains t, and</p><formula xml:id="formula_1303">V * st = V st \ t. Define Z st (x t ) = y (V * st ) ∈F (V * st ) U V st (x (t) ∧ y (V * st ) ).</formula><p>This Z st coincides with the previously introduced Z t , computed with any tree in which the edge {s, t} is oriented from s to t. Equation (14.8) can be rewritten with this new notation in the form:</p><formula xml:id="formula_1304">Z st (x (t) ) = ϕ t (x (t) ) t ′ ∈V t \{s} x (t ′ ) ∈F t ′ ϕ tt ′ (x (t) , x (t ′ ) )Z tt ′ (x (t ′ ) ) . (14.9)</formula><p>This equation is usually written in terms of "messages" defined by</p><formula xml:id="formula_1305">m ts (x (s) ) = x (t) ∈F t ϕ st (x (s) , x (t) )Z st (x (t) )</formula><p>which yields Z st (x (t) ) = ϕ t (x (t) )</p><formula xml:id="formula_1306">t ′ ∈V t \{s} m t ′ t (x (t) )</formula><p>and the message consistency relation</p><formula xml:id="formula_1307">m ts (x (s) ) = x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ).<label>(14.10)</label></formula><p>Also, because one can start building a tree from G ♭ using any vertex as a root, (14.6) is valid for any s ∈ V , in the form (applying (14.8) to the root)</p><formula xml:id="formula_1308">π s (x (s) ) = 1 ζ s ϕ s (x (s) )</formula><p>t∈V s m ts (x (s) ) <ref type="bibr">(14.11)</ref> where ζ s is chosen to ensure that the sum of probabilities is 1. (In fact, looking at lemma 14.1, we have Z s = Z, independent of s.)</p><p>Similarly, (14.7) can be written p st (x (s) , x (t) ) = m ts (x (s) ) -1 ϕ st (x (s) , x (t) )ϕ t (x (t) )</p><p>t ′ ∈V t \{s} m t ′ t (x (t) ) (14.12) which provides the edge transition probabilities. Combining this with (14.11), we get the edge marginal probabilities: s) ). (14.13) Remark 14.2 We can modify (14.10) by multiplying the right-hand side by an arbitrary constant q ts without changing the resulting estimation of probabilities: this only multiplies the messages by a constant, which cancels after normalization. This remark can be useful in particular to avoid numerical overflow; one can, for example, define q ts = 1/ x s ∈F s m ts (x s ) so that the messages always sum to 1. This is also useful when applying belief propagation (see next section) to loopy networks, for which (14.10) may diverge while the normalized version converges. ♦</p><formula xml:id="formula_1309">π st (x (s) , x (t) ) = 1 ζ ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x (</formula><p>The following summarizes this message passing algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 14.4 (Belief propagation on acyclic graphs)</head><p>Given a family of interactions ϕ s :</p><formula xml:id="formula_1310">F s → [0, +∞), ϕ st : F s × F t → [0, +∞),<label>(1)</label></formula><p>Initialize functions (messages) m ts : F s → R, e.g., taking m ts (x (s) ) = 1/|F s |.</p><p>(2) Compute unnormalized messages mts (•) =</p><formula xml:id="formula_1311">x (t) ∈F t ϕ st (•, x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )</formula><p>and let m ts (•) = q ts mts (•), for some choice of constant q ts , which must be a fixed function of mts (•), such as</p><formula xml:id="formula_1312">q ts =          x (s) ∈F s mts (x (s) )          -1</formula><p>.</p><p>(3) Stop the algorithm when the messages stabilize (which happens after a finite number of updates). Compute the edge marginal distributions using (14.13).</p><p>It should be clear, from the previous analysis that messages stabilize in finite time, starting from the outskirts of the acyclic graph. Indeed, messages starting from a terminal t (a vertex with only one neighbor) are automatically set to their correct value in (14.10), m ts (x s ) =</p><p>x t ∈F t ϕ st (x s , x t )ϕ t (x t ), at the first update. These values then propagate to provide messages that satisfy (14.10) starting from the next-to-terminal vertexes (those that have only one neighbor left when the terminals are removed) and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.3">Belief propagation and free energy approximation 14.3.1 BP stationarity</head><p>It is possible to run Algorithm 14.4 on graphs that are not acyclic, since nothing in its formulation requires this property. However, while the method stabilizes in finite time for acyclic graphs, this property, or even the convergence of the messages is not guaranteed for general, loopy, graphs. Convergence, however, has been observed in a large number of applications, sometimes with very good approximations of the true marginal distributions.</p><p>We will refer to stable solutions of Algorithm 14.4 as BP-stationary points, as formally stated in the next definition, which allows for a possible normalization of messages, which is particularly important with loopy networks. Definition 14.3 Let G = (V , E) be an undirected graph and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. We say that a family of joint probability distributions (π ′ st , {s, t} ∈ E) is BP-stationary for (G, Φ) if there exists messages x t ∈ F t → m st (x t ), constants ζ st for t ∼ s and α s for s ∈ V satisfying</p><formula xml:id="formula_1313">m ts (x (s) ) = α s ζ ts x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) (14.14) such that π ′ st (x (s) , x (t) ) = 1 ζ st ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x (s) ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(14.15)</head><p>There is no loss of generality in the specific form chosen for the normalizing constants in (14.14) and (14.15), in the sense that, if the messages satisfy (14.15) and</p><formula xml:id="formula_1314">m ts (x (s) ) = q ts x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )</formula><p>for some constants q ts , then</p><formula xml:id="formula_1315">ζ st = x (s) ∈F s ,x (t) ∈F t ϕ st (x (s) , x (t) )ϕ s (x (s) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) ) s ′ ∈V s \{t} m s ′ s (x (s) ) = 1 q ts x (s) ∈F s ϕ s (x (s) ) s ′ ∈V s m s ′ s (x (s) )</formula><p>so that ζ st q ts (which has been denoted α s ) does not depend on t. Of course, the relevant questions regarding BP-stationarity is whether the collection of pairwise probability π ′ st exists, how to compute them, and whether π ′ st (x (s) , x (t) ) provides a good approximation of the marginals of the probability distribution π that is associated to Φ, namely</p><formula xml:id="formula_1316">π(x) = 1 Z s∈V ϕ s (x (s) )</formula><p>{s,t}∈E ϕ st (x (s) , x (t) ).</p><p>A reassuring statement for BP-stationarity is that it is not affected when the functions in Φ are multiplied by constants, which does not affect the underlying probability π. This is stated in the next proposition. Proposition 14.4 Let Φ be as above a family of edge and vertex interactions. Let c st , {s, t} ∈ E, c s , s ∈ V be families of positive constants, and define Φ = ( φst , φs ) by φst = c st ϕ st and φs = c s ϕ s . Then,</p><formula xml:id="formula_1317">π ′ is BP-stationary for (G, Φ) ⇔ π ′ is BP-stationary for (G, Φ).</formula><p>Proof Indeed, if (14.14) and (14.15) are true for (G, Φ), it suffices to replace α s by α s c s and ζ st by ζ st c st c t to obtain (14.14) and (14.15) for (G, Φ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>It is also important to notice that, if G is acyclic, definition 14.3 is no more general than the message-passing rule we had considered earlier. More precisely, we have (see remark 14.2), Proposition 14.5 Let G = (V , E) be undirected acyclic and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. Then, the only BP-stationary distributions are the marginals of the distribution π associated to Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.3.2">Free-energy approximations</head><p>A partial justification of the good behavior of BP with general graphs has been provided in terms of a quantity introduced in statistical mechanics, called the Bethe free energy. We let G = (V , E) be an undirected graph and assume that a consistent family of pair interactions is given (denoted Φ = (ϕ s , s ∈ V , ϕ st , {s, t} ∈ E)) and consider the associated distribution, π, on F (V ), given by</p><formula xml:id="formula_1318">π(x) = 1 Z s∈V ϕ s (x (s) )</formula><p>{s,t}∈E ϕ st (x (s) , x (t) ). (14.16)</p><p>It will also be convenient to use the function</p><formula xml:id="formula_1319">ψ st (x (s) , x (t) ) = ϕ s (x (s) )ϕ t (x (t) )ϕ st (x (s) , x (t) ) such that π(x) = 1 Z s∈V ϕ s (x (s) ) 1-|V s | {s,t}∈E</formula><p>ψ st (x (s) , x (t) ). (14.17)</p><p>We will consider approximations π ′ of π that minimize the Kullback-Leibler divergence, KL(π ′ ∥π) (see (4.3)), subject to some constraints. We can write</p><formula xml:id="formula_1320">KL(π ′ ∥π) = -E π ′ (log π) -H(π ′ ) = -log Z - s∈V (1 -|V s |)E π ′ (log ϕ s ) - {s,t}∈E E π ′ (log ψ st ) -H(π ′ )</formula><p>(where H(π ′ ) is the entropy of π ′ ). Introduce the one-and two-dimensional marginals of π ′ , denoted π ′ s ad π ′ st . Then</p><formula xml:id="formula_1321">KL(π ′ ∥π) = -log Z - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ st ) + s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st ) -H(π ′ ).</formula><p>The Bethe free energy is the function F β defined by</p><formula xml:id="formula_1322">F β (π ′ ) = - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ st ) ; (14.18) so that KL(π ′ ∥π) = F β (π ′ ) -log Z + ∆ G (π ′ ) with ∆ G (π ′ ) = s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st ) -H(π ′ ).</formula><p>Using this computation, one can consider the approximation problem: find π′ that minimizes KL(π ′ ∥π) over a class of distributions π ′ for which the computation of the first and second order marginals is easy. This problem has an explicit solution when the distribution π ′ is such that all variables are independent, leading to what is called the mean-field approximation of π. Indeed, in this case, we have</p><formula xml:id="formula_1323">∆ G (π ′ ) = {s,t}∈G (H(π ′ s ) + H(π ′ t )) + s∈S (1 -|V s |)H(π ′ s ) - s∈S H(π ′ s ) = 0 and F β (π ′ ) = - s∈V (1 -|V s |)E π ′ (log ϕ s π ′ s ) - {s,t}∈E E π ′ (log ψ st π ′ s π ′ t ) .</formula><p>F β must be minimized with respect to the variables π ′ s (x (s) ), s ∈ S, x s ∈ F S subject to the constraints x s ∈F s π ′ s (x (s) ) = 1. The corresponding necessary optimality conditions equations provide the mean-field consistency equations, described in the following definition.</p><formula xml:id="formula_1324">Proposition 14.6 A local minimum of F β (π ′ ) over all probability distributions π ′ of the form π ′ (x) = s∈V π ′ s (x (s) )</formula><p>must satisfy the mean field consistency equations:</p><formula xml:id="formula_1325">π s (x (s) ) = 1 Z s ϕ s (x (s) ) 1-|V s |</formula><p>t∼s exp E π t (log ψ st (x (,) .)) . <ref type="bibr">(14.19)</ref> Proof Since all constraints are affine, we can use Lagrange multipliers, denoted (λ s , s ∈ S) for each of the constraints, to obtain necessary conditions for a minimizer, yielding</p><formula xml:id="formula_1326">∂F β ∂π s (x s ) -λ s = 0, s ∈ S, x s ∈ F s .</formula><p>This gives:</p><formula xml:id="formula_1327">-(1 -|V s |) log ϕ s (x s ) π s (x s ) -1 - t∼s x t ∈F t log ψ st (x s , x t ) π s (x s )π t (x t ) -1 π t (x t ) = λ s .</formula><p>Solving this with respect to π s (x s ) and regrouping all constant terms (independent from x s ) in the normalizing constant Z s yields (14. <ref type="bibr" target="#b37">19</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The mean field consistency equations can be solved using a root-finding algorithm or by directly solving the minimization problem. We will retrieve this method, with more details, in our discussion of variational approximations in chapter 16.</p><p>In the particular case in which G is acyclic and the approximation is made by G-Markov processes, the Kullback-Leibler distance is minimized with π ′ = π (since π belongs to the approximating class). A slightly non-trivial remark is that π is optimal also for the minimization of the Bethe free energy F β , because this energy coincides, up to the constant term log Z, with the Kullback-Leibler divergence, as proved by the following proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 14.7 If G is acyclic and π</head><formula xml:id="formula_1328">′ is G-Markov, then ∆ G (π ′ ) = 0.</formula><p>This proposition is a consequence of the following lemma that has its own interest:</p><formula xml:id="formula_1329">Lemma 14.8 If G is acyclic and π is a G-Markov distribution, then π(x) = s∈V π s (x (s) ) 1-|V s | {s,t}∈E π st (x (s) , x (t) ).</formula><p>(14.20)</p><p>Proof (of lemma 14.8) We know that, if G = (V , Ẽ) is a tree such that G♭ = G, we have, letting s 0 be the root in G</p><formula xml:id="formula_1330">π(x) = π s 0 (x (s 0 ) ) (s,t)∈ Ẽ p st (x (s) , x (t) ) = π s 0 (x (s 0 ) ) (s,t)∈ Ẽ(π st (x (s) , x (t) )π(x (s) ) -1 ). Each vertex s in V has |V s | -1 children in G, except s 0 which has |V s 0 | children. Using this, we get π(x) = π s 0 (x (s 0 ) )π s 0 (x (s 0 ) ) -|V s 0 | s∈V \{s 0 } π s (x (s) ) 1-|V s | (s,t)∈ Ẽ π st (x (s) , x (t) ) = s∈V π s (x (s) ) 1-|V s | {s,t}∈E π st (x (s) , x (t) ).</formula><p>Proof (of proposition 14.7) If π ′ is given by (14.20), then</p><formula xml:id="formula_1331">H(π ′ ) = -E π ′ log π ′ = - s∈V (1 -|V s |)E π ′ log π ′ s - {s,t}∈E E π ′ log π ′ st = s∈V (1 -|V s |)H(π ′ s ) + {s,t}∈E H(π ′ st )</formula><p>which proves that ∆ G (π ′ ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>In view of this, it is tempting to "generalize" the mean field optimization procedure and minimize F β (π ′ ) over all possible consistent singletons and pair marginals (π ′ s and π ′ st ), then use the optimal ones as an approximation of π s and π st . What we have just proved is that this procedure provides the exact expression of the marginals when G is acyclic. For loopy graphs, however, it is not justified, and is at best an approximation. A very interesting fact is that this procedure provides the same consistency equations as belief propagation. To see this, we first start with the characterization of minimizers of F β . Proposition 14.9 Let G = (V , E) be an undirected graph and π be given by <ref type="bibr">(14.16)</ref>. Consider the problem of minimizing the Bethe free energy F β in <ref type="bibr">(14.18)</ref> with respect to all possible choices of probability distributions (π ′ st , {s, t} ∈ E), (π ′ s , s ∈ V ) with the constraints</p><formula xml:id="formula_1332">π ′ s (x (s) ) = x (t) ∈F t π ′ st (x (s) , x (t) ), ∀x (s) ∈ F s and t ∼ s.</formula><p>Then a local minimum of this problem must take the form</p><formula xml:id="formula_1333">π ′ st (x (s) , x (t) ) = 1 Z st ψ st (x (s) , x (t) )µ st (x (t) )µ ts (x (s) ) (14.21)</formula><p>where the functions µ st : F t → [0, +∞) are defined for all (s, t) such that {s, t} ∈ E and satisfy the consistency conditions:</p><formula xml:id="formula_1334">µ ts (x (s) ) -(|V s |-1) s ′ ∼s µ s ′ s (x (s) ) =          e Z st x (t) ∈F t ψ st (x (s) , x (t) )ϕ t (x (t) )µ st (x (t) )          |V s |-1 . (14.22)</formula><p>Proof We introduce Lagrange multipliers: λ ts (x (s) ) for the constraint</p><formula xml:id="formula_1335">π ′ s (x (s) ) = x (t) ∈F t π ′ st (x (s) , x (t) )</formula><p>and γ st for</p><formula xml:id="formula_1336">x (s) ,x (t) π ′ st (x (s) , x (t) ) = 1,</formula><p>which covers all constraints associated to the minimization problem. The associated Lagrangian is</p><formula xml:id="formula_1337">F β (π ′ ) - s∈V x (s) ∈F s t∼s λ ts (x (s) )          x (t) ∈F t π ′ st (x (s) , x (t) ) -π ′ s (x (s) )          - {s,t}∈E γ st          x (s) ∈F s ,x (t) ∈F t π ′ st (x (s) , x (t) ) -1          .</formula><p>The derivative with respect to π ′ st (x (s) , x (t) ) yields the condition</p><formula xml:id="formula_1338">log π ′ st (x (s) , x (t) ) -log ψ st (x (s) , x (t) ) + 1 -λ ts (x (s) ) -λ st (x (t) ) -γ st = 0.</formula><p>which implies π ′ st (x (s) , x (t) ) = ϕ st (x (s) , x (t) ) exp(γ st -1) exp(λ ts (x (s) ) + λ st (x (t) )).</p><p>We let Z st = exp(1γ st ), with γ st chosen so that π ′ st is a probability. The derivative with respect to π ′ s (x (s) ) gives</p><formula xml:id="formula_1339">(1 -|V s |)(log π ′ s (x (s) ) -log ϕ s (x (s) ) + 1) + t∼s λ ts (x (s) ) = 0.</formula><p>Combining this with the expression just obtained for π ′ st , we get, for t ∼ s,</p><formula xml:id="formula_1340">(1 -|V s |) log x (t) ∈F t ψ st (x (s) , x (t) )e λ st (x (t) ) + (1 -|V s |)λ ts (x (s) ) + (1 -|V s |)(1 -log Z st -log ϕ s (x (s) )) + s ′ ∼s λ s ′ s (x (s) ) = 0,</formula><p>which gives (14.22) with µ st = exp(λ st ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>A family π ′ st satisfying conditions (14.21) and (14.22) of proposition 14.9 will be called Bethe-consistent. A very interesting remark states that Bethe-consistency is equivalent to BP-stationarity, as stated below. Proposition 14.10 Let G = (V , E) be an undirected graph and Φ = (ϕ st , {s, t} ∈ E, ϕ s , s ∈ V ) a consistent family of pair interactions. Then a family π ′ of joint probability distributions is BP-stationary if and only if it is Bethe-consistent.</p><p>Proof First assume that π ′ is BP-stationary with messages m st , so that (14.14) and (14.15) are satisfied. Take</p><formula xml:id="formula_1341">µ st = a t t ′ ∈V t ,t ′ s m t ′ t (x (t) )</formula><p>for some constant a t that will be determined later. Then, the left-hand side of (14. <ref type="formula" target="#formula_2063">22</ref>) is</p><formula xml:id="formula_1342">µ ts (x (s) ) -(|V s |-1) s ′ ∈V s µ s ′ s (x (s) ) = a s         s ′ ∈V s ,s ′ t m s ′ s (x (s) )         -(|V s |-1) s ′ ∈V s s ′′ ∈V s ,s ′′ s ′ m s ′′ s (x (s) ) = a s m ts (x (s) ) |V s |-1 .</formula><p>The right-hand side is equal to (using (14.14))</p><formula xml:id="formula_1343">ea t ζ st Z st α s m ts (x (s) ) |V s |-1</formula><p>, so that we need to have</p><formula xml:id="formula_1344">a s = ea t ζ st Z st α s |V s |-1</formula><p>.</p><p>We also need Z st =</p><p>x (s) ,x (t)   ψ st (x (s) , x (t) )µ st (x (t) )µ ts (x (s) ) = a s a t ζ st .</p><p>Solving these equations, we find that (14.21) and (14.22) are satisfied with</p><formula xml:id="formula_1345">       a s = (e/α s ) (|V s |-1)/|V s | Z st = ζ st a s a t which proves that π ′ is Bethe-consistent.</formula><p>Conversely, take a Bethe-consistent π ′ , and µ st , Z st satisfying (14.21) and <ref type="bibr">(14.22)</ref>. For s such that |V s | &gt; 1, define, for t ∈ V s , m ts (x (s) ) = µ ts (x (s) ) -1</p><formula xml:id="formula_1346">s ′ ∼s µ s ′ s (x (s) ) 1/(|V s |-1) . (<label>14</label></formula><p>.23) Define also, for |V s | &gt; 1, ρ ts (x (s) ) = s ′ ∈V s ,s ′ t m s ′ s (x (s) ). (If |V s | = 1, take ρ ts ≡ 1.) Using (14.23), we find ρ ts = µ ts when |V s | &gt; 1, and this identity is still valid when |V s | = 1, since in this case, (14.22) implies that µ ts (x (s) ) = 1. We need to find constants α t and ζ st such that (14.14) and (14.15) are satisfied. But (14.15) implies ζ ts = x t ,x s ψ st (x (s) , x (t) )ρ st (x (t) )ρ ts (x (s) ) and (14.21) implies ζ ts = Z ts . We now consider (14.14), which requires m ts (x (s) ) = α s ζ st x (t)</p><p>ϕ st (x (s) , x (t) )ϕ t (x (t) )ρ st (x (t) ).</p><p>It is now easy to see that this identity to the power |V s | -1 coincides with (14.22) as soon as one takes α s = e. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.4">Computing the most likely configuration</head><p>We now address the problem of finding a configuration that maximizes π(x) (mode determination). This problem turns out to be very similar to the computation of marginals, that we have considered so far, and we will obtain similar algorithms.</p><p>Assume that G is undirected and acyclic and that π can be written as</p><formula xml:id="formula_1347">π(x) = 1 Z {s,t}∈E ϕ st (x (s) , x (t) )</formula><p>s∈V ϕ s (x (s) ).</p><p>Maximizing π(x) is equivalent to maximizing</p><formula xml:id="formula_1348">U (x) = {s,t}∈E ϕ st (x (s) , x (t) )</formula><p>s∈V ϕ s (x (s) ). <ref type="bibr">(14.24)</ref> Assume that a root has been chosen in G, with the resulting edge orientation yielding a tree G = (V , Ẽ) such that G♭ = G. We partially order the vertexes according to G, writing s ≤ t if there exists a path from s to t in G (s is an ancestor of t). Let V + s contain all t ∈ V with t ≥ s, and define</p><formula xml:id="formula_1349">U s (x (V + s ) ) = {t,u}∈E V + s ϕ tu (x (t) , x (u) ) t&gt;s ϕ t (x (t) ) and U * s (x (s) ) = max U s (y (V + s ) ), y (s) = x (s) . (14.25)</formula><p>Since we can write</p><formula xml:id="formula_1350">U s (x (V + s ) ) = t∈s + ϕ st (x (s) , x (t) )ϕ t (x (t) )U t (x (V + t ) ),<label>(14.26)</label></formula><p>we have</p><formula xml:id="formula_1351">U * s (x (s) ) = max x (t) ,t∈s +        t∈s + ϕ t (x (t) )ϕ st (x (s) , x (t) )U * t (x (t) )        = t∈s + max x t ∈F t (ϕ t (x (t) )ϕ st (x (s) , x (t) )U * t (x (t) )).<label>(14.27)</label></formula><p>This provides a method to compute U * s (x (s) ) for all s, starting with the leaves and progressively updating the parents. (When s is a leaf, U * s (x (s) ) = 1, by definition.)</p><p>Once all U * s (x (s) ) have been computed, it is possible to obtain a configuration x * that maximizes π. This is because an optimal configuration must satisfy U * s (x (s)</p><formula xml:id="formula_1352">* ) = U s (x (V + s ) *</formula><p>) for all s ∈ V , i.e., x (V + s \{s}) * must solve the maximization problem in <ref type="bibr">(14.25)</ref>. But because of (14.26), we can separate this problem over the children of s and obtain the fact that, it t ∈ s + ,</p><formula xml:id="formula_1353">x (t) * = argmax x (t) ϕ t (x (t) )ϕ st (x (s) * , x (t) )U * t (x (t) ) .</formula><p>This procedure can be rewritten in a slightly different form using messages similar to the belief propagation algorithm. It s ∈ t + , define</p><formula xml:id="formula_1354">µ st (x (t) ) = max x s ∈F s (ϕ t (x (t) )ϕ ts (x (t) , x (s) )U * s (x (s) ))</formula><p>and ξ st (x (t) ) = argmax</p><p>x (s) ∈F s (ϕ t (x (t) )ϕ ts (x (t) , x (s) )U * s (x (s) )).</p><p>Using section 14.4, we get</p><formula xml:id="formula_1355">µ st (x (t) ) = max x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        , ξ st (x t ) = argmax x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        .</formula><p>An optimal configuration can now be computed using x</p><formula xml:id="formula_1356">(t) * = ξ ts (x (s) * ), with s ∈ pa(t).</formula><p>This resulting algorithm therefore first operates upwards on the tree (from leaves to root) to compute the µ st 's and ξ st 's, then downwards to compute x * . This is summarized in the following algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 14.5</head><p>A most likely configuration for</p><formula xml:id="formula_1357">π(x) = 1 Z {s,t}∈E ϕ st (x s , x t ) s∈V ϕ s (x s ).</formula><p>can be computed after iterating the following updates, based on any acyclic orientation of G:</p><p>(1) Compute, from leaves to root:</p><formula xml:id="formula_1358">µ st (x (t) ) = max x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        and ξ st (x (t) ) = argmax x (s) ∈F s        ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈s + µ us (x (s) )        .</formula><p>(2) Compute, from root to leaves:</p><formula xml:id="formula_1359">x (t) * = ξ ts (x (s) * ), with s = pa(t).</formula><p>Similar to the computation of marginals, this algorithm can be rewritten in an orientation-independent form. The main remark is that the value of µ st (x (t) ) does not depend on the tree orientation, as long as it is chosen such that s ∈ t + , i.e., the edge {s, t} is oriented from t to s. This is because such a choice uniquely prescribes the orientation of the edges of the descendants of s for any such tree, and µ st only depends on this structure. Since the same remark holds for ξ st , this provides a definition of these two quantities for any pair s, t such that {s, t} ∈ E. The updating rule now becomes µ st (x (t) ) = max</p><formula xml:id="formula_1360">x (s) ∈F s         ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈V s \{t} µ us (x (s) )         , (14.28) ξ st (x (t) ) = argmax x (s) ∈F s         ϕ ts (x (t) , x (s) )ϕ s (x (s) ) u∈V s \{t} µ us (x (s) )         (14.29) with x (t) * = ξ ts (x<label>(s)</label></formula><p>* ) for any pair s ∼ t. Like with the m ts in the previous section, looping over updating all µ ts in any order will finally stabilize to their correct values, although, if an orientation is given, going from leaves to roots is obviously more efficient.</p><p>The previous analysis is not valid for loopy graphs but section 14.4 and section 14.4 provide well defined iterations when G is an arbitrary undirected graph, and can therefore be used as such, without any guaranteed behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.5">General sum-prod and max-prod algorithms 14.5.1 Factor graphs</head><p>The expressions we obtained for message updating with belief propagation and with mode determination respectively took the form</p><formula xml:id="formula_1361">m ts (x (s) ) ← x (t) ∈F t ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} m t ′ t (x (t) )</formula><p>and</p><formula xml:id="formula_1362">µ ts (x (s) ) ← max x (t) ∈F t         ϕ st (x (s) , x (t) )ϕ t (x (t) ) t ′ ∈V t \{s} µ t ′ t (x (t) )         .</formula><p>They first one is often referred to as the "sum-prod" update rule, and the second as the "max-prod". In our construction, the sum-prod algorithm provided us with a method computing</p><formula xml:id="formula_1363">σ s (x (s) ) = y (V \{s}) U (x (s) ∧ y (V \{s}) ) with U (x) = s ϕ s (x (s) )</formula><p>{s,t}∈E ϕ st (x (s) , x (t) ).</p><p>Indeed, we have, according to (14.11)</p><formula xml:id="formula_1364">σ s (x (s) ) = ϕ s (x (s) )</formula><p>t∈V s m ts (x (s) ).</p><p>Similarly, the max-prod algorithm computes</p><formula xml:id="formula_1365">ρ s (x (s) ) = max y V \{s} U (x (s) ∧ y (V \{s}) ) via the relation ρ s (x (s) ) = ϕ s (x (s) )</formula><p>t∈V s µ ts (x (s) ).</p><p>We now discuss generalizations of these algorithms to situations in which the function U does not decompose as a product of bivariate functions. More precisely, let S be a subset of P (V ), and assume the decomposition</p><formula xml:id="formula_1366">U (x) = C⊂S ϕ C (x C ).</formula><p>The previous algorithms can be generalized using the concept of factor graphs associated with the decomposition. The vertexes of this graph are either indexes s ∈ V or sets C ∈ S, and the only edges link indexes and sets that contain them. The formal definition is as follows.</p><p>Definition 14.11 Let V be a finite set of indexes and S a subset of P (V ). The factor graph associated to V and S is the graph G = (V ∪ S, E), E being constituted of all pairs {s, C} with C ∈ S and s ∈ C.</p><p>We assign the variable x (s) to a vertex s ∈ V of the factor graph, and the function ϕ C to C ∈ S. With this in mind, the sum-prod and max-prod algorithms are extended to factor graphs as follows. Definition 14.12 Let G = (V ∪S, E) be a factor graph, with associated functions ϕ C (x C ). The sum-prod algorithm on G updates messages m sC (x s ) and m Cs (x s ) according to the rules</p><formula xml:id="formula_1367">               m sC (x (s) ) ← C,s∈ C, C C m Cs (x (s) )</formula><p>m Cs (x (s) ) ← y C :y (s) =x (s)   ϕ C (y (C) ) t∈C\{s} m tC (y (t) ) (14.30)</p><p>Similarly, the max-prod algorithm iterates</p><formula xml:id="formula_1368">               µ sC (x (s) ) ← C,s∈ C, C C µ Cs (x (s) )</formula><p>µ Cs (x (s) ) ← max y (C) :y (s) =x (s)   ϕ C (y (C) ) t∈C\{s} µ tC (y (t) ) (14.31)</p><p>These algorithms reduce to the original ones when only single vertex and pair interactions exist. Let us check this with sum-prod. In this case, the set S contains all singletons C = {s}, with associated function ϕ s , and all edges {s, t} with associated function ϕ st . We have links between s and {s} and s and {s, t} ∈ E. For singletons, we have m s{s} (x (s) ) ← t∼s m s{s,t} (x (s) ) and m {s}s (x (s) ) ← ϕ s (x (s) ).</p><p>For pairs, m s{s,t} (x (s) ) ← ϕ s (x (s) )</p><p>t∈V s \{t} m {s, t}s (x (s) ) and m {s,t}s (x (s) ) ←</p><formula xml:id="formula_1369">y (t)</formula><p>ϕ st (x (s) , y (t) )m t{s,t} (y (t) )</p><p>and, combining the last two assignments, it becomes clear that we retrieve the initial algorithm with m {s,t}s taking the role of what we previously denoted m ts .</p><p>The important question, obviously, is whether the algorithms converge. The following result shows that this is true when the factor graph is acyclic. Proposition 14.13 Let G = (V ∪ S, E) be a factor graph with associated functions ϕ C . Assume that G is acyclic. Then the sum-prod and max-prod algorithms converge in finite time.</p><p>After convergence, we have σ s (x (s) ) = C,s∈C m Cs (x (s) ) and ρ s (x (s) ) = C,s∈C µ Cs (x (s) ).</p><p>Proof Let us assume that G is connected, which is without loss of generality, since the following argument can be applied to each component of G separately. Since G is acyclic, we can arbitrarily select one of its vertexes as a root to form a tree. This being done, we can see that the messages going upward in the tree (from children to parent) progressively stabilize, starting with leaves. Leaves in the factor graph indeed are either singletons, C = {s}, or vertexes s ∈ V that belong to only one set C ∈ S. In the first case, the algorithm imposes (taking, for example, the sum-prod case) m {s}s (x (s) ) = ϕ s (x (s) ), and in the second case m sC (x (s) ) = 1. So the messages sent upward by the leaves are set at the first step. Since the messages going from a child to its parents only depend on the messages that it received from its other neighbors in the acyclic graph, which are its children in the tree, it is clear that all upward messages progressively stabilize until the root is reached. Once this is done, messages propagate downward from each parent to its children. This stabilizes as soon as all incoming messages to the parent are stabilized, since outgoing messages only depend on those. At the end of the upward phase, this is true for the root, which can then send its stable message to its children. These children now have all their incoming messages and can now send their messages to their own children and so on down to the leaves.</p><p>We now consider the second statement, proceeding by induction, assuming that the result is true for any smaller graph than the one considered. Let s 0 be the selected root, and consider all vertexes s s 0 such that there exists C s ∈ S such that s 0 and s both belong to C s . Given s, there cannot be more than one such C s since this would create a loop in the graph. For each such s, consider the part G s of G containing all descendants of s. Let V s be the set of vertexes among the descendants of s and C s the set of C's below s. Define</p><formula xml:id="formula_1370">U s (x (V s ) ) = C∈C s ϕ C (x (C) ).</formula><p>Since the upward phase of the algorithm does not depend on the ancestors of s, the messages incoming to s for the sum-prod algorithm restricted to G s are the same as with the general algorithm, so that, using the induction hypothesis y (V s ) ,y (s) =x (s)   U s (y (V s ) ) = C∈C s ,s∈C m Cs (x (s) ) = m sC s (x (s) ). Now let C 1 , . . . , C n list all the sets in C that contain s 0 , which must be non-intersecting (excepted at {s 0 }), again not to create loops. Write</p><formula xml:id="formula_1371">C 1 ∪ • • • ∪ C n = {s 0 , s 1 , . . . , s q }.</formula><p>Then, we have</p><formula xml:id="formula_1372">U (x) = n j=1 ϕ C j (x (C j ) ) q i=1 U s i (x (V s i ) )</formula><p>and letting S = n j=1 C j \ {s 0 },</p><formula xml:id="formula_1373">σ s 0 (x (s 0 ) ) = y (V ) :y (s 0 ) =x (s 0 ) n j=1 ϕ C j (y (C j ) ) q i=1 U s i (y (V s i ) ) = y ( ) S:y (s 0 ) =x (s 0 ) n j=1 ϕ C j (y (C j ) ) q i=1 m s i C s i (y (s i ) ) = n j=1 y ( ) C j :y (s 0 ) =x (s 0 ) ϕ C j (y (C j ) ) s∈C j \{s 0 } m sC s (y (s) ) = n j=1 m C j s 0 (x (s 0 ) )</formula><p>which proves the required result (note that, when factorizing the sum, we have used the fact that the sets C j \ {s 0 } are non intersecting). An almost identical argument holds for the max-prod algorithm.</p><p>■ Remark 14.14 Note that these algorithms are not always feasible. For example, it is always possible to represent a function U on F (V ) with the trivial factor graph in which S = {V } and E contains all {s, V }, s ∈ V (using ϕ V = U ), but computing m V s is identical to directly computing σ s with a sum over all configurations on V \ {s} which grows exponentially. In fact, the complexity of the sum-prod and max-prod algorithms is exponential in the size of the largest C in S which should therefore remain small. ♦ Remark 14.15 It is not always possible to decompose a function so that the resulting factor graph is acyclic with small degree (maximum number of edges per vertex). Sum-prod and max-prod can still be used with loopy networks, sometimes with excellent results, but without theoretical support. ♦ Remark 14. <ref type="bibr" target="#b34">16</ref> One can sometimes transform a given factor graph into an acyclic one by grouping vertexes. Assume that the set S ⊂ P (V ) is given. We will say that a partition ∆ = (D 1 , . . . D k ) of V is S-admissible if, for any C ∈ S and any j ∈ {1, . . . , k}, one has either</p><formula xml:id="formula_1374">D j ∩ C = ∅ or D j ⊂ C.</formula><p>If ∆ is S-admissible, one can define a new factor graph G as follows. We first let Ṽ = {1, . . . , k}. To define S ⊂ P ( Ṽ ) assign to each C ∈ S the set J C of indexes j such that D j ⊂ C. From the admissibility assumption, <ref type="bibr">(14.32)</ref> so that C → J C is one-to-one. Let S = {J C , C ∈ S}. Group variables using x(k) = x (D k ) , so that Fk = F (D k ). Define Φ = ( φ C , C ∈ S) by φ C = ϕ C where C is given by (14.32).</p><formula xml:id="formula_1375">C = j∈J C D j ,</formula><p>In other terms, one groups variables (x (s) , s ∈ V ) into clusters, to create a simpler factor graph, which may be acyclic even if the original one was not. For example, if V = {a, b, c, d}, S = {A, B} with A = {a, b, c} and B = {b, c, d}, then (A, c, B, b) is a cycle in the associated factor graph. If, however, one takes D 1 = {a}, D 2 = {b, c} and D 3 = {d}, then (D 1 , D 2 , D 3 ) is S-admissible and the associated factor graph is acyclic. In fact, in such a case, the resulting factor graph, considered as a graph with vertexes given by subsets of V , is a special case of a junction tree, which is defined in the next section.♦ 14.5.2 Junction trees Definition 14.17 Let V be a finite set. A junction tree on V is an undirected acyclic graph G = (S, E) where S ⊂ P (V ) is a family of subsets of V that satisfy the following property, called the running intersection constraint: if C, C ′ ∈ S and s ∈ C ∩ C ′ , then all sets C ′′ in the (unique) path connecting C and C ′ in G must also contain s.</p><p>Remark 14.18 Let us check that the clustered factor graph G defined in remark 14.16 is equivalent to a junction tree when acyclic. Using the same notation, let Ŝ = {D 1 , . . . , D k } ∪ S, removing if needed sets C ∈ S that coincide with one of the D j 's. Place an edge between D j and C if and only if D j ⊂ C.</p><p>Let (C 1 , D i 1 , . . . , D i n-1 , C n ) be a path in that graph. Assume that s ∈ C 1 ∩ C 2 . Let D i n be the unique D j that contains s. It is such that from the the admissibility assumption, D i n ⊂ C 1 and D i n ⊂ C n , which implies that (C 1 , D i 1 , . . . , C n , D i n , C 1 ) is a path in G. Since G is acyclic, this path must be a union of folded paths. But it is easy to see that any folded path satisfies the running intersection constraint. (Note that there was no loss of generality in assuming that the path started and ended with a "C", since any "D" must be contained in the C that follows or precedes it.) ♦</p><p>We now consider a probability distribution written in the form</p><formula xml:id="formula_1376">π(x) = 1 Z C∈S ϕ C (x (C) )</formula><p>and we make the assumption that S can be organized as a junction tree.</p><p>Belief propagation can be extended to junction trees. Fixing a root C 0 ∈ S, we first choose an orientation on G, which induces as usual a partial order on S. For C ∈ S, define S + C as the set of all B ∈ S such that B &gt; C. Define also</p><formula xml:id="formula_1377">V + C = B∈S + C B.</formula><p>We want to compute sums</p><formula xml:id="formula_1378">σ C (x (C) ) = y (V \C) U (x (C) ∧ y (V \C) ),</formula><p>where U (x) = C∈S ϕ C (x (C) ). We have</p><formula xml:id="formula_1379">σ C (x (C) ) = y (V \C) ϕ C (x (C) ) B∈S\{C} ϕ B (x (B∩C) ∧ y (B\C) ). Define σ + C (x (C) ) = y (V + C \C) B&gt;C ϕ B (x (B∩C) ∧ y (B\C) ).</formula><p>Note that we have σ C 0 = ϕ C 0 σ + C 0 at the root. We have the recursion formula</p><formula xml:id="formula_1380">σ + C (x (C) ) = y (V + C \C) C→B        ϕ B (x (B∩C) ∧ y (B\C) ) B ′ &gt;B ϕ B ′ (x (B ′ ∩C) ∧ y (B ′ \C) )        = C→B y (B∪V + B \C) ϕ B (x (B∩C) ∧ y (B\C) ) B ′ &gt;B ϕ B ′ (x (B ′ ∩C) ∧ y (B ′ \C) ) = C→B y (B\C) ϕ B (x (B∩C) ∧ y (B\C) )σ + B (x (B∩C) ∧ y (B\C) ).</formula><p>The inversion between the sum and product in the second equation above was possible because the sets B ∪ V + B \ C, C → B are disjoint. Indeed, if there existed B, B ′ such that C → B and C → B ′ , and descendants C ′ of B ′ and C ′′ of B ′′ with a nonempty intersection, then this intersection would have to be included in every set in the (non-oriented) path connecting C ′ and C ′′ in G. Since this path contains C, the intersection must also be included in C, so that the sets</p><formula xml:id="formula_1381">B ∪ V + B \ C, with C → B are disjoint. Introduce messages m + B (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) )σ + B (x (B∩C) ∧ y (B\C) )</formula><p>where C is the parent of B. Then</p><formula xml:id="formula_1382">m + B (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) ) B→B ′ m + B ′ (x (B∩C) ∧ y (B\C) ) with σ + C (x (C) ) = C→B m + B (x (C) )</formula><p>which provides σ C at the root. Reinterpreting this discussion in terms of the undirected graph, we are led to introducing messages m BC (x (C) ) for B ∼ C in G, with the message-passing rule</p><formula xml:id="formula_1383">m BC (x (C) ) = y (B\C) ϕ B (x (B∩C) ∧ y (B\C) ) B ′ ∼B,B ′ C m B ′ B (x (B∩C) ∧ y (B\C) ). (<label>14.33)</label></formula><p>Messages progressively stabilize when applied in G, and at convergence, we have</p><formula xml:id="formula_1384">σ C (x (C) ) = ϕ C (x (C) ) B∼C m BC (x (C) ). (14.34)</formula><p>Note that the complexity of the junction tree algorithm is exponential in the cardinality of the largest C ∈ S. This algorithm will therefore be unfeasible if S contains sets that are too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6">Building junction trees</head><p>There is more than one family of set interactions with respect to which a given probability π can be decomposed (notice that, unlike in the Hammersley-Clifford Theorem, we do not assume that the interactions are normalized), and not all of them can be organized as a junction tree. One can however extend any given family into a new one on which one can build a junction tree. Definition 14.19 Let V be a set of vertexes, and S 0 ⊂ P (V ). We say that a set S ⊂ P (V ) is an extension of S 0 if, for any C 0 ∈ S 0 , there exists a C ∈ S such that C 0 ⊂ C.</p><p>A tree G = (S, E) is a junction-tree extension of S 0 if S is an extension of S 0 and G is a junction tree.</p><formula xml:id="formula_1385">If Φ 0 = (ϕ 0 C , C ∈ S 0</formula><p>) is a consistent family of set interactions, and S is an extension of S 0 , one can build a new family, Φ = (ϕ C , C ∈ S), of set interactions which yields the same probability distribution, i.e., such that, for all x ∈ F (V ),</p><formula xml:id="formula_1386">C∈S ϕ C (x (C) ) ∝ C 0 ∈S 0 ϕ 0 C 0 (x (C 0 ) ).</formula><p>For this, it suffices to build a mapping say T : S 0 → S such that C 0 ⊂ T (C 0 ) for all C 0 ∈ S 0 , which is always possible since S is an extension of S 0 (for example, arbitrarily order the elements of S and let T (S 0 ) be the first element of S, according to this order, that contains C 0 ). One can then define</p><formula xml:id="formula_1387">ϕ C (x (C) ) = C 0 :T (C 0 )=C ϕ 0 C 0 (x (C 0 ) ).</formula><p>Given Φ 0 , our goal is to design a junction-tree extension which is as feasible as possible. So, we are not interested by the trivial extension G = (V , ∅), since the resulting junction-tree algorithm is unfeasible as soon as V is large. Theorem 14.24 in the next section will be the first step in the design of an algorithm that computes junction trees on a given graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.1">Triangulated graphs</head><p>Definition 14.20 Let G = (V , E) be an undirected graph. Let (s 1 , s 2 , . . . , s n ) be a path in G. One says that this path has a chord at s j , with j ∈ {2, . . . , n} , if s j-1 ∼ s j+1 , and we will refer to (s j-1 , s j , s j+1 ) as a chordal triangle. A path in G is achordal if it has no chord.</p><p>One says that G is triangulated (or chordal) if it has no achordal loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 14.21 The graph G is decomposable if it satisfies the following recursive condition: it is either complete, or there exists disjoint subsets</head><formula xml:id="formula_1388">(A, B, C) of V such that • V = A ∪ B ∪ C, • A and B are not empty, • C is clique in G, C separates A and B,</formula><p>• the restricted graphs, G A∪C and G B∪C are decomposable.</p><p>These definitions are in fact equivalent, as stated in the following proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 14.22 An undirected graph is triangulated if and only if it is decomposable</head><p>Proof To prove the "if" part, we proceed by induction on n = |V |. Note that every graph for n ≤ 3 is both decomposable and triangulated (we leave the verification to the reader). Assume that the statement "decomposable ⇒ triangulated" holds for graphs with less than n vertexes, and take G with n vertexes. Assume that G is decomposable. If it is complete, it is obviously triangulated. Otherwise, there exists A, B, C such that V = A ∪ B ∪ C, with A and B non-empty such that G A∪C and G B∪C are decomposable, hence triangulated from the induction hypothesis, and such that C is a clique which separates A and B. Assume that γ is an achordal loop in G. Since it cannot be included in A ∪ C or B ∪ C, γ must go from A to B and back, which implies that it passes at least twice in C. Since C is complete, the original loop can be shortcut to form subloops in A ∪ C and B ∪ C. If one of (or both) these loops has cardinality 3, this would provide γ with a chord, which contradicts the assumption. Otherwise, the following lemma also provides a contradiction, since one of the two chords that it implies must also be a chord in the original γ. Lemma 14.23 Let (s 1 , . . . , s n , s n+1 = s 1 ) be a loop in a triangulated graph, with n ≥ 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then the path has a chord at two non-contiguous vertexes at least.</head><p>To prove the lemma, assume the contrary and let (s 1 , . . . , s n , s n+1 = s 1 ) be a loop that does not satisfy the condition, with n as small as possible. If n &gt; 4, the loop must have a chord, say at s j , and one can remove s j from the loop to still obtain a smaller loop that must satisfy the condition in the lemma, since n was as small as possible. One of the two chords must be at a vertex other than the two neighbors of s j , and thus provide a second chord in the original loop, which is a contradiction. Thus n = 4, but G being triangulated implies that this 4-point loop has a diagonal, so that the condition in the lemma also holds, which provides a contradiction.</p><p>For the "only if" part of proposition 14.22, assume that G is triangulated. We prove that the graph is decomposable by induction on |G|. The induction will work if we can show that, if G is triangulated, it is either complete or there exists a clique in G such that V \ C is disconnected, i.e., there exist two elements a, b ∈ V \ C which are related by no path in V \C. Indeed, we will then be able to decompose V = A∪B∪C, where A and B are unions of (distinct) connected components of V \ C. Take, for example, A to be the set of vertexes connected to A in G \ C, and B = V \ (A ∪ C), which is not empty since it contains b. Note that restricted graphs from triangulated graphs are triangulated too. So, assume that G is triangulated, and not complete. Let C be a subset of V that satisfies the property that V \ C is disconnected, and take C minimal, so that V \ C ′ is connected for any C ′ ⊂ C, C ′ C. We want to show that C is a clique, so take s and t in C and assume that they are not neighbors to reach a contradiction.</p><p>Let A and B be two connected components of V \ C. For any a ∈ A, b ∈ B, and s, t ∈ C, we know that there exists a path between a and b in V \ C ∪ {s} and another one in V \C ∪{t}, the first one passing by s (because it would otherwise connect a and b in V \ C) and the second one passing by t. Any point before s (or t) in these paths must belong to A, and any point after them must belong to B. Concatenating these two paths, and removing multiple points if needed, we obtain a loop passing in A, then by s, then in B, then by t. We can recursively remove all points at which these paths have a chord. We can also notice that we cannot remove s nor t in this process, since this would imply an edge between A and B, and that we must leave at least one element in A and one in B because removing the last one would require s ∼ t. So, at the end, we obtain an achordal loop with at least four points, which contradicts the fact that G is triangulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We can now characterize graphs that admit junction trees over the set of their maximal cliques. (ii) G is triangulated/decomposable.</p><p>Proof The proof works by induction on the number of maximal cliques, |C * G |. If G has only one maximal clique, then G is complete, because any point not included in this clique will have to be included in another maximal clique, which leads to a contradiction. So G is decomposable, and, since any single node obviously provides a junction tree, (i) is true also. Now, fix G and assume that the theorem is true for any graph with fewer maximal cliques. First assume that C * G has a junction tree, T . Let C 1 be a leaf in T , connected, say, to C 2 , and let T 2 be T restricted to</p><formula xml:id="formula_1389">C 2 = C * G \{C 1 }. Let V 2 be the unions of maximal cliques from nodes in T 2 . A maximal clique C in G V 2 is a clique in G V and therefore included in some maximal clique C ′ ∈ C V . If C ′ ∈ C 2 , then C ′ is also a clique in G V 2 ,</formula><p>and for C to be maximal, we need C = C ′ . If C ′ = C 1 , we note that we must also have</p><formula xml:id="formula_1390">C = C∈C 2 C ∩ C</formula><p>and whenever C ∩ C is not empty, this set must be included in any node in the path in T that links C to C 1 . Since this path contains C 2 , we have</p><formula xml:id="formula_1391">C ∩ C ⊂ C 2 so that C ⊂ C 2 , but, since C is maximal, this would imply that C = C 2 = C 1 which is impossible. This shows that C * G 2 = C 2 .</formula><p>This also shows that T 2 is a junction tree over C 2 . So, by the induction hypothesis, G V 2 is decomposable. If s ∈ V 2 ∩ C 1 , then s also belongs to some clique C ′ ∈ C 2 , and therefore belongs to any clique in the path between</p><formula xml:id="formula_1392">C ′ and C 1 , which includes C 2 . So s ∈ C 1 ∩ C 2 and C 1 ∩ V 2 = C 1 ∩ C 2 . So, letting A = C 1 \ (C 1 ∩ C 2 ), B = V 1 \ (C 1 ∩ C 2 ), S = C 1 ∩ C 2 ,</formula><p>we know that G A∪S and G B∪S are decomposable (the first one being complete), and that S is a clique. To show that G is decomposable, it remains to show that S separates A from B. If a path connects A to B in G, it must contain an edge, say {s, t}, with s ∈ V \S and t ∈ S; {s, t} must be included in a maximal clique in G. If this clique is C 1 , we have s ∈ C 1 ∩ V 2 = S. The same argument shows that this is the only possibility, because, if {s, t} is included in some maximal clique in C 2 , then we would find t ∈ C 1 ∩ C 2 . So S separates A and B in G. Let us now prove the converse statement, and assume that G is decomposable. If G is complete, it has only one maximal clique and we are done. Otherwise, there exists a partition V = A ∪ B ∪ S such that G A∪S and G B∪S are decomposable, A and B separated by S which is complete. Let C * A be the maximal cliques in G A∪S and C * B the maximal cliques in G B∪S . By hypothesis, there exist junction trees T A and T B over C * A and C * B . Let C be a maximal clique in G A∪S . Assume that C intersect A; C can be extended to a maximal clique, C ′ , in G, but C ′ cannot intersect B (since this would imply a direct edge between A and B) and is therefore included in A ∪ S, so that C = C ′ . Similarly, all maximal cliques in G B∪S that intersect B also are maximal cliques in G. The clique S is included in some maximal clique S * A ∈ C * A . From the previous discussion, we have either S * A = S or S * A ∈ C * G . Similarly, S can be extended to a maximal clique S * B ∈ C * B , with S * B = S or S * B ∈ C * G . Notice also that at least one of S * A</p><p>or S * B must be a maximal clique in G: indeed, assume that both sets are equal to S, which, as a clique, can extended to a maximal clique S * in G; S * must be included either in A ∪ S or in B ∪ S, and therefore be a maximal clique in the corresponding graph which yields S * = S. Reversing the notation if needed, we will assume that</p><formula xml:id="formula_1393">S * A ∈ C * G .</formula><p>All elements of C * G must belong either to C * A or C * B since any maximal clique, say C, in G must be included in either A ∪ S or B ∪ S, and therefore also provide a maximal clique in the related graph. So the nodes in T A and T B enumerate all maximal cliques in G, and we can build a tree T over C *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G by identifying S *</head><p>A and S * B to S * and merging the two trees at this node. To conclude our proof, it only remains to show that the running intersection property is satisfied. So consider two nodes C, C ′ in T and take s ∈ C ∩ C ′ . If the path between these nodes remain in C * A , or in C * B , then s will belong to any set along that path, since the running intersection is true on T A and T B . Otherwise, we must have s ∈ S, and the path must contain S * to switch trees, and s must still belong to any clique in the path (applying the running intersection property between the beginning of the path and S * , and between S * and the end of the path).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This theorem delineates a strategy in order to build a junction tree that is adapted to a given family of local interactions Φ = (ϕ C , C ∈ C). Letting G be the graph induced by these interactions, i.e., s ∼ G t if and only if there exists C ∈ C such that {s, t} ⊂ C, the method proceeds as follows. Steps (JT4) and (JT5) have already been discussed, and we now explain how the first three steps can be implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.2">Building triangulated graphs</head><p>First consider step (JT1). To triangulate a graph G = (V , E), it suffices to order its vertexes so that V = {s 1 , . . . , s n }, and then run the following algorithm. • Add an edge to any pair of neighbors of s k (unless, of course, they are already linked).</p><p>• Let E k-1 be the new set of edges.</p><p>Then the graph G * = (V , E 0 ) is triangulated. Indeed taking any achordal loop, and selecting the vertex with highest index in the loop, say s k , brings a contradiction, since the neighbors of s k have been linked when building E k-1 .</p><p>However, the quality of the triangulation, which can be measured by the number of added edges, or by the size of the maximal cliques, highly depends on the way vertexes have been numbered. Take the simple example of the linear graph with three vertexes A ∼ B ∼ C. If the point of highest index is B, then the previous algorithm will return the three-point loop A ∼ B ∼ C ∼ A. Any other ordering will leave the linear graph, which is already triangulated, invariant.</p><p>So, one must be careful about the order with which nodes will be processed. Finding an optimal ordering for a given global cost is an NP-complete problem. However, a very simple modification of the previous algorithm, which starts with s n having the minimal number of neighbors, and at each step defines s k to be the one with fewest neighbors that haven't been visited yet, provides an efficient way for building triangulations. (It has the merit of leaving G invariant if it is a tree, for example). Another criterion may be preferred to the number of neighbors (for example, the number of new edges that would be needed if s is added).</p><p>If G is triangulated, there exists an ordering of V such that the algorithm above leaves G invariant. We now proceed to a proof of this statement and also show that such an ordering can be computed using an algorithm called maximum cardinality search, which, in addition, allows one to decide whether a graph is triangulated. We start with a definition that formalizes the sequence of operations in the triangulation algorithm. Definition 14.25 Let G = (V , E) be an undirected graph. A node elimination consists in selecting a vertex s ∈ V and building the graph G (s) = (V (s) , E (s) ) with V (s) = V \ {s}, and E (s) containing all pairs {t, t ′ } ⊂ V (s) such that either {t, t ′ } ∈ E or {t, t ′ } ⊂ V s . G (s) is called the s-elimination graph of G. The set of added edges, namely E (s) \(E∩E (s) ) is called the deficiency set of s and denoted D(s) (or D G (s)).</p><p>So, the triangulation algorithm implements a sequence of node eliminations, successively applied to s n , s n-1 , etc. One says that such an elimination process is perfect if, for all k = 1, . . . , n, the deficiency set of s k in the graph obtained after elimination of s n , . . . , s k+1 is empty (so that no edge is added during the process). We will also say that (s 1 , . . . , s n ) provides a perfect ordering for G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 14.26 An undirected graph G = (V , E) admits a perfect ordering if and only if it is triangulated.</head><p>Proof The "only if" part is obvious, since, the triangulation algorithm following a perfect ordering does not add any edge to G, which must therefore have been triangulated to start with. We now proceed to the "if" part. For this it suffices to prove that for any triangulated graph, there exists a vertex s such that D G (s) = ∅. One can then easily prove the result by induction, since, after removing this s, the remaining graph G (s) is still triangulated and would admit (by induction) a perfect ordering that completes this first step.</p><p>To prove that such an s exists, we take a decomposition V = A ∪ S ∪ B, in which S is complete and separates A and B, such that |A ∪ S| is minimal (or |B| maximal). We claim that A ∪ S must be complete. Otherwise, since A ∪ S is still triangulated, There exists a similar decomposition A ∪ S = A ′ ∪ S ′ ∪ B ′ . One cannot have S ∩ A ′ and S ∩ B ′ non empty simultaneously, since this would imply a direct edge from A ′ to B ′ (S is complete). Say that S ∩ A ′ = ∅, so that A ′ ⊂ A. Then the decomposition</p><formula xml:id="formula_1394">V = A ′ ∪ S ′ ∪ (B ′ ∪ B) is such that S ′ separates A ′ from B ∪ B ′ . Indeed, a path from A ′ to b ∈ B ∪ B ′ must pass in S ′ if b ∈ B ′ ,</formula><p>and, if b ∈ B, it must pass in S (since it links A and B). But S ⊂ S ′ ∪ B ′ so that the path must intersect S ′ . We therefore obtain a decomposition that enlarges B, which is a contradiction and shows that A ∪ S is complete. Given this, any element s ∈ A can only have neighbors in A ∪ S and is therefore such that D G (s) = ∅, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>If a graph is triangulated, there is more than one perfect ordering of its vertexes. One of these orderings is provided the maximum cardinality search algorithm, which also allows one to decide whether the graph is triangulated. We start with a definition/notation. Definition 14.27 If G = (V , E) is an undirected graph, with |V | = n, any ordering V = (s 1 , . . . , s n ) can be identified with the bijection α : V → {1, . . . , n} defined by α(s k ) = k. In other terms, α(s) is the rank of s in the ordering. We will refer to α as an ordering, too.</p><p>Given an ordering α, we define incremental neighborhoods V α,k s , for s ∈ V and k = 1, . . . , n to be the intersections of V s with the sets α -1 ({1, . . . , k}), i.e.,</p><formula xml:id="formula_1395">V α,k s = {t ∈ V , t ∼ s, α(t) ≤ k} .</formula><p>One says that α satisfies the maximum cardinality property if, for all k = {2, . . . , n}</p><formula xml:id="formula_1396">|V α,k-1 s k | = max α(s)≥k |V α,k-1 s |. (14.35)</formula><p>where</p><formula xml:id="formula_1397">s k = α -1 (k).</formula><p>Given this, we have the proposition:</p><formula xml:id="formula_1398">Proposition 14.28 If G = (V , E</formula><p>) is triangulated, then any ordering that satisfies the maximum cardinality property is perfect.</p><p>Equation (14.35) immediately provides an algorithm that constructs an ordering satisfying the maximum cardinality property given a graph G. From proposition 14.28, we see that, if for some k, the largest set V α,k-1 s k is not a clique, then G is not triangulated. We now proceed to the proof of this proposition.</p><p>Proof Let G be triangulated, and assume that α is an ordering that satisfies <ref type="bibr">(14.35)</ref>. Assume that α is not proper in order to reach a contradiction.</p><p>Let k be the first index for which V α,k-1 s k is not a clique, so that s k has two neighbors, say t and u, such that α(t) &lt; k, α(u) &lt; k and t ≁ u. Assume that α(t) &gt; α(u). Then t must have a neighbor that is not neighbor of s, say t ′ , such that α(t ′ ) &lt; α(t) (otherwise, s would have more neighbors than t at order less than α(t), which contradicts the maximum cardinality property). The sequence t ′ , t, s, u forms a path that is such that α increases from t ′ to s, then decreases from s to u, and contains no chord. Moreover, t ′ and u cannot be neighbors, since this would yield an achordal loop and a contradiction. The proof of proposition 14.28 consists in showing that this construction can be iterated until a contradiction is reached.</p><p>More precisely, assume that an achordal path s 1 , . . . , s k has been obtained, such that α(s) is first increasing, then decreasing along the path, and such that, at extremities one either has α(s 1 ) &lt; α(s k ) &lt; α(s 2 ) or α(s k ) &lt; α(s 1 ) &lt; α(s k-1 ). In fact, one can switch between these last two cases by reordering the path backwards. Both paths (u, s, t) and (u, s, t, t ′ ) in the discussion above satisfy this property.</p><p>• Assume, without loss of generality, that α(s 1 ) &lt; α(s k ) &lt; α(s 2 ) and note that, in the considered path, s 1 and s k cannot be neighbors (for, if j is the last index smaller than k -1 such that s j and s k are neighbors, then j must also be smaller than k -2 and the loop s j , . . . , s k-1 , s k would be achordal).</p><p>• Since α(s 2 ) &gt; α(s k ), and s 1 and s 2 are neighbors, s k must have a neighbor, say s ′ k , such that s ′ k is not neighbor of s 2 and α(s ′ k ) &lt; α(s k ). • Select the first index j &gt; 2 such that s j ∼ s ′ k , and consider the path (s 1 , . . . , s j , s ′ k ). This path is achordal, by construction, and one cannot have s 1 ∼ s ′ k since this would create an achordal loop. Let us show that α first increases and then decreases along this path. Since s 2 is in the path, α must first increase, and it suffices to show that α(s ′ k ) &lt; α(s j ). If α increases from s 1 to s j , then α(s j ) &gt; α(s 2 ) &gt; α(s k ) &gt; α(s ′ k ). If α started decreasing at some point before s j , then α(s j ) &gt; α(s k ) &gt; α(s ′ k ). • Finally, we need to show that the α-value at one extremity is between the first two α-values on the other end of the path. If α(s ′ k ) &lt; α(s 1 ), and since we have just seen that α(s j ) &gt; α(s k ) &gt; α(s 1 ), we do get α(s</p><formula xml:id="formula_1399">′ k ) &lt; α(s 1 ) &lt; α(s j ). If α(s ′ k ) &gt; α(s 1 ), then, since by construction α(s 2 ) &gt; α(s k ) &gt; α(s ′ k ), we have α(s 2 ) &gt; α(s ′ k ) &gt; α(s 1</formula><p>). • So, we have obtained a new path that satisfies the same property that the one we started with, but with a maximum value at end points smaller than the initial one, i.e., max(α(s 1 ), α(s ′ k )) &lt; max(α(s 1 ), α(s k )). Since α takes a finite number of values, this process cannot be iterated indefinitely, which yields our contradiction. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.3">Computing maximal cliques</head><p>At this point, we know that a graph must be triangulated for its maximal cliques to admit junction trees, and we have an algorithm to decide whether a graph is triangulated, and extend it into a triangulated one if needed. This provides the first step, (JT1), of our description of the junction tree algorithm. The next step, (JT2), requires computing a list of maximal cliques. Computing maximal cliques in general graph is an NP complete problem, for which a large number of algorithms has been developed (see, for example, <ref type="bibr" target="#b167">[149]</ref> for a review). For graphs with a perfect ordering, however, this problem can always be solved in a polynomial time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indeed, assume that a perfect ordering is given for</head><formula xml:id="formula_1400">G = (V , E), so that V = {s 1 , . . . , s n } is such that, for all k, V ′ s k := V s k ∩ {s 1 , . . . , s k-1 } is a clique. Let G k be G restricted to {s 1 , . . . , s k } and C * k be the set of maximal cliques in G k . Then the set C k := {s k } ∪ V ′</formula><p>s k is the only maximal clique in G k that contains s k : it is a clique because the ordering is perfect, and any clique that contains s k must be included in it (because its elements are either s k or neighbors of s k ). It follows from this that the set C * k can be deduced from</p><formula xml:id="formula_1401">C * k-1 by        C * k = C * k-1 ∪ {C k } if V ′ k C * k-1 C * k = (C * k-1 ∪ {C k }) \ {V ′ k } if V ′ k ∈ C * k-1</formula><p>This allows one to enumerate all elements in C * G = C * n , starting with C * 1 = {{s 1 }}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="14.6.4">Characterization of junction trees</head><p>We now discuss the last remaining point, (JT3</p><p>). For this, we need to form the clique graph of G, which is the undirected graph G = (C * G , E) defined by (C, C ′ ) ∈ E if and only if C ∩ C ′ ∅. We then have the following fact: Proposition 14.29 The clique graph G of a connected triangulated undirected graph G is connected. Proof We proceed by induction, and assume that the result is true if |V | = n -1 (the proposition obviously holds if |V | = 1). Assume that a perfect order on G has been chosen, say V = {s 1 , . . . , s n }. Let G ′ be G restricted to {s 1 , . . . , s n-1 }, and G ′ the associated clique graph. Because {s n } ∪ V s n is a clique, any path in G provides a valid path in G ′ after removing all occurrences of s n (because any two neighbors of s n are linked). The induction hypothesis also implies that G ′ is connected. Since G is connected, V s n is not empty. Moreover, C := {s n } ∪ V s n must be a maximal clique in G (since we assume that the order is perfect) and it is the only maximal clique in G that contains s n (all other maximal cliques in G therefore are maximal cliques in G ′ also). To prove that G is connected, it suffices to prove that C is connected to any other maximal clique, C ′ , in G by a path in G. If t ∈ C, t s n , there exists a maximal clique, say C ′′ , in G ′ that contains t, and, since G ′ is connected, there exists a path (C 1</p><formula xml:id="formula_1402">= C ′ , . . . , C q = C ′′ ) connecting C ′ to C ′′ in G ′ .</formula><p>Let j be the first integer such that C j = V n (take j = q + 1 if this never happens). Then (C 1 , . . . , C j-1 , C) is a path linking C ′ and C in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We hereafter assume that G, and hence G, is connected. This is not real loss of generality because connected components in undirected graphs yields independent processes that can be handled separately. We assign weights to edges of the clique graph of G by defining w(C, C ′ ) = |C ∩ C ′ |. A subgraph T of any given graph G is called a spanning tree if T is a tree with set of vertexes equal to the set of vertexes of G. If T = (C * G , E ′ ) is a spaning tree of G, we define the total weight</p><formula xml:id="formula_1403">w(T ) = {C,C ′ }∈E ′ w(C, C ′ ).</formula><p>We then have the proposition: Proposition 14.30 <ref type="bibr" target="#b117">[99]</ref> If G is a connected triangulated graph, the set of junction trees over C * G coincides with the set of maximizers of w(T ) over all spanning trees of G.</p><p>(Notice that G being connected implies that spanning trees over G exist.)</p><p>Before proving this proposition, we discuss some properties related to maximal (or maximum-weight) spanning trees over an undirected graph. For this discussion, we let G = (V , E) be any undirected graph with weight (w(e), e ∈ E). We will then apply these results to a clique graph when will switch back to the general notation of this section. Maximal spanning trees can be computed using the so-called Prim's algorithm <ref type="bibr" target="#b116">[98,</ref><ref type="bibr" target="#b173">155,</ref><ref type="bibr" target="#b81">63]</ref>. Algorithm 14.7 (Prim's algorithm) Initialize the algorithm with a single-node tree T 1 = ({s 1 }, ∅), for some arbitrary</p><formula xml:id="formula_1404">s 1 ∈ V . Let T k-1 = (V k-1 , E k-1</formula><p>) be the tree obtained at step k -1 of the algorithm. If k ≤ n, the next tree is built as follows.</p><p>(1) Let</p><formula xml:id="formula_1405">V k = {s k } ∪ V k-1 (s k V k-1 .) (2) Let E k = {e k } ∪ E k-1 , such that e k = {s k , s} for some s ∈ V k-1 satisfying w(e k ) = max w({t, t ′ }), {t, t ′ } ∈ E, t V k-1 , t ′ ∈ V k-1 . (14.36)</formula><p>The ability of this algorithm to always build a maximal spanning tree is summarized in the following proposition <ref type="bibr" target="#b99">[81,</ref><ref type="bibr" target="#b147">129]</ref>. <ref type="figure">,</ref> <ref type="figure">E,</ref> <ref type="figure">w</ref>) is a weighted, connected undirected graph, Prim's algorithm, as described above, provides a sequence T k = (V k , E k ), for k = 1, . . . , n of subtrees of G such that V n = V and, for all k, T k is a maximal spanning tree for the restriction</p><formula xml:id="formula_1406">Proposition 14.31 If G = (V</formula><formula xml:id="formula_1407">G V k of G to V k .</formula><p>Moreover, any maximal spanning tree of G, can be realized as T n , where (T 1 , . . . , T n ) is a sequence provided by Prim's algorithm.</p><p>Proof We first prove that, for all k, T k is a maximal spanning tree on the graph G V k .</p><p>We will prove a slightly stronger statement, namely, that, for all k, T k can be extended to form a maximal spanning tree of G. This is stronger, because, if T k = (V k , E k ) can be extended to a maximal spanning tree T = (V , E), and if</p><formula xml:id="formula_1408">T ′ k = (V k , E ′ k ) is a spanning tree for G V k such that w(T k ) &lt; w(T ′ k ), then the graph T ′ = (V , E ′ ) with E ′ = (E \ E k ) ∪ E ′ k</formula><p>would be a spanning tree for G with w(T ) &lt; w(T ′ ), which is impossible. (To see that T ′ is a tree, notice that paths in T ′ are in one-to-one correspondence with paths in T by replacing any subpath within T ′ k by the unique subpath in T k that has the same extremities.) Clearly, T 1 , which only has one vertex, can be extended to a maximal spanning tree. Let k ≥ 1 be the last integer for which this property is true for all j = 1, . . . , k. If k = n, we are done. Otherwise, take a maximum spanning tree, T , that extends T k . This tree cannot contain the new edge added when building T k+1 , namely e k+1 = {s k+1 , s} as defined in Prim's algorithm, since it would otherwise also extend T k+1 . Consider the path γ in T that links s to s k . This path must have an edge e = {t, t ′ } such that t ∈ V k and t ′ V k , and by definition of e k+1 , we must have w(e) ≤ w(e k+1 ). Notice that e is uniquely defined, because a path leaving V k cannot return in this set, since one would be otherwise able to close it into a loop by inserting the only path in T k that connects its extremities.</p><p>Replace e by e k+1 in T . The resulting graph, say T ′ , is still a spanning tree for G. From any path in T , one can create a path in T ′ with the same extremities by replacing any occurrence of the edge, e, by the concatenation of the unique path in T going from t to s, followed by (s, s k+1 ), followed by the unique path in T going from s k+1 to t ′ . This implies that T ′ is connected. It is also acyclic, since any loop in T would have to contain e k+1 (since T is acyclic), but there is no other path than (s, s k+1 ) in T ′ that links s and s k , because this path would have to be in T , and we have removed the only possible one from T by deleting the edge e.</p><p>As a conclusion, T ′ is an extension of T k+1 , and a spanning tree with total weight larger or equal to the one of T , and must therefore be optimal, too. But this contradicts the fact that T k+1 cannot be extended to a maximal tree, so that k = n and the sequence of trees provided by Prim's algorithm is optimal.</p><p>To prove the second statement, let T be an optimal spanning tree. Let k be the largest integer such that there exists a sequence (T 1 , . . . , T k ) generated by Prim's algorithm, such that, for all j = 1, . . . , k, T j is a subtree of T . One necessarily has j ≥ 1, since T extends any one-vertex tree. If k = n, we are done. Assuming otherwise, let T k = (V k , E k ) and make one more step of Prim's algorithm, selecting an edge e k+1 = (s k+1 , s) satisfying <ref type="bibr">(14.36)</ref>. By assumption, e k+1 is not in T . Take as before the unique path linking s and s k+1 in T and let e be the unique edge at which this path leaves V k . Replacing e by e k+1 in T provides a new spanning tree, T ′ . One must have w(e) ≥ w(e k+1 ) because T is optimal, and w(e k+1 ) ≥ w(e) by <ref type="bibr">(14.36)</ref>. So w(e) = w(e k+1 ), and one can use e instead of e k+1 for the (k + 1)th step of Prim's algorithm. But this contradicts the fact that k was the largest integer in a sequence of subtrees of T that is generated by Prim's algorithm, and one therefore has k = n. ■ The proof of proposition 14.30, that we provide now, uses very similar "edgeswitching" arguments.</p><p>Proof (Proof of proposition 14.30) Let us start with a maximum weight spanning tree for G, say T , and show that it is a junction tree. Since T has maximum weight, we know that it can be obtained via Prim's algorithm, and that there exists a sequence T 1 , . . . , T n = T of trees constructed by this algorithm. Let T k = (C k , E k ).</p><p>We proceed by contradiction. Let k be the largest index such that T k can be extended to a junction tree for C * G , and let T ′ be a junction tree extension of T k . Assume that k &lt; n, and let e k+1 = (C k+1 , C ′ ) be the edge that has been added when building T k+1 , with C k+1 = {C k+1 } ∪ C k . This edge is not in T ′ , so that there exists a unique edge e = (B, B ′ ) in the path between</p><formula xml:id="formula_1409">C k and C ′ in T ′ such that B ∈ C k and B ′ C k . We must have w(e) = |B ∩ B ′ | ≤ w(e k+1 ) = |C k+1 ∩ C ′ |.</formula><p>But, since the running intersection property is true for T ′ , both B and B ′ must contain C k+1 ∩C ′ so that B∩B ′ = C k+1 ∩C ′ . This implies that, if one modifies T ′ by replacing edge e by edge e k+1 , yielding a new spanning tree T ′′ , the running intersection property is still satisfied in T ′ . Indeed if a vertex s ∈ V belongs to both extremities of a path containing B and B ′ in T ′ , then it must belong to B ∩ B ′ , and hence to C k+1 ∩ C ′ , and therefore to any set in the path in T ′ that linked C k+1 and C ′ . So we found a junction tree extension of T k+1 , which contradicts our assumption that k was the largest. We must therefore have k = n and T is a junction tree.</p><p>Let us now consider the converse statement and assume that T is a junction tree. Let k be the largest integer such that there exists a sequence of subgraphs of T that is provided by Prim's algorithm. Denote such a sequence by (T 1 , . . . , T k ), with T j = (C j , E j ). Assume (to get a contradiction) that k &lt; n, and consider a new step for This implies that adding e instead of e k+1 at step k + 1 is a valid choice for Prim's algorithm, and contradicts the fact that k was the largest number of such steps that could provide a subtree of T . So k = n and T is maximal. We first introduce some notation. Let G = (V , E) be a directed acyclic graph. The parents of s ∈ V are vertexes t such that (t, s) ∈ E, and its children are t's such that (s, t) ∈ E. The set of parents of s is denoted pa(s), and the set of its children is ch(s), with V s = ch(s) ∪ pa(s).</p><p>Similarly to trees, the vertexes of G can be partially ordered by s ≤ G t if and only if there exists a path going from s to t. Unlike trees, however, there can be more than one minimal element in V , and we still call roots vertexes that have no parent, denoting V 0 = {s ∈ V : pa(s) = ∅} .</p><p>We also call leaves, or terminal nodes, vertexes that have no children. Unless otherwise specified, we assume that all graphs are connected.</p><p>Bayesian networks over G are defined as follows. We use the same notation as with Markov random fields to represent the set of configurations F (V ) that contains collections x = (x s , s ∈ V ) with x s ∈ F s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 15.1 A random variable X with values in F (V ) is a Bayesian network over a DAG G = (V , E) if and only if its distribution can be written in the form</head><formula xml:id="formula_1410">P X (x) = s∈V 0 p s (x (s) ) s∈V \V 0 p s (x (pa(s)) , x (s) ) (15.1)</formula><p>where p s is, for all s ∈ V , a probability distribution with respect to x (s) .</p><p>Using the convention that conditional distributions given the empty set are just absolute distributions, we can rewrite (15.1) as</p><formula xml:id="formula_1411">P X (x) = s∈V p s (x (pa(s)) , x (s) ). (15.2)</formula><p>One can verify that x∈Ω P X (x) = 1. Indeed, when summing over x, we can start summing over all x (s) with ch(s) = ∅ (the leaves). Such x (s) 's only appear in the corresponding p s 's, which disappear since they sum to 1. What remains is the sum of the product over V minus the leaves, and the argument can be iterated until the remaining sum is 1 (alternatively, work by induction on |V |). This fact is also a consequence of proposition 15.5 below, applied with A = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2">Conditional independence graph 15.2.1 Moral graph</head><p>Bayesian networks have a conditional independence structure which is not exactly given by G, but can be deduced from it. Indeed, fixing S ⊂ V , we can see, when computing the probability of X (S) = x (s) given X (S c ) = x (S c ) , which is</p><formula xml:id="formula_1412">P(X (S) = x (S) | X (S c ) = x (S c ) ) = 1 Z(x (S c ) ) s∈V p s (x (pa(s)) , x (s) ),</formula><p>that the only variables x (t) , t S that can be factorized in the normalizing constant are those that are neither parent nor children of vertexes in S, and do not share a child with a vertex in S (i.e., they intervene in no p s (x (pa(s)) , x (s) ) that involve elements of S). This suggests the following definition.</p><p>Definition 15.2 Let G be a directed acyclic graph. We denote G ♯ = (V , E ♯ ) the undirected graph on V such that {s, t} ∈ E ♯ if one of the following conditions is satisfied</p><formula xml:id="formula_1413">• Either (s, t) ∈ E or (t, s) ∈ E. • There exists u ∈ V such that (s, u) ∈ E and (t, u) ∈ E.</formula><p>G ♯ is sometimes called the moral graph of G (because it forces parents to marry !). A path in G ♯ can be visualized as a path in G ♭ (the undirected graph associated with G) which is allowed to jump between parents of the same vertex even if they were not connected originally.</p><p>The previous discussion implies:</p><p>Proposition 15.3 Let X be a Bayesian network on G. We have</p><formula xml:id="formula_1414">(S T | U ) G ♯ ⇒ (X (S) X (T ) | X (U ) ), i.e., X is G ♯ -Markov.</formula><p>This proposition can be refined by noticing that the joint distribution of X (S) , X (T ) and X (U ) can be deduced from a Bayesian network on a graph restricted to the ancestors of S ∪T ∪U . Definition 13.21 for restricted graphs extends without change to directed graphs, and we repeat it below for convenience. Definition 15.4 Let G = (V , E) be a graph (directed or undirected), and A ⊂ V . The restricted graph G A = (A, E A ) is such that the elements of E A are the edges (s, t) (or {s, t}) in E such that both s and t belong to A.</p><p>Moreover, for a directed acyclic graph G and s ∈ V , we define the set of ancestors of s by</p><formula xml:id="formula_1415">A s = {t ∈ V , t ≤ G s} (15.3)</formula><p>for the partial order on V induced by G.</p><p>If S ⊂ V , we denote A S = s∈S A s . Note that, by definition, S ⊂ A S . The following proposition is true. Proposition 15.5 Let X be a Bayesian network on G = (V , E) with distribution given by (15.2). Let S ⊂ V and A = A S . Then the distribution of X (A) is a Bayesian network over G A given by P(X (A) = x (A) ) = s∈A p s (x (pa(s)) , x (s) ). <ref type="bibr">(15.4)</ref> There is no ambiguity in the notation pa(s), since the parents of s ∈ A are the same in G A as in G.</p><p>Proof One needs to show that s∈A p s (x (pa(s)) , x (s) ) =</p><p>x A c s∈V p s (x (pa(s)) , x (s) ).</p><p>This can be done by induction on the cardinality of V . Assume that the result is true for graphs of size n, and let |V | = n + 1 (the result is obvious for graphs of size 1).</p><p>If A = V , there is nothing to prove, so assume that A c is not empty. Then A c must contain a leaf in G, since otherwise, A would contain all leaves and their ancestors which would imply that A = V .</p><p>If s ∈ A c is a leaf in G, one can remove the variable x (s) from the sum, since it only appear in p s and transition probabilities sum to one. But one can now apply the induction assumption to the restriction of G to V \ {s}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>Step 1. We first note that the v-junction clause is redundant in <ref type="bibr" target="#b20">(2)</ref>. It can be removed without affecting the condition. Indeed, if a path in G ♭ passes in V \ A {s,t}∪U one can follow this path downward (i.e., following the orientation in G) until a v-junction is met. This has to happen before reaching the extremities of the path, since u would be an ancestor of s or t otherwise. We can therefore work with the weaker condition (that we will denote (2)') in the rest of proof.</p><p>Step 2. Assume that U separates s and t in (G A {s,t}∪U ) ♯ . Take a path γ between s and t in G ♭ . We need to show that the path satisfies (1) or ( <ref type="formula">2</ref>)'. So assume that (2)' is false (otherwise we are done) so that γ is included in A {s,t}∪U . We can modify γ by removing all the central nodes in v-junctions and still keep a valid path in (G A {s,t}∪U ) ♯ (since parents are connected in the moral graph). The remaining path must intersect U by assumption, and this cannot be at a v-junction in γ since we have removed them. So ( <ref type="formula" target="#formula_13">1</ref>) is true.</p><p>Step 3. Conversely, assume that (1) or ( <ref type="formula">2</ref>) is true for any path in G ♭ . Consider a path γ in (G A {s,t}∪U ) ♯ between s and t. Any edge in γ that is not in G ♭ must involve parents of a common child in A {s,t}∪U . Insert this child between the parents every time this occurs, resulting in a v-junction added to γ. Since the added vertexes are still in A {s,t}∪U , the new path still has no intersection with V \ A {s,t}∪U and must therefore satisfy <ref type="bibr" target="#b19">(1)</ref>. So there must be an intersection with U without a v-junction, and since the new additions are all at v-junctions, the intersection must have been originally in γ, which therefore passes in U . This shows that U separates s and t in (G A {s,t}∪U ) ♯ . ■ Condition (2) can be further restricted to provide the notion of d-separation. Proof It suffices to show that if condition ((D1) or (D2)) holds for any path between s and t in G ♭ , then so does ((1) or ( <ref type="formula">2</ref>)). So take a path between s and t: if (D1) is true for this path, the conclusion is obvious, since (D1) and ( <ref type="formula" target="#formula_13">1</ref>) are the same. So assume that (D1) (and therefore (1)) is false and that (D2) is true. Let u be a vertex in V \ A U at which γ passes with a v-junction.</p><p>Assume that (2) is false. Then u must be an ancestor of either s or t. Say it is an ancestor of s: there is a path in G going from u to s without passing by U (otherwise u would be an ancestor of U ); one can replace the portion of the old path between s and u by this new one, which does not pass by u with a v-junction anymore. So the new path still does not satisfy (D1) and must satisfy (D2). Keep on removing all intersections with ancestors of s and t that have v-junctions to finally obtain a path that satisfies neither (D1) or (D2) and a contradiction to the fact that s and t are d-separated by U . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2.3">Chain-graph representation</head><p>The d-separability property involves both unoriented and oriented edges. It is in fact a property of the hybrid graph in which the orientation is removed from the edges that are not involved in a v-junction, and retained otherwise. Such graphs are particular instances of chain graphs. Definition 15.12 A chain graph G = (V , E, Ẽ) is composed with a finite set V of vertexes, a set E ⊂ P 2 (V ) of unoriented edges and a set Ẽ ⊂ E × E \ {(t, t), t ∈ E} of oriented edges with the property that E ∩ Ẽ♭ = ∅, i.e., two vertexes cannot be linked by both an oriented and an unoriented edge.</p><p>A path in a chain graph is a a sequence of vertexes s 0 , . . . , s N such that for all k ≥ 1, s k-1 and s k form an edge, which means that either</p><formula xml:id="formula_1416">{s k-1 , s k } ∈ E or (s k-1 , s k ) ∈ Ẽ.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A chain graph is acyclic if it contains no loop. It is semi-acyclic if it contains no loop containing oriented edges.</head><p>We start with the following equivalence relation within vertexes in a semi-acyclic chain graph. Proposition 15.13 Let G = (V , E, Ẽ) be a semi-acyclic chain graph. Define the relation s R t if and only if there exists a path in the unoriented subgraph (V , E) that links s and t. Then R is an equivalence relation.</p><p>The proposition is obvious. This relation partitions V in equivalence classes, the set of which being denoted V R . If S ∈ V R , then any pair s, t in S is related by an unoriented path, and if S S ′ ∈ V R , no elements s ∈ S and t ∈ S ′ can be related by such a path. Moreover, no path in G between two elements of S ∈ V R , can contain a directed edge, since these elements must also be related by an undirected path, and this would create a loop in G containing an undirected edge. So the restriction of G to S is an undirected graph.</p><p>One can define a directed graph over equivalence classes as follows. Let G R = (V R , E R ) be such that (S, S ′ ) ∈ E R if and only if there exists s ∈ S and t ∈ S ′ such that (s, t) ∈ Ẽ. The graph G R is acyclic: any loop in G R would induce a loop in G containing at least one oriented edge.</p><p>We now can formally define a probability distribution on a semi-acyclic chain graph.</p><p>Definition 15.14 Let G = (V , E, Ẽ) be a semi-acyclic chain graph. One says that a random variable X decomposes on G if and only if: (X (S) , S ∈ V R ) is a Bayesian network on G R and the conditional distribution of X (S) given X (S ′ ) , S ′ ∈ pa(S) is G S -Markov, such that, for s ∈ S, P (X (s) = x (s) | X (t) , t ∈ S, X S ′ , S ′ ∈ pa(S)) only depends on x (t) with {s, t} ∈ E or (t, s) ∈ Ẽ.</p><p>Returning to our discussion on Bayesian networks, we have the following. Associate to a DAG G = (V , E) the chain graph G † = (V , E † , Ẽ † ) defined by: {s, t} ∈ E † if and only if (s, t) or (t, s) ∈ E and is not involved in a v-junction, and (s, t) ∈ Ẽ † if (s, t) ∈ E and is involved in a v-junction. This graph is acyclic; indeed, take any loop in G † : when its edges are given their original orientations in E, the sequence cannot contain a v-junction, since the orientation in v-junctions are kept in G † ; the path therefore constitutes a loop in G which is a contradiction.</p><p>All, excepted at most one, vertexes in an equivalence class S ∈ G † R have all their parents in S. Indeed, assume that two vertexes, s and t, in S have parents outside of S. There exists an unoriented path, s 0 = s, s 1 , . . . , s N = t, in G † connecting them, since they belong to the same equivalence class. The edge at s must be oriented from s to s 1 in G, since otherwise s 1 would be a second parent to s in G, creating a v-junction, and the edge would have remained oriented in G † . Similarly, the last edge in the path must be oriented from t to s N -1 in G. But this implies that there exists a vjunction in the original orientation along the path, which cannot be constituted with only unoriented edges in G † . So we get a contradiction. Thus, random variables that decompose on G † are "Bayesian networks" of acyclic graphs, or trees since we know these are equivalent. The root of each tree must have multiple (vertex) parents in the parent tree in G R . The following theorem states that all Bayesian networks are equivalent to such a process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 15.15 Let G = (V , E) be a DAG. The random variable X is a Bayesian network on G if and only if it decomposes over G † .</head><p>Proof Assume that X is a Bayesian network on G. We can obviously rewrite the probability distribution of X in the form</p><formula xml:id="formula_1417">π(x) = S∈G † R s∈S p s (x (pa(s)) , x (s) ).</formula><p>Since every vertex in S has its parents in S or in T ∈pa(S) T , this a fortiori takes the form</p><formula xml:id="formula_1418">π(x) = S∈G † R p S ((x (T ) , T ∈ S -), x (s) ). So X (S) , S ∈ V R is a Bayesian network. Moreover, p S ((x (T ) , T ∈ S -), x (s) ) = s∈S p s (x (pa(s)) , x (s) )</formula><p>is a tree distribution with the required form of the individual conditional distributions.</p><p>Now assume that X decomposes on G † . Then the conditional distribution of X (S) given X (T ) , T ∈ pa(S) is Markov for the acyclic undirected graph G S , and can therefore be expressed as a tree distribution consistent with the orientation of G. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2.4">Markov equivalence</head><p>While the previous discussion provides a rather simple description of Bayesian networks in terms of chain graphs, it does not go all the way in reducing the number of oriented edges in the definition of a Bayesian network. The issue is, in some way, addressed by the notion of Markov equivalence, which is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 15.16 Two directed acyclic graphs on the same set of vertexes G = (V , E) and G = (V , Ẽ) are Markov-equivalent if any family of random variables that decomposes as a (positive) Bayesian network over one of them also decomposes as a Bayesian network over the other.</head><p>The notion of Markov equivalence is exactly described by d-separation. This is stated in the following theorem, due to Geiger and Pearl <ref type="bibr" target="#b95">[77,</ref><ref type="bibr" target="#b94">76]</ref>, that we state without proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 15.17 G and G are Markov equivalent if and only if, whenever two vertexes are d-separated by a set in one of them, the same separation is true with the other.</head><p>This property can be expressed in a strikingly simple condition. One says that a v-junction (s, t, u) in a DAG is unlinked if s and u are not neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 15.18 G and G are Markov equivalent if and only if G ♭ = G♭ and G and G have the same unlinked v-junctions.</head><p>Proof Step 1. We first show that a given pair of vertexes in a DAG is unlinked if and only if it can be d-separated by some set in the graph. Clearly, if they are linked, they cannot be d-separated (which is the "if" part), so what really needs to be proved is that unlinked vertexes can be d-separated. Let s and t be these vertexes and let U = A {s,t} \ {s, t}. Then U d-separates s and t since any path between s and t in (G A {s,t}∪U ) ♯ = (G A {s,t} ) ♯ must obviously pass in U .</p><p>Step 2. We now prove the only-if part of theorem 15.18 and therefore assume that G and G are Markov equivalent, or, as stated in theorem 15.17, that d-separation coincides in G and G. We want to prove that G ♭ = G♭ and unlinked v-junctions are the same.</p><p>Step 2.1. The first statement is obvious from Step 1: d-separation determines the existence of a link, so if d-separation coincides in the two graphs, then the same holds for links and G ♭ = G♭ .</p><p>Step 2.2. So let us proceed to the second statement and let (s, t, u) be an unlinked vjunction in G. We want to show that it is also a v-junction in G (obviously unlinked since links coincide). We will denote by ÃS the ancestors of some set S ⊂ V in G (while A S still denotes its ancestors in G). Let U = A {s,u} \ {s, u}. Then, as we have shown in Step 1, U d-separates s and u in G, so that, by assumption it also d-separates them in G. We know that t U , because it cannot be both a child and an ancestor of {s, u} in G (this would induce a loop). The path (s, t, u) links s and u and does not pass in U , which is only possible (since U d-separates s and t in G) if it passes in V -ÃU at a v-junction: so (s, t, u) is a v-junction in G, which is what we wanted to prove.</p><p>Step 3. We now consider the converse statement and assume that G ♭ = G♭ and unlinked v-junctions coincide. We want to show that d-separation is the same in G and G. So, we assume that U d-separates s and t in G, and we want to show that the same is true in G. Thus, what we need to prove is: Claim 1. Consider a path γ between s and t in G♭ = G ♭ . Then γ either (D1) passes in U without a v-junction in G, or (D2) in V \ ÃU with a v-junction in G.</p><p>We will prove Claim 1 using a series of lemmas. We say that γ has a three-point loop at u if (v, u, w) are three consecutive points in γ such that v and w are linked. So (v, u, w, v) forms a loop in the undirected graph.</p><p>Lemma 15.19 If γ is a path between s and t that does not satisfy (D2) for G and passes in U without three-point loops, then γ satisfies (D1) for G.</p><p>The proof is easy: since γ does not satisfy (D2) in G, it satisfies (D1) and passes in U without a v-junction in G. But this intersection cannot be a v-junction in G since it would otherwise have to be linked and constitute a three-point loop in γ, which proves that (D1) is true for γ in G. The next step is to remove the three-point loop condition in lemma 15.19. This will be done using the next two results.</p><p>Lemma 15.20 Let γ be a path with a three-point loop at u ∈ U for G. Assume that γ \ u (which is a valid path in G ♭ ) satisfies (D1) or (D2) in G. Then γ satisfies (D1) or (D2) in G.</p><p>To prove the lemma, let v and w be the predecessor and successor of u in γ. First assume that γ \ u satisfies (D1) in G. If this does not happen at v or at w, then this will apply also to γ and we are done, so let us assume that v ∈ U and that (v <ref type="figure">then (v,</ref> <ref type="figure">u,</ref> <ref type="figure">w</ref>) is not and (D1) is true too.</p><formula xml:id="formula_1419">′ , v, w) is not a v-junction in G, where v ′ is the predecessor of v. If (v ′ , v, u) is not a v-junction in G, then (D1) is true for γ in G. If it is a v-junction,</formula><p>Assume now that (D2) is true for γ \u in G. Again, there is no problem if (D2) occurs for some point other than v or w, so let us consider the case for which it happens at v. This means that v ÃU and (v ′ , v, w) is a v-junction. But, since u ∈ U , the link between u and v must be from u to v in G so that there is no v-junction at u and (D1) is true in G. This proves lemma 15.20.</p><p>Lemma 15.21 Let γ be a path with a three-point loop at u ∈ U for G. Assume that γ does not satisfy (D2) in G. Then γ \ u does not satisfy this property either.</p><p>Let us assume that γ \ u satisfies (D2) and reach a contradiction. Letting (v, u, w) be the three-point loop, (D2) can only happen in γ \ u at v or w, and let us assume that this happens at v, so that, v ′ being the predecessor of v, (v ′ , v, w) is a v-junction in G with v A U . Since v A U , the link between u and v in G must be from u to v, but this implies that (v ′ , v, u) is a v-junction in G with v A U which is a contradiction: this proves lemma 15.21.</p><p>The previous three lemmas directly imply the next one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 15.22 If γ is a path between s and t that does not satisfy (D2) for G, then γ satisfies (D1) or (D2) for G.</head><p>Indeed, if we start with γ that does not satisfy (D2) for G, lemma 15.21 allows us to progressively remove three-point loops from γ until none remains with a final path that satisfies the assumptions of lemma 15.19 and therefore satisfies (D1) in G, and lemma 15.20 allows us to add the points that we have removed in reverse order while always satisfying (D1) or (D2) in G.</p><p>We now partially relax the hypothesis that (D2) is not satisfied with the next lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 15.23 If γ is a path between s and t that does not pass in V \ A U at a linked v-junction for G, then γ satisfies (D1) or (D2) for G.</head><p>Assume that γ does not satisfy (D2) for G (otherwise the result is proved). By lemma 15.22, γ must satisfy (D2) for G. So, take an intersection of γ with V \A U that occurs at a v-junction in G, that we will denote (v, u, w). This is still a v-junction in G since we assume it to be unlinked. Since (D2) is false in G, we must have u ∈ ÃU , and there is an oriented path, τ, from u to U in G.</p><p>We can assume that τ has no v-junction in G. If a v-junction exists in τ, then this v-junction must be linked (otherwise this would also be a v-junction in G and contradict the fact that τ is consistently oriented in G), and this link must be oriented from u to U in G to avoid creating a loop in this graph. This implies that we can bypass the v-junction while keeping a consistently oriented path in G, and iterate this until τ has no v-junction in G. But this implies that τ is consistently oriented in G, necessarily from U to u since u A U .</p><p>Denote τ = (u 0 = u, v 1 , . . . , u n ∈ U ). We now prove by induction that each (v, u k , w) is an unlinked v-junction. This is true when k = 0, and let us assume that it is true for k -1. Then (u k , u k-1 , v) is a v-junction in G but not in G: so it must be linked and there exists an edge between v and u k . In G, this edge must be oriented from v to u k , since (v, u k-1 , u k , v) would form a loop otherwise. For the same reason, there must be an edge in G from w to u k so that (v, u k , w) is an unlinked v-junction.</p><p>Since this is true for k = n, we can replace u by u n in γ and still obtain a valid path. This can be done for all intersections of γ with V \A U that occur at v-junctions. This finally yields a path (denote it γ) which does not satisfy (D2) in G anymore, and therefore satisfies (D1) or (D2) in G: so γ must either pass in U without a v-junction or in V \ ÃU at a v-junction. None of the nodes that were modified can satisfy any of these conditions, since they were all in U with a v-junction, so that the result is true for the original γ also. This proves lemma 15.23.</p><p>So the only unsolved case is when γ is allowed to pass in V \A U at linked v-junctions. We define an algorithm that removes them as follows. Let γ 0 = γ and let γ k be the path after step k of the algorithm. One passes from γ k to γ k+1 as follows.</p><p>• If γ k has no linked v-junctions in V \ A U for G, stop.</p><p>• Otherwise, pick such a v-junction and let (v, u, w) be the three nodes involved in it.</p><formula xml:id="formula_1420">(i) If v ∈ U , v ′ U and (v ′ , v, u) is a v-junction in G, remove v from γ k to define γ k+1 . (ii) Otherwise, if w ∈ U , w ′ U and (u, w, w ′ ) is a v-junction in G, remove w from γ k to define γ k+1 . (iii) Otherwise, remove u from γ k to define γ k+1 .</formula><p>None of the considered cases can disconnect the path. This is clear for case (iii) since v and w are linked. For case (i), note that, in G, (v ′ , v, u) cannot be a v-junction since (v, u, w) is one. This implies that the v-junction in G must be linked and that v ′ and u are connected.</p><p>The algorithm will stop at some point with some γ n that does not have any linked v-junction in V \ A U anymore, which implies that (D1) or (D2) is true in G for γ n . To prove that this statement holds for γ, it suffices to show that if (D1) or (D2) is true in G with γ k+1 , it must have been true with γ k at each step of the algorithm. So let's assume that γ k+1 satisfies (D1) or (D2) in G.</p><p>First assume that we passed from γ k to γ k+1 via case (iii). Assume that (D2) is true for γ k+1 , with as usual the only interesting case being when this occurs at v or w. Assume it occurs at v so that (v ′ , v, w) is a v-junction and v ÃU . If (v ′ , v, u) is a v-junction, then (D2) is true with γ k . Otherwise, there is an edge from v to u in G which also implies an edge from w to u since (v, u, w, v) would be a loop otherwise. So (v, u, w) is a v-junction in G, and u cannot be in ÃU since its parent, v would be in that set also. So (D2) is true in G. Now, assume that (D1) is true at v, so that (v ′ , v, w) is not a v-junction and v ∈ U . If (v ′ , v, u) is not a v-junction either, we are done, so assume the contrary. If v ′ ∈ U , then we cannot have a v-junction at v ′ and (D1) is true. But v ′ U is not possible since this leads to case (i). Now assume that we passed from γ k to γ k+1 via case (i). Assume that (D1) is true for γ k : this cannot be at v ′ since v ′ U , neither at u since u A U , so it will also be true for γ k+1 . The same statement holds with (D2) since (v ′ , v, u) is a v-junction in G with v ∈ U which implies that both v ′ and u are in ÃU . Case (ii) is obviously addressed similarly.</p><p>With this, the proof of theorem 15.18 is complete. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2.5">Probabilistic inference: Sum-prod algorithm</head><p>We now discuss the issue of using the sum-prod algorithm to compute marginal probabilities, P(X (s) = x (s) ) for s ∈ V when X is a Bayesian network on G = (V , E). By definition, P(X = x) can be written in the form</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><formula xml:id="formula_1421">= x) = C∈C ϕ C (x (C) )</formula><p>where C contains all subsets C s := {s} ∪ pa(s), s ∈ V . Marginal probabilities can therefore be computed easily when the factor graph associated to C is acyclic, according to proposition 14.13. However, because of the specific form of the ϕ C 's (they are conditional probabilities), the sum-prod algorithm can be analyzed in more detail, and provide correct results even when the factor graph is not acyclic.</p><p>The general rules for the sum-prod algorithm are</p><formula xml:id="formula_1422">               m sC (x (s) ) ← C,s∈ C, C C m Cs (x (s) ) m Cs (x (s) ) ← y (C) :y (s) =x (s) ϕ C (y (C) ) t∈C\{s} m tC (y (t) )</formula><p>They take a particular form for Bayesian networks, using the fact that a vertex s belongs to C s , and to all C t for t ∈ ch(s). (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )m tC t (y (t) ) u∈pa(s),u t m uC t (y (u) ), for t ∈ ch(s).</p><formula xml:id="formula_1423">m sC s (x (s) ) ← t∈ch(s) m C t s (x (s) ), m sC t (x (s) ) ← m C s s (x (s) ) u∈ch(s),u t m C u s (x (s) ), for t ∈ ch(s), m C s s (x (s) ) ← y (C s ) ,y (s) =x (s) p s (y (pa(s)) , x (s) ) t∈pa(s) m tC s (y (t) ), m C t s (x (s) ) ← y (C t ) ,y (s) =x</formula><p>These relations imply that, if pa(s) = ∅ (s is a root), then m C s s = p s (x (s) ). Also, if ch(s) = ∅ (s is a leaf) then m sC s = 1. The following proposition shows that many of the messages become constant over time.</p><p>Proposition 15.24 All upward messages, m sC s and m C t s with t ∈ ch(s) become constant (independent from x (s) ) in finite time.</p><p>Proof This can be shown recursively as follows. Assume that, for a given s, m tC t is constant for all t ∈ ch(s) (this is true if s is a leaf). Then, m C t s (x (s) ) ← peyC t ,y (s) =x (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )m tC t (y (t) ) u∈pa(s),u t m uC t (y (u) ), = m tC t y (C t ) ,y (s) =x (s)   p t (x (s) ∧ y (pa(s)\{t}) , y (t) )</p><formula xml:id="formula_1424">u∈pa(s),u t m uC t (y (u) ) = m tC t y (C t \{t}) ,y (s) =x (s) u∈pa(s),u t m uC t (y (u) ) = m tC t u∈pa(s),u t y (u) m uC t (y (u) ) which is constant. Now m sC s (x (s) ) ← t∈ch(s) m C t s (x (s) )</formula><p>is also constant. This proves that all m sC s progressively become constant, and, as we have just seen, this implies the same property for m C t s , t ∈ ch(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This proposition implies that, if initialized with constant messages (or after a finite time), the sum-prod algorithm iterates (s)   p s (y ( ) pa(s), x (s) ) t∈pa(s) m tC s (y (t) )</p><formula xml:id="formula_1425">m sC s ← t∈ch(s) m C t s m C s s (x (s) ) ← y (C s ) ,y (s) =x</formula><formula xml:id="formula_1426">m sC t (x (s) ) ← m C s s (x (s) ) u∈ch(s),u t m C u s , t ∈ ch(s) m C t s ← m sC s u∈pa(t),u s y (u) m uC s (y (u) ), t ∈ ch(s).</formula><p>From this expression, we can conclude Proposition 15.25 If the previous algorithm is first initialized with upward messages, m sC s = m C t s all equal to 1, and if downward messages are computed top down from the roots to the leaves, the obtained configuration of messages is invariant for the sum-prod algorithm.</p><p>Proof If all upward messages are equal to 1, then clearly, the downward messages sum to 1 once they are updated from roots to leaves, and this implies that the upward messages will remain equal to 1 for the next round. The obtained configuration is invariant since the downward messages are recursively uniquely defined by their value at the roots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The downward messages, under the previous assumptions, satisfy m sC t (x (s) ) = m C s s (x (s) ) for all t ∈ ch(s) and therefore m C s s (x (s) ) = y (C s ) ,y (s) =x (s)   π(y (pa(s)) , x (s) ) t∈pa(s) m C t t (y (t) ). <ref type="bibr">(15.5)</ref> Note that the associated "marginals" inferred by the sum-prod algorithm are</p><formula xml:id="formula_1427">σ s (x (s) ) = C,s∈C m Cs (x (s) ) = m C s s (x (s) )</formula><p>since m C t s (x (s) ) = 1 when t ∈ ch(s).</p><p>Although the sum-prod algorithm initialized with unit messages converges to a stable configuration if run top-down, the obtained σ s 's do not necessarily provide the correct single site marginals. There is a situation for which this is true, however, which is when the initial directed graph is singly connected, as we will see below.</p><p>Before this, let us analyze the complexity resulting from an iterative computation of the marginal probabilities, similar to what we have done with trees.</p><p>We define the depth of a vertex in G as follows.</p><p>Definition 15.26 Let G = (V , E) be a DAG. The depth of a vertex s in V is defined recursively by depth(s) = 0 if s has no parent.</p><p>depth(s) = 1 + max depth(t), t ∈ pa(s) otherwise.</p><p>The recursive computation of marginal distributions is made possible (although not always feasible) with the following remark. Lemma 15.27 Let X be a Bayesian network on the DAG G = (V , E), and S ⊂ V , such that all elements in S have the same depth. Let pa(S) be the set of parents of elements in S, and T = depth -(S) the set of vertexes in V with depth strictly smaller than the depth of S. Then (X (S) X (T \pa(S)) | X (pa(S)) ) and the variables X (s) , s ∈ S are conditionally independent given X (pa(S)) .</p><p>Proof It suffices to show that vertexes in S are separated from T \ pa(S) and from other elements of S by pa(S) for the graph (G S∪T ) ♯ . Any path starting at s ∈ S must either pass by a parent of s (which is what we want), or by one of its children, or by another vertex that shares a child with s in G S∪T . But s cannot have any child in G S∪T , since this child cannot have a smaller depth than s, and it cannot be in S either since all elements in S have the same depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>This lemma allows us to work recursively as follows. Assume that we can compute marginal distributions over sets S with maximal depth no larger than d. Take a set S of maximal depth d + 1, and let S 0 be the set of elements of depth d + 1 in S. Then, letting T = depth -(S) = depth -(S 0 ), and S 1 = S \ S 0 , P(X (S) = x (S) ) = y (T \S 1 ) P(X (S 0 ) = x (S 0 ) | X (T ) = y (T \S 1 ) ∧ x (S 1 ) )P (X (T ∪S 1 ) = y (T \S 1 ) ∧ x (S 1 ) ) = y (pa(S)\S 1 ) s∈S 0 p s ((y ∧ x) (pa(s)) ∧ x (S 1 ) , x (s) )P(X (pa(S 0 )∪S 1 ) = y (pa(S 0 )\S 1 ) ∧ x (S 1 ) ) <ref type="bibr">(15.6)</ref> Since pa(S)∪S 1 has maximal depth strictly smaller than the maximal depth of S, this indeed provides a recursive formula for the computation of marginal over subsets of V with increasing maximal depths. However, because one needs to add parents to the considered set when reducing the depth, one may end up having to compute marginals over very large sets, which becomes intractable without further assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2.6">Conditional probabilities and interventions</head><p>One of the main interests of graphical models is to provide an ability to infer the behavior of hidden variables of interest given other, observed, variables. When dealing with oriented graphs the way this should be analyzed is, however, ambiguous. work interpretation of this graph is that both events (which may be true or false) "Bad weather" and "Broken HVAC" happen first, and that they are independent. Then, given their observation, the "No school" event may occur, probably more likely if the weather is bad or the HVAC is broken or snow, and even more likely if both happened at the same time.</p><p>Now consider the following passive observation: you wake up, you haven't checked the weather yet or the news yet, and someone tells you that there is no school today. Then you may infer that there is more chances than usual for bad weather or the HVAC broken at school. Conditionally to this information, these two events become correlated, even if they were initially independent. So, even if the "No school" event is considered as a probabilistic consequence of its parents, observing it influences our knowledge on them. Now, here is an intervention, or manipulation: the school superintendent has declared that he has given enough snow days for the year and declared that there would be school today whatever happens. So you know that the "no-school" event will not happen. Does it change the risk of bad weather of broken HVAC? Obviously not: an intervention on a node does not affect the distribution of the parents.</p><p>Manipulation and passive observation are two very different ways of affecting unobserved variables in Bayesian networks. Both of them may be relevant in applications. Of the two, the simplest to analyze is intervention, since it merely consists in clamping one of the variables while letting the rest of the network dynamics unchanged. This leads to the following formal definition of manipulation. Definition 15.30 Let G = (V , E) be a directed acyclic graph and X a Bayesian network on G. Let S be a subset of G and x (S) ∈ F S a given configuration on S. Then the manipulated distribution of X with fixed values x (S) on S is the Bayesian network on the restricted graph G S , with the same conditional probabilities, using the value x (s) every time a vertex</p><formula xml:id="formula_1428">s ∈ S is a parent of t ∈ V \ S in G.</formula><p>So, if the distribution of X is given by (15.2), then its distribution after manipulation on S is π(y (V \S) ) =</p><p>t∈V \S p t (y (pa(t)) , y (t) )</p><p>where pa(t) is the set of parents of t in G, and y (s) = x (s) whenever s ∈ pa(t) ∩ S.</p><p>The distribution of a Bayesian network X after passive observation X (S) = x (S) is not so easily described. It is obviously the conditional distribution P (X (V \S) = y (V \S) | X (S) = x (S) ) and therefore requires using the conditional dependency structure, involving the moral graph and/or d-separation.</p><p>Let us discuss this first in the simpler case of trees, for which the moral graph is the undirected acyclic graph underlying the tree, and d-separation is simple separation on this acyclic graph. We can then use proposition 13.22 to understand the new structure after conditioning: it is a G ♭ V \S -Markov random field, and, for t ∈ V \ S, the conditional distribution of X (t) = y (t) given its neighbors is the same as before, using the value x (s) when s ∈ S. But note that when doing this (passing to G ♭ ), we broke the causality relation between the variables. We can however always go back to a tree (or forest, since connectedness may have been broken) with the same edge orientation as they initially were, but this requires reconstituting the edge joint probabilities from the new acyclic graph, and therefore using (acyclic) belief propagation.</p><p>With general Bayesian networks, we know that the moral graph can be loopy and therefore a source of difficulties. The following proposition states that the damage is circumscribed to the ancestors of S. Proposition 15.31 Let G = (V , E) be a directed acyclic graph, X a Bayesian network on G, S ⊂ V and x (A S ) ∈ F (A S ). Then the conditional distribution of X (A c S ) given by X (A S ) = x (A S ) coincides with the manipulated distribution in definition 15.30.</p><p>Proof The conditional distribution is proportional to s∈V p(y (pa(s)) , y (s) ) with y (t) = x (t) if t ∈ A S . Since s ∈ A S implies pa(s) ⊂ A S , all terms with s ∈ A S are constant in the sum and can be factored out after normalization. So the conditional distribution is proportional to s∈A c S p(y (pa(s)) , y (s) ) with y (t) = x (t) if t ∈ A S . But we know that such products sum to 1, so that the conditional distribution is equal to this expression and therefore provides a Bayesian network on G A c S .</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.3">Structural equation models</head><p>Structural equation models (SEM's) provides an alternative (and essentially equivalent) formulation of Bayesian networks, which may be more convenient to use, especially when dealing with variables taking values in general state spaces.</p><p>Let G = (V , E) be a directed acyclic graph. SEMs are associated to families of functions Φ s : F (pa(s)) × B s → F s and random variables ξ s : Ω → B s (where B s is some measurable set), for s ∈ V . The random field X : Ω → F (V ) associated to the SEM satisfies the equations X s = Φ (s) (X (s-) , ξ (s) ). <ref type="bibr">(15.8)</ref> Because of the DAG structure, these equations uniquely define X once ξ is specified. As a consequence, there exists a function Ψ such that X = Ψ (ξ).</p><p>The model is therefore fully specified by the functions Φ (s) and the probability distributions of the variables ξ (s) . We will assume that they have a density, denoted g (s) , s ∈ V , with respect to some measure µ s on B s . They are typically chosen as uniform distributions on B s (continuous and compact, or discrete) or as standard Gaussian when B s = R d s for some d s . One also generally assumes that the variables (ξ (s) , s ∈ V ) are jointly independent, and we make this assumption below.</p><p>Let V k , k ≥ 0, be the set of vertexes in V with depth k (c.f. definition 15.26) and</p><formula xml:id="formula_1429">V &lt;k = V 0 ∪ • • •∪ V k-1 . Then (using the independence of (ξ (s) , s ∈ V ), for s ∈ V k , the con- ditional distribution of X (s) given X (V &lt;k ) = x (V &lt;k ) is the distribution of Φ (s) (x (s-) , ξ (s) ).</formula><p>Formally this is given by</p><formula xml:id="formula_1430">Φ (s) (x (s-) , •) ♯ (g (s) µ s ),</formula><p>the pushforward of the distribution of ξ (s) by Φ (s) (x (s-) , •).</p><p>More concretely, assume that ξ s follows a uniform distribution on B s = [0, 1] h for some h, and assume that F s is finite for all s. Then,</p><formula xml:id="formula_1431">P (X (s) = x (s) | X (V &lt;k ) = x (V &lt;k ) ) = Volume(U s (x (pa(s)) , x (s) )) ∆ = p s (x (pa(s)) , x (s) )</formula><p>where</p><formula xml:id="formula_1432">U s (x (pa(s)) , x (s) ) = ξ ∈ [0, 1] h : Φ (s) (x (s-) , ξ) = x (s) .</formula><p>Since variables X (s) , s ∈ V k are conditionally independent given X (V &lt;k) , we find that X decomposes as a Bayesian network over G, P (X = x) = s∈V p s (x (pa(s)) , x (s) ).</p><p>Similarly, if</p><formula xml:id="formula_1433">F s = B s = R d s , ξ (s) ∼ N (0, Id R d s ), and ξ (s) → Φ (s) θ (x (pa(s)) , ξ (s) ) is invertible, with C 1 inverse x (s) → Ψ (s)</formula><p>θ (x (pa(s)) , x (s) ), then X is a Bayesian network, with continuous variables, and, using the change of variable formula, the conditional distribution of X (s) given X (pa(s)) = x (s-) has p.d.f.</p><formula xml:id="formula_1434">p s (x (pa(s)) , x (s) ) = 1 (2π) d s /2 exp - 1 2 |x s -Ψ (s) θ (x (pa(s)) , x (s) )| 2 det(∂ x (s) Ψ (s) θ (x (pa(s)) , x (s) )) .</formula><p>A simple and commonly used special case for this example are linear SEMs, with</p><formula xml:id="formula_1435">X (s) = a s + b T s X (s) + σ s ξ (s) .</formula><p>In this case, the inverse mapping is immediate and the Jacobian determinant in the change of variables is 1/σ</p><formula xml:id="formula_1436">d s s .</formula><p>Chapter 16</p><p>Latent Variables and Variational Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.1">Introduction</head><p>We will describe, in the next chapters, methods that fit a parametric model to the observation while introducing unobserved, or "latent," components in their models, whose inference typically attaches interpretable information or structure to the data. We have seen one such example in the form of the mixture of Gaussian in chapter 4, that we will revisit in chapter 19. We now provide a presentation of the variational Bayes paradigm that provides a general strategy to address latent variable problems <ref type="bibr" target="#b161">[143,</ref><ref type="bibr" target="#b115">97,</ref><ref type="bibr" target="#b32">14,</ref><ref type="bibr" target="#b118">100]</ref>.</p><p>The general framework is as follows. Variables in the model are divided in two groups: the observable part, that we denote X, and the latent part, denoted Z. In many models Z represents some unobservable structure, such that X conditional to Z has some relatively simple distribution (in a Bayesian estimation context, Z often contains model parameters). The quantity of interest, however, is the conditional distribution of Z given X (also called the "posterior distribution"), which allows one to infer the latent structure from the observations, and will also have an important role in maximum likelihood parametric estimation, as we will see below. This conditional distribution is not always easy to compute or simulate, and variational Bayes provides a framework under which it can be approximated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.2">Variational principle</head><p>We consider a pair of random variables X and Z, where X is considered as "observed" and Z is hidden, or "latent". We will use U = (X, Z) to denote the two variables taken together. We denote as usual by P U the probability law of U , defined on R U = R X × R Z by P U (A) = P(U ∈ A). We will also assume that there exists a measure µ on R U that decomposes as a product measure µ = µ X × µ Z (where µ X and µ Z are measures on R X and R Z ), such that P U ≪ µ (π U is absolutely continuous with respect to µ). This implies that P U has a density with respect to µ that we will denote f U . If both R X and R Z are discrete, µ is typically the counting measure, and if they are both Euclidean space, µ can be the Lebesgue measure on the product. 1   The variables X and Z then have probability density functions with respect to µ X ad µ Z , given by</p><formula xml:id="formula_1437">f X (x) = R Z f U (x, z)µ Z (dz) and f Z (z) = R X f U (x, z)µ X (dx). The conditional distribution of X given Z = z, denoted P X ( • | Z = z), has density f X (x | z) = f U (x, z)/f Z (z) with respect to µ X and that of Z given X = x, denoted P Z ( • | X = x), has density f Z (z | x) = f U (x, z)/f X (x)</formula><p>with respect to µ Z . We will be mainly interested by approximations of P Z ( • | X = x), assuming that P Z and P X ( • | Z = z) (and hence P U ) are easy to compute or simulate.</p><p>We will use the Kullback-Liebler divergence to quantify the accuracy of the approximation. As stated in proposition 4.1, we have</p><formula xml:id="formula_1438">P Z ( • | X = x) = argmin ν∈M 1 (R Z ) KL(ν ∥ P Z (• | X = x))</formula><p>where M 1 (R Z ) denotes the set of all probability distributions on R Z . Note that all distributions ν for which KL(ν ∥ π Z (•|X = x)) is finite must be absolutely continuous with respect to µ Z and therefore take the form ν = gµ Z . One has</p><formula xml:id="formula_1439">KL(gdµ Z ∥P Z (•|X = x)) = R Z log g(z) f Z (z|x) g(z)µ Z (dz) = R Z log g(z) f U (x, z) g(z)µ Z (dz) + log f X (x). (16.1)</formula><p>We will denote by P (µ Z ), or just P when there is no ambiguity, the set of all p.d.f.'s g with respect to µ Z , i.e., the set of all non-negative measurable functions on R Z with</p><formula xml:id="formula_1440">R Z g(z)µ Z (dz) = 1.</formula><p>The basic principle of variational Bayes methods is to replace P by a subset P and to define the approximation 1 The reader unfamiliar with measure theory may want to read this discussion by replacing dµ X by dx, dµ Z by dz and dµ U by dx dz, i.e., in the context of continuous probability distributions having p.d.f.'s with respect to the Lebesgue's measure.</p><formula xml:id="formula_1441">P Z ( • |X = x) = argmin g∈ P KL(gµ Z ∥P Z ( • |X = x)).</formula><p>For the approximation to be practical, the set P must obviously be chosen so that the computation of P Z ( • |X = x) is computationally feasible. We now review a few examples, before passing to the EM algorithm and its approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.3">Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.3.1">Mode approximation</head><p>Assume that R Z is discrete and µ Z is the counting measure so that</p><formula xml:id="formula_1442">KL(gdµ Z ∥ P Z ( • | X = x)) -log f X (x) = z∈R Z log g(z) f U (x, z) g(z),</formula><p>the sum being infinite if there exists z such that ν(z) &gt; 0 and f U (x, z) = 0. Take</p><formula xml:id="formula_1443">P = {1 z : z ∈ R Z } ,</formula><p>the family of all Dirac functions on R Z . Then,</p><formula xml:id="formula_1444">KL(1 z ∥ P Z ( • |X = x)) -log f X (x) = -log f U (x, z).</formula><p>The variational approximation of P Z ( • | X = x) over P therefore is the Dirac measure at point(s) z ∈ R Z at which f U (x, z) is largest, i.e., the mode(s) of the posterior distribution. This approximation is often called the MAP approximation (for maximum a posteriori).</p><p>If R Z is, say, R q and µ Z = dz is Lebesgue's measure, then the previous construc-</p><p>tion does not work because 1 z is not a p.d.f. with respect to µ Z . In place of Dirac functions, one can use constant functions on small balls. Let B(z, ϵ) denote the open ball with radius ϵ, and let |B(z, ϵ)| denote its volume. Let u z,ϵ = 1 B(z,ϵ) /|B(z, ϵ)|. Fixing ϵ, we can consider the set</p><formula xml:id="formula_1445">P = u z,ϵ : z ∈ R q .</formula><p>Now, one has (leaving the computation to the reader)</p><formula xml:id="formula_1446">KL(u z,ϵ dz ∥ P Z ( • |X = x)) -log f X (x) = -log 1 |B(z, ϵ) B(z,ϵ) f U (x, z ′ )dz ′ .</formula><p>The limit for small ϵ (assuming that f U (x, •) is continuous at z, or defining the limit up to sets of measure zero) islog f U (x, z), justifying again choosing the mode of the posterior distribution of Z for the approximation.</p><p>The mode approximation has some limitations. First, it is in general a very crude approximation of the posterior distribution. Second, even with the assumption that f U has closed form, this p.d.f. is often difficult to maximize (for example when defining models over large discrete sets). In such cases, the mode approximation has limited practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.3.2">Gaussian approximation</head><p>Let us still assume that R Z = R q and that µ Z = dz. Let P be the family of all Gaussian distributions N (m, Σ) on R q . Then, denoting by ϕ( • ; m, Σ) the density of N (m, Σ),</p><formula xml:id="formula_1447">KL(ϕ( • ; m, Σ)∥P Z ( • | X = x)) -log f X (x) = - q 2 log 2π - q 2 - 1 2 log det(Σ) - R q log f U (x, z)ϕ(z; m, Σ)dz.</formula><p>In order to provide the best approximation, m and Σ must therefore maximize</p><formula xml:id="formula_1448">R q log f U (x, z)ϕ(z; m, Σ)dz + 1 2 log det(Σ). (16.</formula><p>2)</p><p>The resulting optimization problem does not have a closed form solution in general (see section 18.2.2 for an example in which stochastic gradient methods are used to solve this problem). Another approach that is commonly used in practice is to push the approximation further by replacing log f U (x, z) by its second order expansion around its maximum as a function of z. Let m(x) be the posterior mode, i.e., the value of z at which x → log f U (x, z) is maximal, that we will assume to be unique. Let H(x) denote the q × q Hessian matrix formed by the second partial derivatives oflog f U (x, z) (with respect to z) at z = m(x). This matrix is positive semidefinite according to the choice made for m(x), and we will assume that it is positive definite. Since the first derivatives of log f U (x, z) at m(x) must vanish, we have the expansion:</p><formula xml:id="formula_1449">log f U (x, z) = log f U (x, m(x)) - 1 2 (z -m(x)) T H(x)(z -m(x)) + • • •</formula><p>Plugging the expansion into the integral in (16.2) yields</p><formula xml:id="formula_1450">- 1 2 trace(H(x)Σ) - 1 2 (m -m(x)) T H(x)(m -m(x)) + 1 2 log det Σ.</formula><p>To maximize this expression, one must clearly take m = m(x). Moreover,</p><formula xml:id="formula_1451">∂ Σ (-trace(H(x)Σ) + log det Σ) = -H(x) T + (Σ T ) -1 = -H(x) + Σ -1 ,</formula><p>and we see that one must take Σ = H(x) -1 . This provides the Laplace approximation <ref type="bibr" target="#b80">[62]</ref> of the posterior, N (m(x), H(x) -1 ), which is practical when the mode and corresponding second derivatives are feasible to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.3.3">Mean-field approximation</head><p>This section generalizes the approach discussed in proposition 14.6 for Markov random fields. Assume that R Z can be decomposed into several components R</p><formula xml:id="formula_1452">[1] Z , . . . , R [K] Z ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.3.">EXAMPLES</head><p>writing z = (z [1] , . . . , z [K] ) (for example, taking K = q and z [i] = z (i) , the ith coordinate of z if R Z = R q ). Also assume that µ Z splits into a product measure µ</p><formula xml:id="formula_1453">[1] Z ⊗ • • • ⊗ µ [K]</formula><p>Z . Mean-field approximation consists in assuming that probabilities ν in P split into independent components, i.e., their densities g take the form: g(z) = g [1] (z [1] ) • • • g [K] (z [K] ).</p><p>Then,</p><formula xml:id="formula_1454">KL(ν ∥ P Z ( • | X = x)) -log f X (x) = K j=1 R [j] Z</formula><p>log g [j] (z [j] )g [j] (z [j] )µ</p><p>[j] Z (dz [j] )</p><formula xml:id="formula_1455">- R Z log f U (x, z) q j=1</formula><p>g [j] (z [j] )µ Z (dz). <ref type="bibr">(16.3)</ref> The mean-field approximation may be feasible when log f U (x, z) can be written as a sum of products of functions of each z [j] . Indeed, assume that</p><formula xml:id="formula_1456">log f U (x, z) = α∈A K j=1 ψ α,j (z [j] , x) (16.4)</formula><p>where A is a finite set. To shorten notation, let us denote by ⟨ψ⟩ the expectation of a function ψ with respect to the product p.d.f. g. Then, (16.3) can be written as</p><formula xml:id="formula_1457">KL(ν∥P Z ( • | X = x)) -log f X (x) = K j=1 ⟨log g (j) (z [j] )⟩ - α∈A K j=1</formula><p>⟨ψ α,j (z [j] , x)⟩.</p><p>The following lemma will allow us to identify the form taken by the optimal p.d.f. g [j] . Lemma 16.1 Let Q be a set equipped with a positive measure µ. Let ψ : Q → R be a measurable function such that</p><formula xml:id="formula_1458">C ψ ∆ = Q exp(ψ(q))µ(dq) &lt; ∞. Let g ψ (q) = 1 C ψ exp(ψ(q)).</formula><p>Let g be any p.d.f. with respect to µ, and define</p><formula xml:id="formula_1459">F(g) = Q (log g(q) -ψ(q))g(q)µ(dq).</formula><p>Then F(g ψ ) ≤ F(g).</p><p>Proof We note that g ψ &gt; 0, and that</p><formula xml:id="formula_1460">KL(g∥g ψ ) = F(g) + log C ψ = F(g) -F(g ψ ),</formula><p>which proves the result, since KL divergences are always non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Applying this lemma separately to each function g [j] implies that any optimal g must be such that g [j] (z [j] ) ∝ exp</p><formula xml:id="formula_1461">       α∈A M α,j ψ α,j (z [j] , x)        with M α,j = K j ′ =1,j ′ j ⟨ψ α,j ′ (z [j ′ ] , x)⟩.</formula><p>We therefore have</p><formula xml:id="formula_1462">⟨ψ α,j (z [j] , x)⟩ = R [j] Z ψ α,j (z [j] , x) exp α ′ ∈A M α ′ ,j ψ α ′ ,j (z [j] , x) µ [j] Z (dz [j] ) R [j] Z exp α ′ ∈A M α ′ ,j ψ α ′ ,j (z [j] , x) µ [j]</formula><p>Z (dz [j] ) <ref type="bibr">(16.5)</ref> This specifies a relationship expressing ⟨ψ α,j (z [j] , x)⟩ as a function of the other expectations ⟨ψ α ′ ,j ′ (z (j ′ ) , x)⟩ for j j ′ . These equations put together are called the mean-field consistency equations. When these equations can be written explicitly, i.e., when the integrals in (16.5) can be evaluated analytically (which is generally the case when the p.d.f.'s g [j] can be associated with standard distributions), one obtains an algorithm that iterates (16.5) over all α and j until stabilization (each step reducing the objective function in <ref type="bibr">(16.3)</ref>).</p><p>Let us retrieve the result obtained in proposition 14.6 using the current formalism. Assume that R X finite and R Z = {0, 1} L , where L can be a large number, with</p><formula xml:id="formula_1463">f U (x, z) = 1 C exp         L j=1 α j (x)z (j) + L i,j=1,i&lt;j β ij (x)z (i) z (j)        </formula><p>.</p><p>Take K = L, z [j] = z (j) . Applying the previous discussion, we see that g [j] must take the form g [j] (z (j) ) = exp α j (x)z (j) + i j β ij (x)⟨z (i) ⟩z (j)</p><p>1 + exp α j (x) + i j β ij (x)⟨z (i) ⟩</p><p>In particular</p><formula xml:id="formula_1464">⟨z (j) ⟩ = exp α j (x) + i j β ij (x)⟨z (i) ⟩ 1 + exp α j (x) + i j β ij (x)⟨z (i) ⟩</formula><p>providing the mean-field consistency equations.</p><p>In this special case, it is also possible to express the objective function as a simple function of the expectations ⟨z (j) ⟩'s. We indeed have, letting ρ j = ⟨z (j) ⟩,</p><formula xml:id="formula_1465">z∈R Z log f U (x, z) L j=1 g [j] (z (j) ) = -log C + L j=1 α j (x)ρ j + L i,j=1,i&lt;j β ij (x)ρ i ρ j .</formula><p>The values of ρ 1 , . . . , ρ L are then obtained by maximizing</p><formula xml:id="formula_1466">L j=1 α j (x)ρ j + L i,j=1,i&lt;j β ij (x)ρ i ρ j - L j=1 ρ j log ρ j + (1 -ρ j ) log(1 -ρ j ) .</formula><p>The consistency equations express the fact that the derivatives of this expression with respect to each ρ j vanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.4">Maximum likelihood estimation 16.4.1 The EM algorithm</head><p>We now consider maximum likelihood estimation with latent variables and use the notation of section 16.2. The main tool is the following obvious consequence of (16.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 16.2 One has</head><formula xml:id="formula_1467">log f X (x) = max g∈P (µ Z ) R Z log f U (x, z) g(z) g(z)dµ Z (z)</formula><p>and the maximum is achieved for g</p><formula xml:id="formula_1468">(z) = f Z (z | x), the conditional p.d.f. of Z given X = x.</formula><p>Proof Equation (16.1) implies that</p><formula xml:id="formula_1469">R Z log f U (x, z) g(z) g(z)dµ Z (z) = log f X (x) -KL(g µ Z ∥P Z ( • |X = x))</formula><p>and the r.h.s. is indeed maximum when the Kullback-Liebler divergence vanishes, that is, when g is the p.d.f. of P Z ( • | X = x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We will use this proposition for the derivation of the expectation-maximization (or EM) algorithm for maximum likelihood with latent variables. We now assume that P U , and therefore f U , is parametrized by θ ∈ Θ, and that a training set T = (x 1 , . . . , x N ) of X is observed. To indicate the dependence in θ, we will write f U (x, z ; θ), or f Z (z | x ; θ). The maximum likelihood estimator (m.l.e.) then maximizes</p><formula xml:id="formula_1470">ℓ(θ) = x∈T log f X (x ; θ) .</formula><p>The EM algorithm is useful when the computation of the m.l.e. for complete observations, i.e., the maximization of log f U (x, z ; θ) when both x and z are given, is easy, whereas the same problem with the marginal distribution is hard.</p><p>From the proposition, we have:</p><formula xml:id="formula_1471">x∈T log f X (x ; θ) = x∈T max g x ∈P (µ Z ) R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz)</formula><p>Therefore the maximum likelihood requires to compute max</p><formula xml:id="formula_1472">θ,g x ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz). (<label>16.6)</label></formula><p>The maximization can therefore be done by iterating the following two steps.</p><p>1. Given θ n , compute argmax</p><formula xml:id="formula_1473">g x ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz). 2. Given g 1 , . . . , g N , compute argmax θ x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz) = argmax θ x∈T R Z log (f U (x, z ; θ)) g x (z)µ Z (dz).</formula><p>Step 1. is explicit and its solution is g x (z) = f Z (z | x ; θ). Using this, both steps can be grouped together, yielding the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 16.1 (EM algorithm)</head><p>Let a statistical model with density f U (x, z ; θ) modeling an observable variable X and a latent variable Z be given, and a training set T = (x 1 , . . . , x N ). Starting with an initial guess of the parameter, θ(0), the EM algorithm iterate the following equation until numerical stabilization, .</p><formula xml:id="formula_1474">θ n+1 = argmax θ ′ x∈T R Z log (f U (x, z ; θ ′ )) f Z (z | x ; θ n )µ Z (dz). (<label>16</label></formula><p>.7) Equation (16.7) maximizes (in θ ′ ) a function defined as an expectation (for θ n ), justifying the name "Expectation-Maximization." 16.4.2 Application: Mixtures of Gaussian A mixture of Gaussian (MoG) model was introduced in chapter 4 ((4.4)). We now reinterpret it (in a slightly generalized version) as a model with partial observations and show how the EM algorithm can be applied. Let ϕ(x ; m, Σ) denote the p.d.f. of the d-dimensional multivariate Gaussian distribution with mean m and covariance matrix Σ. We model f X (x ; θ) as</p><formula xml:id="formula_1475">f X (x ; θ) = p j=1 α j ϕ(x, ; c j , Σ j ).</formula><p>Here, θ contains all sequences α 1 , . . . , α p (non-negative numbers that sum to one), c 1 , . . . , c p ∈ R d and Σ 1 , . . . , Σ p (d × d positive definite matrices).</p><p>Using the previous notation, we therefore have R X = R d , and µ X the Lebesgue measure on that space. The variable Z will take values in R Z = {1, . . . , p}, with µ Z being the counting measure. We model the joint density function for (X, Z) as</p><formula xml:id="formula_1476">f U (x, z ; θ) = α z ϕ(x; c z , Σ z ). (16.8)</formula><p>Clearly f X is the marginal p.d.f. of f U . One can therefore consider Z as a latent variable, and therefore estimate θ using the EM algorithm.</p><p>We now make (16.7) explicit for mixtures of Gaussian. For given θ and θ ′ and x ∈ R, let</p><formula xml:id="formula_1477">U x (θ, θ ′ ) = d 2 log 2π + R Z log (f U (x, z ; θ ′ )) f Z (z|x ; θ)dµ Z (z) = p z=1 log α ′ z - 1 2 log det Σ ′ z - 1 2 (x -c ′ z ) T Σ ′ z -1 (x -c ′ z ) f Z (z|x ; θ) with f Z (z | x ; θ) = (det Σ z ) -1 2 α z e -1 2 (x-c z ) T Σ -1 z (x-c z ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )</formula><p>.</p><p>If θ n is the current parameter in the EM, the next one, θ n+1 must maximize</p><formula xml:id="formula_1478">N x∈T U x (θ n , θ ′</formula><p>). This can be solved in closed form. To compute α ′ 1 , . . . , α ′ p , one must maximize</p><formula xml:id="formula_1479">x∈T p z=1 (log α ′ z )f Z (z|x ; θ)</formula><p>subject to the constraint that z α ′ z = 1. This yields</p><formula xml:id="formula_1480">α ′ z = x∈T f Z (z|x ; θ) p j=1 x∈T f Z (j|x ; θ) = ζ z / N with ζ z = x∈T f Z (z|x ; θ). The centers c ′ 1 , . . . , c ′ p must minimize x∈T (x -c ′ z ) T Σ ′ z -1 (x -c ′ z )f Z (z|x ; θ), which yields c ′ z = 1 ζ z x∈T xf Z (z|x ; θ). Finally, Σ ′ z must minimize ζ z 2 log det Σ ′ z + 1 2 x∈T (x -c ′ z ) T Σ ′ z -1 (x -c ′ z )f Z (z|x ; θ),</formula><p>which yields</p><formula xml:id="formula_1481">Σ ′ z = 1 ζ z x∈T (x -c ′ z )(x -c ′ z ) T f Z (z|x ; θ).</formula><p>We can now summarize the algorithm.</p><p>Algorithm 16.2 (EM for Mixture of Gaussian distributions) 1. Initialize the parameter θ(0) = (α(0), c(0), Σ(0)). Choose a small constant ϵ and a maximal number of iterations M.</p><p>2. At step n of the algorithm, let θ = θ(n) be the current parameter, writing for short θ = (α, c, Σ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compute, for x ∈ T and i</head><formula xml:id="formula_1482">= 1, . . . , p f Z (i | x ; θ) = (det Σ i ) -1 2 α i e -1 2 (x-c i ) T Σ -1 i (x-c i ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )</formula><p>and let ζ i = x∈T f Z (i|x ; θ), i = 1, . . . , p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Let α</head><formula xml:id="formula_1483">′ i = ζ i /N . 5. For i = 1, . . . , p, let c ′ i = 1 ζ i x∈T xf Z (i | x ; θ).</formula><p>6. For i = 1, . . . , p, let</p><formula xml:id="formula_1484">Σ ′ i = 1 ζ i x∈T (x -c ′ i )(x -c ′ i ) T f Z (i | x ; θ). 7. Let θ ′ = (µ ′ , c ′ , Σ ′ ). If |θ ′ -θ| &lt; ϵ or n + 1 = M: return θ ′ and</formula><p>exit the algorithm. 8. Set θ(n + 1) = θ ′ and return to step 2. Remark 16.3 Algorithm 16.2 can be simplified by making restrictions on the model. Here are some examples.</p><p>(i) One may restrict to Σ i = σ 2 i Id R d to reduce the number of free parameters. Then, Step 7 of the algorithm needs to be replaced by:</p><formula xml:id="formula_1485">(σ ′ i ) 2 = 1 dζ i x∈T |x -c ′ i | 2 f Z (i | x ; θ).</formula><p>(ii) Alternatively, the model may be simplified by assuming that all covariance matrices coincide: Σ i = Σ for i = 1, . . . , p. Then, Step 7 becomes</p><formula xml:id="formula_1486">Σ ′ i = 1 N p i=1 x∈T (x -c ′ i )(x -c ′ i ) T f Z (i | x ; θ). ♦ (iii)</formula><p>Finally, one may assume that Σ is known and fixed in the algorithm (usually in the form Σ = σ 2 Id R d for some σ &gt; 0) so that Step 7 of the algorithm can be removed.</p><p>(iv) One may also assume also that the (prior) class probabilities are known, typically set to α i = 1/p for all i, so that Step 4 can be skipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.4.3">Stochastic approximation EM</head><p>The stochastic approximation EM (or SAEM) algorithm has been proposed by Delyon et al. <ref type="bibr" target="#b76">[58]</ref> (see this reference for convergence results) to address the situation in which the expectations for the posterior distribution cannot be computed in closed form, but can be estimated using Monte-Carlo simulations. SAEM uses a special form of stochastic approximation, different from the SGD algorithm described in section 3.3. It updates, at each step n, an approximate objective function that we will denote λ n and a current parameter θ(n). It implements the following iterations:</p><formula xml:id="formula_1487">                   ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), x ∈ T λ n+1 (θ ′ ) = 1 - 1 n + 1 λ n (θ ′ ) + 1 n + 1 x∈T log f U (x, ξ (x) n+1 ; θ ′ ) -λ n (θ ′ ) , θ ′ ∈ Θ θ n+1 = argmax θ ′ λ n+1 (θ ′ ) (16.9)</formula><p>The second step means that</p><formula xml:id="formula_1488">λ n (θ ′ ) = N x∈T         1 n n j=1 log f U (x, ξ (x) j ; θ ′ )         . Given that ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), one expects this expression to approximate x∈T R Z log (f U (x, z ; θ ′ )) f Z (z | x ; θ)dµ Z (z)</formula><p>so that the third step of (16.9) can be seen as an approximation of (16.7). Sufficient conditions under which this actually happens (and θ(n) converges to a local maximizer of the likelihood) are provided in Delyon et al. <ref type="bibr" target="#b76">[58]</ref> (see also <ref type="bibr">Kuhn and Lavielle [112]</ref> for a convergence result under more general hypotheses on how ξ is simulated).</p><p>To be able to run this algorithm efficiently, one needs the simulation of the posterior distribution to be feasible. Importantly, one also needs to be able to update efficiently the function λ n . This can be achieved when the considered model belongs to an exponential family, which corresponds to assuming that the p.d.f. of U takes the form</p><formula xml:id="formula_1489">f U (x, z ; θ) = 1 C(θ) exp ψ(θ) T H(x, z)</formula><p>for some functions ψ and H. For example, the MoG model of equation (4.4) takes this form, with</p><formula xml:id="formula_1490">ψ(θ) T = log α 1 - 1 2 m T 1 Σ -1 1 m 1 - 1 2 log det Σ 1 , . . . , log α p - 1 2 m T p Σ -1 p m p - 1 2 log det Σ p , Σ -1 1 m 1 , . . . , Σ -1 p m p , Σ -1 1 , . . . , Σ -1 p , H(x, z) T = 1 z=1 , . . . , 1 z=p , x1 z=1 , . . . , x1 z=p , - 1 2 xx T 1 z=1 , . . . , - 1 2 xx T 1 z=p and C(θ) = (2π) pd/2 .</formula><p>For such a model, we can replace the algorithm in (16.9) by the more manageable one:</p><formula xml:id="formula_1491">                             ξ (x) n+1 ∼ P Z ( • | X = x ; θ n ), x ∈ T η (x) n+1 = 1 - 1 n + 1 η (x) n + 1 n + 1 (H(x, ξ (x) n+1 ) -η (x) n ) λ n+1 (θ ′ ) = ψ(θ ′ ) T x∈T η (x) n+1 -log C(θ ′ ) θ n+1 = argmax θ ′ λ n+1 (θ ′ ) (16.10)</formula><p>We leave as an exercise the computation leading to the implementation of this algorithm for mixtures of Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.4.4">Variational approximation</head><p>Returning to proposition 16.2 and (16.6), we see that one can make a variational approximation of the maximum likelihood by computing max <ref type="bibr">(16.11)</ref> where P ⊂ P is a class of p.d.f. with respect to µ Z . The resulting algorithm is then implemented by iterating the computation of g x , x ∈ T , using approximations similar to those provided in section 16.3, and maximization in θ for given g x , x ∈ T . This variational approximation of the maximum likelihood estimator is therefore provided by the following algorithm.</p><formula xml:id="formula_1492">θ∈Θ,g x ∈ P ,x∈T x∈T R Z log f U (x, z ; θ) g x (z) g x (z)µ Z (dz),</formula><p>Algorithm 16.3 (Variational Bayes approximation of the m.l.e.) Let a statistical model with density f U (x, z ; θ) modeling an observable variable X and a latent variable Z be given, and a training set T = (x 1 , . . . , x N ) be observed. Let P be a set of p.d.f. on R Z and define</p><formula xml:id="formula_1493">g( • ; x, θ) = argmin g∈ P R Z log g(z) f U (x, z ; θ) g(z)µ Z (dz)</formula><p>(assuming that this minimizer is uniquely defined).</p><p>Starting with an initial guess of the parameter, θ 0 , iterate the following equation until numerical stabilization:</p><formula xml:id="formula_1494">θ(n + 1) = argmax θ ′ x∈T R Z log (f U (x, z ; θ ′ )) g(z|x ; θ(n))µ Z (dz). (16.12)</formula><p>Assume that the distributions in P are also parametrized, denoting their parameter by η, belonging to some Euclidean domain H. Let g(•; η) denote the p.d.f. in P with parameter η. Letting η = (η x , x ∈ T ) denote an element of H T (parameters in H indexed by elements of the training set), <ref type="bibr">(16.11)</ref> can then be written as the maximization of <ref type="bibr">.13)</ref> This expression is amenable to a stochastic gradient ascent implementation. We have</p><formula xml:id="formula_1495">F(θ, η) = x∈T R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz). (<label>16</label></formula><formula xml:id="formula_1496">∂ θ R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz) = R Z ∂ θ log f U (x, z ; θ)g(z; η x )µ Z (dz)</formula><p>and</p><formula xml:id="formula_1497">∂ η x R Z log f U (x, z ; θ) g(z; η x ) g(z; η x )µ Z (dz) = R Z -∂ η log g(z; η x )g(z; η x ) + log f U (x, z ; θ) g(z; η x ) ∂ η g(z; η x ) µ Z (dz) = R Z log f U (x, z ; θ) g(z; η x ) ∂ η log g(z; η x )g(z; η x )µ Z (dz)</formula><p>Here, we have used the fact that, for all η,</p><formula xml:id="formula_1498">R Z ∂ η log g(z; η)g(z; η) µ Z (dz) = R Z ∂ η g(z; η)µ Z (dz) = 0 16.5. REMARKS since R Z g(x, η)µ Z (dz) = 1.</formula><p>Denote by π η the probability distribution of the random variable Z taking val-</p><formula xml:id="formula_1499">ues in R |T |</formula><p>Z obtained by sampling Z = (Z x , x ∈ T ) such that the components Z x are independent and with p.d.f. g(•; η x ) with respect to µ Z . Define</p><formula xml:id="formula_1500">Φ 1 (θ, z) = x∈T ∂ θ log f U (x, z x ; θ) and Φ 2 (theta, η, z) = x∈T log f U (x, z ; θ) g(z; η x ) ∂ η log g(z x ; η x ).</formula><p>Then, following section 3.3, one can maximize (16.13) using the algorithm</p><formula xml:id="formula_1501">θ n+1 = θ n + γ n+1 Φ 1 (θ n , Z n+1 ) η n+1 = η n + γ n+1 Φ 2 (θ n , η n , Z n+1 ) (16.14)</formula><p>where Z n+1 ∼ π η n .</p><p>Alternatively (for example when T is large), one can also sample from x ∈ T at each update. This would require defining π η as the distribution on T ×R Z with p.d.f. ϕ η (x, z) = g(z; η x )/N , where N = |T |. One can now use</p><formula xml:id="formula_1502">Φ 1 (θ, x, z) = ∂ θ log f U (x, z ; θ) and Φ 2 (θ, η, z) = log f U (x, z ; θ) g(z; η) ∂ η log g(z; η), one can use            θ n+1 = θ n + γ n+1 ∂ θ log f U (X n+1 , Z n+1 ; θ n ) η n+1,X n+1 = η n,X n+1 + γ n+1 log f U (X n+1 , Z n+1 ; θ n ) g(Z n+1 ; η n,X n+1 ) ∂ η log g(Z n+1 ; η n,X n+1 ) (16.15)</formula><p>with (X n+1 , Z n+1 ) ∼ π η n . Sampling from a single training sample at each step can be replaced by sampling from a minibatch with obvious modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.5">Remarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.5.1">Variations on the EM</head><p>Based on the formulation of the EM as the solution of (16.6), it should be clear that solving <ref type="bibr">(16.7)</ref> at each step can be replaced by any update of the parameter that increases <ref type="bibr">(16.6)</ref>. For example, (16.7) can be replaced by a partial run of a gradient ascent algorithm, stopped before convergence. One can also use a coordinate ascent strategy. Assume that θ can be split into several components, say two, so that θ = (θ (1) , θ (2) ). Then, (16.7) may then be split into</p><formula xml:id="formula_1503">θ (1) n+1 = argmax</formula><p>θ (1)  x∈T R Z log f U (x, z ; θ (1) , θ</p><p>(2)</p><formula xml:id="formula_1504">n ) f Z (z | x ; θ(n))µ Z (dz) θ (2) n+1 = argmax θ (2) x∈T R Z log f U (x, z ; θ (1)</formula><p>n+1 , θ (2) )</p><formula xml:id="formula_1505">f Z (z | x ; θ(n))µ Z (dz).</formula><p>Doing so is, in particular, useful when both these steps are explicit, but not (16.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.5.2">Direct minimization</head><p>While the EM algorithm is widely used in the context of partial observations, it is also possible to make explicit the derivative of</p><formula xml:id="formula_1506">log f X (x ; θ) = log R Z f U (x, z ; θ)µ Z (dz)</formula><p>with respect to the parameter θ. Indeed, differentiating the integral and writing</p><formula xml:id="formula_1507">∂ θ f U = f U ∂ θ log f U , we have ∂ θ log f X (x ; θ) = R Z ∂ θ log f U (x, z ; θ) f U (x, z ; θ) f X (x ; θ) µ Z (dz) = R Z ∂ θ log f U (x, z ; θ)f Z (z | x, θ)µ Z (dz).</formula><p>In other terms, the derivative of the log-likelihood of the observed data is the conditional expectation of the derivative of the log-likelihood of the full data given the observed data. When computable, this expression can be used with standard gradient-based optimization methods, such as those described in chapter 3. This expression is also amenable to a stochastic gradient ascent algorithm, namely</p><formula xml:id="formula_1508">θ n+1 = θ n + γ n+1 x∈T ∂ θ f U (x, Z n+1,x , θ n ) (16.16)</formula><p>where Z n+1,x follows the distribution with density f Z (• | x, θ n ) with respect to µ Z . An alternative SGA implementation can use the discussion in section 16.4.4, with the density g η x replaces by f Z (• | x, η x ), which leads to</p><formula xml:id="formula_1509">         θ n+1 = θ n + γ n+1 x∈T ∂ θ log f U (x, Z n+1,x , θ n ) η n+1,x = η n,x -γ n+1 ∂ η x log f Z (Z n+1,x | x, η x ), x ∈ T</formula><p>where Z n+1,x follows the distribution with density f Z (• | x, η n,x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="16.5.3">Product measure assumption</head><p>We have worked, in this chapter, under the assumption that π U was absolutely continuous with respect to a product measure µ U = µ X ⊗ µ Z . This is not a mild assumption, as it fails to include some important cases, for example when X and Z have some deterministic relationship, the simplest instance being when X = F(Z) for some function F. In many cases, however, one can make simple transformations on the model that will make it satisfy this working assumption. For example, if X = F(Z), one can generally split Z into Z = (Z (1) , Z (2) ) so that the equation X = F(Z) is equivalent to Z (2) = G(X, Z (1) ) for some function G. One can then apply the discussion above to U = (X, Z (1) ) instead of U = (X, Z).</p><p>If one is ready to step further into measure theoretic concepts, however, one can see that this product decomposition assumption was in fact unnecessary. Indeed, one can assume that the measure µ U can "disintegrate" in the following sense: there exists, a measure µ X on R X and, for all x ∈ R X , a measure µ Z ( • |x) on R Z such that, for all functions ψ defined on R U ,</p><formula xml:id="formula_1510">R U ψ(x, z)µ U (dx, dz) = R X R Z ψ(x, z)µ Z (dz|x)µ X (dx).</formula><p>This is in fact a fairly general situation <ref type="bibr" target="#b52">[34]</ref> as soon as one assumes that µ U (R) is finite (which is not a real loss of generality as one can reduce to this case by replacing if needed µ U by an equivalent probability distribution).</p><p>With this assumption, the marginal distribution of X had a p.d.f. with respect to µ X given by</p><formula xml:id="formula_1511">f X (x) = R Z f U (x, z)µ Z (dz|x)</formula><p>and the conditional distributions</p><formula xml:id="formula_1512">P Z ( • | x) have a p.d.f. relative to µ Z ( • | x) given by f Z (z | x) = f U (x, z) f X (x) .</formula><p>The computations and approximations made earlier in this chapter can then be applied with essentially no modification.</p><p>Chapter 17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Graphical Models</head><p>We discuss, in this chapter, several methods designed to learn parameters of graphical models, starting with the somewhat simpler case of Bayesian networks, than passing to Markov random fields on loopy graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1">Learning Bayesian networks 17.1.1 Learning a Single Probability</head><p>Since Bayesian networks are specified by probabilities and conditional probabilities of configurations of variables, we start with a discussion of the basic problem of estimating discrete probability distributions.</p><p>The obvious way to estimate the probability of an event A based on a series of N independent experiments is by using relative frequencies</p><formula xml:id="formula_1513">f A = #{A occurs} N .</formula><p>This estimation is unbiased (E(f A ) = P(A)) and its variance is P(A)(1 -P(A))/N . This implies that the relative error δ A = f A /P(A) -1 has zero mean and variance</p><formula xml:id="formula_1514">σ 2 = 1 -P(A) N P(A)</formula><p>.</p><p>This number can clearly become very large when P(A) ≃ 0. In particular, when P(A) is small compared to 1/N , the relative frequency will often be f A = 0, leading to the false conclusion that A is not just rare, but impossible. If there are reasons to expect beforehand that A is indeed possible, it is important to inject this prior belief in the procedure, which suggest using Bayesian estimation methods.</p><p>The main assumption for these methods is to consider the unknown probability, p = P(A), as a random variable, yielding a generative process in which a random probability is first obtained, and then N instances of A or not-A are generated using this probability.</p><p>Assume that the "prior distribution" of p (which determines a prior belief) has a p.d.f. q (with respect to Lebesgue's measure) on the unit interval. Given on N independent observations of occurrences of A, each following a Bernoulli distribution b(p), the joint likelihood of all involved variables is given by</p><formula xml:id="formula_1515">N k p k (1 -p) N -k q(p),</formula><p>where k is the number of times the event A has been observed.</p><p>The conditional density of p given the observation (k occurrences of A) is called the posterior distribution. Here, it is given by</p><formula xml:id="formula_1516">q(p | k) = q(p) C k p k (1 -p) N -k</formula><p>where C k is a normalizing constant. If there was no specific prior knowledge on p (so that q(p) = 1), the resulting distribution is a beta distribution with parameters k + 1 and Nk + 1, the beta distribution being defined as follows. </p><formula xml:id="formula_1517">ρ(t) = Γ (a + b) Γ (a)Γ (b) t a-1 (1 -t) b-1 if t ∈ [0, 1]</formula><p>and ρ(t) = 0 otherwise, with</p><formula xml:id="formula_1518">Γ (x) = ∞ 0 t x-1 e -t dt.</formula><p>From the definition of a beta distribution, it is clear also that, if we choose the prior to be β(a + 1, νa + 1) then the posterior is</p><formula xml:id="formula_1519">β(k + a + 1, N + ν -(k + a) + 1).</formula><p>The posterior therefore belongs to the same family of distributions as the prior, and one says that the beta distribution is a conjugate prior for the binomial distribution. The mode of the posterior distribution (which is the maximum a posteriori (MAP) estimator) is given by p = k + a N + ν .</p><p>This estimator now provides a positive value even if k = 0. By selecting a and ν, one therefore includes the prior belief that p is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1.2">Learning a Finite Probability Distribution</head><p>Now assume that F is a finite space and that we want to estimate a probability distribution p = (p(x), x ∈ F) using a Bayesian approach as above. We cannot use the previous approach to estimate each p(x) separately, since these probabilities are linked by the fact that they sum to 1. We can however come up with a good (conjugate) prior, identified, as done above, by computing the posterior associated to a uniform prior distribution.</p><p>Letting N x be the number of times x ∈ F is observed among N independent samples of a random variable X with distribution P X (•) = p(•), the joint distribution of (N x , x ∈ F) is multinomial, given by</p><formula xml:id="formula_1520">P(N x , x ∈ F | p(•)) = N ! x∈F N x ! x∈F p(x) N x .</formula><p>The posterior distribution of p(•) given the observations with a uniform prior is proportional to x∈F p(x) N x . It belongs to the family of Dirichlet distributions, described in the following definition. Definition 17.2 Let F be a finite set and S F be the simplex defined by</p><formula xml:id="formula_1521">S F =        (p(x), x ∈ F) : p(x) ≥ 0, x ∈ F and x∈F p(x) = 1        .</formula><p>The Dirichlet distribution with parameters a = (a(x), x ∈ F) (abbreviated Dir(a)) has density</p><formula xml:id="formula_1522">ρ(p(•)) = Γ (ν) x∈F Γ (a(x)) x∈F p(x) a(x)-1 , if x ∈ S F</formula><p>and 0 otherwise, with ν = x∈F a(x).</p><p>Note that, if F has cardinality 2, the Dirichlet distribution coincides with the beta distribution. Similarly to the beta for the binomial, and almost by construction, the Dirichlet distribution is a conjugate prior for the multinomial. More precisely, if the prior distribution for p(•) is Dir(1+a(x), x ∈ F), then the posterior after N observations of X is Dir(1 + N x + a(x), x ∈ F), and the MAP estimator is given by</p><formula xml:id="formula_1523">p(x) = N x + a(x) N + ν with ν = x∈F a(x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1.3">Conjugate Prior for Bayesian Networks</head><p>We now consider a Bayesian network on the set F (V ) containing configurations x = (x (s) , s ∈ V ) with x (s) ∈ F s . We want to estimate the conditional probabilities in the representation P(X = x) = s∈V p s (x (pa(s)) , x (s) ).</p><p>Assume that N independent observations of X have been made. Define the counts N s (x (s) , x (pa(s)) ) to be the number of times the observation x ({s}∪pa(s)) has been made. Then, it is straightforward to see that, assuming a uniform prior for the p s , their posterior distribution is proportional to</p><formula xml:id="formula_1524">s∈V x (pa(s)) ∈F pa(s) x (s) ∈F s p s (x (pa(s)) , x (s) ) N s (x (s) ,x (s -) ) .</formula><p>This implies that, for the posterior distribution, the conditional probabilities p s (x (pa(s)) , •) are independent and follow a Dirichlet distribution with parameters 1 + N s (x (s) , x (pa(s)) ), x (s) ∈ F s . So, independent Dirichlet distributions indexed by configurations of parents of nodes provide a conjugate prior for the general Bayesian network model. This prior is specified by a family of positive numbers a s (x (s) , x (pa(s)) ), s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s)) , <ref type="bibr">(17.1)</ref> yielding a prior probability proportional to s∈V x (pa(s)) ∈F pa(s) x (s) ∈F s p s (x (pa(s)) , x (s) ) a s (x (s) ,x (s -) )-1 .</p><p>and a MAP estimator ps (x (pa(s)) , x (s) ) = N s (x (s) , x (pa(s)) ) + a s (x (s) , x (s -) )</p><formula xml:id="formula_1525">N s (x (s -) ) + ν s (x (s -) ) (17.2)</formula><p>where N s (x (pa(s)) ) = x (s) ∈F s N s (x (s) , x (pa(s)) ) and ν s (x (pa(s)) ) = x (s) ∈F s a s (x (s) , x (pa(s)) ).</p><p>One can restrict the huge class of coefficients described by (17.1) to a smaller class by imposing the following condition. Definition 17.3 One says that the family of coefficients a = (a s (x (s) , x (pa(s)) ), s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s))), is consistent if there exists a positive scalar ν and a probability distribution P ′ on F (V ) such that a s (x (s) , x (pa(s)) ) = νP ′ {s}∪pa(s) (x ({s}∪pa(s)) ).</p><p>The class of products of Dirichlet distributions with consistent families of coefficients still provides a conjugate prior for Bayesian networks (the proof being left to the reader). Within this class, the simplest choice (and most natural in the absence of additional information) is to assume that P ′ is uniform, so that</p><formula xml:id="formula_1526">a s (x (s) , x (pa(s)) ) = ν ′ |F ({s} ∪ pa(s))| . (17.3)</formula><p>With this choice, ν ′ is the only parameter that needs to be specified. It is often called the equivalent sample size for the prior distribution.</p><p>We can see from (17.2) that using a prior distribution is quite important for Bayesian networks, since, when the number of parents increases, some configurations on F (pa(s)) may not be observed, resulting in an undetermined value for the ratio N s (x (s) , x (pa(s)) )/N s (x (s -) ), even though, for the estimated model, the probability of observing x (pa(s)) may not be zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1.4">Structure Scoring</head><p>Given a prior defined as a family of Dirichlet distributions associated to a = (a s (x (s) , x (pa(s)) ) for s ∈ V , x (s) ∈ F s , x (pa(s)) ∈ F (pa(s)), the joint density of the observations and parameters is given by</p><formula xml:id="formula_1527">P (x, θ) = s,x (pa(s))</formula><p>D(a s (•, x (pa(s)) ))</p><p>s,x (s) ,x (pa(s))</p><p>p(x (pa(s)) , x (s) ) N s (x (s) ,x (pa(s)) )+a s (x (s) ,x (pa(s)) )-1</p><p>with</p><formula xml:id="formula_1528">D(a(λ), λ ∈ F) = Γ (ν)</formula><p>λ Γ (a(λ)) and ν = λ a(λ). Here, θ represents the parameters of the model, i.e., the conditional distributions that specify the Bayesian network. Note that P (x, θ) is a density over the product space F (V ) × Θ where Θ is the space of all these conditional distributions. The marginal of this likelihood over all possible parameters, i.e., P (x) = P (x, θ)dθ provides the expected likelihood of the sample relative to the distribution of the parameters, and only depends on the structure of the network. In our case, integrating with respect to θ yields</p><formula xml:id="formula_1529">log P (x) = s,x pa(s) log D(a s (•, x (pa(s)) )) D(a s (•, x (pa(s)) ) + N s (•, x (pa(s)) ))</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Letting γ(s, pa(s)) =</head><p>x (pa(s)) log D(a s (•, x (pa(s)) )) D(a s (•, x (pa(s)) ) + N s (•, x (pa(s)) ))</p><p>,</p><formula xml:id="formula_1530">the decomposition log P (x) = s∈V γ(s, pa(s))</formula><p>expresses this likelihood as a sum of "scores" (associated to each node and its parents), which depends on the observed sample. The scores that are computed above are often called Bayesian scores because they derive from a Bayesian construction. One can also consider simpler scores, such as penalized likelihood:</p><formula xml:id="formula_1531">γ(s, pa(s)) = - x (pa(s)) Ĥ(X (s) | X (pa(s)) ) |F (pa(s))| -ρ|pa(s)|,</formula><p>where Ĥ is the conditional entropy for the empirical distribution based on observed samples. Structure learning algorithms <ref type="bibr" target="#b162">[144,</ref><ref type="bibr" target="#b126">108]</ref> are designed to optimize such scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.1.5">Reducing the Parametric Dimension</head><p>In the previous section, we estimated all conditional probabilities intervening in the network. This is obviously a lot of parameters and, even with a regularizing prior, the estimated values are likely to be be inaccurate for small sample sizes. It then becomes desirable to simplify the parametric complexity of the model.</p><p>When the sets F s are not too large, which is common in practice, the parametric explosion is due to the multiplicity of parents, since the number of conditional probabilities p s (x (pa(s)) , •) grows exponentially with |pa(s)|. One way to simplify this is to assume that the conditional probability at s only depends on x (pa(s)) via some "global-effect" statistic g s (x (pa(s)) ). The idea, of course, is that the number of values taken by g s should remain small, even if the number of parents is large.</p><p>Examples of some functions g s can be max(x (t) , t ∈ pa(s)), or the min, or some simple (quantized) function of the sum. With binary variables (F s = {0, 1}), logical operators are also available ("and", "or", "xor"), as well as combinations of them. The choice made for the functions g s is part of building the model, and would rely on the specific context and prior information on the process, which is always important to account for, in any statistical problem.</p><p>Once the g s 's are fixed, learning the network distribution, which is now given by π(x) = s∈V p s (g s (x (pa(s)) ), x (s) ) can be done exactly as before, the parameters being all p s (w, λ), λ ∈ F s , w ∈ W s , where W s is the range of g s , and Dirichlet priors can be associated to each p s (w, •) for s ∈ V and w ∈ W s . The counts provided in <ref type="bibr">(17.</ref>3) now can be chosen as</p><formula xml:id="formula_1532">a s (x s , w) = ν ′ |F| |g -1 s (w)| . (17.4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2">Learning Loopy Markov Random Fields</head><p>Like everything else, parameter estimation for loopy networks is much harder than with trees or Bayesian networks. There is usually no closed form expression for the estimators, and their computation relies on more or less tractable numerical procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.1">Maximum Likelihood with Exponential Models</head><p>In this section, we consider a parametrized model for a Gibbs distribution</p><formula xml:id="formula_1533">π θ (x) = 1 Z θ exp(-θ T U (x)) (17.5)</formula><p>where θ is a d-dimensional parameter and U is a function from F (V ) to R d . For example, if π is an Ising model with</p><formula xml:id="formula_1534">π(x) = 1 Z exp α s∈V x (s) + β s∼t x (s) x (t) ,</formula><p>then θ = (α, β) and U (x) = -( s x (s) , s∼t x (s) x (t) ). Most of the Markov random fields models that are used in practice can be put in this form. The constant Z θ in (17.5) is</p><formula xml:id="formula_1535">Z θ = x∈F (V ) exp(-θ T U (x))</formula><p>and is usually not computable. Now, assume that an N -sample, x 1 , . . . , x N , is observed for this distribution. The maximum likelihood estimator maximizes</p><formula xml:id="formula_1536">ℓ(θ) = 1 N N k=1 log π θ (x k ) = -θ T ŪN -log Z θ with ŪN = (U (x 1 ) + • • • + U (x N ))/N .</formula><p>We have the following proposition, which is a well-known property of exponential families of probabilities. where E θ denotes the expectation with respect to π θ and Var θ the covariance matrix under the same distribution.</p><p>We skip the proof, which is just computation. This proposition implies that a local maximum of θ → ℓ(θ) must also be global. Any such maximum must be a solution of</p><formula xml:id="formula_1537">E θ (U ) = ŪN (x 0 )</formula><p>and conversely. There are some situations in which the maximum does not exist, or is not unique. Let us first discuss the second case.</p><p>If several solutions exist, the log-likelihood cannot be strictly concave: there must exist at least one θ for which Var θ (U ) is not definite. This implies that there exists a nonzero vector u such that var θ (u T U ) = u T Var θ (U )u = 0. This is only possible when u T U (x) = cst for all x ∈ F V . Conversely, if this is true, Var θ (U ) is degenerate for all θ.</p><p>So, the non-uniqueness of the solutions is only possible when a deterministic affine relation exists between the components of U , i.e., when the model is overdimensioned. Such situations are usually easily dealt with by removing some parameters. In all other cases, there exists at most one maximum.</p><p>For a concave function like ℓ to have no maximum, there must exist what is called a direction of recession <ref type="bibr" target="#b185">[167]</ref>, which is a direction α ∈ R d such that, for all θ, the function t → ℓ(θ + tα) is increasing. In this case the maximum is attained "at infinity". Denoting U α (x) = α T U (x), the derivative in t of ℓ(θ + tα) is</p><formula xml:id="formula_1538">E θ+tα (U α ) -Ūα where Ūα = α T</formula><p>ŪN . This derivative is positive for all t if and only if</p><formula xml:id="formula_1539">Ūα = U * α := min{U α (x), x ∈ F (V )} (17.8)</formula><p>and U α is not constant. To prove this, assume that the derivative is positive. Then U α is not constant (otherwise, the derivative would be zero). Let F * α ⊂ F (V ) be the set of configurations x for which U α (x) = U * α . Then</p><formula xml:id="formula_1540">E θ+tα (U α ) = x∈F (V ) U α (x) exp(-θ T U (x) -tU α (x)) x∈F (V ) exp(-θ T U (x) -tU α (x)) = x∈F (V ) U α (x) exp(-θ T U (x) -t(U α (x) -U * α )) x∈F (V ) exp(-θ T U (x) -t(U α (x) -U * α )) = U * α x∈F * α exp(-θ T U (x)) + x F * α U α (x) exp(-θ T U (x) -t(U α (x) -U * α )) x∈F * α exp(-θ T U (x)) + x F * α exp(-θ T U (x) -t(U α (x) -U * α ))</formula><p>.</p><p>When t tends to +∞, the sums over x F * α tend to 0, which implies that E θ+tα (U α ) tends to U * α . So, if E θ+tα (U α ) -Ūα &gt; 0 for all t, then Ūα = U * α and U α is not constant. The converse statement is obvious.</p><p>As a conclusion, the function ℓ has a finite maximum if and only if there is no direction α ∈ R d such that α T (U (x)-ŪN ) ≤ 0 for all x ∈ F (V ). Equivalently, ŪN must belong to the interior of the convex hull of the finite set</p><formula xml:id="formula_1541">{U (x), x ∈ F (V )} ⊂ R d .</formula><p>In such a case, that we hereafter assume, computing the maximum likelihood estimator boils down to solving the equation</p><formula xml:id="formula_1542">E θ (U ) = ŪN .</formula><p>Because the maximization problem is concave, we know that numerical algorithms such as gradient ascent,</p><formula xml:id="formula_1543">θ(t + 1) = θ(t) + ϵ(E θ(t) (U ) -ŪN ),<label>(17.9)</label></formula><p>converge to the optimal parameter. Unfortunately, the computation of the expectations and covariance matrices can only be made explicitly for acyclic models, for which parameter estimation is not a problem anyway. For general loopy graphical models, the expectation can be estimated iteratively using Monte-Carlo methods. It turns out that this estimation can be synchronized with gradient descent to obtain a consistent algorithm, which is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.2">Maximum likelihood with stochastic gradient ascent</head><p>As remarked above, for fixed θ, we have designed, in chapter 14, Markov chain Monte Carlo algorithms that asymptotically sample form π θ . Select one of these algorithms, and let p θ be the corresponding transition probabilities for a given θ, so that p θ (x, y) = P(X n+1 = y | X n = x) for the sampling chain. Then, define the iterative algorithm, initialized with arbitrary θ 0 and x 0 ∈ F (V ), that loops over the following two steps.</p><p>(SG1) Sample from the distribution p θ t (x t , •) to obtain a new configuration x t+1 .</p><p>(SG2) Update the parameter using</p><formula xml:id="formula_1544">θ t+1 = θ t + γ t+1 (U (x t+1 ) -ŪN ).<label>(17.10)</label></formula><p>This algorithm differs from the situation considered in section 3.3 in that the distribution of the sampled variable x t+1 depends on both the current parameter θ t and on the current variable x t . Convergence requires additional constraints on the size of the gains γ(t) and we have the following theorem <ref type="bibr" target="#b224">[206]</ref>.</p><p>Theorem 17.5 If p θ corresponds to the Gibbs sampler or Metropolis algorithm, and γ t+1 = ϵ/(t +1) for small enough ϵ, the algorithm that iterates (SG1) and (SG2) converges almost surely to the maximum likelihood estimator.</p><p>The speed of convergence of such algorithms depends both on the speed of convergence of the Monte-Carlo sampling and of the original gradient ascent. The latter can be improved somewhat with variants similar to those discussed in section 3.3, for example by choosing data-adaptive gains as in the ADAM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.3">Relation with Maximum Entropy</head><p>The maximum likelihood estimator is closely related to what is called the maximum entropy extension of a set of constraints. Let the function U from F (V ) to R d be given. An element u ∈ R d is said to be a consistent assignment for U if there exists a probability distribution π on F (V ) such that E π (U ) = u. An example of consistent assignment is any empirical average Ū based on a sample (x (1) , . . . , x (N ) ), since Ū = E π (U ) for</p><formula xml:id="formula_1545">π = 1 N N k=1 δ x (k) .</formula><p>Given U and a consistent assignment, u, the associated maximum entropy extension is defined as a probability distribution π maximizing the entropy, H(π), subject to the constraint E π (U ) = u. This is a convex optimization problem, with constraints</p><formula xml:id="formula_1546">                     x∈F (V ) π(x) = 1 x∈F (V ) U j (x)π(x) = u j , j = 1, . . . , d π(x) ≥ 0, x ∈ F (V ) (17.11)</formula><p>Because the entropy is strictly convex, there is a unique solution to this problem. We first discuss non-positive solutions, i.e., solutions for which π(x) = 0 for some x. An important fact is that, if, for a given x, there exists π 1 such that E π 1 (U ) = u and π 1 (x) &gt; 0, then the optimal π must also satisfy π(x) &gt; 0. This is because, if π(x) = 0, then, letting π ϵ = (1ϵ)π + ϵπ 1 , we have E π ϵ (U ) = u since this constraint is linear, π ϵ (x) &gt; 0 and</p><formula xml:id="formula_1547">H(π ϵ ) -H(π) = - y,π(y)&gt;0 (π ϵ (y) log π ϵ (y) -π(y) log π(y)) - y,π(y)=0 ϵπ 1 (y)(log(ϵ) + log π 1 (y)) = -ϵ log ϵ y,π(y)=0 π 1 (y) + O(ϵ)</formula><p>which is positive for small enough ϵ, contradicting the fact that π is a maximizer.</p><p>Introduce the set N u containing all configurations x ∈ F (V ) such that π(x) = 0 for all π such that E π (U ) = u. Then we know that the maximum entropy extension satisfies π(x) &gt; 0 if x N u . Introduce Lagrange multipliers θ 0 , θ 1 , . . . , θ d for the d + 1 equality constraints in <ref type="bibr">(17.11)</ref>, and the Lagrangian</p><formula xml:id="formula_1548">L = H(π) + x∈F (V )\N u (θ 0 + θ T U (x))π(x)</formula><p>in which we have set θ = (θ 1 , . . . , θ d ), we find that the optimal π must satisfy</p><formula xml:id="formula_1549">               log π(x) = -θ 0 -1 -θ T U (x) x π(x) = 1 E π (U ) = ū</formula><p>In other terms, the maximum entropy extension is characterized by</p><formula xml:id="formula_1550">π(x) = 1 Z θ exp(-θ T U (x))1 N c u (x)</formula><p>and E π (U ) = u.</p><p>In particular, if N u = ∅, then the maximum entropy extension is positive. If, in addition, u = Ū for some observed sample, then it coincides with the maximum likelihood estimator for <ref type="bibr">(17.5)</ref>. Notice that, in this case, the condition N u ∅ coincide with the condition that there exists α such that α T U (x) ≥ α T u for all x, with α T U (x) not constant. Indeed, assume that the latter condition is true. Then, if E π (U ) = u, then E π (α T U ) = α T u, which is only possible if π(x) = 0 for all x such that α T U (x) &lt; α T u. Such x's exist by assumption, and therefore N u ∅. Conversely, assume N u ∅. If condition (17.8) is not satisfied, then we have shown when discussing maximum likelihood that an optimal parameter for the exponential model would exist, leading to a positive distribution for which E π (U ) = u, which is a contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.4">Iterative Scaling</head><p>Iterative scaling is a method that is well-adapted to learning distributions given by (17.5), when U can be interpreted as a random histogram, or a collection of them. More precisely, assume that for all x ∈ F (V ), one has</p><formula xml:id="formula_1551">U (x) = (U 1 (x), . . . , U q (x)) with q j=1 U j (x) = 1 and U j (x) ≥ 0.</formula><p>Let the parameter be given by θ = (θ 1 , . . . , θ q ). Assume that x 1 , . . . , x N have been observed, and let u ∈ R d be a consistent assignment for U , with u j &gt; 0 for j = 1, . . . , d and such that N u = ∅. Iterative scaling computes the maximum entropy extension of E π (U ) = u, that we will denote π * . It is supported by the following lemma. Lemma 17.6 Let π be a probability on F (V ) with π &gt; 0 and define</p><formula xml:id="formula_1552">π ′ (x) = π(x) ζ d j=1 u j E π (U j ) U j (x)</formula><p>where ζ is chosen so that π ′ is a probability. Then π ′ &gt; 0 and KL(π * ∥π ′ ) -KL(π * ∥π) ≤ -KL(u∥E π (U )) ≤ 0 (17.12)</p><p>Proof Note that, since π &gt; 0, E π (U j ) must also be positive for all j, since E π (U j ) = 0 would otherwise imply U j = 0 and u j = 0 for u to be consistent. So, π ′ is well defined and obviously positive.</p><p>We have</p><formula xml:id="formula_1553">KL(π * ∥π ′ ) -KL(π * ∥π) = log ζ - x∈F (V ) π * (x) d j=1 U j (x) log u j E π (U j ) = log ζ - d j=1 u j log u j E π (U j ) = log ζ -KL(u∥E π (U )).</formula><p>(We have used the identity E π * (U ) = u.) So it suffices to prove that ζ ≤ 1. We have</p><formula xml:id="formula_1554">ζ = x∈F (V ) π(x) d j=1 u j E π (U j ) U j (x) ≤ d j=1 x∈F (V ) π(x)U j (x) u j E π (U j ) = d j=1 E π (U j ) u j E π (U j ) = 1,</formula><p>which proves the lemma (we have used the fact that, for x i , w i positive numbers with</p><formula xml:id="formula_1555">i w i = 1, one has i x w i i ≤ i w i x i</formula><p>, which is a consequence of the concavity of the logarithm).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Consider the iterative algorithm</p><formula xml:id="formula_1556">π n+1 (x) = π n (x) ζ n d j=1 u j E π n (U j ) U j (x)</formula><p>initialized with a uniform distribution. Equivalently, using the exponential formulation, define, for j = 1, . . . , d, θ n+1,j = θ n,j + log E θ n (U j )</p><formula xml:id="formula_1557">u j + KL(u∥E θ n (U )),<label>(17.13)</label></formula><p>with π θ given by (17.5), initialized with θ 0 = 0. Note that adding a term that is independent of j to θ does not change the value of π θ , because the U j 's sum to 1. The model is in fact overparametrized, and the addition of the KL divergence in (17.13) ensures that d i=1 u i θ i = 0 at all steps.</p><p>This algorithm always reduces the Kullback-Leibler distance to the maximum entropy extension. This distance being always positive, it therefore converges to a limit, which, still according to lemma 17.6, is only possible if KL(u∥E π n (U )) also tends to 0, that is E π n (U ) → u. Since the space of probability distributions is compact, the Heine-Borel theorem implies that the sequence π θ n has at least one accumulation point, that we now identify. If π is such a point, one must have E π (U ) = u. Moreover, we have π &gt; 0, since otherwise KL(π * ∥π) = +∞. To prove that π = π * (and therefore the limit of the sequence), it remains to show that it can be put in the form <ref type="bibr">(17.5)</ref>. For this, define the vector space V of functions v : F (V ) → R which can be written in the form</p><formula xml:id="formula_1558">v(x) = α 0 + g j=1 α j U j (x).</formula><p>Since log π θ n ∈ V for all n, so is its limit, and this proves that log π belongs to V . We have obtained the following proposition.</p><p>Proposition 17.7 Assume that for all x ∈ F (V ), one has U (x) = (U 1 (x), . . . , U d (x)) with d j=1 U j (x) = 1 and U j (x) ≥ 0.</p><p>Let u be a consistent assignment for the expectation of U such thatN u = ∅. Then, the algorithm described in <ref type="bibr">(17.13</ref>) converges to the maximum entropy extension of u. This is the iterative scaling algorithm. This method can be extended in a straightforward way to handle the maximum entropy extension for a family of functions U (1) , . . . , U (K) , such that, for all x and for all k, U (k) (x) is a d k -dimensional vector such that</p><formula xml:id="formula_1559">d k j=1 U (k) j (x) = 1.</formula><p>The maximum entropy extension takes the form</p><formula xml:id="formula_1560">π θ (x) = 1 Z θ exp - K k=1 (θ (k) ) T U (k) (x) ,</formula><p>where θ (k) is d k -dimensional, and iterative scaling can then be implemented by updating only one of these vectors at a time, using (17.13) with U = U (k) .</p><p>The restriction to U (x) providing a discrete probability distribution for all x is, in fact, no loss of generality. This is because adding a constant to U does not change the resulting exponential model in <ref type="bibr">(17.5)</ref>, and multiplying U by a constant can be also compensated by dividing θ by the same constant in the same model. So, if u -is a lower bound for min j,x U j (x), one can replace U by (Uu -), and therefore assume that U ≥ 0, and if u + is an upper bound for j U j (x), we can replace U by U /u + and therefore assume that j U j (x) ≤ 1. Define</p><formula xml:id="formula_1561">U d+1 (x) = 1 - d j=1 U j (x) ≥ 0.</formula><p>Then, the maximum entropy extension for (U 1 , . . . , U d ) with assignment (u 1 , . . . , u d ) is obviously also the extension for (U 1 , . . . , U d+1 ), with assignment (u 1 , . . . , u d+1 ), where</p><formula xml:id="formula_1562">u d+1 = 1 - d j=1 u j ,</formula><p>and the latter is in the form required in proposition 17.7. Note that iterative scaling requires to compute the expectation of U 1 , . . . , U d before each update. These are not necessarily available in closed form and may have to be estimated using Monte-Carlo sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.5">Pseudo likelihood</head><p>Maximum likelihood estimation is a special case of minimal contrast estimators. These estimators are based on the definition of a measure of dissimilarity, say C(π∥ π), between two probability distributions π and π. The usual assumptions on C are that C(π∥ π) ≥ 0, with equality if and only if π = π, and that C is -at least -continuous in π and π. Minimal contrast estimators approximate the problem of minimizing θ → C(π true ∥π θ ) over a parameter θ ∈ Θ, (which is not feasible, since π true , the true distribution of the data, is unknown) by the minimization of θ → C( π∥π θ ) where π is the empirical distribution computed from observed data. Under mild conditions on C, these estimators are generally consistent when N tends to infinity, which means that the estimated parameter asymptotically (in the sample size N ) provides the best (according to C) approximation of π true by the family π θ , θ ∈ Θ.</p><p>The contrast that is associated with maximum likelihood is the Kullback-Leibler divergence. Indeed, given a sample x 1 , . . . , x N , we have</p><formula xml:id="formula_1563">KL( π∥π θ ) = E π log π -E π log π θ = E π log π - N k=1 log π θ (x k ).</formula><p>Since E π log π does not depend on θ, minimizing KL( π∥π θ ) is equivalent to maximizing N k=1 log π θ (x k ) which is the log-likelihood.</p><p>Maximum pseudo-likelihood estimators form another class of minimal contrast estimators for graphical models. Given a distribution π on F (V ), define the local specifications π s (x (s) | x (t) , t s) to be conditional distributions at one vertex given the others, and the contrast</p><formula xml:id="formula_1564">C(π∥ π) = s∈V E π (log π s πs ).</formula><p>Because we can write, using standard properties of conditional expectations,</p><formula xml:id="formula_1565">C(π∥ π) = s∈V E π E π s (log π s πs ) = s∈V E(KL(π s (• | X (t) , t s)∥ πs (• | X (t) , t s)),</formula><p>we see that C(π, π) is always positive, and vanishes (under the assumption of positive π) only if all the local specifications for π and π coincide, and this can be shown to imply that π = π. Indeed, for any x, y ∈ F (V ), and choosing some order V = {s 1 , . . . , s n } on V , one can write</p><formula xml:id="formula_1566">π(x) π(y) = n k=1 π(x (s k ) |x (s 1 ) , . . . , x (s k-1</formula><p>) , y (s k+1 ) , . . . , y (s n ) ) π(x (s k ) |x (s 1 ) , . . . , x (s k-1 ) , y (s k+1 ) , . . . , y (s n ) )</p><p>and the ratios π(x)/π(y), for x ∈ F (V ), combined with the constraint that x π(x) = 1 uniquely define π.</p><p>So C is a valid contrast and</p><formula xml:id="formula_1567">C( π∥π θ ) = s∈V E π log πs - s∈V N k=1 log π θ,s (x (s) k |x (t) k , t s).</formula><p>This yields the maximum pseudo-likelihood estimator (or pseudo maximum likelihood) defined as a maximizer of the function (called log-pseudo-likelihood)</p><formula xml:id="formula_1568">θ → s∈V N k=1 log π θ,s (x (s) k |x (s) k , t s).</formula><p>Although maximum likelihood is known to provide the most accurate approximations in many cases, maximum of pseudo likelihood has the important advantage to be, most of the time, computationally feasible. This is because, for a model like <ref type="bibr">(17.5)</ref>, local specifications are given by</p><formula xml:id="formula_1569">π θ,s (x (s) | x (t) , t s) = exp(-θ T U (x)) y (s) ∈F s exp(-θ T U (y (s) ∧ x (V \s) ))</formula><p>. and therefore include no intractable normalizing constant. Maximum of pseudolikelihood estimators can be computed using standard maximization algorithms.</p><p>For exponential models such as (17.5), the log-pseudo-likelihood is, like the loglikelihood, a concave function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.2.6">Continuous variables and score matching</head><p>The methods that were presented so far for discrete variables formally generalize to more general state spaces, even though consistency or convergence issues in noncompact cases can be significantly harder to address. Score matching is a parameter estimation method that was introduced in <ref type="bibr" target="#b113">[95]</ref> and was designed, in its original version, to estimate parameters for statistical models taking the form</p><formula xml:id="formula_1570">π θ (x) = 1 C(θ) exp (-F(x, θ))</formula><p>with x ∈ R d . We assume below suitable integrability and differentiability conditions, in order to justify differentiation under integrals whenever they are needed. The "score function" is defined as</p><formula xml:id="formula_1571">s(x, θ) = -∇ x log π θ (x) = ∇ x F(x, θ)</formula><p>where ∇ x denotes the gradient with respect to the x variable. Letting π true denote the p.d.f. of the true data distribution (not necessarily part of the statistical model), score matching minimizes</p><formula xml:id="formula_1572">f (θ) = R d |s(x, θ) -s true (x)| 2 π true (x)dx</formula><p>where s true = -∇ log π true . This integral can be restricted to the support of π true , if we don't want to assume that π true is non-vanishing. Note, however that f (θ) = 0 implies that log π θ (•, θ) = log π true π true -almost everywhere, so that π θ (x) = cπ true (x) for some constant c and x in the support of π true . Only if π true (x) &gt; 0 for all x ∈ R d , can we conclude that this requires π θ = π true .</p><p>Expanding the squared norm and applying the divergence theorem yield</p><formula xml:id="formula_1573">f (θ) = R d |∇ x log π θ (x)| 2 π true (x)dx -2 R d ∇ x log π θ (x) T ∇π true (x)dx + R d |s true (x)| 2 π true (x)dx = R d |∇ x log π θ (x)| 2 π true (x)dx + 2 R d ∆ log π θ (x) T π true (x)dx + R d |s true (x)| 2 dx</formula><p>To justify the use of the divergence theorem, one needs to assume two derivatives in the log-likelihoods with sufficient decay at infinity (see Hyvärinen and Dayan <ref type="bibr" target="#b113">[95]</ref> for details). This shows that minimizing f is equivalent to minimizing</p><formula xml:id="formula_1574">g(θ) = R d |∇ x log π θ (x)| 2 π true (x)dx + 2 R d ∆ log π θ (x) T π true (x)dx = E(|∇ x log π θ (X)| 2 + 2∆ log π θ (X)).</formula><p>In this form, the objective function can be approximated by a sample average, so that, given observed data x 1 , . . . , x N , one can define the score-matching estimator as a minimizer of</p><formula xml:id="formula_1575">N k=1 |∇ x log π θ (x k )| 2 + 2∆ log π θ (x k ) . (<label>17.14)</label></formula><p>Remark 17.8 The method can be adapted to deal with discrete variables replacing derivatives with differences. Let X take values in a finite set, R X , on which a graph structure can be defined, writing x ∼ y if x and y are connected by an edge. For example, if X is itself a Markov random field on a graph G = (V , E), so that R X = F (V ), one can define x ∼ y if and only if x (s) = y (s) for all but one s ∈ V . One can then define the score function</p><formula xml:id="formula_1576">s θ (x, y) = 1 - π θ (y) π θ (x)</formula><p>defined over all x, y ∈ R X such that x ∼ y. Now the score matching functional is</p><formula xml:id="formula_1577">f (θ) = x∈R X         y∼x |s θ (x, y) -s * (x, y)| 2         π * (x),</formula><p>whose minimization is, after reordering terms, equivalent to that of</p><formula xml:id="formula_1578">g(θ) = x∈R X y∼x 1 - π θ (y) π θ (x) 2 π * (x) + 2 x∈R X y∼x π θ (x) π θ (y) - π θ (y) π θ (x) π * (x).</formula><p>Based on training data, a discrete score matching estimator is a minimizer of Missing variable sin the context of graphical models may correspond to real processes that cannot be measured, which is common, for example, with biological data. They may be more conceptual objects that are interpretable but are not parts of the data acquisition process, like phonemes in speech recognition, or edges and labels in image processing and object recognition. They may also be variables that have been added to the model to increase its parametric dimension without increasing the complexity of the graph. However, as we will see, dealing with incomplete or imperfect observations brings the parameter estimation problem to a new level of difficulty.</p><formula xml:id="formula_1579">N k=1 y∼x k 1 - π θ (y) π θ (x k ) 2 + 2 N k=1 y∼x k π θ (x k ) π θ (y) - π θ (y) π θ (x k ) . (<label>17</label></formula><p>Since it is the most common approach to address incomplete or noisy observations, we start with a description of how the EM algorithm (Algorithm 16.1) applies to graphical models, and of its limitations. We assume a graphical model on an undirected graph G = (V , E), in which we assume that V is separated in two nonintersecting subsets, V = S ∪ H. Letting X be a G-Markov random field, the part X (S) is assumed to be observable, and X (H) is hidden.</p><p>We assume that X takes values in F (V ), where we still denote by F s the sets in which X s takes values for s ∈ V . We let the model distribution belong to an exponential family, with</p><formula xml:id="formula_1580">π θ (x) = 1 Z(θ) exp -θ T U (x) , x ∈ F (V ).<label>(17.16)</label></formula><p>Assume that an N -sample x</p><formula xml:id="formula_1581">(S) 1 , . . . , x (S)</formula><p>N is observed over S. Since</p><formula xml:id="formula_1582">log π θ (x) = -log Z(θ) -θ T U (x),</formula><p>the transition from θ n to θ n+1 in Algorithm 16.1 is done by maximizing</p><formula xml:id="formula_1583">-log Z(θ) -θ T Ūn (<label>17.17)</label></formula><p>where</p><formula xml:id="formula_1584">Ūn = 1 N N k=1 E θ n (U (X) | X (S) = x (S) k ).<label>(17.18)</label></formula><p>So, the M-step of the EM, which maximizes (17.17), coincides with the completedata maximum-likelihood problem for which the empirical average of U is replaced by the average of its conditional expectations given the observations, as given in <ref type="bibr">(17.18)</ref>, which constitutes the E-step. As a consequence, a strict application of the EM algorithm for graphical models is unfeasible, since each step requires running an algorithm of similar complexity maximum likelihood for complete data, that we already identified as a challenging, computationally costly problem. The same remark holds for the SAEM algorithm of section 16.4.3, which also requires solving a maximum likelihood problem at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.2">Stochastic gradient ascent</head><p>The stochastic gradient ascent described in section 17.2.2 can be extended to partial observations <ref type="bibr" target="#b225">[207]</ref>, even though it loses the global convergence guarantee that resulted from the concavity of the log-likelihood for complete observations. Indeed, applying the computation of section 16.5.2, to a model given by (17.16), we get using proposition 17.4,</p><formula xml:id="formula_1585">∂ θ log ψ θ = E θ (E θ (U ) -U | X (S) = x (S) ) = E θ (U ) -E θ (U | X (S) = x (S) )</formula><p>where we ψ θ (x (S) ) denotes the marginal distribution of π θ on S.</p><p>Let π θ (x (H) | x (S) ) denotes the conditional probability P (X (H) = x (H) | X (S) = s (S) ) for the distribution π θ , therefore taking the form</p><formula xml:id="formula_1586">π θ (x (H) | x (S) ) = 1 Z(θ, x (S) ) exp -θ T U (x (S) ∧ x (H) ) .</formula><p>Assume given an ergodic transition probability p θ on F (V ), and a family of ergodic transition probabilities p x (S) θ , x (S) ∈ F (S), such that the invariant distribution of p θ is π θ , and the one of p x (S)  θ is π θ (• | x (S) ). Then the following SGA algorithm can be used to estimate θ Algorithm 17.1 Start the algorithm with an initial parameter θ(0) and initial configurations x(0) and</p><formula xml:id="formula_1587">x (H) k (0), k = 1, . . . , N . Then, at step n, (SGH1) Sample from the distribution p θ(n) (x(n), •) to obtain new configurations x(n+ 1) ∈ F (V ). (SGH2) For k = 1, . . . , N , sample from the distribution p x (S) k θ(n) (x (H) k (n), •) to obtain a new configuration x ( )</formula><p>k H(n + 1) over the hidden vertexes. (SGH3) Update the parameter using</p><formula xml:id="formula_1588">θ(n + 1) = θ(n) + γ(n + 1)        U (x(n + 1)) - 1 N N k=1 U (x (S) k ∧ x (H) k (n + 1))        . (17.19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.3">Pseudo-EM Algorithm</head><p>The EM update</p><formula xml:id="formula_1589">θ n+1 = argmax θ N k=1 E θ n log π θ (X) | X (S) = x (S) k .</formula><p>being challenging for Markov random fields, it is tempting to replace the log-likelihood in the expectation by an other contrast, such as the log-pseudo-likelihood. A similar approach to that described here was introduced in Chalmond <ref type="bibr" target="#b69">[51]</ref>, for situations when the conditional distribution of X (S) given X (H) is "simple enough" (for example, if the variables X s , s ∈ S are conditionally independent given X (H) ) and when the cardinality of the sets F s , s ∈ H is small (binary, or ternary, variables).</p><p>The algorithm has the following variational interpretation. Fix x (S) ∈ F (S) and s ∈ H. Also denote</p><formula xml:id="formula_1590">µ s = 1/|F (H \ {s})|. If q is a transition probability from F (H \ {s}) to F s , let ∆ (s) θ (q, x (S) ) = y∈F (H) log π θ,s (y (s) ∧ x (S) | y (H\{s}) )</formula><p>q(y (H\{s}) , y (s) )µ s q(y (H\{s}) , y (s) )µ s . <ref type="bibr">(17.20)</ref> This function is concave in q, since its first partial derivative with respect to q(y (H\{s}) , y (s) ) (for each y ∈ F (H)) is given by µ s log π θ,s (y (s) ∧ x (S) | y (H\{s}) )µ s (y (H\{s}) )µ s log(q(y (H\{s}) , y (s) )µ s )µ s so that its Hessian is the diagonal matrix with negative entries -µ s /q(y (H\{s}) , y (s) ).</p><p>Using Lagrange multipliers to express the constraints y (s) ∈F s q(y (H\{s}) , y (s) ) = 1 for all y (H\{s}) , we find that ∆ (s) θ (q, x (S) ) is maximized when q(y (H\{s}) , y (s) ) is proportional to π θ,s (y (s) ∧ x (S) | y (H\{s}) ), yielding q(y (H\{s}) , y (s) ) = π θ,s (y (s) | x (S) ∧ y (H\{s}) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now, consider the problem of maximizing</head><formula xml:id="formula_1591">N k=1 s∈H ∆ (n) θ (q (s) k , x (s) k ) (17.21)</formula><p>with respect to θ and q (s)</p><p>k , k = 1, . . . , N , s ∈ H. Consider an iterative maximization scheme in which, from a current parameter θ n , one first, maximizes (17.21) with respect to transition probabilities q (s) k , then with respect to θ to obtain θ n+1 . This scheme provides the iteration</p><formula xml:id="formula_1592">θ n+1 = argmax θ N k=1 s∈H y∈F (H) log π θ,s (y (s) ∧ x (S) k | y (H\{s}) ) π θ n ,s (y (s) | x (S) k ∧ y (H\{s}) )µ s .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.4">Partially-observed Bayesian networks on trees</head><p>We now consider the situation in which the joint distribution of X = X (S) ∧ X (H) is a Bayesian network over a directed acyclic graph G = (V , E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assume that x</head><formula xml:id="formula_1593">(S) 1 , . . . , x (S)</formula><p>N are observed. The parameter θ is the collection of all p(x (pa(s)) , x (s) ) for s ∈ V . Define the random variables I s,x (y) equal to one if y ({s}∪pa(s)) = x ({s}∪pa(s)) and zero otherwise. We can write log π(y) = s∈S log p s (y (pa(s)) , y (s) ) = s∈S x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) )I s,x (y) This implies that</p><formula xml:id="formula_1594">N k=1 E θ n log π(x (S) k , X (H) ) | X (S) = x (S) k = x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) ) N k=1 E θ n (I s,x (X) | X (S) = x (S) k ) = x ({s}∪pa(s)) ∈F ({s}∪pa(s)) log p s (x (pa(s)) , x (s) ) N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k ).</formula><p>The EM iteration at step n then is</p><formula xml:id="formula_1595">p (n+1) s (x (pa(s)) , x (s) ) = 1 Z s (x (s -) ) N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k ) with π θ n (x) = s∈V p (n) (x (pa(s)) , x (s) ),</formula><p>Z s being a normalization constant.</p><p>If the estimation is solved with a Dirichlet prior Dir(1+a s (x (s) , x (pa(s)) )), the update formula becomes</p><formula xml:id="formula_1596">p (n+1) s (x (pa(s)) , x (s) ) = 1 Z s (x (s -) )        a s (x (s) , x (pa(s)) ) + N k=1 π θ n (x ({s}∪pa(s)) | X (S) = x (S) k )        . (17.22)</formula><p>This algorithm is very simple when the conditional distributions π θ n (x (s∪pa(s)) | X (S) = x (S) k ) can be easily computed, which is not always the case for a general Bayesian network, since conditional distributions do not always have a structure of Bayesian network. The computation is simple enough for trees, however, since conditional tree distributions are still trees (or forests). More precisely, the conditional distribution given the observed variables can be written in the form</p><formula xml:id="formula_1597">π(y (H) | x (S) ) = 1 Z(x (S) ) s∈H ϕ s,x (y (s) )</formula><p>t∼s,{s,t}⊂H ϕ st (y (s) , y (t) )</p><p>with ϕ s,pa(s) (y (s) , y (pa(s)) ) = p s (y (pa(s)) , y (s) ) and, letting ϕ s (y (s) ) = p s (y (s) ) if pa(s) = ∅ and 1 otherwise, ϕ s,x (y (s) ) = ϕ s (y (s) )</p><p>t∼s,t∈S ϕ st (y (s) , x (t) ).</p><p>So, the marginal joint distribution of a vertex and its parents are directly given by belief propagation, using the just defined interactions. This training algorithm is summarized below.</p><p>Algorithm 17.2 (Learning tree distributions with hidden variables) Start with some initial guess of the conditional probabilities (for example, those given by the prior). The iterate the following two steps providing the transition from θ n to step θ n+1 .</p><p>(1) For k = 1, . . . , N , use belief propagation (or sum-prod) to compute all π θ n (x ({s}∪pa(s)) |</p><formula xml:id="formula_1598">X (S) = x (S)</formula><p>k ). Note that these probabilities can be 0 or 1 when s ∈ S and/or pa(s) ⊂ S. (2) Use <ref type="bibr">(17.22)</ref> to compute the next set of parameters.</p><p>The tree case includes the important example of hidden Markov models, which are defined as follows. S and H are ordered, with same cardinality, say S = {s 1 , . . . , s q } and H = {h 1 , . . . , h q }. Edges are (h 1 , h 2 ), . . . , (h q-1 , h q ) and (h 1 , s 1 ), . . . , (h q , s q ). The interpretation generally is that the hidden variables, h s , are the variables of interest, and behave like a Markov chain, and that the observations, x s , are either noisy or transformed versions of them. A major application is in speech recognition, where the h s 's are labels that represent specific phonemes (little pieces of spoken words) and the x s 's are measured signals. The transitions between hidden variables then describe how phonemes are likely to appear in sequence for a given language, and those between hidden and observed variables describe how each phoneme is likely to be pronounced and heard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="17.3.5">General Bayesian networks</head><p>The algorithm in the general case can move from tractable to intractable depending on the situation. This must generally be handled in a case by case basis, by analyzing the conditional structure, for a given model, knowing the observations. In practice, it is always possible to use loopy belief propagation to obtain some approximation of the conditional probabilities, even if it is not sure that the algorithm will converge to the correct marginals. When feasible, junction trees can be used, too. Monte-Carlo sampling is also an option, although quite computational.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 18</head><p>Deep Generative Methods We develop, in this chapter, methods that model stochastic processes using a feedforward approach that generates complex random variables using non-linear transformations of simpler ones. Many of these methods can be seen as instances of structural equation models (SEMs), described in section 15.3, with, for deep-learning implementations, high-dimensional parametrizations of <ref type="bibr">(15.8)</ref>.</p><p>With start with the formally simple case where the modeled variable takes values in R d and is modeled as</p><formula xml:id="formula_1599">X = g(Z)</formula><p>where Z also takes values in R d , with a known distribution and g is C 1 , invertible, with a C 1 inverse on R d , i.e., is a diffeomorphism of R d . Let us denote by h the inverse of g.</p><p>If Z has a p.d.f. f Z with respect to Lebesgue's measure, then, using the change of variable formula, the p.d.f. of X is</p><formula xml:id="formula_1600">f X (x) = f Z (h(x)) | det ∂ x h(x)|.</formula><p>Now, given a training set T = (x 1 , . . . , x N ), the log-likelihood, considered as a function of h, is given by</p><formula xml:id="formula_1601">ℓ(h) = N k=1 log f Z (h(x k )) + N k=1 log | det ∂ x h(x k )| . (<label>18.1)</label></formula><p>This expression should then be maximized with respect to h, subject to some restrictions or constraints to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.1.2">A greedy computation</head><p>One can define a rich class of diffeomorphisms through iterative compositions of simple transformations. This framework was introduced in <ref type="bibr" target="#b205">[187]</ref>, where a greedy approach was suggested to build such compositions. The method was termed "normalizing flows," since it create a discrete flow of diffeomorphisms that transform the data into a sample of a normal distribution.</p><p>We quickly describe the basic principles of the algorithm. One starts with a parametrized family, say (ψ α , α ∈ A) of diffeomorphisms of R. Such families are relatively easy to design, one example proposed in <ref type="bibr" target="#b205">[187]</ref> being a smoothed version of the piecewise linear function</p><formula xml:id="formula_1602">u → v 0 + (1 -σ )u + γ|(1 -σ )u -u 0 |</formula><p>which is increasing as soon as 0 ≤ max(σ , γ) &lt; 1. The smoothed version has an additional parameter, ϵ, and takes the form</p><formula xml:id="formula_1603">u → v 0 + (1 -σ )u + γ ϵ 2 + ((1 -σ )u -u 0 ) 2 .</formula><p>This transformation is parametrized by α = (v 0 , σ , γ, u 0 , ϵ). Other families of parametrized transformations can be designed. A multivariate transformation ϕ α,U : R d → R d can then be associated to families α = (α 1 , . . . , α d ) and orthogonal matrices U by taking (1) )</p><formula xml:id="formula_1604">ϕ α,U (x) =           ψ α 1 (y</formula><p>. . .</p><formula xml:id="formula_1605">ψ α d (y (d) )           with y = U x.</formula><p>The algorithm in <ref type="bibr" target="#b205">[187]</ref> is initialized with h 0 = id [d] and update the transformation at step n according to</p><formula xml:id="formula_1606">h n = ϕ α n ,U n • h n-1 .</formula><p>In this update, U n is generated as a random rotation matrix, and α n is determined as a gradient ascent update (starting from α = 0) for the maximization of</p><formula xml:id="formula_1607">α → ℓ(ϕ α,U n • h n-1 ).</formula><p>(Here, the current value h n-1 is not revisited, therefore providing a "greedy" optimization method.) Letting z n,k = h n (x k ), the chain rule implies that</p><formula xml:id="formula_1608">ℓ(ϕ α,U n • h n-1 )) = N k=1 log f Z (ϕ α,U n (z n-1,k )) + N k=1 log | det ϕ α,U n (z n-1,k )| + N k=1 log | det ∂ x h n-1 (x k )| .</formula><p>Since the last term does not depend on α, we see that it suffices to keep track of the "particle" locations, z n-1,k to be able to compute α n . Note also that these locations are easily updated with z n,k = ϕ α n ,U n (z n-1,k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.1.3">Neural implementation</head><p>This iterated composition of diffeomorphisms obviously provides a neural architecture similar to those discussed in chapter 11. Fixing the number of iterations to be, say, m, one can consider families of diffeomorphisms (ϕ θ ) indexed by a parameter w (we had w = (α, U ) in the previous discussion), and optimize (18.1) over all functions h taking the form</p><formula xml:id="formula_1609">h = ϕ w m • • • • • ϕ w 1 . Letting z j,k = ϕ w j • • • • • ϕ w 1 (x k ) for j ≤ m (with z 0,k = x k ), we can write ℓ(h) = N k=1 log f Z (z m,k ) + N k=1 m j=1 log | det ∂ x ϕ w j (z j-1,k )|.</formula><p>Normalizing flows in this form are described in <ref type="bibr" target="#b179">[161,</ref><ref type="bibr" target="#b125">107,</ref><ref type="bibr" target="#b166">148]</ref>. The gradient of ℓ with respect to the parameters w 1 , . . . , w m can be computed by backpropagation. We note however that, unlike typical neural implementations, the parameters may come with specific constraints, such as U ∈ O d (R) when w = (α, U ), so that the gradient and associated displacement may have to be adapted compared to standard gradient ascent implementations (see section 20.6.3 for a discussion of first-order implementations of gradient methods for functions of orthogonal matrices, and <ref type="bibr" target="#b19">[1]</ref> for more general methods on optimization over matrix groups).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.1.4">Time-continuous version</head><p>In section 11.6, we described how diffeomorphisms could be generated as flows of differential equations, and this remark can be used to provide a time-continuous version of normalizing flows. Using <ref type="bibr">(11.3)</ref>, one generates trajectories z(•) by solving over, say, [0, T ]</p><formula xml:id="formula_1610">∂ t z(t) = ψ w(t) (z(t))</formula><p>with z(0) = x for some function w : t → w(t). Letting z(t) = h w (t, x) (which defines h w ), we know that, under suitable assumptions on ψ, the mapping x → h w (t, x) is a diffeomorphism of R d . One can then maximize</p><formula xml:id="formula_1611">ℓ(h w (T , •)) = N k=1 log f Z (h w (T , x k )) + N k=1 log | det ∂ x h w (T , x k )| with respect to the function w. Let z k (t) = h w (t, x k ) and J k (t) = log | det ∂ x h w (t, x k )|. We have, by definition ∂ t z k (t) = ψ w(t) (z k (t))</formula><p>with z k (0) = x k . One can also show that</p><formula xml:id="formula_1612">∂ t J k (t) = ∇ • ψ w(t) (z k (t))</formula><p>with J k (0) = 0, where the r.h.s. is the divergence of ψ w(t) evaluated at z k (t). We provide a quick (and formal) justification of this fact. First note that differentiating</p><formula xml:id="formula_1613">∂ t h w (t, x) = ψ w(t) (h w (t, x)) with respect to x yields ∂ t ∂ x h w (t, x) = ∂ x ψ w(t) (h w (t, x))∂ x h w (t, x).</formula><p>The mapping J : A → log | det(A)| is differentiable on the set of invertible matrices and is such that dJ (A)H = trace(A -1 H). Applying the chain rule, we find</p><formula xml:id="formula_1614">∂ t log | det ∂ x h w (t, x)| = trace(∂ x h w (t, x) -1 ∂ x ψ w(t) (h w (t, x))∂ x h w (t, x)) = trace(∂ x ψ w(t) (h w (t, x))) = ∇ • ψ w(t) (h w (t, x)).</formula><p>From this, it follows that the time-continuous normalizing flow problem can be reformulated as maximizing</p><formula xml:id="formula_1615">N k=1 log f Z (z k (T )) + N k=1 J k (T ) subject to ∂ t z k (t) = ψ w(t) (z k (t)), ∂ t J k (t) = ∇ • ψ w(t) (z k (t)), z k (0) = x k , J k (0) = 0.</formula><p>This is an optimal control problem, whose analysis can be done similarly to that made in section 11.6.1, provided that ∇ • ψ w(t) can be expressed in closed form.</p><p>Note that the inverse of h w (T , •), which provides the generative model going from Z to X can also be obtained as the solution of an ODE. Namely, if one solves the differential equation</p><formula xml:id="formula_1616">∂ t x(t) = -ψ w(T -t) (x(t))</formula><p>with initial condition x(0) = z, then x(T ) solves the equation h w (T , •) = z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2.">NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS437</head><p>18.2 Non-diffeomorphic models and variational autoencoders</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2.1">General framework</head><p>The previous discussion addressed the situation X = g(Z) when g is a diffeomorphism, which required, in particular, that X and Z are real vectors with identical dimensions. This may not always be desirable, as one may prefer a small-dimensional variable Z (in the spirit of the factor analysis methods discussed in chapter 20), or a high-dimensional Z to increase, for example the modeling power. In addition, the observation variables may be discrete, which precludes the use of the change of variables formula. In such cases, Z has to be treated as a hidden variable using one of the methods discussed in chapter 16.</p><p>It will convenient to model the generative process in the form of a conditional distribution of X given Z rather than a deterministic function. We place ourselves in the framework of chapter 16 (with slightly modified notation) and let R X and R Z denote the measured spaces over where X and Z take their values, with measures µ X and µ Z , and assume that the conditional distribution of X given Z = z has density f X (x | z, θ) with respect to µ X , for some parameter θ. We also assume that Z has a distribution with density f Z with respect to µ Z , that we assume given and unparametrized. One can then directly apply the algorithms provided in chapter 16, and in particular the variational methods described in section 16.4.4 with an appropriate definition of the approximation of the conditional density of Z given X. An important example in this context is provided by variational autoencoders (VAEs) that we now present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2.2">Generative model for VAEs</head><p>VAEs <ref type="bibr" target="#b121">[103,</ref><ref type="bibr" target="#b122">104]</ref> model X ∈ R d as X = g(Z, θ)+ϵ where ϵ is a centered Gaussian noise with covariance matrix Q. The function g is typically non-linear, and VAEs have been introduced with this function modeled as a deep neural network (see chapter 11). Letting ϕ N ( • ; 0, Q) denote the p.d.f. of the Gaussian distribution N (0, Q), the conditional distribution of X given Z = z has density</p><formula xml:id="formula_1617">f X (x | z, θ) = ϕ N (x -g(z, θ)) ; 0, Q) with respect to Lebesgue's measure on R d .</formula><p>Following the procedure in section 16.4.4, we define an approximation of the conditional distribution of Z given X. Assuming that Z ∈ R p , we let this distribution be N (µ(x, w), Σ(x, w)) for some functions µ and Σ, w being a parameter. To ensure that Σ ⪰ 0, we will represent it in the form Σ(x, w) = S(x, w) 2 where S is a symmetric matrix. In <ref type="bibr" target="#b121">[103]</ref>, both functions µ and S are represented as neural networks parametrized by w. The joint density of X and Z is such that</p><formula xml:id="formula_1618">log f X,Z (x, z ; θ, Q) = log ϕ N (x -g(z, θ)) ; 0, Q) + log f Z (z) = - 1 2 (x -g(z, θ)) T Q -1 (x -g(z, θ)) - 1 2 log det Q - d 2 log 2π + log f Z (z)</formula><p>We also have</p><formula xml:id="formula_1619">log ϕ N (z ; µ(x, w), S(x, w) 2 ) = - 1 2 (z-µ(x, w)) T S(x, w) -2 (z-µ(x, w))-log det S(x, w)- p 2 log 2π.</formula><p>We can then rewrite the algorithm in <ref type="bibr">(16.15)</ref> as</p><formula xml:id="formula_1620">                     θ n+1 = θ n + γ n+1 ∂ θ log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) Q n+1 = Q n + γ n+1 ∂ Q log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) w n+1 = w n + γ n+1 log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) × ∂ w log ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) (18.2)</formula><p>where X n+1 is drawn uniformly from the training data and</p><formula xml:id="formula_1621">Z n+1 ∼ N (µ(X n+1 , w n ), S(X n+1 , w n ) 2 ).</formula><p>The derivatives in this system can be computed from those of g, µ and S (typically involving back-propagation) and the expression of the derivatives of the determinant and inverse of a matrix provided in (1.4) and <ref type="bibr">(1.6)</ref>.</p><p>The computation can be simplified if one assumes that f Z is the p.d.f. of a standard Gaussian, i.e., f Z = ϕ N (•; 0, Id R p ). Indeed, in that case, the integral in <ref type="bibr">(16.11)</ref>, which is, using the current notation</p><formula xml:id="formula_1622">R p log ϕ N (x -g(z, θ) ; 0, Q)ϕ N (z ; 0, Id R p ) ϕ N (z ; µ(x, w), S(x, w) 2 ) ϕ N (z ; µ(x, w), S(x, w) 2 )dz,<label>(18.3)</label></formula><p>can be partially computed. For any two p-dimensional Gaussian p.d.f.'s, one has</p><formula xml:id="formula_1623">R p log ϕ N (z ; µ 1 , Σ 1 ) ϕ N (z ; µ 2 , Σ 2 ) dz = - 1 2 trace(Σ -1 1 Σ 2 ) - 1 2 (µ 2 -µ 1 ) T Σ -1 1 (µ 2 -µ 1 ) - 1 2 log det(Σ 1 ) - p 2 log(2π). (18.4)</formula><p>As a consequence, (18.3) becomes</p><formula xml:id="formula_1624">- 1 2 E w (X -g(Z, θ)) T Q -1 (X -g(Z, θ)) - 1 2 log det Q - d 2 log 2π -E w 1 2 trace(S(X, w) 2 ) + 1 2 |µ(X, w)| 2 -log det(S(X, w)) + p 2 ,<label>(18.5)</label></formula><p>where E w denotes the expectation for the random variable (X, Z) where X follows a uniform distribution over training data and the conditional distribution of Z given X = x is N (µ(x, w) , S(x, w) 2 ).</p><p>The algorithm proposed in Kingma and Welling <ref type="bibr" target="#b121">[103]</ref> introduces a change of variable Z = µ(X, w) + S(X, w)U where U ∼ N (0, Id R p ), rewriting (18.5) as</p><formula xml:id="formula_1625">- 1 2 E (X -g(µ(X, w) + S(X, w)U , θ)) T Q -1 (X -g(µ(X, w) + S(X, w)U , θ)) -E w 1 2 trace(S(x, w) 2 ) + 1 2 |µ(X, w)| 2 -log det(S(X, w)) - 1 2 log det Q - d 2 log 2π + p 2 ,<label>(18.6)</label></formula><p>with a modified version of (18.2). Letting</p><formula xml:id="formula_1626">F(θ, Q, w, x, u) = - 1 2 (x -g(µ(x, w) -S(x, w)U , θ)) T Q -1 (x -g(µ(x, w) -S(x, w)U , θ)) - 1 2 log det Q - 1 2 trace(S(x, w) 2 ) - 1 2 |µ(x, w)| 2 + log det(S(x, w))</formula><p>the resulting algorithm is</p><formula xml:id="formula_1627">           θ n+1 = θ n + γ n+1 ∂ θ F(θ n , Q n , w n , X n+1 , U n+1 ) Q n+1 = Q n + γ n+1 ∂ Q F(θ n , Q n , w n , X n+1 , U n+1 ) w n+1 = w n -γ n+1 ∂ w F(θ n , Q n , w n , X n+1 , U n+1 ) (18.7)</formula><p>where X n+1 is drawn uniformly from the training data and U n+1 ∼ N (0, Id R p ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.2.3">Discrete data</head><p>This framework can be easily adapted to situations in which the observations are discrete. Consider, as an example, the situation in which X takes values in {0, 1} V , where V is a set of vertexes, i.e., X is a binary Markov random field on V . Assume, as a generative model, that conditionally to the latent variable Z ∈ R p , the variables X (s) , s ∈ V are independent and X (s) follows a Bernoulli distribution with parameter g s (z, θ), where g : R p → [0, 1] V . Assume also that Z ∼ N (0, Id R p ), and define, as above, an approximation of the conditional distribution of Z given X = x as a Gaussian with mean µ(x, w) and covariance matrix S(x, w) 2 . Then, the joint density of X and Z (with respect to the product of the counting measure on {0, 1} V and Lebesgue's measure on R p ) is</p><formula xml:id="formula_1628">log f X,Z (x, z ; θ) = x log g(z, θ) + (1 -x) log(1 -g(z, θ)) + log ϕ N (z ; 0, Id R p ) and (18.2) becomes                      θ n+1 = θ n + γ n+1 ∂ θ log f X,Z (X n+1 , Z n+1 ; θ n , Q n ) w n+1 = w n + γ n+1 log f X,Z (X n+1 , Z n+1 ; θ n ) ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) × ∂ w log ϕ N (X n+1 ; µ(X n+1 , w n ), S(X n+1 , w n ) 2 ) (18.8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.3">Generative Adversarial Networks (GAN) 18.3.1 Basic principles</head><p>Similarly to the methods discussed so far, GANs <ref type="bibr" target="#b100">[82]</ref>, use a one-step nonlinear generator X = g(Z, θ), with θ ∈ R K , to model observed data (we here switch back to a deterministic relation), where Z has a known distribution, with p.d.f. f Z , for example Z ∼ N (0, Id R p ). However, unlike the exact or approximate likelihood maximization that were discussed in sections 18.1 and 18.2, GANs us a different criterion for estimating the parameter θ by minimizing metrics that can be approximated by optimizing a classifier. The classifier is a function x → f (x, w), parametrized by w ∈ R M , whose goal is to separate simulated samples from real ones: it takes values in [0, 1] and estimates the (posterior) probability that its input x is real. GANs' adversarial paradigm consists in estimating θ and w together so that generated data, using θ, are indistinguishable from real ones using the optimal w. Their basic structure is summarized in Figure <ref type="figure" target="#fig_135">18</ref> "real data" vs. "simulation". Given W , θ is optimized to worsen the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.3.2">Objective function</head><p>Let P θ denote the distribution of g(Z, θ), and P true the target distribution of "real data." One can formalize the "real data" vs. "simulation" problem with a pair of random variables (X, Y ) where Y follows a Bernoulli distribution with parameter 1/2, and the conditional distribution of X given Y is P true when Y = 1 and P θ when Y = 0. Given a loss function r : {0, 1} × [0, 1] → [0, +∞), one can define</p><formula xml:id="formula_1629">U (θ, w) = E θ (r(Y , f (X, w))) and U * (θ) = min w∈R M U (θ, w).</formula><p>We want to maximize U * or, equivalently, solve the optimization problem</p><formula xml:id="formula_1630">θ * = argmax θ min w∈R M U (θ, w). Note that 2U (θ, w) = E true (r(1, f (X, w))) + E θ (r(0, f (X, w)))</formula><p>so that choosing the cost requires to specify the two functions t → r(1, t) and t → r(0, t). In Goodfellow et al. <ref type="bibr" target="#b100">[82]</ref>, they are:</p><formula xml:id="formula_1631">r(1, t) = -log t r(0, t) = -log(1 -t). (<label>18.9) 18.3.3 Algorithm</label></formula><p>Using costs in (18.9), one must compute</p><formula xml:id="formula_1632">θ * = argmin max w∈R M E true (log f (X, w)) + E θ (log(1 -f (X, w))) = argmin max w∈R M E true (log f (X, w)) + E(log(1 -f (g(Z, θ), w))) .</formula><p>Such min-max, or saddle-point problem are numerically challenging. The following algorithm was proposed in Goodfellow et al. <ref type="bibr" target="#b100">[82]</ref>, and also includes a stochastic approximation component. Indeed, in practice, E true is only known through the observation of training data, say x 1 , . . . , x N . Moreover, E θ is only accessible through Monte-Carlo simulation, so that both expectations can only be approximated through finite-sample averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 18.1 (GAN training algorithm)</head><p>1. Extract a batch of m examples from training data, simulate m samples according to P θ and run a few (stochastic) gradient ascent steps with fixed θ to update w, replacing expectations by averages.</p><p>2. Generate m new samples of Z and update θ with fixed w by iterating a few steps of (stochastic) gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.3.4">Associated probability metric and Wasserstein GANs</head><p>Let F be the family of all measurable functions: f : R d → [0, 1]. Given two possible probability distributions P 1 , P 2 (with associated expectations denoted E 1 , E 2 ) of a random variable X taking values in R d , consider the function</p><formula xml:id="formula_1633">D(P 1 , P 2 ) = 2 log 2 + max f ∈F E 1 (log f (X)) + E 2 (log(1 -f (X)))</formula><p>Assume that X under P 1 (resp. under P 2 ) has p.d.f. g 1 (resp. g 2 ) with respect to Lebesgue's measure (this assumption is not needed for the following to hold, but makes the discussion more elementary). Then</p><formula xml:id="formula_1634">E 1 (log f (X)) + E 2 (log(1 -f (X))) = R d (g 1 log f + g 2 log(1 -f ))dx which is maximal at f * = g 1 /(g 1 + g 2 ). For this f * , 2 log 2 + E 1 (log f * (X)) + E 2 (log(1 -f * (X))) = R d g 1 log 2g 1 g 1 + g 2 dx + R d g 2 log 2g 2 g 1 + g 2 dx = KL g 1 + g 2 2 , g 1 + KL g 1 + g 2 2 , g 2</formula><p>This expression is called the Jensen-Shannon divergence between g 1 and g 2 . It is always non-negative, and vanishes only when g 1 = g 2 .</p><p>So, D : (P 1 , P 2 ) → D(P 1 , P 2 ) can be interpreted as a way to evaluate the difference between two probability distributions on R d . One can then define</p><formula xml:id="formula_1635">D(P 1 , P 2 ) = max w∈R M E 1 (log f (X, w)) + E 2 (log(1 -f (X, w)))</formula><p>as an approximation of D in which the set of all possible functions with values in [0, 1] is replaced by those arising from the GAN classification network, parametrized by w. This approximation is useful when g 1 , g 2 are only observable through random sampling or simulation. With this interpretation, GANs minimize D(P true , P θ ). This discussion suggests that new types of GAN may be designed using other discrepancy functions between probability distributions, provided they can be expressed in terms of the maximization of some quantity over some space of functions. Consider, for example the norm in total variation, defined by (for discrete distributions)</p><formula xml:id="formula_1636">D var (P 1 , P 2 ) = 1 2 x |P 1 (x) -P 2 (x)|.</formula><p>or, in the general case D var (P 1 , P 2 ) = max A (P 1 (A) -P 2 (A)).</p><p>If F is the space of continuous functions f : R d → [0, 1], then we also have (under mild assumptions on P 1 and P 2 )</p><formula xml:id="formula_1637">D var (P 1 , P 2 ) = max f ∈F (E 1 (f ) -E 2 (f )).</formula><p>Since neural nets typically generate continuous functions with values in [0, 1], one could train GANs by maximizing</p><formula xml:id="formula_1638">Dvar (P 1 , P 2 ) = max w∈R M E 1 (f (X, w)) -E 2 (f (X, w))</formula><p>However, the total variation distance is too crude to allow for meaningful comparisons between distributions. For example, the distance between two Dirac distributions at, say, x 1 and x 2 in R d is always 1, whatever the distance between x 1 and x 2 , unless x 1 = x 2 . A more sensitive distance can be defined based on the notion of optimal transport.</p><p>The Monge-Kantorovich, also called Wasserstein, and sometimes also called "earthmover", distance evaluates the minimal total distance along which "mass" needs to be transported to transform a distribution, P 1 , into another, P 2 . Its mathematical definition is</p><formula xml:id="formula_1639">D w (P 1 , P 2 ) = inf Q R d ×R d |x 1 -x 2 |Q(dx 1 , dx 2 )</formula><p>where the inf is computed over all joint distributions on R d ×R d whose first marginal is P 1 and second marginal P 2 . Note that the distance D w between δ x 1 and δ x 2 now is</p><formula xml:id="formula_1640">|x 1 -x 2 |.</formula><p>The Wasserstein distance can also be defined by</p><formula xml:id="formula_1641">D w (P 1 , P 2 ) = max f ∈F (E 1 (f ) -E 2 (f ))</formula><p>where F is now the space of contractive (or 1-Lipschitz) functions, i.e., f ∈ F if and only if, for all</p><formula xml:id="formula_1642">x 1 , x 2 ∈ R d , |f (x 1 ) -f (x 2 )| ≤ |x 1 -x 2 |.</formula><p>Using the fact that a neural network with all weights bounded by a constant K generates a function whose Lipschitz constant is controlled solely by K, one can then approximate (up to a multiplicative constant) the Wasserstein distance by</p><formula xml:id="formula_1643">Dw (P 1 , P 2 ) = max w∈W E 1 (f (X, w)) -E 2 (f (X, w))</formula><p>where W is the set of all weights bounded by a fixed constant. Given the distribution P true and the model P θ , Wasserstein GANs (WGANs <ref type="bibr">[11]</ref>) must then solve the saddlepoint problem</p><formula xml:id="formula_1644">U (θ, w) = max w∈W E true (f (X, w)) -E θ (f (X, w)) and U * (θ) = min w∈R M U (θ, w),</formula><p>with an algorithm similar to that described earlier.</p><p>As a final reference, we note the improved WGAN algorithm introduced in Gulrajani et al. <ref type="bibr" target="#b102">[84]</ref> in which the boundedness constraint in the weights is replaced by an explicit control of the derivative in x of the function f . More precisely, introduce a random variable Z with distribution Pθ equal (1 -U )X + U X ′ where U is uniformly distributed over [0, 1] and X and X ′ are independent respectively following the distribution P true and P θ . Then, the following approximation of the Wasserstein distance between P true and P θ that is used in Gulrajani et al. <ref type="bibr" target="#b102">[84]</ref>:</p><formula xml:id="formula_1645">Dw (P true , P θ ) = max w∈W E true (f (X, w)) -E θ (f (X, w)) -Ẽθ ((|∂ z f (Z, w)| -1) 2 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4">Reversed Markov chain models 18.4.1 General principles</head><p>The discussions in sections 18.2 and 18.3 can be applied to sequences of structural equations (describing finite Markov chains) in the form</p><formula xml:id="formula_1646">           Z 0 = ξ 0 Z k+1 = g(Z k , ξ k ; θ k ), k = 0, . . . , m -1 X = Z m</formula><p>where ξ 0 , . . . , ξ m-1 are random variables with fixed distribution.</p><p>Indeed, letting Z = (ξ 0 , . . . , ξ n-1 ) and θ = (θ 0 , . . . , θ m-1 ) the whole system can be considered as a function X = G( Z, θ) as considered in these sections. This representation, however, includes a large number of hidden variables, and it is unclear whether much improvement can be added to the case m = 1 to justify the additional computational load.</p><p>Reversed Markov chain models use a different generative approach in that they first model a forward Markov chain Z n , n ≥ 0 which is ergodic with known (and easy to sample from) limit distribution Q ∞ , and initial distribution Q true , the true distribution of the data. If one fixes a large enough number of steps, say, τ, then it is reasonable to assume that Z τ approximately follows the limit distribution, Q ∞ . One can then (approximately) sample from Q true by sampling Z0 according to Q ∞ and then applying τ steps of the time-reversed Markov chain.</p><p>Reversed chains were discussed in section 12.3.3. Assuming that Q true and P (z, •) have a density with respect to a fixed measure µ on R Z , we found that Zk = Z τ-k is a non-homogeneous Markov chain whose transition probability Pk (x, A) = P ( Zk+1</p><formula xml:id="formula_1647">∈ A | Zk = x) has density pk (x, y) = p(y, x)q τ-k-1 (y) q τ-k (x)</formula><p>with respect to µ, where q n is the p.d.f. of Q n = Q true P n , the distribution of Z n .</p><p>The distributions Q n , n ≥ 0 are unknown, since they depend on the data distribution P true , and the transition probabilities above must be estimated from data to provide a sampling algorithm from the reversed Markov chain. While, at first glance, this does not seem like a simplification of the problem, because one now has to sample from a potentially large number (τ) of distributions instead of one, this leads, with proper modeling and some intensive learning, to efficient and accurate sampling algorithms.</p><p>Several factors can indeed make this approach achievable. First, the forward chain should be making small changes to the current configuration at each step (e.g., adding a small amount of noise). This ensures that the reversed transition probabilities pk (x, •) are close to Dirac distributions and are therefore likely to be well approximated by simple unimodal distributions such as Gaussians. Second, the estimation problem does not have hidden data: given an observed sample, one can simulate τ steps of the forward chain to obtain, after reversing the order, a full observation of the reversed chain. Third, in some cases, analytical considerations can lead to partial computations that facilitate the modeling of the reversed transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4.2">Binary model</head><p>We now take some examples, starting with a discrete one. Let Q true be the distribution of a binary random field with state space {0, 1} over a set of vertexes V , i.e., with the notation of section 13.2, R X = F (V ) with F = {0, 1}. Fix a small ϵ &gt; 0 and define the transition probability p(x, y) for x, y ∈ F (V ) by</p><formula xml:id="formula_1648">p(x, y) = s∈V (1 -ϵ)1 y (s) =x (s) + ϵ1 y (s) =1-x (s) .</formula><p>Since p(x, y) &gt; 0 for all x and y, the chain converges (uniformly geometrically) to its invariant probability Q ∞ and one easily checks that this probability is such that all variables are independent Bernoulli random variables with success probability 1/2. Assuming that τ is large enough so that Q τ ≃ Q ∞ , the sampling algorithm initializes the reversed chain as independent Bernoulli(1/2) variables and runs τ steps using the transitions pk which must be learned from data.</p><p>For this model, we have</p><formula xml:id="formula_1649">q k (x) = y∈F (V ) q k-1 (y)p(y, x).</formula><p>For this transition, the probabililty of flipping two or more values of y is</p><formula xml:id="formula_1650">1 -(1 -ϵ) N -N ϵ(1 -ϵ) N -1 = N (N -1) 2 ϵ 2 + o(ϵ 2 )</formula><p>with N = |V |. We will write x ∼ s y if y (s) = 1x (s) and y (t) = x (t) for s t, and we will write x ∼ y if x ∼ s y for some s. With this notation, we have</p><formula xml:id="formula_1651">q k (x) = (1 -N ϵ)q k-1 (x) + ϵ y:y∼x q k-1 (y) + O(ϵ 2 )</formula><p>Since it implies that q k (x) = q k-1 (x) + o(ϵ), this expression can be reversed as</p><formula xml:id="formula_1652">q k-1 (y) = (1 + N ϵ)q k (y) -ϵ x:x∼y q k (x) + O(ϵ 2 )</formula><p>Similarly, we have</p><formula xml:id="formula_1653">p(y, x) = (1 -N ϵ)1 x=y + ϵ1 x∼y + O(ϵ 2 ).</formula><p>This gives p(y, x)q k-1 (y) = q k (x)1 x=y -ϵ1 x=y</p><formula xml:id="formula_1654">x ′ :x ′ ∼y q k (x ′ ) + ϵq k (y)1 x∼y + O(ϵ 2 ),</formula><p>and we finally get</p><formula xml:id="formula_1655">pk (x, y) =         1 -ϵ x ′ :x ′ ∼y q τ-k (x ′ ) q τ-k (x)         1 x=y + ϵ q τ-k (y) q τ-k (x) 1 x∼y + O(ϵ 2 ) If one lets σ (s) k (x) = q τ-k (y)</formula><p>q τ-k (x) with y ∼ s x, and defines</p><formula xml:id="formula_1656">pk (x, y) = s∈V (1 -ϵσ (s) k (x))1 y (s) =x (s) + ϵσ (s) k (x)1 y (s) =1-x (s) ,</formula><p>one checks easily that pk (x, y) = pk (x, y) + O(ϵ 2 ). This suggests modeling the reversed chain using transitions pk , for which the mapping x → (σ (s) k (x), s ∈ V ) needs to be learned from data (for example using a deep neural network). Note that 1σ k (x) is precisely the score function introduced for discrete distributions in remark 17.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4.3">Model with continuous variables</head><p>We now switch to an example with vector-valued variables, R X = R d , and assume that the forward Markov chain is such that, conditionally to X n = x,</p><formula xml:id="formula_1657">X n+1 ∼ N (x + hf (x), √ hId R d ),</formula><p>where f is C 1 . We saw in section 12.3.7 that, when f = -∇H/2 for a C<ref type="foot" target="#foot_17">foot_17</ref> function H such that exp(-H) is integrable, this chain converges (approximately for small h) to a limit distribution with p.d.f. (with respect to Lebesgue's measure) proportional to exp(-H). In the linear case, in which f (x) = -Ax/2 for some positive-definite symmetric matrix A, so that H(x) = 1 2 x T Ax, the limit distribution can be identified exactly as N (0, Σ h ) where Σ h satisfies the equation</p><formula xml:id="formula_1658">AΣ h + Σ h A - h 2 A 2 -2Id R d = 0</formula><p>whose solution is Σ h = (A -hA 2 /4) -1 (details being left to the reader). This implies that this limit distribution can be easily sampled from for any choice of A.</p><p>We now return to general f 's and make, like in the discrete case, a first-order identification of the reversed chain. We note that, for any smooth function γ,</p><formula xml:id="formula_1659">E(γ(X n+1 ) | X n = x) = E(γ(x + hf (x) + √ hU ))</formula><p>where U ∼ N (0, Id R d ). Making the second order expansion</p><formula xml:id="formula_1660">γ(x + hf (x) + hU ) = γ(x) + √ h∇γ(x) T U + h∇γ(x) T f (x) + h 2 U T ∇ 2 γ(x)U + o(h)</formula><p>and taking the expectation gives</p><formula xml:id="formula_1661">E(γ(X n+1 ) | X n = x) = γ(x) + h∇γ(x) T f (x) + h 2 ∆γ(x) + o(h). (<label>18.10)</label></formula><p>Considering the reversed chain, and letting q k denote the p.d.f. of X k for the forward chain, we have</p><formula xml:id="formula_1662">E(γ(X k-1 ) | X k = x) = R d γ(y) pk (x, y)dy = R d γ(y)p(y, x) q k-1 (y) q k (x) dy = 1 (2πh) d/2 R d γ(y) q k-1 (y) q k (x) e -1 2h |x-y-hf (y)| 2 dy = 1 (2π) d/2 R d γ(x - √ hu) q k-1 (x - √ hu) q k (x) e -1</formula><p>with the change of variable u = (xy)/ √ h. We make a first-order expansion of the terms in this integral, with</p><formula xml:id="formula_1663">γ(x - √ hu)q k-1 (x - √ hu) = γ(x)q k-1 (x) - √ h∇(γq k-1 )(x) T u + h 2 u T ∇ 2 (γq k-1 )(x)u + o(h) and e -1 2 |u- √ hf (x- √ hu)| 2 = e -1 2 |u| 2 e √ hu T f (x)-hu T df (x)u-1 2 |f (x)| 2 +o(h) = e -1 2 |u| 2 1 + √ hu T f (x) -hu T df (x)u - h 2 |f (x)| 2 + h 2 |u T f (x)| 2 + o(h) .</formula><p>Taking products</p><formula xml:id="formula_1664">γ(x - √ hu)q k-1 (x - √ hu)e -1 2 |u- √ hf (x- √ hu)| 2 = e -1 2 |u| 2 γ(x)q k-1 (x) 1 + √ hu T f (x) -hu T df (x)u - h 2 |f (x)| 2 + h 2 |u T f (x)| 2 + e -1 2 |u| 2 - √ h∇(γq k-1 )(x) T u -h(∇(γq k-1 )(x) T u)(f (x) T u) + h 2 u T ∇ 2 (γq k-1 )(x)u + o(h)</formula><p>We now take the integral with respect to u (recall that E(U T AU ) = trace(A) if A is any square matrix and U is standard Gaussian), so that</p><formula xml:id="formula_1665">1 (2π) d/2 R d γ(x - √ hu)q k-1 (x - √ hu)e -1 2 |u- √ hf (x- √ hu)| 2 du = γ(x)q k-1 (x) + h -γ(x)q k-1 (x)∇ • f (x) -∇(γq k-1 )(x) T f (x) + 1 2 ∆(γq k-1 )(x) + o(h) = q k-1 (x)       γ(x) + h       -γ(x)∇ • f (x) - ∇(γq k-1 )(x) q k-1 (x) T f (x) + 1 2 ∆(γq k-1 )(x) q k-1 (x)             + o(h)</formula><p>To compute an expansion of q k (x), it suffices to take γ = 1 above, so that</p><formula xml:id="formula_1666">q k (x) = q k-1 (x)       1 + h       -∇ • f (x) - ∇q k-1 (x) q k-1 (x) T f (x) + 1 2 ∆q k-1 (x) q k-1 (x)             + o(h).</formula><p>We now take the first-order expansion of the ratio, removing terms that cancel, and get</p><formula xml:id="formula_1667">E(γ(X k-1 ) | X k = x) = γ(x) -h∇γ(x) T f (x) + h∇γ(x) T ∇q k-1 (x) q k-1 (x) + h 2 ∆γ(x) + o(h)</formula><p>Comparing with (18.10), we find that Xk = X τ-k behaves, for small h, like the non-homogeneous Markov chain such that the conditional distribution of Xk+1 given</p><formula xml:id="formula_1668">Xk = x is N (x -hf (x) -hs τ-k-1 (x), √ hId R d ), with s τ-k-1 (x) = -∇ log q τ-k-1</formula><p>, the score function introduced in section 17.2.6, and score-matching methods from that section can be used to estimate it from observations of the forward chain initialized with training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4.4">Continuous-time limit</head><p>The forward schemes described in the previous examples can be interpreted as continuous time processes over discrete or continuous variables. In the latter case, the example X k+1 ∼ N (x +hf (x),</p><formula xml:id="formula_1669">√ hId R d ) conditionally to X k = x is a discretization of the stochastic differential equation dx t = f (x t )dt + dw t</formula><p>(see remark 12.5), where w t is a Brownian motion and the diffusion is initialized with Q true . We found that going backward meant (at first order and conditionally to</p><formula xml:id="formula_1670">X k = x) X k-1 ∼ N (x -hf (x) -hs k-1 (x), √ hId)</formula><p>that we can rewrite as</p><formula xml:id="formula_1671">x τ -X k-1 ∼ N (x τ -x + hf (x) + hs k-1 (x), √ hId).</formula><p>Following the definition in Anderson <ref type="bibr" target="#b27">[9]</ref>, this corresponds to a first-order discretization of the reverse diffusion</p><formula xml:id="formula_1672">dx t = (f (x t ) + s t (x t ))dt + d wt , t ≤ τ</formula><p>where wt is also a Brownian motion. This reverse diffusion with X τ ∼ Q ∞ will therefore approximately sample from Q true . (With this terminology, forward and reverse diffusions have similar differential notation, but mean different things.) Note that, in the continuous-time limit, the reverse Markov process follows the distribution of the reversed diffusion exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="18.4.5">Differential of neural functions</head><p>As we have seen in the previous two examples, estimating the reversed Markov chain requires computing the score functions of the forward probabilities. In the case of continuous variables, this score function is typically parametrized as a neural network, so that the function</p><formula xml:id="formula_1673">s k (x) = -∇ log q k (x) is computed as s k (x) = F(x; W k ), with the usual definition F(x, W k ) = z m+1 with z j+1 = ϕ j (z j , w jk ), z 0 = x and W k = (w 0k , . . . , w mk ).</formula><p>Assume that a training set T is observed. Running the forward Markov chain initialized with elements of T generates a new training step at each time step, that we will denote T k at step k. We have seen in section 17.2.6 that the score function s k could be estimated by minimizing, with respect to</p><formula xml:id="formula_1674">W x∈T k |F(x, W )| 2 -2∇ • F(x, W ) .</formula><p>This term involves the differential of F, which is defined recursively by (simply taking the derivative at each step)</p><formula xml:id="formula_1675">dF(x, W ) = ζ m+1 , ζ j+1 = dϕ j (z j , w j )ζ j , with ζ 0 = Id R d .</formula><p>From this recursive definition, back-propagation can be applied, in principle, to compute the derivative of dF(x, W ) with respect to W . The feasibility of this computation, however, is limited when d is large (d could be tens of thousands if one models images) computing the d × d matrix dF(x, W ) is intractable.</p><p>We can note that, for any h ∈ R d , the vector dF(x, W )h also satisfies the recursion</p><formula xml:id="formula_1676">dF(x, W )h = ζ m+1 h, ζ j+1 h = dϕ j (z j , w j )ζ j h, with ζ 0 h = h and ∇ • F(x, W ) = d i=1 e T i dF(x, W )e i</formula><p>where e 1 , . . . , e d is the canonical basis of R d . Putting the divergence of F in this form does not reduce the computation cost (which is, roughly d 2 m, assuming that all z j 's have the same dimension), but expresses the divergence term in a form that is amenable to stochastic gradient descent (which is typically already used to approximate the sum over x). Indeed, if U follows any distribution with zero mean and covariance matrix equal to the identity (such as a standard Gaussian, or the uniform distribution on the unit sphere), then</p><formula xml:id="formula_1677">∇ • F(x, W ) = E(U T dF(x, W )U )</formula><p>so that U can be sampled from in minibatches in SGD implementations (see <ref type="bibr" target="#b198">[180]</ref>, where this approach is called "sliced score matching").</p><p>Chapter 19</p><p>Clustering</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.1">Introduction</head><p>We now describe a collection of methods designed to divide a training set into homogeneous subsets, or clusters. This grouping operation is a key problem in many applications for which it is important to categorize the data in order to obtain improved understanding of the sampled phenomenon, and sometimes to be able to apply a different approach to subsequent processing or analysis adapted to each cluster.</p><p>We will assume that the variables of interest belong a set R = R X where R is equipped with a discrepancy function α : R × R → [0, +∞). Often, α is derived from a distance ρ on R, but this is not always the case. We will assume that the data results from a training set T = (x 1 , . . . , x N ). However, it may happen that only the discrepancy matrix A = (α(x, y), x, y ∈ T ) is observed, while a coordinate representation of the elements of T is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us consider a few examples.</head><p>(i) The simplest case is when R = R d with the standard Euclidean metric. Slightly more generally, a metric may be defined by</p><formula xml:id="formula_1678">ρ 2 (x, y) = ∥h(x) -h(y)∥ 2</formula><p>H , where H is an inner-product space and the feature function h : R → H may be unknown, while its associated "kernel", K(x, y) = ⟨h(x) , h(y)⟩ H is known (this is a metric if h is one-toone). In this case ρ 2 (x, y) = K(x, x) -2K(x, y) + K(y, y).</p><p>Typically, one then takes α = ρ or α = ρ 2 .</p><p>(ii) Very often, however, the data is not Euclidean, and the distance does not correspond to a feature space representation. This is the case, for example, for data belonging to "curved spaces" (manifolds), for which one may use the intrinsic distance provided by the length of shortest paths linking two points (assuming of course that this notion can be given a rigorous meaning). The simplest example is data on the unit sphere, where the distance ρ(x, y) between two points x and y is the length of the shortest large circle that connects them, satisfying |x -y| 2 = 2 -2 cos ρ(x, y).</p><p>Once again, α = ρ or ρ 2 is a typical choice.</p><p>(iii) A more complex example is provided by R being the space of symmetric positive-definite matrices on R d , for which one defines the length of a differentiable curve (S(t</p><formula xml:id="formula_1679">), t ∈ [a, b]) in this space by b a trace((S(t) -1 ∂ t S)(S(t) -1 ∂ t S) T )dt</formula><p>and for which</p><formula xml:id="formula_1680">ρ 2 (S 1 , S 2 ) = d i=1 (log λ i ) 2</formula><p>where λ 1 , . . . , λ d are the eigenvalues of S -1/2 1 S 2 S -1/2 1 or, equivalently, solutions of the generalized eigenvalue problem S 2 u = λS 1 u (see, for example, <ref type="bibr" target="#b90">[72]</ref>).</p><p>(iv) Another common assumption is that the elements of R are vertices of a weighted graph of which T is a subgraph; ρ may then be, e.g., the geodesic distance on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2">Hierarchical clustering and dendograms 19.2.1 Partition trees</head><p>This method builds clusters by organizing them in a binary hierarchy in which the data is divided into subsets, starting with the full training set, and iteratively splitting each subset into two parts until reaching singletons. This results in a binary tree structure, called a dendogram, or partition tree, which is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 19.1 A partition tree of a finite set A is a finite collection of nodes T with the following properties. (i) Each node has either zero or exactly two children. (We will use the notation</head><formula xml:id="formula_1681">v → v ′ to indicate that v ′ is a child of v.</formula><p>(ii) All nodes but one have exactly one parent. The node without parent is the root of the tree.</p><p>(iii) To each node v ∈ T is associated a subset A v ⊂ A. (iv</p><formula xml:id="formula_1682">) If v ′ and v ′′ are the children of v, then (A v ′ , A v ′′ ) forms a partition of A v .</formula><p>Nodes without children are called leaves, or terminal nodes. We will say that the hierarchy is complete if</p><formula xml:id="formula_1683">A v = A if v is the root, and |A v | = 1 for all terminal nodes.</formula><p>An example of partition tree is provided in fig.</p><p>The construction of the tree can follow two directions, the first one being bottomup, or agglomerative, in which the algorithm starts with the collection of all singletons and merges subsets one pair at a time until everything is merged into the full dataset. The second approach is top-down, or divisive, and initializes the algorithm with the full training set which is recursively split until singletons are reached. The first approach, on which we now focus, is more common, and computationally simpler.</p><p>We let T denote the training set and assume that a matrix of dissimilarities (α(x, y), x, y ∈ T ) is given. We will make the abuse of notation of considering that T is a set even though some of its elements may be repeated. This is no loss of generality, since T = (x 1 , . . . , x N ) can always be replaced by the subset</p><formula xml:id="formula_1685">{(k, x k ), k = 1, . . . , N } of N × R.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2.2">Bottom-up construction</head><p>We will extend α to a dissimilarity measure between subsets A, A ′ ⊂ T that we will denote (A, A ′ ) → ϕ(A, A ′ ). Once ϕ is defined, agglomeration works along the following algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.1</head><p>1. Start with the collection T 1 , . . . , T N of all single-node trees associated to each element of T . Let n = 0 and m = N . 2. Assume that, at step n of the algorithm, one has a collection of partition trees T 1 , . . . , T m with root nodes r 1 , . . . , r m associated with subsets A r 1 , . . . , A r m of T . Let the total collection of nodes be indexed as V n = {v 1 , . . . , v N +n }, so that {r 1 , . . . , r m } ⊂ V n .</p><p>3. If m = 1, stop the algorithm.</p><p>4. Select indices i, j ∈ {1, . . . , m} such that ϕ(A r i , A r j ) is minimal, and merge the corresponding trees by creating a new node v n+1+N with the root nodes of T i and T j as children (so that v n+1+N is associated with A r i ∪ A r j ). Add v n+1+N to the collection of root nodes, and remove r i and r j .</p><p>5. Set n → n + 1 and m → m -1 and return to step 2.</p><p>Clearly, the specification of the extended dissimilarity measure (ϕ) is a key element of the method. Some of most commonly used extensions are:</p><formula xml:id="formula_1686">• Minimum gap: ϕ min (A, A ′ ) = min(α(x, x ′ ) : x ∈ A, x ′ ∈ A ′ ). • Maximum dissimilarity: ϕ max (A, A ′ ) = max(α(x, x ′ ) : x ∈ A, x ′ ∈ A ′ ).</formula><p>• Sum of dissimilarities:</p><formula xml:id="formula_1687">ϕ sum (A, A ′ ) = x∈A x ′ ∈A ′ α(x, x ′ )</formula><p>• Average dissimilarity:</p><formula xml:id="formula_1688">ϕ avg (A, A ′ ) = 1 |A| |A ′ | x∈A x ′ ∈A ′ α(x, x ′ ).</formula><p>As shown in the next two propositions, the maximum distance favors clusters with small diameters, while using minimum gaps tends to favor connected clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 19.2 Let diam(A) = max(α(x, y), x, y ∈ A). The agglomerative algorithm using ϕ max is identical to that using ϕ(A, A</head><formula xml:id="formula_1689">′ ) = diam(A ∪ A ′ ).</formula><p>Proof Call Algorithm 1 the agglomerative algorithm using ϕ max , and Algorithm 2 the one using ϕ. At initialization, we have (because all sets are singletons),</p><formula xml:id="formula_1690">ϕ max (A k , A l ) = diam(A k ∪ A l ) for all 1 ≤ k l ≤ m. (<label>19.1)</label></formula><p>We show that this property remains true at all steps of the algorithms. Proceeding by induction, assume that, up to the step n, Algorithms 1 and 2 have been identical and result in sets (A 1 , . . . , A m ) satisfy <ref type="bibr">(19.1)</ref>. Then the next steps of the two algorithms coincide and assume, without loss of generality, that this next step</p><formula xml:id="formula_1691">merges A m-1 with A m . Let A ′ m-1 = A m-1 ∪ A m so that diam(A ′ m-1 ) ≤ diam(A i ∪ A j ) for all 1 ≤ i j ≤ m.</formula><p>We need to show that the new partition satisfies (19.1), which requires that</p><formula xml:id="formula_1692">ϕ max (A ′ m-1 , A k ) = diam(A ′ m-1 ∪ A k ) for k = 1, . . . , m -2. We have diam(A ′ m-1 ∪ A k ) = max(diam(A ′ m-1 ), diam(A k ), ϕ max (A ′ m-1 , A k )), so that we must show that max(diam(A ′ m-1 ), diam(A k )) ≤ ϕ max (A ′ m-1 , A k ). Write ϕ max (A ′ m-1 , A k ) = max(ϕ max (A m , A k ), ϕ max (A m-1 , A k )) = max(diam(A m ∪ A k ), diam(A m-1 ∪ A k ))</formula><p>where the last identity results from the induction hypothesis.</p><p>The fact that</p><formula xml:id="formula_1693">diam(A k ) ≤ max(diam(A m ∪ A k ), diam(A m-1 ∪ A k ))</formula><p>is obvious, and the inequality</p><formula xml:id="formula_1694">diam(A ′ m-1 ) ≤ max(diam(A m ∪ A k ), diam(A m-1 ∪ A k )</formula><p>) results from the fact that A m and A m-1 was an optimal pair. This shows that the induction hypothesis remains true at the next step and concludes the proof of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We now analyze ϕ min and, more specifically, the equivalence between the resulting algorithm and the one using the following measure of connectedness. For a given set A and x, y</p><formula xml:id="formula_1695">∈ A, let αA (x, y) = inf ϵ : ∃n &gt; 0, ∃(x = x 0 , x 1 , . . . , x n-1 , x n = y) ∈ A n+1 : α(x i , x i-1 ) ≤ ϵ for 1 ≤ i ≤ n .</formula><p>So αA is the smallest ϵ such that there exists a sequence of steps of size less than ϵ in A going from x to y. The function conn(A) = max{ αA (x, y) : x, y ∈ A} measures how well the set A is connected relative to the dissimilarity measure α. and we have: Proposition 19.3 The agglomerative algorithm using ϕ min is identical to that using ϕ(A, A ′ ) = conn(A ∪ A ′ ).</p><p>Proof The proof is similar to that of proposition 19.2. Indeed one can note that conn(A ∪ A ′ ) = max(conn(A), conn(A ′ ), ϕ min (A, A ′ )) .</p><p>Given this we can proceed by induction and prove that, if the current decomposition is A 1 , . . . , A m such that ψ(A k ∪ A l ) = ϕ min (A k , A l ) for all 1 ≤ k l ≤ m, then this property is still true after merging using ϕ min and ϕ.</p><p>Assuming again that A m-1 and A m are merged, and letting</p><formula xml:id="formula_1696">A ′ m-1 = A m ∪ A m-1 , we need to show that conn(A k ∪ A ′ m-1 ) = ϕ min (A k , A ′ m-1</formula><p>) for all k = 1, . . . , m -2, which is the same as showing that:</p><formula xml:id="formula_1697">max(conn(A k ), conn(A ′ m-1 )) ≤ ϕ min (A k , A ′ m-1 ) = min(ϕ min (A k , A m-1 ), ϕ min (A k , A m )). From the induction hypothesis, we have min(ϕ min (A k , A m-1 ), ϕ min (A k , A m )) = min(conn(A k ∪ A m-1 ), conn(A k ∪ A m ))</formula><p>and both terms in the right-hand side are larger than conn(A k ) and also larger than conn(A ′ m-1 ) which was a minimizer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2.3">Top-down construction</head><p>The agglomerative method is the most common way to build dendograms, mostly because of the simplicity of the construction algorithm. The divisive approach is more complex, because the division step, which requires, given a set A, to optimize a splitting criterion over all two-partitions of A, may be significantly more expensive than the merging steps in the agglomerative algorithm. The top-down construction therefore requires the specification of a "splitting algorithm" σ : A → (A ′ , A ′′ ) such that (A ′ , A ′′ ) is a partition of A. We assume that, if |A| &gt; 1, then the partition A, A ′′ is not trivial, i.e., neither set is empty.</p><p>Given σ , the top-down construction is as follows.</p><p>Algorithm 19.2 1. Start with the one-node partition tree T 0 = (T ).</p><p>2. Assume that at a given step of the algorithm, the current partition is T .</p><p>3. If T is complete, stop the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">For each terminal node</head><formula xml:id="formula_1698">v in T such that |A v | &gt; 1, compute (A ′ v , A ′′ v ) = σ (A v ) and add two children v ′ and v ′′ to v with A v ′ = A ′ v and A v ′′ = A ′′ v . 5. Return to step 2.</formula><p>The division of a set into two parts is itself a clustering algorithm, and one may apply any of those described in the rest of this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.2.4">Thresholding</head><p>Once a complete hierarchy is built, it provides a complete binary partition tree T . This tree provides in turn a collection of partitions of V , each of them obtained through pruning. We now formalize this operation. Let V T denote the set of terminal nodes in T and V 0 = V \ V T contain the interior nodes. Define a pruning set to be a subset D ⊂ V 0 that contains no pair of nodes v, v ′ such that v ′ is a descendant of v. To any pruning set D, one can associate the pruned subtree T (D) of T consisting of T from which all the vertices that are descendants of elements of D are removed. From any such pruned subtree, one obtain a partition S(D) of T formed by the collection of sets A v for v in the terminal nodes of T (D). Between the extreme case S(v 0 ) = {V } (where v 0 is the root of T ) and S(∅) = ({x}, x ∈ V T ), there exists a huge number of possible partitions obtained in this way.</p><p>It is often convenient to organize these partitions according to the level sets of a well-chosen score function v → h(v) defined over V 0 . For D ⊂ V , we denote by max(D) the set of its deepest elements, i.e., the set formed by those v ∈ D that have no descendant in D. Then, for any λ ∈ R, one can define</p><formula xml:id="formula_1699">D + λ = max {v : h(v) ≥ λ} (resp. D - λ = max {v : h(v) ≤ λ}) and the associated partition S(D + λ ) (resp. S(D - λ ))</formula><p>. The score function h can be linked to the construction algorithm. For example, if one uses a bottom-up construction using an extended dissimilarity ϕ, one can associate to each node v with v ∈ V 0 the value of ϕ(A v ′ , A v ′′ ) where v ′ and v ′′ are the children of v.</p><p>Another way to define such scores functions is by assigning weights to edges in T . Indeed, given a collection w of positive numbers w(v, v ′ ) for v → v ′ in T , one can define a score h w recursively by letting h w (v 0 ) = 0 and</p><formula xml:id="formula_1700">h w (v ′ ) = h w (v) + w(v, v ′ ) if v ′ is a child of v. The choice w(v, v ′ ) = 1 for all v, v ′</formula><p>provide the usual notion of depth in the tree. Scores can also be built bottom-up, letting h(v) = 0 for terminal nodes and, for</p><formula xml:id="formula_1701">v ∈ V 0 , h w (v) = max(h w (v ′ ) + w(v, v ′ ), h w (v ′′ ) + w(v, v ′′ ))</formula><p>where v ′ , v ′′ are the children of v Here, taking w = 1 provides the height of each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3">K-medoids and K-mean 19.3.1 K-medoids</head><p>One of the limitation of hierarchical clustering is that it is a greedy approach that does not optimize a global quality measure associated to the partition. Such qual-ity measures can indeed be defined based on the heuristic that clusters should be homogeneous (for some criterion) and far apart from each other.</p><p>In centroid-based methods, the homogeneity criterion is the minimum, over all possible points in R, of the sum of dissimilarities between elements of the cluster and that point. More precisely, for any A ⊂ R, and any dissimilarity measure α, define the central dispersion index</p><formula xml:id="formula_1702">V α (A) = inf        x∈A α(x, c) : c ∈ R        . (<label>19.2)</label></formula><p>If c achieves the minimum in the definition of V α , it is called a centroid of A for the dissimilarity α.</p><p>The most common choice is α = ρ 2 , where ρ is a metric on R, and in this case, we will just use V in place of V ρ 2 . Note also that it is always possible to limit R to the training set T , in which case the optimization in (19.2) is over a finite number of centers. This makes centroid-based methods also applicable to the situation when the matrix of dissimilarities is the only input provided to the algorithm, or when the set R and the function α are too complex for the optimization in (19.2) to be feasible.</p><p>A centroid, c, in (19.2) may not always exists, and when it exists it may not always be unique. For α = ρ 2 , a point c such that</p><formula xml:id="formula_1703">V (A) = x∈A ρ 2 (x, c)</formula><p>is called a Fréchet mean of the set A. Returning to the examples provided in the beginning of this chapter, two antipodal points on the sphere (whose distance is π) have an infinity of Fréchet means (or midpoints in this case) provided by every point in the equator between them. In contrast, the example provided with symmetric matrices provides a so-called Hadamard space <ref type="bibr" target="#b62">[44]</ref> and the Fréchet mean in that case is unique. Of course, for Euclidean metrics, the Fréchet mean is just the usual one.</p><p>Returning to our general discussion, the K-medoids method optimizes the sum of central dispersions with a fixed number of clusters. Note that the letter K in Kmedoids originally refers to this number of clusters, but this notation conflicts with other notation in this book (e.g., reproducing kernels) and we shall denote by p this target number <ref type="foot" target="#foot_18">1</ref> . So the K-medoids method minimizes</p><formula xml:id="formula_1704">W α (A 1 , . . . , A p ) = p i=1 V α (A i ) over all partitions A 1 , . . . , A p of the training set T . Equivalently, it minimizes W α (A 1 , . . . , A p , c 1 , . . . , c p ) = p i=1 x∈A i α(x, c i ) (19.3)</formula><p>over all partitions of T and c 1 , . . . , c p ∈ R. Finally, taking first the minimum with respect to A i , which corresponds to associating each x to the subset with closest center, K-medoids, an equivalent formulation minimizes</p><formula xml:id="formula_1705">Wα (c 1 , . . . , c p ) = x∈T min α(x, c i ), i = 1, . . . , p .</formula><p>The standard implementation of K-medoids solves this problem using an alternate minimization, as defined in the following algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.3 (K-medoids)</head><p>Let T ⊂ R be the training set. Start with an initial choice of c 1 , . . . , c p ∈ R and iterate over the following two steps until stabilization:</p><p>(1) For i = 1, . . . , p, let A i contain points x ∈ T such that α(x, c i ) = min{α(x, c j ), j = 1, . . . , p}. In case of a tie in this minimum, assign x to only one of the tied sets (e.g., at random) to ensure that A 1 , . . . , A p is a partition.</p><p>(2) For i = 1, . . . , p, let c i be a minimizer of x∈A i α(x, c i ) if A i is not empty, or c i be a random point in T otherwise.</p><p>It should be clear that each step reduces the total cost W α and that this cost should stabilize at some point (which provides the stopping criterion) because there is only a finite number of possible partitions of T . However, there can be many possible limit points that are stable under the previous iterations, and some may correspond to poor "local minima" of the objective function. Since the end-point of the algorithm depends on the initialization, this step requires extra care. One may design ad-hoc heuristics in order to start the algorithm with a good initial point that is likely to provide a good solution at the end. These heuristics may depend on the problem at hand, or use a generic strategy. As a common example of the latter, one may ensure that the initial centers are sufficiently far apart by picking c 1 at random, c 2 as far as possible from c 1 , c 3 maximizing the sum of distances to c 1 and c 2 etc. One also typically runs the algorithm several times with random initial conditions and select the best solution over these multiple runs.</p><p>The second step of Algorithm 19.3 can be computationally challenging depending on the set R and the dissimilarity measure α. When R = R d and α = ρ<ref type="foot" target="#foot_19">foot_19</ref> is the square Euclidean distance, the solution is explicit and c i is simply the average of all points in A i . The resulting algorithm is the original incarnation of K-medoids, and called K-means <ref type="bibr" target="#b200">[182,</ref><ref type="bibr" target="#b139">121,</ref><ref type="bibr" target="#b142">124]</ref>. K-means is probably the most popular clustering method and is often a step in more advanced approaches, as we will discuss later. The two steps of Algorithm 19.3 are then simplified as follows. (1</p><formula xml:id="formula_1706">) For i = 1, . . . , p, let A i contain points x ∈ T such that |x -c i | 2 = min{|x -c j | 2 , j = 1, . . . , p}.</formula><p>In case of tie in this minimum, assign x to only one of the tied sets (e.g., at random) to ensure that A 1 , . . . , A p is a partition.</p><p>(2) For i = 1, . . . , p, let</p><formula xml:id="formula_1707">c i = 1 |A i | x∈A i</formula><p>x if A i is not empty, or c i be a random point in T otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3.2">Mixtures of Gaussian and deterministic annealing</head><p>Mixtures of Gaussian (MoG) were discusssed in chapter 16 and in Algorithm 16.2. Recall that they model the observed data X together with a latent class variable Z ∈ {1, . . . , p} with joint distribution</p><formula xml:id="formula_1708">f (x, z; θ) = (2π) -d 2 (det Σ z ) -1 2 α z e -1</formula><p>where θ contains the weights, α 1 , . . . , α p , the means, c 1 , . . . , c p and the covariance matrices Σ 1 , . . . , Σ p (we create, hopefully without risk of confusion, a short-lived conflict of notation between the weights and the dissimilarity function). The posterior class probabilities</p><formula xml:id="formula_1709">f Z (i|x ; θ) = (det Σ i ) -1 2 α i e -1 2 (x-c i ) T Σ -1 i (x-c i ) p j=1 (det Σ j ) -1 2 α j e -1 2 (x-c j ) T Σ -1 j (x-c j )</formula><p>, i = 1, . . . , p, which are computed in step 3 of Algorithm 16.2 can be interpreted as a likelihood that observation x belongs to group i. As a consequence, the mixture of Gaussian algorithm can also be seen as a clustering method, in which one assigns each x ∈ T to cluster i when i = argmax{f Z (j|x, θ) : j = 1, . . . , p}, making an arbitrary decision in case of a tie.</p><p>In the special case in which all variances are fixed and equal to σ 2 Id R d , and all prior class probabilities are equal to 1/p (see remark 16.3), the EM algorithm for mixtures of Gaussian is also called "soft K-means", because it replaces the "hard" cluster assignments in K-means by "soft" ones represented by the update of the posterior distribution. We repeat its definition here for completeness (where θ = (c 1 , . . . , c p )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.5 (Soft K-means)</head><p>1. Choose a number σ 2 &gt; 0, a small constant ϵ and a maximal number of iterations M. Initialize the centers c = (c 1 , . . . , c p ).</p><p>2. At step n of the algorithm, let c be the current centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compute, for x ∈ T and i</head><formula xml:id="formula_1710">= 1, . . . , p f Z (i|x, θ) = e -1 2σ 2 |x-c i | 2 p j=1 e -1 2σ 2 |x-c j | 2</formula><p>and let ζ i = N k=1 f Z (i|x, θ), i = 1, . . . , p. 4. For i = 1, . . . , p, let</p><formula xml:id="formula_1711">c ′ i = 1 ζ i x∈T xf Z (i|x, θ).</formula><p>5. If |c ′ -c| &lt; ϵ or n = M: stop the algorithm.</p><p>6. Replace c by c ′ and n by n + 1 and return to step 2.</p><p>When σ 2 → 0, f Z ( • |x k , θ) converges to the uniform probability on indexes j such that c j is closest to x k , which is a Dirac measure unless there are ties. Class allocation and center updating become then asymptotically identical to the K-means algorithm. A variant of soft K-means, called deterministic annealing <ref type="bibr" target="#b187">[169]</ref>, applies Algorithm 19.5 while letting σ slowly tend to 0. This new algorithm is experimentally more robust than K-means, in that it is less likely to be trapped in bad local minimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 19.4</head><p>The soft K-means algorithm can also be defined directly as an alternate minimization method for the objective function</p><formula xml:id="formula_1712">F(c, f Z ) = 1 2 x∈T p j=1 f Z (j|x)|x -c j | 2 + σ 2 x∈T p j=1 f Z (j|x) log f Z (j|x),</formula><p>with the constraints f Z (j|x) ≥ 0 for all j and x and p j=1 f Z (j|x) = 1. One can check (we leave this as an exercise) that Step 3 in Algorithm 19.5 provides the optimal f Z for F when c is fixed, and that Step 4 gives the optimal c when f Z is fixed (see ??). ♦ Remark 19.5 We note that, if a K-means, soft K-means or MoG algorithm has been trained on a training set T , it is then easy to assign a new sample x to one of the clusters. Indeed, for K-means, it suffices to determine the center closest to x, and for the other methods to maximize f Z (j| x, θ), which is computable given the model parameters. In contrast, there was no direct way to do so using hierarchical clustering. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3.3">Kernel (soft) K-means</head><p>We now consider the soft K-means algorithm in feature space, and introduce features h k = h(x k ) in an inner product space H such that ⟨h k , h l ⟩ H = K(x k , x l ) for some positive definite kernel. As usual, the underlying assumption is that the computation of h(x) does not need to be feasible, while evaluations of K(x, y) are easy. Let us consider the minimization of</p><formula xml:id="formula_1713">1 2 x∈T p j=1 f Z (j|x)∥h(x) -c j ∥ 2 H + σ 2 x∈T p j=1 f Z (j|x) log f Z (j|x)</formula><p>for some σ 2 &gt; 0 (kernel K-means corresponds to taking the limit σ 2 → 0). Given f Z , the optimal centers are</p><formula xml:id="formula_1714">c j = 1 ζ j x∈T f Z (j|x)h(x)</formula><p>with ζ = x∈T f Z (j|x). They belong to the feature space, H, and are therefore not computable in general. However, the distance between them and a point h(y) ∈ H is explicit and given by</p><formula xml:id="formula_1715">∥h(y) -c j ∥ 2 H = K(y, y) - 2 ζ j x∈T f Z (j|x)K(y, x) + 1 ζ 2 j x,x ′ ∈T f Z (j|x)f Z (j|x ′ )K(x, x ′ ).</formula><p>The class probabilities at each iteration can therefore be updated using</p><formula xml:id="formula_1716">f Z (j|x) = e -∥h(x)-c j ∥ 2 H / 2σ 2 p j ′ =1 e -∥h(y)-c j ′ ∥ 2 H / 2σ 2 .</formula><p>This yields the soft kernel K-means algorithm, that we repeat below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.6 (Kernel soft K-means)</head><p>Let T ⊂ R d be the training set. Initialize the algorithm with some choice for f Z (j|x), j = 1, . . . , p, x ∈ T (for example: f Z (j|x) = 1/p for all j and x).</p><p>(1) For j = 1, . . . , p and x ∈ T compute</p><formula xml:id="formula_1717">∥h(x) -c j ∥ 2 H = K(x, x) - 2 ζ j x ′ ∈T f Z (j|x ′ )K(x, x ′ ) + 1 ζ 2 j x ′ ,x ′′ ∈T f Z (j|x ′ )f Z (j|x ′′ )K(x ′ , x ′′ ) with ζ j = x ′ ∈T f Z (j|x ′ ).</formula><p>(2) Compute, for x ∈ T and j = 1, . . . , p,</p><formula xml:id="formula_1718">f Z (j|x) = e -∥h(x)-c j ∥ 2 H /2σ 2 p j ′ =1 e -∥h(y)-c j ′ ∥ 2 H /2σ 2 .</formula><p>(3) If the variation of f Z compared to the previous iteration is small, or if a maximum number of iterations has been reached, exit the algorithm.</p><p>(4) Return to step 1.</p><p>After convergence, the clusters are computed by assigning x to A i when i = argmax{f Z (j|x) : j = 1, . . . , p}, making an arbitrary decision in case of a tie.</p><p>For "hard" K-means (with σ 2 → 0), step 2 simply updates f Z (j|x) as the uniform probability on the set of indexes j at which ∥h(x)c j ∥ 2 H is minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.3.4">Convex relaxation</head><p>We return to the initial formulation of K-means for Euclidean data, as a minimization, over all partitions A = {A 1 , . . . , A K } of {1, . . . , N } of</p><formula xml:id="formula_1719">W (A) = K j=1 k∈A j |x k -c j | 2</formula><p>where c j is the average of the points x j such that j ∈ A j . We start with a simple transformation expressing this function in terms of the matrix S α of square distances</p><formula xml:id="formula_1720">α(x k , x l ) = |x k -x l | 2</formula><p>. Indeed, we have</p><formula xml:id="formula_1721">k∈A j |x k -c j | 2 = k∈A j |x k | 2 - 1 |A| k∈A j x k = k∈A j |x k | 2 - 1 |A| k,l∈A j x T k x l = 1 2|A j | k,l∈A j (|x k | 2 + |x l | 2 -2x T k x l ) = 1 2|A j | k,l∈A j |x k -x l | 2</formula><p>Introduce the vector u j ∈ R N with coordinates u</p><formula xml:id="formula_1722">(k) j = 1/ |A j | for k ∈ A j and 0 other- wise. Then 1 2|A j | k,l∈A j |x k -x l | 2 = 1 2 u T j S α u J = 1 2 trace(S α u j u T j ) . (19.4) Let Z(A) = p j=1 u j u T j ,</formula><p>so that Z(A) has entries Z (k,l) (A) = 1/|A j | for k, l ∈ A j , j = 1, . . . p and 0 for all other k, l. Summing (19.4) over j, we get</p><formula xml:id="formula_1723">W (A) = 1 2 trace(S α Z(A)).</formula><p>The matrix Z(A) is symmetric, has non-negative entries. It moreover satisfies Z(A)1 N = 1 N and Z(A) 2 = Z(A). Interestingly, these properties characterize matrices Z associated with partitions, as stated in the next proposition <ref type="bibr" target="#b171">[153,</ref><ref type="bibr" target="#b170">152]</ref>.</p><p>Proposition 19.6 Let Z ∈ M N (R) be a symmetric matrix with non-negative entries satisfying Z1 N = 1 N and Z 2 = Z. The there exists a partition A of {1, . . . , N } such that</p><formula xml:id="formula_1724">Z = Z(A).</formula><p>Proof Note that Z being symmetric and satisfying Z 2 = Z imply that it is an orthogonal projection with eigenvalues 0 and 1. In particular Z is positive semidefinite. This implies that, for all i, j ∈ {1, . . . , N }, one has</p><formula xml:id="formula_1725">Z(i, j) 2 ≤ Z(i, i), Z(j, j).</formula><p>This inequality combined with N j=1 Z(k, j) = 1 (expressing Z1 N = 1 N ) shows that all diagonal entries of Z are positive.</p><p>Define on {1, . . . , N } the relation k ∼ j if and only if Z(j, k) &gt; 0. The relation is symmetric and we just checked that k ∼ k for all k. It is also transitive, from the relation (deriving from</p><formula xml:id="formula_1726">Z 2 = Z) Z(k, j) = N i=1 Z(k, i)Z(i, j)</formula><p>which shows (since all terms in the sum are non-negative) that k ∼ i and j ∼ i imply k ∼ j.</p><p>Let A = {A 1 , . . . , A q } be the partition of {1, . . . , N } formed by the equivalence classes for this relation. We now show that Z = Z(A).</p><p>We have, for all k, j ∈ {1, . . . , N</p><formula xml:id="formula_1727">} N i=1 Z(k, i)(Z(k, j) -Z(i, j)) = Z(k, j) N i=1 Z(k, i) - N i=1 Z(k, i)Z(i, j) = Z(k, j) - N i=1 Z(k, i)Z(i, j) = 0</formula><p>Now, if k, j ∈ A s for some s, the identity reduces to i∈A s Z(k, i)(Z(k, j) -Z(i, j)) = 0.</p><p>(19.5)</p><p>Choose k such that Z(k, k) = max{Z(i, i) : i ∈ A s }. Then, for all i, j ∈ A s , Z(i, j) ≤ Z(i, i)Z(j, j) ≤ Z(k, k) and (19.5) for j = k yields</p><formula xml:id="formula_1728">i∈A s Z(k, i)(Z(k, k) -Z(k, i)) = 0, which is only possible (since all Z(k, i) are positive) if Z(k, i) = Z(k, k) for all i ∈ A s . From Z(k, i) ≤ Z(i, i)Z(k, k), we get Z(i, i) = Z(k, k</formula><p>) for all i, and therefore (reapplying what we just found to i insteand of k) Z(i, j) = Z(i, i) = Z(k, k) for all i, j ∈ A s . Finally, we have 1 =</p><formula xml:id="formula_1729">i∈A s Z(k, i) = |A s |Z(k, k)</formula><p>showing that Z(k, k) = 1/|A s | and completing the proof that Z = Z(A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Note that the number of clusters, |A| is equal to the trace of Z(A). This shows that minimizing W (A) over partitions with p clusters is equivalent to the constrained optimization problem minimizing G(Z) = trace(S α Z) <ref type="bibr">(19.6)</ref> over all matrices Z such that Z ≥ 0, Z T = Z, Z1 N = 1 N , trace(Z) = p and Z<ref type="foot" target="#foot_20">foot_20</ref> = Z.</p><p>This is still a difficult problem, since it is equivalent to K-means, which is NP hard.</p><p>Seeing the problem in this form, however, is more amenable to approximations and, in particular, convex relaxations.</p><p>In <ref type="bibr" target="#b170">[152]</ref>, it is proposed to use a semidefinite program (SDP) as a relaxation. The conditions Z = Z T and Z 2 = Z require that all eigenvalues of Z are either 0 or 1, and a direct relaxation is to replace these constraints by Z T = Z and 0 ⪯ Z ⪯ Id R N . The last inequality is however redundant if we add the conditions 2 Z ≥ 0 and Z1 = 1. This is a consequence of the Perron-Frobenius theorem which states that a matrix Z with positive entries has a largest (in modulus) real eigenvalue, which has multiplicity one and is associated with an eigenvector with positive coordinates, the latter eigenvector being (up to multiplication by a constant) the unique eigenvector of Z with positive coordinates. So, if a matrix Z is symmetric, satisfies Z &gt; 0 and Z1 N = 1 N , then Z ⪯ Id R N . Applying this result to Z = (1ϵ)Z + (ϵ/N )1 N 1 T N and letting ϵ tend to 0 shows that any matrix Z with non-negative entries satisfying</p><formula xml:id="formula_1730">Z1 N = 1 N also satisfies Z ⪯ Id R N .</formula><p>This provides the following SDP relaxation of K-means <ref type="bibr" target="#b170">[152]</ref>: minimize</p><formula xml:id="formula_1731">G(Z) = trace(S α Z) (19.7) subject to Z T = Z, Z1 N = 1 N , trace(Z) = p, Z ≥ 0, Z ⪰ 0.</formula><p>Clusters can be immediately inferred from the columns of the matrix Z(A), since they are identical for two indices in the same cluster, and orthogonal to each other for two indices in different clusters. Let These properties will not necessarily be satisfied by a solution, say, Z * , of the SDP relaxation, but, assuming that the approximation is good enough, one may still consider the normalized columns of Z * and expect them to be similar for indices in the same cluster, and away from each other otherwise. Denoting by z * 1 , . . . , z * N these normalized columns, one can then run on them the standard K-means algorithm, or a spectral clustering method such as those described in the next sections, to infer clusters.</p><p>Remark 19.7 Clearly, one can use any symmetric matrix S in the definition of G in <ref type="bibr">(19.6)</ref> and <ref type="bibr">(19.7)</ref>. The method is equivalent to, or to a relaxation of, K-means only when S is formed with squared norms in inner-product spaces, which does include kernel K-means, for which</p><formula xml:id="formula_1732">α(x k , x l ) = K(x k , x k ) -2K(x k , x l ) + K(x l , x l ).</formula><p>If α is an arbitrary discrepancy measure, the minimization of G(Z) still makes sense, since it is equivalent to minimizing</p><formula xml:id="formula_1733">G(Z(A)) = p j=1 D α (A j ).</formula><p>where</p><formula xml:id="formula_1734">D α (A) = 1 |A|</formula><p>x,y∈A α(x, y) . <ref type="bibr">(19.8)</ref> is a (normalized) measure of size, that we will call the α-dispersion of a finite set A.♦ Remark 19.8 Instead of using dissimilarities, some algorithms are more naturally defined in terms of similarities. Given such a similarity measure, say, β, one must maximize rather than minimize the index ∆ β (which becomes, rather than a measure of dispersion, a measure of concentration).</p><p>One passes from a dissimilarity α to a similarity β by applying a decreasing function to the former, a common choice being</p><formula xml:id="formula_1735">β(x, x ′ ) = exp(-α(x, x ′ )/τ)</formula><p>for some τ &gt; 0.</p><p>Alternatively, one can fix an element x 0 ∈ R and let β(x, y) = α(x, x 0 ) + α(y, x 0 )α(x, y)α(x 0 , x 0 ), (note that the last term, α(x 0 , x 0 ) is generally equal to 0). For example, if α(x, y) = |x -y| 2 , then β(x, y) = 2(xx 0 ) T (yx 0 ) (for which it is natural to take x 0 = 0). If α is a distance (not squared!), then β ≥ 0 by the triangular inequality. In this case, we have</p><formula xml:id="formula_1736">∆ β (A 1 , . . . , A p ) = n k=1 D β (A k ) = p k=1 1 |A p | x,y∈A k α(x, x 0 ) + p k=1 1 |A p | x,y∈A k α(y, x 0 ) - p k=1 1 |A p | x,y∈A k α(x 0 , x 0 ) - p k=1 1 |A p | x,y∈A k α(x, x 0 ) = 2 p k=1 x∈A k α(x, x 0 ) - p k=1 |A k |α(x 0 , x 0 ) -∆ α (A 1 , . . . , A p ) = 2 x∈T α(x, x 0 ) -|T |α(x 0 , x 0 ) -∆ α (A 1 , . . . , A p )</formula><p>♦ so that minimizing ∆ α is equivalent to maximizing ∆ β .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.4">Spectral clustering 19.4.1 Spectral approximation of minimum discrepancy</head><p>One refers to spectral methods algorithms that rely on computing eigenvectors and eigenvalues (the spectrum) of data-dependent matrices. In the case of minimizing discrepancies, they can be obtained by further simplifying (19.7), essentially by removing constraints.</p><p>One indeed gets a simpler problem if the non-negativity constraint, Z ≥ 0, is removed. Doing so, one cannot guarantee anymore that Z ⪯ Id R N , so we need to reinstate this constraint. We will first make the further simplification to remove the constraint Z1 N = 1 N , the problem becoming minimizing trace(S α Z) over all Z ∈ S + N (R) such that 0 ⪯ Z ⪯ Id R N and trace(Z) = p. Decomposing Z in an eigenbasis, i.e., looking for it in the form</p><formula xml:id="formula_1737">Z = N j=1 ξ j e j e T j ,</formula><p>this is equivalent to minimizing N j=1 ξ j e T j S α e j <ref type="bibr">(19.9)</ref> subject to 0 ≤ ξ j ≤ 1, N j=1 ξ j = p and u 1 , . . . , u N orthonormal basis of R N . First consider minimization with respect to the basis, fixing ξ. There is obviously no loss of generality in requiring that ξ 1 ≤ ξ 2 ≤ • • • ≤ ξ N , and using corollary 2.4 (adapted to minimizing (19.9) rather than maximizing it) we know that an optimal basis is given by the eigenvectors of S α , ordered with non-decreasing eigenvalues. Letting λ 1 ≤ • • • ≤ λ N denote these eigenvalues, we find that ξ 1 , . . . , x N must be a nondecreasing sequence minimizing The following algorithm (similar to that discussed in <ref type="bibr" target="#b82">[64]</ref>) summarizes this discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.7 (Spectral clustering: version 1)</head><p>Let S α be an N × N discrepancy matrix. Let p denote the number of clusters.</p><p>(1) Compute the eigenvectors of S α associated with the p smallest eigenvalues.</p><p>(2) Denoting these eigenvectors by e 1 , . . . , e p , define y 1 , . . . , y N ∈ R p by y This algorithm needs to be slightly modified if one also wants Z to satisfy Z1 = 1. In that case, 1 is one of the eigenvectors (with eigenvalue 1), and the others are orthogonal to it. As a consequence, one now looks for Z in the form</p><formula xml:id="formula_1738">Z = N -1 k=1 ξ j e j e T j + 1 N 11 T</formula><p>leading to the minimization of</p><formula xml:id="formula_1739">N -1 j=1</formula><p>ξ j e T j S α e j + 1 N 1 T S α 1 over all ξ 1 , . . . , ξ N -1 such that 0 ≤ ξ j ≤ 1 and N j=1 ξ j = p -1, and over all e 1 , . . . , e N -1 such that e 1 , . . . , e N -1 , 1/ √ N form an orthonormal basis. The main difference with the previous problem is that we now need to ensure that all e j are perpendicular to 1.</p><p>To achieve this, introduce the projection matrix P = Id R N -11 T /N and let Sα = P S α P . Then, since u T 1 = 0 implies u T Sα u = u T S α u, it is equivalent to minimize N -1 j=1 ξ j e T j Sα e j over all ξ 1 , . . . , ξ N -1 such that 0 ≤ ξ j ≤ 1 and N j=1 ξ j = p -1, and over all e 1 , . . . , e N -1 such that e 1 , . . . , e N -1 , 1/ √ N form an orthonormal basis. Because Sα 1 = 0, we know that Sα can be diagonalized in an orthonormal basis (e 1 , . . . , e N -1 , 1/ √ N ), and we obtain an optimal solution by selecting the p -1 vectors associated with smallest eigenvalues, with associated ξ j = 1. We therefore get a modified version of the spectral clustering algorithm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.5">Graph partitioning</head><p>Similarity measures are often associated with graph structures, with a goal of finding a partition of their set of vertices. So, let T denote the set of these vertices and assume that to all pairs x, y ∈ T , one attribute a weight given by β(x, y), where β is assumed to be non-negative. We define β for all x, y ∈ T , but we interpret β(x, y) = 0 as marking the absence of an edge between x and y. Let V denote the vector space of all functions f : T → R (we have dim(V ) = |T |).</p><p>This space can be equipped with the standard Euclidean norm, that we will call in this section the L 2 norm (by analogy with general spaces of square integrable functions), letting,</p><formula xml:id="formula_1740">|f | 2 2 = x∈T f (x) 2 .</formula><p>One can also associate a measure of smoothness for a function f ∈ V by computing the discrete "H 1 " semi-norm,</p><formula xml:id="formula_1741">|f | 2 H 1 = x,y∈T β(x, y)(f (x) -f (y)) 2 .</formula><p>With this definition, "smooth functions" tend to have similar values at points x, y in T such that β(x, y) is large while there is less constraint when β(x, y) is small. In particular, |f | H 1 = 0 if and only if f is constant on connected components of the graph. <ref type="foot" target="#foot_21">3</ref>The notion of connected components, combined with thresholding, can be used to build a hierarchical family of partitions of the graph. Define, for all t &gt; 0, the thresholded weights β (t) (x, y) = max(β(x, y)t, 0). The set of connected components associated with the pair (V , β (t) ) forms a partition, say, A (t) , of T . The resulting set of partitions is nested in the sense that, if s &lt; t, the sets forming the partition A (s) are unions of sets forming A (t) . This thresholding procedure is not always satisfactory, however, because there does not always exist a fixed value of t that produces a good quality cluster decomposition.</p><p>If there exists p connected components, then the subspace of all functions f ∈ V such that |f | H 1 = 0 has dimension p. If C 1 , . . . , C p are the connected components, this space is generated by the functions δ C k , k = 1, . . . , p, with δ C k (x) = 1 if x ∈ C k and 0 otherwise. These functions form, in addition, an orthogonal system for the Euclidean inner product:</p><formula xml:id="formula_1742">⟨δ C k , δ C l ⟩ 2 = 0 if k l.</formula><p>One can write 1  2 |f | 2 H 1 = f T Lf where L, called the Laplacian operator associated to the considered graph, is defined by</p><formula xml:id="formula_1743">Lf (x) = y∈T L(x, y)f (y) and L(x, y) =        z∈T β(x, z)        1 x=y -β(x, y). (19.10)</formula><p>The vectors δ C k , k = 1, . . . , p are then an orthogonal basis of the null space of L. Conversely, let (e 1 , . . . , e p ) be any basis of this null space. Then, there exists an invertible matrix A = (a ij , i, j = 1, . . . , p) such that</p><formula xml:id="formula_1744">e i (x) = p j=1 a ij δ C j (x). Associate to each x ∈ T the vector e(x) =           e 1 (x) . . . e p (x)           ∈ R p .</formula><p>Then, for any x, y ∈ T , we have e(x) = e(y) if and only if δ C j (x) = δ C j (y) for all j = 1, . . . , p (because A is invertible), that it, if and only if x and y belong to the same connected component. So, given any basis of the null space of L, the function x → e(x) determines these connected components. So, a-not very efficient-way of determining the connected components of the graph can be to diagonalize the operator L (written as an N by N matrix, where N = |T |), extract the p eigenvectors e 1 , . . . , e p associated with eigenvalue zero and deduce from the function e(x) above the set of connected components. Now, in practice, the graph associated to T and β will not separate nicely into connected components in order to cluster the training set. Most of the time, because of noise or some weak connections, there will be only one such component, or in any case much less than what one would expect when clustering the data. The previous discussion suggests, however, that in the presence of moderate noise in the connection weights, one may expect that the eigenvectors associated to the p smallest eigenvalues of L provide vectors e(x), x ∈ T such that e(x) and e(y) have similar values if x and y belong to the same cluster (see <ref type="bibr">19.2)</ref>. In such cases, these clusters should be easy to determine using, say, K-means on the transformed dataset T = (e(x), x ∈ T ). This is summarized in the following algorithm. Algorithm 19.9 (Spectral Graph Partitioning) Let T ⊂ R be the training set and (x, y) → β(x, y) a similarity measure defined on T × T . Let p be the desired number of clusters.</p><p>(1) Form the Laplacian operator described in <ref type="bibr">(19.10)</ref> and let e 1 , . . . , e p be its eigenvectors associated to the p lowest eigenvalues. For x ∈ T , let e(x) ∈ R p be given by e(x) = (e 1 (x), . . . , e p (x)) T ∈ R p .</p><p>(2) Apply the K-means algorithm (or one of its variants) with p clusters to T = (e(x), x ∈ T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.6">Deciding the number of clusters 19.6.1 Detecting elbows</head><p>The number, p, of subsets with respect to which the population should be partitioned is rarely known a priori, and several methods have been introduced in the literature in order to assess the ideal number of clusters. We now review some of these methods, and denote, for this purpose, by L * (p) the minimized cost function obtained with p clusters, e.g., using (19.  in the case of K-medoids (this definition is algorithm dependent). It is clear that L * is a decreasing function of p. It is also natural to expect that L * should decrease significantly when p is smaller than the correct number of clusters, while the variation should be more marginal when p is overestimated, because the cost in putting together two sets of points that are far apart (which happens when p is too small) is typically larger than the gain in splitting a homogeneous region in two.</p><p>The simplest approach in this context is to visualize L * (p) as a function of p and try to locate at which value the resulting curve makes an "elbow," i.e., switches from a sharply decreasing slope to a milder one. Figure <ref type="figure" target="#fig_82">19</ref>.3 provides an illustration of this visualization when the true number of clusters is three (the data in each cluster following a normal distribution). When the clusters are well separated, an elbow clearly appears on the graph of Γ * α , but this situation is harder to observe when clusters overlap with each other.</p><p>One can measure the "curvature" at the elbow using the distance between each point in the graph of (p, W * α (p)) and the line between its predecessor and successor. The result gives the criterion</p><formula xml:id="formula_1745">C(p) = L * (p + 1) + L * (p -1) -2L * (p) (L * (p + 1) -L * (p -1)) 2 + 4 ,</formula><p>specifying the elbow point as the value of p at which C attains its maximum. For both examples in fig. <ref type="figure" target="#fig_82">19</ref>.3, this method returns the correct number of clusters (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.6.2">The Cali ński and Harabasz index</head><p>Several other criteria have been introduced in the literature. Cali ński and Harabasz <ref type="bibr" target="#b64">[46]</ref> propose to minimize the ratio of normalized between-group and within-groups sums of squares associated with K-means. For a given p, let c 1 , . . . , c p denote the optimal centers, and A 1 , . . . , A p the optimal partition, with N k = |A k |. The normalized between-group sum of squares is</p><formula xml:id="formula_1746">h α (p) = 1 p -1 p k=1 N k |c k -x| 2</formula><p>and the normalized within-group sum of squares is</p><formula xml:id="formula_1747">w α (p) = 1 N -p p k=1 x∈A k |x -c k | 2</formula><p>Cali ński and Harabasz <ref type="bibr" target="#b64">[46]</ref> suggest to maximize γ CH (p) = h α (p)/w α (p). This criterion can be extended to other types of cluster analysis. We have seen in section 19.4 that, when α(x, y) = |x -y| 2 , 1 2</p><formula xml:id="formula_1748">p k=1 x,y∈A k α(x, y)/N k = p k=1 x∈A k |x -c k | 2 .</formula><p>We also have</p><formula xml:id="formula_1749">x∈T |x -x| 2 = p k=1 x∈A k |x -c k | 2 + p k=1 N k |c k -x| 2</formula><p>and the left-hand side is also equal to 1 2N</p><p>x,y∈T α(x, y).</p><p>It follows that, when α(x, y) = |x -y| 2 ,</p><formula xml:id="formula_1750">h α (p) = 1 2(p -1)         1 N x,y∈T α(x, y) - p k=1 x,y∈A k α(x, y)/N k         and w α (p) = 1 2(N -p) p k=1 x,y∈A k α(x, y)/N k .</formula><p>These expressions can obviously be applied to any dissimilarity measure, extending γ CH to general clustering problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.6.3">The "silhouette" index</head><p>For x ∈ T , let This index measures how well x is classified in the partitioning. It is large when the mean distance between x and other objects in its class is small compared to the minimum mean distance between x and any other class. In order to estimate the best number of clusters with this criterion, one then can maximize the average index:</p><formula xml:id="formula_1751">d α (x, A k ) = 1 N k y∈A k α(x, y).</formula><formula xml:id="formula_1752">γ R (p) = 1 N x∈T s α (x, p).</formula><p>Remark 19.9 One can rewrite the Cali ński and Harabasz index using the notation introduced for the silhouette index. Indeed, let A(x) be the cluster A k to which x belongs. Then</p><formula xml:id="formula_1753">h α (p) = 1 2(p -1) x∈T p k=1 N k N (d α (x, A k ) -d α (x, A(x)))</formula><p>and Let us assume that c 1 , . . . , c p are uniformly spaced, so that the sets Āk have similar volumes (close to 1/p) and have roughly spherical shapes (see fig. <ref type="figure" target="#fig_101">19.4</ref>). This implies that</p><formula xml:id="formula_1754">w α (p) = 1 2(N -p) p k=1 x∈A k d α (x, A k ). ♦</formula><formula xml:id="formula_1755">Āk |x -c k | 2 dx ≃ vol(A k ) r 2 p d d + 2</formula><p>where r p is the radius of a sphere of volume 1/p, i.e., pr d p ≃ d/Γ d-1 where Γ d-1 is the surface area of the unit sphere in R d . So, we should have, for some constant C that only depends on d,</p><formula xml:id="formula_1756">x∈A k |x -c k | 2 ≃ N k Āk |x -c k | 2 dx ≃ C(d)(pN )p -2/d-1 = C(d)N p -2/d .</formula><p>This suggests that, for fixed N and d, p 2/d L * (p) should vary slowly when p overesti-mate the number of clusters (assuming that this operation divides an homogeneous cluster). Based on this analysis, Krzanowski and Lai <ref type="bibr" target="#b129">[111]</ref> introduced the differenceratio criterion, namely, Another similar approach, introduced by Sugar and James <ref type="bibr" target="#b203">[185]</ref>, is based on an analysis of mixtures of Gaussian, namely assuming an underlying model with p 0 groups, where data in group k follow a Gaussian distribution N (µ k , Id) (possibly after standardizing the covariance matrix). In that work, the authors show that, if d (the dimension) tends to infinity, with the minimal distance between centers growing proportionally to √ d, then L * (p)/d tends to infinity when p &lt; p 0 . They also show that, with similar assumptions, L * (p)/d behaves like p -2/d for p ≥ p 0 , still for large dimensions. Based on this, they suggest using the criterion</p><formula xml:id="formula_1757">γ KL (p) = (p -1)</formula><formula xml:id="formula_1758">γ SJ (p) = L * (p) d -ν - L * (p -1) d -ν</formula><p>(with the convention that L * (0) = 0) for some positive number ν and select the value of p that maximizes γ SJ . Indeed, in the case of Gaussian mixtures, the choice ν = d/2 ensures that, in large dimensions, γ SJ (p) is small for p &lt; p 0 , that it is close to 1 for p &gt; p 0 and close to p 0 for p = p 0 .</p><p>A more computational approach, based on Monte-Carlo simulations has been introduced in Tibshirani et al. <ref type="bibr" target="#b209">[191]</ref>, defining the gap index</p><formula xml:id="formula_1759">γ T W H (p) = E(L * (p, T ♯ )) -L * (p, T )</formula><p>where the L * (p, T ) denotes the optimal value of the optimized cost with p clusters for a training set T . The notation T ♯ represent a random training set, with same size and dimension as T , generated using an unclustered probability distribution used as a reference. In Tibshirani et al. <ref type="bibr" target="#b209">[191]</ref>, this distribution is taken as uniform (over the smallest hypercube containing the observed data), or uniform on the coefficients of a principal component decomposition of the data (see chapter 20). The expectation E(L * (p, T ♯ )) is computed by Monte-Carlo simulation, by sampling many realizations of the training set T , running the clustering algorithm for each of them and averaging the optimal costs. One can expect L * (p, T ) (for observed data) to decrease much faster (when adding a cluster) than its expectation for homogeneous data when p &lt; p 0 , and the decrease of both terms to be comparable when p ≥ p 0 . So the number of clusters can in principle be estimated by detecting an elbow in the graph of γ T W H (p) as a function of p. The procedure suggested in Tibshirani et al. <ref type="bibr" target="#b209">[191]</ref> in order to detect this elbow if to look for the first index p such that The main parameters in this model were the number of classes, p, and the probabilities α j associated to each cluster, and the parameter of the conditional distribution (e.g., N (c j , σ 2 Id R d )) of X conditionally to being in the jth cluster. In the approach we described, these parameters were estimated from data using maximum likelihood (through the EM algorithm) and probabilities f Z (j|x) were then estimated in order to compute the most likely clustering.We interpreted f Z (j|x) as the conditional probability P (Z = z|X = x), where Z ∈ {1, . . . , p} represents the group variable. The natural generative order is Z → X: first decide to which group the observation belongs to, then sample the value of X conditional to this group. Clustering is in this case reversing the order, i.e., computing the posterior distribution of Z given X.</p><p>In a Bayesian approach, the parameters p, α, c and σ 2 are also considered as random variables, so that (letting θ denote the vector formed by these parameters), the generative random sequence becomes θ → Z → X. Importantly, θ is assumed to be generated once for all, even if several samples of X are observed, yielding the generative sequence for an N -sample, θ → (Z 1 , . . . , Z N ) → (X 1 , . . . , X N ).</p><p>We use below underlined letters to denote configurations of points, Z = (Z 1 , . . . , Z N ), X = (X 1 , . . . , X N ), etc. We also use capital letters or boldface letters (for Greek symbols) to differentiate random variable from realizations.</p><p>Clusters are still evaluated based on the conditional distribution of Z given X, but this distribution must be evaluated by averaging the conditional distribution of   In this expression, P (θ)dθ implies an integration with respect to the prior distribution of the parameters. This distribution is part of the design of the method, but one usually chooses it so that it leads to simple computations, using so-called conjugate priors, which are such that posterior distributions belong to the same parametric family as the prior. For example, the conjugate prior for the mean of a Gaussian distribution (such as c i in our model) is also a Gaussian distribution. The conjugate prior for a scalar variance is the inverse gamma distribution, with density</p><formula xml:id="formula_1760">v u Γ (u) s -u-1 exp(-v/s)</formula><p>for some parameters u, v. A conjugate prior for the class probabilities α = (α 1 , . . . , α p ) is the Dirichlet distribution, with density</p><formula xml:id="formula_1761">D(α 1 , . . . , α p ) = Γ (a 1 + • • • + a p ) Γ (a 1 ) • • • Γ (a p ) p j=1 α a j -1 j</formula><p>on the simplex</p><formula xml:id="formula_1762">S p = {(α 1 , . . . , α p ) ∈ R p : α i ≥ 0, α 1 + • • • + α p = 1}.</formula><p>Note that these conjugate priors have the same form (up to normalization) as the parametric model densities when considered as functions of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.7.2">Model with a bounded number of clusters</head><p>We first discuss the Bayesian approach assuming that the number of clusters is smaller than a fixed number, p. In this example, we assume that c 1 , . . . , c p are modeled as independent Gaussian variables N (0, τ 2 Id R d ), σ 2 with an inverse gamma distribution with parameters u and v and (α 1 , . . . , α p ) using a Dirichlet distribution with parameters (a, . . . , a). Analytical example. The joint probability density of (X, Z) and θ is proportional to</p><formula xml:id="formula_1763">(σ 2 ) -u-1 e -v/σ 2 e - p j=1 |c j | 2 /2τ 2 p j=1 α a-1 j N k=1 e -|x k -c z k | 2 /2σ 2 (σ 2 ) d/2 N k=1 α z k = (σ 2 ) -u-dN /2-1 exp        -(v + 1 2 N k=1 |x k -c z k | 2 )/σ 2        p j=1 α a+N j -1 j .</formula><p>One can explicitly integrate this last expression with respect to σ 2 and α, using the expressions of the normalizing constants in the inverse gamma and Dirichlet distributions, yielding (after integration and ignoring constant terms)</p><formula xml:id="formula_1764">Γ (a + N 1 ) • • • Γ (a + N p ) (v + 1 2 N k=1 |x k -c z k | 2 ) u+dN /2 exp         - p j=1 |c j | 2 /2τ 2         = Γ (a + N 1 ) • • • Γ (a + N p ) (v + 1 2 S w + 1 2 p j=1 N j |c j -xj | 2 ) u+dN /2 exp         - p j=1 |c j | 2 /2τ 2        </formula><p>where S w = N k=1 |x k -xz k | 2 is the within group sum of squares. Note that this sum of squares depends on x and z, and that (N 1 , . . . , N p ), the group sizes, depend on z.</p><p>Let us assume a "non-informative prior" on the centers, which corresponds to letting τ tend to infinity and neglecting the last exponential. The remaining expression can now be integrated with respect to c 1 , . . . , c p by making a change of variables µ j = N j /(2v + S k )(c jx j ) and using the fact that</p><formula xml:id="formula_1765">(R d ) p dc 1 . . . dc p (v + 1 2 S w + 1 2 p j=1 N j |c j -xj | 2 ) u+dN /2 = (2v + S w ) (p-N )d/2-u) p j=1 N -d/2 j (R d ) p dµ 1 . . . dµ p ( 1 2 + 1 2 p j=1 |µ j | 2 ) u+dN /2</formula><p>and the final integral does not depend on x or z. It follows from this that the conditional distribution of Z given x takes the form</p><formula xml:id="formula_1766">P (z|x) = C(x) p j=1 Γ (a + N j ) (2v + S w ) (N -p)d/2+u) p j=1 N d/2 j</formula><p>where C(x) is a normalization constant ensuring that the right-hand side is a probability distribution over configurations z = (z 1 , . . . , z N ) ∈ {1, . . . , p} N . In order to obtain the most likely configuration for this posterior distribution, one should therefore minimize in z the function</p><formula xml:id="formula_1767">((N -p) d 2 + u) log(2v + S w ) + d 2 p j=1 log N j - p j=1 log Γ (a + N j ).</formula><p>This final optimization problem cannot be solved in closed form, but this can be performed numerically. One can simplify it a little by only keeping the main order terms in the last two sums (using Stirling formula for the Gamma function) and minimize</p><formula xml:id="formula_1768">((N -p) d 2 + u) log(2v + S w ) - p j=1 (a + N j ) log(a + N j ).</formula><p>This expression has a nice interpretation, since the first term minimizes the withingroup sum of squares, the same objective function as in K-means, and the second one is an entropy term that favors clusters with similar sizes.</p><p>Monte-Carlo simulation. An alternative to this analytical approach is to use Monte-Carlo simulations to estimate some properties of the posterior distribution numerically. While they are often computationally demanding, Monte-Carlo methods are more flexible and can be used in situations when analytic computations are intractable . In order to sample from the distribution of Z given x, it is actually easier to sample from the joint distribution of (Z, θ) given x, because this distribution has a simpler form. Of course, if the pair (Z, θ) is sampled from the conditional distribution given x, the first component, Z will follow the posterior distribution we are interested in.</p><p>In the context of the discussed example, this reduces to sampling from a distribution proportional to <ref type="bibr">.11)</ref> Sampling from all these variables at once is not tractable, but it is easy to sample from them in sub-groups, conditionally to the rest of the variables. We can, for example, deduce from the expression above the following conditional distributions.</p><formula xml:id="formula_1769">(σ 2 ) -u-1 e -v/σ 2 e - p j=1 |c j | 2 /2τ 2 p j=1 α a-1 j N k=1 e -|x k -c z k | 2 /2σ 2 (σ 2 ) d/2 N k=1 α z k . (<label>19</label></formula><p>(i) Given (α, c, z), σ 2 follows an inverse gamma distribution with parameters u + dN /2 and v + 1</p><formula xml:id="formula_1770">2 N k=1 |x k -c z k | 2 .</formula><p>(ii) Given (z, z, σ 2 ), α follows a Dirichlet distribution with parameters a+N 1 , . . . , a+ N p .</p><p>(iii) Given (z, σ 2 , α), c 1 , . . . , c p are independent and follow a Gaussian distribution, respectively with mean (1 + σ 2 /(N j τ 2 )) -1 xj and variance (N j /σ 2 + 1/τ 2 ) -1 .</p><p>(iv) Given (σ 2 , α, c), z 1 , . . . , z N are independent and</p><formula xml:id="formula_1771">P (z k = j|σ 2 , α, c, x) ∝ α j e -|x k -c j | 2 /2σ 2 .</formula><p>Algorithm 19.10 (Gibbs sampling for mixture of Gaussian (Bayesian case))</p><p>(1) Initialize with variables α, c, σ and z, for example generated according to the prior distribution.</p><p>(2) Loop a large number of times over the following steps.</p><p>(i) Simulate a new value of σ 2 according to an inverse gamma distribution with parameters u + dN /2 and v + 1</p><formula xml:id="formula_1772">2 N k=1 |x k -c z k | 2 .</formula><p>(ii) Simulate new values for α 1 , . . . , α p according to a Dirichlet distribution with parameters a + N 1 , . . . , a + N p .</p><p>(iii) Simulate new values for c 1 , . . . , c p independently, sampling c i according to a Gaussian distribution with mean (1 + σ 2 /(N j τ 2 )) -1 xj and variance (N j /σ 2 + 1/τ 2 ) -1 .</p><p>(iv) Simulate new values of z 1 , . . . , z N independently such that</p><formula xml:id="formula_1773">P (z k = j|σ 2 , α, c, x) ∝ α j e -|x k -c j | 2 /2σ 2 .</formula><p>Note that this algorithm is only asymptotically providing a sample of the posterior distribution (it has to be stopped at some point, of course). Note also that, at each step, the labels z 1 , . . . , z N provide a random partition of the set {1, . . . , N }, and this partition changes at every step.</p><p>To estimate one single partition out of this simulation, several strategies are possible. Using the simulation, one can estimate the probability w kl that x k and x l belong to the same cluster. This can be dome by averaging the number of times that z k = z l was observed along the Gibbs sampling iterations (from which one usually excludes a few early "burn-in" iterations). These weights, w kl can then be used as similarity measures in a clustering algorithm.</p><p>Alternatively, one can average for each k, the values of the class center c z k associated to k, still along the Gibbs sampling iterations. These average values can then be used as input of, say, a K-means algorithm to estimate final clusters.</p><p>Mean-field approximation. We conclude this section with a variational Bayes approximation of the posterior distribution. We will make a mean-field approximation, in which all parameters and latent variables are independent, therefore approximating the distribution in <ref type="bibr">(19.11</ref>) by a product distribution taking the form</p><formula xml:id="formula_1774">g(σ 2 , α, c, z) = g (σ 2 ) (σ 2 )g (α) (α) p j=1 g (c) j (c j ) N k=1 g (z) k (z k ).</formula><p>Here c = (c 1 , . . . , c p ), z = (z 1 , . . . , z N ) and α = (α 1 , . . . , α p ). We have σ 2 ∈ (0, +∞), c ∈ (R d ) p , α ∈ S, the set of all non-negative α 1 , . . . , α p that sum to one, and z ∈ {1, . . . , p} N   (so that g</p><formula xml:id="formula_1775">(x)</formula><p>k is a p.m.f. on {1, . . . , p}. We will use the discussion in section 16.3.3 and lemma 16.1, and use the notation introduced in that section to denote as ϕ the expectation a variable ϕ of the variables above for the p.d.f. g.</p><p>The log-likelihood for a mixture of Gaussian takes the form (ignoring contant terms)</p><formula xml:id="formula_1776">ℓ(σ 2 , α, c, z) = -(u + 1) log σ 2 -vσ -2 - 1 2τ 2 p k=1 |c j | 2 + p j=1 (a -1) log α j - N d 2 log σ 2 - 1 2 σ -2 N k=1 |x k -c z k | 2 + N k=1 log α z k = -(u + 1) log σ 2 -vσ -2 - 1 2τ 2 p k=1 |c j | 2 + p k=1 (a -1) log α j - N d 2 log σ 2 - 1 2 σ -2 N k=1 p j=1 |x k -c j | 2 1 z k =j + N k=1 p j=1 log α j 1 z j =k</formula><p>and can therefore be decomposed as a sum of products of functions of single variables, as assumed in section 16.3.3. Using lemma 16.1, we can identify each of the distributions composing g, namely:</p><p>• g (σ 2 ) is the p.d.f. of an inverse gamma with parameters ũ = u + N d/2 and ṽ = ν + 1 2</p><formula xml:id="formula_1777">N k=1 p j=1 |x k -C j | 2 Z k = j . • g (c)</formula><p>j is the p.d.f. of a Gaussian, with parameters N ( mj , σ 2 j Id R d ), with, letting</p><formula xml:id="formula_1778">ζ(j) = N k=1 Z k = j = N k=1 g (z) k (j), σ 2 j = 1 τ 2 + σ -2 ζ(j) -1</formula><p>and mi = σ -2 σ 2 j N k=1 Z k = j x k . • g (α) of a Dirichlet distribution, with parameters ã1 , . . . , ãk , with ãi = a + ζ(j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Finally g (z)</head><p>k is a p.m.f. on {1, . . . , p} with</p><formula xml:id="formula_1779">g (z) k (j) ∝ exp - 1 2 σ -2 |x k -C j | 2 + log α j .</formula><p>To complete the consistency equations, it now suffices to evaluate the expectations in the formula above as functions of the other parameters. We leave to the reader the verification of the following statements.</p><p>• If σ 2 follows an inverse gamma distribution with parameters ũ and ṽ, then σ -2 = ũ/ ṽ.</p><formula xml:id="formula_1780">• If C j ∼ N ( mj , σ 2 j Id R d ), then |x k -C j | 2 = |x k -mj | 2 + d σ 2 j .</formula><p>• If α follows a Dirichlet distribution with parameters ã1 , . . . , ãp , then log</p><formula xml:id="formula_1781">α j = ψ( ãj ) -ψ( ã1 + • • • + ãp )</formula><p>where ψ is the digamma function (derivative of the logarithm of the gamma function).</p><p>Combining these facts with the expression of the mean-field parameters, we can now formulate a mean-field estimation algorithm for mixtures of Gaussian that iteratively applies the consistency equations.  (8) Compare the updated variables with their previous values and stop if the difference is below a tolerance level. Otherwise, return to (3).</p><formula xml:id="formula_1782">= 1 u + N d/2         v + 1 2 N k=1 p j=1 gk (j)|x k -mj | 2 + d 2 p j=1 σ 2 j ζ(j)         . (<label>5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>After convergence g (z)</head><p>k provides the mean-field approximation of the posterior probability of classes for observation k and can be used to determine clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19.7.3">Non-parametric priors</head><p>The Polya urn In the previous model with p clusters or less, the joint distribution of Z 1 , . . . , Z N is given by</p><formula xml:id="formula_1783">π(z 1 , . . . , z N ) = Γ (pa) Γ (a) p S p p j=1 α a+N j -1 j dα = Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)</formula><p>.</p><p>Conditional to z 1 , . . . , z N , the data model was completed by sampling p sets of parameters, say, θ 1 , . . . , θ p , each belonging to a parameter space Θ and following a prior probability distribution with density, say, ψ and variables X 1 , . . . , X N , where X k ∈ R was drawn according to a law dependent on its cluster, that we will denote ϕ(</p><formula xml:id="formula_1784">• | θ z k ).</formula><p>The complete likelihood of the data is now</p><formula xml:id="formula_1785">L(z, θ, x) = Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a) p j=1 ψ(θ j ) N k=1 ϕ(x k |θ z k ).</formula><p>Note that the right-hand side does not change if one relabels the values of z 1 , . . . , z N , i.e., if one replaces each z k by s(z k ) where s is a permutation of {1, . . . , p}, creating a new configuration denoted s • z. Let [z] denote the equivalence class of z, containing all z ′ = s • z, s ∈ S N : all the labelings in [z] provide the same partition of {1, . . . , N } and can therefore be identified. One defines a probability distribution π over these equivalence classes by letting</p><formula xml:id="formula_1786">π([z]) = |[z]| Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)</formula><p>.</p><p>The first term on the right-hand side is the number of elements in the equivalence class of [z]. To compute it, let p 0 = p 0 (z) denote the number of different values taken by z 1 , . . . , z N , i.e., the "true" number of clusters (ignoring the empty ones), which now is a function of z. Let A 1 , . . . , A p 0 denote the partition associated with z.</p><p>New labelings equivalent to z can be obtained by assigning any index i 1 ∈ {1, . . . , p} to elements of A 1 , then any index i 2 i 1 to elements of A 2 , etc., so that there are |[z]| = p!/(pp 0 )! choices. We therefore find:</p><formula xml:id="formula_1787">π([z]) = p! (p -p 0 )! Γ (pa) Γ (pa + N ) p j=1 Γ (a + N j ) Γ (a)</formula><p>.</p><p>Letting λ = pa and using the formula Γ (x + 1) = xΓ (x), this can be rewritten as</p><formula xml:id="formula_1788">π([z]) = p(p -1) • • • (p -p 0 + 1) λ(λ + 1) . . . (λ + N -1) p j=1 N j -1 i=0 (λ/p + i).</formula><p>Now, the class [z] contains exactly one element ẑ with the following properties</p><formula xml:id="formula_1789">• ẑ1 = 1,</formula><p>• ẑk ≤ max(z j , j &lt; k) + 1 for all k &gt; 1.</p><p>This means that the kth label is either one of those already appearing in ( ẑ1 , . . . , ẑk-1 ) or the next integer in the enumeration. We will call such a ẑ admissible. If we assume that z is admissible in the expression of π, we can write</p><formula xml:id="formula_1790">π([z]) = p 0 j=1 λ(1 -j/p) N j -1 i=1 (λ/p + i) λ(λ + 1) . . . (λ + N -1)</formula><p>.</p><p>If one takes the limit p → ∞ in this expression, one still gets a probability distribution on admissible labelings, namely π([z]) = λ p 0 p 0 j=1 (N j -1)! λ(λ + 1) . . . (λ + N -1)</p><p>. <ref type="bibr">(19.12)</ref> Recall that, in this equation, p 0 is a function of z, equal, for admissible labelings, to the largest j such that N j &gt; 0.</p><p>The probability π is generated by the following sampling scheme, called the Polya urn process simulating admissible labelings. Algorithm 19.12 (Polya Urn)</p><formula xml:id="formula_1791">1 Initialize k = 1, z 1 = 1, j = 1. Let N 1 = 1</formula><p>2 At step k, assume that z 1 , . . . , z k have been generated, with associated number of clusters equal to j and N 1 , . . . , N j elements per cluster. Generate z k+1 such that Using this prior, the complete model for the distribution of the observed data is</p><formula xml:id="formula_1792">z k+1 =            i with probability N i λ + k , for i =</formula><formula xml:id="formula_1793">L(z, θ, x) = λ p 0 p 0 j=1 (N j -1)! λ(λ + 1) . . . (λ + N -1) p 0 j=1 ψ(θ j ) N k=1 ϕ(x k |θ z k )</formula><p>Recall that, in this expression, z is restricted to the set of admissible labelings. We also note that admissible labelings are in one-to-one correspondence with the partitions of {1, . . . , N }, so that the latent variable z in this expression can also be interpreted as representing a random partition of this set.</p><p>Dirichlet processes. As we will see later, the expression of the global likelihood and the Polya urn model will suffice for us to develop non-parametric clustering methods for a set of observations x 1 , . . . , x N . However, this model is also associated to an important class of random probability distributions (i.e., random variables taking values in some set of probability distributions) called Dirichlet processes for which we provide a brief description.</p><p>The distribution in <ref type="bibr">(19.12)</ref> was obtained by passing to the limit from a model that first generates p numbers α 1 , . . . , α p , then generates the labels z 1 , . . . , z N ∈ {1, . . . , p} identified modulo relabeling. This distribution can also be defined directly, by first defining an infinity of positive numbers (α j , j ≥ 1) such that ∞ i=1 α i = 1, followed by the generation of random labels Z 1 , . . . , Z N such that P (Z k = j) = α j , followed once again with an identification up to relabeling.</p><p>The distribution of α that leads to the Polya urn is called the stick breaking process. This process is such that</p><formula xml:id="formula_1794">α j = U j j-1 i=1 (1 -U i )</formula><p>where U 1 , U 2 , . . . is a sequence of i.i.d. variables following a Beta(1, λ) distribution, i.e., with p.d.f. λ(1u) λ-1 for u ∈ [0, 1]. The stick breaking interpretation comes from the way α 1 , α 2 , . . . can be simulated: let α 1 ∼ Beta(1, λ); given α 1 , . . . , α j-1 , let α j = (1 -α 1 -• • • -α j-1 )U j where U j ∼ Beta(1, λ) and is independent from the past. Each step can be thought of as breaking the remaining length, (1 -α 1 -• • • -α j-1 ), of an original stick of length 1 using a beta-distributed variable, U j . This process leads to the distribution (19.12) over admissible distributions, i.e., if α is generated according to the stick breaking process, and Z 1 , . . . , Z N are independent, each such that P (Z k = j) = α j , then the probability that (Z 1 , . . . , Z N ) is identical, after relabeling, to the admissible configuration z is given by <ref type="bibr">(19.12)</ref>. (We skip the proof of this result, which is not straightforward.) Now, take a realization α = (α 1 , α 2 , . . .) of the stick-breaking process, and independent realizations η = (η 1 , η 1 , . . .) drawn according to the p.d.f. ψ. Define ρ = ∞ j=1 α j δ η j . <ref type="bibr">(19.14)</ref> For any realization of α and of η, ρ is a probability distribution on the parameter space Θ (in which one chooses η i with probability α i ). Since α and η are both random variables, this defines a random variable ρ with values in the space of probability measures on Θ.</p><p>This process has the following characteristic property. For any family V 1 , . . . , V k ⊂ Θ forming a partition of that set, the random variable (ρ(U 1 ), . . . , ρ(U k )) follows a Dirichlet distribution with parameters</p><formula xml:id="formula_1795">λ U 1 ψ dη, . . . , λ U 1 ψ dη .</formula><p>This is the definition of a Dirichlet process with parameters (λ, ψ), or, simply, with parameter λψ. Conversely, one can also show that any Dirichlet process can be decomposed as in <ref type="bibr">(19.14)</ref> where α is a stick-breaking process and η independent realizations of ψ.</p><p>Monte-Carlo simulation. The joint distribution of labels, parameters and observed variables can also be deduced from <ref type="bibr">(19.12)</ref>, with a joint p.d.f. given by</p><formula xml:id="formula_1796">λ p 0 -1 p 0 j=1 (N j -1)! (λ + 1) • • • (λ + N -1) p 0 j=1 ψ(η j ) N k=1 ϕ(x k |η z k ). (<label>19.15)</label></formula><p>The forward simulation of this distribution is a straightforward extension of Algorithm 19.12, namely:</p><formula xml:id="formula_1797">Algorithm 19.13 1 Initialize k = 1, z 1 = 1, j = 1. Let N 1 = 1.</formula><p>2 Sample η 1 ∼ ψ and x 1 ∼ ϕ(•|η 1 ).</p><p>3 At step k, assume that z 1 , . . . , z k has been generated, with associated number of clusters equal to j and N 1 , . . . , N j elements per cluster. Generate z k+1 such that</p><formula xml:id="formula_1798">z k+1 =            i with probability N i λ + k , for i = 1, . . . , j j + 1 with probability λ λ + k 4 If z k+1 = i ≤ j, sample x k+1 ∼ ϕ( • |η i ). Replace N i by N i + 1, k by k + 1.</formula><p>5 If z k+1 = j + 1, let N j+1 = 1, sample η j+1 ∼ ψ and x k+1 ∼ ϕ( • |η j+1 ). Replace j by j + 1 and k by k + 1.</p><p>6 If k &lt; N , return to step 2, otherwise, stop. This algorithm cannot be used, of course, to sample from the conditional distribution of Z and η given X = x, and Markov-chain Monte-Carlo must be used for this purpose. In order to describe how Gibbs sampling may be applied to this problem, we use the fact that, as previously remarked, using admissible labelings z is equivalent to using partitions A = (A 1 , . . . , A p 0 ) of {1, . . . , N }, and we will use the latter formalism to describe the algorithm. We will also use the notation η A to denote the parameter associated to A ∈ A so our new notation for the variables is (A, η) where A is a partition of {1, . . . , N } and η is a collection (η A , A ∈ A) with η A ∈ Θ. Given this, we want to sample from a conditional p.d.f.</p><formula xml:id="formula_1799">Φ(A, η|x) ∝ λ |A|-1 A∈A (|A| -1)! (λ + 1) • • • (λ + N -1) A∈A ψ(η A ) k∈A ϕ(x k |η A ).<label>(19.16)</label></formula><p>As an additional notation, given a partition A and an index k ∈ {1. . . . , N }, we let A k denote the set A in A that contains k.</p><p>The following points are relevant for the design of the sampling algorithm.</p><p>(1) The conditional distribution of η given A and the training data is proportional to</p><formula xml:id="formula_1800">A∈A        ψ(η A ) k∈A ϕ(x k |η A )       </formula><p>This shows that the parameters η A , A ∈ A are independent of each other, with η A following a distribution proportional to</p><formula xml:id="formula_1801">η → ψ(η) k∈A j ϕ(x k |η).</formula><p>Sampling from this distribution generally offers no special difficulty, especially if the prior ψ is conjugate to ϕ. Importantly, one does not need to sample exactly from η A , and it is often more convenient to separate η A into several components (such as mean and variance for mixtures of Gaussian) and sample from them alternatively, creating another level of Gibbs sampling.</p><p>(2) We now consider the issue of updating A. We will use for this purpose the formalism of Algorithm 12.2. In particular, for each k ∈ {1, . . . , N }, we associate to the variable (A, η) the pair (A (k) , η (k) ), where A (k) is the partition of {1, . . . , N } \ {k} formed by the sets A (k) = A \ {k} and η (k) A = η A , unless A = {k}, in which case the set and the corresponding η A are dropped.</p><p>We can write Φ(A, η|x) in the form</p><formula xml:id="formula_1802">Φ(A, η|x) ∝ q(A k , η A k )ϕ(x k |η A k ) λ |A (k) |-1 B∈A (k) (|B| -1)! (λ + 1) • • • (λ + N -1) B∈A (k) ψ(η B ) l∈B ϕ(x l |η B ) (19.17) with q(A, θ) = B∈A (k) |B|1 A=B∪{k} + λψ(θ)1 A={k}</formula><p>Partitions A ′ that are consistent with A (k) allocate k to one of the clusters in A (k) or create a new cluster with a new parameter η ′ k . If one replaces (A, η) by (A ′ , η ′ ), only the first two terms in (19.17) will be affected, so that the conditional probability of</p><formula xml:id="formula_1803">A ′ given A (k) is proportional to q(A ′ k , η A ′ k )ϕ(x k |η A ′ k ) and given by              |B|ϕ(x k |η B ) C 1 + λC 2 if A ′ k = B ∪ {k}, η ′ B = η B , B ∈ A (k) λϕ(x k |η ′ k )ψ(η ′ k ) C 1 + λC 2 if A ′ k = {k},</formula><p>where</p><formula xml:id="formula_1804">C 1 = B∈A k |B|ϕ(x k |η B )</formula><p>and C 2 = Θ ϕ(x k |θ)ψ(θ)dθ. Concretely, this means that one first decides to allocate k to a set B in A (k) with probability |B|ϕ(x k |η B )/(C 1 +λC 2 ) and to create a new set with probability λC 2 /(C 1 + λC 2 ). If a new set is created, then the associated parameter η ′ {k} is sampled according to the p.d.f. ϕ(x k |θ)ψ(θ/C 2 .</p><p>(3) However, sampling using this conditional probability requires the computation of the integral C 2 , which can represent a significant computational burden, since this has to be done many times in a Gibbs sampling algorithm. A modification of this algorithm, introduced in Neal <ref type="bibr" target="#b159">[141]</ref>, avoids this computation by adding new auxiliary variables at each step of the computation. These variables are m parameters η * 1 , . . . , η * m ∈ Θ where m is a fixed integer. To define the joint distribution of A, η, η * , one lets the marginal distribution of (A, η) be given by <ref type="bibr">(19.16</ref>) and conditionally to A, η, let η * 1 , . . . , η * m be: (i) independent with density ψ if |A k | &gt; 1;</p><p>(ii) such that η * j = η A k and the other m -1 starred parameters are independent with distribution ψ, where j is randomly chosen in {1, . . . , m} if A k = {k}.</p><p>With this definition, the joint conditional distribution of (A, η, η * ) takes the form</p><formula xml:id="formula_1805">Φ(A, η, η * |x) ∝ q(A k , η A k , η * )ϕ(x k |η A k ) λ |A (k) |-1 B∈A (k) (|B| -1)! (λ + 1) • • • (λ + N -1) B∈A (k) ψ(η B ) l∈B ϕ(x l |η B ) (19.18) with q(A, θ, η * 1 , . . . , η * m ) = B∈A (k) |B|1 θ=η B ,A=B∪{k} m j=1 ψ(η * j ) + λ m m j=1 1 θ=η * j ,A={k} ψ(θ) m i=1,i j ψ(η * i ).</formula><p>Note that Φ depends on k, so that the definition of the auxiliary variables will change at each step of Gibbs sampling. The conditional distribution, for Φ, of A ′ , η ′ given</p><formula xml:id="formula_1806">A (k) , η (k) , η * is such that • A ′ k = B ∪ {k} and η ′ A ′ k = η B with probability |B|ϕ(x k |η B )/C, for B ∈ A (k) . • A ′ k = {k} and η A ′ k = η * j with probability (λ/m)ϕ(x k |η * j )/C, j = 1, . . . , m.</formula><p>The constant C is given by</p><formula xml:id="formula_1807">C = B∈A k |B|ϕ(x k |η B ) + λ m m j=1 ϕ(x k |η * j )</formula><p>and is therefore easy to compute.</p><p>We can now summarize this discussion with Neal's version of the Gibbs sampling algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 19.14 (Neal)</head><p>Initialize the algorithm with some arbitrary partition and parameters (A, η) (for example, generated using the Dirichlet prior). Use the same notation to denote these variables at the end of the previous iteration of the algorithm. The next iteration is then run as follows.</p><p>(1) For k = 1, . . . , N , reallocate k to a cluster as follows.</p><p>(i) Form the new family of sets A (k) and labels η (k) by removing k from the partition A. (ii) If |A k | &gt; 1, generate m variables η * 1 , . . . , η * m according to ψ. If A k = {k}, generate only m -1 such variables and let the last one be equal to η A k . (iii) Allocate k to a new cluster A ′ with parameter η ′ A ′ according to probabilities proportional to</p><formula xml:id="formula_1808">         |B|ϕ(x k |η (k) B ) if A ′ = B ∪ {k} and η ′ A ′ = η (k) B λ m ϕ(x k |η * j ) if A = {k} and η ′ A ′ = η * j , j = 1, . . . , m</formula><p>(2) For A ∈ A, update η A , A ∈ A according to the distribution proportional to</p><formula xml:id="formula_1809">ψ(η) k∈A ϕ(x k |η)</formula><p>either directly, or via one step of Gibbs sampling visiting each of the variables that constitute η A .</p><p>(3) Loop a sufficient number of times over the previous two steps.</p><p>After running this algorithm, the set of clusters should be finalized by using statistics computed along the simulation, as discussed after Algorithm 19.10.</p><p>Full example: Mixture of Gaussian. To conclude this section, we summarize the Monte-Carlo sampling algorithm for mixtures of Gaussian using a non-parametric Bayesian prior. Here, η ∈ Θ is the center c ∈ R d , with prior distribution ψ = N (0, τ 2 Id R d ).</p><p>The previous algorithm must be modified because an additional parameter σ 2 is shared by all classes, with prior given by an inverse gamma distribution with parameters u and v. The conditional distribution of the data is ϕ(x|c, σ ) ∼ N (c, σ 2 Id R d ).</p><p>Algorithm 19.15 (Gibbs sampling for non-parametric mixture of Gaussian) (1) Initialize the algorithm with some arbitrary partition and parameters (A, η).</p><p>(2) For k = 1, . . . , N , reallocate k to a cluster as follows.</p><p>(i) Form the new family of sets A (k) and labels η (k) by removing k from the partition A.</p><p>(ii (iii) Allocate k to a new cluster A ′ with parameter c ′ A ′ according to probabilities proportional to Factor analysis aims at representing potentially high-dimensional data as functions of a (generally) small number of "factors," with a representation taking the general form X = Φ(Y , θ) + residual, <ref type="bibr">(20.1)</ref> where X is the observation, Y provide the factors and Φ is a function parametrized by θ. A factor analysis model must therefore specify Φ (often, a linear function of Y ), add hypotheses on Y (such as its dimension, or properties of its distribution) and on the residuals. The transformation Φ is estimated from training data, but, ideally, the method should also provide an algorithm that infers Y from a new observation of X. Most of the time, Y is small dimensional so that the model also implies a reduction of dimension.</p><formula xml:id="formula_1810">) If |A k | &gt; 1, generate m variables c * i , i = 1, . . . , m independently with c * i ∼ N (0, τ 2 Id R d ). If A k = {k},</formula><formula xml:id="formula_1811">             |B| exp - |x k -c (k) B | 2σ 2 if A ′ = B ∪ {k} and c ′ A ′ = c (k) B λ m exp - |x k -c * B | 2σ 2 if A = {k} and c ′ A ′ = c * j , j = 1, . . . , m.</formula><p>We start our discussion with principal component analysis (or PCA). This methods can be characterized in multiple ways, and we introducing through the angle of data approximation. In the following, the random variable X takes values in a finiteor infinite-dimensional inner-product space H. We will denote, as usual, by ⟨. , .⟩ H the product in this space. Assume that N independent realization of X, denoted x 1 , . . . , x N , are observed, forming our training set T . Our goal is to obtain a small-dimensional representation of these data, while loosing a minimal amount of relevant information. PCA, is the simplest and most commonly used approach developed for this purpose.</p><p>If V is a finite-dimensional subspace of H, we denote by P V (y) the orthogonal projection of y ∈ H on V , i.e., the element ξ ∈ V such that ∥y -ξ∥ 2  H is minimal (see section 6.4). Recall that this orthogonal projection if characterized by the two properties: (i) P V (y) ∈ V and (ii) (y -P V (y)) ⊥ V .</p><p>Given a target dimension p, PCA determines a p-dimensional subspace of H, say, V and a point c ∈ H, such that, letting</p><formula xml:id="formula_1812">R k = x k -c -P V (x k -c)</formula><p>for k = 1, . . . , N , the residual sum of squares</p><formula xml:id="formula_1813">S = N k=1 ∥R k ∥ 2 H (20.2)</formula><p>is as small as possible.</p><p>An optimal choice for c is c = x = N k=1 x k /N . Indeed, using the linearity of the orthogonal projection, we have</p><formula xml:id="formula_1814">S = N k=1 ∥x k -P V (x k ) -(c -P V (c))∥ 2 H = N k=1 ∥x k -P V (x k ) -(x -P V (x))∥ 2 H + N ∥x -P V (x) -(c -P V (c))∥ 2 H .</formula><p>Given this, there would be no loss of generality in assuming that all x k 's have been replaced by x kx and taking c = 0. While this is often done in the literature, there are some advantages (especially when discussing kernel methods) in keeping the average explicit in the notation, as we will continue to do.</p><p>Introducing an orthonormal basis (e 1 , . . . , e p ) of V , one has</p><formula xml:id="formula_1815">P V (x k -x) = p i=1 ρ k (i)e i</formula><p>with ρ ki = ⟨x kx , e i ⟩ H . One can then reformulate the problem in terms of (e 1 , . . . , e p ), which must minimize</p><formula xml:id="formula_1816">S = N k=1 ∥x k -x - p i=1 ⟨x k -x , e i ⟩e i ∥ 2 H = N k=1 ∥x k -x∥ 2 H - p i=1 N k=1 ⟨x k -x , e i ⟩ 2 H . For u, v ∈ H, define ⟨u , v⟩ T = 1 N N k=1 ⟨x k -x , u⟩ H ⟨x k -x ,</formula><p>v⟩ H and ∥u∥ T = ⟨u , u⟩<ref type="foot" target="#foot_23">foot_23</ref>/2 T (the index T refers to the fact that this norm is associated with the training set). This provides a new quadratic form on H. The formula above shows that minimizing S is equivalent to maximizing p i=1 ∥e i ∥ 2 T subject to the constraint that (e 1 , . . . , e p ) is orthonormal in H. Let us consider a slightly more general problem. If H is a separable Hilbert space 1 and µ is a square-integrable probability measure on H, such that H ∥x∥ 2 H dµ(x) &lt; ∞, one can define m = H xdµ(x) and σ 2 µ = H ∥x -m∥ 2 H dµ. One can then define the covariance bilinear form Γ µ (u, v) = H ⟨u , x -m⟩ H ⟨v , x -m⟩ H dµ(x), which satisfies Γ µ (u, v) ≤ σ 2 µ ∥u∥ H ∥v∥ H . With this notation, we have ⟨u , v⟩ T = Γ μT (u, v), where μT = (1/N ) N k=1 δ x k is the empirical measure (and in that case m = x). We can therefore generalize the PCA problem by considering the maximization of p k=1 Γ µ (e k , e k ) (20.3) over all orthonormal families (e 1 , . . . , e p ) in H.</p><p>When µ is square integrable, the associated operator, A µ defined by ⟨u , A µ v⟩ H = Γ µ (u, v) <ref type="bibr">(20.4)</ref> for all u, v ∈ H, is a Hilbert-Schmidt operator <ref type="bibr" target="#b223">[205]</ref>. Such an operator can, in particular, be diagonalized in an orthonormal basis of H, i.e., there exists an orthonormal basis, (f 1 , f 2 , . . .) of H such that A µ f i = λ 2 i f i for a non-increasing sequence of eigenvalues (with</p><formula xml:id="formula_1817">λ 1 ≥ λ 2 ≥ • • • ≥ 0) such that σ 2 µ = ∞ k=1 λ 2 i .</formula><p>The main statement of the following result is in finite dimensions, a simple application of corollary 2.4. We here give a direct proof that also works in infinite dimensions.</p><p>Theorem 20.1 Let (f 1 , f 2 , . . .) be an orthonormal basis of eigenvectors of A µ with associated eigenvalues λ 2 1 ≥ λ</p><p>2 2 ≥ • • • ≥ 0. Then an orthonormal family (e 1 , . . . , e p ) in H maximizes (20.3) if and only if, span(f j : λ 2 j &gt; λ 2 p ) ⊂ span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≥ λ 2 p ). (20.5) In particular f 1 , . . . , f p always provide a solution and span(e 1 , . . . , e p ) = span(f 1 , . . . , f p ) for any other solution as soon as λ 2 p &gt; λ 2 p+1 . Definition 20.2 When µ = μT , the vectors (f 1 , . . . , f p ) are called (with some abuse when eigenvalues coincide) the first p principal components of the training set (x 1 , . . . , x N ). Proof If (e 1 , . . . , e p ) is an orthonormal family in H, let F(e 1 , . . . , e p ) = p k=1</p><p>Γ µ (e k , e k ) .</p><formula xml:id="formula_1818">Note that F(f 1 , . . . , f p ) = λ 2 1 + • • • + λ 2 p . Write e k = ∞ j=1 α (j) k f j (so that α (j) k = ⟨f j , e k ⟩ H ). These coefficients satisfy ∞ j=1 α (j) k α (j) l = 1 if k = l and 0 otherwise. Then Γ µ (e k , e k ) = ∞ j=1 λ 2 j (α (j) k ) 2 .</formula><p>We have</p><formula xml:id="formula_1819">F(e 1 , . . . , e p ) = p k=1 ∞ j=1 λ 2 j (α (j) k ) 2 = p k=1 p j=1 λ 2 j (α (j) k ) 2 + p k=1 ∞ j=p+1 λ 2 j (α (j) k ) 2 ≤ p k=1 p j=1 λ 2 j (α (j) k ) 2 + p k=1 ∞ j=p+1 λ 2 p+1 (α (j) k ) 2 = p j=1 (λ 2 j -λ 2 p+1 ) p k=1 (α (j) k ) 2 +</formula><p>pλ 2 p+1 . Let P denote the orthogonal projection operator from H to span(e 1 , . . . , e p ). We have, for any h ∈ H, ∥P h∥ 2 H ≤ ∥h∥ 2 H with equality if and only if h ∈ span(e 1 , . . . , e p ). Applying this to h = f j , with P (f j ) = p k=1 α (j) k e k , we get p k=1 (α (j) k ) 2 ≤ 1 with equality if and only if f j ∈ span(e 1 , . . . , e p ). As a consequence, the previous upper bound on F(e 1 , . . . , e p ) implies F(e 1 , . . . , e p ) ≤ p j=1 λ 2 j . This upper bound is attained at (e 1 , . . . , e p ) = (f 1 , . . . , f p ), which is therefore a maximizer. Also, inspecting the argument above, we see that F(e 1 , . . . , e p ) &lt; λ 2 1 + • • • + λ 2 p unless (a) for all k ≤ p and j ≥ p + 1: α (j) k = 0 if λ 2 j &gt; λ 2 p+1 , and (b) for all j ≤ p: p k=1 (α (j) k ) 2 = 1 unless λ 2 j = λ 2 p+1 . Condition (a) implies that span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≤ λ 2 p+1 ). If λ 2 p = λ 2 p+1 , the inclusion span(e 1 , . . . , e p ) ⊂ span(f j : λ 2 j ≤ λ 2 p ) therefore holds. If λ 2 p &lt; λ 2 p+1 , condition (b) requires p k=1 (α (j)</p><p>k ) 2 = 1 for all j ≤ p, which implies f j ∈ span(e 1 , . . . , e p ) for j ≤ p, so that span(e 1 , . . . , e p ) = span(f 1 , . . . , f p ) and the inclusion also hold. k ) 2 = 1, hence f j ∈ span(f 1 , . . . , f p ), when λ j &lt; λ p , showing that span(f j : λ 2 j &lt; λ 2 p ) ⊂ span(e 1 , . . . , e p ). Equation (20.5) therefore always holds for (e 1 , . . . , e p ) such that F(e 1 , . . . , e p ) = λ 2  1 + • • • + λ 2 p . Furthermore, conditions (a) and (b) always hold for any orthonormal family that satisfy <ref type="bibr">(20.5)</ref>, showing that any such solution is optimal. ■ Remark 20.4 Sometimes, the metric is specified by giving Q -1 instead of Q (or Q -1 is easy to compute). Then, one can directly solve the generalized eigenvalue problem</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Condition (b) always requires</head><formula xml:id="formula_1820">Σ T f = λ 2 Q -1 f and set f = Q -1 f . The normalization f T Qf = 1 is then obtained by normalizing f so that f T Q -1 f = 1. ♦ Remark 20.5</formula><p>The "standard" version of PCA applies this computation using the Euclidean inner product, with Q = Id R d , and the principal components are the eigenvectors of the covariance matrix of T associated with the largest eigenvalues. ♦ Large dimension. It often happens that the dimension of H is much larger than the number of observations, N . In such a case, the previous approach is quite inefficient (especially when the dimension of H is infinite!) and one should proceed as follows.</p><p>Returning to the original problem, one can remark that there is no loss of generality in assuming that V is a subspace of W := span{x 1x, . . . , x N -x}. Indeed, letting V ′ = P W (V ) (the projection of V on W ), we have, for ξ ∈ W ,</p><formula xml:id="formula_1821">∥ξ -P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨ξ , P V ξ⟩ H + ∥P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨P W ξ , P V ξ⟩ H + ∥P V ξ∥ 2 H = ∥ξ∥ 2 H -2⟨ξ , P W P V ξ⟩ H + ∥P V ξ∥ 2 H ≥ ∥ξ∥ 2 H -2⟨ξ , P W P V x⟩ H + ∥P W P V ξ∥ 2 H = ∥ξ -P W P V ξ∥ 2 H ≥ ∥ξ -P V ′ ξ∥ 2 H .</formula><p>In this computation, we have used the facts that P W ξ = ξ (since ξ ∈ W ), that ∥P W P V ξ∥ H ≤ ∥P V ξ∥ H , that P W P V ξ ∈ V ′ and that P V ′ (ξ) is the best approximation of ξ by an element of V ′ . This shows that (since</p><formula xml:id="formula_1822">x k -x ∈ W for all k) N k=1 ∥x k -x -P V (x k -x)∥ 2 H ≥ N k=1 ∥x k -x -P V ′ (x k -x)∥ 2 H</formula><p>with V ′ a subspace of W of dimension less than p, proving the result. This computation also shows that no improvement in PCA can be obtained by looking for spaces of dimension p ≥ dim(W ) (with dim(W ) ≤ N -1 because the data is centered).</p><p>It therefore suffices to look for f 1 , . . . , f p in the form</p><formula xml:id="formula_1823">f i = N k=1 α (i) k (x k -x).</formula><p>for some α</p><formula xml:id="formula_1824">(i) k , 1 ≤ k ≤ N , 1 ≤ i ≤ p.</formula><p>With this notation, we have ⟨f i , f j ⟩ H = N k,l=1 α</p><formula xml:id="formula_1825">(i) k α (j) l ⟨x k -x , x l -x⟩ H and ⟨f i , f j ⟩ T = 1 N N l=1 ⟨f i , x l -x⟩ H ⟨f j , x l -x⟩ H = 1 N N k,k ′ =1 α (i) k α (j) k ′ N l=1 ⟨x k -x , x l -x⟩ H ⟨x k ′ -x , x l -x⟩ H .</formula><p>Let S be the Gram matrix of the centered data, formed by the inner products ⟨x kx , x l -x⟩ H , for k, l = 1, . . . , N . Let α (i) be the column vector with coordinates α</p><formula xml:id="formula_1826">(i)</formula><p>k , k = 1, . . . , N . We have ⟨f i , f j ⟩ H = (α (i) ) T Sα (j) and ⟨f i , f j ⟩ T = (α (i) ) T S 2 α (j) /N , which implies that, in this representation, the operator A T is given by S/N . Thus, the previous simultaneous orthogonalization problem can be solved in terms of the α's by diagonalizing S and taking the first eigenvectors, normalized so that (α (i) ) T Sα (i) = 1. Let λ 2 j , j = 1, . . . , N be the eigenvalues of S/N (of which only the first min(d, N -1) may be non-zero). In this representation, the decomposition of the projection of x k on the PCA basis is given by</p><formula xml:id="formula_1827">x k = p j=1 β (j) k f j with β (j) k = ⟨x k -x , f j ⟩ H = N l=1 α (j) l ⟨x l -x , x k -x⟩ H = N λ 2 j α (j) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.2">Kernel PCA</head><p>Since the previous computation only depended on the inner products ⟨x kx , x l -x⟩ H , PCA can be performed in reproducing kernel Hilbert spaces, and the resulting method is called kernel PCA. In this framework, X may take values in any set R with a representation h : R → H. The associated kernel, K(x, x ′ ) = ⟨h(x) , h(x ′ )⟩ H , provides a closed form expression of the inner products in terms of the original variables. The feature function itself is most of the time unnecessary.</p><p>The kernel version of PCA consists in replacing x kx with h(x k ) -h where h is the average feature. This leads to defining a "centered kernel:"</p><formula xml:id="formula_1828">K c (x, x ′ ) = ⟨h(x) -h , h(x ′ ) -h⟩ H = ⟨h(x) , h(x ′ )⟩ H -⟨h(x) + h(x ′ ) , h⟩ + h 2 H = K(x k , x l ) - 1 N N k=1 (K(x, x k ) + K(x ′ , x k )) + 1 N 2 N k,l=1 K(x k , x l ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.2.">KERNEL PCA</head><p>Then the Gram matrix in feature space is S with s kl = K c (x k , x l ) and the computation described in the previous section can be applied. Note that, if one denotes, as usual K = K(x 1 , . . . , x N ) the matrix formed by kernel evaluations K(x k , x l ), and if one lets P = Id R N -1 N 1 N /N , then we have the simple matrix expression S = P KP .</p><p>Letting α (1) , . . . , α (p) ∈ R N be the first p eigenvectors of S, normalized so that (α (i) ) T Sα (i) = 1, the principal directions are vectors in feature space given by (using the notation in the previous section in which the kth coordinate of α (i) is α</p><formula xml:id="formula_1829">(i) k ) f i = N k=1 α (i) k (h(x k ) -h) ,</formula><p>and they are not computable when the features not known explicitly. However, a few geometric features associated with these directions can be characterized using the kernel only.</p><p>Consider the line in feature space D i = h + λf i , λ ∈ R . Let Ω i denote the points</p><p>x ∈ R such that h(x) ∈ D i . Then x ∈ Ω i if and only if h(x) coincides with its orthogonal projection on D i , which is equivalent to</p><formula xml:id="formula_1830">⟨h(x) -h , f i ⟩ 2 H = h(x) -h 2</formula><p>H , which can be expressed with the kernel as</p><formula xml:id="formula_1831">K c (x, x) -        N k=1 α (i) k K c (x, x k )        2 = 0 . (20.6)</formula><p>This provides a nonlinear equation in x. In particular, Ω i is generally nonlinear, possibly with several connected components. Note that, by definition, the difference in <ref type="bibr">(20.6</ref>) is always non-negative, so that a way to visualize Ω i is to compute its sublevel sets, i.e., the set of all x such that</p><formula xml:id="formula_1832">K c (x, x) -        N k=1 α (i) k K c (x, x k )        2 ≤ ϵ</formula><p>for small ϵ.</p><p>Similarly, the feature vector h(x) -h belongs to the space generated by the first p components if and only if</p><formula xml:id="formula_1833">p i=1 ⟨h(x) -h , f i ⟩ 2 H = h(x) -h 2 H i.e., p i=1        N k=1 α (i) k K c (x, x k )        2 = K c (x, x).</formula><p>One can also compute the finite-dimensional coordinates of h(x) in the PCA basis, and this computation is easier. The representation is</p><p>x → (u 1 (x), . . . , u p (x))</p><p>with</p><formula xml:id="formula_1834">u i = ⟨h(x) -h , f i ⟩ H = N k=1 α (i) k K c (x, x k ) .</formula><p>This provides an explicit nonlinear transformation that maps each data point x into a p-dimensional point. This representation allows one to easily exploit the reduction of dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.3">Statistical interpretation and probabilistic PCA</head><p>There is a simple probabilistic interpretation of linear PCA. Assume that H = R d with the standard inner product and that X is a centered random vector with covariance matrix Σ. Consider the problem that consists in finding a factor decomposition</p><formula xml:id="formula_1835">X = p i=1 Y (i) e i + R</formula><p>where Y = (Y (1) , . . . , Y (p) ) T forms a p-dimensional centered vector, e 1 , . . . , e p is an orthonormal system, and R is a random vector, independent of Y and as small as possible, in the sense that E(|R| 2 ) is minimal.</p><p>One can see that, in an optimal decomposition, one needs R T e i = 0 for all i, because one can always write</p><formula xml:id="formula_1836">p i=1 Y (i) e i + R = p i=1 (Y (i) + R T e i )e i + R - p i=1 R T e i e i .</formula><p>If R is centered, then so is R -p i=1 R T e i e i and the latter provides a better solution since |R -p i=1 R T e i e i | ≤ |R|. Also, there is no loss of generality in requiring that (Y (1) , . . . , Y (p) ) are uncorrelated, as this can always be obtained after a change of basis in span(e 1 , . . . , e p ).</p><p>Assuming this, we can write</p><formula xml:id="formula_1837">E(|X| 2 ) = p i=1 E((Y (i) ) 2 ) + E(|R| 2 )</formula><p>with Y (i) = e T i X. So, to minimize E(|R| 2 ), one needs to maximize</p><formula xml:id="formula_1838">p i=1 E((e T i X) 2 )</formula><p>which is equal to (letting Σ be the covariance matrix of X)</p><formula xml:id="formula_1839">p i=1 e T i Σe i .</formula><p>The solution of this problem is given by the first p eigenvectors of Σ. PCA (with a Euclidean metric) exactly applies this procedure, with Σ replaced by the empirical covariance.</p><p>"Probabilistic PCA" is based on a slightly different statistical model in which it is assumed that X can be decomposed as</p><formula xml:id="formula_1840">X = p i=1 λ i Y (i) e i + σ R,</formula><p>where R is a d dimensional standard Gaussian vector and Y = (Y (1) , . . . , Y (p) ) T a pdimensional standard Gaussian vector, independent of R. The main difference with standard PCA is that the total variance of the residual, here dσ 2 , is a model parameter and not a quantity to minimize. We can rewrite this model in the form</p><formula xml:id="formula_1841">X = W Y + σ 2 R</formula><p>where the parameters are W and σ 2 , with the constraint that W T W is a diagonal matrix. As a linear combination of independent Gaussian random variables, X is Gaussian with covariance matrix W W T + σ 2 Id. The log-likelihood of the observations x 1 , . . . , x N therefore is</p><formula xml:id="formula_1842">L(W , σ ) = - N 2 d log 2π + log det(W W T + σ 2 Id) + trace((W W T + σ 2 Id) -1 Σ T ) (20.7)</formula><p>where Σ Y is the empirical covariance matrix of x 1 , . . . , x N . This function can be maximized explicitly in W and σ , as stated in the following proposition.</p><p>Proposition 20.6 Assume that the matrix Σ T is invertible. The log-likelihood in (20.7) is maximized by taking (i) W = [λ 1 e 1 , . . . , λ p e p ] where e 1 , . . . , e p are the eigenvectors of Σ T associated to the p largest eigenvalues, and λ i = δ 2 iσ 2 , where δ 2 i is the eigenvalue of Σ associated to e i ; (ii) and</p><formula xml:id="formula_1843">σ 2 = 1 d -p d i=p+1 δ 2 i .</formula><p>Proof We make the following change of variables: let ρ 2 = 1/σ 2 and</p><formula xml:id="formula_1844">µ 2 i = 1 σ 2 - 1 λ 2 i + σ 2 .</formula><p>Let Q = [µ 1 e 1 , . . . , µ p e p ]. We have</p><formula xml:id="formula_1845">(W W T + σ 2 Id) -1 = ρ 2 Id -QQ T .</formula><p>To see this, complete (e 1 , . . . , e p ) into an orthonormal basis of R d , letting e p+1 , . . . , e d denote the added vectors. Then</p><formula xml:id="formula_1846">W W T + σ 2 Id = p i=1 (λ 2 i + σ 2 )e i e T i + d i=p+1</formula><p>σ 2 e i e T i so that</p><formula xml:id="formula_1847">(W W T + σ 2 Id) -1 = p i=1 (λ 2 i + σ 2 ) -1 e i e T i + d i=p+1 σ -2 e i e T i = ρ 2 Id -QQ T .</formula><p>Using these variables, we can reformulate the problem as the minimization of</p><formula xml:id="formula_1848">- p i=1 log(ρ 2 -µ 2 i ) -(d -p) log ρ 2 + ρ 2 trace(Σ) - p j=1 µ</formula><p>2 j e T j Σe j . From theorem 2.3, we have p j=1 µ 2 j e T j Σe j ≤ p j=1 µ 2 j δ 2 j and this upper bound is attained by letting e 1 , . . . , e p be the first p eigenvectors of Σ. Using this, we see that σ 2 , µ 2 1 , . . . , µ 2 p must minimize</p><formula xml:id="formula_1849">- p i=1 log(ρ 2 -µ 2 i ) -(d -p) log ρ 2 + ρ 2 d j=1 δ 2 j - p j=1 µ 2 j δ 2 j .</formula><p>Computing the solution is elementary and left to the reader, and yields, when expressed as functions of σ 2 , λ 2 1 , . . . , λ 2 p , the expressions given in the statement of the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.4">Generalized PCA</head><p>We now discuss a dimension reduction method called generalized PCA (GPCA) <ref type="bibr" target="#b218">[200]</ref> that, instead of looking for the best linear approximation of the training set by one specific subspace, provides an approximation by a finite union of such spaces.</p><p>As a motivation, consider the situation in fig. <ref type="figure" target="#fig_158">20</ref>.1 in which part of the data is aligned along one direction in space, and another part along another direction. Then, the only information that PCA can retrieve (provided that the two directions intersect) is the plane generated by the two directions, which will be captured by the two principal components. PCA will not be able to determine the individual directions. GPCA addresses this type of situation as follows. For simplicity, assume that we are trying to decompose the data along unions of hyperplanes in R d . Such hyperplanes have equations of the form u T x = 0 where x is our notation for the vector (1, x T ) T . If we have two hyperplanes, specified by u 1 and u 2 and all the training samples approximately belong to one of them, then one has, for all k = 1, . . . , N :</p><formula xml:id="formula_1850">(u T 1 xk )(u T 2 xk ) = xT k u 1 u T 2 xk ≃ 0.</formula><p>Similarly, for n hyperplanes, the identity is, for k = 1, . . . , N :</p><formula xml:id="formula_1851">n j=1 (u T j xk ) ≃ 0. Write n j=1 (u T j x) = 1≤i 1 ,...,i n ≤d u 1 (i 1 ) • • • u n (i n )x (i 1 ) • • • x (i n )</formula><p>in the form (by regrouping the terms associated with the same powers of x)</p><formula xml:id="formula_1852">F(x) = p 1 +...+p d =n q p 1 ...p d (x (1) ) p 1 . . . (x (d) ) p d . (<label>20.8)</label></formula><p>The collection of n+d-1 n numbers Q = (q p 1 ...p n , p 1 + • • • + p d = n) takes a specific form (that we will not need to make explicit) as a function of the unknown u 1 , . . . , u n , but the first step of GPCA ignores this constraint and estimates Q minimizing </p><formula xml:id="formula_1853">N k=1         p 1 +...+p d =n q p 1 ...p d (x (1) k ) p 1 . . . (x (d) k ) p d         2 under the constraint q 2 p 1 ...</formula><formula xml:id="formula_1854">Σ = N k=1 V (x k )V (x k ) T .</formula><p>The solution is given by the eigenvector associated with the smallest eigenvalue of Σ. If the model is exact, this eigenvalue should be zero, and if only one decomposition of the data in a set of distinct hyperplanes exists (i.e., if n is not chosen too large), then Q is the unique solution up to a multiplicative constant.</p><p>Once Q is found, it remains to identify the vectors u 1 , . . . , u n . This identification can be obtained by inspecting the gradient of F on the union of hyperplanes. Indeed, one has, for x ∈ R d ,</p><formula xml:id="formula_1855">∇F(x) = n j=1         j ′ j u T j ′ x         u j</formula><p>However, if x belong in one and only one of the hyperplanes, say x T u j = 0, then all terms in the sum vanish but one and ∇F(x) is proportional to u j . So, if the model is exact, one has, for each k = 1, . . . , N , either ∇F(x k ) = 0 (if x k belongs to the intersection of two hyperplanes) or ∇F(x k )/|∇F(x k )| = ±u j for some j, and the sign ambiguity can be removed by ensuring, for example, that the first non-vanishing coordinate of u j is positive. (The gradient of F can be computed from Q using <ref type="bibr">(20.8)</ref>.) The computation of ∇F on training data therefore allows for an exact computation of the hyperplanes.</p><p>In practice, when noise is present, one cannot expect this computation to be exact. The vectors u 1 , . . . , u n can be estimated by clustering the collection of nonvanishing gradients ∇F(x k ), k = 1, . . . , N . For example, one can compute a dissimilarity matrix such as d kl = 1cos 2 (θ kl ), where θ kl is the angle between ∇F(x k ) and ∇F(x l ), and apply one of the methds discussed in section <ref type="bibr">19.4.1.</ref> This analysis provides a decomposition of the training set into n (or fewer) hyperplanes. The computation can then be recursively refined in order to obtain smaller dimensional subspaces by applying the same method separately to each hyperplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.5">Nuclear norm minimization and robust PCA 20.5.1 Low-rank approximation</head><p>One can also interpret PCA in terms of low-rank matrix approximations. Let X c be the N by d matrix (x 1x, . . . , x Nx) T , which, in generic situations, has rank d -1. Then PCA with p components is equivalent to minimizing, over all N by d matrices Z of rank p, the norm of the difference</p><formula xml:id="formula_1856">|X c -Z| 2 = trace((X c -Z) T (X c -Z)) .</formula><p>(20.9)</p><p>The quantity |A| 2 = trace(A T A) is the sum of square of the entries of A, which is often referred to as the (squared) Frobenius norm. We have</p><formula xml:id="formula_1857">|A| 2 = d k=1 σ 2 k</formula><p>where σ 1 , . . . , σ d are the singular values of A, i.e., the square roots of the eigenvalues of A T A.</p><p>We first note the following characterization of rank-p matrices.</p><p>Proposition 20.7 A matrix Z has rank p if and only if it can be written in the form Z = AW T where A is N ×p, and W is d ×p with W T W = Id R p , i.e., W = [e 1 , . . . , e p ] where the columns form an orthonormal family of R d . Proof The "if" part is obvious and we prove the "only if" part. Assume that Z has rank p. Take W = [e 1 , . . . , e p ], where (e 1 , . . . , e p ) is an orthonormal family in Null(Z) ⊥ . Letting e p+1 , . . . , e d denote an orthonormal basis of Null(Z), we have d i=1 e i e T i = Id R d and Z = Z d i=1 e i e T i = Z p i=1</p><p>e i e T i = ZW W T so that one can take A = ZW .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Using this representation and letting z T k be the kth row vector of Z, we have</p><formula xml:id="formula_1858">|X x -Z| 2 = N k=1 |x k -x -z k | 2 = N k=1 x k -x - p j=1 a (j) k e j 2 .</formula><p>With fixed e 1 , . . . , e p , the optimal matrix A has coefficients a (j) k = (x kx) T e j . In matrix form, this is:</p><formula xml:id="formula_1859">Z = X c         p j=1 e j e T j         .</formula><p>We therefore retrieve the PCA formulation that we gave in section 20.1, in the special case of H = R d with the standard Euclidean product. The lowest value achieved by the PCA solution is</p><formula xml:id="formula_1860">|X c -Z| 2 = N d k=p+1 λ 2 k</formula><p>where λ 2  1 , . . . , λ 2 d are the eigenvalues of the covariance matrix computed from x 1 , . . . , x N , who are also the squared singular values of the matrix X c divided by N .</p><p>In this section, we will explore variations on PCA in which the minimization of |X c -Z| 2 is completed with a penalty that depends on the singular values of the matrix Z. As a first example, one can modify PCA by adding a penalty on the rank (i.e., on the number of non-zero singular values), minimizing:</p><formula xml:id="formula_1861">γ|X c -Z| 2 + rank(Z)</formula><p>for some parameter γ &gt; 0. However, the solution to this problem is a small variation of that of standard PCA. It is indeed given by standard PCA with p components where p minimizes</p><formula xml:id="formula_1862">N γ d k=p+1 λ 2 k + p = N γ d k=p+1 (λ 2 k -(N γ) -1 ) + d,</formula><p>i.e., p is the index of the last eigenvalue that is larger than (N γ) -1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.5.2">The nuclear norm</head><p>Based on the fact that rank(Z) is the number of non-zero singular values of Z, one can use the same heuristic as in the development of the lasso, and replace counting the non-zero values by the sum of the absolute values of the singular values, which is just the sum of singular values since they are non-negative. This provides the nuclear norm of A, defined in section 2.4 by</p><formula xml:id="formula_1863">|A| * = d k=1 σ k</formula><p>where σ 1 , . . . , σ d are the singular values of A. We will consider below the problem of minimizing γ|X c -Z| 2 + |Z| * <ref type="bibr">(20.10)</ref> and show that its solution is once again similar to PCA.</p><p>We recall the characterization of the nuclear norm proposition 2.6. If A is an N by d matrix,</p><formula xml:id="formula_1864">|A| * = max trace(U AV T ) : U is N × N and U T U = Id, V is d × d and V T V = Id .</formula><p>In Cai et al. <ref type="bibr" target="#b63">[45]</ref>, the authors consider the minimization of (20.10) and prove the following result. Recall that we have defined the shrinkage function S τ : t → sign(t) max(|t|τ, 0) (with τ ≥ 0), using the same notation S τ (X) when applying S τ to every entry of a vector or matrix X. Following Cai et al. <ref type="bibr" target="#b63">[45]</ref>, we define the singular value thresholding operator A → S τ (A), where A is any rectangular matrix, by Proof Representing Z by its singular value decomposition, we have the equivalent formulation of minimizing</p><formula xml:id="formula_1865">F(U , V , D) = γ|X c -U DV T | 2 + |D| * = γ|X c | 2 -2γtrace(X T c U DV T ) + γ|D| 2 + |D| *</formula><p>over all orthonormal matrices U and V and diagonal matrices with non-negative coefficients D. From theorem 2.1, we know that trace(X T c U DV T ) is less than the sum of the products of the non-increasingly ordered singular values of X c and D and this upper bound is attained by taking U = Ū and V = V where Ū and V are the matrices providing the SVD of X c , i.e., such that X c = Ū ∆ V T where ∆ is diagonal with non-decreasing coefficients along the diagonal. So, letting</p><formula xml:id="formula_1866">λ 1 ≥ • • • ≥ λ d ≥ 0 and µ 1 ≥ • • • ≥ µ d ≥ 0 be</formula><p>the singular values of X c and Z, we have just proved that, for any D,</p><formula xml:id="formula_1867">F(U , V , D) ≥ F( Ū , V , D) = -2γ d i=1 µ i λ i + γ d i=1 µ 2 i + d i=1 µ i .</formula><p>The lower bound is minimized when µ i = max(λ i -1/2γ, 0). This proves the proposition.</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.5.3">Robust PCA</head><p>As a consequence, the nuclear norm penalty provides the same principal directions (after replacing γ by 2γ) as the rank penalty, but applies a shrinking operation rather than thresholding on the singular values. The difference is however more fundamental if, in addition to using the nuclear norm as a penalty, on replaces the squared Frobenius norm on the approximation error by the ℓ 1 norm, where, for an n by m matrix A with coefficients (a(i, j)),</p><formula xml:id="formula_1868">|A| ℓ 1 = i,j |a(i, j)| .</formula><p>This is the formulation of robust PCA <ref type="bibr" target="#b67">[49]</ref>, which minimizes</p><formula xml:id="formula_1869">γ|X c -Z| ℓ 1 + |Z| * (20.11)</formula><p>with respect to Z.</p><p>Robust PCA (which was initially named Principal Component Pursuit by the authors in Candès et al. <ref type="bibr" target="#b67">[49]</ref>) is designed for situations in which X c can be decomposed as the sum of a low-rank matrix Z and of a sparse residual S. Some theoretical justification was provided in the original paper, stating that if X c = Z+S, with Z = U DV T (its singular value decomposition) such that U and V are sufficiently "diffuse" and rank(Z) is small enough, with the residual's sparsity pattern taken uniformly at random over the subsets of entries of S with a sufficiently small cardinality, then robust PCA is able to reconstruct the decomposition exactly with high probability (relative to the random selection of the sparsity pattern of S). We refer to Candès et al. <ref type="bibr" target="#b67">[49]</ref> for the long proof that justifies this statement.</p><p>Robust PCA can be solved using the ADMM algorithm (section 3.5.5) after reformulating the problem as the minimization of</p><formula xml:id="formula_1870">γ|R| ℓ 1 + |Z| * subject to R + Z = X c .</formula><p>The algorithm therefore iterates over the following steps.</p><formula xml:id="formula_1871">                   Z (k+1) = argmin Z |Z| * + 1 2α |Z + R (k) -X x + U (k) | 2 R (k+1) = argmin R γ|R| ℓ 1 + 1 2α |Z (k+1) + R -X c + U (k) | 2 U (k+1) = U (k) + Z (k+1) + R (k+1) -X c (20.12)</formula><p>The first minimization is covered by proposition 2.6 and yields</p><formula xml:id="formula_1872">Z (k+1) = S α (X c -R (k) -U (k) ).</formula><p>The second minimization is solved by a standard shrinking operation, i.e.,</p><formula xml:id="formula_1873">R (k+1) = S γα (X c -Z (k+1) -U (k) ).</formula><p>Using this, we can rewrite the robust PCA algorithm as the sequence of fairly simple iterations.</p><p>Algorithm 20.1</p><p>(1) Choose a small enough constant α and a very small tolerance level ϵ.</p><p>(2) Initialize the algorithm with N by d matrices R (0) and U (0) (e.g., equal to zero).</p><p>(3) At step n, apply the iteration:</p><formula xml:id="formula_1874">             Z (k+1) = S α (X c -R (k) -U (k) ) R (k+1) = S γα (X c -Z (k+1) -U (k) ) U (k+1) = U (k) + Z (k+1) + R (k+1) -X c</formula><p>(20.13) (4) Stop the algorithm is the variation compared to variables at the previous step is below the tolerance level. Otherwise, apply step n + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6">Independent component analysis</head><p>Independent component analysis (ICA) is a factor analysis method that represents a d-dimensional random variable X in the form X = AY where A is a fixed d ×d invertible matrix and Y is a d-dimensional random vector with independent components. There are two main approaches in this setting. The first one optimizes the matrix W = A -1 so that the components of W X are "as independent as possible" according to a suitable criterion. The second one is model-based, where a statistical model is assumed for Y , and its parameters, together with the entries of the matrix A, are estimated via maximum likelihood. Before describing each of these methods, we first discuss the extent to which the coefficients of A are identifiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.1">Identifiability</head><p>A statistical model is identifiable if its parameters (which could be finite-of infinitedimensional) are uniquely defined by the distribution of the observable variables. In the case of ICA, this question boils down to deciding whether AY ∼ A ′ Y ′ (i.e., they have the same probability distribution) implies that A = A ′ (where Y and Y ′ are two random vectors with independent components).</p><p>It should be clear that the answer to this question is negative, because there are trivial transformations of the matrix A that do not break the ICA model. One can, for example, take any invertible diagonal matrix, D, and let A ′ = AD -1 and Y ′ = DY . The same statement can be made if D is replaced by a permutation matrix, P , which reorders the components of Y . So we know that AY ∼ A ′ Y ′ is possible already when A ′ = ADP where D is diagonal and invertible and P is a permutation matrix. Note that iterating such matrices (i.e., letting A ′ = ADP D ′ P ′ ) does not extend the class of transformations because one has DP = P P -1 DP and one can easily check that P -1 DP is diagonal, so that one can rewrite any product of permutations and diagonal matrices as a single diagonal matrix multiplied by a single permutation.</p><p>It is interesting, and fundamental for the well-posedness of ICA, that, under one important additional assumption, the indeterminacy in the identification of A stops at these transformations. The additional assumption is that at most one of the components of Y follows a Gaussian distribution. That such a restriction is needed is clear from the fact that one can transform any Gaussian vector Y with independent components into another, BY , one as soon as BB T is diagonal. If two or more components of Y are Gaussian, one can restrict these matrices B to only affect those components. If only one of them is Gaussian, such an operation has no effect.</p><p>The following theorem is formally stated in Comon <ref type="bibr" target="#b72">[54]</ref>, and is a rephrasing of the Darmois-Skitovitch theorem <ref type="bibr" target="#b75">[57,</ref><ref type="bibr" target="#b197">179]</ref>. The proof of this theorem relies on complex analysis arguments on characteristic functions and is beyond the scope of these notes (see <ref type="bibr">Kagan et al. [101]</ref> for more details). Theorem 20.9 Assume that Y is a random vector with independent components, such that at most one of its components is Gaussian. Let A be an invertible linear transformation and Ỹ = CY . Then the following statements are independent. (i) For all i j, the components Ỹ (i) , Ỹ (j) are independent.</p><p>(ii) Ỹ (1) , . . . , Ỹ (d) are mutually independent.</p><p>(iii) C = DP is the product on a diagonal matrix and of a permutation.</p><p>The equivalence of (ii) and (iii) implies that the ICA model is identifiable up to multiplication on the right by a permutation and a diagonal matrix. Indeed, if X = AY = A ′ Y ′ are two decompositions, then it suffices to apply the theorem to C = (A ′ ) -1 A to conclude. The equivalence of (i) and (ii) is striking, and has the important consequence that, if the data satisfies the ICA model, then, in order to identify A (up to the listed indeterminacy), it suffices to look for Y = A -1 X with pairwise independent components, which is a much lesser constraint than full mutual independence.</p><p>As a final remark on the Gaussian indeterminacy, we point out that, if the mean (m) and covariance matrix (Σ) of X are known (or estimated from data), the ICA problem can be reduced to looking for orthogonal transformations A. Indeed, assuming X = AY and letting X = Σ -1/2 (Xm) and Ỹ = D -1/2 (Y -A -1 m), where D is the (diagonal) covariance matrix of Y , we have</p><formula xml:id="formula_1875">X = Σ -1/2 (AY -m) = Σ -1/2 AD 1/2 Ỹ .</formula><p>Letting Ã = Σ -1/2 AD 1/2 , we have Id R d = E( X XT ) = Ã ÃT so that Ã is orthogonal. This shows that the ICA problem for X in the form X = Ã Ỹ with the restriction that Ã is orthogonal has a solution, and also provides a solution of the original ICA problem by letting A = Σ 1/2 Ã and Y = Ỹ -Ã-1 Σ -1/2 m. Therefore, the indeterminacy associated with Gaussian vectors is as general as possible up to a normalization of first and second moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.2">Measuring independence and non-Gaussianity</head><p>Independence between d variables is a very strong property and its complete characterization is computationally challenging. The fact that the joint p.d.f.of the d variables (we will restrict, to simplify our discussion, to variables that are absolutely continuous) factorizes into the product of the marginal p.d.f.'s of each variable can be measured by computing the mutual information between the variables, defined by (letting ϕ Z denote the p.d.f. of a variable Z)</p><formula xml:id="formula_1876">I(Y ) = ϕ Y (y) d i=1 ϕ Y (i) (y (i) )</formula><p>ϕ Y (y)dy.</p><p>The mutual information is always non-negative and vanishes only if the components of Y are mutually independent. Therefore, one can represent ICA as an optimization problem minimizing I(W X) with respect to all invertible matrices W (so that W = A -1 ). Letting</p><formula xml:id="formula_1877">h(Y ) = -log ϕ Y (y) ϕ Y (y)dy</formula><p>denote the "differential entropy" of Y , we can write</p><formula xml:id="formula_1878">I(Y ) = d i=1 h(Y (i) ) -h(Y ). If Z = W X, then ϕ Z (z) = ϕ X (W -1 x)| det(W )| -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using this expression in h(Z) and making a change of variables yields h(</head><formula xml:id="formula_1879">W Z) = h(X) + log | det W | and I(W X) = d i=1 h(Z (i) ) -log | det(W )| -h(X).</formula><p>This shows that the optimal W can be obtained by minimizing</p><formula xml:id="formula_1880">F(W ) = d i=1 h(W (i) X) -log | det(W )|</formula><p>where W (i) is the ith row of W . This brings a notable simplification, since this expression only involves differential entropies of scalar variables, but still remains a challenging problem.</p><p>In Comon <ref type="bibr" target="#b72">[54]</ref>, it is proposed to use cumulant expansions of the entropy around that of a Gaussian with identical mean and variance to approximate the differential entropy. If ξ ∼ N (m, σ 2 ) , then</p><formula xml:id="formula_1881">h(ξ) = 1 2 + 1 2 log(2πσ 2 ).</formula><p>Define, for a general random variable U with standard deviation σ U , the non-Gaussian entropy, or negentropy, defined by</p><formula xml:id="formula_1882">ν(U ) = 1 2 + 1 2 log(2πσ 2 U ) -h(U ) .</formula><p>One can shows that ν(U ) ≥ 0 and is equal to 0 if and only if U is Gaussian. One can rewrite F(W ) as</p><formula xml:id="formula_1883">F(W ) = d 2 + d 2 log(2π) + d i=1 log(σ 2 W (i) X ) - d i=1 ν(W (i) X) -log | det(W )|</formula><p>As we remarked earlier, if we replace X by Σ -1/2 (Xm) (after estimating the covariance matrix of X), there is no loss of generality in requiring that W is an orthogonal matrix, in which case both σ 2 W (i) X and | det W | are equal to 1. Assuming such a reduction is done, we see that the problem now requires to maximize</p><formula xml:id="formula_1884">d i=1 ν(W (i) X) (20.14)</formula><p>among all orthogonal matrices W . Still in Comon <ref type="bibr" target="#b72">[54]</ref>, an approximation of the negentropy ν(U ) is provided as a function of the third and fourth cumulants of the distribution of U . These are given by</p><formula xml:id="formula_1885">κ 3 = E((U -E(U )) 3 )</formula><p>and κ 4 = E((U -E(U )) 4 ) -3σ 4 U . In particular, when U is normalized, i.e., E(U ) = 0 and σ 2 U = 1, we have κ 3 = E(U 3 ) and κ 4 = E(U 4 ) -3. Under the same assumption, it is proposed in Comon <ref type="bibr" target="#b72">[54]</ref> to use the approximation</p><formula xml:id="formula_1886">ν(U ) ∼ κ 2 3 12 + κ 2 4 48 + 7κ 4 3 48 - κ 2 3 κ 4 8 .</formula><p>This approximation was derived from an Edgeworth expansion of the p.d.f. of U , which can be seen as a Taylor expansion around a Gaussian distribution. Plugging this expression into (20.14) provides an expression that can be maximized in W where the cumulants are replaced by their sample estimates. However, the maximized function involves high-degree polynomials in the unknown coefficients of W , and this simplified problem still presents numerical challenges.</p><p>An alternative approximation of the negentropy has been proposed in Hyvärinen <ref type="bibr" target="#b112">[94]</ref> relying on the maximum entropy principle, described in the following theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Associate to any random variable</head><formula xml:id="formula_1887">Y : G → R the differential entropy h µ (Y ) = - G log ϕ Y (x)ϕ Y (x)dµ(x)</formula><p>if the distribution of Y has a density, denoted ϕ Y , with respect to µ and h µ (Y ) = -∞ otherwise. Use also the same notation</p><formula xml:id="formula_1888">h µ (ϕ) = - G log ϕ(x)ϕ(x)dµ(x)</formula><p>for a p.d.f. ϕ with respect to µ (i.e., such that ϕ is non-negative and has integral 1). Then, the following is true.</p><p>Theorem 20.10 Let g = (g (1) , . . . , g (p) ) T be a function defined on a measurable space G, taking values in R p , and let µ be a measure on G. Let Γ µ be the set of all λ = (λ (1) </p><formula xml:id="formula_1889">, . . . , λ (p) ) ∈ R p such that G exp λ T g(y) dµ(y) &lt; ∞. (<label>20</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.15)</head><p>Then</p><formula xml:id="formula_1890">h µ (Y ) ≤ inf -λ T E(g(Y )) + log G exp λ T g(y) dµ(y) : λ ∈ Γ µ . (20.16) Define, for λ ∈ Γ µ , ψ λ (x) = exp λ T g(x) dµ(x) G exp λ T g(y) dµ(y) . (<label>20</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.17)</head><p>Assume that the infimum in <ref type="bibr">(20.16</ref>) is attained at an interior point λ * of Γ µ . Then</p><formula xml:id="formula_1891">h(ϕ λ * ) = max{h( Ỹ ) : E Ỹ (g) = E Y (g), i = 1, . . . , p}. (<label>20</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.18)</head><p>Proof Let Y be a random variable with p.d.f. ϕ Y with respect to µ (otherwise the lower bound in (20.16) is -∞). Then</p><formula xml:id="formula_1892">h µ (Y ) + λE(g(Y )) -log exp (λg(y)) dµ(y) = - G ϕ Y (x) log ϕ Y (x) ψ λ (x) dµ(x) ≤ 0 since G ϕ Y (x) log ϕ Y (x)</formula><p>ψ λ (x) dµ(x) is a KL divergence and is always non-negative.</p><p>Assume that λ is in Γµ . Then, there exists ϵ &gt; 0 such that, for any u ∈ R p , |u| = 1, λ + ϵu ∈ Γ µ . Using the fact that e β ≥ e α + (βα)e α , we can write ϵu T ge λ T g ≤ e (λ+ϵu) T ge λ T g -ϵu T ge λ T g ≤ e (λ-ϵu) T ge λ T g yielding ϵ|u T g|e λ T g ≤ max(e (λ+ϵu) T g , e (λ-ϵu) T g )e λ T g ≤ e (λ+ϵu) T g + e (λ-ϵu) T ge λ T g .</p><p>Since the upper-bound is integrable with respect to µ, so is the lower bound, showing that (taking u in the canonical basis of R p ) G |g (i) (y)|e λ T g(y) dµ(y) &lt; ∞ for all i, or G |g(y)|e λ T g(y) dµ(y) &lt; ∞.</p><p>Let c = E(g(Y )) and define</p><formula xml:id="formula_1893">Ψ c (λ) = -c T λ + log exp(λ T g(y))dy. (<label>20.19)</label></formula><p>Then</p><formula xml:id="formula_1894">∂ λ Ψ c = -c T + g(x) T exp(λ T g(x))dx exp(λ T g(y))dy = -c T + G g(x) T ψ λ (x)dx .</formula><p>Since λ * is a minimizer, we find that, if Ỹ is a random variable with p.d.f. λ * , then</p><formula xml:id="formula_1895">E( Ỹ ) = c = E(Y ).</formula><p>In that case, the upper-bound in <ref type="bibr">(20.16</ref>) is h µ ( Ỹ ), proving <ref type="bibr">(20.18)</ref>. ■ Remark 20.11 The previous theorem is typically applied with µ equal to Lebesgue's measure on G = R d or to a counting measure with G finite. To rewrite the statement of theorem 20.10 in those cases, it suffices to replace dµ(x) by dx for the former, and integrals by sums over G for the latter. In the rest of the discussion, we restrict to the case when µ is Lebesgue's measure, using h(Y ) instead of h µ (Y ). ♦ Remark 20.12 This principle justifies, in particular, that the negentropy is always non-negative since it implies that a distribution that maximizes the entropy given its first and second moments must be Gaussian. ♦</p><p>The right-hand side of (20.16) provides a variational approximation of the entropy. If one uses this approximation when minimizing h(W (1) </p><formula xml:id="formula_1896">X) + • • • + h(W (d) X),</formula><p>the resulting problem can be expressed as a minimization, with respect to W and λ</p><formula xml:id="formula_1897">1 , . . . , λ d ∈ R p of - d j=1 λ T E(g(W (j) X)) + d j=1 log exp λ T g(y) dy .</formula><p>While it would be possible to solve this optimization problem directly, a further approximation of the upper bound can be developed leading to a simpler procedure.</p><p>We have seen in the previous proof that, defining Ψ c by <ref type="bibr">(20.19)</ref> and denoting by E λ the expectation with respect to ϕ λ , one has</p><formula xml:id="formula_1898">∇Ψ c (λ) = -c + E λ (g) .</formula><p>Taking the second derivative, one finds</p><formula xml:id="formula_1899">∇ 2 Ψ c (λ) = E λ ((g -E λ (g))(g -E λ (g)) T ).</formula><p>Now choose c 0 such that a maximizer of Ψ c 0 (λ), say, λ c 0 , is known. If c is close to c 0 , a first order expansion indicates that, for λ c maximizing Ψ c , one should have</p><formula xml:id="formula_1900">λ c ≃ λ c 0 + ∇ 2 Ψ c (λ c 0 ) -1 (c -c 0 ) with Ψ c (λ c ) ≃ Ψ c (λ c 0 ) -(c -c 0 ) T ∇ 2 Ψ c (λ c 0 ) -1 (c -c 0 ).</formula><p>One can then use the right-hand side as an approximation of the optimal entropy. This leads to simple computations under the following assumptions. First, assume that the first two functions g (1) and g (2) are u and u 2 / √ 3. Let ϕ 0 be the p.d.f. of a standard Gaussian. Assume that the functions g (j) are chosen so that g (i) (u)g (j) (u)ϕ 0 (y)dy = δ ij for i, j = 1, . . . , p and such that g (i) (u)ϕ 0 (y)dy = 0 for i 2. Take</p><formula xml:id="formula_1901">c 0 = gϕ 0 (u)du so that c (1) 0 = 0, c (2) 0 = 1/ √ 3 and c (i) 0 = 0 for i ≥ 2.</formula><p>Then λ c 0 provides, by construction, the distribution ϕ 0 and for any c, ∇ 2 Ψ c (λ c 0 ) = Id R p . With these assumptions, the approximation is</p><formula xml:id="formula_1902">Ψ c (λ) = h(ϕ 0 ) -|c -c 0 | 2 = 1 2 (1 + log 2π) - j≥3 (c (j) ) 2</formula><p>(assuming that the data is centered and normalized so that c (1) = 0 and c (2) = 1/ √ 3). The ICA problem can then be solved by maximizing 2  (20.20) over orthogonal matrices W .</p><formula xml:id="formula_1903">d j=1 p i=1 E(g (i) (W (j) X))</formula><p>Remark 20.13 Without the assumption made on the functions g (j) , one needs to compute S = Cov(g(U )) -1 where U ∼ N (0, 1) and maximize</p><formula xml:id="formula_1904">d j=1 (E(g(W (j) X)) -E(g(U ))) T S(E(g(W (j) X)) -E(g(U ))).</formula><p>Clearly, this expression can be reduced to <ref type="bibr">(20.20)</ref> by replacing g by S -1/2 (g-E(g(U ))).</p><p>Note also that we retrieve here a similar idea to the negentropy, maximizing a deviation to a Gaussian. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.3">Maximization over orthogonal matrices</head><p>In the previous discussion, we reached a few times a formulation of ICA which required optimizing a function W → F(W ) over all orthogonal matrices. We now discuss how such a problem may be implemented.</p><p>In all the examples that were considered, there would have been no loss of generality in requiring that W is a rotation, i.e., det(W ) = 1. This is because one can change the sign of this determinant by simply changing the sign of one of the independent components, which is always possible. (In fact, the indeterminacy in W is by right multiplication by the product of a permutation matrix and a diagonal matrix with ±1 entries.) Let us assume that F(W ) is actually defined and differentiable over all invertible matrices, which form an open subset of the linear space M d (R) of d by d matrices.</p><p>Our optimization problem can therefore be considered as the minimization of F with the constraint that W W T = Id R d .</p><p>Gradient descent derives from the analysis that a direction of descent should be a matrix H such that F(W + ϵH) &lt; F(W ) for small enough ϵ &gt; 0 and on the remark that H = -∇F(W ) provides such a direction. This analysis does not apply to the constrained optimization setting because, unless the constraints are linear, W + ϵH will generally stop to satisfy the constraint when ϵ &gt; 0, requiring the use of more complex procedures. In our case, however, one can take advantage of the fact that orthogonal matrices form a group to replace the perturbation W → W + ϵH by W → W e ϵH (using the matrix exponential) where H is moreover required to be skew symmetric (H + H T = 0), which guarantees that e ϵH is an orthogonal matrix with determinant 1. Now, using the fact that e ϵH = Id + ϵH + o(ϵ), we can write</p><formula xml:id="formula_1905">F(W e ϵH ) = F(W ) + ϵtrace(∇F(W ) T W H) + o(ϵ) .</formula><p>Let ∇ s F(W ) be the skew symmetric part of W T ∇F(W ), i.e.,</p><formula xml:id="formula_1906">∇ s F(W ) = 1 2 (W T ∇F(W ) -∇F(W ) T W ). Then, if H is skew symmetric, trace(∇ s F(W ) T H) = 1 2 trace(∇F(W ) T W H) - 1 2 trace(W T ∇F(W )H) = 1 2 trace(∇F(W ) T W H) + 1 2 trace(W T ∇F(W )H T ) = trace(∇F(W ) T W H) so that F(W e ϵH ) = F(W ) + ϵtrace(∇ s F(W ) T H) + o(ϵ) .</formula><p>This show that H = -∇ s F(W ) provides a direction of descent in the orthogonal group, in the sense that, if ∇ s F(W ) 0,</p><formula xml:id="formula_1907">F(W e -ϵ∇ s F(W ) ) &lt; F(W )</formula><p>for small enough ϵ &gt; 0. As a consequence, the algorithm</p><formula xml:id="formula_1908">W n+1 = W n e -ϵ n ∇ s F(W n )</formula><p>combined with a line search for ϵ n implements gradient descent in the group of orthogonal matrices, and therefore converges to a local minimizer of F.</p><p>If one linearizes the r.h.s. as a function of ϵ, one gets</p><formula xml:id="formula_1909">W n e -ϵ n ∇ s F(W n ) = W n + ϵ n 2 W n ((W n ) T ∇F(W n ) -∇F(W n ) T W n ) + o(ϵ) = W n + ϵ n 2 (∇F(W n ) -W n ∇F(W n ) T W n ) + o(ϵ).</formula><p>As already argued, this linearized version cannot be used when optimizing over the orthogonal group. However, if one denotes by ω(A) the unitary part of the polar decomposition of A, i.e., ω(A) = (AA T ) -1/2 A, then the algorithm</p><formula xml:id="formula_1910">W n+1 = ω W n + ϵ n 2 (∇F(W n ) -W n ∇F(W n ) T W n )</formula><p>also provides a valid gradient descent algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.4">Parametric ICA</head><p>We now describe a parametric version of ICA in which a model is chosen for the independent components of Y . The simplest version of to assume that all Y (j) are i.i.d. with some prescribed p.d.f., say, ψ. A typical example for ψ is a logistic distribution with ψ(t) = 2 (e t + e -t ) 2 .</p><p>If y is a vector in R d , we will use, as usual, the notation ψ(y) = (ψ(y (1) ), . . . , ψ(y (d) )) T for ψ applied to each component of y.</p><p>The model parameter is then the matrix A, or preferably W = A -1 , and it may be estimated using maximum likelihood. Indeed, the p.d.f. of X is</p><formula xml:id="formula_1911">f X (x) = | det W | d j=1 ψ(W (j) x)</formula><p>where W (j) is the jth row of W , so that W can be estimated by maximizing</p><formula xml:id="formula_1912">ℓ(W ) = N log |det(W )| + N k=1 d j=1 log ψ(W (j) x k ) .</formula><p>If we denote by Γ (W ) the matrix with coefficients</p><formula xml:id="formula_1913">γ ij (W ) = N k=1 x k (i) ψ ′ (W (j) x k ) ψ(W (j) x k )</formula><p>and use the fact that the gradient of</p><formula xml:id="formula_1914">W → log | det W | is W -T (the inverse transpose of W ), we can write ∇ℓ(W ) = N W -T + Γ (W ).</formula><p>We need however the maximization to operate on sets of invertible matrices, and it is more natural to move in this set through multiplication than through addition, because the product of two invertible matrices is always invertible, but not necessarily their sum. So, similarly to the previous section, we will look for small variations in the form W → W e ϵH , or simply, in this case, W → W (Id R d + ϵH). In both case, the first order expansion of the log-likelihood gives</p><formula xml:id="formula_1915">ℓ(W ) + ϵtrace((N W -T + Γ (W )) T W H) which suggests taking H = W T (N W -T + Γ (W )) = N Id + W T Γ (W ).</formula><p>Dividing H by N , we obtain the following variant of gradient ascent for maximum likelihood</p><formula xml:id="formula_1916">W n+1 = (1 + ϵ n )W n + ϵ n W n W T n Γ (W n</formula><p>) . This algorithm numerically performs much better than standard gradient ascent. It moreover presents the advantage of avoiding computing the inverse of W at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.6.5">Probabilistic ICA</head><p>Note that the algorithms that we discussed concerning ICA were all formulated in terms of the matrix W = A -1 , which "filters" the data into independent components. As a result, ICA requires as many independent components as the dimension of X. Moreover, because the components are typically normalized to have equal variance, there is no obvious way to perform dimension reduction using this method. Indeed, ICA is typically run after the data is preprocessed using PCA, this preprocessing step providing the reduction of dimension.</p><p>It is however possible to define a model similar to probabilistic PCA, assuming a limited number of components to which a Gaussian noise is added, in the form (1) , . . . , Y (p) independent variables as before, and R ∼ N (0, Id R d ). This model is identifiable (up to permutation and scalar multiplication of the components) as soon as none of the variables Y (j) is Gaussian.</p><formula xml:id="formula_1917">X = p j=1 a j Y (j) + σ R with p &lt; d, a 1 , . . . , a p ∈ R d , Y</formula><p>Let us assume a parametric setting similar to that of the previous section, so that Y (1) , . . . , Y (p) are explicitly modeled as independent variables with p.d.f. ψ. Introduce the matrix A = [a 1 , . . . , a p ], so that the model can also be written X = AY + σ R, where A and σ 2 are unknown model parameters.</p><p>The p.d.f. of X is now given by (1) . . . dy (p) , which is definitely not a closed form. Since we are in a situation in which the pair of random variables is imperfectly observed through X, using the EM algorithm (chapter 16) is an option, but it may, as we shall see below, lead to heavy computation. The basic step of the EM is, given current parameters A 0 , σ 0 , to maximize the conditional expectation (knowing X, for the current parameters) of the joint log-likelihood of (X, Y ) with respect to the new parameters. In this context, the joint distribution of (X, Y ) has density</p><formula xml:id="formula_1918">f X (x; A, σ 2 ) = 1 (2πσ 2 ) d/2 R p e - |x-Ay| 2 2σ 2        p i=1 ψ(y (i) )        dy</formula><formula xml:id="formula_1919">f X,Y (x, y; A, σ 2 ) = 1 (2πσ 2 ) d/2 e - |x-Ay| 2 2σ 2 p i=1 ψ(y (i) )</formula><p>so that, the conditional joint likelihood over the training set is</p><formula xml:id="formula_1920">- N d 2 log(2πσ 2 )- 1 2σ 2 N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k )- N k=1 p j=1 E A 0 ,σ 2 0 (log ψ(Y (j) )|X = x k ).</formula><p>Notice that the last term does not depend on A, σ 2 , and that, given A, the optimal value of σ 2 is given by</p><formula xml:id="formula_1921">σ 2 = 1 N d N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k )</formula><p>The minimization of</p><formula xml:id="formula_1922">N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k ) with respect to A is a least square problem. Let b (j) k = E A 0 ,σ 0 (Y (j) |X = x k ) and s k (i, j) = E A 0 ,σ 0 (Y (i) Y (j) |X = x k ): the gradient of the previous term is -2 N k=1 E A 0 ,σ 2 0 ((x k -AY )Y T |X = x k ) = -2 N k=1 (x k b T k -AS k ),</formula><p>b k being the column vector with coefficients b (j) k and S k the matrix with coefficients s k (i, j). The result therefore is</p><formula xml:id="formula_1923">A =        N k=1 x k b T k               N k=1 S k        -1 .</formula><p>Unfortunately, the computation of the moments of the conditional distribution of</p><formula xml:id="formula_1924">Y given x k (needed in b k and S k ) is a difficult task. The conditional density of Y given X = x k is g(y|x k ) = ψ(y)e - |A 0 y-x| 2 2σ 2 0 /Z(A 0 , σ 0 )</formula><p>from which moments cannot be computed analytically in general. Monte-Carlo sampling algorithms can be used however to approximate these moments, but they are computationally demanding. And they must be run at every step of the EM.</p><p>In place of the exact EM, one may use a mode approximation (section 16.3.1), which replaces the conditional likelihood of Y given X = x k by a Dirac distribution at the mode:</p><formula xml:id="formula_1925">ŷA 0 ,σ 0 (x k ) = argmax y        ψ(y)e - |A 0 y-x k | 2 2σ 2 0        .</formula><p>The maximization step then reduces to maximizing in A, σ 2</p><formula xml:id="formula_1926">- N d 2 log(2πσ 2 ) - 1 2σ 2 N k=1 x k -A ŷA 0 ,σ 0 (x k ) 2 . (<label>20.21)</label></formula><p>This therefore provides a two-step procedure.</p><p>Algorithm 20.2 (Probabilistic ICA: mode approximation) (1) Initialize the algorithm with A 0 , σ 0 .</p><p>(2) At step n:</p><formula xml:id="formula_1927">(i) For k = 1, . . . , N , maximize p i=1 ψ(y (i) )e - |A n y-x k | 2 2σ 2 n</formula><p>to obtain ŷA n ,σ n (x k ). This requires a numerical optimization procedure, such as gradient ascent. The problem is concave when log ψ is concave.</p><p>(ii) Minimize (20.21) with respect to A, σ 2 , yielding</p><formula xml:id="formula_1928">A n+1 =        N k=1 x k b T k               N k=1 S k        -1</formula><p>with b k = ŷA n ,σ n (x k ), S k = ŷA n ,σ n (x k ) ŷA n ,σ n (x k ) T , and</p><formula xml:id="formula_1929">σ 2 n+1 = 1 N d N k=1 x k -A ŷA n ,σ n 2 .</formula><p>(3) Stop if the variation of the parameter is below a tolerance level. Otherwise, iterate to the next step.</p><p>Once A and σ 2 have been estimated, the y components associated to a new observation x can be estimated by ŷA,σ (x), therefore minimizing</p><formula xml:id="formula_1930">1 2σ 2 x k -Ay 2 + p j=1 log ψ(y (j) ),</formula><p>yielding the map estimate, the same convex optimization problem as in step (1) above. Now we can see how the method takes from both PCA and ICA: the columns of A, a 1 , . . . , a p can be considered as p principal directions, and are fixed after learning; they are not orthonormal, and do not satisfy the nesting properties of PCA (that those p contain those for p -1). The coordinates of x with respect to this basis is not a projection, as would be provided by PCA, but the result of a penalized estimation problem. The penalty associated to the logistic case is log ψ(y (j) ) = log 2 -2 log(e y (j) + e -y (j) ).</p><p>This distribution with "exponential tails" has the interest of allowing large values of y (j) , which generally entails sparse decompositions, in which y has a few large coefficients, and many zeros.</p><p>As an alternative to the mode approximation of the EM, which may lead to biased estimators, one may use the SAEM algorithm (section 16.4.3), as proposed in Allassonniere and Younes <ref type="bibr" target="#b21">[3]</ref>. Recall that the EM algorithm replaces the parameters A 0 , σ 2 0 by minimizers of</p><formula xml:id="formula_1931">N d 2 log(σ 2 ) + 1 2σ 2 N k=1 E A 0 ,σ 0 (|x k -AY | 2 |X = x k ) = N d 2 log(σ 2 ) + 1 2σ 2 N k=1 |x k | 2 - 1 σ 2 N k=1 x T k Ab k + 1 2σ 2 N k=1 trace(A T AS k ),</formula><p>where the computation of b</p><formula xml:id="formula_1932">(j) k = E A 0 ,σ 0 (Y (j) |X = x k ) and s k (i, j) = E A 0 ,σ 0 (Y (i) Y (j) |X = x k )</formula><p>was the challenging issue. In the SAEM algorithm, the statistics b k and S k are part of a stochastic approximation scheme, and are estimated in parallel with EM updates as follows. (1) For k = 1, . . . , N , sample y k according to the conditional distribution of Y given X = x k , using the current parameters A and σ 2 .</p><p>(2) Update b k and S k , letting (assuming step t of the algorithm)</p><formula xml:id="formula_1933">b k → b k + γ t (Y k -b k ) S k → S k + γ t (Y k Y T k -S k ) (3) Replace A and σ 2 by A =        N k=1 x k b T k               N k=1 S k        -1</formula><p>and</p><formula xml:id="formula_1934">σ 2 = 1 N d N k=1 x k -A ŷA 0 ,σ 0 2 .</formula><p>The parameter γ t should be decreasing with t, typically so that t γ t = +∞ and t γ 2 t &lt; ∞ (e.g., γ t ∝ 1/t). One way to sample from Y k is to uses a rejection scheme, iterating the procedure which samples y according to the prior and accepts the result with probability M exp(-|x k -Ay| 2 /2σ 2 ) until acceptance. Here M must be chosen so that M max y exp(-|x k -Ay| 2 /2σ 2 ) ≤ 1 (e.g., M = 1). This method will work for small p, but for large p, the probability of acceptance may be very small. In such cases, Y k can be sampled changing one component at a time using a Metropolis-Hastings scheme. If component j is updated, this scheme samples a new value of y (call it y ′ ) by changing only y (j) according to the prior distribution ψ and accept the change with probability min 1, exp(-|x k -Ay ′ | 2 /2σ 2 ) exp(-|x k -Ay| 2 /2σ 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.7">Non-negative matrix factorization</head><p>In this section, we consider factor analysis methods that approximate a random variable X in the form X = p j=1 a (j) Y (j) with the constraint that the scalars a (1) , . . . , a (p) ∈ R and the vectors Y (1) , . . . , Y (p) ∈ R d are respectively non-negative and with non-negative entries. This model makes sense, for example, when X represents the total multivariate production (e.g., in terms of number of molecules of various types) resulting of several chemical reactions that operate together. Another application is when X is a list of preference scores associated with a person for, say, books or movies, and each person is modeled as a positive linear combination of p "typical scorers," represented by the vector Y (j) for j = 1, . . . , p.</p><p>When training data (x 1 , . . . , x N ) is observed and stacked in an N by d matrix X , the decomposition can be summarized for all observations together in the matrix form X = AY T where A is N by p and provides the coefficients a (j) k associated with each observation and Y = [y (1) , . . . , y (p) ] is d by p and provides the p typical profiles. The matrices A and Y are unknown and their estimation subject to the constraint of having nonnegative components represent the non-negative matrix factorization (NMF) problem.</p><p>NMF is often implemented by solving the constrained optimization problem of minimizing |X -AY T | 2 subject to A and Y having non-negative entries. This problem is non-convex in general but the sub-problems of optimizing either A or Y when the other matrix is fixed are simple quadratic programs. This suggests using an alternating minimization method, iterating steps in which A is updated with Y fixed, followed by an update of Y with A fixed. However, solving a full quadratic program at each step would be computationally prohibitive with large datasets, and simpler update rules have been suggested, updating each matrix in turn with a guarantee of reducing the objective function.</p><p>If Y is considered as fixed and A is the free variable, we have</p><formula xml:id="formula_1935">|X -AY T | 2 = |X | 2 -2trace(X T AY T ) + trace(AY T Y A T ) = trace(A T AY T Y ) -2trace(A T (X Y )) + |X | 2 .</formula><p>The next lemma will provide update steps for A. Lemma 20.14 Let M be an n by n symmetric matrix and b ∈ R n , both assumed to have non-negative entries. Let u ∈ R n , also with non-negative coefficients, and let</p><formula xml:id="formula_1936">v (i) = u (i)        b (i) d j=1 m(i, j)u (j)        . Then v T Mv -2b T v ≤ u T Mu -2b T u . Moreover, v = u if and only if u minimizes u T Mu -2b T u subject to u (i) = 0, i = 1, . . . , n. Proof Let F(u) = u T Mu -2b T u. We look for v (i) = β (i) u (i) with β (i) ≥ 0 such that F(v) ≤ F(u). We have F(v) = n i,j=1 β (i) β (j) u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) ≤ 1 2 n i,j=1 ((β (i) ) 2 + (β (j) ) 2 )u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) u (i) = n i,j=1 (β (i) ) 2 u (i) u (j) m(i, j) -2 n i=1 b (i) β (i) u (i)</formula><p>When β = 1 n , this upper-bound is equal to F(u). So, if we choose β minimizing the upper-bound, we will indeed find v such that F(v) ≤ F(u). Rewriting the upperbound as</p><formula xml:id="formula_1937">n i=1 u (i)         (β (i) ) 2         n j=1 m(i, j)u (j)         -2b (i) β (i)        </formula><p>we see that β (i) = b (i) / n j=1 m(i, j)u (j) provides such a minimizer, which proves the first statement of the lemma. For the second statement, we have v (i) = u (i) if and only if u (i) = 0 or n j=1 m(i, j)u (j) = b (i) , and one directly checks that these are exactly the KKT conditions for a minimizer of F over vectors with non-negative entries. ■ To apply the lemma to the minimization in A, let M : A → AY T Y and b = X Y (we are working in the linear space of N by p matrices). Then the update</p><formula xml:id="formula_1938">a (i) k → a (i) k (X Y )(i, k) (AY T Y )(i.k)</formula><p>decreases the objective function.</p><p>Similarly, applying the lemma with the operator Y → Y A T A and b = X T A gives the update for Y , namely</p><formula xml:id="formula_1939">y (i) j → y (i) j (X T A)(i, j) (Y A T A)(i, j) .</formula><p>We have therefore obtained the following algorithm.</p><p>Algorithm 20.4 (NMF, quadratic cost) 1. Fix p &gt; 0 and let X be the N by d matrix containing the observed data. Initialize the procedure with matrices A and Y , respectively of size N by p and d by p, with positive coefficients.</p><p>2. At a given stage of the algorithm, let A and Y be the current matrices providing an approximate decomposition of X .</p><p>3. For the next step, let Ã be the matrix with coefficients</p><formula xml:id="formula_1940">ã(i) k = a (i) k (X Y )(i, k) (AY T Y )(i, k)</formula><p>and Ỹ the matrix with coefficients An alternative version of the method has been proposed, where the objective function is Φ(AY T ), where, for an N by</p><formula xml:id="formula_1941">ỹ(i) j = y (i) j (X T Ã)(i, j) (Y ÃT Ã)(i, j) .</formula><formula xml:id="formula_1942">d matrix Z = [z 1 , . . . , z N ] T , Φ(Z) = N k=1 d i=1 (z (i) k -x (i) k log z (i) k )</formula><p>which is indeed minimal for Z = X . We state and prove a second lemma that will allow us to address this problem.</p><p>Lemma 20.15 Let M be an n by q matrix and x ∈ R n , b ∈ R q , all assumed to have positive entries. For u ∈ (0, +∞) q , define</p><formula xml:id="formula_1943">F(u) = q j=1 b (j) u (j) - n i=1 x (i) log q j=1 m(i, j)u (j) . Define v ∈ (0, +∞) q by v (j) = u (j) n i=1 m(i, j)x (i) /α (i) b (j)</formula><p>with α (i) = q k=1 m(i, k)u (k) . Then F(v) ≤ F(u). Moreover, v = u if and only if u minimizes F subject to u (i) ≥ 0, i = 1, . . . , n.</p><p>Proof Introduce a variable β (j) &gt; 0 for j = 1, . . . , q an let w (j) = u (j) β (j) . Then</p><formula xml:id="formula_1944">F(w) = q j=1 b (j) u (j) β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) β (j) = q j=1 b (j) u (j) β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) β (j) q j=1 m(i, j)u (j) - n i=1 x (i) log q j=1 m(i, j)u (j)</formula><p>Let ρ(i, j) = m(i, j)u (j) /α (i) . Since the logarithm is concave, we have log q j=1 ρ(i, j)β (j) ≥ q j=1 ρ(i, j) log β (j)   so that</p><formula xml:id="formula_1945">F(w) ≤ q j=1 b (j) u (() jβ (j) - n i=1 q j=1 x (i) ρ(i, j) log β (j) - n i=1 x (i) log q j=1 m(i, j)u (j) .</formula><p>The upper bound with β (j) ≡ 1 gives F(u), so minimizing this expression in β will give F(w) ≤ F(u). This minimization is straightforward and gives</p><formula xml:id="formula_1946">β (j) = n i=1 x (i) ρ(i, j) b (j) u (j) = n i=1 m(i, j)x (i) /α (i) b (j)</formula><p>and the optimal w is the vector v provided in the lemma. Finally, one checks that v = u if and only if u satisfies the KKT conditions for the considered problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We can now apply this lemma to derive update rules for Y and A, where the objective is</p><formula xml:id="formula_1947">N k=1 d i=1 p j=1 y (i) j a (j) k - N k=1 d i=1 x (i) k log p j=1 y (i) j a (j) k .</formula><p>Starting with the minimization in A, we apply the lemma to each index k separately, taking n = d and q = p, with b (j) = d i=1 y</p><p>(i) j and m(i, j) = y (j)</p><p>i . Then the update is</p><formula xml:id="formula_1948">a k (j) → a k (j) d i=1 x (i) k y (i) j /α (i) k d i=1 y (i) j with α (i) k = p j=1 y (i) j a (j) k .</formula><p>For Y , we can work with fixed i and apply the lemma with n = N , q = p, b</p><formula xml:id="formula_1949">(j) = N k=1 a (j) k and m(k, j) = a (j)</formula><p>k . This gives the update:</p><formula xml:id="formula_1950">y (i) j → y (i) j N k=1 x (i) k a (j) k /α (i) k N k=1 a (j) k , still with α (i) k = p j=1 y (i) j a (j) k .</formula><p>We summarize this in our second algorithm for NMF. </p><formula xml:id="formula_1951">(j) k = a (j) k d i=1 x (i) k y (i) j /α (i) k d i=1 y (i) j with α (i) k = p j=1 y (i) j a (j) k . 5. Let Ỹ the matrix with coefficients ỹ(i) j = y (i) j N k=1 x (i) k ã(j) k / α(i) k p j=1 ã(j) k with α(i) k = p j=1 y (i) j ã(j) k . 6.</formula><p>Replace A by Ã and Y by Ỹ , iterating until numerical convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.8">Variational Autoencoders</head><p>Variational autoencoders, which were described in section 18.2.2, can be ineterpreted as a non-linear factor model in which X = g(θ, Y ) + ϵ where ϵ is a centered Gaussian noise with covariance matrix Q and Y ∈ R p has a known probability distribution, such as Y ∼ N (0, Id R p ). In this framework, the conditional distribution of Y given X = x was approximated as a Gaussian distribution with mean µ(x, w) and covariance matrix S(x, w) 2 . The implementation in Kingma and Welling <ref type="bibr" target="#b121">[103,</ref><ref type="bibr" target="#b122">104]</ref> use neural networks for the three functions g, µ and S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.9">Bayesian factor analysis and Poisson point processes 20.9.1 A feature selection model</head><p>The expectation in many factor models is that individual observations are obtained by mixing pure categories, or topics, and represented as a weighted sum or linear combination of a small number of uncorrelated or independent variables. Denote p the number of possible categories, which, in this section, can be assumed to be quite large.</p><p>We will assume that each observation randomly selects a small number among these categories before combining them. Let us consider (as an example) the following model.</p><p>• The observations X 1 , . . . , X N take the form of a probabilistic ICA model</p><formula xml:id="formula_1952">X k = p j=1 a k (j)b k (j)Y (j) + σ R k ,</formula><p>where:</p><p>• R k follows a standard Gaussian distribution,</p><p>• a k (1), . . . , a k (p) are independent with a k (j) ∼ N (m j , τ 2 j ), • b k (1), . . . , b k (p) are independent and follow a Bernoulli distribution with parameter π j ,</p><p>• Y (1) , . . . , Y (p) are independent standard Gaussian random variables.</p><p>• σ 2 follows an inverse gamma distribution with parameters α 0 , β 0 .</p><p>• τ 2 1 , . . . , τ 2 p follow independent inverse gamma distributions with parameters α 1 , β 1 . • m j follow a Gaussian N (0, ρ 2 ) and,</p><p>• π j follow a beta distribution with parameters (u, v).</p><p>The priors are, as usual, chosen so that the computation of posterior distributions is easy, i.e., they are conjugate priors. The observed data is therefore obtained by selecting components Y j with probability π j and weighted with a Gaussian random coefficient, then added before introducing noise.</p><p>Let n j = N k=1 b k (j). Ignoring constant factors, the joint likelihood of all variables together is proportional to:</p><formula xml:id="formula_1953">L ∝σ -N d exp         - 1 2σ 2 N k=1 |X k - p j=1 a k (j)b k (j)Y (j) | 2         p j=1        τ -N j exp        - 1 2τ 2 j N k=1 (a k (j) -m j ) 2               exp         - 1 2ρ 2 p j=1 m 2 j         p j=1 π n j j (1 -π j ) N -n j p j=1 (τ 2 j ) -α 1 -1 exp(-β 1 /τ 2 j ) (σ 2 ) α 0 -1 exp(-β 0 /σ 2 ) p j=1 π u-1 j (1 -π j ) v-1 exp        - 1 2 p i=1 |Y (i) | 2       </formula><p>In spite of the complexity of this expression, it is relatively straightforward (by considering each variable in isolation) to see that</p><p>• The conditional distribution of σ 2 , τ 2  1 , . . . , τ 2 p given all other variables remains a product of inverse gamma distributions.</p><p>• The conditional distribution of Y (1) , . . . , Y (p) given the other variables is Gaussian.</p><p>• The conditional distribution of π 1 , . . . , π p given the other variables is a product of beta distributions.</p><p>• The conditional distribution of m 1 , . . . , m p given the other variables remain independent Gaussian.</p><p>• The posterior distribution of a 1 , . . . , a N (considered as p-dimensional vectors) given the other variables is a product of independent Gaussian (but the components a k (j), j = 1, . . . , p are correlated).</p><p>• For the posterior distribution given the other variables, b 1 , . . . , b N (considered as p-dimensional vectors) are independent. The components of each b k are not independent but each b k (j) being a binary variable follows a Bernoulli distribution given the other ones.</p><p>These remarks provide the basis of a Gibbs sampling algorithm for the simulation of the posterior distribution of all unobserved variables (the computation of the parameters of each of the conditional distribution above requires some work, of course, and these details are left to the reader). This simulation does not explicitly provide a matrix factorization of the data (in the sense of a single matrix A such that X = AY , as considered in the previous section), but a probability distribution on such matrices, expressed as A(k, j) = a k (j)b k (j). One can however use the average of the matrices obtained through the simulation for this purpose. Additional information can be obtained through this simulation. For example, the expectation of b k (j) provides a measure of proximity of observation k to category j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.9.2">Non-negative and count variables</head><p>Poisson factor analysis. Many variations can be made on the previous construction. When the observations are non-negative, for example, an additive Gaussian noise may not be well adapted. Alternative models should model the conditional distribution of X given a, b and Y as a distribution over non-negative numbers with mean (a ⊙ b) T Y (for example a gamma distribution with appropriate parameters). The posterior sampling generally is more challenging in this case because simple conjugate priors are not always available.</p><p>An important special case is when X is a count variable taking values in the set of non-negative integers. In this case (starting with a model without feature selection), modeling X as a Poisson variable with mean a(1)Y (1) + • • • + a(p)Y (p) leads to tractable computations, once it is noticed that X can be seen as a sum of random variables Z [1] , . . . , Z [p] where Z [i] follows a Poisson distribution with parameter a(i)Y (i) . This suggests introducing new latent variables (Z [1] , . . . , Z [p] ), which are not observed but follow, conditionally to their sum, which is X and is observed, a multinomial distribution with parameters X, q 1 , . . . , q p , with q i = a(i)Y (i) /( p j=1 a(j)Y (j) ).</p><p>This provides what is referred to as a Poisson factor analysis (PFA). As an example, consider a Bayesian approach where, for the prior distribution, a(1), . . . , a(p) are independent and follow as a gamma distribution with parameters α 0 and β 0 , and Y (1) , . . . , Y (p) are independent, exponentially distributed with parameter 1. The joint likelihood of all data then is (up to constant factors):</p><formula xml:id="formula_1954">L ∝ exp        - N k=1 (a k (1)Y (1) + • • • + a k (p)Y (p) )                N k=1 p i=1 (a k (i)Y (i) ) z [i] k z [i] k !                N k=1 p i=1 a k (i) α-1        exp        -β N k=1 p i=1 a k (i)        exp        - p i=1 Y (i)        .</formula><p>This is the GaP (For Gamma-Poisson) model introduced in Canny <ref type="bibr" target="#b68">[50]</ref>. The conditional distribution of the variables (a k (i)) given (Z [i] k ) and (Y i ) are independent and gamma-distributed, and so are (Y (i) ) given the other variables. Finally, for each k, the family (Z <ref type="bibr" target="#b19">[1]</ref> k , . . . , Z</p><p>[p] k ) follows a multinomial distribution conditionally to their sum, X k , and the rest of the variables, and these variables are conditionally independent across k.</p><p>GaP with feature selection One can include a feature selection step in this model by introducing binary variables b(1), . . . , b(p), with selection probabilities π 1 , . . . , π p , with a Beta(u, v) prior distribution on π i . Doing so, the likelihood of the extended model is:</p><formula xml:id="formula_1955">L ∝ exp        - N k=1 (a k (1)b k (1)Y (1) + • • • + a k (p)b k (p)Y (p) )                N k=1 p i=1 (a k (i)b k (i)Y (i) ) z [i] k z [i] k !                N k=1 p i=1 a k (i) α-1        exp        -β N k=1 p i=1 a k (i)        exp        - p i=1 Y (i)        p j=1 π n j j (1 -π j ) N -n j p j=1 π u-1 j (1 -π j ) v-1 .</formula><p>where, as before, n j = N k=1 b k (j). The conditional distribution of π 1 , . . . , π p given the other variables is therefore still that of a family of independent beta-distributed variables. The binary variables b k (1), . . . , b k (p) are also conditionally independent given the other variables, with b k (i) = 1 with probability one if z</p><formula xml:id="formula_1956">[i] k &gt; 0 and with probability π j exp(-a k (j)Y (j) ) if z [j] k = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.9.3">Feature assignment model</head><p>The previous models assumed that p features were available, modeled as p random variables with some prior distribution, and that each observation picks a subset of them, drawing feature j with probability π j . We denoted by b k (j) the binary variable indicating whether feature j was selected for observation k, and n j was the number of times that feature was selected. Finally, we modeled π j as a beta variable with parameters u and v.</p><p>One can compute, using this model, the probability distribution of of the feature selection variables, b = (b k (j), j = 1, . . . , p, k = 1, . . . , N ). From the model definition, the probability of observing such a configuration is given by</p><formula xml:id="formula_1957">Q(b) = Γ (u + v) p Γ (u) p Γ (v) p p j=1 π n j +u-1 j (1 -π j ) N -n j +v-1 dπ 1 . . . dπ p = p j=1 Γ (u + v)Γ (u + n j )Γ (v + N -n j ) Γ (u)Γ (v)Γ (u + v + N ) = p j=1 u(u + 1) • • • (u + n j -1)v(v + 1) • • • (v + N -n j -1) (u + v)(u + v + 1) • • • (u + v + N -1)</formula><p>Denote by n jk = k-1 l=1 b l (j) the number of observations with index less than k that pick feature j. Using this notation, we can write, using the fact that</p><formula xml:id="formula_1958">u(u + 1) • • • (u + n j -1) = N k=1 (u + n jk ) b k (j)</formula><p>and a similar identity for v(v + 1)</p><formula xml:id="formula_1959">• • • (v + N -n j -1), Q(b) = p j=1 N k=1 (u + n jk ) b k (j) (v + k -1 -n jk ) 1-b k (j) u + v + k -1 = N k=1 p j=1 u + n jk u + v + k -1 b k (j) v + k -1 -n jk u + v + k -1 1-b k (j)</formula><p>.</p><p>Using this last equation, we can interpret the probability Q as resulting from a progressive feature assignment process. The first observation, k = 1, for which n jk = 0 for all j, chooses each feature with probability u/(u + v). When reaching observation k, feature j is chosen with probability (u + n jk )/(u + v + k -1). At all steps, features are chosen independently from each other.</p><p>Let F k be the set of features assigned to observation k, i.e., F k = {j : b k (j) = 1} and</p><formula xml:id="formula_1960">G k = F k \ k-1 l=1 F l</formula><p>be the set of features used in observation k but in no previous observation. Let </p><formula xml:id="formula_1961">C k = F k \ G k and U k = G 1 ∩ • • • ∩ G k-1 .</formula><formula xml:id="formula_1962">Q(S) = N k=1       u u + v + k -1 q k v + k -1 u + v + k -1 p-p k+1 j∈U k u + n jk u + v + k -1 1 j∈C k v + k -1 -n jk u + v + k -1 1 j C k       . Let S k = (G l , C l , l ≤ k).</formula><p>Then the expression of Q shows that, conditionally to S k-1 , G k and C k are independent. Elements in C k are chosen independently for each feature j ∈ U k with probability (u + n jk )/(u + v + k -1). Moreover, the conditional distribution of q k given S k-1 is proportional to</p><formula xml:id="formula_1963">u u + v + k -1 q k v + k -1 u + v + k -1 p-p k -q k</formula><p>i.e., it is a binomial distribution with parameters pp k and u/(u + v + k -1). Finally, given s k-1 and q k , the distribution of G k is uniform among all p-p k q k subsets of</p><formula xml:id="formula_1964">{1, . . . , p} \ (G 1 ∪ • • • ∪ G k-1 ) with cardinality q k .</formula><p>If there is no special meaning in the feature label, which is the case in our discussion of prior models in which all features are sampled independently with the same distribution, we may identify configurations that can be deduced from each other by relabeling (note that relabeling features does not change the value of Q).</p><p>Call a configuration normal if G k = {p k + 1, . . . , p k+1 }. Given S, it is always possible to relabel the features with a permutation σ so that, for each k, σ (G k ) = {p k + 1, . . . , p k+1 }. There are, in fact, q 1 ! . . . q N ! such permutations. We can complete the process generating S by adding at the end a transformation into a normal configuration (picking uniformly at random one of the possible ones). The probability of a normal configuration S obtained through this process is (using a simple counting argument)</p><formula xml:id="formula_1965">Q(S) = N k=1       p -p k q k u u + v + k -1 q k v + k -1 u + v + k -1 p-p k+1 j∈U k u + n jk u + v + k -1 1 j∈C k v + k -1 -n jk u + v + k -1 1 j C k       ,</formula><p>This provides a new incremental procedure that directly samples normalized assignments. First let q 1 follow a binomial distribution bin(p, u/(u + v)) and assign the first observation to features 1 to q 1 . Assume that p k labels have been created before step k. Then select for observation k some of the already labeled features, label j being selected with probability (u + n jk )/(u + v + k -1) as above. Finally, add q k new features where q k follows a binomial distribution bin(pp k , u/(u</p><formula xml:id="formula_1966">+ v + k -1)).</formula><p>This discussion is clearly reminiscent of the one that was made in section 19.7.3 leading to the Polya urn process, and we want here also to let p tend to infinity (with fixed N ) with proper choices of u and v as functions of p in the expression above. Choose two positive numbers c and γ and let u = cγ/p and v = cu. Note that, with the incremental simulation process that we just described, the conditional expectation of the next number of labels, p k+1 given the current one, p k is</p><formula xml:id="formula_1967">E(p k+1 |p k ) = (p -p k )u u + v + k -1 + p k = cγ c + k -1 + 1 - cγ p(c + k -1) p k ≤ cγ c + k -1 + p k</formula><p>Taking expectations on both sides, we get</p><formula xml:id="formula_1968">E(p k+1 ) ≤ k l=1 cγ c + l -1 ≤ N l=1 cγ c + l -1</formula><p>so that this expectation is bounded independently of k. This shows in particular that p k /p tends to 0 in probability (just applying Markov's inequality) and that the binomial distribution bin(pp k , u/(u + v + k -1)) can be approximated by a Poisson distribution with parameter cγ/(c + k -1). So, when p → ∞, we obtain the following incremental simulation process for the feature labels, that we combine with the actual simulation of the features, assumed to follow a prior distribution with p.d.f. ψ. This process is called the Indian buffet process in the literature, the analogy being that a buffet offers an infinite variety of dishes, and each observation is a customer who tastes a finite number of them. (i) Sample an integer q 1 according to a Poisson distribution with parameter γ.</p><p>(ii) Sample features y (1) , . . . , y (q 1 ) according to ψ.</p><p>(iii) Assign these features to observation 1, and let n 2,j = 1 for j = 1, . . . , q 1 .</p><p>2. Assume that observations 1 to k-1 have been obtained, with p k features y (1) , . . . , y (p k ) such that the jth feature has been chosen n k,j times.</p><p>(i) For j = 1, . . . , p k , assign feature j to sample k with probability</p><formula xml:id="formula_1969">n k,j c+k-1 . If j is selected, let n k+1,j = n k,j + 1, otherwise let n k+1,j = n k,j .</formula><p>(ii) Sample an integer q k according to a Poisson distribution with parameter cγ c+k-1 and let p k+1 = p k + q k . (iii) Sample features y (p k +1) , . . . , y (p k+1 ) according to ψ.</p><p>(iv) Assign these features to observation k, and let n k+1,j = 1 for j = p k +1, . . . , p k .</p><p>3. If k = N , stop, otherwise replace k by k + 1 and return to Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.10">Point processes and random measures</head><p>This section assumes that the reader is familiar with measure theory. It can however safely be skipped as it is not reused in the rest of the book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.10.1">Poisson processes</head><p>If Z is a set, we will denote by P c (Z) the set composed with all finite or countable subsets of Z. A point process over Z is a random variable S : Ω → P c (Z), i.e., a variable that provides a countable random subset of Z. If B ⊂ Z one can then define the counting function ν</p><formula xml:id="formula_1970">S (B) = |S ∩ B| ∈ Z ∪ {+∞}.</formula><p>A proper definition of such point processes requires some measure theory. Equip Z with a σ -algebra A and consider the set N 0 of integer-valued measures µ on (Z, A) such that µ(Z) &lt; ∞. Let N be the set formed with all countable sums of measures in N 0 . Then a general point process is a mapping ν : Ω → N such that for all k ∈ N ∪ {+∞} and all B ∈ A, the event {ν(B) = k} is measurable. Recall that, for each B ∈ A, ν(B) is itself a random variable, that we may denote ω → ν ω (B). One then define the intensity of the process as the the function µ : B → E(ν(B)).</p><p>The following proposition provides an important identity satisfied by such models.</p><p>Theorem 20.16 (Campbell identity) Let ν be a point process with intensity µ. For ω ∈ Ω, let X ω : Ω ′ → Z be a random variable with distribution ν ω (defined, if needed, on a different probability space (Ω ′ , P ′ )). Then, for any µ-integrable function f :</p><formula xml:id="formula_1971">E(f (X)) = Z f (z)dµ(z). (20.22)</formula><p>Here, the expectation of f (X) is over both spaces Ω and Ω ′ and corresponds to the average of f . The identity is an immediate consequence of Fubini's theorem.</p><p>We will be mainly interested in the family of Poisson point processes. These processes are themselves parametrized by a measure, say µ, on Z such that µ is σfinite and µ(B) = 0 if B is a singleton. A Poisson process with intensity measure µ is a point process ν such that:</p><p>(i) If B 1 , . . . , B n are non-intersecting pairwise, then ν(B 1 ), . . . , ν(B n ) are mutually independent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(ii) for all B, ν(B) ∼ Poisson(µ(B)).</head><p>We take the convention that ν(B) = 0 (resp. = ∞) almost surely if µ(B) = 0 (resp. = ∞). Note that property (i) also implies that if g 1 , . . . , g n are measurable functions from Z to (0, +∞) such that g i g j = 0 for i j, then the variables ν(g i ) = Z g i (z)dν(z) are independent.</p><p>If µ(Z) &lt; ∞ (i.e., µ is finite), one can represent the distribution of a Poisson point process as follows:</p><formula xml:id="formula_1972">ν = ν(Z) k=1 δ X k with ν(Z) ∼ Poisson(µ(Z))</formula><p>and, conditional to ν(Z) = N , X 1 , . . . , X N are i.i.d. and follow the probability distribution μ = µ/µ(Z). This measure can also be identified with the random set S = {X 1 , . . . , X ν(Z) }. The assumption that µ({z}) = 0 for any singleton implies that ν({z}) = 0 almost surely. It also ensures that the points X 1 , . . . , X N are distinct with probability one.</p><p>If µ is σ -finite, then (by definition), it is a countable sum of finite measures µ 1 , µ 2 , . . .. Then ν can be generated as the sum of independent ν 1 , ν 2 , . . ., where ν i is a Poisson process with intensity µ i . It can moreover be identified with the countable random set S = ∞ i=1 S i , where S i is the random set associated with ν i . Note that, in this construction, one can always assume that the measures µ 1 , µ 2 , . . . are mutually singular (i.e., µ i (B) &gt; 0 for some i implies that µ j (B) = 0 for j i).</p><p>If we consider a Poisson process on (0, +∞) × Z, we can define weighted random measures. Indeed, such a point process takes values in the collection of all sets of the form {(w k , z k ), k ∈ I} where I is finite or countable. These subsets can be represented as the sum of weighted Dirac masses,</p><formula xml:id="formula_1973">ξ = k∈I w k δ z k .</formula><p>To ensure that the points (z k , k ∈ I) generated by this process are all different, we need to assume that the intensity µ of this random process is such that µ((0, +∞) × {z}) = 0 for all z ∈ Z. We will refer to ξ as a weighted Poisson process.</p><p>In the following, we will consider this class of random measures, with the small addition of allowing for an extra term including a measure supported by a fixed set. More precisely, given a (deterministic) countable subset I ⊂ Z, a family of independent random variables (ρ z , z ∈ I ) and a σ -finite measure µ o such that µ o ((0, +∞) × {z}) = 0 for all z ∈ Z, we can define the random measure</p><formula xml:id="formula_1974">ξ = ξ f + ξ o</formula><p>where ξ o is a weighted Poisson process with intensity µ o , assumed independent of (ρ z , z ∈ I ) and</p><formula xml:id="formula_1975">ξ f = z∈I ρ z δ z .</formula><p>The subscripts o and f come from the terminology introduced in Kingman <ref type="bibr" target="#b123">[105]</ref>, which studies "completely random measures," which are a random measures that satisfy point (i) in the definition of a Poisson process. Under mild assumptions, such measures can be decomposed as a sum of a weighted Poisson process (here, ξ o , the ordinary part), of a process with fixed support, (here, ξ f , the fixed part) and of a deterministic measure (which is here taken to be 0).</p><p>Let us rapidly check that ξ satisfies property (i). Let B 1 , . . . , B n be non-overlapping elements of A. Get g i (w, z) = w1 B i (z). Then</p><formula xml:id="formula_1976">ξ(B i ) = ξ f (B i ) + ν o (g i )</formula><p>where ν o is a Poisson process with intensity µ o . Since the sets do not overlap, the variables (ξ f (B i ), i = 1, . . . , n) are independent, and so are (ν o (g i ), i = 1, . . . , n) since g i g j = 0 for i j. Since ξ f and ν o are, in addition independent, we see that (ξ(B i ), i = 1, . . . , n) are independent.</p><p>The intensity measure of such a process is still defined by</p><formula xml:id="formula_1977">η(B) = E(ξ(B)) = z∈I P ((ρ z , z) ∈ B) + (0,+∞)×B wdµ o (w, z)</formula><p>where the last term is an application of Campbell's inequality to the Poisson process ν o and the function g(w, x) = w1 B (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.10.2">The gamma process</head><p>The main example of such processes in factor analysis is the beta process that will be discussed in the next section. We start, however, with a first example that is closely related with the Dirichlet process, called the gamma process.</p><p>In this process, one fixes a finite measure π 0 on Z and defines µ on (0, +∞) × Z by µ(dw, dz) = cw -1 e -cw π 0 (dz)dw.</p><p>Because µ is σ -finite but not finite (the integral over t diverges at t = 0), every realization of ξ is an infinite sum</p><formula xml:id="formula_1978">ξ = ∞ k=1 w k δ z k . The intensity measure of ξ is η(B) = cπ 0 (B) +∞ 0 e -cw dw = π 0 (B).</formula><p>In particular,</p><formula xml:id="formula_1979">∞ k=1 w k = η(Z) = π 0 (Z) &lt; ∞.</formula><p>For fixed B, the variable ξ(B) follows a Gamma distribution. This can be proved by computing the Laplace transform of ξ, E(e -λξ(B) ), and identify it to that of a Gamma. To make this computation, consider the point process ν J restricted to a interval J ⊂ (0, +∞) with min(J) &gt; 0, and ξ J the corresponding weighted process. Let m J (t) = J w -1 ce -(c+t)w dw. Then a realization of ν J can be obtained by first sampling N from a Poisson distribution with parameter µ(J × Z) = m J (0)π 0 (Z) and then sampling N points (w i , z i ) independently from the distribution µ/(m J (0)π 0 (Z)). This implies that</p><formula xml:id="formula_1980">E(e -tξ J (B) ) = ∞ n=0 e -m J (0)π 0 (Z) (m J (0)π 0 (Z)) n n!        ∞ 0 e -tw1 B (z) w -1 ce -cw dwdπ 0 (z) m J (0)π 0 (Z)        n = ∞ n=0 e -m J (0)π 0 (Z) n! π 0 (B)m J (t) + (π 0 (Z) -π 0 (B))m J (0) n = e π 0 (B)(m J (t)-m J (0)) . Now, m J (t) -m J (0) = c J</formula><p>e cw e -tw -1 w dw is finite even when J = (0, +∞). With a little more work justifying passing to the limit, one finds that, for J = (0, +∞),</p><p>E(e -tξ J (B) ) = exp π 0 (B) +∞ 0 e -cw e -tw -1 w dw . ).</p><p>This shows that</p><formula xml:id="formula_1981">E(e -tξ J (B) ) = 1 + t c -cπ 0 (B)</formula><p>which is the Laplace transform of a Gamma distribution with parameters cπ 0 (B) and c, i.e., with density proportional to w cπ 0 (B)-1 e -cw .</p><p>As a consequence, the normalized process δ = ξ/ξ(Z) is a Dirichlet process with intensity cπ 0 . Indeed, if B 1 , . . . , B n is a partition of Z the family (δ(B 1 ), . . . , δ(B n )) is the ratio of n independent gamma variables to their sum, which provides a Dirichlet distribution, and this property characterizes Dirichlet processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.10.3">The beta process</head><p>The definition of the beta process parallels that of the gamma process, with weights taking this time values in (0, 1). Fix again a finite measure π 0 on Z and let µ o on (0, +∞) × Z be defined by</p><formula xml:id="formula_1982">µ o (dw, dz) = cw -1 (1 -w) c-1 π 0 (dz)dw.</formula><p>The associated weighted Poisson process can therefore be represented as a sum</p><formula xml:id="formula_1983">ξ o = ∞ k=1 w k δ z k ,</formula><p>and its intensity measure is</p><formula xml:id="formula_1984">η o (B) = cπ 0 (B) 1 0 (1 -t) c-1 dw = π 0 (B).</formula><p>In particular, since π 0 is finite, we have ∞ k=1 w k &lt; ∞ almost surely. A beta process is the sum of the process ξ o and of a fixed set process</p><formula xml:id="formula_1985">ξ f = z∈I w z δ z</formula><p>where I is a fixed finite set and (w z , z ∈ I ) are independent and follow a beta distribution with parameters (a(z), b(z)).</p><p>If Z is a space of features, such a process provides a prior distribution on feature selections. It indeed provides, in addition to the deterministic set I , a random countable set J ⊂ Z, with a set of random weights w z , z ∈ F := I ∪ J . Given this, one defines the feature process as the selection of a subset A ⊂ F where each feature z is selected with probability w z . Because E(|A|) = z∈F w z is finite, A is finite with probability 1.</p><p>In the same way the Polya urn could be used to sample from a realization of a Dirichlet process without actually sampling the whole process, there exists an algorithm that samples a sequence of feature sets (A 1 , . . . , A n ) from this feature selection process without needing the infinite collection of weights and features associated with a beta process. We assume in the following that the prior process has an empty fixed set. (Non-empty fixed sets will appear in the posterior.)</p><p>The first set of features, A 1 , is obtained as follows according to a Poisson process with intensity π 0 : choose the number N of features in A 1 according to a Poisson distribution with parameter π 0 (Z). Then sample N features independently according to the distribution π 0 /π 0 (Z). Now assume that n -1 sets of features A 1 , . . . , A n have been obtained and we want to sample a new set A n+1 conditionally to their observation. Let J n be the union of all random features obtained up to this point and n(z), for z ∈ J n the number of times this feature was observed in A 1 , . . . , A n . Then the conditional distribution of the beta process ξ given this observation is still a beta process, with fixed set given by I = J n , (a(z), b(z)) = (n(z), c + nn(z)) for z ∈ J n-1 and base measure π n = cπ 0 /(c + n). This implies that the next set A n+1 can be obtained by sampling from the associated feature process. To do this, one first selects features z ∈ J n with probability n(z)/(c + n), then selects additional features z 1 , . . . , z N independently with distribution π 0 /π 0 (Z) where N follows a Poisson distribution with parameter cπ 0 (Z)/(c + n). This is the Indian buffet process, described in Algorithm 20.6 (taking π 0 = γψ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="20.10.4">Beta Process and feature selection</head><p>The beta process can be used as a prior for feature selection within a factor analysis model, as described in the previous paragraph. It is however easier to approximate it with a model with almost surely finite support. Indeed, letting, for ϵ &gt; 0</p><formula xml:id="formula_1986">µ(dw, dz) = Γ (c + 1) Γ (ϵ + 1)Γ (c -ϵ) w ϵ-1 (1 -w) c-ϵ π 0 (dz)dw, one obtains a finite measure since +∞ 0 Z µ o (dw, dz) = cγ ϵ</formula><p>where γ = π 0 (Z). Note that µ is normalized so that E(ξ(B)) = π 0 (B) for B ⊂ Z.</p><p>In this case, the prior generates features by first sampling their number, p, randomly according to a Poisson distribution with mean cγ/ϵ, then select p probabilities w 1 , . . . , w p independently using a beta distribution with parameters ϵ and cϵ, and finally attach to each i a feature z i with distribution π 0 /γ. The features associated with a given sample are then obtained by selecting each z i with probability w i .</p><p>We note also that the model described in section 20.9.3 provides an approximation of this prior using a finite number of features. With our notation here, this corresponds to taking p ≫ 1 and ϵ = cγ/p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 21</head><p>Data Visualization and Manifold Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1">Multidimensional scaling</head><p>The methods described in this chapter aim at representing a dataset in low dimension, allowing for its visual exploration by summarizing its structure in a useraccessible interface. Unlike factor analysis methods, they do not necessarily attempt at providing a causal model expressing the data as a function of a small number of sources, and generally do not provide a direct mechanism for adding new data to the representation. In addition, all these methods take as input similarity dissimilarity matrices between data points and do not require, say, Euclidean coordinates.</p><p>Assuming that a dissimilarity matrix D = (d kl , k, l = 1, . . . , N ) is given, the goals of multidimensional scaling (or MDS) is to determine a small-dimensional Euclidean representation, say y 1 , . . . , y N ∈ R p , such that y ky l 2 ≃ d 2 kl . We review below two versions of this algorithm, referred to as "similarity" and "dissimilarity" matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1.1">Similarity matching (Euclidean case)</head><p>We start with the standard hypotheses of MDS, assuming that the distances d kl derive from a representation in feature space, so that d 2 kl = ∥h kh l ∥ 2 H for some innerproduct space H and (possibly unknown) features h 1 , . . . , h N . Note that, since the Euclidean distance is invariant by translation, there is no loss of generality in assuming h 1 + • • • + h N = 0, which will be done in the following.</p><p>We look for a p-dimensional representation in the form y k = Φh k where Φ is a linear transformation (and we want y k to be computable directly from dissimilarities, since we do not assume that h k is known). Since we are only interested in a transformation of the h 1 , . . . , h N , it suffices to compute Φ in the vector space generated by them, so that we let Φ : span(h 1 , . . . , h N ) → R p , and we want Φ to (approximately) conserve the norm, i.e., be close to being an isometry.</p><p>Because isometries are one-to-one and onto, the existence of an exact isometry would require V ∆ = span(h 1 , . . . , h N ) to be p-dimensional. The mapping Φ could then be defined as Φ(h) = (⟨h , e 1 ⟩ H , . . . , ⟨h , e p ⟩ H ) where e 1 , . . . , e p is any orthonormal basis of V . In the general case, however, where V is not p-dimensional or less, one can replace it by a best p-dimensional approximation of the training data, leading to a problem similar to PCA in feature space.</p><p>Indeed, as we have seen in section 20.1.2, this best approximation can be obtained by diagonalizing the Gram matrix S of h 1 , . . . , h N , which is such that s kl = ⟨h k , h l ⟩ H . (Recall that we assume that h = 0, so we do not center the data here.) Using the notation in section 20.1.2, let α (1) , . . . , α (p) denote the eigenvectors associated with the p largest eigenvalues, normalized so that (α (i) ) T Sα (i) = 1 for i = 1, . . . , p. One can then take</p><formula xml:id="formula_1987">e i = N l=1 α (i) l h l</formula><p>and, for k = 1, . . . , N , j = 1, . . . , p: y</p><formula xml:id="formula_1988">(i) k = λ 2 i α (i) k (21.1)</formula><p>where λ 2 i is the eigenvalue associated with α (i) .</p><p>This does not entirely address the original problem, since the inner products s kl are not given, but only the distances d kl , which satisfy</p><formula xml:id="formula_1989">d 2 kl = -2s kl + s kk + s ll . (<label>21.2)</label></formula><p>This provides a linear system of equations in the unknown s kl . This system is underdetermined, because D is invariant by any transformation h k → h k + h 0 (for a fixed h 0 ), and S is not. However, the assumption h 1 +</p><p>• • • + h N = 0 provides the additional constraint needed to provide a unique solution. Summing (21.2) over l, we then get N l=1 d 2 kl = N s kk + N l=1 s ll . (21.3) Summing this equation over k, we find N k,l=1 d 2 kl = 2N N l=1 s ll . Using this in (21.3), we get s kk = 1 N N l=1 d 2 kl -1 2N 2 N k,l=1 d 2 kl , and, from (21.2)</p><formula xml:id="formula_1990">s kl = - 1 2         d 2 kl - 1 N N k ′ =1 d 2 k ′ l - 1 N N l ′ =1 d 2 kl ′ + 1 N 2 N k ′ ,l ′ =1 d 2 k ′ l ′         .</formula><p>If we denote by D ⊙2 the matrix formed with the squared distances d 2 kl , this identity can we rewritten in the simpler form</p><formula xml:id="formula_1991">S = - 1 2 P D ⊙2 P (21.4) with P = Id R N -1 N 1 T N /N .</formula><p>We now show that this PCA approach to MDS is equivalent to the problem of minimizing</p><formula xml:id="formula_1992">F(y) = N k,l=1 (y T k y l -s kl ) 2<label>(21.5)</label></formula><p>over all y 1 , . . . , y N ∈ R p such that y 1 +• • •+y N = 0, which can be interpreted as matching "similarities" s kl rather than distances. Indeed, letting Y denote the N by p matrix with rows y T 1 , . . . , y T N , we have</p><formula xml:id="formula_1993">F(y) = trace((Y Y T -S) 2 ).</formula><p>Finding Y is equivalent to finding a symmetric matrix M of rank p minimizing trace((M -S) 2 ). We have, using the trace inequality (theorem 2.1), and letting λ</p><formula xml:id="formula_1994">2 1 ≥ • • • ≥ λ 2 N (resp. µ 2 1 ≥ • • • ≥ µ 2 p ) denote the eigenvalues of S (resp. M) trace((M -S) 2 ) = trace(M 2 ) -2trace(MS) + trace(S 2 ) = p k=1 µ 4 k -2trace(MS) + N k=1 λ 4 k ≥ p k=1 µ 4 k -2 p k=1 λ 2 k µ 2 k + N k=1 λ 2 k = p k=1 (λ 2 k -µ 2 k ) 2 + N k=p+1 λ 4 k ≥ N k=p+1 λ 4 k</formula><p>for any connected component Γ of this graph. (If all weights are positive, then the only non-empty connected component is {1, . . . , N } and we retrieve our previous constraint N k=1 y k = 0.)</p><p>Standard nonlinear optimization methods, such as projected gradient descent, may be used to minimize G, but the preferred algorithm for MDS uses a stepwise procedure resulting from the addition of an auxiliary variable. Rewrite</p><formula xml:id="formula_1995">G(y) = N k,l=1 w kl |y k -y l | 2 -2 N k,l=1 w kl d kl |y k -y l | + N k,l=1 d 2 kl .</formula><p>We have, for u ∈ R p : |u| = max{z T u : z ∈ R p , |z| = 1 u 0 } . Using this identity, we can introduce auxiliary variables z kl , k, l = 1, . . . , N in R p , with</p><formula xml:id="formula_1996">|z kl | = 1 if y k y l and define Ĝ(y, z) = N k,l=1 w kl |y k -y l | 2 -2 N k,l=1 w kl d kl (y k -y l ) T z kl + N k,l=1 d 2 kl .</formula><p>We then have</p><formula xml:id="formula_1997">G(y) = min z:|z kl |=1 if y k y k Ĝ(y, z).</formula><p>As a consequence, minimizing G in y can be achieved by minimizing Ĝ in y and z and discarding z when this is done. One can minimize Ĝ iteratively, alternating minimization in y given z and in z given y, both steps being elementary. In order to describe these steps, introduce some matrix notation.</p><p>Let L denote the Laplacian matrix of the weighted graph on {1, . . . , N } associated with the weight matrix W , namely L = (ℓ kl , k, l = 1, . . . , N ) with ℓ kk = N k=1 w klw kk and ℓ kl = -w kl when k l. Then,</p><formula xml:id="formula_1998">N k,l=1 w kl |y k -y l | 2 = 2trace(Y T LY ). Defining u k ∈ R p by u k = N l=1 w kl d kl (z kl -z lk ), and U =           u T 1 . . . u T N           , we have N k,l=1 w kl d kl (y k -y l ) T z kl = trace(U T Y ).</formula><p>With this notation, the optimal matrix Y must minimize 2trace(Y T LY ) -2trace(U T Y ).</p><p>Let m be the number of connected components of the weighted graph. Recall that the matrix L is positive semi-definite and that an orthonormal basis of its null space is provided by vectors, say e 1 , . . . , e m , that are constant on each of the m connected components of the graph, so that the constraint on Y can be written as e where we have used the fact that L-1 e j = e j . We can now identify µ j since</p><formula xml:id="formula_1999">0 = e T j Y = 1 2 e T j L-1 U - 1 4 m j ′ =1 e T j e j ′ µ T j = 1 2 e T j U - 1 4 µ T j so that µ T j = 2e T j U and the optimal Y is Y = 1 2 L-1 U - 1 2 m j=1</formula><p>e j e T j U .</p><p>Note that this expression can be rewritten as</p><formula xml:id="formula_2000">Y = 1 2 P L L-1 U</formula><p>where P L = Id R N -N k=1 e j e T j is the projection onto the space perpendicular to the null space of L (i.e., the range of L). In the case where the graph has a single connected component, one has m = 1 and e 1 = 1 N / √ N yielding</p><formula xml:id="formula_2001">P L = Id R N - 1 N 1 N 1 T N .</formula><p>The minimization in z given y is straightforward: if y k y k , then z kl = (y ky l )/|y ky l |. If y k = y l , then one can take any value for z kl and the simplest if of course z kl = 0. Using the previous computation, we can summarize a training algorithm for multi-dimensional scaling, called SMACOF for "Scaling by Maximizing a Convex Function" (see, e.g., Borg and Groenen <ref type="bibr" target="#b54">[36]</ref> for more details and references).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 21.1 (SMACOF)</head><p>Assume that a symmetric matrix of dissimilarities (d kl , k, l = 1, . . . , N ) is given, together with a matrix of weights (w kl , k, l = 1, . . . , N ). Fix a target dimension, p. Fix a tolerance constant ϵ.</p><p>1. Compute the Laplacian matrix L of the graph associated with the weights, the projection matrix P L onto the range of L and the matrix M = (L + Id R N -P L ) -1 .</p><p>2. Initialize the algorithm with some family y 1 , . . . , y N ∈ R p and let Y =</p><formula xml:id="formula_2002">          y T 1 . . . y T N           .</formula><p>3. At a given step of the algorithm, let Y be the current solution and compute, for k = 1, . . . , N :  </p><formula xml:id="formula_2003">u k = 2 N l=1 w kl d kl y k -y l |y k -y l | 1 y k y l to form the matrix U =           u T 1 . . . u T N           . 4. Compute Y ′ = 1 2 P L MU . 5. If |Y -Y ′ | ≤ ϵ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2">Manifold learning</head><p>The goal of MDS is to map a full matrix of distances into a low-dimensional Euclidean space. Such a representation, however, cannot address the possibility that the data is supported by a low-dimensional, albeit nonlinear, space. For example, people leaving on Earth live, for all purposes, on a two-dimensional structure (a sphere), but any faithful Euclidean representation of the world population needs to use the three spatial dimensions. One may also argue that the relevant distance between points on Earth is not the Euclidean one either (because one would never travel through Earth to go from one place to another), but the distance associated to the shortest path on the sphere, which is measured along great circles.</p><p>To take another example, the left panel in fig. <ref type="figure" target="#fig_5">21</ref>.1 provides the result of applying MDS to a ten-dimensional dataset obtained by applying a random ten-dimensional rotation to a curve supported by a three-dimensional torus. MDS indeed retrieves the correct curve structure in space, which is three dimensional. However, for a person "living" on the curve, the data is one-dimensional, a fact that is captured by the Isomap method that we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.1">Isomap</head><p>Let us return to the example of people living on the spherical Earth. One can define the distance between two points on Earth either as the shortest length a person would have to travel (say, by plane) to go from one point to the other (that we can call the intrinsic distance), or simply the chordal distance in 3D space between the two points. The first one is obviously the most relevant to the spherical structure of the Earth, but the second one is easier to compute given the locations of the points in space.</p><p>For typical datasets, the geometric structure of the data (e.g., that it is supported by a sphere) is unknown, and the only information that is available is their chordal distance in an ambient space (which can be very large). An important remark, however is that, when the points are close to each other, the two distances can be expected to be similar, if we assume that the geometry of the set supporting the data is locally linear (e.g., that it is, like the sphere, a "submanifold" of the ambient space, with small neighborhoods of any data point well approximated, at first order, by points on a tangent space). Isomap uses this property, only trusting small distances in the matrix D, and infers large distances by adding the costs resulting from traveling from data points to nearby data points.</p><p>Fix an integer c. Given D, the c-nearest neighbor graph on V = {1, . . . , N } places an edge between k and l if and only if d k,l is among the c smallest values in {d kl ′ , l ′ k} neighbors or x l among the c smallest values in {d k ′ l , k ′ l}. We will write k ∼ c l to indicate that there exists an edge between k and l in this graph. One then defines the geodesic distance on the graph as</p><formula xml:id="formula_2004">d ( * ) kl = min          m j=1 d k j-1 k j : k 0 , . . . , k m ∈ {1, . . . , N }, k 0 = k ∼ c k 1 ∼ c • • • ∼ c k m-1 ∼ c k m = l, m ≥ 0          .</formula><p>This geodesic distance can be computed incrementally as follows. First define d </p><formula xml:id="formula_2005">d (n) kl = min d (n-1) kl ′ + d (1)</formula><p>ll ′ l ′ = 1, . . . , N until the entries stabilize, i.e., d (n+1) = d (n) , in which case one has d ( * ) = d (n) . The validity of the statement can be easily proved by checking that</p><formula xml:id="formula_2006">d (n) kl = min          n j=1 d (1) k j-1 k j : k 0 , . . . , k n ∈ {1, . . . , N }, k 0 = k, k n = l         </formula><p>, which can be done by induction, the details being left to the reader. It should also be clear that the procedure will stabilize after no more than N steps.</p><p>Once the distance is computed, Isomap then applies standard MDS, resulting in a straightened representation of the data like in fig. 21.1. Another example is provided in fig. <ref type="figure" target="#fig_5">21</ref>.2, where, this time, the input curve is closed and cannot therefore be represented as a one-dimensional structure. One can note, however, that, even in this case, Isomap still provides some simplification of the initial shape of the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.2">Local Linear Embedding</head><p>Local linear embedding (LLE) exploits in a different way the fact that manifolds are locally well approximated by linear spaces. Like Isomap, it starts also with building a c-nearest-neighbor graph on {1, . . . , k}. Assume, for the sake of the discussion, that the distance matrix is computed for possibly unobserved data T = (x 1 , . . . , x N ). Letting N k denote the indices of the nearest neighbors of k (excluding k itself), the basic assumption is that x k should approximately lie in the affine space generated by x l , l ∈ N k . Expressed in barycentric coordinates, this space is defined by</p><formula xml:id="formula_2007">T k =          l∈N k ρ (l) x l : ρ ∈ R N k , l∈N k ρ (l) = 1         </formula><p>, and T k can be interpreted as an approximation of the tangent space at x k to the data manifold. Optimal coefficients (ρ ( ) kl, k = 1, . . . , N , l ∈ N k ) providing the representation of x k in that space can be estimated by minimizing, for all k</p><formula xml:id="formula_2008">x k - l∈N k ρ (l) k x l 2 subject to l∈N k ρ (l) k = 1. This is a simple least-square program. Let c k = |N k | (c k = c</formula><p>in the absence of ties). Order the elements of N k to represent ρ (l) k , l ∈ N k as a vector denoted ρ k ∈ R c k . Similarly, let S k be the Gram matrix associated with x l , l ∈ N k formed with all inner products x T l ′ x l , l, l ′ = 1, . . . , N and let r k be the vector composed with products x T k x l , l ∈ N k . Assume that S k is invertible, which is generally true if c &lt; d, unless the neighbors are exactly linearly aligned. Then, the optimal ρ k and the Lagrange multiplier λ for the constraint are given by</p><formula xml:id="formula_2009">ρ k λ = S k 1 c k 1 T c k 0 -1 r k 1 . (21.6)</formula><p>If S k is not invertible, the problem is under-constrained and one of its solutions can be obtained by replacing the inverse above by a pseudo-inverse.</p><p>The low-dimensional representation of the data, still denoted (y 1 , . . . , y N ) with y k ∈ R p is then estimated so that the relative position of y k to its neighbors is the same as that of x k , i.e., so that</p><formula xml:id="formula_2010">y k ≃ l∈N k ρ (l) k y l .</formula><p>These vectors are estimated by minimizing</p><formula xml:id="formula_2011">F(y) = N k=1 y k - l∈N k ρ (l) k y l 2 .</formula><p>Obviously, some additional constraints are needed to avoid the trivial solution y k = 0 for all k. Also, replacing all y k 's by y ′ k = Ry k + b where R is an orthogonal transformation in R p and b is a translation does not change the value of F, so there is no loss of generality in assuming that N k=1 y k = 0 and that N k=1 y k y T k = D 0 , a diagonal matrix. However, if one lets y ′ k = Dy k where D is diagonal, then</p><formula xml:id="formula_2012">F(y) = p i=1 D 2 ii N k=1         y (i) k - l∈N k ρ (l) k y (i) l         2 .</formula><p>This shows that one should not allow the diagonal coefficients of D 0 to be chosen freely, since otherwise the optimal solution would require to take this coefficient to 0. So D 0 should be a fixed matrix, and by symmetry, it is natural to take D 0 = Id R p . (Any other solution-for a different D 0 -can then be obtained by rescaling independently the coordinates of y 1 , . . . , y N .)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extend ρ (l)</head><p>k to an N -dimensional vector by taking ρ</p><formula xml:id="formula_2013">(k) k = -1 and ρ (l) k = 0 if l k and l N k . We can write F(y) = N k=1 N l=1 ρ (l) k y l 2 .</formula><p>Expanding the square, this is</p><formula xml:id="formula_2014">F(y) = N l,l ′ =1 w ll ′ y T l y l ′ with w ll ′ = N k=1 ρ (l) k ρ (l ′ )</formula><p>k . Introducing the matrix W with entries w kl and the</p><formula xml:id="formula_2015">N × p matrix Y =           y T 1 . . . y T N          </formula><p>, we have the simple expression</p><formula xml:id="formula_2016">F(y) = trace(Y T W Y ) .</formula><p>Note that the constraints are Y T Y = Id R p and Y T 1 N = 0. Without this last constraint, we know that an optimal solution is provided by Y = [e 1 , . . . , e p ] where e 1 , . . . , e p provide an orthonormal family of eigenvectors associated to the p smallest eigenvalues of W (this is a consequence of corollary 2.4). To handle the additional constraint, it suffices to note that W 1 N = 0, so that 1 N is a zero eigenvector. Given this, it suffices to compute p + 1 eigenvectors associated to smallest eigenvalues of W , e 1 , . . . , e p+1 , with the condition that e 1 = ±1 N / √ N (which is automatically satisfied unless 0 is a multiple eigenvalue of W ) and let Y = [e 2 , . . . , e p+1 ].</p><p>Note that e 2 , . . . , e p+1 are also the p smallest eigenvectors of W + λ11 T for any large enough λ, e.g., λ &gt; trace(W )/N . LLE is summarized in the following algorithm.  (3) For k = 1, . . . , N , let S k be the sub-matrix of S matrix associated with x l , l ∈ N k and compute coefficients ρ  Remark 21.1 We note that, for both Isomap and LLE, the c-nearest-neighbors graph can be replaced by the graph formed with edges between all pairs of points that are at distance less than ϵ from each other, for a chosen ϵ &gt; 0, with no change in the algorithms.</p><p>These parameters (c or ϵ) must be chosen carefully and may have an important impact on the output of the algorithm. Choosing them too small would not allow for a correct estimation of distances in Isomap (with possibly some of them being infinite if the graph has more than one connected component), or of the linear approximations in LLE. However, choosing them too large may break the basic hypothesis that the data is locally Euclidean or linear that form the basic principles of these algorithms. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.3">Graph Embedding</head><p>Both Isomap and LLE are based on the construction of a nearest-neighbor graph based on dissimilarity data and the conservation of some of its geometric features when deriving a small-dimensional representation. For LLE, a weight matrix W was first estimated based on optimal linear approximations of x k by its neighbors, and the representation was computed by estimating the eigenvectors associated with the smallest eigenvalues of W (excluding the eigenvector proportional to 1). However, both methods were motivated by the intuition that the dataset was supported by a continuous small-dimensional manifold. We now discuss methods that are solely motivated by the discrete geometry of a graph, for which we use tools that are similar to our discussion of graph clustering in section 19.5.</p><p>Adapting the notation in that section to the present one, we start with a graph with N vertices and weights β kl between these vertices (such that β ll = 0) and we form the Laplacian operator defined by, for any vector u ∈ R N :</p><formula xml:id="formula_2017">1 2 ∥u∥ H 1 = 1 2 N l,l ′ =1 β ll ′ (u (l) -u (l ′ ) ) 2 = u T Lu,</formula><p>so that L is identified as the matrix with coefficients ℓ ll ′ = -β ll ′ for l l ′ and ℓ ll = N l ′ =1 β ll ′ . The matrix W that was obtained for LLE coincides with this graph Laplacian if one lets β ll ′ = -w ll ′ for l l ′ , since we have N l ′ =1 w ll ′ = 0. The usual requirement that weights are non-negative is no real loss of generality, because in LLE (and in the Graph embedding method above), one is only interested in eigenvectors of W (or L below) that are perpendicular to 1, and those remain the same if one replaces W by W -a1 N 1 T N + N aId R N which has negative off-diagonal coefficients wll ′ = w ll ′a for large enough a.</p><p>In graph (or Laplacian) embedding, the starting point is a weighted graph on {1, . . . , N } with edge weights β ll ′ interpreted as similarities between vertexes. These weights may or may not be deduced from measures of dissimilarity (d ll ′ , k, l = 1, . . . , N ) which themselves may or may not be computed as distances between training data x 1 , . . . , x N . If one starts with dissimilarities, it is typical to use simple transformations to compute edge weights, and one the most commonly used is</p><formula xml:id="formula_2018">β ll ′ = exp(-d 2 ll ′ /2τ 2 )</formula><p>for some constant τ. These weights are usually truncated, replacing small values by zeros (or the computation is restricted to nearest neighbors), to ensure that the resulting graph is sparse, which speeds up the computation of eigenvectors for large datasets.</p><p>Given a target dimension p, the graph is then represented as a collection of points y 1 , . . . , y N ∈ R p , where y k is associated to vertex k. For this purpose, one needs to compute the first p + 1 eigenvectors, e 1 , . . . , e p+1 , of the graph Laplacian, with the requirement that e 1 = ±1 N / √ N . (This is always possible and can be achieved numerically by computing eigenvectors of L+c11 T for large enough c.) The graph representation is then given by y</p><formula xml:id="formula_2019">(|) k i = e (k)</formula><p>i+1 for i = 1, . . . , p and k = 1, . . . , N . Note that these are exactly the same operations as those described in steps 4 and 5 of the LLE algorithm.</p><p>One way to interpret this construction is that e 2 , . . . , e p+1 (the coordinate functions for the representation y 1 , . . . , y N ) minimize</p><formula xml:id="formula_2020">p j=1 ∥e i ∥ 2 H 1</formula><p>subject to e 2 , . . . , e p+1 being perpendicular to each other and perpendicular to the constant functions (these constraints being justified for the same reasons as those discussed for LLE). Small H 1 semi-norms being associated with smoothness on the graph, we see that we are looking for the smoothest zero-mean representation of the data.</p><p>Based on our discussion of LLE, we can make an alternative interpretation by introducing a symmetric square root R of the Laplacian matrix L or any matrix such that RR T  An alternate requirement that could have been made for LLE is that N l=1 (ρ (l) k ) 2 = 1 for all k. Instead of having to solve a linear system in step 2 of Algorithm 21.2, one would then compute an eigenvector with smallest eigenvalue of S k . For graph embedding, this constraint can be enforced by modifying the Laplacian matrix, since N l=1 (ρ (l) k ) 2 is just the (k, k) coefficient of RR T . Given this, let D be the diagonal matrix formed by the diagonal elements of L, and define the so-called "symmetric Laplacian" L = D -1/2 LD -1/2 . One obtain an alternative, and popular, graph embedding method by replacing e 1 , . . . , e p+1 above by the first p eigenvectors of L.</p><formula xml:id="formula_2021">= L. Writing R = [ρ 1 , . . . , ρ N ], one has L = N k=1 ρ k ρ T</formula><p>If one introduces the eigenvectors ē1 , . . . , ēN of the normalized Laplacian, still associated with non-decreasing eigenvalues λ1 = 0, . . . , λN , and arranges without loss of generality that ē1 ∝ D 1/2 1 N , then</p><formula xml:id="formula_2022">P s = D -1/2        N i=1 (1 -λi ) s ēi ēT i        D 1/2 .</formula><p>This shows that, for s large enough, the transitions of this Markov chain are well approximated by its first terms, suggesting using the alternative representation based on the normalized Laplacian: ȳk (i) = ēi+1 (k).</p><p>Both representations (using normalized or un-normalized Laplacians) are commonly used in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.4">Stochastic neighbor embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General algorithm</head><p>Stochastic neighbor embedding (SNE, Hinton and Roweis <ref type="bibr" target="#b108">[90]</ref>), and its variant (t-SNE, Maaten and Hinton <ref type="bibr" target="#b140">[122]</ref>) have become a popular tool for the visualization of high-dimensional data based on dissimilarity matrices. One of the key contributions of this algorithm is to introduce a local data rescaling step, that allows for visualization of more homogeneous point clouds.</p><p>Assume that dissimilarities D = (d kl , k, l = 1, . . . , N ) are observed. The basic principle in SNE is to deduce from the dissimilarities a family of N probability distributions on {1, . . . , N }, that we will denote π k , k = 1, . . . , N , with the property that π k (k) = 0. The computation of these probabilities include the local normalization step, and we will return to this later. Given the π k 's, one then estimate low-dimensional representations y = (y 1 , . . . , y N ) such that π k ≃ ψ k where ψ k is given by</p><formula xml:id="formula_2023">ψ k (l; y) = exp -β |y k -y l | 2 N l ′ =1,l ′ k exp -β |y k -y l ′ | 2 1 l k .</formula><p>Here, β : [0, +∞) → [0, +∞) is an increasing differentiable function that tends to +∞ at infinity. The derivative is denoted ∂β. The original version of SNE <ref type="bibr" target="#b108">[90]</ref> uses β(t) = t and t-SNE <ref type="bibr" target="#b140">[122]</ref> takes β(t) = log(1 + t).</p><p>The determination of the representation can then be performed by minimizing a measure of discrepancy between the probabilities π k and ψ k . In Hinton and Roweis <ref type="bibr" target="#b108">[90]</ref>, it is suggested to minimize the sum of Kullback-Liebler divergences, namely</p><formula xml:id="formula_2024">N k=1 KL(π k ∥ψ k (•; y))</formula><p>or, equivalently, to maximize</p><formula xml:id="formula_2025">F(y) = N k,l=1 π k (l) log ψ k (l; y) = - N k,l=1 β(|y k -y l | 2 )π k (l) + N k=1 log         N l=1,l k exp(-β(|y k -y l | 2 ))        </formula><p>The gradient of this function can be computed by evaluating the derivative at ϵ = 0 of f : ϵ → F(y + ϵh). This computation gives</p><formula xml:id="formula_2026">f ′ (0) = -2 N k,l=1 ∂β(|y k -y l | 2 )(y k -y l ) T (h k -h l )π k (l) + 2 N k=1 N l=1 ∂β(|y k -y l | 2 )(y k -y l ) T (h k -h l )ψ k (l; y) = -2 N k=1 h T k N l=1 ∂β(|y k -y l | 2 )(y k -y l )(π k (l) + π l (k) -ψ k (l; y) -ψ l (k; y)) This shows that ∂ y k F(y) = -2 N l=1 β(|y k -y l | 2 )(y k -y l )(π k (l) + π l (k) -ψ k (l; y) -ψ l (k; y)).</formula><p>This is a rather simple expression that can be used with any first-order optimization algorithm to maximize F. The algorithm in Hinton and Roweis <ref type="bibr" target="#b108">[90]</ref> uses gradient ascent with momentum, namely iterating</p><formula xml:id="formula_2027">y (n+1) = y (n) + γ∇F(y (n) ) + α (n) (y (n) -y (n-1) )</formula><p>Choosing α (n) = 0 provides standard gradient ascent with fixed gain γ (of course, other optimization methods may be used). The momentum can be interpreted, in a loose sense, as a "friction term".</p><p>A variant of the algorithm replaces the node-dependent probabilities π k by a single, symmetric, joint distribution π on {1, . . . , N } 2 , (k, l) → π(k, l), satisfying π(k, k) = 0 and π(k, l) = π(l, k). The target distribution ψ then becomes</p><formula xml:id="formula_2028">ψ(k, l; y) = exp(-β(|y k -y l | 2 )) N k ′ l ′ =1 exp(-β(|y k ′ -y l ′ | 2 ))</formula><p>.</p><p>With such a choice, the objective function has a simpler form, namely minimizing KL( π∥ ψ(•, y)) or maximizing the expected likelihood</p><formula xml:id="formula_2029">F(y) = N k,l=1 π(k, l) log ψ(k, l; y) = - N k,l=1 β(|y k -y l | 2 ) π(k, l) + log        N k l=1 exp(-β(|y k -y l | 2 ))        .</formula><p>The gradient of this symmetric version of F can be computed similarly to the previous one and is given by</p><formula xml:id="formula_2030">∂ y k F(y) = -4 N l=1 ∂β(|y k -y l | 2 )(y k -y l )( π( k, l) -ψ(k, l; y)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting initial probabilities</head><p>The probabilities π k (l) or π(k, l) are deduced from the dissimilarities as</p><formula xml:id="formula_2031">π k (l) = e -d 2 kl /2σ 2 k N l ′ =1,l ′ k e -d 2 kl ′ /2σ 2 k for l k and π(k, l) = π k (l) + π l (k) 2n .</formula><p>The coefficients σ 2 k , k = 1, . . . , N operate the local normalization, justifying, in particular, the parameter-free expression chosen for ψ and ψ. These coefficients are estimated so as to adjust the entropies of all π k to a fixed value, which is a parameter of the algorithm. Note that, letting t Using Schwartz inequality, we see that ∂ t H(π k ) ≤ 0 so that H(π k ) is decreasing as a function of t, i.e., increasing as a function of σ 2 k . When σ 2 k → 0, π k converges to the uniform distribution on the set of nearest neighbors of k (the indexes l k such that d 2 kl is minimal) and, letting ν k denote their number, which is typically equal to 1, H(π k ) converges to log ν k . When σ 2 k tends to infinity, π k converges to the uniform distribution over indexes l k, whose entropy is log(N -1). This shows that e H(π k ) , which is called the perplexity of π k can take any value between ν k and N -1. The common target value of the perplexity can therefore be taken anywhere between max k ν k and N -1. In Maaten and Hinton <ref type="bibr" target="#b140">[122]</ref>, it is recommended to choose a value between 5 and 50.</p><formula xml:id="formula_2032">= 1/2σ 2 k and H(π k ) = -N l=1 π k (l) log π k (l), ∂ t H(π k ) = - N l=1 ∂ t π k (l) log π k (l) - N l=1 ∂ t π k (l) = - N l=1 ∂ t π k (l) log π k (l) Now ∂ t log π k (l) = -d 2 kl + d2 k with d2 k = N l ′ =1 d 2 kl ′ π k (l ′ ). Writing</formula><p>Remark 21.2 The complexity of the computation of the gradient of the objective function (either F or F ) scales like the square of the size of the training set, which may be prohibitive when N is large. In Van Der Maaten <ref type="bibr" target="#b211">[193]</ref>, an accelerated procedure, that involves an approximation of the gradient is proposed. (This procedure is however limited to representations in dimensions 2 or 3.) ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2.5">Uniform manifold approximation and projection (UMAP)</head><p>UMAP is similar in spirit to t-SNE, with a few important differences that result in a simpler optimization problem and faster algorithms. Like Isomap, the approach is based on matching distances between the high-dimensional data and the lowdimensional representation. But while Isomap estimates a unique distance on the whole training set (the geodesic distance on the nearest-neighbor graph), UMAP estimates as many "local distances" as observations before "patching" them to form the final representation.</p><p>The goal of transporting possibly non-homogeneous locally defined objects on initial data to a homogeneous low-dimensional visualization is what makes UMAP similar to t-SNE. The difference is that t-SNE transports local probability distributions, while UMAP transports metric spaces. More precisely, given distances (d kl , k, l = 1, . . . , N ) and an integer m provided as input, the algorithm builds, for each k = 1, . . . , N a (pseudo-)metric δ k on the associated data graph by letting</p><formula xml:id="formula_2033">δ (k) (k, l) = δ (k) (l, k) = 1 σ k d kl -min l ′ k d kl ′</formula><p>if l is among the m nearest neighbors of k, where m is a parameter of the algorithm, with all other distances being infinite. The normalization parameter σ k has a role similar to that of the same parameter in t-SNE in that it tends to make the representation homogeneous. Here, it is computed such that l exp(-δ (k) (l, l ′ )) = log 2 m .</p><p>Each such metric provides a weighted graph structure on {1, . . . , N } by defining weights w (k) ll ′ = exp(-δ (k) (l, l ′ )). In UMAP, these weights are interpreted in the framework of fuzzy sets, where a fuzzy set is defined by a pair (A, µ) where A is a set and µ a function µ : A → [0, 1] <ref type="bibr" target="#b228">[210]</ref>. The function µ is called the membership function and µ(x) for x ∈ A is the membership strength of x to A. Letting V = {1, . . . , N } and E = V × V , one then interprets the weights as defining the membership strength of edges to the graph, i.e., one defines the "fuzzy graph" G (k) = (V , E, µ (k) ) where µ (k) (l, l ′ ) = w (k) ll ′ is the membership strength of edge (l, l ′ ) to G (k) . This is, of course, just a reinterpretation of weighted graphs in terms of fuzzy sets, but it allows one to combine the collection (G (k) , k = 1, . . . , N ) using simple fuzzy sets operations, namely, defining the combined (fuzzy) graph G = (V , E, µ) with</p><formula xml:id="formula_2034">(E, µ) = N k=1 (E, µ (k) )</formula><p>being the fuzzy union of the edge sets. There are, in fuzzy logic, multiple ways to define set unions <ref type="bibr" target="#b103">[85]</ref>, and the one selected for UMAP define (A, µ) ∪ (A ′ , µ ′ ) = (A∪A ′ , ν) with ν(x) = µ(x)+µ ′ (x)-µ(x)µ ′ (x) (µ(x) and µ ′ (x) being defined as 0 is x A or x A ′ respectively). In UMAP, each edge µ (k) (l, l ′ ) is non-zero only is k = l or l ′ so that µ(l, l ′ ) = w (l)</p><formula xml:id="formula_2035">ll ′ + w (l ′ ) ll ′ -w (l) ll ′ w (l ′ ) ll ′ .</formula><p>This defines an input fuzzy graph structure on {1, . . . , N } that serves as target for an optimized similar structured associated with the representation y = (y 1 , . . . , y N ). This representation, since it is designed as a homogeneous representation of the data, provides a unique fuzzy graph H(y) = (V , E, ν(•; y)) and the edge membership function is defined by ν(l, l ′ ; y) = ϕ a,b (y l , y l ′ ) with where ρ 0 is an input parameter of the algorithm. This function ψ ρ 0 takes the same form as the membership function defined for local graphs G (k) , and its replacement by ϕ a,b makes possible the use of gradient-based methods for the determination of the optimal y (ψ ρ 0 is not differentiable everywhere). Note the important simplification compared to the similar function F is t-SNE, in that the logarithm of a potentially large sum is avoided. We have The optimization can be implemented using stochastic gradient ascent. Introduce random variables ξ kl and ξ ′ kl both taking value in {0, 1}, all independent of each other and such that P (ξ kl = 1) = µ kl and P (ξ ′ kl = 1) = ϵ. Define This corresponds to SGA iterations in which:</p><p>(1) Each edge (k, l) is selected with probability µ(k, l) (which are zero for unless k and l are neighbors);</p><p>(2) If (k, l) is selected, one selects an additional edges (k, l ′ ) each with probability ϵ.</p><p>Letting l 1 , . . . , l m be the number of edges selected, y k is updated according to 3 If one prefers using probability rather than fuzzy set theory, the graphs G (k) may also be interpreted as random graphs in which edges are added independently from each other and each edge (l, l ′ ) is drawn with probability µ (k) (l, l ′ ). The combined graph G is then the random graph in which (l, l ′ ) is present if and only if it is in at least one of the G (k) and the objective function C coincides with the KL divergence between this random graph and the random graph similarly defined for y.</p><formula xml:id="formula_2036">y k ← y k + 2γ         ∂</formula><p>However, this fuzzy/random graph formulation of UMAP-which corresponds to current practical implementations-is only a special case of the theoretical construction made in McInnes et al. <ref type="bibr" target="#b148">[130]</ref> which builds on the theory of (fuzzy) simplicial sets and their representation of metric spaces. We refer the interested reader to this reference, which requires a mathematical background beyond the scope of these notes. ♦</p><p>Chapter 22</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalization Bounds</head><p>We provide, in this chapter, an introduction to some theoretical aspects of statistical (or machine) learning, mostly focusing on the derivation of "generalization bounds" that provide high-probability guarantees on the generalization error of predictors using training data. While these bounds are not always of practical use, because making them small in realistic situations would require an enormous amount of training data, their derivations and the form they take for specific model classes bring important insight on the structure of the learning problem, and help understand why some methods may perform well while others do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.1">Notation</head><p>We here recall some notation introduced in chapter 5. We consider a pair of random variables (X, Y ), with X : Ω → R X and Y : Ω → G. Regression problems correspond to R Y = R (or R q if multivariate) and classification to R Y being a finite set. A predictor is a function f : R X → R Y . The general prediction problem is to find such a predictor within a class of functions, denoted F , minimizing the prediction (or generalization error) A good learning algorithm should be such that the generalization error R( fT ) is small, at least in average (i.e., E(R( fT )) is small). Our main goal in this chapter is to describe generalization bounds trying to find upper-bounds for R( fT ) based on E T and properties of the function class F . These bounds will reflect the bias-variance trade-off, in that, even though large function classes provide smaller in-sample errors, they will also induce a large additive term in the upper-bound, accounting for the "variance" associated to the class.</p><formula xml:id="formula_2037">R(f ) = E(</formula><p>Remark 22.1 Both variables X and Y are assumed to be random in the previous setting, but there are often situations when one of them is "more random" than the other. Randomness in Y is associated to measurement errors, or ambiguity in the decision. Randomness in X more generally relates to the issue of sampling a dataset in a large dimensional space. In some cases, Y is not random at all: for example, in object recognition, the question of assigning categories for images such as those depicted in fig. <ref type="figure" target="#fig_136">22</ref>.1 has a quasi-deterministic answer. Sometimes, it is X who is not random, for example when observing noisy signals where X is a deterministic discretization of a time interval and Y is some function of X perturbed by noise. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2">Penalty-based Methods and Minimum Description Length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.1">Akaike's information criterion</head><p>We make a computation under the following assumptions. We assume a regression model Y = f θ (X) + ϵ where ϵ ∼ N (0, σ 2 ) and f is some function parametrized by θ ∈ R m . We also assume that the true distribution is actually covered by this model and represented by a parameter θ 0 . Let θT denote the parameter estimated by least squares using a training set T , and denote for short fT = f θT .</p><p>The in-sample error is</p><formula xml:id="formula_2038">E T = 1 N N k=1</formula><p>(y k -fT (x k )) 2 .</p><p>with</p><formula xml:id="formula_2039">I = 1 2σ 2 E(∂ 2 θ (Y -f θ (X)) 2 | θ-θ 0 ).</formula><p>As a consequence, we can write (taking expectations in both Taylor expansions) ∆ N = σ 2 E ( θTθ 0 ) T J T ( θTθ 0 ) + σ 2 E ( θTθ 0 ) T I( θTθ 0 ) + o(E(| θTθ 0 | 2 )).</p><p>(We skip hypotheses and justification for the analysis of the residual term.)</p><p>We now note that, because we are assuming a Gaussian noise, and that the true data distribution belongs to the parametrized family, the least-square estimator is also a maximum likelihood estimator. Indeed, the likelihood of the data is</p><formula xml:id="formula_2040">1 (2πσ 2 ) N /2 exp        - 1 2σ 2 N k=1 (Y k -f θ (X k )) 2        N k=1 ϕ X (X k )</formula><p>where ϕ X is the p.d.f. of X and does not depend on the unknown parameter.</p><p>We can therefore apply classical results from mathematical statistics <ref type="bibr" target="#b212">[194]</ref>. Under some mild smoothness assumptions on the mapping θ → f θ , θT converges to θ 0 in probability when N tends to infinity, the matrix J T converges to I, which is the model's Fisher information matrix, and √ N ( θTθ 0 ) converges in distribution to a Gaussian N (0, I -1 ) . This implies that both N ( θTθ 0 ) T J T ( θTθ 0 ) and N ( θTθ 0 ) T I( θTθ 0 ) converge to a chi-square distribution with m degrees of freedom, whose expectation is m, which indicates that ∆ N has order 2σ 2 m/N . This analysis can be used to develop model selection rules, in which one chooses between models of dimensions k 1 &lt; k 2 &lt; • • • &lt; k q = m (e.g., by truncating the last coordinates of X). The rule suggested by the previous computation is to select j minimizing E (j)</p><formula xml:id="formula_2041">T ( fT ) + 2σ 2 k j N ,</formula><p>where E (j) is the in-sample error computed using the k j -dimensional model. This is an example of a penalty-based method, using the so-called Akaike's information criterion (AIC) <ref type="bibr" target="#b20">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.2">Bayesian information criterion and minimum description length</head><p>Other penalty-based methods are more size-averse and replace the constant, 2, in AIC by a function of N = |T |, for example log N . Such a change can be justified by a Bayesian analysis, yielding the Bayesian information criterion (BIC) <ref type="bibr" target="#b192">[174]</ref>. The approach in this case is not based on an evaluation of the error, but on an asymptotic estimation of the posterior distribution resulting from a Bayesian model selection principle. Like in the previous section, we content ourselves with a heuristic discussion.</p><p>Let us consider a statistical model parametrized by θ ∈ Θ, where Θ is an open convex subset of R m with p.d.f. given by f (z; θ) = exp(θ T U (z) -C(θ)) , with U : R d → R m and z = (x, y). We are given a family of sub-models represented by M 1 , . . . , M q , where, for each j, M j is the intersection of Θ with a k j -dimensional affine subspace of R m . We are also given a prior distribution for θ in which a submodel is first chosen, with probabilities α 1 , . . . , α q , and given that, say, M j is selected, θ ∈ M j is chosen with a probability distribution with density ϕ j with respect to Lebesgue's measure on M j (denoted dm j ). Given training data T = (z 1 , . . . , z N ), Bayesian model selection consists in choosing the model M j where j maximizes the posterior log-likelihood Note that the first derivative of ℓ is ∂ θ ℓ = Ū -E θ (U ) where E θ is the expectation for f (•, θ). The second derivative is -var θ (U ) (showing that ℓ is concave) and the third derivative involves third-order moments of U for E θ and (like the second derivative) does not depend on ŪT . In particular, we can assume that, for any M &gt; 0, there exists a constant C M such that whenever max(|θ|, | θj |) ≤ M, we have R j (θ, θj ) ≤ C M .</p><formula xml:id="formula_2042">µ(M j |T ) = log</formula><p>The law of large numbers implies that ŪT converges to a limit when N tends to infinity, and our assumptions imply that θj converges to the parameter providing the best approximation of the distribution of Z for the Kullback-Leibler divergence. In particular, with probability 1, there exists an N such that θj belongs to any large enough, but fixed, compact set. Moreover, the second derivative ℓ( θj ) will also converge to a limit, -Σ j .</p><p>For any ϵ &gt; 0, write R m α j e N (θ T ŪT -C(θ)) ϕ j dm j (θ) = |θ-θj |≤ϵ e N (θ T ŪT -C(θ)) ϕ j dm j (θ) + |θ-θj |≥ϵ e N (θ T ŪT -C(θ)) ϕ j dm j (θ) .</p><p>The second integral converges to 0 exponentially fast when N tends to ∞. The first one behaves essentially like</p><formula xml:id="formula_2043">M j e -1</formula><p>2 N (θ-θj ) T Σ -1 j (θ-θj )+log ϕ j (θ) dm j (θ) .</p><p>Neglecting log ϕ j (θ), this integral behaves like (2π det(Σ j /N )) -1/2 , whose logarithm is (-k j (log N )/2) plus constant terms. As a consequence, we find that µ(M j | T ) = max θ∈M j ℓ(θ) -k j 2 log N + bounded terms. Consider, as an example, linear regression with Y = β 0 + b T x + σ 2 ν where ν is a standard Gaussian random variable. Assume that the distribution of X is known, or, preferably, make the previous discussion conditional to X 1 , . . . , X N . Let sub-models M j correspond to the assumption that all but the first k j -1 coefficients of b vanish. Then, up to bounded terms, the Bayesian estimator must minimize (over such parameters b)</p><formula xml:id="formula_2044">1 2σ 2 N k=1 (y k -β 0 -b T x k ) 2 + k j 2 log N .</formula><p>or</p><formula xml:id="formula_2045">E (j) T + k j σ 2 N log N .</formula><p>We now turn to another interesting point of view, which provides the same penalty, based on maximum description length principle (MDL; Rissanen <ref type="bibr" target="#b180">[162]</ref>) measuring the coding efficiency of a model. Let us fix some notation. We assume that one has q competing models for predicting Y from X, for example, linear regression models based on different subsets of the explanatory variables. Denote these models M 1 , . . . , M q . Each model will be seen, not as an assumption on the true joint distribution of X and Y , but rather as a tool to efficiently encode the training set ((x 1 , y 1 ), . . . , (x N , y N )). To describe MDL,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.2.">PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH579</head><p>which selects the model that provides the most efficient code, we need to reintroduce a few basic concepts of information theory.</p><p>The entropy of a discrete probability P over a set Ω is (The logarithm in base 2 is used because of the tradition of coding with bits in information theory.) For a discrete random variable X, the entropy H 2 (X) is H 2 (P X ) where P X is the probability distribution of X. The relation between the entropy and coding theory is as follows: a code is a function which associates to any element ω ∈ Ω a string of bits c(ω). The associated code-length is denoted l c (ω), which is simply the number of bits in c(ω). When P is a probability on Ω, the efficiency of a code is measured by the average code-length: E P (l c ) = ω∈Ω l c (ω)P (ω).</p><p>Shannon's theorem <ref type="bibr" target="#b193">[175,</ref><ref type="bibr" target="#b73">55]</ref> states that, under some conditions on the code (ensuring that any sequence of words can be recognized as soon as it is observed: one says that it is instantaneously decodable) the average code length can never be larger than the entropy of P . Moreover, it states that there exists codes that achieve this lower bound with no more than one bit loss, such that for all ω, l c (ω) ≤ log 2 (P (ω))+1. These optimal codes, such as the Huffman code <ref type="bibr" target="#b73">[55]</ref>, can completely be determined from the knowledge of P . This allows one to interpret a probability P on Ω as a tool for designing codes with code-lengths essentially equal to (-log 2 P ). This statement can be generalized to continuous random variables (replacing the discrete probability P by a probability density function, say ϕ) if one introduces a coding precision level, denoted δ 0 , meaning that the decoded values may differ by no more than δ 0 from the encoded ones. The result is that the optimal code-length at precision δ 0 can be estimated (up to one extra bit) bylog 2 ϕlog 2 δ 0 .</p><p>In our context, each model of the conditional distribution of Y given X, with conditional density ϕ(y|x), provides a way to encode the training set with a total code length, for (y 1 , . . . , y N ), of -N k=1 log 2 ϕ(y k | x k ) -N log 2 δ 0 (working, as before, conditionally to x 1 , . . . , x N ). We assume that the precision at which the data is encoded is fixed, which implies that the last term does not affect the model choice. Now, assume a sequence of m parametrized model classes, M 1 , . . . , M m and let ϕ(y | x, θ, M j ) denote the conditional distribution with parameter θ in the class M j . Within model M j , the optimal code length corresponds to the maximum likelihood:</p><p>- If the models are nested, which is often the case, the most efficient will always be the largest model, since the maximization is on a larger set. However, the minimum description length (MDL) principle uses the fact that, in order to decode the compressed data, the model, including its optimal parameters, has to be known, so that the complete code needs to include a model description. The decoding algorithm will then be: decode the model, then use it to decode the data.</p><p>So assume that a model (one of the M j 's) has a k j -dimensional parameter θ. Also assume that a probability distribution, π(θ | M j ), is used to encode θ. Also choose a precision level, δ ij , for each coordinate in θ, i = 1, . . . , k j . (Previously, we could consider the precision of the y k , δ 0 , as fixed, but now, the precision level for parameters is a variable that will be optimized.) The total description length using this model now becomes If π is interpreted as a prior distribution of the parameters, θ(j) is the maximum a posteriori Bayes estimator. We now take the correction caused by (δ ij , i = 1, . . . , k j ) into account, by assuming that the ith coordinate in θ(j) is truncated tolog 2 δ i bits.</p><p>Let θ (j) denote this approximation. A second-order expansion of L(θ|M j ) around θ(j) yields (assuming sufficient differentiability)</p><p>L(θ (j) | M j ) = L( θ(j) | M j ) + 1 2 (θ (j) -θ(j) ) T S θ(j) (θ (j) -θ(j) ) + o(|θ</p><formula xml:id="formula_2046">(j) -θ(j) | 2 )</formula><p>where S θ is the matrix of second derivatives of L(• | M j ) at θ. Approximating θ (j) -θ(j) by δ (j) (the k j -dimensional vector with coordinates δ ij , i = 1, . . . , k j ), we see that the precision should maximize 1 2 (δ (j) ) T S θ(j) δ (j) + k j i=1 log 2 δ ij .</p><p>Note that S θ(j) must be negative semi-definite, since θ is a local maximum. Assuming it is non-singular, the previous expression can be maximized and yields S θ(j) δ (j) = -1 log 2 1 δ (j) <ref type="bibr">(22.1)</ref> where 1/δ (j) is the vector with coordinates (1/δ ij ).</p><p>Let us now make an asymptotic evaluation. Because L(θ | M j ) includes a sum over N independent terms, it is reasonable to assume that S θ(j) has order N , and more precisely, that S θ(j) /N has a limit. Rewrite <ref type="bibr">(22.1)</ref> as (j)  .</p><formula xml:id="formula_2047">S θ(j) N √ N δ (j) = - 1 log 2 1 √ N δ</formula><p>This implies that √ N δ (j) is the solution of an equation which stabilizes with N , and it is therefore reasonable to assume that the optimal δ ij takes the form δ ij = c i (N | M j )/ √ N , with c i (N | M j ) converging to some limit when N tends to infinity. The total cost can therefore be estimated by The last two terms are O(1), and can be neglected, at least when N is large compared to k j . The final criterion becomes the penalized likelihood</p><formula xml:id="formula_2048">l d (θ | M j ) = L(θ|M j ) - k j 2 log 2 N</formula><p>in which we see that the dimension of the model appears with a factor log 2 N as announced (one needs to normalize both terms by N to compare with the previous paragraph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3">Concentration inequalities</head><p>The discussion of the AIC was a first attempt at evaluating a prediction error. It was however done under very specific parametric assumptions, including the fact that the true distribution of the data was within the considered model class. It was, in addition, a bias evaluation, i.e., we estimated how much, in average, the in-sample error was less than the generalization error. We would like to obtain upper bounds to the generalization error that hold with high probability, and rely as little as possible on assumptions on the true data distribution.</p><p>One of the main tools used in this context are concentration inequalities, which provide upper bounds on the various probabilities of events involving a large number of random variables. The current section provides a review of some of these inequalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.1">Cramér's theorem</head><p>If X 1 , X 2 , . . . are independent, integrable random variables with identical distributions (to that of a random variable X), the law of large numbers tells us that the empirical mean XN = (X 1 + • • • + X N )/N converges with probability one to m = E(X). When the variables are square integrable, Chebychev's inequality provides an easy proof of the weak law of large numbers. Indeed,</p><formula xml:id="formula_2049">P | Xn -m| &gt; ϵ ≤ 1 ϵ 2 E ( XN -m) 2 1 | Xn -m|&gt;ϵ ≤ var( XN ) ϵ 2 = var(X) N ϵ 2 .</formula><p>A stronger assumption on the moments of X yields a stronger inequality. One says that X has exponential moments if there exists λ 0 &gt; 0 such that E(e λ 0 |X| ) &lt; ∞. In this case, the cumulant-generating function, defined, for λ ∈ R, by M X (λ) = log E(e λX ) ∈ [0, +∞], <ref type="bibr">(22.2)</ref> is finite for λ ∈ [-λ 0 , λ 0 ].</p><p>Here are a few straightforward properties of the cumulant-generating function.</p><p>(i) One has M X (0) = 0.</p><p>(ii) For any a ∈ R, one has M aX (λ) = M X (aλ).</p><p>(iii) If X 1 and X 2 are independent variables, one also has</p><formula xml:id="formula_2050">M X 1 +X 2 (λ) = M X 1 (λ) + M X 2 (λ).</formula><p>In particular, M X+a (λ) = M X (λ) + λa, so that M X-E(X) (λ) = M X (λ) -λE(X).</p><p>(iv) Finally, Markov's inequality (which states that, for any non-negative variable Y , P (Y &gt; t) ≤ E(Y )/t) applied to Y = e λX for λ &gt; 0 yields P(X &gt; t) = P(e λX &gt; e λt ) ≤ e M X (λ)-λt . <ref type="bibr">(22.3)</ref> (Note that this inequality is trivially true for λ = 0.) the probability of a deviation by t at least of Xn from its mean decays exponentially fast. The derivation of the inequality above was quite easy: apply Markov's inequality in a parametrized form and optimize over the parameter. It is therefore surprising that this inequality is sharp, in the sense that a similar lower bound also holds. Even though we are not going to use it in the rest of this chapter, it is worth sketching the argument leading to this lower bound, which involves an interesting step making a change of measure.</p><p>Assume (without loss of generality) that m = 0 and consider P( Xn &gt; t). Assume, to simplify the discussion, that the supremum of λ → ϵλ -M X (λ) is attained at some λ t . We have</p><formula xml:id="formula_2051">∂ λ M X (λ) = E(Xe λX ) E(e λX )</formula><p>.</p><p>Let q λ (x) = e λx E(e λX ) and P λ (with expectation E λ ) the probability distribution on Ω with density q λ (X) with respect to P, so that ∂ λ M X (λ) = E λ (X). We have, since λ t is a maximizer, E λ t (X) = t. Moreover, fixing δ &gt; 0,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P( XN</head><formula xml:id="formula_2052">&gt; t) = E(1 XN &gt;t ) ≥ E(1 | Xn -t-δ|&lt;δ )</formula><p>≥ E 1 | Xn -t-δ|&lt;δ e N λ XN -N t-2N δ = e -N (t+2δ) M X (λ) N P λ (| XNt -δ| &lt; δ)</p><p>If one takes λ = λ t+δ , this implies that P( XN &gt; t) ≥ e -N M * X (t+δ) e -N δ P λ t+δ (| XN -t -δ| &lt; δ) . By the law of large numbers (applied to P λ t+δ ), P λ t+δ (| XN -t -δ| &lt; δ) tends to 1 when N tends to infinity. This implies that the logarithmic rate of convergence to 0 of P( XN &gt; t) is larger than N (M * X (t + δ) + δ), for any δ &gt; 0, to be compared with the rate N M * X (t) for the upper bound. In Large Deviation theory, the upper and lower bounds are often simplified by considering the limit of log P( XN &gt; t)/N , which, in this case, is M * X (t) (and this result is called Cramér's therorem).</p><p>While Cramér's upper bound is sharp, its computation requires an exact knowledge of the distribution of X, which is not a common situation. The following sections optimize the upper bound in situations where only partial information on the variable is known, such as its moments or its range. As a first example, we consider concentration of the mean for sub-Gaussian variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.2">Sub-Gaussian variables</head><p>If X has exponential moments, then, (applying again Markov's inequality)</p><formula xml:id="formula_2053">P (|X| &gt; x) ≤ Ce -λx</formula><p>for some positive constants C and λ. Reducing if needed the value of λ, one can assume that C takes some predetermined (larger than 1) value, say, C = 2, the simple argument being left to the reader. A random variable such that, for some λ &gt; 0 P(|X| &gt; x) ≤ 2e -λx is called sub-exponential (and this property is equivalent to X having exponential moments). Similarly, one says that X is sub-Gaussian if, some σ &gt; 0, Proposition 22.3 Assume that X is sub-Gaussian, so that (22.5) holds for some σ 2 &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(|X| &gt; x) ≤ 2e</head><p>Then, for any t &gt; 0, we have</p><formula xml:id="formula_2054">P( Xn -E(X) &gt; t) ≤ 1 + 4t 2 σ 2 N e -N t 2 2σ 2 .</formula><p>Proof Let us assume, without loss of generality, that E(X) = 0. For λ &gt; 0, we then have E(e λX ) = 1 + E(e λX -λX -1) .</p><p>Let ϕ(t) = e tt -1. We have ϕ(t) ≥ 0 for all t, ϕ(0) = 0 and, for z &gt; 0, the equation z = ϕ(t) has two solutions, one positive and one negative that we will denote g + (z) &gt; 0 &gt; g -(z). We have where Φ is the cumulative distribution function of the standard Gaussian and we have used Φ(-t) -Φ(t) ≤ 2t/ √ 2π. We therefore have M X (λ) ≤ log 1 + 4λ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E(ϕ(λX)) =</head><p>2 σ 2 e λ 2 σ 2 2 ≤ λ 2 σ 2 2 + log(1 + 4λ 2 σ 2 ) . This implies M * X (t) = sup λ&gt;0 (λt -M X (λ)) ≥ t 2 σ 2 -M X (t/σ 2 ) ≥ t 2 2σ 2log(1 + 4t 2 σ 2 ) so that P( Xn &gt; t) ≤ 1 + 4t 2 σ 2 N e -N t 2 2σ 2 . ■ The following result allows one to control the expectation of a non-negative sub-Gaussian random variable. Proposition 22.4 Let X be a non-negative random variable such that P(X &gt; t) ≤ Ce -t 2 /2σ 2 for some constants C and σ 2 . Then, E(X) ≤ 3σ log C. Proof For any α ∈ (1, C], one has min(1, Ce -t 2 /2σ 2 ) ≤ αe -t 2 log α 2σ 2 log C , which implies that E(X) = +∞ 0 P(X &gt; t)dt ≤ α 2 log α √ 2πσ log C Taking α = √ e gives E(X) ≤ √ πeσ log C ≤ 3σ log C. ■ 22.3.3 Bennett's inequality</p><p>The following proposition (see <ref type="bibr" target="#b42">[24]</ref>) provides an upper bound for M X (λ) as a function of E(X) and var(X) under the additional assumption that X is bounded from above. if X &lt; b and E(X 2 ) ≤ σ 2 . Indeed, if this inequality is true for m = 0 and λ = 1, <ref type="bibr">(22.6)</ref> in the general case will result from letting X = Y /λ + m and applying the special case to Y .</p><p>The right-hand side of (22.7) is exactly E(e X ) when X follows the discrete distribution P 0 supported by two points x 0 and b, and such that E(X) = 0 and E(X 2 ) = σ 2 , which requires x 0 = -σ 2 /b and P (X = x 0 ) = b 2 /(σ 2 + b 2 ). Now consider the quadratic function v(x) = αx 2 + βx + γ which intersects x → e x at x = x 0 and x = b, and is tangent to it at x = x 0 , i.e., v(b) = e b and v(x 0 ) = v ′ (x 0 ) = e x 0 (this uniquely defines v). Then e x ≤ v(x) for x &lt; b, yielding E(e X ) ≤ ασ 2 + γ. However, since v(X) = e X almost surely when X ∼ P 0 , this upper bound is attained and equal to that provided in <ref type="bibr">(22.7)</ref>. Computing the derivative in µ and equating it to 0 gives</p><formula xml:id="formula_2055">µ = 1 1 + ρ log ρ + x ρ(1 -x) ,</formula><p>which is non-negative since ρ + xρ(1x) = (1 + ρ)x. For this value of µ, we have e -µ(ρ+x) + ρe µ (1-x)  ρ + 1 = e -µ(ρ+x) 1 + ρe µ(1+ρ) ρ + 1 = e -µ(ρ+x)</p><p>1 + ρ ρ+x ρ <ref type="bibr">(1-x)</ref> ρ + 1 = e -µ(ρ+x) 1x and log e -µ(ρ+x) + ρe µ (1-x)  ρ + 1 = µ(ρ + x) + log(1x)</p><formula xml:id="formula_2056">= ρ + x 1 + ρ log ρ + x ρ(1 -x) + log(1 -x) = ρ + x 1 + ρ log ρ + x ρ + 1 -x 1 + ρ log(1 -x) .</formula><p>This provides a lower bound for M * X (m + (bm)x), and yields the following corollary. Corollary 22.6 Assume that X satisfy the conditions of proposition 22. Bennett's inequality is sometimes stated in a slightly weaker, but simpler form <ref type="bibr" target="#b145">[127]</ref>. Returning to the proof of proposition 22.5 and using the fact that log u ≤ u -1, equation ( <ref type="formula" target="#formula_2063">22</ref> We will use the following lemma.</p><p>Lemma 22.7 The function ϕ : u → (e uu -1)/u 2 is non-decreasing.</p><p>Proof We have ϕ ′ (u) = ψ(u)/u 3 where ψ(u) = ue u -2e u + u + 2, yielding ψ ′ (u) = ue ue u + 1, ψ ′′ (u) = ue u . Therefore, ψ ′ is has its minimum at u = 0 with ψ ′ (0) = 0 so that ψ is increasing. Since ψ(0) = 0, we have ψ(u)/u 3 ≥ 0. We summarize this in the following corollary. This estimate can be further simplified as follows. Let g be such that g ′′ (u) = (1 + u/3) -3 and g(0) = g ′ (0) = 0, which gives g(u) = u 2 /(2 + 2u/3). Noting that h ′′ (u) = (1 + u) -1 and that (1 + u) -1 ≥ (1 + u/3) -3 , for u ≥ 0 we find, integrating twice, that h(u) ≥ g(u) for u ≥ 0. This shows that the following upper-bound is also true: This upper bound is known as Bernstein's inequality.</p><formula xml:id="formula_2057">P( XN &gt; m + t) ≤ exp - N t 2</formula><p>Remark 22.9 It should be clear that, in the previous discussion, one may relax the assumption that X 1 , . . . , X N are identically distributed as long as there is a common function M such that M X k (λ) ≤ m k + M(λ) for all k, with m k = E(X k ). We have in this case P( XN &gt; mN + t) ≤ exp(-N M * (t))</p><p>with mN = (m 1 + • • • + m N )/N and M * (t) = sup λ (λt -M(λ)). This remark can be, in particular, applied to the situation in which X 1 , . . . , X N satisfy the conditions of proposition 22.5 with the same constants b and σ 2 , yielding the same upper bound as in equation <ref type="bibr">(22.8)</ref>. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.4">Hoeffding's inequality</head><p>We now consider the case in which the random variables X 1 , . . . , X N are bounded from above and from below, and start with the following consequence of proposition 22.5. This shows that, if λ ≥ 0, we can apply proposition 22.5 with σ 2 = (bm)(ma), which provides the first inequality in <ref type="bibr">(22.11)</ref>. To handle the case λ ≤ 0, it suffices to apply this inequality with λ = -λ, X = -X, ã = -b, b = -a and m = -m. Let f (α) denote the difference between the right-hand side and left-hand side. Then f (0) = 0,</p><formula xml:id="formula_2058">f ′ (α) = α 4</formula><p>ue α 1u + ue α + u, (so that f ′ (0) = 0) and</p><formula xml:id="formula_2059">f ′′ (α) = 1 4 - u(1 -u)e α (1 -u + ue α ) 2 .</formula><p>For positive numbers x = 1u and y = ue α , one has (x + y) 2 ≥ 4xy, which shows that f ′′ (α) ≥ 0. This proves that f ′ is non-decreasing with f ′ (0) = 0, proving that f is minimized at α = 0, so that f (α) ≥ 0 as needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We can then deduce the following theorem <ref type="bibr" target="#b110">[92]</ref>. The upper bound is minimized for λ = 4t/|c| 2 , yielding <ref type="bibr">(22.12)</ref>. Equation <ref type="bibr">(22.13</ref>) is obtained by applying (22.12) to -X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>An important special case of this inequality is when X 1 , . . . , X N are i.i.d. taking values in an interval of length δ. Then P( XN &gt; E(X) + t) ≤ exp -2N t 2 δ 2 . <ref type="bibr">(22.14)</ref> This inequality is obtained after applying Hoeffding's inequality to X 1 /N , . . . , X N /N , therefore taking c 1 = • • • = c N = δ/N and |c| 2 = δ 2 /N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.5">McDiarmid's inequality</head><p>One can relax the assumption that the random variables X 1 , . . . , X N are independent and only assume that these variables behave like "martingale increments," as stated in the following proposition <ref type="bibr" target="#b77">[59]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We will use this proposition to prove the "bounded difference," or McDiarmid's inequality.</p><p>Theorem 22.13 (McDiarmid's inequality) Let X 1 , . . . , X N be independent random variables and g : R N → R a function such that there exists c 1 , . . . , c N such that |g(x 1 , . . . , x k-1 , x k , x k+1 , . . . , x N )g(x 1 , . . . , x k-1 , xk , x k+1 , . . . , x N )| ≤ c k <ref type="bibr">(22.15)</ref> for all k = 1, . . . , N and x 1 , . . . , x k-1 , x k , xk , x k+1 , . . . , x N . Then P (g(X 1 , . . . , X N ) &gt; E(g(X 1 , . . . , X N )) + t) ≤ e -2t 2 /|c| 2</p><formula xml:id="formula_2060">with |c| 2 = c 2 1 + • • • + c 2 N .</formula><p>Proof Let m = E(g(X 1 , . . . , X N )). Let Z 0 = 0,</p><formula xml:id="formula_2061">Y k = E(g(X 1 , . . . , X N ) | X 1 , . . . , X k ) -m and Z k = Y k -Y k-1 .</formula><p>Note that Z k is a function of X 1 , . . . , X k and can therefore be omitted from the conditional expectation given (X 1 , Z 1 , . . . , X k-1 , Z k-1 ).</p><p>We have E(Y k ) = 0 and E(Y k | X 1 , . . . , X k-1 ) = Y k-1 so that E(Z k | X 1 , . . . , X k-1 ) = 0.</p><p>Because the variables are independent, we have, letting X1 , . . . , XN be independent copies of X 1 , . . . , X N , Z k = E(g(X 1 , . . . , X k-1 , X k , Xk+1 , . . . , XN ) | X 1 , . . . , X k ) -E(g(X 1 , . . . , X k-2 , Xk-1 , Xk , . . . , XN ) | X 1 , . . . , X k-1 ) .</p><p>For fixed X 1 , . . . , X k-1 , (22.15) implies that Z k varies in an interval of length c k at most (whose bounds depend on X 1 , . . . , X k-1 ) so that |Z k -E(Z k )| ≤ c k . Proposition 22.12 implies that P(Z 1 + • • • + Z N ≥ t) ≤ e -2t 2 /|c| 2 , which concludes the proof since Z 1 + • • • + Z N = g(X 1 , . . . , X N ) -E(g(X 1 , . . . , X N )).</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.3.6">Boucheron-Lugosi-Massart inequality</head><p>The following result <ref type="bibr" target="#b56">[38]</ref>, that we state without proof, extends on the same idea.</p><p>Theorem 22.14 Let X 1 , . . . , X N be independent random variables. Let Z = g(X 1 , . . . , X N )</p><p>with g : R N → [0, +∞) and for k = 1, . . . , N , Z k = g k (X 1 , . . . , X k-1 , X k+1 , . . . , X N ) loss in a classification problem, Hoeffding's inequality implies, for training sets of size N , P(R(f ) -RT (f ) &gt; t) ≤ e -2N t 2 . Now corollary 22.11 does not hold if we replace f by fT , i.e., if f is estimated from the training set T , which is, unfortunately, the situation we are interested in.</p><p>Before addressing this problem, we point out that this inequality does apply to the case in which f = fT 0 where T 0 is another training set, independent from T , so that P(R( fT 0 ) -RT ( fT 0 ) &gt; t) ≤ e -2N t 2 , which is proved by writing P(R( fT 0 ) -RT ( fT 0 ) &gt; t) = E(P(R( fT ) -RT ( fT ) |&gt; t|T 0 = T )) .</p><p>In this situation, the empirical risk is computed on a test or validation set (T ) independent of the set used to estimate f (T 0 ).</p><p>If one does not have a test set, and fT is optimized over a set F of possible predictors, one can rarely do much better than starting from a variation of the trivial upper bound P(R( fT ) -E T &gt; t) ≤ P sup f ∈F (R(f ) -E T (f )) &gt; t</p><p>(with E T = RT ( fT )) and the concentration inequalities discussed in section 22.3 need to be extended to provide upper bounds to the right-hand side.</p><p>Remark 22.15 Computing supremums of functions over non countable sets may bring some issues regarding measurability. To avoid complications, we will always assume, when computing supremums over infinite sets, that such supremums can be reduced to maximizations over finite sets, i.e., when considering sup f ∈F Φ(f ) for some function Φ, we will assume that there exists a nested sequence of finite subsets F n ⊂ F such that sup{Φ(f ) : f ∈ F } = lim n→∞ sup{Φ(f ) : f ∈ F n } . <ref type="bibr">(22.17)</ref> This is true, for example, when F has a topology that admits a countable dense subset, with respect to which Φ is continuous. ♦ When F is a finite set, one can use a "union bound" with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(sup</head><formula xml:id="formula_2062">f ∈F (R(f ) -E T (f )) &gt; t) ≤ f ∈F P(R(f ) -E T (f ) &gt; t) ≤ |F | max f ∈F P(R(f ) -E T (f ) &gt; t).</formula><p>Such bounds cannot be applied to the typical case in which F is infinite, and is likely to provide very poor estimates even when F is finite, but |F | is large. However, all proofs of concentration inequalities applied to such supremums require using a union bound at some point, often after considerable preparatory work. Union bounds will in particular appear in conjunction with the Vapnik-Chervonenkis dimension that we now discuss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4.2">Vapnik's theorem</head><p>We consider a classification problem with two classes, 0 and 1, and therefore let F be a set of binary functions, i.e., taking values in {0, 1}. We also assume that the risk function r takes values in the interval [0, 1] (using, for example, the 0-1 loss). Let</p><formula xml:id="formula_2063">U (t) = P sup f ∈F (R(f ) -E T (f )) &gt; t . (<label>22.18)</label></formula><p>A fundamental theorem of Vapnik provides an estimate of U (t) based on the number of possible ways to split a training set of 2N points into two classes using functions in F . The rest of this section is devoted to a discussion of this result and related notions. The following theorem controls U in <ref type="bibr">(22.18)</ref> in terms of S F . Theorem 22.16 (Vapnik) With the notation above, one has, for t ≥ √ 2/N : P sup f ∈F (R(f ) -E T (f )) &gt; t ≤ 2S F (2N )e -N t 2 /8 , (22.19) which implies that, with probability at least 1δ, we have ∀f ∈ F : R(f ) ≤ E T (f )) + 8 N log S F (N ) + log 2 δ (22.20) (The requirement that t ≥ √ 2/N does not really reduce the range of applicability of (22.19), since, for t ≤ √ 2/N , the upper bound in that equation is typically much larger than 1.) Proof We first show that the problem can be symmetrized with the inequality, valid if N t 2 ≥ 2,</p><formula xml:id="formula_2064">P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2P sup f ∈F (E T ′ (f ) -E T (f )) ≥ t 2 (22.21)</formula><p>in which T ′ is a second training set (independent of T ) with N samples also. In view of assumption <ref type="bibr">(22.17)</ref>, there is no loss of generality in assuming that F is finite. Associate to any training set T , a classifier f T ∈ F maximizing R(f T ) -E(f T ). One then has</p><formula xml:id="formula_2065">P       sup f ∈F (E T ′ (f ) -E T (f )) ≥ t 2       ≥ P (E T ′ (f T ) -E T (f T )) ≥ t 2 ≥ P (R(f T ) -E T ′ (f T ) ≤ t 2 and R(f T ) -E T (f T )) ≥ t = E 1 R(f T )-E T (f T ))≥t P R(f T ) -E T ′ (f T ) ≤ t 2 T</formula><p>Conditional to T , E T ′ (f T ) is the average of M i.i.d. Bernoulli random variables, with variance bounded from above by 1/4 and</p><formula xml:id="formula_2066">P R(f T ) -E T ′ (f T ) ≤ t 2 T ≥ 1 - 1/4 N t 2 /4 ≥ 1 2 .</formula><p>It follows that</p><formula xml:id="formula_2067">P sup f ∈F (E T ′ (f ) -E T (f )) &gt; t 2 ≥ 1 2 P R(f T ) -E T (f T )) ≥ t = 1 2 P sup f ∈F (R(f ) -E T (f )) ≥ t .</formula><p>This justifies <ref type="bibr">(22.21)</ref>.</p><p>Now consider a family of independent Rademacher random variables ξ 1 , . . . , ξ N , also independent of T and T ′ , taking values -1 and +1 with equal probability. By Now, there are at most |F (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N )| different sets of coefficients in front of ξ 1 , . . . , ξ N in the above sum when f varies in F , so that, conditioning on T , T ′ and taking a union bound , we have</p><formula xml:id="formula_2068">P sup f ∈F N k=1 ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ ≤ |F (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N )| sup f ∈F P N k=1 ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ The variables ξ k (r(Y k , f (X k )) -r(Y ′ k , f (X ′ k )</formula><p>) are centered and belong to the interval [-1, 1], which has length 2, so that Hoeffding's inequality implies with R(f ) ≤ E T (f ) + t for all f with probability 1δ or more.</p><p>■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4.3">VC dimension</head><p>To obtain a practical bound, the quantity S F (2N ), or its upper-bound S * F (2N ), needs to be estimated. We prove below an important property of S * F , namely that, either S * F (M) = 2 M for all M, or there exists an M 0 for which S * F (M 0 ) &lt; 2 M 0 , and taking M 0 to be the largest one for which an equality occurs, S * F (M) has order M M 0 for all M ≥ M 0 . This motivates the following definition of the VC-dimension of the model class.</p><p>From this lemma, it results that if VC-dim(F ) = D &lt; ∞, then S * F (M) is bounded by the total number of subsets of cardinality D or less in a set of cardinality M. This provides the following result, which implies that the term in front of the exponential in <ref type="bibr">(22.18)</ref> grows polynomially in N if F have finite VC-dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4.4">Examples</head><p>The following result provides the VC-dimension of the collection of linear classifiers. Then there is no function f ∈ F (taking the form x → sign(β x)) that maps (x 1 , . . . , x d+2 ) to (sign(α 1 ), . . . , sign(α d+1 ), -1) (where the definition of sign(0) = ±1 is indifferent), since any such function satisfies</p><formula xml:id="formula_2069">β T xd+2 = d+1 k=1 α k β T xk &gt; 0 .</formula><p>This proves VC-dim(F ) &lt; d + 2. To prove that VC-dim(F ) = d + 1, it suffices to exhibit a set of d + 1 vectors in R d that can be shattered by F . Choose x 1 , . . . , x d+1 such that x1 , . . . , xd+1 are linearly independent (for example x i = i-1 k=1 e i , where (e 1 , . . . , e d ) is the canonical basis of R d ). This linear independence implies that, for any vector α = (α 1 , . . . , α d+1 ) T ∈ R d+1 , there exists a vector β ∈ R d+1 such that xT i β = α i for all i = 1, . . . , d + 1. This shows that any combination of signs for xT i β can be achieved, so that (x 1 , . . . , x d+1 ) is shattered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Upper-bounds on VC dimensions of more complex models have also been proposed in the literature. As an example, the following theorem, that we provide without proof, considers feed-forward neural networks with piecewise linear units (such as ReLU, see chapter 11). This theorem is a special case of Theorem 7 in Bartlett et al. <ref type="bibr">[21]</ref>, in which the more general case of networks with piecewise polynomial units is provided. Given integers L, U 1 , . . . , U L and W 1 , . . . , W L , define the function class F (L, (U i ), (W i ), p) that consists of feed-forward neural networks with L layers, U i piecewise linear computational units with less than p pieces in the ith layer, and such that the total number of parameters involved in layers 1, 2, . . . , j is less than W j . </p><formula xml:id="formula_2070">       4ep L i=1 iU i log 2        L i=1 (2epiU i )               .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4.5">Data-based estimates</head><p>Approximations of the shattering numbers can be computed using training data. One can, in particular, prove a concentration inequality <ref type="bibr" target="#b56">[38]</ref> on log S F (X 1 , . . . , X N ), which may in turn be used to estimate log(S F (2N )). In the following, we let H VC (N , F ) denote the expectation of log S F (X 1 , . . . , X N ). It is often referred to as the VC entropy of F .</p><p>Theorem 22.24 One has, letting H VC = H VC (N , F ): P(log S F (X 1 , . . . , X N ) ≥ H VC + t) ≤ exp -t 2 2H VC + 2t/3 and P(log S F (X 1 , . . . , X N ) ≤ H VCt) ≤ exp -t 2 2H VC</p><p>Proof We show that the random variable Z = log 2 S F (X 1 , . . . , X N ) satisfies the assumptions of theorem 22.14, with Z k = log 2 S F (X 1 , . . . , X k-1 , X k+1 , . . . , X N ).</p><p>Clearly, 0 ≤ Z, 0 ≤ Z -Z k ≤ 1, because one can do no more than double S F by adding one point. We need to show that N k=1 (Z -Z k ) ≤ Z. <ref type="bibr">(22.24)</ref> Note that Z is the base-two entropy of the uniform distribution, π, on the set F (X 1 , . . . , X N ) ⊂ {-1, 1} N .</p><p>We will use the following lemma.</p><p>Lemma 22.25 Let A be a finite set and ψ a probability distribution on A N . Let ψ k be its marginal when the kth variable is removed. Then: This lemma is a special case of a collection of results on non-negative entropy measures developed in Han <ref type="bibr" target="#b104">[86]</ref>, and we provide a direct proof below for completeness.</p><p>Given the lemma, let π k denote the marginal distribution of π when the kth variable is removed, i.e., π k (ϵ 1 , . . . ϵ k-1 , ϵ k+1 , . . . , ϵ N ) = π(ϵ 1 , . . . ϵ k-1 , -1, ϵ k+1 , . . . , ϵ N ) + π(ϵ 1 , . . . ϵ k-1 , 1, ϵ k+1 , . . . , ϵ N ).</p><p>We have:</p><formula xml:id="formula_2071">N k=1 (H 2 (π) -H 2 (π k )) ≤ H(π)</formula><p>from which <ref type="bibr">(22.24)</ref> derives since Z = H 2 (π) and Z k ≥ H 2 (π k ). The result then follows from theorem 22.14.</p><p>We now prove lemma 22.25 by induction (this proof requires some basic notions of information theory). For convenience, introduce random variables (ξ 1 , . . . , ξ N ) such that ξ k ∈ A, with joint probability distribution given by ψ. Let Y = (ξ 1 , . . . , ξ N ), Y (k) the (N -1)-tuple formed from Y by removing ξ k , Y (k,l) the (N -2)-tuple obtained by removing ξ k and ξ l , etc. Inequality <ref type="bibr">(22.25)</ref> can then be rewritten This inequality is obviously true for N = 1, and it is true also for N = 2 since it gives in this case the well-known inequality H 2 (Y 1 , Y 2 ) ≤ H 2 (Y 1 ) + H 2 (Y 2 ). Fix M &gt; 2 and assume that the lemma is true for any N &lt; M. To prove the statement for N = M, we will use the following inequality, which holds for any three random variables U 1 , U</p><p>2 , U 3 : H 2 (U 1 , U 3 ) + H 2 (U 2 , U 3 ) ≥ H 2 (U 1 , U 2 , U 3 ) + H 2 (U 3 ) . This inequality is equivalent to the statement on conditional entropies that H 2 (U 1 , U 2 | U 3 ) ≤ H 2 (U 1 | U 3 ) + H 2 (U 2 | U 3 ). We apply it, for given k l, to U 1 = Y l , U 2 = Y k , U 3 = Y (k,l) , yielding H 2 (Y (k) ) + H 2 (Y (l) ) ≥ H 2 (Y ) + H 2 (Y (k,l) ). We now sum over all pairs k l, yielding 2(N -1) N k=1 H 2 (Y (k) ) ≥ N (N -1)H 2 (Y ) + k l H 2 (Y (k,l) ). We finally use the induction hypothesis to write that, for all k l k H 2 (Y (k,l) ) ≥ (N -2)H 2 (Y (k) ) and obtain 2(N -1) N k=1 H 2 (Y (k) ) ≥ N (N -1)H 2 (Y ) + (N -2) N k=1 H 2 (Y (k) ), which provides the desired result after rearranging the terms. ■ Note that theorem 22.16 involves S F (2N ), with: log 2 (S F (2N )) = log 2 E(S F (X 1 , . . . , X 2N )) ≥ H VC (2N , F ) from Jensen's inequality. This implies that the high-probability upper bound on H VC (2N , F ) that results from the previous theorem is not necessarily an upper bound on log(S F (2N )). It is however proved in Boucheron et al. [38] that log 2 E(S F (X 1 , . . . , X 2N )) ≤ 1 log 2 H VC (2N , F ) also holds (as a consequence of (22.16)). A little more work (see Boucheron et al. [38]) combining theorem 22.16 and theorem 22.24 implies the following bound, which holds with probability 1δ at least: ∀f ∈ F : R(f ) ≤ E(f ) + 6 log S F (X 1 , . . . , X N ) N + 4 log(2/δ) N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5">Covering numbers and chaining</head><p>The upper bounds using the VC dimension relied on the number of different values taken by a set of functions when evaluated on a finite set, this number being used to apply a union bound. A different point of view may be applied when one relies on some notion of continuity of the family of functions on which a uniform concentration bound is needed, with respect to a given metric. This viewpoint is furthermore applicable when the sets F (X 1 , . . . , X N ) are infinite. To develop these tools, we will need some new concepts measuring the size of sets in a metric space.</p><p>Assume that N (G, ρ ∞ , ϵ) &lt; ∞, for all ϵ &gt; 0 (which requires the set G to be precompact for the ρ ∞ metric). Take t &gt; 0, 0 &lt; ϵ &lt; t and choose a set G ⊂ G such that |G| = N (G, ρ ∞ , ϵ). Then, using a union bound, for some µ(g) &gt; 0, then, assuming that µ(G) ∆ = max g∈G µ(g) is finite, we find that, for 0 &lt; ϵ &lt; t, We now apply this inequality to the case of binary classification, where a binary variable Y is predicted by an input variable X, with a model class of classifiers F and the 0-1 loss function. If A is a finite family of elements of R, we define, for f , f ′ ∈ F</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(sup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(sup</head><formula xml:id="formula_2072">ρ A (f , f ′ ) = 1 |A| x∈A 1 f (x) f ′ (x) .</formula><p>Let N (F , ϵ, N ) = E N (F , ρ {X 1 ,...,X N } , ϵ)</p><p>where X 1 , . . . , X N is an i.i.d. sample of X. We then have the following proposition. Proof A key step in the proof of theorem 22.16, was to show that <ref type="bibr">(22.30)</ref> where ξ 1 , . . . , ξ N are Rademacher random variables and T , T ′ are two independent training sets of size N . We start from this inequality and bound the conditional expectation</p><formula xml:id="formula_2073">P sup f ∈F (R(f )-E T (f )) ≥ t ≤ 2P sup f ∈F N k=1 ξ k (r(Y ′ k , f (X ′ k ))-r(Y k , f (X k ))) ≥ N t/2 .</formula><formula xml:id="formula_2074">P sup f ∈F N k=1 ξ k (r(Y ′ k , f (X ′ k )) -r(Y k , f (X k ))) ≥ N t/2 T , T ′ (22.31)</formula><p>and therefore consider r(Y ′ k , f (X ′ k ))r(Y k , f (X k )) as constants that we will denote c k (f ). Since we are using a 0-1 loss, we have c k (f ) ∈ {-1, 0, 1} and, for f , f ′ ∈ F ,</p><formula xml:id="formula_2075">|c k (f ) -c k (f ′ )| ≤ 1 f (X k ) f ′ (X k ) + 1 f (X ′ k ) f ′ (X ′ k ) .</formula><p>(22.32)</p><p>Consider the random variable Z = (ξ 1 , . . . , ξ N ), and let</p><formula xml:id="formula_2076">G = g f , f ∈ F with g f (ξ 1 , . . . , ξ N ) = 1 N N k=1 c k (f )ξ k .</formula><p>We have Let A = (X 1 , . . . , X N , X ′ 1 , . . . , X ′ N ) so that</p><formula xml:id="formula_2077">ρ ∞ (g f , g f ′ ) = 1 N N k=1 |c k (f ) -c k (f ′ )| .</formula><formula xml:id="formula_2078">ρ A (f , f ′ ) = 1 2N N k=1 1 f (X k ) f ′ (X k ) + 1 f (X ′ k ) f ′ (X ′ k ) .</formula><p>Using <ref type="bibr">(22.32)</ref>, we have ρ ∞ (g f , g f ′ ) ≤ 2ρ A (f , f ′ ), which implies</p><formula xml:id="formula_2079">N (G, ϵ, ρ ∞ ) ≤ N (F , ϵ/2, ρ A ) .</formula><p>Using this in <ref type="bibr">(22.33)</ref> and taking the expectation in <ref type="bibr">(22.31)</ref>, we get So <ref type="bibr">(22.29)</ref> provides a family of equations that depend on a parameter ϵ which, in the limit ϵ → 0, includes theorem 22.16 as a particular case. For a given N , optimizing (22.29) over ϵ may give a better upper bound, provided one has a good way to estimate N (F , ϵ/2, N ) (which is, of course, far from obvious).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5.3">Evaluating covering numbers</head><p>Covering numbers can be evaluated in some simple situations. The following proposition provides an example in finite dimensions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>One can also obtain entropy number estimates in infinite dimensions. Here, we quote a result applicable to spaces of smooth functions, referring to Van der Vaart and Wellner <ref type="bibr" target="#b213">[195]</ref> for a proof. Let G be the unit ball for this norm,</p><formula xml:id="formula_2080">G = f ∈ C p (Z) : ∥f ∥ p,∞ ≤ 1 .</formula><p>Let Z (1) be the set of all x ∈ R d at distance less than 1 from R.</p><p>Then there exists a constant K depending only on p and d such that log N (ϵ, G, ρ ∞ ) ≤ Kvolume(Z (1) ) 1 ϵ d/p</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5.4">Chaining</head><p>The distance ρ ∞ may not always be the best one to analyze the set of functions, G. For example, if G is a class of functions with values in {-1, 1}, then ρ ∞ (g, g ′ ) = 2 unless g = g ′ . In such contexts, it is often preferable to use distances that compute average discrepancies, such as ρ p (g, g ′ ) = E(|g(Z)g ′ (Z)| p ) 1/p , <ref type="bibr">(22.35)</ref> for some random variable Z. Such distances, by definition, do not provide uniform bounds on differences between functions (that we used to write <ref type="bibr">(22.28)</ref>), but can rather be used in upper-bounds on the probabilities of deviations from zero, which have to be handled somewhat differently. We here summarize a general approach called "chaining," following for this purpose the presentation made in Talagrand <ref type="bibr" target="#b207">[189]</ref> (see also <ref type="bibr">Audibert and Bousquet [15]</ref>). From now on, we assume that (G, ρ) is a (pseudo-)metric space of functions g : Z → R and Z a random variable taking values in Z. We will make the basic assumption that, for all g, g ′ ∈ G and t &gt; 0, P(|g(Z)g ′ (Z)| &gt; t) ≤ 2e</p><p>t 2 2ρ(g,g ′ ) 2 .</p><p>Note that this assumption includes cases in which P(|g(Z)g ′ (Z)| &gt; t) ≤ 2e</p><p>t 2 2ρ(g,g ′ ) α .</p><p>for some α ∈ (0, 2], because, if ρ is a distance, then so is ρ α/2 if α ≤ 2. We will also assume that E(g(Z)) = 0 in order to avoid centering the variables at every step.</p><p>We are interested in upper bounds for P(sup g∈G g(Z) &gt; t). To build a chaining argument, consider a family (G 0 , G 1 , . . .) of subsets of G. Assume that |G k | ≤ N k with N k chosen, for future simplicity, so that N k-1 N k ≤ N k+1 . For g ∈ G, let π k (g) denote a closest point to g in G k . Also assume that G 0 = {g 0 } is a singleton, so that π 0 (g) = g 0 for all g ∈ G. (One can generally assume without harm that 0 ∈ G, in which case one should choose g 0 = 0 in the following discussion.) For g ∈ G n , we therefore have</p><formula xml:id="formula_2081">g -g 0 = n k=1 (π k (g) -π k-1 (g)) .</formula><p>Let (t 1 , t 2 , . . .) be a sequence of numbers that will be determined later. Let 2 2 k+1 e -2 k-1 t 2 . The upper bound converges (as a function of n) as soon as t &gt; 2 log 2. Moreover, one has 2 n k=1 2 2 k+1 e -2 k-1 t 2 = 2e -t 2 2 n k=1 e -2 k-2 (t 2 -8 log 2) ≤ 2e -t 2 2 ∞ k=1 e -2 k-2 when t &gt; 1 + 8 log 2. This provides a concentration bound for P(sup g∈G n g(Z)g 0 (Z) &gt; tS n ), that we may rewrite as P(sup g∈G n g(Z)g 0 (Z) &gt; t) ≤ Ce -t 2 2S 2 n (22.37) for t &gt; 2S n log 2, C = 2 ∞ k=1 e -2 k-2 and S n given by (22.36), with t k = 2 k/2 . Moreover, we have S n = max and this simpler upper bound can be used in <ref type="bibr">(22.37)</ref>.</p><p>We haven't made many assumptions so far on the sequence G 0 , G 1 , . . ., beyond bounding their cardinality, but it is natural to require that they are built in order to behave like a dense subset of G, so that lim n→∞ max g∈G ρ(x, G n ) = 0. <ref type="bibr">(22.38)</ref> Note that this requires that the set G is precompact for the distance ρ. We will also assume that lim The exponential rate of convergence in the right-hand side of <ref type="bibr">(22.41)</ref> is the quantity S, and the upper bound will be improved when building the sequence (G 0 , G 1 , . . .) so that S is as small as possible. Such an optimization for a given family of functions is however a formidable problem. It is however interesting to see (still following <ref type="bibr" target="#b206">[188]</ref>) that theorem 22.31 implies a classical inequality in terms of what is called the metric entropy of the metric space (G, ρ). ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5.5">Metric entropy</head><formula xml:id="formula_2082">≥ 1 - √ 2 2 ∞ n=1 2 n/2 e(2 2 n ).</formula><p>is obvious. The same inequality will be true for some x 1 , . . . , x N with N = 2, except in the uninteresting case where f (x) = 1 (or -1) for every x ∈ R.</p><p>A similar inequality holds for entropy numbers with the ρ 1 distance (cf. <ref type="bibr">(22.35)</ref>) because E(|r(Y , f (X))r(Y , f ′ (X))|) ≤ P(f (X) f ′ (X))</p><p>whenever r takes values in [0, 1], which implies that</p><formula xml:id="formula_2083">N (G, ρ 1 , ϵ) ≤ N (F , ρ 1 , ϵ)</formula><p>for all ϵ &gt; 0. Note however that evaluating this upper bound may still be challenging and would rely on strong assumptions on the distribution of X allowing to control P(f (X) f ′ (X)).</p><p>We now assume that functions in F define "posterior probabilities" on G. More precisely, given λ ∈ R we can define the probability π λ on {-1, 1} by so that entropy numbers in G can be estimated from entropy numbers in F . As an example, let F be a space of affine functions x → a 0 + b T x, x ∈ R d . Assume that the random variable X is bounded, so that one can take R to be an open ball centered at 0 with radius, say, U . For M &gt; 0, let</p><formula xml:id="formula_2084">F M = {f : x → a 0 + b T x : |b| ≤ M, |a 0 | ≤ U M} .</formula><p>The restriction |b| ≤ M is equivalent to using a penalty method, such as, for example, ridge logistic regression. Moreover, if |b| ≤ M, it is natural to assume that |a 0 | ≤ U M because otherwise f would have a constant sign on R. In this case, we get We have</p><formula xml:id="formula_2085">E T ′ (f ) -E γ,T (f ) = 1 N N k=1 (r 0 (Y ′ k , f (X ′ k )) -r γ (Y k , f (X k )))</formula><p>and because (X k , Y k ) and (X ′ k , Y ′ k ) have the same distribution, sup f ∈F (E T ′ (f ) -E γ,T (f )) has the same distribution as</p><formula xml:id="formula_2086">∆ T ,T ′ (ξ 1 , . . . , ξ N ) = sup f ∈F 1 N N k=1 (r 0 (Y ′ k , f (X ′ k )) -r γ (Y k , f (X k )))ξ k + (r 0 (Y k , f (X k )) -r γ (Y ′ k , f (X ′ k )))(1 -ξ k )</formula><p>where ξ 1 , . . . , ξ N is a sequence of Bernoulli random variables with parameter 1/2.</p><p>We now estimate P(∆ T ,T ′ (ξ 1 , . . . , ξ N ) &gt; t/2 | T , T ′ ) and we therefore consider T and T ′ as fixed. Let F be a subset of F , with cardinality N ∞ (γ/2, 2N ), such that for all f ∈ F there exists an f ′ ∈ F such that |f (x)-f ′ (x)| ≤ γ/2 for all x ∈ {X 1 , . . . , X N , X ′ 1 , . . . , X ′ N }. Then we claim that ∆ T ,T ′ (ξ 1 , . . . , ξ N ) ≤ ∆ ′ T ,T ′ (ξ 1 , . . . , ξ N ) where</p><formula xml:id="formula_2087">∆ ′</formula><p>T ,T ′ (ξ 1 , . . . , ξ N ) = max</p><formula xml:id="formula_2088">f ∈F 1 N N k=1 (2ξ k -1) r γ 2 (Y ′ k , f (X ′ k )) -r γ 2 (Y k , f (X k )) .</formula><p>This is because, for any (x, y) ∈ R × {0, 1}, and f , f ′ such that |f (x)f ′ (x)| &lt; γ/2, we have r 0 (y, f (x)) ≤ r γ/2 (y, f ′ (x)) and r γ/2 (y, f ′ (x)) ≤ r γ (y, f (x)): if an example is misclassified by f (resp. f ′ ) at a given margin, it must be misclassified by f ′ (resp. f ) at this margin plus γ/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now,</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(∆ ′</head><p>T ,T ′ (ξ 1 , . . . , ξ N ) &gt; t 2 )</p><formula xml:id="formula_2089">≤ |F| max f ∈F P 1 N N k=1 (2ξ k -1)(r γ 2 (Y ′ k , f (X ′ k )) -r γ 2 (Y k , f (X k ))) &gt; t 2</formula><p>to which we can apply Hoeffding's inequality, yielding In order to evaluate the covering numbers N ∞ (ϵ, N ) using quantities similar to VC-dimensions, a different type of set decomposition and shattering has been proposed. Following Alon et al. <ref type="bibr" target="#b22">[4]</ref>, we introduce the following notions. Recall that a family of functions F : R → {0, 1} shatters a finite set A ⊂ R if and only if |F (A)| = 2 |A| . The following definitions are adapted to functions taking values in a continuous set. (i) One says that F P -shatters A if there exists a function g A : R → R such that, for each B ⊂ A, there exists a function f ∈ F such that f (x) ≥ g A (x) if x ∈ B and f (x) &lt; g A (x) if x ∈ A \ B.</p><formula xml:id="formula_2090">P ∆ ′</formula><p>(ii) Let γ be a positive number. One says that F P γ -shatters A if there exists a function g A : R → R such that, for each B ⊂ A, there exists a function f ∈ F such that f (x) ≥</p><formula xml:id="formula_2091">g A (x) + γ if x ∈ B and f (x) ≤ g A (x) -γ if x ∈ A \ B.</formula><p>Note that only the restriction of g A to A matters in this definition. This function acts as a threshold for binary classification. More precisely, given a function g : A → R, one can associate to every f ∈ F the binary function f g with f g (x) equal to 1 if f (x) ≥ g(x) and to 0 otherwise. Letting F g = {f g : f ∈ F } we see that F P-shatters A if there exists a function g A such that F g A shatters A. The definition of P γ -shattering introduces a margin in the definition of f g (with f g (x) equal to 1 if f (x) ≥ g(x) + γ, to 0 if f (x) ≤ g(x)γ and is ambiguous otherwise), and A is P γ -shattered by F if, for some g A , the corresponding F g A shatters A without ambiguities. Definition 22.36 One then defines the P -dimension of F by P-dim(F ) = max{|A| : A ⊂ R, F P -shatters A}, and similarly the P γ -dimension of F is P γ -dim(F ) = max{|A| : A ⊂ R, F P γ -shatters A}.</p><p>The P γ -dimension of F will replace the VC-dimension in order to control the covering numbers. More precisely, we have the following theorem <ref type="bibr" target="#b22">[4]</ref>. .</p><p>Proof The proof is quite technical and relies on a combinatorial argument in which F is first assumed to take integer values before addressing the continuous case.</p><p>Step 1. We first assume that functions in F take values in the finite set {1, . . . , r} where r is an integer. For the time of this proof, we introduce yet another notion of shattering called S-shattering (for strong shattering) which is essentially the same as P 1 -shattering, except that functions g are restricted to take values in {1, . . . , r}. Let A be a finite subset of R. Given a function g : R → {1, . . . , r}, we say that (F , g) Sshatters A if, for any B ⊂ A, there exist f ∈ F satisfying f (x) ≥ g(x) + 1 for x ∈ B and f (x) ≤ g(x) -1 if x ∈ A \ B. We say that F S-shatters A if (F , g) S-shatters A for some g. The S-dimension of F is the cardinality of the largest subset of R that can be S-shattered and will be denoted S-dim(F ). The first, and most difficult, part of the proof is to show that, if S-dim(F ) = D, then and ⌈i⌉ denotes the smallest integer larger than u ∈ R. Here, M is the packing number defined in section 22.5.1.</p><formula xml:id="formula_2092">M(F (A), ρ ∞ , 2) ≤ 2(</formula><p>To prove this, we can assume that r ≥ 3, since, for r ≤ 2, M(F (A), ρ ∞ , 2) = 1 (the diameter of F for the ρ ∞ distance is 0 or 1). Let G(A) = {1, . . . , r} A be the set of all functions f : A → {1, . . . , r} and let</p><formula xml:id="formula_2093">U A = F ⊂ G(A) : ∀f , f ′ ∈ F, ∃x ∈ A with |f (x) -f ′ (x)| ≥ 2 .</formula><p>For F ∈ U A , let S A (F) = {(B, g) : B ⊂ A, B ∅, g : B → {1, . . . , r}, (F, g) S-shatters B}.</p><p>Let t A (h) = min{|S A (F)| : F ∈ U A , |F| = h} (where the minimum of the empty set is +∞). Since we are considering in U A all possible functions from A to {1, . . . , r}, it is clear that t A (h) only depends on |A|, and we will also denote it by t(h, |A|).</p><p>Note that, by definition, if (B, g) ∈ S A (F), and F ⊂ F , then |B| ≤ D. So, the number of elements in S A (F) for such an F is less or equal than the number of possible such pairs (B, g), which is strictly less than y = D k=1 |A| k r k . So, if t(h, |A|) ≥ y, then there cannot be any F ⊂ F in the set U A and M(F (A), ρ ∞ , 2) &lt; h. The rest of the proof consists in showing that t(h, |A|) ≥ y.</p><p>For any n ≥ 1, we have t(2, n) = 1: fix x ∈ A, and F = {f 1 , f 2 } ∈ G such that f 1 (x) = 1, f 2 (x) = 3 and f 1 (y) = f 2 (y) if y x. Then only ({x}, g) is S-shattered by F, with g such that g(x) = 2. Now, assume that, for some integer m, t(2mnr 2 , n) &lt; ∞, so that there exists F ∈ U A such that |F| = 2mnr 2 . Arrange the elements of F into mnr 2 pairs {f i , f ′ i }. For each such pair, there exists x i ∈ A such that |f i (x i )f ′ i (x i )| &gt; 1. Since there are at most n selected x i , one of them must be appearing at least mr 2 times. Call it x and keep (and reindex) the corresponding mr 2 pairs, still denoted {f i , f ′ i }. Now, there are at most r(r -1)/2 possible distinct values for the unordered pairs {f i (x), f ′ i (x)}, so that one of them must be appearing at least 2mr 2 /r(r -1) &gt; 2m times. Select these functions, reindex them and exchange the role of f i and f ′ i if needed to obtain 2m pairs {f i , f ′ i } such that f i (x) = k and f ′ i (x) = l for all i and fixed k, l ∈ {1, . . . , r} such that k + 1 &lt; l. Let F 1 = {f 1 , . . . , f 2m } and F ′ 1 = {f ′ 1 , . . . , f ′ 2m }. Let A ′ = A \ {x}. Then both F 1 and F ′ 1 belong to U A ′ , which implies that both S A ′ (F 1 ) and S A ′ (F ′ 1 ) have cardinality at least t(2m, n-1). Moreover, both sets are included in S A (F), and if (B, g) ∈ S A ′ (F 1 )∩S A ′ (F ′ 1 ), then (B ∪ {x}, g ′ ) ∈ S A (F), with g ′ (y) = g(y) for y ∈ B and g ′ (x) = k + 1. This provides 2t(2m, n-1) elements in S A (F) and shows the key inequality (which is obviously true when the left-hand side is infinite) t(2mnr 2 , n) ≥ 2t(2m, n -1) .</p><p>This inequality can now be used to prove by induction that for all 0 ≤ k &lt; n, one has t(2(nr 2 ) k , n) ≥ 2 k , since t(2((n + 1)r 2 ) k+1 , n + 1) ≥ 2t(2((n + 1)r 2 ) k , n) ≥ 2t(2(nr 2 ) k , n).</p><p>For k ≥ n, one has 2(nr 2 ) k &gt; r n , where r n is the number of functions in G(A), so that t(2(nr 2 ) k , n) = +∞. So, t(2(nr 2 ) k , n) ≥ 2 k is valid for all k and it suffices to take k = ⌈log 2 y⌉ to obtain the desired result.  To prove (a), assume that F η S-shatters A, so that there exists g such that, for all B ⊂ A, there exists f ∈ F such that f η (x) ≥ g(x) + 1 for x ∈ B and f η (x) ≤ g(x) -1 for x ∈ A \ B. Using the fact that 2ηf η (x) -1 ≤ f (x) &lt; 2ηf η (x) + 2η -1, we get f (x) ≥ 2ηg(x)+2η-1 for x ∈ B and f (x) ≤ 2ηg(x)-1 for x ∈ A\B. So taking g(x) = 2ηg(x)+η-1 as threshold function (which does not depend on B), we see that F P γ -shatters A if γ ≤ η. This shows, in particular, that (letting ξ 1 , . . . , ξ N be independent Rademacher random variables)</p><formula xml:id="formula_2094">P        Λ N k=1 ξ k + N k=1 ξ k x k ≥ N γ        = 1.</formula><p>where U T (γ) depends on data and is increasing (as a function of γ), and C(γ) is a decreasing function of γ. Consider a decreasing sequence (γ k ) that converges to 0 (for example γ k = L2 -k ). Choose also an increasing function ϵ(γ). Then P R 0 ( fT ) &gt; min{U T (γ) + t 2 + 2 log C(γ) + ϵ 2 (γ) : 0 ≤ γ ≤ L} ≤ P R 0 ( fT ) &gt; min{U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ 2 (γ k ) : k ≥ 1} .</p><formula xml:id="formula_2095">Moreover P R 0 ( fT ) &gt; min{U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ 2 (γ k ) : k ≥ 1} ≤ ∞ k=0 P R 0 ( fT ) &gt; U T (γ k ) + t 2 + 2 log C(γ k-1 ) + ϵ(γ k ) ≤ ∞ k=0 C(γ k ) C(γ k-1 )</formula><p>e -mϵ 2 (γ k )/2-mt 2 /2 .</p><p>So, it suffices to choose ϵ(γ) so that</p><formula xml:id="formula_2096">C 0 = ∞ k=1 C(γ k ) C(γ k-1 )</formula><p>e -mϵ 2 (γ k )/2 &lt; ∞ to ensure that P R 0 ( fT ) &gt; min{U T (γ) + t 2 + 2 log C(γ) + ϵ 2 (γ) : γ 0 ≤ γ ≤ L} ≤ C 0 e -mt 2 /2 .</p><p>For example, if γ k = L2 -k , one can take</p><formula xml:id="formula_2097">ϵ(γ) = 2 m log C(γ) C(γ/2) + log γ -1</formula><p>which yields C 0 ≤ L. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.6.2">Maximum discrepancy</head><p>Let T be a training set and let T 1 and T 2 form a fixed partition of the training set in two equal parts. Assume, for simplicity, that N is even and that the method for selecting the two parts is deterministic, e.g., place the first half of T in T 1 and second one in T 2 . Following Bartlett et al. <ref type="bibr">[20]</ref>, one can then define the maximum discrepancy on T by C T = sup </p><formula xml:id="formula_2098">ξ k r(Y k , f (X k )) T = T .</formula><p>The mean Rademacher complexity is then the expectation of this quantity over the training set distribution. The Rademacher complexity can be computed with a-costly-Monte-Carlo simulation, in which the best estimator is computed with randomly flipped labels corresponding to the values of k such that ξ k = -1. This measure of complexity was introduced to the machine learning framework in Koltchinskii and Panchenko <ref type="bibr" target="#b127">[109]</ref>, <ref type="bibr">Bartlett and Mendelson [19]</ref>, and Rademacher sums have been extensively studied in relation to empirical processes (cf. Ledoux and Talagrand <ref type="bibr" target="#b135">[117]</ref>, chapter 4).</p><p>One can bound the Rademacher complexity in terms of VC dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.6.5">PAC-Bayesian bounds</head><p>Our final discussion of concentration bounds for the empirical error uses a slightly different paradigm from that discussed so far. The main difference is that, instead of computing one predictor fT from a training set T , it would return a random variable with values in F , or, equivalently, a probability distribution on F (therefore assuming that this space is measurable) that we will denote μT . The training set error is now defined by: ĒT (µ) = E T (f )dµ(f ) ,</p><p>for any probability distribution µ on F , while the generalization error is:</p><formula xml:id="formula_2099">R(µ) = F R(f )dµ(f ) .</formula><p>Our goal is to obtain upper bounds on R(µ T )-ĒT (µ T ) that hold with high probability.</p><p>In this framework, we have the following result, in which Q denotes the space of probability distributions on F . The term log 2N is however superfluous in this simple context, because one can write, for any t &gt; 0</p><formula xml:id="formula_2100">P sup f ∈F 0 R(f ) -E T (f ) ≥ t - log(π(f )) 2N ≤ f ∈F 0</formula><p>e -2N (t log(π(f ))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2N</head><p>) = e -2N t so that, with probability 1δ (letting t = log(1/δ)/2N ), for all f ∈ F 0 : R(f ) -E T (f ) ≤ log δlog π(f ) 2N . ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.7">Application to model selection</head><p>We now describe how the previous results can, in principle, be applied to model selection <ref type="bibr">[20]</ref>. We assume that we have a countable family of nested models classes (F (j) , j ∈ J ). Denote, as usual, by E T (f ) the empirical prediction error in the training set for a given function f . We will denote by f (j) T a minimizer of the in-sample error for F (j) , such that E T ( f (j) T ) = min</p><formula xml:id="formula_2101">f ∈F (j)</formula><p>E T (f ).</p><p>In the model selection problem, one would like to determine the best model class, j = j(T ), such that the prediction error R( f (j) T ) is minimal, or, more realistically, determine j * such that R( f (j * ) T ) is not too far from the optimal one.</p><p>We will consider penalty-based methods in which one minimizes ẼT (f ) = E T (f ) + C T (j) to determine j(T ). The penalty, C T , may also be data-dependent, and will therefore be a random variable. The previous concentration inequalities provided highly probable upper-bounds for R( f (j) T ), each exhibiting a random variable Γ</p><p>T that is larger than R( f (j) T ) with probability close to one. More precisely, we obtained inequalities taking the form (when applied to F (j) ) P(R T ( f (j) ) ≥ Γ (j) T + t) ≤ c j e -mt 2 <ref type="bibr">(22.48)</ref> for some known constants c j and m. For example, the VC-dimension bounds have</p><formula xml:id="formula_2103">Γ (j)</formula><p>T = E T ( f (j) T ), c j = 2S F (j) (2N ) and m = N /8.</p><p>Given such inequalities, one can develop a model selection strategy that relies on a priori weights, provided by a sequence π j of positive numbers such that j∈J π j = 1. Define The selected model class is then F (j * ) where j * minimizes Γ The same proof as that provided at the end of section 22.6.5 justifies this procedure. Indeed, for t &gt; 0, P R( fT ) -ẼT ( fT ) ≥ t ≤ P max j (R( f (j) T ) -ẼT ( f (j) T )) ≥ t</p><formula xml:id="formula_2104">≤ P        max j (R( f (j) T ) ≥ R * j + t + - log πj m        ≤ c j π j e -mt 2</formula><p>≤ ce -mt 2 with c = ∞ j=1 π j /c j .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4 .</head><label>4</label><figDesc>The set of m × d real matrices with real entries is denoted M m,d (R), or simply M m,d (M d,d will also be denoted M d ). The set of invertible d × d matrices will be denote GL d (R). Given m column vectors x 1 , . . . , x m ∈ R d , the notation [x 1 , . . . , x m ] refers to the d by m matrix with j th column equal to x j , so that, for example, Id R d = [e 1 , . . . , e d ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 .</head><label>5</label><figDesc>The space of d × d real symmetric matrices is denoted S d , and its subsets containing positive semi-definite (resp. positive definite) matrices is denotedS + d (resp. S ++ d ). If m ≤ d, O m,d denotes the set of m × d matrices A such that AA T = Id R m , and one writes O d for O d,d , the space of d-dimensional orthogonal matrices. Finally, SO d is the subset O d containing orthogonal matrices with determinant 1, i.e., rotation matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>|a| = max{a(x 1 , . . . , x k ) : |x j | ≤ 1, j = 1, . . . , k} so that |a(x 1 , . . . , x k )| ≤ |a| k j=1 |x j | for all x 1 , . . . , x k ∈ R d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>∀x, y ∈ B : ρ(x, y) = 0 ⇔ x = y, (1.2a) ∀x, y ∈ B : ρ(x, y) = ρ(y, x), (1.2b) ∀x, y, z ∈ B : ρ(x, z) ≤ ρ(x, y) + ρ(y, z). (1.2c) Equation (1.2c) is called the triangle inequality. The norm of the difference between two points: ρ(x, y) = |x -y|, is a distance on R d . The definition of open and closed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 ). 6 ) 4 .</head><label>564</label><figDesc>Consider now the function I(A) =A → A -1 defined on GL d (R), which is an open subset of M d (R). Using AI(A) = Id R d andthe product rule, we getA(dI(A)H) + HI(A) = 0 or dI(A)H = -A -1 HA -1 . (1Higher-order partial derivatives ∂ i k • • • ∂ i 1 f : U → R m aredefined by iterating the definition of first-order derivatives, namely</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 2 . 1 (</head><label>21</label><figDesc>Von Neumann) Let A, B ∈ M n,d (R) have singular values (λ 1 , . . . , λ m ) and (µ 1 , . . . , µ m ), respectively, where m = min(n, d). Assume that these eigenvalues are listed in decreasing order so that λ 1 ≥ • • • ≥ λ m and µ 1 ≥ • • • ≥ µ m . Then, trace(A T B) ≤ m i=1 λ i µ i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(2. 2 )</head><label>2</label><figDesc>Let us consider the first sum in the upper-bound. Let ξ d = λ d (resp. η d = µ d ) and ξ i = λ iλ i+1 (resp. η i = µ iµ i+1 ) for i = 1, . . . , d -1. Since singular values are nonincreasing, we have ξ i , η i ≥ 0 and i = 1, . . . , d. We have d i,j=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Corollary 2 . 4</head><label>24</label><figDesc>Let A ∈ S d (R) be a symmetric matrix with eigenvaluesλ 1 ≥ • • • ≥ λ d . For p ≤ d, let µ 1 ≥ • • • ≥ µ p &gt;0 and define F(e 1 , . . . , e p ) = p i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 2 . 5</head><label>25</label><figDesc>Let A ∈ M d,d (R) be a symmetric matrix with eigenvalues λ 1 ≥ • • • ≥ λ d . Then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proposition 2 . 6</head><label>26</label><figDesc>Let A be an n by d matrix. Then|A| * = max trace(U AV T ) : U ∈ M n,n and U T U = Id, V ∈ M d,d and V T V = Id .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3. 2 . 1</head><label>21</label><figDesc>Conditions for optimality (general case) Consider a function F : Ω → R where Ω is an open subset of R d . We first discuss the unconstrained optimization problem of finding x * ∈ argmin Ω F.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>|x| 2 -</head><label>2</label><figDesc>|∇F(0)|This shows that F(x) &gt; F(0) if |x| &gt; 2|∇F(0)|/m := r so that argmin F = argmin B(0,r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Definition 3 .</head><label>3</label><figDesc>15 A function F : Ω → R m is L-C k , L being a positive number, if it is C k and |d k F(x)d k F(y)| ≤ L|x -y|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>■</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><figDesc>.16a) for some constant c 2 ∈ (c 1 , 1). The strong Wolfe conditions require (3.15) and|h T ∇F(x + αh)| ≤ c 2 |h T ∇F(x)|. (3.16b) (Since h is a direction of descent, (3.16b) requires (3.16a) and the fact that h T ∇F(x + αh) does not take too large positive values.) If F is L-C 1 , these conditions, with (3.12a) and (3.12b), imply (3.14). Indeed, (3.16a) and the L-C 1 condition imply -(1c 2 )h T ∇F(x) ≤ h T (∇F(x + αh) -∇F(x)) ≤ Lα|h| 2 and (3.12a) and (3.12b) give (1c 2 )ϵ|∇F(x)| 2 ≤ αLγ 2 2 |∇F(x)| 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>■ 3 . 3</head><label>33</label><figDesc>Stochastic gradient descent 3.3.1 Stochastic approximation methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><figDesc>Finally, let∆(ρ, T ) = max s∈[ρ,ρ+T ] s ρ η c (u)du .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Definition 3 .</head><label>3</label><figDesc>30 A point x ∈ Ω satisfies the Mangasarian-Fromovitz constraint qualifications (MF-CQ) if the following two conditions are satisfied. (MF1) The vectors (∇γ i (x), i ∈ E) are linearly independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>3. 5</head><label>5</label><figDesc>General convex problems 3.5.1 Epigraphs Definition 3.39 Let F be a convex function. The epigraph of F is the set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><figDesc>for all λ ∈ D and x ∈ Ω. Define d = sup{L * (λ) : λ ∈ D} and p = inf{F(x) : x ∈ Ω}, whose computations respectively represent the dual and primal problems. Then, we have d ≤ p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><figDesc>56)for all x ∈ R d and λ ∈ D. Such a pair (x * , λ * ) is called a saddle point of the function L. Conversely, any saddle point of L, i.e., any (x * , λ * ) ∈ R d × D satisfying(3.56), must be such that x * ∈ Ω (to ensure that L(x * , •) is bounded), and satisfies the KKT conditions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>3. 7 . 1</head><label>71</label><figDesc>Proof of proposition 3.44 We start with a few general remarks. If x ∈ R d , the set {x} is convex and relint({x}) = {x}. If Ω is any convex set such that x relint(Ω), then theorem 3.63 implies that there exist b ∈ R d and β ∈ R such that b T y ≥ β ≥ b T x for all y ∈ Ω (with b T y &gt; b T x for at least one y). If x is in Ω \ (relint(Ω)) (so that x is a point on the relative boundary of Ω), then, necessarily b T x = β and we can write b T y ≥ b T x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Lemma 3 .</head><label>3</label><figDesc>65 Let F be a convex function with epigraph epi(F) = {(y, a) : y ∈ dom(F), F(y) ≤ a}. Then relint(epi(G)) = {(y, a) : y ∈ ridom(F), F(y) &lt; a}. Proof Let Γ = {(y, a) : y ∈ ridom(F), F(y) &lt; a}. Assume that (y, a) ∈ relint(epi(F)). Then (y, b) ∈ epi(F) for all b &gt; a and there exists ϵ &gt; 0 such that (y, a)ϵ((y, b) -(y, a)) ∈ epi(F) which requires that F(y) ≤ aϵ(b -1) &lt; a. Now, take x ∈ dom(F).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>■ 3 . 7 . 2</head><label>372</label><figDesc>Proof of theorem 3.45</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Proposition 4 . 1</head><label>41</label><figDesc>Let µ and ν be two probability measures on Ω. Then KL(µ∥ν) ≥ 0 and vanishes if and only if µ = ν.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><figDesc>4.1 and fig. 4.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 4 . 1 :Figure 4 . 2 :</head><label>4142</label><figDesc>Figure 4.1: Kernel density estimators using a Gaussian kernel and various values of σ when the true distribution of the data is a standard Gaussian (Orange: true density; Blue: estimated density, Red dots: training data).</figDesc><graphic coords="103,144.39,370.70,165.84,124.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Fixing a training</head><figDesc>set T , one can compute, for each λ, the cross-validation error e T (λ) = RCV,T (A λ ). Model selection is then performed by finding λ * (T ) = argmin λ e T (λ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Definition 6 . 1</head><label>61</label><figDesc>A function K : R × R → R satisfying properties [K1] and [K2] is called a positive kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><figDesc>is attained at â0 = ȳ -xT b with the usual definitions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>■</head><figDesc>This shows that there is no loss of generality in restricting the minimization of the residual sum of squares to b ∈ V . Such a b takes the form b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Proposition 7 . 5</head><label>75</label><figDesc>The function λ → U (β λ ) is nondecreasing, and λ → ϕ(β λ ) is nonincreasing, with lim λ→∞ ϕ(β λ ) = inf(ϕ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><figDesc>terminate the algorithm without updating ζ and set b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 7 . 1 :</head><label>71</label><figDesc>Figure 7.1: The function V defining the SVM risk function.</figDesc><graphic coords="161,173.42,288.63,265.17,198.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>■</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Algorithm 8 . 2 (</head><label>82</label><figDesc>Logistic lasso)(1) Input: (i) training data (x 1 , y 1 , . . . , x N , y N ) with x i ∈ R d and y i ∈ R Y ; (ii) coefficients ρ g , g ∈ R Y with non-zero sum and target value c ∈ R; (iii) algorithm step ϵ; (iv) penalty coefficient λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 8 . 1 :</head><label>81</label><figDesc>Figure 8.1: Left: Original (training) data with three classes. Right: LDA scores, where the x axis provides γ 1 and the y axis γ 2 .</figDesc><graphic coords="181,92.36,85.04,212.12,159.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><figDesc>8.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 8 . 2 :</head><label>82</label><figDesc>Figure 8.2: The green line is preferable to the purple one in order to separate the data.</figDesc><graphic coords="195,183.17,347.38,245.66,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Algorithm 10 . 3 (</head><label>103</label><figDesc>Randomized node insertion: RNode(T , j)) (a) Given T and j, let T ν = T and L(ν) = j. (b) If σ (T ) = 1, let C(ν) = ∅, γ ν = "None" and f ν = fT ,CF .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 11 . 1 : 1 yFigure 11 . 2 :</head><label>1111112</label><figDesc>Figure 11.1: Linear net with increasing layer depths and decreasing layer width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><figDesc>11.5.1 Mini-batchesFix ℓ ≪ N . Consider the set of B ℓ of binary sequences ξ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Algorithm 12 . 1 (</head><label>121</label><figDesc>Rejection sampling with acceptance function a and base p.d.f. g) (1) Sample a realization z of a random variable with p.d.f. g.(2) Generate b ∈ {0, 1} with P(b = 1) = a(z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>( 3 )</head><label>3</label><figDesc>If b = 1, return Z = z and exit.(4) Otherwise, return to step 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>a 2 2</head><label>2</label><figDesc>∆q(y) + ∇H(y) T ∇q(y) + q(y)∆H(y) = 0 (12.15) is satisfied by the function y → e -2H(y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>. 4 . 2</head><label>42</label><figDesc>Example: Ising modelWe will see several examples of applications of Gibbs sampling in the next few chapters. Here, we consider a special instance of Markov random field (see chapter 13) called the Ising model. For this example, B = {0, 1} L , and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><figDesc>Denoting the Markov chain by (Z n , M n ), we assume that the next pair Z n+1 , M n+1 is computed by (i) sampling M ′ n ∼ N (0, Id R d ); (ii) solving (12.23), with initial conditions ζ(0) = Z n and µ(0) = M ′ n ; (iii) taking Z n+1 = ζ(θ) and sampling M n+1 ∼ N (0, Id R d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>. 37 )</head><label>37</label><figDesc>with ρ = max(ρ(|x|), ρ(|y|)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Proposition 13 . 26</head><label>1326</label><figDesc>With the notation above, π S|T (• | y (T ) ) is associated to the family of local interactions Φ |y T = (ϕ C|y (T ) , C ∈ C S ) with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>C⊃ B,C⊂B (- 1 )</head><label>1</label><figDesc>|C|-| B| = 0 (for B = B, the sum is obviously equal to 1). Indeed, if s ∈ B, s B, we have C⊃ B,C⊂B (-1) |C|-| B| = C⊃ B,C⊂B,s∈C (-1) |C|-| B| + C⊃ B,C⊂B,s C (-1) |C|-| B| = C⊃ B,C⊂B,s C ((-1) |C∪{s}|-| B| + (-1) |C|-| B| ) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 13 . 3 :</head><label>133</label><figDesc>Figure 13.3: Graph forming a two-dimensional regular grid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Theorem 14 .</head><label>14</label><figDesc>24 Let G = (V , E) be an undirected graph, and C * G be the set of all maximum cliques in G. The following two properties are equivalent.(i) There exists a junction tree over C * G .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>(</head><figDesc>JT1) Extend G by adding edges to obtain a triangulated graph G * . (JT2) Compute the set C * of maximal cliques in G * , which therefore extend C. (JT3) Build a junction tree over C * . (JT4) Assign interaction ϕ C to a clique C * ∈ C * such that C ⊂ C * . (JT5) Run the junction-tree belief propagation algorithm to compute the marginal of π (associated to Φ) over each set C * ∈ C * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Algorithm 14 . 6 (</head><label>146</label><figDesc>Graph triangulation)Initialize the algorithm with k = n and E k = E. Given E k , determine E k-1 as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><figDesc>Prim's algorithm, adding a new edge e k+1 = {C k+1 , C ′ } to T k . Take as before the path in T linking C ′ to C k+1 in T , and select the edge e at which this path leaves C k . If e = (B, B ′ ), we must have w(e) = |B ∩ B ′ | ≤ w(e k ) = |C k+1 ∩ C ′ |, and the running intersection property in T implies that C k+1 ∩ C ′ ⊂ B ∩ B ′ , which implies that w(e) = w(e k+1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><figDesc>Bayesian networks are graphical models supported by directed acyclic graphs (DAG), which provide them with an ordered organization (directed graphs were introduced in definition 13.35).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Definition 15 . 10</head><label>1510</label><figDesc>One says that two vertexes s and t in G are d-separated by a set U if and only if any path between s and t in G ♭ must either (D1) Pass at a vertex in U without a v-junction. (D2) Pass in V \ A U with a v-junction. Then we have: Theorem 15.11 Two vertexes s and t in G are separated by a set U in (G A {s,t}∪U ) ♯ if and only if they are d-separated by U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>■</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Let's considerFigure 15 . 1 :</head><label>151</label><figDesc>Figure 15.1: Example of causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Definition 17 . 1</head><label>171</label><figDesc>The beta distribution with parameters a and b (abbreviated β(a, b)) has density with respect to Lebesgue's measure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>Proposition 17 . 4</head><label>174</label><figDesc>The log-likelihood, ℓ, is a concave function of θ, with ∇ℓ(θ) = E θ (U ) -ŪN(17.6)    and∇ 2 ℓ(θ) = -Var θ (U ) (17.7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>.15) ♦ 17 . 3</head><label>173</label><figDesc>Incomplete observations for graphical models 17.3.1 The EM Algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>18. 1</head><label>1</label><figDesc>Normalizing flows 18.1.1 General concepts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>Figure 18 . 1 :</head><label>181</label><figDesc>Figure 18.1: Basic structure of GANs: W is optimized to improve the prediction problem: "real data" vs. "simulation". Given W , θ is optimized to worsen the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>1 :Figure 19 . 1 :</head><label>1191</label><figDesc>Figure 19.1: A partition tree of the set {a, b, c, d, e, f }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>■</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><figDesc>Algorithm 19.4 (K-means) Let T ⊂ R d be the training set. Start with an initial choice of c 1 , . . . , c p ∈ R d and iterate over the following two steps until stabilization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><figDesc>z 1 (A), . . . , z N (A) denote the columns of Z(A) and zk (A) = z k (A)/|z k (A)|. One has |z k (A) -zl (A)| = 0 if k and l belong to the same cluster and √ 2 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><figDesc>subject to 0 ≤ ξ k ≤ 1 and N j=1 ξ j = p. The optimal solution is obtained by takingξ 1 = • • • = ξ p = 1, since, for any other solution λ j )(1ξ j ) ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><figDesc>Run K-means on (y 1 , . . . , y N ) to determine a partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head>Algorithm 19 . 8 (( 2 )</head><label>1982</label><figDesc>Spectral clustering: version 2)Let S α be an N × N discrepancy matrix. Let p denote the number of clusters. LetP = Id R N -1 N 1 T N /N .(1) Compute Sα = P S α P Compute the eigenvectors of Sα associated with the p -1 smallest eigenvalues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>( 3 )</head><label>3</label><figDesc>Denoting these eigenvectors by e 1 , . . . , e p-1 , define y 1 , . . . , y N ∈ R p-1 by y Run K-means on (y 1 , . . . , y N ) to determine a partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><figDesc>3), L * (p) = min{W α (A 1 , . . . , A p , c 1 , . . . , c p ) : A 1 , . . . , A p partition of T , c 1 , . . . , c p ∈ R},</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head>Figure 19 . 2 :</head><label>192</label><figDesc>Figure 19.2: Example of data transformed using the eigenvectors of the graph Laplacian. Left: Original data. Center: Result of a Kmeans algorithm with three clusters applied to the transformed data (2D projection). Right: Visualization of the cluster labels on the original data.</figDesc><graphic coords="473,85.04,85.04,132.58,99.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_74"><head>Figure 19 . 3 :</head><label>193</label><figDesc>Figure 19.3: Elbow graphs for K-means clustering for two populations generated as mixtures of Gaussian.</figDesc><graphic coords="474,127.71,218.61,176.76,132.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_75"><figDesc>Let a α (x, p) = d α (x, A(x)) and b(x, p) = min{d α (x, A k ) : A k A(x)}. Define the silhouette index of x in the segmentation [170]by s α (x, p) = b α (x, p)a α (x, p) max(b α (x, p), a α (x, p)) ∈ [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_76"><head>Figure 19 . 4 :</head><label>194</label><figDesc>Figure 19.4: Division of the unit square into clusters for uniformly distributed data.</figDesc><graphic coords="476,140.28,85.04,331.46,248.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_77"><head>2 d 2 d L * (p) p 2 d 2 d</head><label>2222</label><figDesc>L * (p -1)p L * (p) -(p + 1) L * (p + 1), and estimate the number of clusters by taking p maximizing γ KL .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_78"><figDesc>Figures figs.<ref type="bibr" target="#b37">19</ref>.5 to 19.7 provide a comparative illustration of some of these indexes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_79"><head>Figure 19 . 5 :Figure 19 . 6 :</head><label>195196</label><figDesc>Figure 19.5: Comparison of cluster indices for Gaussian clusters. First row: original data and ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali ński and Harabasz; silhouette; Sugar and James)</figDesc><graphic coords="479,85.04,339.89,441.93,276.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_80"><head>Figure 19 . 7 :</head><label>197</label><figDesc>Figure 19.7: Comparison of cluster indices for Gaussian clusters. First row: original data and ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali ński and Harabasz; silhouette; Sugar and James).</figDesc><graphic coords="481,85.04,315.58,441.93,316.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_81"><head>Z</head><figDesc>and θ given X with respect to θ, formally4 , P (z|x) = P (z, θ|x)P (θ)dθ ∝ N k=1 P (x k |z k , θ)P (z k |θ)P (θ)dθ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_82"><head>Algorithm 19 .</head><label>19</label><figDesc>11 (Mean-field algorithm for mixtures of Gaussian) (1) : Input: training set (x 1 , . . . , x N ), number of clusters p, prior parameters u, v, τ 2 and a . (2) Initialize variables σ 2 1 , . . . , σ 2 p , m1 , . . . , mp , ã1 , . . . , ãp , gk (j), k = 1, . . . , N , j = 1, . . . , p. (3) Let ζ(j) = N k=1 gk (j), j = 1, . . . , p. (4) Let ρ2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_83"><head>)( 6 )( 7 )</head><label>67</label><figDesc>For j = 1, . . . , p,Let ãi = a + ζ(j), j = 1, . . . , p. For k = 1, . . . , N , j = 1, . . . , p, let gk (j) ∝ exp -1 2 ρ2 |x k -mj | 2 + d σ 2 j + ψ( ãj ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_84"><figDesc>1, . . . , j j + 1 with probability λ λ + k (19.13) 3 If z k+1 = i ≤ j, then replace N i by N i + 1, k by k + 1. 4 If z k+1 = j + 1, let N j+1 = 1, replace j by j + 1 and k by k + 1. 5 If k &lt; N , return to step 2, otherwise, stop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_85"><figDesc>generate only m -1 such pairs of variables and let the last one be equal to c A k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_86"><head>( 3 ) 2 N</head><label>32</label><figDesc>Simulate a new value of σ 2 according to an inverse gamma distribution with parameters u + dN /2 and v + 1k=1 |x kc A k | 2 .(4) Simulate new values for c A , A ∈ A independently, sampling c A according to a Gaussian distribution with mean (1 + σ 2 /(N j τ 2 )) -1 xA and variance (|A|/σ 2 + 1/τ 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_88"><head>In addition to σ 2 ,</head><label>2</label><figDesc>the model is parametrized by the coordinates of e 1 , . . . , e p and the values of λ 1 , . . . , λ p . Introduce the d × p matrix W = [λ 1 e 1 , . . . , λ p e p ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_89"><head>Figure 20 . 1 :</head><label>201</label><figDesc>Figure 20.1: PCA cannot distinguish between the situations depicted in the two datasets.</figDesc><graphic coords="509,105.61,521.35,198.86,149.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_90"><figDesc>p n = 1 (to avoid trivial solutions). Choosing an ordering on the set of indices (p 1 , . . . , p d ) such that p 1 + • • • p d = n, one can stack the coefficients in Q and the monomials (x (1) k ) p 1 . . . (x (d) k ) p d to form two vectors denoted Q (with some abuse of notation) and V (x k ). One can then rewrite the problem of determining Q as minimizing Q T ΣQ subject to |Q| 2 = 1, where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_91"><head>S</head><figDesc>τ (A) = U S τ (∆)V T when A = U ∆V T is a singular value decomposition of A.Proposition 20.8 Let us assume without loss of generality that N ≥ d. The function Z → γ|X c -Z| 2 + |Z| * is minimized by Z = S 1/2γ (X ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_92"><head>Algorithm 20 . 3 (</head><label>203</label><figDesc>SAEM for probabilistic ICA)Initialize the algorithm with parameters A, σ 2 . Define a sequence of decreasing steps, γ t .Let, for k = 1, . . . , N , b k = 0 and S k = Id. Iterate the following steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_93"><head>4 .</head><label>4</label><figDesc>Replace A by Ã and Y by Ỹ , iterating until numerical convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_94"><head>Algorithm 20 . 5 ( 2 .</head><label>2052</label><figDesc>NMF, logarithmic cost) 1. Fix p &gt; 0 and let X be the N by d matrix containing the observed data.Initialize the procedure with matrices Y and A, respectively of size N by p and d by p, with positive coefficients.3. At a given stage of the algorithm, let A and Y be the current matrices decomposing X .4. Let Ã be the matrix with coefficients ã</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_95"><figDesc>Instead of considering configurations b = (b k (j), i = 1, . . . , d, k = 1, . . . , N ) we may alternatively consider the family of sets S = (G k , C k , 1 ≤ k ≤ N ). Such a family must satisfy the property that the sets G k and C k are non-intersecting, C k ⊂ U k and G l ∩ G k = ∅ for l &lt; k. It provide a unique configuration b by letting b k (j) = 1 if and only if j ∈ G k ∪ C k . We will let, in the following, q k = |G k | and p k = |U k |. The probability Q(b) can be re-expressed in terms of S, letting (with some abuse of notation)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_96"><head>Algorithm 20 . 6 (</head><label>206</label><figDesc>Indian buffet process) 1. Initialization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_97"><figDesc>+ c) -1 ds = -c log(1 + t c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_98"><head>for</head><figDesc>T j Y = 0 for j = 1, . . . , m. Introduce the matrix L = L + m k=1 e k e T k which is positive definite. Our minimization problem is then equivalent to minimizing 2trace(Y T LY ) -2trace(U T Y ), subject to e T j Y = 0 for j = 1, . . . , m. The derivative of this function is 4 LY -2U so that an optimal Y must satisfy 4 LY -2U + Lagrange multipliers µ 1 , . . . , µ m ∈ R p . This shows that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_99"><figDesc>exit and return Y ′ .6. Return to step 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_100"><head>Figure 21 . 1 :</head><label>211</label><figDesc>Figure 21.1: Left: Multidimensional scaling applied to a 3D curve embedded in a 10dimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies the one-dimensional nature of the data.</figDesc><graphic coords="556,71.09,71.09,226.76,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_101"><head>( 1 )</head><label>1</label><figDesc>kl = |x kx l | if k ∼ c l and d (1) kl = +∞ otherwise (and also let d (1) kk = 0). Then, given d (n-1) , define</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_102"><head>Figure 21 . 2 :</head><label>212</label><figDesc>Figure 21.2: Left: Multidimensional scaling applied to a 3D curve embedded in a 10dimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies the one-dimensional nature of the data.</figDesc><graphic coords="558,71.09,71.09,226.76,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_103"><head>Algorithm 21 . 2 (( 1 )</head><label>2121</label><figDesc>Local linear embedding)The input of the algorithm is (i) Either a training set T = (x 1 , . . . , x N ), or its Gram matrix S containing all inner products x T k x l (or more generally inner products in feature space), or a dissimilarity matrix D = (d kl ).(ii) An integer c for the graph construction.(iii) An integer p for the target dimension.If not provided in input, compute the Gram matrix S and distance matrix D (using (21.2) and (21.4)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_104"><head>( 2 )</head><label>2</label><figDesc>Build the c-nearest-neighbor graph associated with the distances. Let N k be the set of neighbors of k, with c k = |N k |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_105"><figDesc>∈ N k stacked in a vector ρ k ∈ R c k by solving(21.6).(4) Form the matrix W with entriesw ll ′ = N k=1 ρ (l) k ρ (l ′ )k with ρ extended so that ρ(k) k = -1 and ρ (l) k = 0 if l k and l N k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_106"><head>Figure 21 . 3 :</head><label>213</label><figDesc>Figure 21.3: Local linear embedding with target dimension 3 applied to the data in fig. 21.1 and fig. 21.2.</figDesc><graphic coords="561,91.66,71.09,226.76,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_107"><head>2 (</head><label>2</label><figDesc>kand N k=1 ρ k = 0. With this notation, we can interpret Laplacian embedding as the minimization of subject to previous orthogonality constraints). In other terms, y 1 , . . . , y N are determined so that the linear relationships ρ kk y k =are satisfied, which is similar to the LLE condition, without the requirement that ρ k (k) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_108"><figDesc>∂ t π k (l) = π k (l)∂ t log π k (l), we have ∂ t H(π k ) = N l=1 (d kl log π k (l))π k (l) -dk N l=1 π k (l) log π k (l).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_109"><figDesc>ϕ a,b (y, y ′ ) = 1 1 + a|yy ′ | b .The parameters a and b are adjusted so that ϕ a,b provides a differentiable approximation of the functionψ ρ 0 (y, y ′ ) = exp(-max(0, |yy ′ |ρ 0 ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_110"><figDesc>The representation y is optimized by minimizing the "fuzzy set cross-entropy"C(µ∥ν(•, y)) = (k,l)∈E µ(k, l) log µ(k, l) ν(k, l|y) + (1µ(k, l)) log 1µ(k, l) 1ν(k, l|y)or, equivalently, maximizing (using, for short, ϕ = ϕ a,b )F(y) = (k,l)∈E (µ(k, l) log ν(k, l|y) + (1µ(k, l)) log(1ν(k, l|y))) = (k,l)∈E(µ(k, l) log ϕ(y k , y l ) + (1µ(k, l)) log(1ϕ(y k , y l )))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_111"><figDesc>∂ y k F(y) =2 N l=1 µ(k, l)∂ y k log ϕ(y k , y l ) + 2 N l=1 (1µ(k, l))∂ y k log(1ϕ(y k , y l )) =2 N l=1 µ(k, l)∂ y k log ϕ(y k , y l ) 1ϕ(y k , y l ) + 2 N l=1∂ y k log(1ϕ(y k , y l )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_112"><figDesc>H k (y, ξ, ξ ′ ) = 2 N l=1 ξ kl ∂ y k log ϕ(y k , y l ) 1ϕ(y k , y l ) ξ kl ξ ′ kl ′ ∂ y k log(1ϕ(y k , y l ′ )).Then, if one takes c k = 1/(ϵ l µ(k, l)) one hasE(H k (y, ξ, ξ ′ )) = ∂ y k F(y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_113"><figDesc>y k log ϕ(y k , y l ) 1ϕ(y k , y l ) + c k m j=1 ∂ y k log(1ϕ(y k , y l ′ ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_114"><figDesc>r(Y , f (X)))where r : R Y × R Y → [0. + ∞) is a risk function.A training set is a family T = ((x 1 , y 1 ), . . . , (x N , y N )) ∈ (R X × R Y ) N , the set T of all possible training sets therefore being the set of all finite sequences in R X × R Y . A training algorithm can then be seen as a function A : T → F which associates to each training set T a function A(T ) = fT .Given T ∈ T , The training set error associated to a functionf ∈ F is RT (f ) = 1 |T | (x,y)∈T r(y, f (x)))and the in-sample error associated to a learning algorithm is the function T → E T ∆ = RT ( fT ). Fixing the size (N ) of T , one also considers the random variable T with values in T distributed as an N -sample of the distribution of (X, Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_115"><figDesc>N (θ T ŪT -C(θ)) ϕ j dm j(θ)    where ŪT = (U (z 1 ) +• • • + U (z N ))/N .Consider the maximum likelihood estimator θj within M j , maximizing ℓ(θ, ŪT ) = θT  ŪT -C(θ) over M j . Then one hasℓ(θ, ŪT ) = ℓ( θj , ŪT ) + 1 2 (θ -θj ) T ∂ 2 θ ℓ( θj , ŪT )(θ -θj ) + R j (θ, θj )|θ -θj | 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_116"><head>H 2 (</head><label>2</label><figDesc>P ) = -x∈Ω p x log 2 p x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_117"><figDesc>(x, y | θj , M j ) =max θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_118"><figDesc>(y k | x k ; θ, M j )log 2 π(θ | M j ) -k j i=1 log 2 (δ ij ).Let θ(j) be the parameter that maximizesL(θ | M j ) = N k=1 log 2 ϕ(y k | x k ; θ, M j ) + log 2 π(θ | M j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_119"><head>log 2</head><label>2</label><figDesc>c i (N | M j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_120"><head>1 P≤ 1</head><label>11</label><figDesc>random variables are such that M(λ) &lt; ∞ for all λ ∈ R. Indeed, for(|X| &gt; λ -1 log z)dz</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_121"><head>0 P 0 P 0 P 0 P 0 P 0 P 2 dv = 2λσ e λ 2 σ 2 2 √≤ 4λ 2 σ 2 e λ 2 σ 2 2</head><label>000000222</label><figDesc>(λX &gt; g + (z))dz + ∞ (λX &lt; g -(z))dzThe change of variable u = g + (z) in the first integral is equivalent to u &gt; 0, ϕ(u) = z with dz = (e u -1)du. Similarly, u = -g -(z) in the second integral gives u &gt; 0, ϕ(-u) = z and dz = (1e -u )du so thatE(ϕ(λX)) = ∞ (λX &gt; u)(e u -1)du + ∞ (λX &lt; -u)(1e -u )du ≤ ∞ (λ|X| &gt; u)(e ue -u )du.(Using the fact that max(P(λX &gt; u), P(λX &lt; -u)) ≤ P(λ|X| &gt; u).) We have∞ (λ|X| &gt; u)(e ue -u )du ≤ 2 +∞ 0 (e ue -u )eve -λσ v )e -v 2 2π(Φ(-σ λ) -Φ(σ λ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_122"><head>Proposition 22 . 5</head><label>225</label><figDesc>Let m = E(X) and assume that for some constant b, one has X ≤ b with probability one. Then, for any σ 2 &gt; 0 such that var(X) ≤ σ 2 , one hasE(e λX ) ≤ e λm (bm) 2 (bm) 2 + σ 2 e -λσ 2 (b-m) + σ 2 (bm) 2 + σ 2 e λ(b-m) (22.6)for any λ ≥ 0.Proof There is no loss of generality in assuming that m = 0 and λ = 1, in which case one must show thatE(e X ) ≤ b 2 b 2 + σ 2 e -σ 2 b + σ 2 b 2 + σ 2 e b(22.7)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_123"><head>■ 2 .</head><label>2</label><figDesc>If F(λ) denotes the right-hand side of(22.6), we have, for m ≤ u &lt; b,M * X (t) ≥ sup λ≥0 (λulog F(λ))and we now estimate this lower bound. Maximizing λylog F(λ) is equivalent to minimizingλ → (bm) 2 e -λ(σ 2 +(u-m)) b-m + σ 2 e λ(b-u) (bm) 2 + σ Introduce the notation ρ = σ 2 /(bm) 2 , µ = λ(bm) and x = (um)/(bm), so that the function to minimize is µ →e -µ(ρ+x) + ρe µ(1-x) 1 + ρ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_124"><figDesc>5. ThenP( XN&gt; m + t) ≤ exp -N ρ + x 1 + ρ log ρ + x ρ + 1x 1 + ρ log(1x) (22.8) with x = t/(bm) and ρ = σ 2 /(bm) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_125"><head>σ 2 b 2 +</head><label>2</label><figDesc>.7) implies log E(e X ) ≤ b 2 b 2 + σ 2 e -σ 2 b + σ 2 b 2 + σ 2 e bσ 2 (e bb -1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_126"><head>σ 2 b 2 +</head><label>2</label><figDesc>σ 2 (e bb -1) = b 2 b 2 + σ 2 σ 4 b 2 ϕ(-σ 2 /b) + σ 2 b 2 + σ 2 b 2 ϕ(b) ≤ σ 4 b 2 + σ 2 + σ 2 b 2 b 2 + σ 2 ϕ(b) = σ 2 b 2 (e bb -1)This shows that log E(e λX ) ≤σ 2 b 2 (e λbλbt/σ 2e λb + λb + 1) = σ 2 b 2 h(bt/σ 2 ) where h(u) = (1 + u) log(1 + u)u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_127"><head>Corollary 22 . 8</head><label>228</label><figDesc>Assume that X satisfy the conditions of proposition 22.5. Then, for t &gt; 0,P( XN &gt; m + t) ≤ exp -N σ 2 (bm) 2 h (bm)t σ 2 (22.9)where h(u) = (1 + u) log(1 + u)u.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_128"><figDesc>2σ 2 + 2t(bm)/3 . (22.10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_129"><head>Proposition 22 . 8 ( 22 . 11 )</head><label>2282211</label><figDesc>10 Let X be a random variable taking values in the interval [a, b]. Let m = E(X). Then E(e λX ) ≤ bm ba e λa + ma ba e λb ≤ e λm e λ 2 (b-a) 2for all λ ∈ R.Proof We first note that, if X takes values in [a, b], then var(X) ≤ (b -m)(m-a) (using σ 2 = (bm)(ma) in (22.6)). To prove the upper bound on the variance, introduce the functiong(x) = (xa)(xb) so that g(x) ≤ 0 on [a, b]. Noting that one can write g(x) = (xm) 2 + (2mab)(xm) + (am)(bm), we have E(g(X)) = var(X) -(bm)(ma) ≤ 0,which proves the inequality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_130"><figDesc>additional work. Letting u = (ma)/(ba), α = λ(ba) and taking logarithms, we need to prove that log(1u + ue α )uα ≤ α 2 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_131"><head>Corollary 22 . 11 (Proof 2 8</head><label>22112</label><figDesc>Hoeffding Inequality) If X 1 , . . . , X N are independent, taking values, respectively, in intervals of length, c 1 , . . . , c N andY = X 1 + • • • + X N , then P(Y &gt; E(Y ) + t) ≤ exp -We have, by proposition 22.10, for any λ &gt; 0P(Y &gt; E(Y ) + t) ≤ e λt-N k=1 M X k (λ) ≤ e -(λt-λ |c| 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_132"><head>Proposition 22 .λ 2 c 2 k 8 .λ 2 c 2 k 8 E 2 k</head><label>22882</label><figDesc>12 Let X 1 , . . . , X N , Z 1 , . . . , Z N be two sequences ofN random variables such that E(Z k | X 1 , Z 1 , . . . , X k-1 , Z k-1 ) = m k is constant and |Z km k | ≤ c k for some constants c 1 , . . . , c N . Then P(Y &gt; E(Y ) + t) ≤ e -2t 2 /|c| 2 with Y = Z 1 + • • • + Z N and |c| 2 = N k=1 c 2 k .Proof Proposition 22.10 applied to the conditional distribution implies that, for λ ≥ 0:log E(e λ(Z k -m k ) | X 1 , Z 1 , . . . , X k-1 , Z k-1 ) ≤ log E(e λ|Z k -m k | | X 1 , Z 1 . . . , X k-1 , Z k-1 ) ≤ Let S k = k j=1 (Z jm j ). Then E(e λS k ) = E(e λS k-1 E(e λ(Z k -m k ) | X 1 , Z 1 , . . . , X k-1 , Z k-1 )) ≤ eand the result follows from Markov's inequality optimized over λ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_133"><head>If</head><figDesc>A is a finite subset of R, we let F (A) denote the set {f | A : f ∈ F } of restrictions of elements of F to the set A. As a convention, we let F (∅) = {f ∅ }, containing the socalled empty function. Since F only contains binary functions, we have |F (A)| ≤ 2 |A| . If x 1 , . . . , x M ∈ R, we let, with a slight abuse of notation,F (x 1 , . . . , x M ) = F (A)where A = {x i , i = 1, . . . , M}. This provides the number of possible splits of a training set T = (x 1 , . . . , x M ) using classifiers in F . Fixing in this section a random variable X, we letS F (M) = E(|F (X 1 , . . . , X M )|)where the expectation is taken over all M i.i.d. realizations from X. We also let S * F (M) = max{|F (A)| : A ⊂ R, |A| ≤ M}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_134"><figDesc>(E T ′ (f ) -E T (f )) = sup f ∈F N k=1 (r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/Nhas the same distribution as supf ∈F N k=1 ξ k (r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_135"><head>8 N</head><label>8</label><figDesc>r(Y k , f (X k ))r(Y ′ k , f (X ′ k )))/N ≥ t/2 T , T ′ ≤ e -2N (t/2) 2 /4 = e -N t 2 /8and taking expectation over T and T ′ yieldsP F (2N )e -Nt 2 /8 . Equation (22.20) is then obtained from letting δ = 2S F (2N )e -N t 2 /8 so that t = log 2S F (2N ) δ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_136"><head>Proposition 22 .■</head><label>22</label><figDesc>20 (Sauer-Shelah's lemma) If D is the VC-dimension of F , then, for N ≥ D, the statement of the proposition derives from the standard upper bound We can therefore state a corollary to theorem 22.16 for model classes with finite VC-dimension. Corollary 22.21 Assume that VC-dim(F ) = D &lt; ∞. Then, for t ≥ √ 2/N and N ≥ D, f ) -E T (f )) ≤ 8 N D log eN D + log 2 δ ≥ 1δ. (22.23)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_137"><head>Proposition 22 .</head><label>22</label><figDesc>22 Let R = R d and F = x → sign(a 0 + b T x) :β 0 ∈ R, b ∈ R d . Then VC-dim(F ) = d + 1.ProofLet us show that no set of d +2 points can be shattered by F . Use the notation x = (1, x T ) T and β = (a 0 , b T ) T , and consider d + 2 points x 1 , . . . , x d+2 . Then x1 , . . . , xd+2 are linearly dependent and one of them, say, xd+2 can be expressed as a linear combination of the others. Write xd+2 = d+1 k=1 α k xk .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_138"><head>Theorem 22. 23 VC</head><label>23</label><figDesc>-dim(F (L, (U i ), (W i ), p)) = O( LW L log(pU )).whereU = U 1 + • • • + U L andNote that p = 2 for ReLU networks. Theorem 7 in Bartlett et al.[21]  also provides a more explicit upper bound, namelyVC-dim(F (L, (U i ), (W ), p)) ≤ L + LW L log 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_139"><head>H 2 (</head><label>2</label><figDesc>ψ k ) -(N -1)H 2 (ψ) ≥ 0. (22.25)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_140"><head>H 2 (</head><label>2</label><figDesc>Y (k) ) -(N -1)H 2 (Y ) ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_141"><figDesc>g∈G g(Z) ≥ t) ≤ P(sup g∈G g(Z) ≥ tϵ) (22.28) ≤ N (G, ρ ∞ , ϵ) sup g∈G P(g(Z) ≥ tϵ). Now, if each function in G satisfies a concentration inequality, say, P(g(Z) ≥ u) ≤ e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_142"><figDesc>g∈G g(Z) ≥ t) ≤ N (G, ρ ∞ , ϵ) e -(t-ϵ) 2 2µ(G) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_143"><head>Proposition 22 . 28 4 .</head><label>22284</label><figDesc>For all ϵ &gt; 0, one hasP sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2 N (F , ϵ/2, N )e -N (t/2-ϵ) 2(22.29)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_144"><head>4 = e -N u 2 2 and 2 .</head><label>422</label><figDesc>Applying Hoeffding's inequality, we have, for u &gt; 0 and using the fact thatc k ∈ [-1, 1] P(g f (Z) &gt; u | T , T ′ ) ≤ e -2N u 2the discussion preceding the theorem yields the fact that, for any ϵ &gt; 0:P(supf ∈F g f (Z) &gt; t/2 | T , T ′ ) ≤ N (G, ϵ, ρ ∞ )e -N (t/2-ϵ) 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_145"><head>2 ( 22 . 34 )</head><label>22234</label><figDesc>f ) -E T (f )) ≥ t ≤ 2 N (F , ϵ/2, N )e -N (t/2-ϵ) 2which is valid for all ϵ &gt; 0.■ One can retrieve the bound obtained in theorem 22.16 using the obvious fact thatN (F , ϵ, ρ A ) ≤ |F (A)|, for any A ⊂ R, so that P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2S(F , 2N )e -N (t/2-ϵ) 2 2for any ϵ &gt; 0, and letting ϵ go to zero,P sup f ∈F (R(f ) -E T (f )) ≥ t ≤ 2S(F , 2N )e -N t 2 8 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_146"><head>Proposition 22 . 29</head><label>2229</label><figDesc>Assume that G is a parametric family of functions, so that G = {g θ , θ ∈ Θ} where Θ ⊂ R m . Assume also that, for some constantC, ρ ∞ (g θ , g θ ′ ) ≤ C|θ -θ ′ | for all θ, θ ′ ∈ Θ. Let G (M) = {g θ : θ ∈ Θ, |θ| ≤ M}. Then N (G, ρ ∞ , ϵ) ≤ 1 + 2CM ϵ mProof Letting ρ denote the Euclidean distance in R m , our hypotheses imply thatN (G (M) , ρ ∞ , ϵ) is bounded by N (B M , ρ, ϵ/C) where B M is the ball with radius M in R m . Now, if θ 1 , . . . , θ n is an α-covering of B M , then θ 1 /M, . . . , θ n /M is an (α/M)covering of B 1, which shows (together with a symmetric argument) that N (B M , ρ, α) = N (B 1 , ρ, α/M) and we getN (G (M) , ρ ∞ , ϵ) ≤ N (B 1 , ρ, ϵ/MC)and we only need to evaluate N (B 1 , ρ, α) for α &gt; 0. Using proposition 22.27, one can instead evaluate M(B 1 , ρ, α). So let A be an α-net in B 1 . Then x∈A B ρ (x, α/2) ⊂ B ρ (0, 1 + α/2) and, since the sets in the union are disjoint, x∈A volume(B ρ (x, α/2)) = |A|volume(B ρ (0, α/2)) ≤ volume(B ρ (0, 1 + α/2)) .Letting C m denote the volume of the unit ball in R m , this shows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_147"><head>Theorem 22 .</head><label>22</label><figDesc>30 Let Z be a bounded convex subset of R d with non-empty interior. Forp ≥ 1 and f ∈ C p (Z), let ∥f ∥ p,∞ = max |D k (f (x)| : k = 0, . . . , p, x ∈ Z .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_148"><head>1 Pt 2 t 2 k 2</head><label>12</label><figDesc>)g 0 (Z) &gt; tS n ) ≤ P(∃g ∈ G n , ∃k ≤ n : π k (g)(Z)π k-1 (g)(Z) &gt; tt k ρ(π k (g), π k-1 (g))) ≤ P(∃k ≤ n, ∃g ∈ G k , g ′ ∈ G k-1 : g(Z)g ′ (Z) &gt; tt k ρ(g, g ′ )) g∈G k ,g ′ ∈G k-(g(Z)g ′ (Z) &gt; tt k ρ(g, g ′ )) If one takes N k = 2 2 k , which satisfies N k N k-1 = 2 2 k +2 k-1 ≤ N k+1, and t k = 2 k/2 , one finds that P(sup g∈G n g(Z)g 0 (Z) &gt; tS n ) ≤ 2 n k=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_149"><figDesc>2 ρ(π k (g), π k-1 (g)) 2 (ρ(g, G k ) + ρ(g, G k-1 )) 2 ρ(g, G k )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_150"><head>2 2S 2 ( 22 . 41 )</head><label>222241</label><figDesc>have proved the following result<ref type="bibr" target="#b206">[188]</ref>.Theorem 22.31 Let G 0 , G 1 , . . . be a family of subsets of G satisfying(22.38)  and(22.39)   and such that G 0 = {g 0 } and|G n | ≤ 2 2 n for n ≥ 0. Let S = 2 sup g∈G ∞ n=0 2 n/2 ρ(g, G n ) (22.40)Then, for t &gt; S 1 + 8 log 2,)g 0 (Z) &gt; t) ≤ Ce -t with C = 2 ∞ k=1 e -2 k-2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_151"><figDesc>If S is given by(22.40), we haveS = 2 sup ∞ n=0 2 n/2 ρ(g, G n ) : g ∈ G ≤ 2 ∞ n=0 2 n/2 sup{ρ(g, G n ) : g ∈ G}Take G n achieving the minimum in the entropy number e(G, ρ, 2 2 n ). Then,(22.41)   holds with S replaced by G, ρ, ϵ)dϵ,(22.42)    which is known as Dudley's metric entropy of the space (G, ρ). We haveh(G, ρ) = N (ϵ)dϵ. If ϵ ∈ [e(22 n-1 ), e(2 2 n )), we have N (ϵ) &gt; 2 2 n so that h(G, ρ) ≥ e(2) log 3 + ∞ n=1 2 n/2 (e(2 2 n )e(2 2 n-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_152"><figDesc>π λ (y) =e λy e -λ + e λ . Now, if F is a class of real-valued functions, we can define the risk functionr(y, f (x)) = log 1 π f (x) (y).Since |∂ λ log π λ (y)| = |ytanh λ| ≤ 2 for y ∈ {-1, 1}, we have|r(y, f (x))r(y, f ′ (x))| ≤ 2|f (x)f ′ (x)|</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_153"><head>ρ</head><figDesc>∞ (r(y, f (x)), r(y, f ′ (x))) ≤ |a 0a ′ 0 | + U |bb ′ |and a small modification of the proof of proposition 22.29 shows thatN (F , ρ ∞ , ϵ) ≤ 1 + 4CU ϵwhich is proved exactly the same way as(22.21)  in theorem 22.16, and we skip the argument.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_154"><head>T</head><figDesc>,T ′ (ξ 1 , . . . , ξ N ) &gt; t 2 ≤ |F|e -N t 2 /8 , ■ which concludes the proof, since, by proposition 22.27, |F| ≤ N ∞ (γ/2, 2N ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_155"><head>Definition 22 .</head><label>22</label><figDesc><ref type="bibr" target="#b53">35</ref> Let F be a family of functions f : R → [-1, 1] and A a finite subset of R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_156"><head>Theorem 22 . 37 2 D</head><label>22372</label><figDesc>Let γ &gt; 0 and assume that F has P γ/4 -dimension D &lt; ∞. Then,N ∞ (γ, N ) ≤ 2 16N γ log(4eN /(Dγ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_157"><head>|A|r 2 )</head><label>2</label></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_158"><head>Step 2 .</head><label>2</label><figDesc>The next step uses a discretization scheme to extend the previous result to functions with values in [-1, 1]. More precisely, given f : R → [0, 1], and η &gt; 0, letf η (x) = max{k ∈ N : 2kη -1 ≤ f (x)} which takes values in {0, . . . , r} for r = ⌊η -1 ⌋. If F is a class of functions with values in [-1, 1], define F η = {f η : f ∈ F }.With this notation, the following holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_159"><figDesc>(a) For all γ ≤ η:S-dim(F η ) ≤ P γ -dim(F ) (b) For all ϵ ≥ 4η and A ⊂ R: M(F (A), ρ ∞ , ϵ) ≤ M ∞ (F η (A), ρ ∞ , 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_160"><head>ForStep 3 ..</head><label>3</label><figDesc>(b), we deduce from the definition of f η that |f η (x) -f η (x)| &gt; (2η) -1 |f (x)f (x)| -1 so that, if ϵ = 4η, |f (x) -f (x)| ≥ ϵ implies |f η (x) -f η (x)| &gt; 1, or, equivalently |f η (x) -f η (x)| ≥ 2.We can now conclude. Taking γ &gt; 0, we have, if|A| = N N (F (A), ρ ∞ , γ) ≤ M(F (A), ρ ∞ , γ) ≤ M(F γ/4 (A), ρ ∞ , 2) Since the maximum of N (F (A), ρ ∞ , γ) over A with cardinality N is N ∞ (γ, N ), the proof is complete.■ One can use this result to evaluate margin bounds on linear classifiers with bounded data. Let R be the ball with radius Λ in R d and consider the model class containing all functions f (x) = a 0 + b T x with a 0 ∈ [-Λ, Λ] and b ∈ R d , |b| ≤ 1. Let A = {x 1 , . . . , x N } be a finite subset of R. Then, F P γ -shatters A if and only if there exists g 1 , . . . , g N ∈ R such that, for any sequences ξ = (ξ 1 , . . . , ξ N ) ∈ {-1, 1} N , there exists a ξ 0 ∈ [-Λ, Λ] and b ξ ∈ R d , |b ξ | ≤ 1 with ξ k (a ξ 0 + (b ξ ) T x kg k ) ≥ γ for k = 1, . . . , N . Summing over N , we find that This shows that, for any sequence ξ 1 , . . . , ξ N , Applying the same inequality after changing the signs of ξ 1 , . . . , ξ N yields N γ ≤ N γ +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_161"><head>9 . 22 . 6 . 3</head><label>92263</label><figDesc>f ∈F (E T 1 (f ) -E T 2 (f ))One can then use McDiarmid's inequality (theorem 22.13) after noticing that, lettingz k = (x k , y k ) for k = 1, . . . , N , max z 1 ,...,z N ,z ′ k Φ(z 1 , . . . , z N ) -Φ(z 1 , . . . , z k-1 , z ′ k , z k+1 , . . . , z N ) f ) -E T (f )) ≥ C T + ϵ) ≤ e -2N ϵ 2Rademacher complexityWe now extend the previous definition by computing discrepancies over random two-set partitions of the training set, which have equal size in average. This leads to the empirical Rademacher complexity of the function class. Let ξ 1 , . . . , ξ N be a sequence of Rademacher random variables (equal to -1 and +1 with equal probability 1/2). Then, the (empirical) Rademacher complexity of the training set T for the model class F is rad(T ) = E sup</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_162"><head>Proposition 22 .</head><label>22</label><figDesc>40 Let F be a function class such that D = VC-dim(F ) &lt; ∞. Then rad(T ) ≤ 3 √ N 2D log(eN /D) .Proof One has, using Hoeffding's inequalityP sup f ∈F 1 N N k=1 ξ k r(y k , f k ) &gt; t ≤ |F (T )| sup f ∈F P 1 N N k=1 ξ k r(y k , f k ) &gt; t ≤ |F (T )|e -N t 2 /2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_163"><head>= 0 P 1</head><label>01</label><figDesc>Assume that the loss function r takes its values in [0, 1]. Recall that KL(µ∥π) is the Kullback-Leibler divergence from µ to π, defined byKL(µ∥π) = F log(ϕ(f ))ϕ(f )dπ(f )if µ has a density ϕ with respect to π and +∞ otherwise. Then, the following theorem holds.Theorem 22.43 (McAllester<ref type="bibr" target="#b146">[128]</ref>) With the notation above, for any fixed probability distribution π ∈ Q,P sup µ∈Q ( R(µ) -ĒT (µ)) &gt; t + KL(µ∥π) 2N ≤ 2N e -Nt . (22.46) Taking t = log(2N /δ)/2N , the theorem is equivalent to the statement that, with probability 1δ, one has R(µ) -ĒT (µ) ≤ log 2N /δ + KL(µ∥π) 2N . (22.47)Proof We first show that, for any probability distributions π, µ on F , and any function H on F ,F H(f )dµlog F e H(f ) dπ ≤ KL(µ∥π) .Indeed, assume that µ has a density ϕ with respect to π (otherwise the upper bound is infinite) and letKL(µ∥ϕ H π) ≥ 0,which proves the result (and also shows that one can only have equality when ϕ = ϕ H π-almost surely.)Let χ(u) = max(u, 0) 2 . We can use this inequality to show that, for any probabilityQ ∈ Q and λ &gt; 0, λχ( R(µ) -ĒT (µ)) ≤ λ F χ(R(f ) -E T (f ))dµ(f ) ≤ KL(µ∥π) + log F e λχ(R(f )-E T (f )) dπwhere we have applied Jensen's inequality to the convex function χ. This yields e λχ( R(µ)-ĒT (Q)) ≤ eKL(µ∥π)   F e λχ(R(f )-E T (f )) dπ.Hoeffding's inequality implies that, for all f ∈ F and t ≥ 0P(χ(R(f ) -E T (f )) &gt; t) = P(R(f ) -E T (f ) &gt; √ t) ≤ e -2N tso thatE e λχ(R(f )-E T (f )) = ∞ (λχ(R(f ) -E T (f )) &gt; log t)dtFrom this and Markov's inequality, we get, for any λ &gt; 0:P(sup µ∈Q χ( R(µ) -ĒT (µ)) &gt; t + KL(µ∥π)/λ) ≤ e -λt 1 + λ e λ-2N -1 λ -2N . Taking λ = 2N yields P(sup µ∈Q χ( R(µ) -ĒT (µ)) &gt; t + KL(µ∥π)/2N ) ≤ 2N e -2N t ,which impliesP sup µ∈Q R(µ) -ĒT (µ) &gt; t + KL(µ∥π)/2N ≤ 2N e -2N t ,concluding the proof.■Remark 22.44 Note that the proof, which follows that given in Audibert and Bousquet<ref type="bibr" target="#b33">[15]</ref>, provides a family of inequalities obtained by taking λ = 2N /c in the final step, with c &gt; 1. In this case1 + λ e λ-2N -1 λ -2N ≤ ) -ĒT (µ) &gt; t + cKL(µ∥π)/2N ≤ c c -1 2N e -2N t . ♦ Remark 22.45 One special case of theorem 22.43 is when π is a discrete probability measure supported by a subset F 0 of F and µ corresponds to a deterministic predictor optimized over F 0 , and is therefore a Dirac measure on some element f ∈ F 0 .Because δ f has density ϕ(g) = 1/π(g) if g = f and 0 otherwise with respect to π, we have KL(δ f ∥π) =log π(f ) and theorem 22.43 implies that, with probability larger than 1δ, R(f ) -E T (f ) ≤ log 2N /δlog π(f ) 2N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_164"><figDesc>-based method that requires the minimization ofẼT (f ) = (E T (f ) -E T ( f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="480,85.04,108.91,441.93,222.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="480,85.04,332.69,441.92,299.79" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Unless mentioned otherwise, all matrices are assumed to be real.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Upper-semi continuous is sufficient.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>∝ is the notation for "proportional to"</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>⌈x⌉ is the smallest integer larger than x (ceiling).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4"><p>All vector spaces in these notes will be real, and will therefore only be referred as vector spaces.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>Note that we are using double bars for the norm in H, which, in most applications, is infinite dimensional</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_6"><p>The convolution between two absolutely integrable functions f and g is defined byf * g(u) = R d f (z)g(uz) dz</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_7"><p>This part of the proof uses some measure theory.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_8"><p>Their computation is feasible unless N is very large, and the matrix inversion in Newton's iteration also requires d to be not too large.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>In this section only, the notation x does not refer to (1, x T ) T .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_10"><p>The operator A + ρId H is invertible as soon as A is symmetric positive semi-definite.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_11"><p>Note that, even if the training data is linearly separable, there are generally samples that are on the right side of the hyperplane, but at a distance to the hyperplane strictly lower that the "nominal margin" C = 1/|b|. This is due to our relaxation of the original problem of finding a separating hyperplane with maximal margin.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_12"><p>If f and g are representable as trees, f + g can be represented as a tree whose depth is the sum as those of the original trees, simply by inserting copies of g below each leaf of f .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_13"><p>For every z and every ϵ &gt; 0, there exists z ′ &lt; z such that for allz ′′ ∈ [z ′ , z), |F(z ′′ ) -F(z -0)| &lt; ϵ.Moreover, taking any y ∈ (z ′ , z), there exists y ′ &lt; y such that for all y ′′ ∈ [y ′ , y), |F(y ′′ ) -F(y -0)| &lt; ϵ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_14"><p>We will assume in this chapter that B is a complete metric space with a dense countable subset, with the associated Borel σ -algebra.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_15"><p>The "almost everywhere" statement a priori depends on A, but can be made independent of it under the mild assumption (that we will always make) that B has a countable basis of open sets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_16"><p>In the general case, R X , R Y , . . . are metric spaces with a countable dense subset with σ -algebras S X , S Y , . . .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_17"><p>|u-√ hf (x-√ hu)| 2 dy,</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_18"><p>We still call the method K-medoids rather than p-medoids, to keep the name universally used in the literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_19"><p>(x-c z ) T Σ -1 z (x-c z )</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_20"><p>Recall that Z ⪰ 0 means that Z is positive definite, while Z ≥ 0 indicates that all its entries are non-negative.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_21"><p>Two nodes x and y are connected in the graph if there is a sequence z 0 , . . . , z n in T such that z 0 = x, z n = y and β(z i , z i-1 ) &gt; 0 for i = 1, . . . , n. This provides an equivalence relation and equivalent classes are called connected components.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_22"><p>The symbol ∝ means "equal up to a multiplicative constant".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_23"><p>A Hilbert space is an inner-product space which is complete for its norm. A separable Hilbert space must have a dense countable subset, which, in particular, implies that it has orthonormal bases.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>So we have the equivalence:</head><p>(X Y | Z) P ⇔ ∀z : P(Z = z) &gt; 0 ⇒ (X Y ) P(|Z=z) .</p><p>Absolute independence is like "independence conditional to no variable", and we will use the notation ∅ for the "empty" random variable that contains no information (for example, a set-valued random variable that always returns the empty set, or any constant variable). So we have the tautology</p><p>Note that, dealing with discrete variables, all previous definitions automatically extend to groups of variables: for example, if Z 1 , Z 2 are two discrete variables, so is Z = (Z 1 , Z 2 ) and we immediately obtain a definition for the conditional independence of X and Y given Z 1 and Z 2 , denoted (X Y | Z 1 , Z 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="13.1.2">Fundamental properties</head><p>Proposition 13.5 below lists important properties of conditional independence that will be used repeatedly in this chapter. Before stating this proposition, we need the following definition. <ref type="bibr">Definition 13.4</ref> One says that the joint distribution of the random variables (X 1 , . . . , X N ) is positive if there exists subsets Rk ⊂ R X k , k = 1, . . . , N such that P(X k ∈ Rk ) = 1 and:</p><p>Note that the condition implies P(X k = x k ) &gt; 0 for all x k ∈ Rk , so that Rk = {x k ∈ R X k : P(X k = x k ) &gt; 0}, i.e., Rk is the support of P X k . One can interpret the definition as expressing the fact that any conjunction of events for different X k 's has positive probability, as soon as each of them has positive probability (if all events may occur, then they may occur together).</p><p>Note that the sets Rk depend on X 1 , . . . , X N . However, if this family of variables is fixed, there is no loss in generality in restricting the space R X k to Rk and there for assume that P(X 1 = x 1 , . . . , X N = x N ) &gt; 0 everywhere. Proposition 13.5 Let X, Y , Z and W be random variables. The following properties are true.</p><p>(CI1) Symmetry:</p><p>Given proposition 15.5, proposition 15.3 can therefore be refined as follows.</p><p>Proposition 15.6 Let X be a Bayesian network on G. We have</p><p>Proposition 15.5 is also used in the proof of the following proposition.</p><p>Proposition 15.7 Let G = (V , E) be a directed acyclic graph, and X be a Bayesian network over G. Then, for all s ∈ S P(X (s) = x (s) | X (A s \{s}) = x (A s \{s}) ) = P(X (s) = x (s) | X (pa(s)) = x (s -) ) = p s (x (pa(s)) , x (s) ).</p><p>Proof By proposition 15.5, we can without loss of generality assume that V = A s . Then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><p>disappears when the conditional probability is normalized. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="15.2.2">Reduction to d-separation</head><p>We now want to reformulate proposition 15.6 in terms of the unoriented graph G ♭ and specific features in G called v-junctions, that we now define.</p><p>, s and u are parents of t).</p><p>We will say that a path</p><p>We have the lemma: Lemma 15.9 Two vertexes s and t in G are separated by a set U in (G A {s,t}∪U ) ♯ if and only if any path between s and t in G ♭ must either <ref type="bibr" target="#b19">(1)</ref> Pass at a vertex in U without a v-junction.</p><p>(2) Pass in V \ A {s,t}∪U at a v-junction.</p><p>A way to reduce the complexity is to assume that the graph G is singly connected, as defined below.</p><p>Definition 15.28 A DAG G is singly connected if there exists at most one path in G that connects any two vertexes. Such a property is true for a tree, but also holds for some networks with multiple parents. We have the following nice property in this case.</p><p>Proposition 15.29 Let G be a singly connected DAG and X a Bayesian network on G. If s is a vertex in G, the variables (X (t) , t ∈ pa(s)) are mutually independent.</p><p>Proof We have, using proposition 15.5,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(X</head><p>p u (y (pa(u)) , y (u) ).</p><p>Because the graph is singly connected, two parents of s cannot have a common ancestor (since there would then be two paths from this ancestor to S). So A pa(s) is the disjoint union of the A t 's for t ∈ pa(s) and we can write P(X (pa(s)) = x (pa(s)) ) = y (A pa(s) ) ,y (pa(s)) =x (pa(s)) t∈pa(s) u∈A t p u (y (pa(u)) , y</p><p>This proves the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Section 15.2.5 can be simplified under the assumption of a singly connected graph, at least for the computation of single vertex marginals; we have, if s ∈ V and G is singly connected</p><p>p s (y (pa(s)) , x (s) ) t∈pa(s) P(X (t) = y (t) ). <ref type="bibr">(15.7)</ref> This is now recursive in single vertex marginal probabilities. It moreover coincides with the recursive equation that defines the messages m C s s in <ref type="bibr">(15.5)</ref>, which shows that the sum-prod algorithm provides the correct answer in this case.</p><p>Notice that the optimal S in (20.2) is such that</p><p>Remark 20.3 The interest of discussing PCA associated with a covariance operator for a square integrable measure (in which case it is often called a Karhunen-Loeve (KL) expansion) is that this setting is often important when discussing infinitedimensional random processes (such as Gaussian random fields). Moreover, these operators quite naturally provide asymptotic versions of sample-based PCA. Interesting issues, that are part of functional data analysis <ref type="bibr" target="#b176">[158]</ref> </p><p>Introduce the covariance matrix of the data</p><p>Write A T = A μT , for short, in <ref type="bibr">(20.4)</ref>. We have:</p><p>The eigenvectors, f , of A T are such that Q 1/2 f are eigenvectors of the symmetric matrix Q 1/2 Σ T Q 1/2 , which shows that they form an orthogonal system in H, which will be orthonormal if the eigenvectors are normalized so that f T Qf = 1. Equivalently, they solve the generalized eigenvalue problem QΣ T Qf = λ 2 Qf , which may be preferred numerically to diagonalizing the non-symmetric matrix Σ T Q. This lower bound is attained when M and S can be diagonalized in the same orthonormal basis with λ 2 k = µ 2 k for k = 1, . . . , p. So, letting S = U DU T , where U is orthogonal and D is diagonal with decreasing numbers on the diagonal, an optimal M is given by M = U p D p U T p , where U p is formed with the first p columns of A and D p is the first p × p block of D. This shows that the matrix Y = U p D 1/2 provides a minimizer of F. The matrix U = [u (1) , . . . , u (N ) ] differs from the matrix A = [α (1) , . . . , α (N ) ] above through the normalization of its column vectors: we have</p><p>k , the same expression that was obtained before.</p><p>The minimization of F is called similarity matching. Clearly, this method can be applied when one starts directly with a matrix of dissimilarities S, provided it satisfies N l=1 s kl = 0 for all k. If this is not the case, then interpreting s kl as an inner product h T k h l , it is natural to replace s kl by what would give (h k -h) T (h l -h), namely, by</p><p>Interestingly, this discussion provides us with yet another interpretation of PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1.2">Dissimilarity matching</head><p>While the minimization of (21.5) did not provide us with a new way of analyzing the data (since it was equivalent to PCA), the direct comparison of dissimilarities, that is, the minimization of</p><p>over y 1 , . . . , y N ∈ R p , provides a different approach. Since this may be useful in practice and does not bring in much additional difficulty, we will allow for the possibility of weighting the differences in G and consider the minimization of</p><p>where W = (w kl ) is a symmetric matrix of non-negative weights. The only additional complexity resulting by adding weights is that the indeterminacy on y 1 , . . . , y N is that G(y) = G(y ′ ) as soon as yy ′ is constant on every connected component of the graph associated with the weight matrix W , so that the constraint on y should be replaced by</p><p>Another interpretation of this representation can be based on the random walk associated with the graph structure. Consider the random process t → q(t) defined as follows. The initial position, q(0) is selected according to some arbitrary distribution, say π 0 . Conditional to q(t) = k, the next position is determined by setting random waiting times τ kl , each distributed as an exponential distribution with rate β kl (or expectation 1/β kl ), and the process moves to the position l for which τ kl is smallest after waiting for that time. Let P (t) be the matrix with coefficients P (t, k, l) = P (q(t + s) = l | q(s) = k). Then, one has</p><p>where the right-hand side is the matrix exponential. If</p><p>are the eigenvalues of L with corresponding eigenvectors e 1 , . . . , e N , then</p><p>In particular, restricting the first eigenvectors of L provides an approximation of this stochastic process, i.e.,</p><p>We could also have considered the discrete-time version of the walk, for which, considering integer times t ∈ N,</p><p>Introducing the matrix B of similarities β kl (with zero on the diagonal) and the diagonal matrix D with coefficients d kk = N l=1,l k β kl , the r.h.s. of the previous equation is the k, l entry of the matrix P = D -1 B. Then, for any integer s, P (q(t +s</p><p>The Laplacian matrix L is given by L = D -B. The normalized Laplacian is We want to compare the training-set-averaged prediction error and the average insample error, namely compute the error bias</p><p>We make a heuristic argument to evaluate ∆ N . We can use the fact that θT minimizes the empirical error and write</p><p>which is an m by m symmetric matrix. Now, using the fact that θ 0 minimizes the mean square error (since f θ 0 (x) = E(Y |X = x)), we can write, for any T :</p><p>From these properties, one can easily derive a concentration inequality for the mean of independent random variables. We have M XN (λ) = N M X (λ/N ) and applying (22.3) we get, for any λ ≥ 0 and t &gt; 0</p><p>where the right-hand side may be infinite. Because this inequality is true for any λ, we have</p><p>where M * X,+ (u) = sup λ≥0 (λu -M X (λ)), which is non-negative since the maximized quantity vanishes for λ = 0. A symmetric computation yields</p><p>(this is the Fenchel-Legendre transform of the cumulant generating function, sometimes called the Cramér transform of X). One has M * X (m + t) = M * X,+ (m + t) for t &gt; 0. Indeed, because x → e λx is convex, Jensen's inequality implies that E(e λX ) ≥ e λm so that λ(m + t) -M X (λ) ≤ λt &lt; 0 if λ &lt; 0. Similarly, M * X (mt) = M * X,-(mt) for t &gt; 0. We therefore have the following result. Theorem 22.2 Let X 1 , . . . , X N be independent and identically distributed random variables. Assume that these variables are integrable and let m = E(X 1 ). Then, for all t &gt; 0,</p><p>The last inequality derives from</p><p>This is our first example of concentration inequality that shows that, when</p><p>with g k : R N -1 → R. Assume that, for all k = 1, . . . , N , one has 0 ≤ Z -Z k ≤ 1 and that</p><p>Then</p><p>Finally, for all λ ∈ R log E(e λ(Z-E(Z)) ) ≤ E(Z)(e λλ -1). <ref type="bibr">(22.16)</ref> 22.4 Bounding the empirical error with the VC-dimension</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.4.1">Introduction</head><p>Section 22.3 provides some of the most important inequalities used to evaluate the deviation of various combinations of independent random variables (e.g., their empirical mean) from their expectations (the reader may refer to Ledoux and Talagrand <ref type="bibr" target="#b135">[117]</ref>, Devroye et al. <ref type="bibr" target="#b78">[60]</ref>, Talagrand <ref type="bibr" target="#b206">[188]</ref>, Dembo and Zeitouni <ref type="bibr" target="#b77">[59]</ref>, Vershynin <ref type="bibr" target="#b217">[199]</ref> and other textbooks on the subject for further developments).</p><p>We now return to the problem of estimating the generalization error based on training data. For a given predictor f , concentration bounds allow us to control the probability P(R(f</p><p>where</p><p>and</p><p>for a training set T = (x 1 , y 1 , . . . , x N , y N ).</p><p>If this probability is small, then R(f ) ≤ RT (f ) + t with high probability, providing a likely upper bound to the generalization error of f . For example, if r is the 0-1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 22.17 The Vapnik-Chervonenkis dimension (or VC dimension) of the model class</head><p>(where the infimum of an empty set is +∞).</p><p>Remark 22.18 If, for a finite set A ⊂ R, one has |F (A)| = 2 |A| , one says that A is shattered by F . So VC-dim(F ) is the largest integer M such that there exists a set of cardinality M in R that is shattered by F . ♦</p><p>We now evaluate the growth of S * F (M) in terms of the VC-dimension, starting with the following lemma, which states that, if A is a finite subset of R, there are at least |F (A)| subsets of A that are shattered by F . Proceeding by induction, assume that the result is true if |A| ≤ N , and consider a set A ′ with |A ′ | = N +1. Assume that |F (A ′ )| ≥ 2 (otherwise there is nothing to prove), which implies that there exists x ∈ A ′ such that |F (x)| = 2. Take such an x and write</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and the induction hypothesis implies</head><p>If B ⊂ A is shattered by F 0 or F 1 , it is obviously shattered by F . Moreover, if B is shattered by both, then B ∪ {x} is shattered by F . The upper bound in the equation above is therefore less than the total number of sets shattered by F , which proves the lemma. Let γ &gt; 0. The γ-packing number M(G, ρ, γ), is the largest number n such that there exists a subset A ⊂ G with cardinality n such that any two distinct elements of A are at distance strictly larger than γ (such sets are called γ-nets).</p><p>When G and ρ are well understood from the context, we will write simply N (ϵ) and M(γ). Proposition 22.27 One has, for any γ &gt; 0:</p><p>Proof Let A be a maximal γ-net. Then, for all x ∈ G, there exists y ∈ A such that ρ(x, y) ≤ γ: otherwise A ∪ {x} would also be a γnet. This shows that max(ρ(x, A), x ∈ G) ≤ γ and N (G, ρ, γ) ≤ |A|.</p><p>Conversely, let A be a 2γ-net. Let G be an optimal γ-covering. Associate to each y ∈ A a point x ∈ G at distance less than γ: at least one exists because G is a covering. This defines a function f : A → G, which is necessarily one-to-one, because if two points in A map to the same point in G, the distance between these two points would be less than or equal to 2γ. This shows that M(G, ρ, 2γ) ≤ N (G, ρ, γ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>The entropy numbers of (G, ρ), denoted, for an integer N , e(G, ρ, N ) (or just e(N )) represent the best accuracy that can be achieved by subsets of G of size N , namely e(G, ρ, N ) = min G⊂G,|G|=N max{ρ(g, G) : g ∈ G}.</p><p>(22.26)</p><p>We have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5.2">A first union bound</head><p>Let Z be a random variable Z : Ω → Z. We will consider a space G of functions g : Z → R, such that (to simplify the discussion) E(g(Z)) = 0 for all g ∈ G. In this section, we assume that functions in G are bounded and let</p><p>and this upper bound can also be used to obtain a simpler (but weaker) form of theorem 22.31.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 22.32</head><p>The covering numbers of a class G of binary functions g with values in {-1, 1} can be controlled by the VC dimension of the class. Here, we consider ρ(g, g ′ ) = P(g g ′ ) = ρ 1 (g, g ′ )/2. Then, the following theorem holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 22.33 Let G be a class of binary functions such that</head><p>Then, there is a universal constant K such that, for any ϵ ∈ (0, 1),</p><p>with ρ(g, g ′ ) = P(g g ′ ).</p><p>We refer to Van der Vaart and Wellner <ref type="bibr" target="#b213">[195]</ref>, Theorem 2.6.4 for a proof, which is rather long and technical. ♦</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.5.6">Application</head><p>We quickly show how this discussion can be turned into results applicable to the classification problem. If F is a function class of binary classifiers and r is the risk function, one can consider the class</p><p>If the two sets in the right-hand side are not empty, i.e., the numbers N (1) and N (-1) of k's such that y k = 1 or y k = -1 are not zero, then</p><p>which is less that 2 N as soon as N &gt; 2. So, taking N &gt; 2, for (x 1 , y 1 , . . . , x N , y N ) to be shattered by G, we need N (1) = N or N (-1) = N and in this case, the inequality:</p><p>22.6 Other complexity measures 22.6.1 Fat-shattering and margins VC-dimension and metric entropy are measures that control the complexity of a model class, and can therefore be evaluated a priori without observing any data. These bounds can be improved, in general, by using information derived from the training set, and, particular the classification margin that has been obtained <ref type="bibr" target="#b36">[18]</ref>.</p><p>For this discussion, we need to return to the definition of covering numbers. If F is a function class, ρ ∞ the supremum metric on F , ϵ &gt; 0 and N is an integer, we let</p><p>that we will abbreviate in N ∞ (ϵ, N ) when F is known from the context. We will assume that functions in F take values values in [-1, 1], and we define for γ ≥ 0, y ∈ {0, 1}, u ∈ R:</p><p>correctly predicts y with margin γ and to 1 otherwise. We then define the classification error with margin γ as</p><p>and, given a training set T of size N</p><p>We then have the following theorem <ref type="bibr" target="#b28">[10]</ref>.</p><p>or, equivalently, with probability larger than 1δ, one has, for all f ∈ F ,</p><p>Proof We first note that, for N t 2 &gt; 2,</p><p>This discrepancy measures the extent to which estimators may differ when trained on two independent half-sized training sets. For a binary classification problem, the estimation of C T can be made with the same algorithm as the initial classifier, since E T 1 (f ) -E T 2 (f ) is, up to a constant, exactly the classification error for the training set in which the class labels are flipped for the data in T 2 .</p><p>Following <ref type="bibr">[20]</ref>, we now discuss concentration bounds that rely on C T and start with the following Lemma.</p><p>Lemma 22.39 Introduce the function</p><p>Then E(Φ(T )) ≤ 0.</p><p>Proof Note that, if T ′ is a training set, independent of T with identical distribution, then, for any</p><p>Now, for a given f , we have E T (f ) = 1 2 (E T 1 (f ) + E T 2 (f )) and splitting T ′ the same way,</p><p>we have</p><p>where we have used the fact that both (T ′ 1 , T 1 ) and (T ′ 2 , T 2 ) form random training sets with identical distribution to (T 1 , T 2 ). This proves that E(Φ(T )) ≤ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>Using the lemma, one can write</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P(sup</head><p>This implies that </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>■</head><p>We now discuss generalization bounds using Rademacher's complexity. While we still consider binary classification problems (with R Y = {-1, 1}), we will assume that F contains functions that can take arbitrary scalar values, and the 0-1 loss function becomes r(y, y ′ ) = 1 yy ′ ≤0 with y ∈ {-1, 1} and y ′ ∈ R. We will also consider functions that dominate this loss, i.e., functions ρ : R Y × R → [0, 1] such that r(y, y ′ ) ≤ ρ(y, y ′ ) for all y ∈ R Y , y ′ ∈ R. Some examples are the margin loss ρ * h (y, y ′ ) = 1 yy ′ ≤h for h ≥ 0, or the piecewise linear function</p><p>Our previous notation can then be rewritten as rad(T ) = Rad G (z 1 , . . . , z n ) where z i = (x i , y i ) and G is the space of functions: g : (x, y) → r(y, f (x)) for f ∈ F . The following theorem is proved in Koltchinskii and Panchenko <ref type="bibr" target="#b127">[109]</ref>, Bartlett and Mendelson <ref type="bibr" target="#b37">[19]</ref>. </p><p>Then P(sup</p><p>Proof For f ∈ F , we have</p><p>where </p><p>of which the statement of the theorem is a direct consequence. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="22.6.4">Algorithmic Stability</head><p>Another result using McDiarmid's inequality is proved in Bousquet and Elisseeff <ref type="bibr" target="#b57">[39]</ref>, and is based on the stability of a classifier when one removes a single example from the training set. As before, we consider training sets T of size N , where T is a random variable.</p><p>For k ∈ {1, . . . , N }, and a training set T = (x 1 , y 1 , . . . , x N , y N ), we let T (k) be the training set with sample (x k , y k ) removed. One says that the predictor (T → fT ) has uniform stability β N for the loss function r if, for all T of size N , all k ∈ {1, . . . , N }, and all x, y: |r( fT (x), y)r( fT (k) (x), y)| ≤ β N . <ref type="bibr">(22.45)</ref> With this definition, the following theorem holds. Of course, this theorem is interesting only when β N is small as a function of N , i.e., when N β N is bounded.</p><p>Proof Let Z i = (X i , Y i ) and F(Z 1 , . . . , Z N ) = R( fT ) -E T ( fT ). We want to apply McDiarmid inequality (theorem 22.13) to F, and therefore estimate Using this, we have</p><p>from which one deduces that</p><p>We therefore obtain P R( fT ) ≥ E T ( fT ) + ϵ + 2β N ≤ exp -</p><p>as required.</p><p>■</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Mini-batches . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">6 Continuous time limit and dynamical systems . . . . . . . . . . . . . . 11.6.1 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11.6.2 Adding a running cost . . . . . . . . . . . . . . . . . . . . . . . 20.6.3 Maximization over orthogonal matrices . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Parametric ICA . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Probabilistic ICA . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">7 Non-negative matrix factorization . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">8 Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">9 Bayesian factor analysis and Poisson point processes . . . . . . . . . . 20.9.1 A feature selection model . . . . . . . . . . . . . . . . . . . . . 20.9.2 Non-negative and count variables . . . . . . . . . . . . . . . . . 20.9.3 Feature assignment model . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">10Point processes and random measures . . . . . . . . . . . . . . . . . . 20.10.1Poisson processes . . . . . . . . . . . . . . . . . . . . . . . . . . 20.10.2The gamma process . . . . . . . . . . . . . . . . . . . . . . . . . 20.10.3The beta process . . . . . . . . . . . . . . . . . . . . . . . . . . . 20.10.4Beta Process and feature selection . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Data Visualization and Manifold Learning 549 21.1 Multidimensional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . 21.1.1 Similarity matching (Euclidean case) . . . . . . . . . . . . . . . 21.1.2 Dissimilarity matching . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Manifold learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.2.1 Isomap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21.2.2 Local Linear Embedding . . . . . . . . . . . . . . . . . . . . . . 21.2.3 Graph Embedding . . . . . . . . . . . . . . . . . . . . . . . . . 21.2.4 Stochastic neighbor embedding . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>21.2.5 Uniform manifold approximation and projection (UMAP) . .</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Generalization Bounds 573 22.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 Penalty-based Methods and Minimum Description Length . . . . . . . 22.2.1 Akaike&apos;s information criterion . . . . . . . . . . . . . . . . . . . 22.2.2 Bayesian information criterion and minimum description length576 22.3 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . 22.3.1 Cramér&apos;s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 22.3.2 Sub-Gaussian variables . . . . . . . . . . . . . . . . . . . . . . . 22.3.3 Bennett&apos;s inequality . . . . . . . . . . . . . . . . . . . . . . . . . 22.3.4 Hoeffding&apos;s inequality . . . . . . . . . . . . . . . . . . . . . . . 22.3.5 McDiarmid&apos;s inequality . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
	<note>22.3.6 Boucheron-Lugosi-Massart inequality . . . . . . . . . . . . .</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">4 Bounding the empirical error with the VC-dimension . . . . . . . . . 22.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.4.2 Vapnik&apos;s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 22.4.3 VC dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.4.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.4.5 Data-based estimates . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">5 Covering numbers and chaining . . . . . . . . . . . . . . . . . . . . . . 22.5.1 Covering, packing and entropy numbers . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">2 A first union bound . . . . . . . . . . . . . . . . . . . . . . . . . 22.5.3 Evaluating covering numbers . . . . . . . . . . . . . . . . . . . 22.5.4 Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.5.5 Metric entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22.5.6 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">6 Other complexity measures . . . . . . . . . . . . . . . . . . . . . . . . 22.6.1 Fat-shattering and margins . . . . . . . . . . . . . . . . . . . . . 22.6.2 Maximum discrepancy . . . . . . . . . . . . . . . . . . . . . . . 22.6.3 Rademacher complexity . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Algorithmic Stability . . . . . . . . . . . . . . . . . . . . . . . . 22.6.5 PAC-Bayesian bounds . . . . . . . . . . . . . . . . . . . . . . .</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">7 Application to model selection . . . . . . . . . . . . . . . . . . . . . . . Bibliography</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optimization algorithms on matrix manifolds</title>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Absil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolphe</forename><surname>Sepulchre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Information theory and an extension of the maximum likelihood principle</title>
		<author>
			<persName><forename type="first">Hirotugu</forename><surname>Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Symposium on Information Theory</title>
		<imprint>
			<publisher>Akademiai Kaido</publisher>
			<date type="published" when="1973">1973. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A stochastic algorithm for probabilistic independent component analysis</title>
		<author>
			<persName><forename type="first">Stéphanie</forename><surname>Allassonniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="160" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalesensitive dimensions, uniform convergence, and learnability</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Noga Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="615" to="631" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernels for vector-valued functions: A review</title>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000036</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<idno type="ISSN">1935-8237</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="266" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convergence properties of the gibbs sampler for perturbations of gaussians</title>
		<author>
			<persName><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Shape quantization and recognition with randomized trees</title>
		<author>
			<persName><forename type="first">Yali</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1545" to="1588" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random fields and inverse problems in imaging</title>
		<author>
			<persName><forename type="first">Alano</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobuyuki</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ecole d&apos;ete de Probabilites de Saint-Flour XVIII-1988</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="115" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reverse-time diffusion equation models. Stochastic Processes and their Applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="313" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neural network learning: Theoretical foundations</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
	<note>ICML&apos;17</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Theory of Reproducing Kernels</title>
		<author>
			<persName><forename type="first">Nachman</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the convergence of the markov chain simulation method</title>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">B</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hani</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="100" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Variational Baysian Framework for Graphical Models</title>
		<author>
			<persName><forename type="first">Hagai</forename><surname>Attias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining pac-bayesian and generic chaining bounds</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="863" to="889" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalizing swendsen-wang to sampling arbitrary posterior probabilities</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1239" to="1253" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Viorel</forename><surname>Barbu</surname></persName>
		</author>
		<title level="m">Differential equations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalization performance of support vector machines and other pattern classifiers</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel methodssupport vector learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model selection and error estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="85" to="113" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">63</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Introduction to nonlinear optimization: Theory, algorithms, and applications with MATLAB</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dynamics of stochastic approximation algorithms</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Benaïm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminaire de probabilites</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">XXXIII</biblScope>
			<biblScope unit="page" from="1" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Probability inequalities for the sum of independent random variables</title>
		<author>
			<persName><forename type="first">George</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">297</biblScope>
			<biblScope unit="page" from="33" to="45" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive algorithms and stochastic approximations</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Benveniste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Métivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Priouret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Berglund</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12998</idno>
		<title level="m">Long-time dynamics of stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex optimization theory</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matrix analysis</title>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Bhatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Science &amp; Business Media</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Mathematical statistics: basic ideas and selected topics</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kjell</forename><forename type="middle">A</forename><surname>Doksum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Simultaneous analysis of lasso and dantzig selector</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">B</forename><surname>Ya'acov Ritov</surname></persName>
		</author>
		<author>
			<persName><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Probability and measure</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Billingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Convergence of probability measures</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Billingsley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Vorlesungen über fouriersche integrale</title>
		<author>
			<persName><forename type="first">Salomon</forename><surname>Bochner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull Amer Math Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">184</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Measure Theory</title>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">I</forename><surname>Bogachev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Numerical optimization: theoretical and practical aspects</title>
		<author>
			<persName><forename type="first">Joseph-Frédéric</forename><surname>Bonnans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Charles</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Lemaréchal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><forename type="middle">A</forename><surname>Sagastizábal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J F</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Convex analysis and nonlinear optimization: theory and examples</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Borwein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A sharp concentration inequality with applications</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="277" to="292" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Stability and generalization</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="499" to="526" />
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<title level="m">Classification and regression trees</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A course in metric geometry</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Iu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><forename type="middle">A</forename><surname>Burago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><surname>Ivanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A dendrite method for cluster analysis</title>
		<author>
			<persName><forename type="first">Tadeusz</forename><surname>Cali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerzy</forename><surname>Harabasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-theory and Methods</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. information theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The dantzig selector: statistical estimation when p is much larget</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terence</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gap: a factor model for discrete data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 27th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An iterative Gibbsian technique for reconstruction of m-ary images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chalmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="747" to="761" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6571" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Independent component analysis, a new concept? Signal processing</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Comon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joy</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Probabilistic networks and expert systems</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">G</forename><surname>Cowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Philip</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Analyse générale des liaisons stochastiques: etude particulière de l&apos;analyse factorielle linéaire</title>
		<author>
			<persName><forename type="first">George</forename><surname>Darmois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Revue de l&apos;Institut international de statistique</title>
		<imprint>
			<biblScope unit="page" from="2" to="8" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Convergence of a stochastic approximation version of the em algorithm</title>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Delyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lavielle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="94" to="128" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Large deviations techniques and applications</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ofer</forename><surname>Zeitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Applications of Mathematics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="1998">1998. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A Probabilistic Theory of Pattern Recognition</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lázl Ó Györfi</surname></persName>
		</author>
		<author>
			<persName><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">The total variation distance between high-dimensional gaussians with the same mean</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Devroye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommy</forename><surname>Reddad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08693</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Infinitesimal Calculus</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Dieudonné</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Edsger</surname></persName>
		</author>
		<author>
			<persName><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische mathematik</title>
		<idno type="ISSN">0029-599X</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Clustering large graphs via the singular value decomposition</title>
		<author>
			<persName><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Frieze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwanathan</forename><surname>Vinay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="9" to="33" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Hybrid monte carlo</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Duane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">J</forename><surname>Pendleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Roweth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics letters B</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="222" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Real analysis and probability</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Random iterative models</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Duflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Springer Science &amp; Business Media</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Nonlinear optimization: Methods and applications</title>
		<author>
			<persName><forename type="first">Carl-Louis</forename><surname>Ha Eiselt</surname></persName>
		</author>
		<author>
			<persName><surname>Sandblom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Markov processes: Characterization and convergence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Ethier</surname></persName>
		</author>
		<author>
			<persName><surname>Kurtz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. International journal of computer vision</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">An interruptible algorithm for perfect sampling via Markov chains</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Fill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="162" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Principal geodesic analysis on symmetric spaces: Statistics of diffusion tensors</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarang</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and mathematical methods in medical and biomedical image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)</title>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of statistics</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="337" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
			<publisher>JSTOR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">On the logic of causal models</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine intelligence and pattern recognition</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Identifying independence in bayesian networks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1002/net.3230200504</idno>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="507" to="534" />
			<date type="published" when="1990-08">August 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Classifying gene expression profiles from pairwise mrna comparisons</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Q</forename><surname>Christian D'avignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimond L</forename><surname>Naiman</surname></persName>
		</author>
		<author>
			<persName><surname>Winslow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical applications in genetics and molecular biology</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Stochastic relaxation, gibbs distributions, and the bayesian restoration of images</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Nonparametric maximum likelihood estimation by the method of sieves</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chii-Ruey</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="401" to="414" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Gondran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minoux</surname></persName>
		</author>
		<title level="m">Graphs and algorithms</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Abstract Inference</title>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Grenander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Theory of T-norms and fuzzy inference methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<idno type="ISSN">0165-0114</idno>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="450" />
			<date type="published" when="1991-04">April 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Nonnegative entropy measures of multivariate symmetric correlations</title>
		<author>
			<persName><forename type="first">Te</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="133" to="156" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hastings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Stochastic neighbor embedding. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Optimal Control: An Introduction to the Theory with Applications</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">M</forename><surname>Hocking</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Probability inequalities for sums of bounded random variables</title>
		<author>
			<persName><forename type="first">Wassily</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Collected Works of Wassily Hoeffding</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="409" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix analysis</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">New approximations of differential entropy for independent component analysis and projection pursuit</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="273" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Stochastic differential equations and diffusion processes</title>
		<author>
			<persName><forename type="first">Nobuyuki</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinzo</forename><surname>Watanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Variational methods for inference and estimation in graphical models</title>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Sakari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakkola</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">O jistem problemu minimalnim (about a certain minimal problem)</title>
		<author>
			<persName><forename type="first">Vojtech</forename><surname>Jarnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prace Moravske Prirodovedecke Spolecnosti</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="57" to="63" />
			<date type="published" when="1930">1930</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Optimal junction trees</title>
		<author>
			<persName><forename type="first">Finn</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Tenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="360" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">Characterization problems in mathematical statistics</title>
		<author>
			<persName><forename type="first">Abram</forename><forename type="middle">M</forename><surname>Kagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calyampudi Radhakrishna</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurij</forename><surname>Vladimirovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnik</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">An Introduction to Variational Autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="307" to="392" />
			<date type="published" when="2019">2019</date>
			<publisher>Now Publishers, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Completely random measures</title>
		<author>
			<persName><forename type="first">John</forename><surname>Kingman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pacific Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="78" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Numerical solutions of stochastic differential equations</title>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Kloeden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Platen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Normalizing flows: An introduction and review of current methods</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3964" to="3979" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Empirical margin distributions and bounding the generalization error of combined classifiers</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Koltchinskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="50" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A criterion for determining the number of groups in a data set using sum-of-squares clustering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wojtek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Coupling a stochastic approximation version of EM with an MCMC procedure</title>
		<author>
			<persName><forename type="first">Estelle</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lavielle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESAIM: Probability and Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="115" to="131" />
			<date type="published" when="2004">2004</date>
			<publisher>EDP Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<monogr>
		<title level="m" type="main">Stochastic approximation and recursive algorithms and applications</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>George Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title/>
		<author>
			<persName><surname>Steffen L Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical models</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
	<note>The handbook of brain theory and neural networks</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Theory of point estimation</title>
		<author>
			<persName><forename type="first">Erich</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Simulating Hamiltonian Dynamics</title>
		<author>
			<persName><forename type="first">Benedict</forename><surname>Leimkuhler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cambridge Monographs on Applied and Computational Mathematics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Analysis of recursive stochastic algorithms</title>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Ljung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<idno type="ISSN">1558-2523</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="575" />
			<date type="published" when="1977-08">August 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<title level="m" type="main">Introduction to Optimal Control Theory</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Macki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Strauss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Adam A Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katia</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><surname>Stolovitzky</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-7-S1-S7</idno>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<idno type="ISSN">1471- 2105</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2006-01">January 2006</date>
		</imprint>
	</monogr>
	<note>Riccardo Dalla Favera, and Andrea Califano Suppl 1:S7</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Simulated tempering: a new monte carlo scheme</title>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Marinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhysics letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">451</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Concentration inequalities and model selection</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Massart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Pac-bayesian model averaging</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="164" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main">Algorithmic graph theory</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Mchugh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Prentice-Hall Inc</publisher>
			<pubPlace>New Jersey</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<title level="m" type="main">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<idno>arXiv: 1802.03426</idno>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Stochastic integrals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><surname>Mckean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Rates of convergence of the hastings and metropolis algorithms</title>
		<author>
			<persName><forename type="first">Kerrie</forename><forename type="middle">L</forename><surname>Mengersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="121" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Equation of state calculations by fast computing machines</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Metropolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arianna</forename><forename type="middle">W</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><forename type="middle">N</forename><surname>Rosenbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusta</forename><forename type="middle">H</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1092" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Stability of markovian processes ii: Continuous-time processes and sampled chains</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="487" to="517" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Stability of markovian processes iii: Foster-lyapunov criteria for continuous-time processes</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="518" to="548" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Markov chains and stochastic stability</title>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Tweedie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">A trace inequality of john von neumann. Monatshefte für mathematik</title>
		<author>
			<persName><forename type="first">Leon</forename><surname>Mirsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="303" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Théorèmes de convergence presque sure pour une classe d&apos;algorithmes stochastiques à pas décroissant. Probability Theory and related fields</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Métivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Priouret</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="403" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elizbar</surname></persName>
		</author>
		<author>
			<persName><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Sampling from multimodal distributions using tempered transitions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="353" to="366" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.1901</idno>
		<title level="m">Mcmc using hamiltonian dynamics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Learning Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Neapolitan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Robust Stochastic Approximation Approach to Stochastic Programming</title>
		<author>
			<persName><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<idno type="ISSN">1052- 6234</idno>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009-01">January 2009</date>
			<publisher>Publisher: Society for Industrial and Applied Mathematics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Nonlinear Equations</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main">General irreducible Markov chains and non-negative operators</title>
		<author>
			<persName><forename type="first">Esa</forename><surname>Nummelin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">The maximum clique problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Panos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<idno type="ISSN">0925-5001</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="328" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Probabilistic reasoning in intelligent systems</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988. 2012</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Approximating k-means-type clustering via semidefinite programming</title>
		<author>
			<persName><forename type="first">Jiming</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on optimization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="205" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title level="m" type="main">A new theoretical framework for k-means-type clustering. Foundations and advances in data mining</title>
		<author>
			<persName><forename type="first">Jiming</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="79" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Functional estimation for density, regression models and processes</title>
		<author>
			<persName><forename type="first">Odile</forename><surname>Pons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>World scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Shortest connection networks and some generalizations</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Prim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1389" to="1401" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Exact sampling with coupled Markov chains and applications to statistical mechanics</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Propp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures and Algorithms</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1&amp;2</biblScope>
			<biblScope unit="page" from="223" to="252" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">How to get a perfectly random sample from a generic Markov chain and generate a random spanning tree of a directed graph</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">G</forename><surname>Propp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="170" to="217" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<author>
			<persName><forename type="first">Jim</forename><forename type="middle">O</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<title level="m">Functional Data Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Nonparametric functional estimation</title>
		<author>
			<persName><forename type="first">Rao</forename><surname>Bls Prakasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<title level="m" type="main">Markov chains</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Revuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<title level="m" type="main">Stochastic complexity in statistical inquiry</title>
		<author>
			<persName><forename type="first">Jorma</forename><surname>Rissanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Herbert Robbins Selected Papers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">On the geometric convergence of the gibbs sampler</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="377" to="384" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">General state space markov chains and mcmc algorithms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="20" to="71" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Exponential convergence of langevin distributions and their discrete approximations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tweedie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="363" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Tyrrell</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joachim</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alejandro</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">A deterministic annealing approach to clustering</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eitan</forename><surname>Gurewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="589" to="594" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<title level="m" type="main">Real and Complex Analysis</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Tata McGraw Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<title level="m" type="main">Metric spaces and completely monotone functions. Annals of Mathematics</title>
		<author>
			<persName><forename type="first">Isaac</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1938">1938</date>
			<biblScope unit="page" from="811" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Bell system technical journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Communication in the presence of noise</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Institute of Radio Engineers</title>
		<meeting>Institute of Radio Engineers</meeting>
		<imprint>
			<date type="published" when="1949">1949</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">A reliable data-based bandwidth selection method for kernel density estimation</title>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">J</forename><surname>Sheather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="690" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main">Density estimation for statistics and data analysis</title>
		<author>
			<persName><forename type="first">Bernard</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Chapman et Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Linear forms of independent random variables and the normal distribution law</title>
		<author>
			<persName><forename type="first">Pavlovich</forename><surname>Viktor</surname></persName>
		</author>
		<author>
			<persName><surname>Skitovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematicheskaya</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="200" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Sliced score matching: A scalable approach to density and score estimation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Sur la division des corp materiels en parties</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Steinhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Acad. Polon. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">804</biblScope>
			<biblScope unit="page">801</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<title level="m" type="main">Consistent nonparametric regression. The annals of statistics</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="595" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictions</title>
		<author>
			<persName><forename type="first">Mervyn</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="133" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Finding the number of clusters in a dataset: An information-theoretic approach</title>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Sugar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gareth</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">463</biblScope>
			<biblScope unit="page" from="750" to="763" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Nonuniversal critical dynamics in monte carlo simulations</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">H</forename><surname>Swendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Sheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<monogr>
		<title level="m" type="main">Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent of the log-likelihood</title>
		<author>
			<persName><forename type="first">Esteban</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">The generic chaining: upper and lower bounds of stochastic processes</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<monogr>
		<title level="m" type="main">Upper and lower bounds for stochastic processes: modern methods and classical problems</title>
		<author>
			<persName><forename type="first">Michel</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Simple decision rules for classifying human cancers from gene expression profiles</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">Q</forename><surname>Aik Choon Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Naiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimond</forename><forename type="middle">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Winslow</surname></persName>
		</author>
		<author>
			<persName><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="3896" to="3904" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Estimating the number of clusters in a data set via the gap statistic</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walther</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="423" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Markov Chains for Exploring Posterior Distributions</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Tierney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<idno type="ISSN">0090-5364</idno>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2168" to="8966" />
			<date type="published" when="1994-12">December 1994</date>
			<publisher>Publisher: Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<title level="m" type="main">Van der Vaart. Asymptotic statistics</title>
		<author>
			<persName><forename type="first">W</forename><surname>Aad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main">Weak convergence and empirical processes with applications to statistics</title>
		<author>
			<persName><forename type="first">Aad</forename><forename type="middle">W</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Vaart</surname></persName>
		</author>
		<author>
			<persName><surname>Wellner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer science &amp; business media</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<title level="m" type="main">High-dimensional probability: An introduction with applications in data science</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Generalized principal component analysis (gpca)</title>
		<author>
			<persName><forename type="first">Rene</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1945" to="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Spline Models for Observational Data</title>
		<author>
			<persName><forename type="first">Grace</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smooth regression analysis</title>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="page" from="359" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main">Image analysis, random fields and Markov chain Monte Carlo methods</title>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main">Optimization for data analysis</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Kôsaku Yosida. Functional Analysis</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Estimation and annealing for gibbsian fields</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. de l&apos;Inst. Henri Poincaré</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Parametric inference for imperfectly observed gibbsian fields</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prob. Thry. Rel. Fields</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="625" to="645" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastics: An International Journal of Probability and Stochastic Processes</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="177" to="228" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">Diffeomorphic learning</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Fuzzy sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="394" to="432" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
